FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Shi, JY
   Li, TY
   Xu, JX
AF Shi, Jiayi
   Li, Taiyong
   Xu, Jiaxuan
TI Recursive lightweight convolutional neural networks that make noisy
   images purer and purer
SO VISUAL COMPUTER
LA English
DT Article
DE Image denoising; Convolutional neural networks; Lightweight networks;
   Recursive networks; Depthwise separable convolution; Pointwise
   convolution
ID CNN; MINIMIZATION; FRAMEWORK; SPARSE
AB Convolutional neural network (CNN) has shown its superpower in image denoising in recent years. However, most CNN models suffer from a large number of model parameters and the effect of image denoising still needs to be improved. To cope with these issues, we propose a recursive lightweight CNN approach that can make the noisy images purer and purer, namely PPNets, in this paper. The PPNets mainly consist of four parts: separable convolution-batch normalization-ReLU (SCBR) blocks to extract coarse features, bottlenecks with skip connection to integrate coarse features and refined features to enhance expression ability of model, noise proposal network with an attention mechanism to predict noise level and recursive strategies to stack the denoising model to make the noisy images purer and purer. Since SCBR uses depthwise convolution and pointwise convolution to replace traditional convolution operations, the proposed PPNets have fewer weight parameters. We conduct extensive experiments on two gray image datasets and three color image datasets. The experimental results demonstrate that the PPNets are significantly superior to the traditional models in denoising effectiveness. At the same time, the PPNets outperform the compared state-of-the-art CNN models in terms of both denoising effectiveness and the number of model parameters.
C1 [Shi, Jiayi; Li, Taiyong; Xu, Jiaxuan] Southwestern Univ Finance & Econ, Sch Comp & Artificial Intelligence, Chengdu 611130, Peoples R China.
C3 Southwestern University of Finance & Economics - China
RP Li, TY (corresponding author), Southwestern Univ Finance & Econ, Sch Comp & Artificial Intelligence, Chengdu 611130, Peoples R China.
EM litaiyong@gmail.com
RI Li, Taiyong/ABG-3630-2020; Li, Taiyong/ABE-4602-2021
OI Li, Taiyong/0000-0002-1546-8015; Li, Taiyong/0000-0002-1546-8015
FU Ministry of Education of Humanities and Social Science Project;
   Scientific Research Fund of Sichuan Provincial Education Department; 
   [19YJAZH047];  [17ZB0433]
FX AcknowledgementsThis work was supported by the Ministry of Education of
   Humanities and Social Science Project (Grant no. 19YJAZH047) and the
   Scientific Research Fund of Sichuan Provincial Education Department
   (Grant no. 17ZB0433).
CR Bai Yu., 2022, NEUROCOMPUTING MSPNE
   Benesty J, 2010, INT CONF ACOUST SPEE, P205, DOI 10.1109/ICASSP.2010.5496033
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Burger H., 2012, CVPR
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chen C, 2018, LECT NOTES COMPUT SC, V11215, P3, DOI 10.1007/978-3-030-01252-6_1
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   El Mahdaoui A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062199
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fang FM, 2014, SIAM J IMAGING SCI, V7, P969, DOI 10.1137/130919696
   Franzen R, 1999, Kodak lossless true color image suite
   Goyal B, 2020, INFORM FUSION, V55, P220, DOI 10.1016/j.inffus.2019.09.003
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang J, 2022, KNOWL-BASED SYST, V255, DOI 10.1016/j.knosys.2022.109776
   Islam MR, 2017, INT J PATTERN RECOGN, V31, DOI 10.1142/S0218001417540222
   Jain V., 2009, ADV NEURAL INFORM PR, P769, DOI DOI 10.5555/2981780.2981876
   Jia F, 2021, IEEE INT CONF COMP V, P354, DOI 10.1109/ICCVW54120.2021.00044
   Jia F, 2021, IEEE SIGNAL PROC LET, V28, P1600, DOI 10.1109/LSP.2021.3100263
   Jia XF, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.6.063031
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li G, 2020, IEEE ACCESS, V8, P27495, DOI 10.1109/ACCESS.2020.2971760
   Li WM, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14092215
   Li XH, 2021, ISPRS J PHOTOGRAMM, V179, P14, DOI 10.1016/j.isprsjprs.2021.07.007
   Liu G, 2022, INFORM SCIENCES, V610, P171, DOI 10.1016/j.ins.2022.07.122
   Liu HS, 2022, IEEE T IMAGE PROCESS, V31, P5677, DOI 10.1109/TIP.2022.3193754
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Muhammad W, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0249278
   Qiao S, 2022, KNOWL-BASED SYST, V254, DOI 10.1016/j.knosys.2022.109587
   Quan YH, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107639
   Rawat S, 2021, BIOMED SIGNAL PROCES, V69, DOI 10.1016/j.bspc.2021.102859
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth S, 2005, PROC CVPR IEEE, P860
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109050
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Tian CW, 2019, CAAI T INTELL TECHNO, V4, P17, DOI 10.1049/trit.2018.1054
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang JW, 2020, APPL INTELL, V50, P1045, DOI 10.1007/s10489-019-01587-1
   Wang SS, 2012, J VIS COMMUN IMAGE R, V23, P1008, DOI 10.1016/j.jvcir.2012.06.011
   Xuan L, 2022, IEEE T CIRCUITS-II, V69, P4003, DOI 10.1109/TCSII.2022.3180553
   Zhang JH, 2022, IET IMAGE PROCESS, V16, P2446, DOI 10.1049/ipr2.12499
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   [张黎明 ZHANG Liming], 2011, [生态环境学报, Ecology and Environmental Sciences], V20, P1
   Zhang ML, 2017, IET IMAGE PROCESS, V11, P54, DOI 10.1049/iet-ipr.2016.0098
   Zhang XD, 2013, IEEE T IMAGE PROCESS, V22, P408, DOI 10.1109/TIP.2012.2214043
   Zhang YL, 2021, IEEE T IMAGE PROCESS, V30, P6255, DOI 10.1109/TIP.2021.3093396
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang Z, 2020, SIGNAL IMAGE VIDEO P, V14, P737, DOI 10.1007/s11760-019-01606-1
   Zhano M, 2008, IEEE T IMAGE PROCESS, V17, P2324, DOI 10.1109/TIP.2008.2006658
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
   Zuo ZM, 2022, IEEE WINT CONF APPL, P739, DOI 10.1109/WACVW54805.2022.00081
NR 64
TC 1
Z9 1
U1 2
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6571
EP 6587
DI 10.1007/s00371-022-02749-y
EA DEC 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000904857900002
DA 2024-07-18
ER

PT J
AU Yi, YF
   Xu, YP
   Ye, ZY
   Li, LH
   Hu, XL
   Tian, Y
AF Yi, Yufan
   Xu, Yiping
   Ye, Ziyi
   Li, Linhui
   Hu, Xinli
   Tian, Yan
TI STAN: spatiotemporal attention network for video-based facial expression
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Video-based facial expression recognition; Spatiotemporal attention
   network; Multilevel spatial attention network; Attention-based score
   fusion
ID EMOTION RECOGNITION; IMAGE; DEEP
AB Video-based facial expression recognition is a very challenging task. The expression features portrayed by traditional ResNet18 are not rich enough, while the classical method LSTM to process expression videos may not extract effective temporal features for cases with weaker emotion intensity. This paper proposes a spatiotemporal attention network to extract more diverse spatial features and more effective temporal relationships. Firstly, a spatial attention module is used to enhance the expression features extracted by the ResNet18 and remove redundancy. Then, multiple levels of information are combined to extract richer expression features. Meanwhile, the video stream is divided into a series of video clips using a sliding window, and the temporal features are extracted from each small clip by the LSTM, which is a simple but effective way to divide the video. Third, considering the different importance of expression features extracted from each window for the results of expression recognition of the whole video, an attention-based score fusion module is proposed to fuse expression information from multiple windows. We perform comprehensive experiments on in-the-wild FER benchmarks (AFEW8.0 and HUST-MM). Quantitative and qualitative analyses demonstrate the effectiveness of our proposed method.
C1 [Yi, Yufan; Xu, Yiping; Ye, Ziyi; Li, Linhui; Tian, Yan] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Hu, Xinli] Chinese Acad Sci, Inst Remote Sensing Applicat, Beijing 100094, Peoples R China.
C3 Huazhong University of Science & Technology; Chinese Academy of Sciences
RP Xu, YP (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM yufanyi@hust.edu.cn; xuyiping@hust.edu.cn; yeziyi19980703@126.com;
   lilinhui@hust.edu.cn; huxl@radi.ac.cn; tianyan@hust.edu.cn
RI matin, aston/IYJ-4678-2023
OI matin, aston/0000-0003-4527-5270
FU National Key R&D Program of China [2020YFC0833102]
FX This study was supported by the National Key R&D Program of China under
   Grant 2020YFC0833102.
CR Cao Z., 2022, arXiv
   Cao ZW, 2021, IEEE WINT CONF APPL, P1187, DOI 10.1109/WACV48630.2021.00123
   Chen LF, 2018, INFORM SCIENCES, V428, P49, DOI 10.1016/j.ins.2017.10.044
   Choi DY, 2020, MULTIMED TOOLS APPL, V79, P28169, DOI 10.1007/s11042-020-09412-5
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao JX, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.763100
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2022, IEEE T CIRC SYST VID, V32, P1089, DOI 10.1109/TCSVT.2021.3074259
   Hu M, 2022, VISUAL COMPUT, V38, P2617, DOI 10.1007/s00371-021-02136-z
   Hu M, 2019, J VIS COMMUN IMAGE R, V59, P176, DOI 10.1016/j.jvcir.2018.12.039
   Huang D.-Y., 2017, P 19 ACM INT C MULT, P577, DOI DOI 10.1145/3136755.3143012
   Huang QH, 2021, INFORM SCIENCES, V580, P35, DOI 10.1016/j.ins.2021.08.043
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang XX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2881, DOI 10.1145/3394171.3413620
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kim D.H., 2017, P ICMI 2017 19 ACM I, P529, DOI [10.1145/3136755.3143005, DOI 10.1145/3136755.3143005]
   Kumar Vikas, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P756, DOI 10.1007/978-3-030-66415-2_53
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Mavani V, 2017, IEEE INT CONF COMP V, P2783, DOI 10.1109/ICCVW.2017.327
   Meng DB, 2019, IEEE IMAGE PROC, P3866, DOI [10.1109/ICIP.2019.8803603, 10.1109/icip.2019.8803603]
   Müller R, 2019, ADV NEUR IN, V32
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Salgado P, 2021, LECT NOTES COMPUT SC, V12861, P322, DOI 10.1007/978-3-030-85030-2_27
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sun N, 2019, PATTERN RECOGN LETT, V119, P49, DOI 10.1016/j.patrec.2017.10.022
   Tan JX, 2022, IEEE T NETW SCI ENG, V9, P888, DOI 10.1109/TNSE.2021.3139671
   Vielzeuf V., 2017, P 19 ACM INT C MULT, P569
   Wang HW, 2018, LECT NOTES COMPUT SC, V10996, P109, DOI 10.1007/978-3-319-97909-0_12
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wen Z., 2021, arXiv
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yan L, 2022, ARXIV
   Yan LQ, 2022, IEEE T CIRC SYST VID, V32, P6642, DOI [10.1109/TCSVT.2022.3177320, 10.1109/tcsvt.2022.3177320]
   Zhang CB, 2021, IEEE T IMAGE PROCESS, V30, P5984, DOI 10.1109/TIP.2021.3089942
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 48
TC 0
Z9 0
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6205
EP 6220
DI 10.1007/s00371-022-02721-w
EA NOV 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000885420600001
DA 2024-07-18
ER

PT J
AU Liu, Y
   Wan, L
   Lyu, F
   Feng, W
AF Liu, Ye
   Wan, Liang
   Lyu, Fan
   Feng, Wei
TI Fine-grained scale space learning for single image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Single-image super-resolution; Fine-grained scale space; Base model;
   Boosting and cascading
ID NETWORK
AB Recent deep convolutional neural networks have achieved great reconstruction accuracy for single image super-resolution (SISR). Most of them, however, need to train a specific set of parameters for a single scaling factor or a particular group of scaling factors. This means, multiple sets of model parameters have to be used for different scaling factors, each of which can be already very large. In this paper, we study a new problem of fine-grained scale space learning of SISR, which uses one set of parameters while achieving varying scales. Specifically, we aim to use an arbitrary base SISR x 2 model to realize high-quality SISR for a continuous-integer spectrum of scaling factors, e.g., 2 similar to 8. To this end, first for the base scaling factor 2, we propose low-resolution reconstruction, blind kernel estimation and recursive error compensation, to generate three loss functions, helping to boost the training quality of the base model. Then, we cascade the boosted SISR x2 model and extend the low-resolution reconstruction to incorporate multiple LR loss functions covering {3, 4, ..., 2(n)} scales. By this way, the SISR x2 model can be effectively tuned to work well for continuous-integer scaling factors, with exactly the same set of parameters. Extensive experiments verify the capability of our approach to enable state-of-the-art methods to realize fine-grained scale space learning of SISR, with higher accuracy and much less parameters.
C1 [Liu, Ye; Wan, Liang; Lyu, Fan; Feng, Wei] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
C3 Tianjin University
RP Wan, L (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
EM liuye321@tju.edu.cn; lwan@tju.edu.cn; fanlyu@tju.edu.cn;
   wfeng@tju.edu.cn
RI Feng, Wei/E-3985-2016; Lyu, Fan/AAC-3865-2020
OI Lyu, Fan/0000-0002-0878-5485
FU research fund for The Tianjin Key Lab for Advanced Signal Processing,
   Civil Aviation University of China [2019ASP-TJ01]; National Natural
   Science Foundation of China [61902275]
FX The work is supported by the research fund for The Tianjin Key Lab for
   Advanced Signal Processing, Civil Aviation University of China
   (2019ASP-TJ01) and the National Natural Science Foundation of China
   (Grant No. 61902275).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chen H., 2021, ADDERNET WE REALLY N
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Feng W, 2015, IEEE I CONF COMP VIS, P1260, DOI 10.1109/ICCV.2015.149
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kong XT, 2021, PROC CVPR IEEE, P12011, DOI 10.1109/CVPR46437.2021.01184
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Y., CGI 2022
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Pan JS, 2020, AAAI CONF ARTIF INTE, V34, P11807
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Thornton MW, 2006, INT J REMOTE SENS, V27, P473, DOI 10.1080/01431160500207088
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang Lucy Lu, 2020, CORD 19 COVID 19 OPE
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yoon, 2020, ARXIV PREPRINT ARXIV
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 40
TC 2
Z9 2
U1 4
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3377
EP 3389
DI 10.1007/s00371-022-02551-w
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000825206000001
DA 2024-07-18
ER

PT J
AU Chu, YF
   Liu, Z
   Liu, TT
   Samsonovich, AV
   Chai, YJ
AF Chu, Yifan
   Liu, Zhen
   Liu, Tingting
   Samsonovich, Alexei V.
   Chai, Yanjie
TI Physical simulation of oscillation and falling effects of objects in
   indoor earthquake scenarios
SO VISUAL COMPUTER
LA English
DT Article
DE Earthquake; Nonstructural components; Scenario simulation; Physical
   model
ID EVACUATION SIMULATION; CROWD EVACUATION; PERFORMANCE; BEHAVIOR; DESIGN
AB When an earthquake occurs, indoor objects oscillate and fall, creating a hazardous evacuation environment. However, physical effects of oscillating and falling indoor objects during earthquakes are often ignored in the existing crowd emergency evacuation simulation studies. As a result, existing models will produce predictions that differ from the outcomes of real events. Here we propose a physics-based simulation model for an indoor seismic event scenario, focusing on movable and flexible components. We predict the motion of movable components during earthquakes using simulations based on seismic data and physical laws. In doing this, we also simulate oscillations of flexible components using a driven harmonic oscillator model. The results showed that the simulated scenario had a high degee of physical realism and rationality.
C1 [Chu, Yifan; Liu, Zhen; Chai, Yanjie] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo, Peoples R China.
   [Liu, Tingting] Ningbo Univ, Coll Sci & Technol, Ningbo, Peoples R China.
   [Samsonovich, Alexei V.] Natl Res Nucl Univ MEPhI, Moscow, Russia.
C3 Ningbo University; Ningbo University; National Research Nuclear
   University MEPhI (Moscow Engineering Physics Institute)
RP Liu, Z (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo, Peoples R China.
EM liuzhen@nbu.edu.cn
FU Ministry of Science and Higher Education of the Russian Federation
   [0723-2020-0036]; Ningbo Science Technology Plan projects [2021S091]
FX This work was supported by the Ministry of Science and Higher Education
   of the Russian Federation (state assignment Project No. 0723-2020-0036)
   and Ningbo Science Technology Plan projects (Grants 2021S091).
CR [Anonymous], 2011, Fundamentals of Physics
   [Anonymous], 2008, 177422008 GBT
   Bauchau OA, 2009, SOLID MECH APPL, V163, P173
   Behera D, 2016, Fuzzy Differential Equations and Applications for Engineers and Scientists, DOI DOI 10.1002/9781119004233
   Chen JY, 2021, SAFETY SCI, V142, DOI 10.1016/j.ssci.2021.105378
   Choi MJ, 2019, COMPUT METHOD APPL M, V351, P153, DOI 10.1016/j.cma.2019.03.032
   Dong B, 2018, BUILD SIMUL-CHINA, V11, P899, DOI 10.1007/s12273-018-0452-x
   Du Z, 2021, RUSS PHYS J+, V64, P1303, DOI 10.1007/s11182-021-02456-6
   Feng ZA, 2020, ADV ENG INFORM, V46, DOI 10.1016/j.aei.2020.101134
   Feng ZA, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101118
   He J, 2021, EARTHQ SPECTRA, V37, P95, DOI 10.1177/8755293020957353
   Klepeis NE, 2001, J EXPO ANAL ENV EPID, V11, P231, DOI 10.1038/sj.jea.7500165
   Li CY, 2017, IEEE T VIS COMPUT GR, V23, P1388, DOI 10.1109/TVCG.2017.2656958
   Li ZW, 2020, AUTOMAT CONSTR, V120, DOI 10.1016/j.autcon.2020.103395
   Liu Q, 2018, PHYSICA A, V502, P315, DOI 10.1016/j.physa.2018.02.136
   Liu TT, 2019, SIMUL-T SOC MOD SIM, V95, P65, DOI 10.1177/0037549717753294
   Lovreglio R, 2018, ADV ENG INFORM, V38, P670, DOI 10.1016/j.aei.2018.08.018
   Lu XZ, 2020, ADV ENG SOFTW, V143, DOI 10.1016/j.advengsoft.2020.102792
   Lu XZ, 2019, SAFETY SCI, V114, P61, DOI 10.1016/j.ssci.2018.12.028
   Macklin M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3338695
   Ministry of Housing and Urban-Rural Development, 2016, 500112010 MIN HOUS U
   Miranda E, 2012, EARTHQ SPECTRA, V28, pS453, DOI 10.1193/1.4000032
   Pan ZR, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197565
   Ramirez M, 2009, FAM COMMUNITY HEALTH, V32, P105, DOI 10.1097/FCH.0b013e3181994662
   Ronchi E, 2019, FIRE SAFETY J, V106, P197, DOI 10.1016/j.firesaf.2019.05.002
   Seyhan E, 2014, EARTHQ SPECTRA, V30, P1007, DOI 10.1193/062913EQS180M
   Sticco IM, 2021, PHYSICA A, V561, DOI 10.1016/j.physa.2020.125299
   Sun Q, 2020, ADV ENG INFORM, V44, DOI 10.1016/j.aei.2020.101093
   Tang BH, 2017, INT J DISAST RISK RE, V21, P159, DOI 10.1016/j.ijdrr.2016.12.003
   Tian XY, 2020, CHAOS SOLITON FRACT, V139, DOI 10.1016/j.chaos.2020.110099
   Tian ZN, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106451
   WANG J, 2021, NONLINEAR DYN
   Wang K, 2021, PROCESS SAF ENVIRON, V147, P146, DOI 10.1016/j.psep.2020.09.033
   Xu Z, 2020, ADV ENG INFORM, V43, DOI 10.1016/j.aei.2019.101025
   Xu Z, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9173465
   Xu Z, 2018, AUTOMAT CONSTR, V90, P9, DOI 10.1016/j.autcon.2018.02.015
   Zang Y, 2021, SIMUL MODEL PRACT TH, V112, DOI 10.1016/j.simpat.2021.102354
   Zeng X, 2016, NAT HAZARDS, V83, P177, DOI 10.1007/s11069-016-2307-z
   Zhang FR, 2021, ADV ENG INFORM, V49, DOI 10.1016/j.aei.2021.101351
   Zhou JX, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0197964
NR 40
TC 0
Z9 0
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3513
EP 3523
DI 10.1007/s00371-022-02558-3
EA JUL 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705400002
DA 2024-07-18
ER

PT J
AU Guo, C
   Lin, YJ
   Xu, MY
   Shao, MW
   Yao, JF
AF Guo, Chen
   Lin, Yaojin
   Xu, Meiyan
   Shao, Mingwen
   Yao, Junfeng
TI Inverse transformation sampling-based attentive cutout for fine-grained
   visual recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Fine-grained visual categorization; Image synthesis; Attention-based
   sampling; Data augmentation
AB Recent works on fine-grained visual categorization rely on detecting discriminative regions that correspond to specific visual patterns. Promising progress has been obtained by constructing complicated network architecture, which either involves explicitly or implicitly capturing subtle differences to learn part-level representations. Instead of sophisticated model designs, we consider the learning paradigm of data augmentation to utilize subtle cues through a single vanilla neural network (e.g., ResNet). However, the powerful regional dropout strategy, Cutout, which randomly overlays a square patch of inputs in training, may produce inefficient images for fine-grained classification. This is because constant and causal block extent is potentially inflexible for the variety of object position and size. To generate more reasonable samples, we propose an enhanced image synthesis strategy called Attentive Cutout, which purposefully conceals informative details by performing attention-guided content sampling on the high responses from channels. As the feature channels generally represent the multitude of different clues for specified categories, our method is capable of selecting distinct parts to occlude in every iteration. Compare with previous synthesizing training data approaches, Attentive Cutout ensures more diversity and attends to part-level features among generated images. Extensive experiments and analysis studies demonstrate the effectiveness of our approach, which is efficient but easy to implement and achieves competitive performance with structure-based methods.
C1 [Guo, Chen; Lin, Yaojin; Xu, Meiyan] Minnan Normal Univ, Sch Comp Sci, Zhangzhou 363000, Peoples R China.
   [Guo, Chen; Lin, Yaojin; Xu, Meiyan] Minnan Normal Univ, Lab Data Sci & Intelligence Applicat, Zhangzhou 363000, Peoples R China.
   [Shao, Mingwen] Chinese Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Peoples R China.
   [Yao, Junfeng] Xiamen Univ, Xiamen, Peoples R China.
C3 MinNan Normal University; MinNan Normal University; China University of
   Petroleum; Xiamen University
RP Lin, YJ (corresponding author), Minnan Normal Univ, Sch Comp Sci, Zhangzhou 363000, Peoples R China.; Lin, YJ (corresponding author), Minnan Normal Univ, Lab Data Sci & Intelligence Applicat, Zhangzhou 363000, Peoples R China.; Yao, JF (corresponding author), Xiamen Univ, Xiamen, Peoples R China.
EM zzlinyaojin@163.com; yao0010@xmu.edu.cn
RI Yao, Junfeng/ABE-6440-2020; guo, chen/KTR-9590-2024
FU National Natural Science Foundation of China [62076116, 61672272];
   Natural Science Foundation of Fujian Province [2020J01811, 2020J01792]
FX We are very grateful to the anonymous reviewers for their valuable
   comments and suggestions. This work is supported by Grants from the
   National Natural Science Foundation of China (Nos. 62076116, and
   61672272), the Natural Science Foundation of Fujian Province (Nos.
   2020J01811 and 2020J01792).
CR Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   DeVries Terrance, 2017, Dataset Augmentation in Feature Space
   Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu C, 2021, APPL INTELL, V51, P7903, DOI 10.1007/s10489-021-02280-y
   Maji S., 2013, Technical report
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Niu Y, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107947
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sedik A, 2022, NEURAL COMPUT APPL, V34, P11423, DOI 10.1007/s00521-020-05410-8
   Sedik A, 2020, VIRUSES-BASEL, V12, DOI 10.3390/v12070769
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Song KT, 2020, IEEE T IMAGE PROCESS, V29, P7006, DOI 10.1109/TIP.2020.2996736
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Tokozume Y, 2018, PROC CVPR IEEE, P5486, DOI 10.1109/CVPR.2018.00575
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Verma V, 2019, PR MACH LEARN RES, V97
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Walawalkar D., 2020, ARXIV200313048
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu XH, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108067
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 53
TC 3
Z9 3
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2597
EP 2608
DI 10.1007/s00371-022-02481-7
EA JUN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000805052800001
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Liu, JW
AF Liu, Ze-yu
   Liu, Jian-wei
TI Hypergraph attentional convolutional neural network for salient object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Cross-level feature mining; Hypergraph
   encoder; Feature propagation
ID FRAMEWORK
AB Learning discriminative features and mining salient visual patterns play an important role in salient object detection (SOD) task. Existing SOD methods suffer from limited receptive field and insufficient cross-level feature mining. To this end, we propose a hypergraph attentional convolutional neural network for SOD task. Specifically, our method consists of (1) an attention based feature fusion module, which efficiently fuses lower layer as well as higher layer features, (2) a hypergraph-based long-range dependency encoder, which enhances the receptive field and global context for detection model, (3) a feature refinement layer, which highlights discriminative features and fuses attentional inputs, and (4) a dual iterative feature propagation decoder, which propagates features and upscale lower level feature maps to higher resolution. Both qualitative and quantitively experiments on public datasets verify the effectiveness of our proposed method. Compared with previous works, our model plays favorably against the state-of-the-arts methods.
C1 [Liu, Ze-yu; Liu, Jian-wei] China Univ Petr, Coll Informat Sci & Engn, Dept Automat, Beijing, Peoples R China.
C3 China University of Petroleum
RP Liu, JW (corresponding author), China Univ Petr, Coll Informat Sci & Engn, Dept Automat, Beijing, Peoples R China.
EM 2236677012@qq.com
RI Liu, Jian-Wei/C-5013-2008
CR Achanta R., 2009, 2009 IEEE C COMPUTER
   [Anonymous], 2005, CEUR WORKSHOP P
   Bai S, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107637
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cai Q, 2019, PATTERN RECOGN, V93, P147, DOI 10.1016/j.patcog.2019.04.019
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng Z., 2018, P 27 INT JOINT C ART
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Eun-Sol Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14569, DOI 10.1109/CVPR42600.2020.01459
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan D.-P., 2018, P 27 INT JOINT C ART
   Fan D.P., 2017, IEEE INT C COMPUTER
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Feng M., 2019, 2019 IEEECVF C COMPU
   Feng Y., 2019, P AAAI C ARTIFICIAL, V33
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Gavrila D. M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P87, DOI 10.1109/ICCV.1999.791202
   Gupta AK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3064423
   Gupta AK, 2021, PATTERN ANAL APPL, V24, P625, DOI 10.1007/s10044-020-00925-1
   Gupta AK, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22101174
   Harada S, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-3378-0
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hou Q., 2017, P IEEE C COMPUTER VI
   Huo LN, 2016, PATTERN RECOGN, V49, P162, DOI 10.1016/j.patcog.2015.07.005
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Jiang J., 2019, P 28 INT JOINT C ART
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kipf TN, 2017, INT C LEARN REPR
   Li G., 2015, 2015 IEEE C COMPUTER
   Li G., ARXIV PREPRINT ARXIV
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li Guohao, 2019, P IEEE CVF INT C COM
   Li L., 2021, VISUAL COMPUT, P1432
   Li S., 2017, P IEEE C COMPUTER VI
   Li X., 2013, 2013 IEEE INT C COMP
   Li XL, 2020, IEEE T IMAGE PROCESS, V29, P9165, DOI 10.1109/TIP.2020.3023774
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li ZY, 2016, PATTERN RECOGN, V52, P317, DOI 10.1016/j.patcog.2015.10.009
   Liang J, 2018, PATTERN RECOGN, V76, P476, DOI 10.1016/j.patcog.2017.11.024
   Liang Z, 2012, PATTERN RECOGN, V45, P3886, DOI 10.1016/j.patcog.2012.04.017
   Liu J.-J., 2019, P IEEECVF C COMPUTER
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Liu Y., 2019, P IEEECVF INT C COMP
   Liu Y., IEEE T TYCBETICS
   Lu H., 2019, 2019 IEEECVF C COMPU
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Luo A., 2020, 16 EUR C COMP VIS EC, P2020
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Margolin R., 2014, 2014 IEEE C COMPUTER
   Mohammadi S, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107303
   Park J.W, ARXIV PREPRINT ARXIV
   Perazzi F., 2012, 2012 IEEE C COMPUTER
   Powers, ABS201016061 CORR
   Qian Q, 2021, VISUAL COMPUT, V37, P1029, DOI 10.1007/s00371-020-01850-4
   Qin X., 2019, P IEEECVF C COMPUTER
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanchez-Gonzalez A, 2018, PR MACH LEARN RES, V80
   Shen Y., 2018, P EUROPEAN C COMPUTE
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tu ZZ, 2021, IEEE T CIRC SYST VID, V31, P582, DOI 10.1109/TCSVT.2020.2980853
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang T., 2017, 2017 IEEE INT C COMP
   Wang T., 2018, 2018 IEEECVF C COMPU
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wei J, ARXIV PREPRINT ARXIV
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu Z., 2019, P IEEECVF INT C COMP
   Wu Z., 2019, 2019 IEEECVF C COMPU
   Yan Q., 2013, P IEEE C COMPUTER VI
   Yan Y., 2020, P IEEECVF C COMPUTER
   Yang C., 2013, 2013 IEEE C COMPUTER
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zeng Y., 2019, 2019 IEEECVF C COMPU
   Zhang JX, 2017, LECT NOTES COMPUT SC, V10559, P368, DOI 10.1007/978-3-319-67777-4_32
   Zhang L., 2018, 2018 IEEECVF C COMPU
   Zhang P., 2017, P IEEE INT C COMPUTE
   Zhang P., 2017, 2017 IEEE INT C COMP
   Zhang QD, 2021, IEEE T IMAGE PROCESS, V30, P7578, DOI 10.1109/TIP.2021.3108018
   Zhang X., 2018, 2018 IEEECVF C COMPU
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhi XH, 2018, PATTERN RECOGN, V80, P241, DOI 10.1016/j.patcog.2018.03.010
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
   Zhuge M., ARXIV PREPRINT ARXIV
NR 89
TC 5
Z9 5
U1 9
U2 40
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2881
EP 2907
DI 10.1007/s00371-022-02499-x
EA MAY 2022
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000793005600001
DA 2024-07-18
ER

PT J
AU Anzid, H
   le Goic, G
   Bekkari, A
   Mansouri, A
   Mammass, D
AF Anzid, Hanan
   le Goic, Gaetan
   Bekkari, Aissam
   Mansouri, Alamin
   Mammass, Driss
TI A new SURF-based algorithm for robust registration of multimodal images
   data
SO VISUAL COMPUTER
LA English
DT Article
DE Image registration; Feature matching; Feature description; Multimodal
   imaging; SURF; Cultural Heritage
ID MAXIMIZATION; DESCRIPTOR; TENSOR
AB In this paper, we propose an original algorithm allowing the registration of multimodal images. Indeed, the large and nonlinear intensity changes between different modalities often lead to mismatching of the interest points, which can alter the quality of the registration. Our method builds on the SURF for the identification and the description of the interest points. In order to boost the confidence on the subsequent matching operation of the registration, an applicable classification on both the distance and orientation SURF features is presented despite the low structural similarity of the multimodal acquisition. The proposed algorithm is tested and validated on a large dataset extracted from acquisitions on cultural heritage wall paintings that implement 4 imaging modalities covering UV, IR, visible and fluorescence. The comparison with conventional SIFT/SURF approaches, PIIFD, SURF-PIIFD-PRM,CFOG and HTCPR for registration highlights the effectiveness of the method and its robustness to outliers in SIFT/SURF descriptors.
C1 [Anzid, Hanan; Bekkari, Aissam; Mammass, Driss] Ibn Zohr Univ, Fac Sci, IRF SIC Lab, Agadir, Morocco.
   [le Goic, Gaetan; Mansouri, Alamin] Univ Bourgogne Franche Comt, ImVia Lab Imagery & Artificial Vis, Besanon, France.
C3 Ibn Zohr University of Agadir
RP Anzid, H (corresponding author), Ibn Zohr Univ, Fac Sci, IRF SIC Lab, Agadir, Morocco.
EM hanananzid@gmail.com
RI Goïc, Gaëtan Le/L-8484-2013
FU COST Action [TD1201]; PHC Toubkal program [16/31: 34676YA]
FX The authors thank the Chateau de Germolles managers for providing data
   and expertise and the COST Action TD1201 "Colour and Space in Cultural
   Heritage (COSCH)" (www.cosch.info) for supporting this case study. The
   authors also thank the PHC Toubkal/16/31: 34676YA program for the
   financial support
CR Ahmad S, 2018, VISUAL COMPUT, V34, P21, DOI 10.1007/s00371-016-1307-z
   Alred G.J., 2003, HDB TECHNICAL WRITIN
   Anzid H., 2016, 2016 IEEE ACS 13 INT, V13, P1
   Anzid H., 2018, INT J COMPUT TECHNOL, V18, P7418
   Barrera F, 2013, PATTERN RECOGN LETT, V34, P52, DOI 10.1016/j.patrec.2012.08.009
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Beckouche S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P180, DOI 10.1109/ICCVW.2011.6130241
   Brock KK, 2017, MED PHYS, V44, pE43, DOI 10.1002/mp.12256
   Chen BL, 2011, VISUAL COMPUT, V27, P121, DOI 10.1007/s00371-010-0535-x
   Chen. C.H., 2007, SIGNAL IMAGE PROCESS, P515
   Chen JA, 2010, IEEE T BIO-MED ENG, V57, P1707, DOI 10.1109/TBME.2010.2042169
   Chen J, 2009, PROG NAT SCI-MATER, V19, P643, DOI 10.1016/j.pnsc.2008.06.029
   Chen S., 2019, ALGORITHMS TECHNOL A, V10986, P109
   Cosentino A, 2014, HERIT SCI, V2, DOI 10.1186/2050-7445-2-8
   Cui M, 2007, VISUAL COMPUT, V23, P607, DOI 10.1007/s00371-007-0164-1
   Das A., 2018, INT J PURE APPL MATH, V118, P1521
   DAVIES ER, 1988, IEE PROC-E, V135, P49, DOI 10.1049/ip-e.1988.0006
   De Silva T., 2020, MED IMAGING, V11313, P113
   Firmenich D, 2011, IEEE IMAGE PROC, P181, DOI 10.1109/ICIP.2011.6115818
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fridman L., 2019, THESIS
   Ghassabi Z, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-25
   Gourrame K, 2019, MULTIMED TOOLS APPL, V78, P2621, DOI 10.1007/s11042-018-6302-0
   GULL SF, 1989, FUND THEOR, V36, P53
   HANSON KM, 1993, P SOC PHOTO-OPT INS, V1898, P716, DOI 10.1117/12.154577
   Harris C., 1988, ALVEY VISION C, P147151
   Hossain M. T., 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P197, DOI 10.1109/DICTA.2011.40
   Hu YP, 2018, MED IMAGE ANAL, V49, P1, DOI 10.1016/j.media.2018.07.002
   Istenic R., 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P106, DOI 10.1109/IWSSIP.2007.4381164
   Jiangtao Feng, 2019, 2019 International Conference on Communications, Information System and Computer Engineering (CISCE). Proceedings, P616, DOI 10.1109/CISCE.2019.00144
   Konstantinos K., 2014, CVPR, P329
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Liu YJ, 2002, VISUAL COMPUT, V18, P368, DOI 10.1007/s003710100124
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   Lv GH, 2018, MULTIMED TOOLS APPL, V77, P12607, DOI 10.1007/s11042-017-4907-3
   Ma WP, 2017, IEEE GEOSCI REMOTE S, V14, P3, DOI 10.1109/LGRS.2016.2600858
   Maes F, 1997, IEEE T MED IMAGING, V16, P187, DOI 10.1109/42.563664
   Misra I., 2012, Proceedings of the 2012 1st International Conference on Recent Advances in Information Technology (RAIT 2012), P68, DOI 10.1109/RAIT.2012.6194482
   Mistry S., 2016, Int. Res. J. Eng. Technol. (IRJET), V3, P2220
   Ouerghi H, 2022, VISUAL COMPUT, V38, P1427, DOI 10.1007/s00371-021-02077-7
   Patel MI, 2016, PROCEDIA COMPUT SCI, V93, P382, DOI 10.1016/j.procs.2016.07.224
   Paul S, 2018, ADV INTELL SYST, V518, P123, DOI 10.1007/978-981-10-3373-5_11
   Pluim JPW, 2000, IEEE T MED IMAGING, V19, P809, DOI 10.1109/42.876307
   Rengarajan V, 2017, IEEE T PATTERN ANAL, V39, P1959, DOI 10.1109/TPAMI.2016.2630687
   Rister B, 2017, IEEE T IMAGE PROCESS, V26, P4900, DOI 10.1109/TIP.2017.2722689
   Roche A, 1998, LECT NOTES COMPUT SC, V1496, P1115, DOI 10.1007/BFb0056301
   Roche A, 2000, INT J IMAG SYST TECH, V11, P71, DOI 10.1002/(SICI)1098-1098(2000)11:1<71::AID-IMA8>3.0.CO;2-5
   Rohith G, 2021, VISUAL COMPUT, V37, P1965, DOI 10.1007/s00371-020-01957-8
   Rominger C., 2011, REV NATL TECHNOLOGIE, V21, P231
   Saleem S, 2014, IEEE SIGNAL PROC LET, V21, P400, DOI 10.1109/LSP.2014.2304073
   Shen XY, 2014, LECT NOTES COMPUT SC, V8692, P309, DOI 10.1007/978-3-319-10593-2_21
   Sheng H, 2018, J MED IMAG HEALTH IN, V8, P583, DOI 10.1166/jmihi.2018.2322
   Teng SW, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.013013
   Ulysses J.N., 2010, MEASURING SIMILARITY, V17, P1
   Wang G, 2015, BIOMED SIGNAL PROCES, V19, P68, DOI 10.1016/j.bspc.2015.03.004
   Wang GK, 2016, INT GEOSCI REMOTE SE, P2803, DOI 10.1109/IGARSS.2016.7729724
   Wang VT, 2017, IEEE J OCEANIC ENG, V42, P901, DOI 10.1109/JOE.2016.2634078
   Wang X., 2017, LECT NOTES COMPUT SC, V1, P128
   Xia RB, 2013, PROC SPIE, V8919, DOI 10.1117/12.2031615
   Yang Zhuoqian, 2017, 2017 13th IEEE International Conference on Electronic Measurement & Instruments (ICEMI), P554, DOI 10.1109/ICEMI.2017.8265890
   Ye YX, 2019, IEEE T GEOSCI REMOTE, V57, P9059, DOI 10.1109/TGRS.2019.2924684
   Ye YX, 2017, IEEE T GEOSCI REMOTE, V55, P2941, DOI 10.1109/TGRS.2017.2656380
   Zhang JY, 2015, INT J COMPUT ASS RAD, V10, P1765, DOI 10.1007/s11548-015-1219-9
   Zhao D, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/5598177
   Zhao D, 2014, NEUROCOMPUTING, V131, P87, DOI 10.1016/j.neucom.2013.10.037
   Zheng Q., 2021, COMPUT MATH METHODS, V2021, P1
NR 68
TC 7
Z9 8
U1 6
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1667
EP 1681
DI 10.1007/s00371-022-02435-z
EA MAR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000772546700001
DA 2024-07-18
ER

PT J
AU Wang, Y
   Hu, ZR
   Yao, SW
   Liu, H
AF Wang, Yu
   Hu, Ziran
   Yao, Shouwen
   Liu, Hui
TI Using visual feedback to improve hand movement accuracy in
   confined-occluded spaces in virtual reality
SO VISUAL COMPUTER
LA English
DT Article
DE Visual collision feedback; Occluded interaction; 3D occlusion
   management; Hand movement performance; See-through visualization
ID PERFORMANCE
AB Accurate and informative hand-object collision feedback is of vital importance for hand manipulation in virtual reality (VR). However, to our best knowledge, the hand movement performance in fully-occluded and confined VR spaces under visual collision feedback is still under investigation. In this paper, we firstly studied the effects of several popular visual feedback of hand-object collision on hand movement performance. To test the effects, we conducted a within-subject user study (n=18) using a target-reaching task in a confined box. Results indicated that users had the best task performance with see-through visualization, and the most accurate movement with the hybrid of proximity-based gradation and deformation. By further analysis, we concluded that the integration of see-through visualization and proximity-based visual cue could be the best compromise between the speed and accuracy for hand movement in the enclosed VR space. On the basis, we designed a visual collision feedback based on projector decal,which incorporates the advantages of see-through and color gradation. In the end, we present demos of potential usage of the proposed visual cue.
C1 [Wang, Yu; Hu, Ziran; Yao, Shouwen; Liu, Hui] Beijing Inst Technol, Beijing, Peoples R China.
C3 Beijing Institute of Technology
RP Yao, SW (corresponding author), Beijing Inst Technol, Beijing, Peoples R China.
EM armysw@bit.edu.cn
OI yao, shou wen/0000-0001-9947-6049; Wang, Yu/0000-0002-6318-0446
FU National Natural Science Foundation of China [51975051]
FX This study was supported by the National Natural Science Foundation of
   China under Grant 51975051.
CR Argelaguet F, 2016, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2016.7504682
   Ariza O, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P327, DOI 10.1109/VR.2018.8446317
   B L., 2006, ACM SIGGRAPH 2006 SK
   Bell JD, 2019, HUM MOVEMENT SCI, V67, DOI 10.1016/j.humov.2019.102515
   Biocca F, 2001, PRESENCE-VIRTUAL AUG, V10, P247, DOI 10.1162/105474601300343595
   Biocca F.A., 2002, PRESENCE, P410
   Bloomfield A, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P163
   Chapoulie Emmanuelle, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P109, DOI 10.1109/3DUI.2015.7131734
   Cortes G., 2018, P 15 ACM S APPL PERC, P1
   Dey S., 2015, INT J SCI RES, V4, P148
   Dienes Z, 1999, BEHAV BRAIN SCI, V22, P735, DOI 10.1017/S0140525X99002186
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Eren MT, 2018, VISUAL COMPUT, V34, P405, DOI 10.1007/s00371-016-1346-5
   Fan Hehe, 2021, IEEE T CIRCUITS SYST
   Fricke N., 2009, P HUM FACT ERG SOC A, VVol. 53, P1815, DOI DOI 10.1177/154193120905302315
   Fu MJ, 2012, PRESENCE-TELEOP VIRT, V21, P305, DOI 10.1162/PRES_a_00115
   González-Alvarez C, 2007, OPHTHAL PHYSL OPT, V27, P265, DOI 10.1111/j.1475-1313.2007.00476.x
   Greene E.D., 2011, THESIS U WATERLOO
   Iachini T, 2017, COGNITION, V166, P107, DOI 10.1016/j.cognition.2017.03.024
   Kawabe T, 2020, IEEE T HAPTICS, V13, P18, DOI 10.1109/TOH.2019.2961883
   Lawson G, 2016, APPL ERGON, V53, P323, DOI 10.1016/j.apergo.2015.06.024
   Li M, 2015, MED BIOL ENG COMPUT, V53, P1177, DOI 10.1007/s11517-015-1309-4
   Lindeman R.W., 2004, ACM Symposium on Virtual Reality Software and Technology, P146, DOI DOI 10.1145/1077534.1077562
   Liu L, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P219, DOI 10.1109/VR.2009.4811026
   Louison C, 2018, INT J HUM-COMPUT INT, V34, P1015, DOI 10.1080/10447318.2017.1411665
   MacKenzie I. S., 1992, Human-Computer Interaction, V7, P91, DOI 10.1207/s15327051hci0701_3
   Ng AWY, 2012, LECT NOTES ENG COMP, P1449
   Peter Z., 2008, PRODUCT ENG TOOLS ME, P277
   Prachyabrued M, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P19, DOI 10.1109/3DUI.2014.6798835
   Reddy GR, 2020, IEEE INT S MIX AUGM
   Sagardia M, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P153, DOI 10.1145/2993369.2993386
   Samad M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300550
   Sarlegna FR, 2009, ADV EXP MED BIOL, V629, P317, DOI 10.1007/978-0-387-77064-2_16
   Schwind Valentin., 2018, Proceedings of the 15th ACM Symposium on Applied Perception, SAP'18, p9:1
   Shumway-Cook A., 2017, Motor control: Translating research into clinical practice, V5th
   Sreng J, 2006, IEEE T VIS COMPUT GR, V12, P1013, DOI 10.1109/TVCG.2006.189
   UltraLeap, 2010, UN MOD LEAP MOT
   UltraLeap, 2017, HAND LEAP MOT C SDK
   UltraLeap, 2020, UN MOD LEAP MOT UN S
   Valentina G, 2013, THESIS U BRADFORD
   Vance JM, 2011, PROCEEDINGS OF THE ASME WORLD CONFERENCE ON INNOVATIVE VIRTUAL REALITY - 2011, P273
   Vosinakis S, 2018, VIRTUAL REAL-LONDON, V22, P47, DOI 10.1007/s10055-017-0313-4
   Waller D., 2013, HDB SPATIAL COGNITIO, P3, DOI [DOI 10.1007/978-1-4419-8432-6_1, DOI 10.1007/978-1-4419-8432-61]
   Weber Bernhard, 2013, Virtual Augmented and Mixed Reality. Designing and Developing Augmented and Virtual Environments. 5th International Conference, VAMR 2013 Held as Part of HCI International 2013. Proceedings: LNCS 7936, P241, DOI 10.1007/978-3-642-39405-8_28
   Xia PJ, 2012, INT J ADV MANUF TECH, V58, P379, DOI 10.1007/s00170-011-3381-8
   Zhai SM, 2004, PRESENCE-TELEOP VIRT, V13, P113, DOI 10.1162/1054746041382393
   Zhang LC, 2015, HUM MOVEMENT SCI, V40, P1, DOI 10.1016/j.humov.2014.11.009
NR 47
TC 4
Z9 4
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1485
EP 1501
DI 10.1007/s00371-022-02424-2
EA FEB 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000759383200001
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, CX
   Xu, LL
   Liu, LG
AF Wang, Chunxue
   Xu, Linlin
   Liu, Ligang
TI Structure-texture image decomposition via non-convex total generalized
   variation and convolutional sparse coding
SO VISUAL COMPUTER
LA English
DT Article
DE Structure-texture image decomposition; Non-convex total generalized
   variation regularization; Convolutional sparse coding; Alternating
   minimization scheme; Detail-preserving
ID TOTAL VARIATION MINIMIZATION; RESTORATION; ALGORITHMS; REDUCTION; MODELS
AB Image decomposition is a fundamental but challenging ill-posed problem in image processing and has been widely applied to compression, enhancement, texture removal, etc. In this paper, we introduce a novel structure-texture image decomposition model via non-convex total generalized variation regularization (NTGV) and convolutional sparse coding (CSC). NTGV aims to characterize the detailed-preserved structural component ameliorating the staircasing artifacts existing in total variation-based models, and CSC aims to characterize image fine-scale textures. Moreover, we incorporate both structure-aware and texture-aware measures to well distinguish structural and textural component. The proposed model is numerically implemented by an alternating minimization scheme based on alternating direction method of multipliers. Experimental results demonstrate the effectiveness of our approach on several applications including texture removal, high dynamic range image tone mapping, detail enhancement and non-photorealistic abstraction.
C1 [Wang, Chunxue] Dunhuang Acad, Jiuquan, Gansu, Peoples R China.
   [Xu, Linlin] Inner Mongolia Univ Finance & Econ, Sch Comp Informat Management, Hohhot, Inner Mongolia, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Inner Mongolia University of Finance & Economics; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Wang, CX (corresponding author), Dunhuang Acad, Jiuquan, Gansu, Peoples R China.
EM chunxuewang2019@163.com
RI XU, Lin/JDM-5554-2023; xu, Linlin/JCF-2403-2023
OI XU, Lin/0000-0003-1781-1638; Wang, Chunxue/0000-0002-6828-3487
FU Youth Science and Technology Foundation of Gansu [20JR5RA050]
FX This study was funded by the Youth Science and Technology Foundation of
   Gansu (20JR5RA050).
CR Aach T, 2006, IEEE T IMAGE PROCESS, V15, P3690, DOI 10.1109/TIP.2006.884921
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Buades A, 2008, INT J COMPUT VISION, V76, P123, DOI 10.1007/s11263-007-0052-1
   Chalasani R., 2013, The 2013 International Joint Conference on Neural Networks (IJCNN), P1, DOI [10.1109/IJCNN.2013.6706854, DOI 10.1109/IJCNN.2013.6706854]
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Dowson N, 2011, IEEE T PATTERN ANAL, V33, P485, DOI 10.1109/TPAMI.2010.114
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Fan YR, 2017, J FRANKLIN I, V354, P3170, DOI 10.1016/j.jfranklin.2017.01.037
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Gu YA, 2019, J MATH IMAGING VIS, V61, P1329, DOI 10.1007/s10851-019-00909-9
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jahne B., 1993, Spatio-temporal image processing: theory and scientific applications
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Jiang LL, 2009, J SYST ENG ELECTRON, V20, P254
   Kang M, 2017, J SCI COMPUT, V72, P172, DOI 10.1007/s10915-017-0357-3
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   Kavukcuoglu K., 2010, Advances in neural information processing systems, P1
   Kim Y, 2019, IEEE T IMAGE PROCESS, V28, P2692, DOI 10.1109/TIP.2018.2889531
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Le TM, 2005, MULTISCALE MODEL SIM, V4, P390, DOI 10.1137/040610052
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Lianqing Shu, 2020, ICMSSP 2020: Proceedings of the 2020 5th International Conference on Multimedia Systems and Signal Processing, P26, DOI 10.1145/3404716.3404727
   Lieu LH, 2008, APPL MATH OPT, V58, P167, DOI 10.1007/s00245-008-9047-8
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Liu XW, 2018, IEEE SIGNAL PROC LET, V25, P1221, DOI 10.1109/LSP.2018.2850218
   Liu Z, 2022, IEEE T VIS COMPUT GR, V28, P4418, DOI 10.1109/TVCG.2021.3088118
   Lu CW, 2016, IET IMAGE PROCESS, V10, P495, DOI 10.1049/iet-ipr.2015.0573
   Lu CW, 2011, J SYST ENG ELECTRON, V22, P358, DOI 10.3969/j.issn.1004-4132.2011.02.026
   Lu KY, 2018, LECT NOTES COMPUT SC, V11208, P229, DOI 10.1007/978-3-030-01225-0_14
   Lu ZW, 2018, IEEE SIGNAL PROC LET, V25, P1585, DOI 10.1109/LSP.2018.2867896
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Meyer Y., 2001, Memoirs of the American Mathematical Society
   Ochs P, 2015, SIAM J IMAGING SCI, V8, P331, DOI 10.1137/140971518
   Ochs P, 2013, PROC CVPR IEEE, P1759, DOI 10.1109/CVPR.2013.230
   Ono S, 2017, IEEE T IMAGE PROCESS, V26, P1554, DOI 10.1109/TIP.2017.2651392
   Osher S, 2003, MULTISCALE MODEL SIM, V1, P349, DOI 10.1137/S1540345902416247
   Paris S, 2015, COMMUN ACM, V58, P81, DOI 10.1145/2723694
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Qingnan Fan, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275081
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Starck JL, 2005, IEEE T IMAGE PROCESS, V14, P1570, DOI 10.1109/TIP.2005.852206
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Valkonen T, 2013, SIAM J IMAGING SCI, V6, P487, DOI 10.1137/120867172
   Vese L., 1997, REDUCED NONCONVEX FU
   Vese LA, 2003, J SCI COMPUT, V19, P553, DOI 10.1023/A:1025384832106
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wohlberg B, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6854992
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu JL, 2014, J SYST ENG ELECTRON, V25, P168, DOI 10.1109/JSEE.2014.00020
   Xu JL, 2014, SIGNAL IMAGE VIDEO P, V8, P39, DOI 10.1007/s11760-012-0420-3
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu PP, 2019, IEEE T IMAGE PROCESS, V28, P4354, DOI 10.1109/TIP.2019.2904847
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yin H, 2021, MULTIDIM SYST SIGN P, V32, P313, DOI 10.1007/s11045-020-00742-z
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Yin WT, 2005, LECT NOTES COMPUT SC, V3752, P73
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang H., 2016, P BRIT MACH VIS C BM
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhou F, 2020, IEEE T IMAGE PROCESS, V29, P3458, DOI 10.1109/TIP.2019.2961232
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 77
TC 4
Z9 4
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1121
EP 1136
DI 10.1007/s00371-021-02392-z
EA FEB 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000752196300003
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Zhang, H
AF Zhang, Haowan
   Zhang, Hong
TI LungSeek: 3D Selective Kernel residual network for pulmonary nodule
   diagnosis
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Medically assisted diagnosis; Pulmonary nodules; Residual
   network; 3D Selective Kernel network
ID IMAGE DATABASE CONSORTIUM; FALSE-POSITIVE REDUCTION; NEURAL-NETWORK; CT
   IMAGES; CANCER
AB Early detection and diagnosis of pulmonary nodules is the most promising way to improve the survival chances of lung cancer patients. This paper proposes an automatic pulmonary cancer diagnosis system, LungSeek. LungSeek is mainly divided into two modules: (1) Nodule detection, which detects all suspicious nodules from computed tomography (CT) scan; (2) Nodule Classification, classifies nodules as benign or malignant. Specifically, a 3D Selective Kernel residual network (SK-ResNet) based on the Selective Kernel Network and 3D residual network is located. A deep 3D region proposal network with SK-ResNet is designed for detection of pulmonary nodules while a multi-scale feature fusion network is designed for the nodule classification. Both networks use the SK-Net module to obtain different receptive field information, thereby effectively learning nodule features and improving diagnostic performance. Our method has been verified on the luna16 data set, reaching 89.06, 94.53% and 97.72% when the average number of false positives is 1, 2 and 4, respectively. Meanwhile, its performance is better than the state-of-the-art method and other similar networks and experienced doctors. This method has the ability to adaptively adjust the receptive field according to multiple scales of the input information, so as to better detect nodules of various sizes. The framework of LungSeek based on 3D SK-ResNet is proposed for nodule detection and nodule classification from chest CT. Our experimental results demonstrate the effectiveness of the proposed method in the diagnosis of pulmonary nodules.
C1 [Zhang, Haowan; Zhang, Hong] Wuhan Univ Sci & Technol, Coll Comp Sci & Technol, Wuhan 430081, Peoples R China.
   [Zhang, Haowan; Zhang, Hong] Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan, Peoples R China.
C3 Wuhan University of Science & Technology
RP Zhang, H (corresponding author), Wuhan Univ Sci & Technol, Coll Comp Sci & Technol, Wuhan 430081, Peoples R China.; Zhang, H (corresponding author), Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan, Peoples R China.
EM zhanghong_wust@163.com
CR Adiyoso Setio A. A., 2017, VALIDATION COMP COMB
   Alghamdi A, 2024, MULTIMED TOOLS APPL, V83, P14913, DOI 10.1007/s11042-020-08769-x
   Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204
   Betz, 1999, ARCHITECTURE CAD DEE, DOI [10.1007/978-1-4615-5145-4, DOI 10.1007/978-1-4615-5145-4]
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chen Yunpeng., 2017, NIPS
   Dey R, 2018, I S BIOMED IMAGING, P774, DOI 10.1109/ISBI.2018.8363687
   Dou Q, 2017, IEEE T BIO-MED ENG, V64, P1558, DOI 10.1109/TBME.2016.2613502
   Ferlay J, 2015, INT J CANCER, V136, pE359, DOI 10.1002/ijc.29210
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hammad M, 2022, MULTIMEDIA SYST, V28, P1373, DOI 10.1007/s00530-020-00728-8
   Hammad M, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3033072
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Lei B., 2018, VISUAL COMPUT, V34
   Li X, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232127
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YM, 2020, I S BIOMED IMAGING, P1866, DOI [10.1109/ISBI45749.2020.9098317, 10.1109/isbi45749.2020.9098317]
   Li Z., 2019, MVP NET MULTIVIEW FP, DOI [10.1007/978-3-030-32226-7_2, DOI 10.1007/978-3-030-32226-7_2]
   Liao FZ, 2019, IEEE T NEUR NET LEAR, V30, P3484, DOI 10.1109/TNNLS.2019.2892409
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Messay T, 2015, MED IMAGE ANAL, V22, P48, DOI 10.1016/j.media.2015.02.002
   Patroumpas K, 2013, J SPAT INT SCI, P45, DOI 10.5311/JOSIS.2013.7.132
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sedik A, 2020, VIRUSES-BASEL, V12, DOI 10.3390/v12070769
   Setio AAA, 2017, MED IMAGE ANAL, V42, P1, DOI 10.1016/j.media.2017.06.015
   Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809
   Shen SW, 2019, EXPERT SYST APPL, V128, P84, DOI 10.1016/j.eswa.2019.01.048
   Shen W, 2017, PATTERN RECOGN, V61, P663, DOI 10.1016/j.patcog.2016.05.029
   Siegel RL, 2021, CA-CANCER J CLIN, V71, P7, DOI [10.3322/caac.21387, 10.3322/caac.20073, 10.3322/caac.21332, 10.3322/caac.21601, 10.3322/caac.21254, 10.3322/caac.21654, 10.3322/caac.20006, 10.3322/caac.21551]
   Torre LA, 2016, ADV EXP MED BIOL, V893, P1, DOI 10.1007/978-3-319-24223-1_1
   Wood DE, 2018, J NATL COMPR CANC NE, V16, P412, DOI 10.6004/jnccn.2018.0020
   Yan X, 2017, LECT NOTES COMPUT SC, V10118, P91, DOI 10.1007/978-3-319-54526-4_7
   Zhang GB, 2018, COMPUT BIOL MED, V103, P287, DOI 10.1016/j.compbiomed.2018.10.033
   Zhu W., 2017, DEEPLUNG 3D DEEP CON, DOI [10.1101/189928, DOI 10.1101/189928]
NR 39
TC 9
Z9 9
U1 1
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 679
EP 692
DI 10.1007/s00371-021-02366-1
EA JAN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000749067200001
PM 35103029
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Haris, M
   Hou, J
   Wang, XM
AF Haris, Malik
   Hou, Jin
   Wang, Xiaomin
TI Lane line detection and departure estimation in a complex environment by
   using an asymmetric kernel convolution algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Lane line detection; Lane departure estimation; Asymmetric kernel CNN
   (AK-CNN); Scale perception; CULane dataset
ID TRACKING; TIME
AB Deep learning has made tremendous advances in the domains of image segmentation and object classification. However, real-time lane line detection and departure estimates in complex traffic conditions have proven to be hard in autonomous driving research. Traditional lane line detection methods require manual parameter modification, but they have some limitations that are still susceptible to interference from obscuring objects, lighting changes, and pavement deterioration. The development of accurate lane line detection and departure estimate algorithms is still a challenge. This article investigated a convolutional neural network (CNN) for lane line detection and departure estimate in a complicated road environment. CNN includes a weight-sharing function that lowers the training parameters. CNN can learn and extract features frequently in image segmentation, object detection, classification, and other applications. The symmetric kernel convolution of classical CNN is upgraded to the structure of asymmetric kernel convolution (AK-CNN) based on lane line detection and departure estimation features. It reduces the CNN network's computational load and improves the speed of lane line detection and departure estimates. The experiment was carried out on the CULane dataset. The lane line detection results have high accuracy in a complex environment by 80.3%. The detection speed is 84.5 fps, which enables real-time lane line detection.
C1 [Haris, Malik; Hou, Jin; Wang, Xiaomin] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 611756, Sichuan, Peoples R China.
   [Haris, Malik; Hou, Jin; Wang, Xiaomin] Southwest Jiaotong Univ, Natl Engn Lab Integrated Transportat Big Data App, Chengdu 611756, Sichuan, Peoples R China.
C3 Southwest Jiaotong University; Southwest Jiaotong University
RP Hou, J (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 611756, Sichuan, Peoples R China.; Hou, J (corresponding author), Southwest Jiaotong Univ, Natl Engn Lab Integrated Transportat Big Data App, Chengdu 611756, Sichuan, Peoples R China.
EM malikharis@my.swjtu.edu.cn; jhou@swjtu.edu.cn; xmwang@swjtu.edu.cn
RI Haris, Malik/AAM-2476-2021
OI Haris, Malik/0000-0002-6450-1715; Wang, Xiaomin/0000-0002-1761-7459;
   Wang, Xiaomin/0000-0003-4934-4288
FU Key program for Sichuan Science and Technology, China [2019YFH0097,
   2020YFG0353]
FX This work was funded by the Key program for Sichuan Science and
   Technology, China (grant number 2019YFH0097 and 2020YFG0353).
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   Bailo O, 2017, IEEE WINT CONF APPL, P760, DOI 10.1109/WACV.2017.90
   Barsan I.A., 2020, ARXIV201110902
   Bingke Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P719, DOI 10.1007/978-3-030-58523-5_42
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2013, ADV MATER RES-SWITZ, V781-784, P2383, DOI 10.4028/www.scientific.net/AMR.781-784.2383
   Chen ZB, 2020, IEEE T IMAGE PROCESS, V29, P5431, DOI 10.1109/TIP.2020.2982832
   Chetlur S., 2014, ARXIV14100759
   Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
   Cui GT, 2014, IET IMAGE PROCESS, V8, P269, DOI 10.1049/iet-ipr.2013.0371
   Ding L., 2021, GEOM VIS, V1386, P175, DOI [10.1007/978-3-030-72073-5_14, DOI 10.1007/978-3-030-72073-5_14]
   Gao Q, 2017, PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P1230, DOI 10.1109/ITNEC.2017.8284972
   Garnett N, 2019, IEEE I CONF COMP VIS, P2921, DOI 10.1109/ICCV.2019.00301
   Gopalan R, 2012, IEEE T INTELL TRANSP, V13, P1088, DOI 10.1109/TITS.2012.2184756
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Guo JY, 2020, IEEE T INTELL TRANSP, V21, P3135, DOI 10.1109/TITS.2019.2926042
   [郭斯羽 Guo Siyu], 2012, [计算机科学, Computer Science], V39, P196
   Guotian FAN., 2021, ZTE COMMUN, V18, P69
   Gurghian A, 2016, IEEE COMPUT SOC CONF, P38, DOI 10.1109/CVPRW.2016.12
   Hang Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P689, DOI 10.1007/978-3-030-58555-6_41
   Haris M, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116413
   Haris M, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10161932
   Haris M, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091102
   Haris M, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174719
   He B, 2016, IEEE INT VEH SYM, P1041, DOI 10.1109/IVS.2016.7535517
   He Z, 2020, VISUAL COMPUT, V36, P1157, DOI 10.1007/s00371-019-01724-4
   Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110
   Jeppsson H, 2018, ACCIDENT ANAL PREV, V111, P311, DOI 10.1016/j.aap.2017.12.001
   Jia BZ, 2015, VISUAL COMPUT, V31, P281, DOI 10.1007/s00371-014-0918-5
   Jiajun Zhu, 2021, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P2026, DOI 10.1109/IMCEC51613.2021.9482067
   Kim J, 2017, NEURAL NETWORKS, V87, P109, DOI 10.1016/j.neunet.2016.12.002
   Kim J, 2014, LECT NOTES COMPUT SC, V8834, P454, DOI 10.1007/978-3-319-12637-1_57
   Ko Y, 2022, IEEE T INTELL TRANSP, V23, P8949, DOI 10.1109/TITS.2021.3088488
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Lee H, 2017, IEEE INT VEH SYM, P1434, DOI 10.1109/IVS.2017.7995911
   Li HT, 2022, VISUAL COMPUT, V38, P1759, DOI 10.1007/s00371-021-02103-8
   Li J, 2017, IEEE T NEUR NET LEAR, V28, P690, DOI 10.1109/TNNLS.2016.2522428
   Li X., 2021, IEEE GEOSCI REMOTE S
   Li Y., 2016, SCI TECHNOL ENG, V16, P1671
   Liang D., 2018, LINENET ZOOMABLE CNN
   Liu L., 2021, ARXIV210505003
   Liu Y.-B., 2020, ARXIV200715602
   Mammar S, 2006, IEEE T INTELL TRANSP, V7, P226, DOI 10.1109/TITS.2006.874707
   McCall JC, 2006, IEEE T INTELL TRANSP, V7, P20, DOI 10.1109/TITS.2006.869595
   NCSA, 2018, NCSA DAT RES WEBS FA, P20
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Qin Z., 2020, COMPUTER VISION ECCV, V12369, P276, DOI DOI 10.1007/978-3-030-58586-017
   Qu Z, 2021, PROC CVPR IEEE, P14117, DOI 10.1109/CVPR46437.2021.01390
   Shuai Liu, 2020, 2020 IEEE International Conference on Mechatronics and Automation (ICMA), P1101, DOI 10.1109/ICMA49215.2020.9233837
   SINGH K, 2019, INT J MACHINE LEARNI, P1, DOI DOI 10.31223/OSF.IO/62GFR
   Srivastava Sukriti, 2015, Journal of Automation and Control Engineering, V3, P258, DOI 10.12720/joace.3.3.258-264
   Su JF, 2021, POSTGRAD MED, V133, P565, DOI 10.1080/00325481.2021.1912466
   Tabelini L, 2021, PROC CVPR IEEE, P294, DOI 10.1109/CVPR46437.2021.00036
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Tran N., 2018, GLOB STAT REP ROAD S, P5
   Wang X., 2017, J COMMAND CONTROL, V3, P154
   Wang Z., 2018, Lanenet: Real-time lane detection networks for autonomous driving
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yang TJ, 2022, VISUAL COMPUT, V38, P2871, DOI 10.1007/s00371-021-02161-y
   Ye YY, 2018, IET INTELL TRANSP SY, V12, P513, DOI 10.1049/iet-its.2017.0143
   Yoo S, 2020, IEEE COMPUT SOC CONF, P4335, DOI 10.1109/CVPRW50498.2020.00511
   Yu Zhaowei, 2017, Computer Engineering, V43, P43, DOI 10.3969/j.issn.1000-3428.2017.02.008
   Zhao K, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P1084, DOI 10.1109/IVS.2012.6232168
   Zheng T., 2020, ARXIV200813719
NR 66
TC 9
Z9 9
U1 6
U2 74
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 519
EP 538
DI 10.1007/s00371-021-02353-6
EA JAN 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600004
DA 2024-07-18
ER

PT J
AU Park, H
   Yu, R
   Lee, Y
   Lee, K
   Lee, J
AF Park, Hwangpil
   Yu, Ri
   Lee, Yoonsang
   Lee, Kyungho
   Lee, Jehee
TI Understanding the stability of deep control policies for biped
   locomotion
SO VISUAL COMPUTER
LA English
DT Article
DE Biped locomotion; Deep reinforcement learning; Gait analysis; Physically
   based simulation; Push-recovery stability
ID RECOVERY
AB Achieving stability and robustness is the primary goal of biped locomotion control. Recently, deep reinforcement learning (DRL) has attracted great attention as a general methodology for constructing biped control policies and demonstrated significant improvements over the previous state-of-the-art control methods. Although deep control policies are more advantageous compared with previous controller design approaches, many questions remain: Are deep control policies as robust as human walking? Does simulated walking involve strategies similar to human walking for maintaining balance? Does a particular gait pattern affect human and simulated walking similarly? What do deep policies learn to achieve improved gait stability? The goal of this study is to address these questions by evaluating the push-recovery stability of deep policies compared with those of human subjects and a previous feedback controller. Furthermore, we conducted experiments to evaluate the effectiveness of variants of DRL algorithms.
C1 [Park, Hwangpil; Yu, Ri] Seoul Natl Univ, Seoul, South Korea.
   [Lee, Jehee] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Park, Hwangpil] Samsung Elect, Suwon, South Korea.
   [Lee, Yoonsang] Hanyang Univ, Comp Sci, Seoul, South Korea.
   [Lee, Kyungho] NC Soft, Sungnam, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU);
   Samsung Electronics; Samsung; Hanyang University
RP Lee, J (corresponding author), Seoul Natl Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM jehee@mrl.snu.ac.kr
RI Yu, Ri/HLX-4689-2023
OI Yu, Ri/0000-0002-2377-8654
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) - Korea government (MSIT) [2017-0-00878]
FX This work was supported by Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) (No. 2017-0-00878, (SW StarLab) Human motion
   simulation based on deep learning).
CR Al Borno M, 2013, IEEE T VIS COMPUT GR, V19, P1405, DOI 10.1109/TVCG.2012.325
   Bergamin K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356536
   Brauer SG, 2001, J GERONTOL A-BIOL, V56, pM489, DOI 10.1093/gerona/56.8.M489
   Constantinescu R, 2007, PARKINSONISM RELAT D, V13, P133, DOI 10.1016/j.parkreldis.2006.05.034
   Coros S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781156
   da Silva M, 2008, COMPUT GRAPH FORUM, V27, P371, DOI 10.1111/j.1467-8659.2008.01134.x
   Dingwell JB, 2001, J BIOMECH ENG-T ASME, V123, P27, DOI 10.1115/1.1336798
   Fujimoto S, 2018, PR MACH LEARN RES, V80
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Heess N., 2017, CoRR, abs/1707.02286.
   Hodgins J. K., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P71, DOI 10.1145/218380.218414
   Holden D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392440
   Hong S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322963
   Kavafoglu Z, 2018, VISUAL COMPUT, V34, P359, DOI 10.1007/s00371-016-1338-5
   Kwon T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983616
   Kwon Taesoo, 2010, P 2010 ACM SIGGRAPHE, P129
   Lasa de, 2010, ACM Trans. Graph., V29, P1
   Lee J, 1999, COMP GRAPH, P39
   Lee J., 2018, J OPEN SOURCE SOFTWA, V3, P500, DOI DOI 10.21105/JOSS.00500
   Lee S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3196492
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Lee Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818124
   Lee Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661233
   Lillicrap, 2015, ARXIV150902971, P1
   Liu LB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201315
   Liu LB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083723
   Liu LB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2893476
   Liu LB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366173
   Low K.H., 2011, Defense Science Research Conference and Expo DSR, P1
   Millard M, 2012, J COMPUT NONLIN DYN, V7, DOI 10.1115/1.4005462
   Mnih V., 2015, ARXIV PREPRINT ARXIV
   Pan ZR, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197565
   Park S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356501
   Paszke A, 2019, ADV NEUR IN, V32
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3197517.3201311, 10.1145/3450626.3459670]
   Peng XB, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099567
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Reda D, 2020, MOTION INTERACTION G, P1
   Rogers MW, 2001, J GERONTOL A-BIOL, V56, pM589, DOI 10.1093/gerona/56.9.M589
   Schafer R.C., 1987, CLIN BIOMECHANICS MU
   Schulman J., 2017, ARXIV
   Schulman J, 2015, ADV NEUR IN, V28
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Sok KW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276511, 10.1145/1239451.1239558]
   Tsai YY, 2010, IEEE T VIS COMPUT GR, V16, P325, DOI 10.1109/TVCG.2009.76
   Vicovaro M, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2617916
   WANG J, 2010, ACM T GRAPHIC, V29, P1
   Wang JH, 2012, ADV DIFFER EQU-NY, P1, DOI 10.1186/1687-1847-2012-96
   Wight DL, 2008, J COMPUT NONLIN DYN, V3, DOI 10.1115/1.2815334
   Won J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392381
   Won J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356499
   Xie ZM, 2020, COMPUT GRAPH FORUM, V39, P213, DOI 10.1111/cgf.14115
   Ye YT, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778811
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
   Yu WH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201397
   Yuan Y., 2020, ARXIV PREPRINT ARXIV
NR 56
TC 3
Z9 3
U1 3
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 473
EP 487
DI 10.1007/s00371-021-02342-9
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000737096800001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zardoua, Y
   Astito, A
   Boulaala, M
AF Zardoua, Yassir
   Astito, Abdelali
   Boulaala, Mohammed
TI A survey on horizon detection algorithms for maritime video
   surveillance: advances and future techniques
SO VISUAL COMPUTER
LA English
DT Article
DE Horizon line; Sea-sky line; Horizon detection; Target tracking; Maritime
   video surveillance; Real-time performance; Computational efficiency
ID ATTITUDE ESTIMATION; TRACKING
AB On maritime images, the horizon is a linear shape separating the sea and non-sea regions. This visual cue is essential in several sea video surveillance applications, including camera calibration, digital video stabilization, target detection and tracking, and distance estimation of detected targets. Given the nature of these applications, the horizon detection algorithm must satisfy robustness and real-time constraints. Our first aim in this paper is to provide a comprehensive review of horizon detection algorithms. After analyzing assumptions and test results reported in the horizon detection literature, we found a high trade-off between robustness and real-time performance. Thus, our second aim is to propose and describe three workable techniques to reduce this trade-off. The first technique aims to increase the robustness against contrast-degraded horizons. The non-sea region right above the horizon mainly depicts the sky, coast, ship, or combination of these classes. Thus, the second technique suggests a way to handle such class variation. While we believe that the last two techniques require relatively little computations, the third technique concerns using an alternative convolutional neural network (CNN) architecture to avoid a significant quantity of redundant computations in a previous CNN-based algorithm.
C1 [Zardoua, Yassir; Astito, Abdelali; Boulaala, Mohammed] Abdelmalek Essaadi Univ UAE, Fac Sci & Tech Tangier FST, Lab Informat Signals & Telecommun LIST, Ziaten BP 416, Tangier, Morocco.
C3 Abdelmalek Essaadi University of Tetouan
RP Zardoua, Y (corresponding author), Abdelmalek Essaadi Univ UAE, Fac Sci & Tech Tangier FST, Lab Informat Signals & Telecommun LIST, Ziaten BP 416, Tangier, Morocco.
EM yassirzardoua@gmail.com; abdelali_astito@yahoo.com; m.boulaala@gmail.com
RI Boulaala, Mohammed/HSH-5112-2023
OI Boulaala, Mohammed/0000-0002-7823-0549; ZARDOUA,
   Yassir/0000-0002-0032-465X
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Ali A, 2021, PROC SPIE, V11605, DOI 10.1117/12.2586983
   [Anonymous], 2014, INT J RECENT RES APP
   [Anonymous], COMPUT VIS PATTERN R
   Bao GQ, 2005, IEEE T INSTRUM MEAS, V54, P1067, DOI 10.1109/TIM.2005.847234
   Bouma H., 2008, ELECTROOPTICAL REMOT, V7114
   Cai CT, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0240-z
   Cane T, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P247
   Cornall T., 2005, 11 AUSTR INT AER C M
   Cornall TD, 2006, ELECTRON LETT, V42, P744, DOI 10.1049/el:20060547
   Das B, 2022, VISUAL COMPUT, V38, P179, DOI 10.1007/s00371-020-02010-4
   Dumble SJ, 2012, J INTELL ROBOT SYST, V68, P339, DOI 10.1007/s10846-012-9684-7
   Ettinger S.M., 2002, FLOR C REC ADV ROB 2
   Fefilatyev S, 2006, ICMLA 2006: 5TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P17
   Fefilatyev S, 2012, OCEAN ENG, V54, P1, DOI 10.1016/j.oceaneng.2012.06.028
   Gershikov E., 2013, Int. J. Adv. Intell. Syst, V6, P79, DOI DOI 10.1080/18756891.2013.756225
   Gershikov E, 2014, PROC INT CONF ADV, P262, DOI 10.1109/ATC.2014.7043395
   Grelsson B, 2016, J FIELD ROBOT, V33, P967, DOI 10.1002/rob.21639
   Grelsson B, 2013, LECT NOTES COMPUT SC, V7944, P478
   Grishin VA, 2018, J NAVIGATION, V71, P437, DOI 10.1017/S0373463317000650
   Hough PVC, 1962, U. S. Patent, Patent No. [3,069,654, 3069654]
   Jeong CY, 2018, ELECTRON LETT, V54, P760, DOI 10.1049/el.2018.0989
   Jeong CY, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718790753
   Jeong C, 2019, MULTIDIM SYST SIGN P, V30, P1187, DOI 10.1007/s11045-018-0602-4
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Kristan M, 2016, IEEE T CYBERNETICS, V46, P641, DOI 10.1109/TCYB.2015.2412251
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Kumeechai Pisanu, 2019, 2019 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON). Proceedings, P790, DOI 10.1109/ECTI-CON47248.2019.8955361
   Lee JT, 2017, IEEE I CONF COMP VIS, P3249, DOI 10.1109/ICCV.2017.350
   Lee S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0104-y
   Li FX, 2021, SIGNAL IMAGE VIDEO P, V15, P139, DOI 10.1007/s11760-020-01733-0
   Li K, 2016, NEUROCOMPUTING, V184, P207, DOI 10.1016/j.neucom.2015.07.137
   Liang D, 2020, IEEE T INSTRUM MEAS, V69, P45, DOI 10.1109/TIM.2019.2893008
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lipschutz I., 2013, Int. J. Comput. Eng. Res, V3, P1197
   Liu JY, 2021, MOBILE NETW APPL, V26, P1372, DOI 10.1007/s11036-021-01752-2
   Morris DD, 2007, PROC SPIE, V6497, DOI 10.1117/12.706099
   Petkovic M, 2020, TRANS MARIT SCI-TOMS, V9, P106, DOI 10.7225/toms.v09.n01.010
   Pires N., 2010, Navigation, V58, P1
   Praczyk T, 2019, TRANS MARIT SCI-TOMS, V8, P46, DOI 10.7225/toms.v08.n01.005
   Praczyk T, 2018, J MAR SCI TECH-JAPAN, V23, P164, DOI 10.1007/s00773-017-0464-8
   Prasad DK, 2017, IEEE T INTELL TRANSP, V18, P1993, DOI 10.1109/TITS.2016.2634580
   Prasad DK, 2016, J OPT SOC AM A, V33, P2491, DOI 10.1364/JOSAA.33.002491
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schwendeman M, 2015, J ATMOS OCEAN TECH, V32, P164, DOI 10.1175/JTECH-D-14-00047.1
   Shen YF, 2013, IEEE T AERO ELEC SYS, V49, P294, DOI 10.1109/TAES.2013.6404104
   Shen YF, 2013, IEEE GEOSCI REMOTE S, V10, P111, DOI 10.1109/LGRS.2012.2194473
   Sun Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092825
   van den Broek SP, 2008, PROC SPIE, V6969, DOI 10.1117/12.777542
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang B, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040543
   Wei H, 2009, GLOB HOM SEC 5 BIOM, V7306
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Zafarifar B, 2008, PROC SPIE, V6822, DOI 10.1117/12.766689
   Zhan WQ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9020213
   Zhang H, 2011, T I MEAS CONTROL, V33, P734, DOI 10.1177/0142331209342201
   Zhang W., 2012, US Patent, Patent No. [8,259,174, 8259174]
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zou X, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061682
NR 59
TC 10
Z9 10
U1 2
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 197
EP 217
DI 10.1007/s00371-021-02321-0
EA OCT 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000710083400001
DA 2024-07-18
ER

PT J
AU Wang, ZH
   Li, YJ
   Liu, JZ
   Ma, WY
   Deng, CY
AF Wang, Zhihao
   Li, Yajuan
   Liu, Jianzhen
   Ma, Weiyin
   Deng, Chongyang
TI Gauss-Seidel progressive iterative approximation (GS-PIA) for
   subdivision surface interpolation
SO VISUAL COMPUTER
LA English
DT Article
DE Gauss-Seidel iterative method; Progressive iterative approximation;
   Surface interpolation; Loop subdivision surface; Catmull-Clark
   subdivision surface
ID MESHES
AB We propose Gauss-Seidel progressive iterative approximation (GS-PIA) for subdivision surface interpolation by combining the Gauss-Seidel iterative method for linear systems and progressive iterative approximation (PIA) for free-form curve and surface interpolation. We address the details of GS-PIA for Loop and Catmull-Clark surface interpolation and prove that they are convergent. In addition, GS-PIA may also be applied to surface interpolation for other stationary approximating subdivision schemes with explicit limit position formula/masks. GS-PIA inherits many good properties of PIA, such as having intuitive geometric meaning and being easy to implement. Compared with some other existing interpolation methods by approximating subdivision schemes, GS-PIA has three main advantages. First, it has a faster convergence rate than PIA and weighted progressive iterative approximation (W-PIA). Second, GS-PIA does not need to compute optimal weights while W-PIA does. Third, GS-PIA does not modify the mesh topology but some methods with fairness measures do. Numerical examples for Loop and Catmull-Clark subdivision surface interpolation illustrated in this paper show the efficiency and effectiveness of GS-PIA.
C1 [Wang, Zhihao; Li, Yajuan; Liu, Jianzhen; Deng, Chongyang] Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
   [Ma, Weiyin] City Univ Hong Kong, Dept Mech Engn, Hong Kong, Peoples R China.
C3 Hangzhou Dianzi University; City University of Hong Kong
RP Deng, CY (corresponding author), Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
EM dcy@hdu.edu.cn
RI ; Deng, Chongyang/E-4422-2017
OI wang, zhihao/0000-0001-6348-7125; Deng, Chongyang/0000-0002-8725-4622
FU NSFC [61872121, 61761136010]; Natural Science Foundation of Zhejiang
   Province [LQ17A010009]; Research Grants Council of Hong Kong (GRF)
   [CityU 11206917]
FX This work has been supported by the NSFC (61872121, 61761136010),
   Natural Science Foundation of Zhejiang Province (LQ17A010009) and
   Research Grants Council of Hong Kong (GRF Grant No. CityU 11206917).
CR Brunet P., 1988, Computer-Aided Geometric Design, V5, P41, DOI 10.1016/0167-8396(88)90019-2
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Cheng FH, 2009, J COMPUT SCI TECH-CH, V24, P39, DOI 10.1007/s11390-009-9199-2
   Deng CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487231
   Deng CY, 2012, COMPUT AIDED DESIGN, V44, P424, DOI 10.1016/j.cad.2011.12.001
   Deng CY, 2010, SCI CHINA INFORM SCI, V53, P1765, DOI 10.1007/s11432-010-4049-y
   Deng Chongyang, 2010, Journal of Computer Aided Design & Computer Graphics, V22, P312
   Deng CY, 2010, VISUAL COMPUT, V26, P137, DOI 10.1007/s00371-009-0393-6
   DouglasFaires, 2015, NUMERICAL ANAL, P460
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Hoppe H., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P295, DOI 10.1145/192161.192233
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   Lai SH, 2006, VISUAL COMPUT, V22, P865, DOI 10.1007/s00371-006-0072-9
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Litke N, 2001, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2001.964527
   Loop C, 1987, THESIS U UTAH
   Ma WY, 2004, COMPUT AIDED DESIGN, V36, P525, DOI 10.1016/S0010-4485(03)00160-X
   Ma WY, 2002, VISUAL COMPUT, V18, P415, DOI 10.1007/s003710100159
   NASRI AH, 1987, ACM T GRAPHIC, V6, P29, DOI 10.1145/27625.27628
   Thall A., 2002, TR02001 U N CAR COMP
   Van Loan, 1996, MATRIX COMPUTATIONS, P512
   Weiyin Ma, 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P274, DOI 10.1109/GMAP.2000.838259
   Zheng JM, 2006, IEEE T VIS COMPUT GR, V12, P301, DOI 10.1109/TVCG.2006.49
   Zhou Qingnan, 2016, Thingi10k: A dataset of 10,000 3d-printing models
   Zorin D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P189, DOI 10.1145/237170.237254
   Zorin D., 2000, Subdivision for Modeling and Animation
NR 29
TC 2
Z9 2
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 139
EP 148
DI 10.1007/s00371-021-02318-9
EA OCT 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000707681700001
DA 2024-07-18
ER

PT J
AU Panda, MR
   Kar, SS
   Nanda, AK
   Priyadarshini, R
   Panda, S
   Bisoy, SK
AF Panda, Mohit Ranjan
   Kar, Sarthak Saurav
   Nanda, Aakash Kumar
   Priyadarshini, Rojanina
   Panda, Susmita
   Bisoy, Sukant Kishoro
TI Feedback through emotion extraction using logistic regression and CNN
SO VISUAL COMPUTER
LA English
DT Article
DE Face detection; Cropping; Logistic regression emotion classification;
   CNN emotion classification; Integrated machine generated feedback
ID RECOGNITION; SELECTION
AB The feedback process in the contemporary world is done on a timely basis filled particularly by the individual concerned. This hectic procedure often turns out to be peer-driven jeopardization of the primary objective of the process. To prevent this vulnerability, this work proposes a dynamic method of generating feedback automatically based on emotion classification by nonlinear logistic regression model and neural network-based convolutional neural networks (CNN). For a given test sample, our working project detects multiple faces followed by the cropping of these detected faces and finally, these cropped faces are stored in a destination folder. Iterating through the contents in this destination folder one by one, first, the binary classifier logistic regression gives a probabilistic output in the form of a percentage, the level of interest found on the concerned cropped facial image. Second, these iterated contents are passed on to the sophisticated CNN model, having the capability to detect and extract specific emotion features from an image. The CNN gives a detailed analysis report of the concerned individual by classifying them into emotions like Anger, Disgust, Contempt, Happiness, Neutral, Surprise or Fear. The outputs of these two models that are machine-generated feedback, would effectively encourage organizational, structural or end-user policy changes necessary to develop and evolve in the current competitive world.
C1 [Panda, Mohit Ranjan] KIIT Deemed Univ, Sch Comp Engn, Bhubaneswar, India.
   [Kar, Sarthak Saurav; Nanda, Aakash Kumar; Priyadarshini, Rojanina; Bisoy, Sukant Kishoro] CV Raman Global Univ, Dept Comp Sci & Engn, Bhubaneswar, India.
   [Panda, Susmita] Siksha O Anusandhan Deemed Univ, Dept Elect & Commun Engn, Inst Tech Educ & Res, Bhubaneswar, India.
C3 Kalinga Institute of Industrial Technology (KIIT); Siksha 'O' Anusandhan
   University
RP Panda, MR (corresponding author), KIIT Deemed Univ, Sch Comp Engn, Bhubaneswar, India.
EM mohit.pandafcs@kiit.ac.in
RI Priyadarshini, Rojalina/GZK-6275-2022
OI Priyadarshini, Rojalina/0000-0002-5481-5251; SONI,
   PRATEEK/0000-0002-7526-4559
CR Ali H, 2015, J MED IMAG HEALTH IN, V5, P1272, DOI 10.1166/jmihi.2015.1527
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen ZC, 2021, VISUAL COMPUT, V37, P685, DOI 10.1007/s00371-020-01880-y
   Garimella R.M, 2017, FACIAL EMOTION DETEC
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Li HH, 2019, APPL INTELL, V49, P2956, DOI 10.1007/s10489-019-01427-2
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mehta D, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081897
   Mittal V, VISUAL COMPUT, P1
   Mopuri Konda Reddy, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P62, DOI 10.1109/CVPRW.2015.7301273
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Nidhi, 2015, INT J APPL RES, V1, P396
   Pao J., 2016, EMOTION DETECTION FA
   Salah KB., 2021, VISUAL COMPUT, V38, P1
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Vanlalhruaia, 2017, 2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS), P3883, DOI 10.1109/ICECDS.2017.8390191
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Vivek TV, 2015, INT CONF COMM SYST, P472, DOI 10.1109/CSNT.2015.124
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   WU D, 2017, P IEEE C COMP VIS PA, P66
   Xu JY, 2021, VISUAL COMPUT, V37, P765, DOI 10.1007/s00371-020-01976-5
   Zhang YD, 2016, IEEE ACCESS, V4, P8375, DOI 10.1109/ACCESS.2016.2628407
   Zhang ZQ, 2021, VISUAL COMPUT, V37, P673, DOI 10.1007/s00371-020-01827-3
   Zhao Q, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/8196906
   Zhu YN, 2016, INT CONF SYST INFORM, P876, DOI 10.1109/ICSAI.2016.7811074
NR 28
TC 3
Z9 3
U1 3
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1975
EP 1987
DI 10.1007/s00371-021-02260-w
EA AUG 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000685399300003
DA 2024-07-18
ER

PT J
AU Ding, Y
   Duan, ZK
   Li, SR
AF Ding, Yi
   Duan, Zhikui
   Li, Shiren
TI Source-free unsupervised multi-source domain adaptation via proxy task
   for person re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-source domain adaptation; Person re-identification; Transfer
   knowledge
ID NETWORK
AB Most of existing unsupervised domain adaptation methods focus on aligning the feature discrepancy between labeled source and unlabeled target data. However, in practice, the source data may not be accessible due to transfer issue, privacy problem, etc. In such case, transferring knowledge through only the trained source model without the labeled source data remains a challenging problem. In this paper, we propose a novel method for source-free (without accessing any source domain data) multi-source domain adaptation in person re-identification (Re-ID). Two proxy tasks including proxy label learning and domain discriminative learning are designed to transfer knowledge from source models to the target domain with the inspiration of self-supervised learning. In the proxy label learning process, a subset of the unlabeled data in the target domain is randomly selected to trained a proxy label generator for measuring the similarity between each sample and the selected subset. With the proxy label as input, in the domain discriminative learning process, a domain discriminator is learnt to assign weights for measuring similarity between each source domain and the target one. With the combination of these two proxy tasks, knowledge from multiple source models can be properly aggregated and adaptively transferred to the target domain without any source data. Extensive evaluations on benchmark datasets, which are DukeMTMC and Market-1501, demonstrate the superior performance of the proposed method.
C1 [Ding, Yi] Hunan Univ Arts & Sci, Changde, Peoples R China.
   [Duan, Zhikui] Foshan Univ, Foshan, Peoples R China.
   [Li, Shiren] Sun Yat Sen Univ, Guangzhou, Peoples R China.
C3 Hunan University of Arts & Science; Foshan University; Sun Yat Sen
   University
RP Ding, Y (corresponding author), Hunan Univ Arts & Sci, Changde, Peoples R China.
EM mrtbs99@163.com
FU  [2019A1515110127]
FX The fundedwas grant by FoshanUniversity (2019A1515110127).
CR Ahmed Sk Miraj, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12141, DOI 10.1109/CVPR42600.2020.01216
   [Anonymous], 2017, ICML
   [Anonymous], 2018, ICML
   [Anonymous], 2019, CVPR
   Bai S, 2021, IEEE T PATTERN ANAL, V43, P2119, DOI 10.1109/TPAMI.2020.3031625
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding C., 2020, IEEE T PATTERN ANAL
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gidaris S., 2018, P 6 INT C LEARNING R
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Long M., 2017, ICML
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Quiñonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Song GL, 2018, AAAI CONF ARTIF INTE, P7347
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun SL, 2015, INFORM FUSION, V24, P84, DOI 10.1016/j.inffus.2014.12.003
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang Jiaqi, 2021, CVPR
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XP, 2021, IEEE T CIRC SYST VID, V31, P4020, DOI 10.1109/TCSVT.2020.3043444
   Wang XP, 2021, IEEE T IMAGE PROCESS, V30, P3017, DOI 10.1109/TIP.2021.3056223
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2019, PROC CVPR IEEE, P1187, DOI 10.1109/CVPR.2019.00128
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao H., 2018, NEURIPS
   Zhao SC, 2020, AAAI CONF ARTIF INTE, V34, P12975
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, arXiv preprint arXiv
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
NR 56
TC 3
Z9 3
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1871
EP 1882
DI 10.1007/s00371-021-02246-8
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000679759600001
DA 2024-07-18
ER

PT J
AU Li, YZ
   Luo, F
   Li, WJ
   Zheng, SJ
   Wu, HH
   Xiao, CX
AF Li, Yuanzhen
   Luo, Fei
   Li, Wenjie
   Zheng, Shenjie
   Wu, Huan-huan
   Xiao, Chunxia
TI Self-supervised monocular depth estimation based on image texture detail
   enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Texture detail enhancement; Monocular depth estimation; Structural
   integrity
ID VIDEO
AB We present a new self-supervised monocular depth estimation method with multi-scale texture detail enhancement. Based on the observation that the image texture detail and the semantic information have essential significance on the depth estimation, we propose to provide them to the network to learn more sharpness and structural integrity of depth. Firstly, we generate the filtered images and detail images by multi-scale decomposition and use a deep neural network to automatically learn their weights to construct the texture detail enhanced image. Then, we consider the semantic features by putting deep features from the VGG-19 network into a self-attention network, guide the depth decoder network to focus on the integrity of objects in the scene. Finally, we propose a scale-invariant smooth loss to improve the structural integrity of the predicted depth. We evaluate our method on the KITTI 2015 and Make3D datasets and apply the predicted depth to novel view synthesis. The experimental results show that it has achieved satisfactory results compared with the existing methods.
C1 [Li, Yuanzhen; Luo, Fei; Li, Wenjie; Zheng, Shenjie; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Whuhan 430072, Hubei, Peoples R China.
   [Wu, Huan-huan] Tarim Univ, Coll Informat Engn, Alar 843300, Xinjiang, Peoples R China.
C3 Wuhan University; Tarim University
RP Luo, F; Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Whuhan 430072, Hubei, Peoples R China.
EM luofei@whu.edu.cn; cxxiao@whu.edu.cn
RI Luo, Fei/IZQ-5485-2023
OI Luo, Fei/0000-0002-8481-8357
FU Key Technological Innovation Projects of Hubei Province [2018AAA062];
   NSFC [61972298]; Science and Technology Cooperation Project of The
   Xinjiang Production and Construction Corps [2019BC008]
FX This work is partially supported by the Key Technological Innovation
   Projects of Hubei Province (2018AAA062), NSFC (No. 61972298), Science
   and Technology Cooperation Project of The Xinjiang Production and
   Construction Corps (No. 2019BC008), Wuhan University-Huawei
   GeoInformatices Innovation Lab.
CR Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dippel S, 2002, IEEE T MED IMAGING, V21, P343, DOI 10.1109/TMI.2002.1000258
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fan XY, 2020, VISUAL COMPUT, V36, P2175, DOI 10.1007/s00371-020-01916-3
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Fu YP, 2020, PROC CVPR IEEE, P5949, DOI 10.1109/CVPR42600.2020.00599
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Garg V, 2012, INT J ADV COMPUT SC, V3, P130
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Kingma D. P., 2014, arXiv
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Klodt M, 2018, LECT NOTES COMPUT SC, V11214, P713, DOI 10.1007/978-3-030-01249-6_43
   Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Liao J, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1979
   Liu C, 2019, PROC CVPR IEEE, P10978, DOI 10.1109/CVPR.2019.01124
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258
   Luo Y, 2018, PROC CVPR IEEE, P155, DOI 10.1109/CVPR.2018.00024
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mehta I, 2018, INT CONF 3D VISION, P314, DOI 10.1109/3DV.2018.00044
   Niklaus S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356528
   Paszke Adam, 2017, NIPSW
   Pillai S, 2019, IEEE INT CONF ROBOT, P9250, DOI [10.1109/icra.2019.8793621, 10.1109/ICRA.2019.8793621]
   Poggi M, 2018, INT CONF 3D VISION, P324, DOI 10.1109/3DV.2018.00045
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225
   Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51
   Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50
   Yang ZH, 2018, AAAI CONF ARTIF INTE, P7493
   Yang ZH, 2018, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2018.00031
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 66
TC 10
Z9 11
U1 4
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2567
EP 2580
DI 10.1007/s00371-021-02206-2
EA JUN 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000667441600003
DA 2024-07-18
ER

PT J
AU Yang, TJ
   Liang, RQ
   Huang, L
AF Yang, Tiejun
   Liang, Ruiqiang
   Huang, Lin
TI Vehicle counting method based on attention mechanism SSD and state
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Vehicle counting; State detection; Attention mechanism; Saliency
   enhancement; Single-shot object detection
ID TRACKING; SYSTEM
AB To address the "overdetection" and "overtracking" problems experienced by traditional vehicle counting methods at low frame intervals, this paper proposes a vehicle counting method based on attention mechanism single-shot object detection (SSD) and state detection. First, we designed an attention-based saliency enhancement (ASE) module to fuse different scales of attention-enhanced feature mapping. Second, ASE modules were integrated into the SSD layers (from conv11 to conv15_2) to construct a vehicle detector for multiscale targets, named SSD-ASE. SSD-ASE was then used to detect and filter out vehicles located far from the identification line, where a filtering mechanism based on state detection that divides vehicles into four states based on their relative positions and the position identification line was used. Finally, by using the proposed state-based detector, the states of a vehicle were tracked using a relevant filtering algorithm, and counting was performed according to the state changes. The results of experiments performed on real traffic images/videos that we collected and on those obtained from the GRAM-RTM dataset showed that the proposed method achieved a higher detection accuracy (mAP = 95.94%) than the state-of-the-art methods and a higher tracking accuracy than traditional counting methods, while reducing the tracking number by approximately 60%.
C1 [Yang, Tiejun; Liang, Ruiqiang; Huang, Lin] Guilin Univ Technol, Guilin, Guangxi, Peoples R China.
C3 Guilin University of Technology
RP Huang, L (corresponding author), Guilin Univ Technol, Guilin, Guangxi, Peoples R China.
EM hlcucu@qq.com
RI Yang, Tiejun/AAJ-8197-2020
OI Yang, Tiejun/0000-0002-8644-4651; Huang, Lin/0000-0002-2678-2085
FU Guangxi Natural Science Foundation [2018GXNSFBA281081]; National Natural
   Science Foundation of China [61941202]
FX This research was supported in part by the Guangxi Natural Science
   Foundation (2018GXNSFBA281081) and the National Natural Science
   Foundation of China (61941202).
CR Abd-Elmagid MA, 2019, IEEE GLOB COMM CONF, DOI [10.1109/globecom38437.2019.9013924, 10.1109/itce.2019.8646549, 10.1109/ITCE.2019.8646549]
   Ahmed, 2005, 2005 5 INT C INF COM, P324
   Anastasiu David C., 2020, Journal of Big Data Analytics in Transportation, V2, P235, DOI 10.1007/s42421-020-00026-9
   [Anonymous], 2018, TSSD TEMPORAL SINGLE
   Barba-Guaman L, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9040589
   Bouvié C, 2013, IEEE IMTC P, P812
   Dai XR, 2019, SIGNAL PROCESS-IMAGE, V70, P79, DOI 10.1016/j.image.2018.09.002
   Deng ZP, 2017, IEEE J-STARS, V10, P3652, DOI 10.1109/JSTARS.2017.2694890
   Devi RB, 2021, VISUAL COMPUT, V37, P1207, DOI 10.1007/s00371-020-01862-0
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Guerrero-Gomez-Olmedo Ricardo, 2013, Natural and Artificial Computation in Engineering and Medical Applications. 5th International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2013. Proceedings: LNCS 7931, P306, DOI 10.1007/978-3-642-38622-0_32
   He YH, 2019, IEEE INT CONF ROBOT, P8339, DOI [10.1109/icra.2019.8793673, 10.1109/ICRA.2019.8793673]
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ke X, 2020, NEUROCOMPUTING, V399, P247, DOI 10.1016/j.neucom.2020.02.101
   Li H, 2020, VISUAL COMPUT, V36, P1693, DOI 10.1007/s00371-019-01769-5
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mandal Vishal., 2020, OBJECT DETECTION TRA
   Park HT, 2003, LECT NOTES ARTIF INT, V2718, P296
   Quesada J, 2016, IEEE IMAGE PROC, P3822, DOI 10.1109/ICIP.2016.7533075
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Song HS, 2019, EUR TRANSP RES REV, V11, DOI 10.1186/s12544-019-0390-4
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Wang L, 2017, IEEE INT CON MULTI, P1135, DOI 10.1109/ICME.2017.8019461
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu HX, 2017, SIGNAL IMAGE VIDEO P, V11, P905, DOI 10.1007/s11760-016-1038-7
   Zapletal D, 2016, IEEE COMPUT SOC CONF, P1568, DOI 10.1109/CVPRW.2016.195
   Zhang SH, 2020, IET IMAGE PROCESS, V14, P1621, DOI 10.1049/iet-ipr.2019.0465
NR 38
TC 5
Z9 5
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2871
EP 2881
DI 10.1007/s00371-021-02161-y
EA MAY 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000656115900001
DA 2024-07-18
ER

PT J
AU Minimol, PV
   Mishra, D
   Gorthi, RKSS
AF Venugopal Minimol, Pallavi
   Mishra, Deepak
   Gorthi, R. K. Sai Subrahmanyam
TI Guided MDNet tracker with guided samples
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural network; Correlation measure; Guided samples;
   Visual tracking
AB Visual tracking is the process of estimating the position of an object in a video sequence and plays a very important role in the field of autonomous video processing. Recent work renders that the trackers developed using deep learning techniques such as the convolutional neural network (CNN) exhibits outstanding performances in terms of accuracy and robustness as compared to other state-of-the-art trackers. Multi-domain convolutional neural network (MDNet) is a deep tracker which uses the CNN for estimating the target in each frame of the video sequence. The majority of the tracking challenges could be very easily handled by the MDNet tracker due to its offline training and online tracking features. The offline training stage helps in capturing the target representations into the shared layers of the CNN, while the online tracking uses a large number of probable random samples of bboxes (bounding boxes) around the previous target for estimating the target in the current frame. Once the target is estimated, a process of fine-tuning is performed which will update the weights of shared layers of CNN. The large number of random samples used for target estimation and the huge number of random training samples generated for the fine-tuning during the online stage makes tracking by MDNet computationally complex and slow. The major contribution of this paper is to suggest using guided samples to the input of the CNN rather than random samples. Moreover, it generates a lesser number of highly efficient training samples for the fine-tuning which helps in decreasing the computational complexity of the tracker by half without much compromise on the performance and thus improves the speed of the tracking process. An extensive evaluation has been performed on the proposed Guided MDNet with different datasets like ALOV300++, OTB and VOT, and its performances are measured in terms of metrics like F-score, one-pass evaluation, robustness and accuracy.
C1 [Venugopal Minimol, Pallavi; Mishra, Deepak] IIST Trivandrum, Thiruvananthapuram, Kerala, India.
   [Gorthi, R. K. Sai Subrahmanyam] IIT Tirupati, Tirupati, Andhra Pradesh, India.
C3 Department of Space (DoS), Government of India; Indian Institute of
   Space Science & Technology; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Tirupati
RP Mishra, D (corresponding author), IIST Trivandrum, Thiruvananthapuram, Kerala, India.
EM pallavivm91@gmail.com; deepak.mishra@iist.ac.in; rkg@iittp.in
RI Gorthi, Rama Krishna/T-5256-2019; MISHRA, DEEPAK/AER-1076-2022
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2010, P IEEE INT WORKSH PE
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Briechle K, 2001, PROC SPIE, V4387, P95, DOI 10.1117/12.421129
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Danelljan M., 2017, ARXIV60803773
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Han, 2015, VISUAL OBJECT TRACKI
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hsu HW, 2017, ASIAPAC SIGN INFO PR, P657, DOI 10.1109/APSIPA.2017.8282115
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kakanuru S, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3010006
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Nam H., MODELING PROPAGATING
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
NR 31
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1135
EP 1149
DI 10.1007/s00371-021-02072-y
EA FEB 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000616038400001
DA 2024-07-18
ER

PT J
AU Al-Gabalawy, M
AF Al-Gabalawy, Mostafa
TI RETRACTED: Temporal and non-temporal contextual saliency analysis for
   generalized wide-area (GWA) search within UAV video (Retracted Article)
SO VISUAL COMPUTER
LA English
DT Article; Retracted Publication
DE Unmanned aerial vehicles; Temporal contextual saliency; Generalized
   wide-area (GWA) search; Long&#8211; short-term memory; Deep
   convolutional neural networks
ID MODEL
AB Unmanned aerial vehicles (UAVs) can be used to great effect for wide-area searches such as search and rescue operations. UAV enables search and rescue teams to cover large areas more efficiently and in less time. However, using UAV for this purpose involves the creation of large amounts of data, typically in video format, which must be analyzed before any potential findings can be uncovered and actions taken. This is a slow and expensive process, which can result in significant delays to the response time after a target is seen by the UAV. To solve this problem, a deep model using a visual saliency approach to automatically analyze and detect anomalies in UAV video is proposed. Temporal contextual saliency model is based on the state of the art in visual saliency detection using deep convolutional neural networks and considers local and scene context, with novel additions in utilizing temporal information through a convolutional LSTM layer and modifications to the base model. Additionally, the impact of temporal versus non-temporal reasoning for this task is evaluated. This model achieves improved results on a benchmark dataset with the addition of temporal reasoning showing significantly improved results compared to the state of the art in saliency detection.
C1 [Al-Gabalawy, Mostafa] Pyramids Higher Inst Engn & Technol, Elect Power Engn & Automat Control Dept, 6th Of October City, Egypt.
RP Al-Gabalawy, M (corresponding author), Pyramids Higher Inst Engn & Technol, Elect Power Engn & Automat Control Dept, 6th Of October City, Egypt.
EM mostafagabalawy@gmail.com
FU Ain Shams University
FX Funding was provided by Ain Shams University.
CR Al-Gabalawy M, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106715
   [Anonymous], 2016, BIOMED RES INT, DOI DOI 10.1109/EUCAP.2016.7481633
   Azaza Aymen, 2018, 2018 International Conference on Advanced Systems and Electric Technologies (IC_ASET), P355, DOI 10.1109/ASET.2018.8379878
   Bozic-Stulic D, 2019, INT J COMPUT VISION, V127, P1256, DOI 10.1007/s11263-019-01177-1
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Gotovac S, 2016, INT CONF SOFTW, P378
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   King DB, 2015, ACS SYM SER, V1214, P1
   Krassanakis V, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040036
   Li J, 2015, IEEE I CONF COMP VIS, P190, DOI 10.1109/ICCV.2015.30
   Liu N., 2016, ABS16100 CORR
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Perrin AF, 2019, LECT NOTES COMPUT SC, V11678, P311, DOI 10.1007/978-3-030-29888-3_25
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi XJ, 2015, ADV NEUR IN, V28
   Sokalski J., 2010, P 25 INT C UNM AIR V, P111
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Wang L, 2011, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2011.6126231
   Zhang Y., 2013, P SPIE, V8918
   Zhang YJ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040652
NR 23
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 443
EP 443
DI 10.1007/s00371-020-02026-w
EA JAN 2021
PG 1
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604490300003
OA Bronze
DA 2024-07-18
ER

PT J
AU Rallis, I
   Bakalos, N
   Doulamis, N
   Doulamis, A
   Voulodimos, A
AF Rallis, Ioannis
   Bakalos, Nikolaos
   Doulamis, Nikolaos
   Doulamis, Anastasios
   Voulodimos, Athanasios
TI Bidirectional long short-term memory networks and sparse hierarchical
   modeling for scalable educational learning of dance choreographies
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Virtual Worlds and Games for Serious
   Applications (VS-Games)
CY SEP 05-07, 2018
CL Wurzburg, GERMANY
DE Bidirectional LSTM; Dance summarization; Intangible cultural heritage;
   Pose identification; Educational learning
ID CLASSIFICATION
AB Recently, several educational game platforms have been proposed in the literature for choreographic training. However, their main limitation is that they fail to provide a quantitative assessment framework of a performing choreography against a groundtruth one. In this paper, we address this issue by proposing a machine learning framework exploiting deep learning paradigms. In particular, we introduce a long short-term memory network with the main capability of analyzing 3D captured skeleton feature joints of a dancer into predefined choreographic postures. This pose identification procedure is capable of providing a detailed (fine) evaluation score of a performing dance. In addition, the paper proposes a choreographic summarization architecture based on sparse modeling representative selection (SMRS) in order to abstractly represent the performing choreography through a set of key choreographic primitives. We have modified the SMRS algorithm in a way to extract hierarchies of key representatives. Choreographic summarization provides an efficient tool for a coarse quantitative evaluation of a dance. Moreover, hierarchical representation scheme allows for a scalable assessment of a choreography. The serious game platform supports advanced visualization toolkits using Labanotation in order to deliver the performing sequence in a formal documentation.
C1 [Rallis, Ioannis; Bakalos, Nikolaos; Doulamis, Nikolaos; Doulamis, Anastasios] Natl Tech Univ Athens, 9 Herroon Polytech Str, Athens 15773, Greece.
   [Voulodimos, Athanasios] Univ West Attica, Dept Informat & Comp Engn, Agiou Spyridonos Str, Athens 12243, Greece.
C3 National Technical University of Athens; University of West Attica
RP Rallis, I (corresponding author), Natl Tech Univ Athens, 9 Herroon Polytech Str, Athens 15773, Greece.
EM irallis@central.ntua.gr; ndoulam@cs.ntua.gr; avoulod@uniwa.gr
RI Voulodimos, Athanasios/ABC-1836-2021; Doulamis, Anastasios/AAL-5972-2021
OI Rallis, Ioannis/0000-0003-4491-5854
CR [Anonymous], 2018, 2018 10 INT C VIRTUA
   [Anonymous], 1992, P SIGGRAPH COURS NOT
   [Anonymous], 2011, COMPUTER ANIMATION A
   [Anonymous], 2015, INT J COMPUTER ELECT
   [Anonymous], 2017, ARXIV170505548
   [Anonymous], 2014, P 2014 ACM SIGGRAPHE
   Aristidou A, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099566
   Aristidou A, 2018, VISUAL COMPUT, V34, P1725, DOI 10.1007/s00371-017-1452-z
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Aristidou A, 2015, ACM J COMPUT CULT HE, V8, DOI 10.1145/2755566
   Ballas A., 2017, IEEE 6th Global Conference on Consumer Electronics, GCCE, P1
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chowdhury G. G., 2010, Introduction to modern information retrieval
   Dimitropoulos K, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P773
   Doulamis AD, 2003, IEEE T NEURAL NETWOR, V14, P150, DOI 10.1109/TNN.2002.806645
   Doulamis A, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P451, DOI 10.5220/0006347304510460
   Doulamis N., 2017, Mixed reality and gamification for cultural heritage, P567, DOI DOI 10.1007/978-3-319-49607-8_23
   Doulamis N, 2016, IEEE CONF IMAGING SY, P318, DOI 10.1109/IST.2016.7738244
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Griesbeck Christian., 1996, INTRO LABANOTATION
   Hachimura K, 2001, ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P122, DOI 10.1109/ROMAN.2001.981889
   Hisatomi K, 2011, INT J COMPUT VISION, V94, P78, DOI 10.1007/s11263-011-0434-2
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsu H.M. J., 2011, INT J INFORM ED TECH, V1, P365, DOI [10.7763/IJIET.2011.V1.59, DOI 10.7763/IJIET.2011.V1.59]
   Kim D, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061261
   Kitsikidis A, 2015, LECT NOTES COMPUT SC, V9177, P472, DOI 10.1007/978-3-319-20684-4_46
   Kojima K, 2002, IEEE ROMAN 2002, PROCEEDINGS, P59, DOI 10.1109/ROMAN.2002.1045598
   Kurin R, 2004, MUSEUM INT, V56, P66, DOI 10.1111/j.1350-0775.2004.00459.x
   Laggis A., 2017, LOW COST MARKERLESS, P413
   Masurelle A., 2013, 2013 14 INT WORKSH I, P1, DOI [10.1109/WIAMIS.2013.6, DOI 10.1109/WIAMIS.2013.6]
   Pforsich J., 1997, Handbook for Laban Movement Analysis
   Protopapadakis E, 2017, INT ARCH PHOTOGRAMM, V42-2, P587, DOI 10.5194/isprs-archives-XLII-2-W3-587-2017
   Rallis I, 2018, COMPUT GRAPH-UK, V73, P88, DOI 10.1016/j.cag.2018.04.003
   Rallis I, 2017, INT CONF GAMES VIRTU, P94, DOI 10.1109/VS-GAMES.2017.8056576
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sheppard R.M., 2008, Proceeding of the 16th ACM international conference on Multimedia, MULTIMEDIA '08, P579
   Stavrakis E., 2012, Proceedings, P404, DOI 10.1007/978-3-642-34234-9_41
   Voulodimos A, 2020, MULTIMED TOOLS APPL, V79, P3243, DOI 10.1007/s11042-018-6935-z
   Voulodimos A, 2018, INT C PATT RECOG, P3013, DOI 10.1109/ICPR.2018.8545078
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang JJ, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.011028
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 43
TC 4
Z9 4
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 47
EP 62
DI 10.1007/s00371-019-01741-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA QE3UO
UT WOS:000616134400005
DA 2024-07-18
ER

PT J
AU Das, B
   Ebenezer, JP
   Mukhopadhyay, S
AF Das, Bijaylaxmi
   Ebenezer, Joshua Peter
   Mukhopadhyay, Sudipta
TI A comparative study of single image fog removal methods
SO VISUAL COMPUTER
LA English
DT Article
DE Fog removal; Image restoration; Transmission map; Color correction;
   Contrast enhancement; Deep learning
ID HAZE REMOVAL; VISIBILITY; WEATHER
AB The presence of fog degrades visibility in natural scene conditions. Computer vision applications like navigation, tracking, and surveillance need clear atmospheric images or videos as prerequisites for optimal performance. However, foggy atmosphere creates problems for computer vision applications due to reduced visibility. Different fog removal techniques are used to improve the visual quality of images and videos. The fog density depends on the depth information. Scene depth information estimation needs multiple images, which limits its real-life application. Hence, a single image fog removal requires some prior knowledge and/or assumptions to get the depth information. In this paper, the recent fog removal techniques are grouped into three broad categories: (1) filter-based methods, (2) color correction based methods, and (3) learning-based methods, for ease of understanding. The primary objective is to provide an introduction to this field and compare performance (both qualitative and quantitative) of representative techniques for each category. It is found that filter-based methods are doing overall better compared to other categories.
C1 [Das, Bijaylaxmi; Ebenezer, Joshua Peter; Mukhopadhyay, Sudipta] Indian Inst Technol Kharagpur, Elect & Elect Commun Engn Dept, Kharagpur, W Bengal, India.
   [Ebenezer, Joshua Peter] Univ Texas Austin, Austin, TX 78712 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur; University of Texas System; University of
   Texas Austin
RP Mukhopadhyay, S (corresponding author), Indian Inst Technol Kharagpur, Elect & Elect Commun Engn Dept, Kharagpur, W Bengal, India.
EM bijaylaxmi.das20@gmail.com; joshuaebenezer@utexas.edu; smukho@gmail.com
RI Ebenezer, Joshua/GMX-3219-2022
OI Ebenezer, Joshua/0000-0003-4936-9784
FU Indian Institute of Technology Kharagpur, India
FX The first and third authors are getting research scholar fellowship and
   salary under the employment of Indian Institute of Technology Kharagpur,
   India. The second author worked on this while at IIT Kharagpur and was
   not funded by any university or agency. This study is not funded by any
   other agency.
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen BH, 2016, J DISP TECHNOL, V12, P964, DOI 10.1109/JDT.2016.2552232
   Chen BH, 2016, J DISP TECHNOL, V12, P753, DOI 10.1109/JDT.2016.2518646
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo JM, 2017, IEEE T IMAGE PROCESS, V26, P4217, DOI 10.1109/TIP.2017.2706526
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He LY, 2017, IEEE T IMAGE PROCESS, V26, P1063, DOI 10.1109/TIP.2016.2644267
   Hu HM, 2019, IEEE T IMAGE PROCESS, V28, P2882, DOI 10.1109/TIP.2019.2891901
   Huang SC, 2015, IEEE T IND ELECTRON, V62, P2962, DOI 10.1109/TIE.2014.2364798
   Huang SC, 2014, IEEE T CIRC SYST VID, V24, P1814, DOI 10.1109/TCSVT.2014.2317854
   Jha DK, 2016, IET COMPUT VIS, V10, P331, DOI 10.1049/iet-cvi.2014.0449
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Kuanar S., 2019, ARXIV190200855
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Ling ZG, 2018, IEEE T MULTIMEDIA, V20, P1699, DOI 10.1109/TMM.2017.2778565
   Liu PJ, 2019, IEEE T IMAGE PROCESS, V28, P2212, DOI 10.1109/TIP.2018.2823424
   Liu RS, 2019, IEEE T NEUR NET LEAR, V30, P2973, DOI 10.1109/TNNLS.2018.2862631
   Liu W, 2020, IEEE T IMAGE PROCESS, V29, P7819, DOI 10.1109/TIP.2020.3007844
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Mandal S, 2020, IEEE T IMAGE PROCESS, V29, P2478, DOI 10.1109/TIP.2019.2957931
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Saxena A., 2006, NIPS, P1161, DOI DOI 10.1109/TPAMI.2015.2505283A
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Shi LF, 2018, IEEE T MULTIMEDIA, V20, P2503, DOI 10.1109/TMM.2018.2807593
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Singh D, 2018, MULTIMED TOOLS APPL, V77, P9595, DOI 10.1007/s11042-017-5321-6
   Son CH, 2018, IEEE T CIRC SYST VID, V28, P3111, DOI 10.1109/TCSVT.2017.2748150
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tripathi AK, 2012, IET IMAGE PROCESS, V6, P966, DOI 10.1049/iet-ipr.2011.0472
   Tripathi AK, 2012, IETE TECH REV, V29, P148, DOI 10.4103/0256-4602.95386
   Wang AN, 2019, IEEE T IMAGE PROCESS, V28, P381, DOI 10.1109/TIP.2018.2868567
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2545, DOI 10.1109/TMM.2019.2908375
   Yeh CH., 2019, IEEE T IMAGE PROCESS, VPP
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 58
TC 9
Z9 9
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 179
EP 195
DI 10.1007/s00371-020-02010-4
EA NOV 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000591128700001
DA 2024-07-18
ER

PT J
AU Imran, A
   Li, JQ
   Pei, Y
   Akhtar, F
   Mahmood, T
   Zhang, L
AF Imran, Azhar
   Li, Jianqiang
   Pei, Yan
   Akhtar, Faheem
   Mahmood, Tariq
   Zhang, Li
TI Fundus image-based cataract classification using a hybrid convolutional
   and recurrent neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Cataract detection; Fundus images; CNN; Retinal diseases; Transfer
   learning
ID RETINAL IMAGES
AB Cataract is the most prevailing reason for blindness across the globe, which occupies about 4.2% population of the world. Even with the developments in visual sciences, fundus image-based diagnosis is deemed as a gold standard for cataract detection and grading. Though the increase in the workload of ophthalmologists and complexity of fundus images, the results may be subject to intelligence. Therefore, the development of an automatic method for cataract detection is necessary to prevent visual impairment and save medical resources. This paper aims to provide a novel hybrid convolutional and recurrent neural network (CRNN) for fundus image-based cataract classification. The proposed CRNN fuses the advantages of convolution neural network and recurrent neural network to preserve long- and short-term spatial correlation between the patches. Coupled with transfer learning, we adopt AlexNet, GoogLeNet, ResNet and VGGNet to extract multilevel feature representation and to analyse how well these models perform cataract classification. The results demonstrate that the proposed method outperforms state-of-the-art methods with an average accuracy of 0.9739 for four-class cataract classification and provides a compelling reason to be applied for other retinal diseases.
C1 [Imran, Azhar; Li, Jianqiang; Akhtar, Faheem; Mahmood, Tariq] Beijing Univ Technol, Sch Software Engn, Beijing 100124, Peoples R China.
   [Li, Jianqiang] Beijing Engn Res Ctr IoT Software & Syst, Beijing 100124, Peoples R China.
   [Pei, Yan] Univ Aizu, Comp Sci Div, Aizu Wakamatsu, Fukushima 9658580, Japan.
   [Akhtar, Faheem] Sukkur IBA Univ, Dept Comp Sci, Sukkur 65200, Pakistan.
   [Mahmood, Tariq] Univ Educ, Div Sci & Technol, Lahore 54000, Pakistan.
   [Zhang, Li] Capital Med Univ, Beijing Tongren Hosp, Beijing Tongren Eye Ctr, Beijing, Peoples R China.
C3 Beijing University of Technology; University of Aizu; Sukkur IBA
   University; Capital Medical University
RP Li, JQ (corresponding author), Beijing Univ Technol, Sch Software Engn, Beijing 100124, Peoples R China.; Li, JQ (corresponding author), Beijing Engn Res Ctr IoT Software & Syst, Beijing 100124, Peoples R China.
EM lijianqiang@bjut.edu.cn
RI Pei, Yan/B-1356-2010; li, jian/GSE-0245-2022; li, jy/HTT-1535-2023; LI,
   Jing/HNB-5575-2023; Li, Jing/GYU-5036-2022; l, j/HNC-5728-2023; li,
   jian/IAQ-2794-2023; Liu, Jing/IQX-0664-2023; Mahmood,
   Tariq/AAQ-6709-2020; IMRAN, AZHAR/W-2615-2018; LI, JIAN/JAX-3092-2023;
   LI, JIAN/GRY-2197-2022
OI Pei, Yan/0000-0003-1545-9204; Mahmood, Tariq/0000-0002-4299-7756; IMRAN,
   AZHAR/0000-0003-3598-2780; 
FU Beijing Municipal Science and Technology [KM201910005028]
FX The funding was provided by Beijing Municipal Science and Technology
   (Grand No. KM201910005028)
CR Abramoff Michael D, 2010, IEEE Rev Biomed Eng, V3, P169, DOI 10.1109/RBME.2010.2084567
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   Awan R, 2018, LECT NOTES COMPUT SC, V10882, P788, DOI 10.1007/978-3-319-93000-8_89
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cao LC, 2020, INFORM FUSION, V53, P196, DOI 10.1016/j.inffus.2019.06.022
   Chorage SS, 2017, 2017 INTERNATIONAL CONFERENCE OF ELECTRONICS, COMMUNICATION AND AEROSPACE TECHNOLOGY (ICECA), VOL 1, P638, DOI 10.1109/ICECA.2017.8203617
   Dong SR, 2016, DESTECH TRANS SOC, P166
   Dong YX, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP), P17
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fan WM, 2015, 2015 17TH INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATION & SERVICES (HEALTHCOM), P459, DOI 10.1109/HealthCom.2015.7454545
   Güven A, 2013, COMPUT METHOD BIOMEC, V16, P425, DOI 10.1080/10255842.2011.623677
   Hagos M.T., 2019, ARXIV190507203
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoover A, 1998, J AM MED INFORM ASSN, P931
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2018, INT C FRONT COMP
   Li JQ, 2018, IEEE SYS MAN CYBERN, P3964, DOI 10.1109/SMC.2018.00672
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Manchalwar M., 2017, INT J ENG TECHNOL IJ
   Nayak J, 2009, J MED SYST, V33, P337, DOI 10.1007/s10916-008-9195-z
   Nazeri K, 2018, LECT NOTES COMPUT SC, V10882, P717, DOI 10.1007/978-3-319-93000-8_81
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pascolini D, 2012, BRIT J OPHTHALMOL, V96, P614, DOI 10.1136/bjophthalmol-2011-300539
   Pizzarello L, 2004, ARCH OPHTHALMOL-CHIC, V122, P615, DOI 10.1001/archopht.122.4.615
   Ran J, 2018, IEEE I C NETW INFRAS, P155, DOI 10.1109/ICNIDC.2018.8525852
   Röhlig M, 2018, VISUAL COMPUT, V34, P1209, DOI 10.1007/s00371-018-1486-x
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516044442, 10.1146/annurev-bioeng-071516-044442]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Somasundaram SK, 2017, J MED SYST, V41, DOI 10.1007/s10916-017-0853-x
   Song W, 2019, ESEC/FSE'2019: PROCEEDINGS OF THE 2019 27TH ACM JOINT MEETING ON EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P362, DOI 10.1145/3338906.3338950
   Souza MB, 2010, CLINICS, V65, P1223, DOI 10.1590/S1807-59322010001200002
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Xiong L, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/5645498
   Xu X, 2020, IEEE J BIOMED HEALTH, V24, P556, DOI 10.1109/JBHI.2019.2914690
   Yang JJ, 2016, COMPUT METH PROG BIO, V124, P45, DOI 10.1016/j.cmpb.2015.10.007
   Zhang L, 2017, COMPLEXITY, DOI 10.1155/2017/8917258
   Zhang LM, 2018, 2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P155, DOI 10.1109/ICCT.2018.8600116
   Zheng J, 2014, IEEE CONF IMAGING SY, P90, DOI 10.1109/IST.2014.6958452
NR 41
TC 20
Z9 20
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2407
EP 2417
DI 10.1007/s00371-020-01994-3
EA NOV 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000587645300002
DA 2024-07-18
ER

PT J
AU Kremer, M
   Haworth, B
   Kapadia, M
   Faloutsos, P
AF Kremer, Melissa
   Haworth, Brandon
   Kapadia, Mubbasir
   Faloutsos, Petros
TI Modelling distracted agents in crowd simulations
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Behavioural modelling; Distracted behaviours
ID SYNTHETIC VISION; VISUAL-ATTENTION; GAIT; BEHAVIORS; MEMORY; YOUNG
AB Multi-agent simulations can provide useful insights into the movement of pedestrians in arbitrary environments for predictive planning and analysis. The fidelity of such agents is important for the validity of the associated analyses. Current methods tend to employ agent models that are largely homogeneous in both physical abilities and behaviours. However, actual pedestrians exhibit a wide range of locomotion abilities and behaviours. In this work, we take a first step towards identifying and modellingdistractedbehaviours, such as walking and texting on a cell phone. Our models relate reported changes to the locomotion patterns and sensory abilities of distracted pedestrians to the corresponding parameters of a commonly used crowd simulation steering approach. We demonstrate experimentally that accounting for even a few of these behaviours significantly alters the flow patterns of the simulated agents. This impact affects overall crowd behaviour and is reflected in several crowd statistics including flow rate, effort, and kinetic energy.
C1 [Kremer, Melissa; Faloutsos, Petros] York Univ, Toronto, ON, Canada.
   [Haworth, Brandon] Univ Victoria, Victoria, BC, Canada.
   [Kapadia, Mubbasir] Rutgers State Univ, New Brunswick, NJ USA.
   [Faloutsos, Petros] Univ Hlth Network, Toronto Rehabil Inst, Toronto, ON, Canada.
C3 York University - Canada; University of Victoria; Rutgers University
   System; Rutgers University New Brunswick; University of Toronto;
   University Health Network Toronto; Toronto Rehabilitation Institute
RP Kremer, M (corresponding author), York Univ, Toronto, ON, Canada.
EM mkremer@cse.yorku.ca; bhaworth@uvic.ca; mk1353@cs.rutgers.edu;
   pfal@eecs.yorku.ca
OI Haworth, Brandon/0000-0001-8134-0047; Kremer,
   Melissa/0000-0002-8914-6449
FU National Science Foundation [RE08-054];  [IIS-1703883];  [SAS-1723869]
FX Fundingwas provided byOntario Research Foundation (Grant No. RE08-054)
   and National Science Foundation (Grant Nos. IIS-1703883, S&AS-1723869).
CR Agostini V, 2015, J NEUROENG REHABIL, V12, DOI 10.1186/s12984-015-0079-4
   Allbeck J. M., 2008, CREATING CROWD VARIA
   [Anonymous], 2005, TECHNICAL REPORT
   [Anonymous], 2015, SYNTH LECT VIS COMPU
   Bailly G, 2010, SPEECH COMMUN, V52, P598, DOI 10.1016/j.specom.2010.02.015
   Cha Jaeyun, 2015, Physical therapy rehabilitation science, V4, P32, DOI 10.1447s/ptrs.2015.4.1.32
   Curtis S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P128, DOI 10.1109/ICCVW.2011.6130234
   Durupinar F., 2008, P 7 INT JOINT C AUT, P1217
   Funge J., 1999, COGNITIVE MODELING K
   Grillon H, 2009, COMPUT ANIMAT VIRT W, V20, P111, DOI 10.1002/cav.293
   Guy S.J., SIMULATING HETEROGEN
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Haga S, 2015, PROCEDIA MANUF, V3, P2574, DOI 10.1016/j.promfg.2015.07.564
   Haworth B, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1783
   Haworth M.B., 2019, THESIS
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hill R., 1999, P 8 C COMP GEN FORC, P563
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jeon S, 2016, WORK, V53, P241, DOI 10.3233/WOR-152115
   Ju E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866162
   Kapadia M, 2015, PROCEEDINGS - I3D 2015, P85, DOI 10.1145/2699276.2699279
   Karamouzas I, 2009, LECT NOTES COMPUT SC, V5884, P41, DOI 10.1007/978-3-642-10347-6_4
   Khullar SC, 2001, AUTON AGENT MULTI-AG, V4, P9, DOI 10.1023/A:1010010528443
   Kokkinara E, 2011, COMPUT ANIMAT VIRT W, V22, P361, DOI 10.1002/cav.425
   Kuffner JJ, 1999, COMP ANIM CONF PROC, P118, DOI 10.1109/CA.1999.781205
   Lamberg EM, 2012, GAIT POSTURE, V35, P688, DOI 10.1016/j.gaitpost.2011.12.005
   Lee KC, 2007, 2007 MOBILE NETWORKING FOR VEHICULAR ENVIRONMENTS, P109, DOI 10.1109/MOVE.2007.4300814
   Licence S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133281
   Maples W C, 2008, Optometry, V79, P36, DOI 10.1016/j.optm.2007.04.102
   Narang S, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P91, DOI 10.1145/2993369.2993378
   Niederer D, 2018, GAIT POSTURE, V62, P415, DOI 10.1016/j.gaitpost.2018.04.007
   Patten CJD, 2004, ACCIDENT ANAL PREV, V36, P341, DOI 10.1016/S0001-4575(03)00014-9
   Peters C, 2002, COMPUT GRAPH FORUM, V21, P743, DOI 10.1111/1467-8659.00632
   Pizzamiglio S, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00460
   Plummer P, 2015, GAIT POSTURE, V41, P46, DOI 10.1016/j.gaitpost.2014.08.007
   Plummer-D'Amato P, 2012, J AGING RES, V2012
   Prupetkaew P, 2019, GAIT POSTURE, V68, P30, DOI 10.1016/j.gaitpost.2018.11.003
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Schabrun SM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0084312
   유경훈, 2015, [Journal of the Korean Society of Physical Medicine, 대한물리의학회지], V10, P25
   Tian Y, 2018, ERGONOMICS, V61, P1507, DOI 10.1080/00140139.2018.1493153
   Xue ZX, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9100239
   Yoshiki S., 2017, URBAN REGIONAL PLANN, V4, P138
   Yu QX, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P119
   Zhang X., 2019, P SIMAUD
   Zheng LP, 2016, NEUROCOMPUTING, V172, P180, DOI 10.1016/j.neucom.2014.12.103
NR 46
TC 5
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 107
EP 118
DI 10.1007/s00371-020-01969-4
EA SEP 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574081000002
DA 2024-07-18
ER

PT J
AU Wu, M
   Jin, X
   Jiang, Q
   Lee, SJ
   Liang, WT
   Lin, G
   Yao, SW
AF Wu, Min
   Jin, Xin
   Jiang, Qian
   Lee, Shin-jye
   Liang, Wentao
   Lin, Guo
   Yao, Shaowen
TI Remote sensing image colorization using symmetrical multi-scale DCGAN in
   YUV color space
SO VISUAL COMPUTER
LA English
DT Article
DE Image colorization; Multi-scale convolutional; Remote sensing image;
   Deep convolutional generative adversarial networks
ID SUPERRESOLUTION
AB Image colorization technique is used to colorize the gray-level image or single-channel image, which is a very significant and challenging task in image processing, especially the colorization of remote sensing images. This paper proposes a new method for coloring remote sensing images based on deep convolution generation adversarial network. The adopted generator model is a symmetrical structure using the principle of auto-encoder, and a multi-scale convolutional module is specially designed to introduce into the generator model. Thus, the proposed generator can enable the whole model to retain more image features in the process of up-sampling and down-sampling. Meanwhile, the discriminator uses residual neural network 18 that can compete with the generator, so that the generator and discriminator can effectively optimize each other. In the proposed method, the color space transformation technique is first utilized to convert remote sensing images from RGB to YUV. Then, the Y channel (a gray-level image) is used as the input of the neural network model to predict UV channels. Finally, the predicted UV channels are concatenated with the original Y channel as a whole YUV that is then transformed into RGB space to get the final color image. Experiments are conducted to test the performance of different image colorization methods, and the results show that the proposed method has good performance in both visual quality and objective indexes on the colorization of remote sensing image.
C1 [Wu, Min; Jin, Xin; Jiang, Qian; Liang, Wentao; Lin, Guo; Yao, Shaowen] Yunnan Univ, Sch Software, Kunming, Yunnan, Peoples R China.
   [Lee, Shin-jye] Natl Chiao Tung Univ, Inst Technol Management, Hsinchu, Peoples R China.
C3 Yunnan University
RP Jiang, Q (corresponding author), Yunnan Univ, Sch Software, Kunming, Yunnan, Peoples R China.
EM xinxin_jin@163.com; jiangqian_1221@163.com
RI jin, xin/GQZ-5811-2022; Jin, Xin/S-9172-2017
OI Jin, Xin/0000-0003-2211-2006; LEE, SHIN-JYE/0000-0003-4265-5016; Jiang,
   Qian/0000-0003-3097-0721
FU National Natural Science Foundation of China [61863036]; China
   Postdoctoral Science Foundation [2020T130564, 2019M653507]; Yunnan
   Province Postdoctoral Science Foundation; Doctoral Candidate Academic
   Award of Yunnan Province; Yunnan University's Research Innovation Fund
   for Graduate Students [2019164, 2019166]
FX This study is supported by the National Natural Science Foundation of
   China (No. 61863036). We also thank to the support of China Postdoctoral
   Science Foundation (Nos. 2020T130564, 2019M653507), Yunnan Province
   Postdoctoral Science Foundation, Doctoral Candidate Academic Award of
   Yunnan Province, and Yunnan University's Research Innovation Fund for
   Graduate Students (Nos. 2019164, 2019166).
CR Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7
   Arjovsky M., 2017, ARXIV170107875
   Avanaki AN, 2009, OPT REV, V16, P613, DOI 10.1007/s10043-009-0119-z
   Cer D, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Cheggoju N, 2018, VISUAL COMPUT, V34, P563, DOI 10.1007/s00371-017-1361-1
   Chen CJ, 2019, DIGIT SIGNAL PROCESS, V87, P155, DOI 10.1016/j.dsp.2019.01.021
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Cheng ZH, 2017, IEEE T IMAGE PROCESS, V26, P5491, DOI 10.1109/TIP.2017.2740620
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Chiang TW, 2005, IEEE SYS MAN CYBERN, P351
   Dong JY, 2019, IEEE GEOSCI REMOTE S, V16, P173, DOI 10.1109/LGRS.2018.2870880
   Fang FM, 2020, IEEE T VIS COMPUT GR, V26, P2931, DOI 10.1109/TVCG.2019.2908363
   Gauge C, 2012, TOMATED COLORIZATION
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gravey M, 2019, ISPRS J PHOTOGRAMM, V147, P242, DOI 10.1016/j.isprsjprs.2018.11.003
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu HB, 2012, SENSOR LETT, V10, P177, DOI 10.1166/sl.2012.1837
   Hua W, 2012, VISUAL COMPUT, V28, P755, DOI 10.1007/s00371-012-0710-3
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li B, 2019, IEEE T IMAGE PROCESS, V28, P4606, DOI 10.1109/TIP.2019.2912291
   Li B, 2017, NEUROCOMPUTING, V266, P687, DOI 10.1016/j.neucom.2017.05.083
   Li W, 2020, APPL INTELL, V50, P341, DOI 10.1007/s10489-019-01515-3
   Li YM, 2018, APPL INTELL, V48, P4128, DOI 10.1007/s10489-018-1200-8
   Limmer M, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P61, DOI [10.1109/ICMLA.2016.0019, 10.1109/ICMLA.2016.114]
   Liu H, 2018, J VIS COMMUN IMAGE R, V53, P20, DOI 10.1016/j.jvcir.2018.02.016
   Luan Q., 2007, Proceedings of the 18th Eurographics conference on Rendering Techniques, P309
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Maryam K, 2019, APPL INTELL, V20
   Nazeri K., 2018, IEEE INT CONF COMP V
   Nida N, 2016, IIOAB J, V7, P202
   Radford A., 2015, ARXIV
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Richart M, 2017, 2017 WORKSHOP OF COMPUTER VISION (WVC), P55, DOI 10.1109/WVC.2017.00017
   Suárez PL, 2017, IEEE COMPUT SOC CONF, P212, DOI 10.1109/CVPRW.2017.32
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Nguyen TN, 2015, VISUAL COMPUT, V31, P391, DOI 10.1007/s00371-014-0934-5
   Todo H, 2019, VISUAL COMPUT, V35, P1
   Varga D, 2017, LECT NOTES COMPUT SC, V10424, P184, DOI 10.1007/978-3-319-64689-3_15
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Welsh T, 2002, P 29 ANN C COMPUTER, P77
   Zeng K, 2019, APPL INTELL, V49, P292, DOI 10.1007/s10489-018-1270-7
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zong GG, 2015, INT SYM COMPUT INTEL, P366, DOI 10.1109/ISCID.2015.128
NR 53
TC 20
Z9 20
U1 2
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1707
EP 1729
DI 10.1007/s00371-020-01933-2
EA AUG 2020
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000563612200002
DA 2024-07-18
ER

PT J
AU Wang, C
   Xing, XY
   Yao, GL
   Su, ZX
AF Wang, Cong
   Xing, Xiaoying
   Yao, Guangle
   Su, Zhixun
TI Single image deraining via deep shared pyramid network
SO VISUAL COMPUTER
LA English
DT Article
DE Single image deraining; Deep neural network; Shared parameters; Pyramid
   network; Dense connections
AB Single image deraining is a highly ill-posed problem. Existing deep neural network-based algorithms usually use larger deep models to solve this problem, which is less effective and efficient. In this paper, we propose a deep neural network based on feature pyramid to solve image deraining. Our algorithm is motivated that the features at different pyramid levels share similar structures. Based on this property, we develop an effective deep neural network, where the deep models at different feature pyramid levels share the same weight parameters. In addition, we further develop a multi-stream dilation convolution to deal with complex rainy streaks. To preserve the image detail, we develop dense connections that can maintain important features from different levels. Our algorithm is trained in an end-to-end manner. Quantitative and qualitative experimental results demonstrate that the proposed method performs favorably against state-of-the-art deraining methods in terms of accuracy as well as model sizes. The source code and dataset will be available at.
C1 [Wang, Cong; Su, Zhixun] Dalian Univ Technol, Dalian, Peoples R China.
   [Xing, Xiaoying] Tsinghua Univ, Beijing, Peoples R China.
   [Yao, Guangle] Chengdu Univ Technol, Chengdu, Peoples R China.
   [Su, Zhixun] Guilin Univ Elect Technol, Guilin, Peoples R China.
C3 Dalian University of Technology; Tsinghua University; Chengdu University
   of Technology; Guilin University of Electronic Technology
RP Wang, C (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.
EM supercong94@gmail.com
OI Wang, Cong/0000-0002-6068-0103
FU National Science and Technology Major Project [2018ZX04041001-007]
FX This work was supported by the National Science and Technology Major
   Project [Grant Nos. 2018ZX04041001-007].
CR Althoff M, 2009, IEEE T INTELL TRANSP, V10, P299, DOI 10.1109/TITS.2009.2018966
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fan X, 2019, VISUAL COMPUT, V35, P565, DOI 10.1007/s00371-018-1485-y
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Gonzalezgarcia A., 2018, ADV NEURAL INFORM PR, P1287
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kingma D. P., 2014, arXiv
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li XW, 2015, 2015 INTERNATIONAL CONFERENCE ON ENVIRONMENT, MANUFACTURING INDUSTRY AND ECONOMIC DEVELOPMENT, (EMIED 2015), P262
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang C, 2020, APPL INTELL, V50, P1437, DOI 10.1007/s10489-019-01567-5
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang W., 2017, PROC CVPR IEEE, P1685, DOI DOI 10.1109/CVPR.2017.183
   Yuan Q, 2020, VISUAL COMPUT, V36, P1591, DOI 10.1007/s00371-019-01762-y
   Zhang CY, 2009, ISI: 2009 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS, P164, DOI 10.1109/ISI.2009.5137290
   Zhang H, 2019, IEEE TCSVT
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang SD, 2018, IEEE INT CON MULTI
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang W, 2019, IEEE WORK ADV ROBOT, P355, DOI [10.1109/ARSO46408.2019.8948741, 10.1109/arso46408.2019.8948741]
NR 39
TC 10
Z9 10
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1851
EP 1865
DI 10.1007/s00371-020-01944-z
EA AUG 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000554347900001
DA 2024-07-18
ER

PT J
AU Choi, J
   Son, MG
   Lee, YY
   Lee, KH
   Park, JP
   Yeo, CH
   Park, J
   Choi, S
   Kim, WD
   Kang, TW
   Ko, K
AF Choi, Junho
   Son, Moon Gu
   Lee, Yong Yi
   Lee, Kwan H.
   Park, Jin Pyo
   Yeo, Chang Hun
   Park, Jungseo
   Choi, Sungin
   Kim, Won Don
   Kang, Tae Won
   Ko, Kwanghee
TI Position-based augmented reality platform for aiding construction and
   inspection of offshore plants
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented reality; Offshore plants; Tracking; Registration; Fabrication;
   Inspection
ID POINT CLOUDS; REGISTRATION
AB We propose an augmented reality platform to help workers with fabrication and inspection of offshore structures. The platform is designed to work under various constraints that are commonly encountered in an industrial environment and often make the existing AR methods fail to work properly. It consists of modules for tracking, registration, and augmentation. The tracking module estimates the worker's position in real time. The registration module aligns the actual objects with 3D computer-aided design models to estimate an accurate pose of the worker's mobile device. The augmentation module correctly augments information on the target objects on the device screen using the pose data. We test several application scenarios to demonstrate the feasibility of the proposed platform for the fabrication and inspection processes of an offshore plant.
C1 [Choi, Junho; Son, Moon Gu; Lee, Yong Yi; Lee, Kwan H.; Ko, Kwanghee] Gwangju Inst Sci & Technol, Sch Mech Engn, 123 Cheomdangwagiro, Gwangju 61005, South Korea.
   [Park, Jin Pyo; Yeo, Chang Hun] SOFTHILLS Co Ltd, Seoul 04780, South Korea.
   [Park, Jungseo; Choi, Sungin] SAMSUNG Heavy Ind, Geoju 53261, South Korea.
   [Kim, Won Don; Kang, Tae Won] Marine Tech Co Ltd, Busan 48059, South Korea.
C3 Gwangju Institute of Science & Technology (GIST); Samsung
RP Ko, K (corresponding author), Gwangju Inst Sci & Technol, Sch Mech Engn, 123 Cheomdangwagiro, Gwangju 61005, South Korea.
EM khko@gist.ac.kr
RI Kan, Tae Wook/CAI-4750-2022
FU National IT Industry Promotion Agency (NIPA) - Korean Government
   Ministry of Science and ICT (MSIT)
FX This work was supported by the National IT Industry Promotion Agency
   (NIPA), grant funded by the Korean Government Ministry of Science and
   ICT (MSIT). Grant No. S0602-17-1021, for development of a smart mixed
   reality technology for improving the pipe installation and inspection
   processes in the offshore structure fabrication.
CR 214 Vicente M.-G., 2018, ADVANCEMENTS COMPUTE, P74
   Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Bosché F, 2012, ADV ENG INFORM, V26, P90, DOI 10.1016/j.aei.2011.08.009
   Bueno M, 2018, AUTOMAT CONSTR, V89, P120, DOI 10.1016/j.autcon.2018.01.014
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Feigl T., 2020, 15 INT JOINT C COMP
   Filippeschi A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061257
   Lee JM, 2010, J MECH SCI TECHNOL, V24, P197, DOI 10.1007/s12206-009-1129-2
   Leu MC, 2013, CIRP ANN-MANUF TECHN, V62, P799, DOI 10.1016/j.cirp.2013.05.005
   Lu CH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020420
   Madgwick SOH, 2011, INT C REHAB ROBOT
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mohssen N., 2014, ARXIV14112156 CORR
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Norrdine A, 2016, IEEE SENS J, V16, P6766, DOI 10.1109/JSEN.2016.2585599
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Son H, 2015, ADV ENG INFORM, V29, P172, DOI 10.1016/j.aei.2015.01.009
   Triggs B., 2000, VISION ALGORITHMS TH, P298, DOI DOI 10.1007/3-540-44480-7_21THISWORKWASSUPPORTEDINPARTBYTHEEUROPEAN
   Wang L, 2011, IEEE INT SOC CONF, P24, DOI 10.1109/SOCC.2011.6085070
   Wang XY, 2014, AUTOMAT CONSTR, V40, P96, DOI 10.1016/j.autcon.2013.12.003
   Yun DH, 2017, GRAPH MODELS, V90, P1, DOI 10.1016/j.gmod.2017.02.001
   Yun D, 2015, ADV ENG INFORM, V29, P930, DOI 10.1016/j.aei.2015.09.008
   Yun X, 2007, IEEE INT CONF ROBOT, P2526, DOI 10.1109/ROBOT.2007.363845
   Zollmann S, 2014, P IEEE, V102, P137, DOI 10.1109/JPROC.2013.2294314
NR 26
TC 8
Z9 10
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2039
EP 2049
DI 10.1007/s00371-020-01902-9
EA JUL 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000547242400003
DA 2024-07-18
ER

PT J
AU Shi, WL
   Du, HQ
   Mei, WB
   Ma, ZF
AF Shi, Wenling
   Du, Huiqian
   Mei, Wenbo
   Ma, Zhifeng
TI (SARN)spatial-wise attention residual network for image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Spatial attention; Non-local block; Residual network
AB Recent research suggests that attention mechanism is capable of improving performance of deep learning-based single image super-resolution (SISR) methods. In this work, we propose a deep spatial-wise attention residual network (SARN) for SISR. Specifically, we propose a novel spatial attention block (SAB) to rescale pixel-wise features by explicitly modeling interdependencies between pixels on each feature map, encoding where (i.e., attentive spatial pixels in feature map) the visual attention is located. A modified patch-based non-local block can be inserted in SAB to capture long-distance spatial contextual information and relax the local neighborhood constraint. Furthermore, we design a bottleneck spatial attention module to widen the network so that more information is allowed to pass. Meanwhile, we adopt local and global residual connections in SISR to make the network focus on learning valuable high-frequency information. Extensive experiments show the superiority of the proposed SARN over the state-of-art methods on benchmark datasets in both accuracy and visual quality.
C1 [Shi, Wenling; Du, Huiqian; Mei, Wenbo; Ma, Zhifeng] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Du, HQ (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM 3120180804@bit.edu.cn; duhuiqian@bit.edu.cn; wbmei@bit.edu.cn
CR [Anonymous], 2018, P EUR C COMP VIS
   [Anonymous], 2018, C NEUR INF PROC SYST
   [Anonymous], 2019, IEEE C COMP VIS PATT
   [Anonymous], 2017, C NEUR INF PROC SYST
   [Anonymous], BAM BOTTLENECK UNPUB
   [Anonymous], 2018, IEEE C COMP VIS PATT
   [Anonymous], 2011, IEEE C COMP VIS PATT
   [Anonymous], 2012, BRIT MACH VIS C BMVC
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma Diederik P, 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yu J., 2019, BMVC
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang Yulun, 2019, ARXIV191203046
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
NR 39
TC 22
Z9 22
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1569
EP 1580
DI 10.1007/s00371-020-01903-8
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000547242400001
DA 2024-07-18
ER

PT J
AU Shi, LK
   Ma, HQ
   Zhang, J
AF Shi, Lukui
   Ma, Hongqi
   Zhang, Jun
TI Automatic detection of pulmonary nodules in CT images based on 3D Res-I
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Pulmonary nodule detection; U-net; Faster R-CNN; Rectangular
   convolutional kernels; Grouping convolution; Pre-activation
ID FALSE-POSITIVE REDUCTION; LUNG NODULES; SEGMENTATION; PREDICTION;
   ALGORITHM; SELECTION
AB It is difficult for the existing detection methods of the pulmonary nodules to take into account the global and local features simultaneously. It will lead to over-fitting and lower sensitivity since the extracted features of 3D pulmonary nodules is too complex. To solve these problems, a model based an improved 3D residual structure (3D Res-I) was proposed to detect pulmonary nodules. In the model, the basic residual structure is improved by using rectangular convolution kernel, grouping convolution and pre-activation. Rectangular convolution kernel expands the receptive filed of the convolution, which effectively takes into account the global and local features of the pulmonary nodules. Grouping convolution reduces the computational cost of the model. Pre-activation operation alleviates over-fitting phenomenon. 3D Res-I structure is combined with the improved U-Net network as the feature extraction network of Faster R-CNN. The experimental results on LUNA16 dataset show that the proposed model improves the detection accuracy of pulmonary nodules and reduces the average number of false positives and the size of the generated model.
C1 [Shi, Lukui; Ma, Hongqi; Zhang, Jun] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
   [Shi, Lukui; Ma, Hongqi; Zhang, Jun] Hebei Univ Technol, Hebei Key Lab Big Data Comp, Tianjin 300401, Peoples R China.
C3 Hebei University of Technology; Hebei University of Technology
RP Shi, LK (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.; Shi, LK (corresponding author), Hebei Univ Technol, Hebei Key Lab Big Data Comp, Tianjin 300401, Peoples R China.
EM shilukui@scse.hebut.edu.cn
RI Zhang, Jun/GZG-5788-2022
OI Zhang, Jun/0000-0003-4345-454X
FU Natural Science Foundation of Hebei Province of China [F2017202145]
FX This work was supported by the Natural Science Foundation of Hebei
   Province of China [Grant No. F2017202145].
CR Alakwaa W, 2017, INT J ADV COMPUT SC, V8, P409
   Aykac D, 2003, IEEE T MED IMAGING, V22, P940, DOI 10.1109/TMI.2003.815905
   Bae KT, 2005, RADIOLOGY, V236, P286, DOI 10.1148/radiol.2361041286
   Bakshi A, 2018, PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P305, DOI 10.1109/ICCSP.2018.8524220
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Boeroezky L, 2006, IEEE T INF TECHNOL B, V10, P504, DOI 10.1109/TITB.2006.872063
   Chen Xu, 2002, Journal of Shanghai Jiaotong University, V36, P946
   [代双凤 Dai Shuangfeng], 2016, [电子与信息学报, Journal of Electronics & Information Technology], V38, P2358
   Dehmeshi J, 2008, IEEE T MED IMAGING, V27, P467, DOI 10.1109/TMI.2007.907555
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gu Y, 2018, COMPUT BIOL MED, V103, P220, DOI 10.1016/j.compbiomed.2018.10.011
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   IOFFE S, 2016, 32 INT C MACH LEARN, P676
   Keshani M, 2013, COMPUT BIOL MED, V43, P287, DOI 10.1016/j.compbiomed.2012.12.004
   Kitasaka T, 2002, PROC SPIE, V4684, P1496, DOI 10.1117/12.467116
   Ko JP, 2004, J THORAC IMAG, V19, P136, DOI 10.1097/01.rti.0000135973.65163.69
   Kuanar S., 2019, ARXIV190200855
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Lassen BC, 2015, PHYS MED BIOL, V60, P1307, DOI 10.1088/0031-9155/60/3/1307
   Li D, 2019, DIAGNOSTICS, V9, DOI 10.3390/diagnostics9040207
   Li Q, 2008, ACAD RADIOL, V15, P165, DOI 10.1016/j.acra.2007.09.018
   LI Y, 2011, DETECTION ALGORITHM
   LUO X, 2015, RES DETECTION ALGORI
   MIAO F, 2018, DETECTION PULMONARY
   Murphy K, 2009, MED IMAGE ANAL, V13, P757, DOI 10.1016/j.media.2009.07.001
   Okumura T, 1998, INT C PATT RECOG, P1671, DOI 10.1109/ICPR.1998.712041
   Pu J, 2008, COMPUT MED IMAG GRAP, V32, P452, DOI 10.1016/j.compmedimag.2008.04.005
   Qi Dou, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10435, P630, DOI 10.1007/978-3-319-66179-7_72
   QUI S, 2012, STUDY ALGORITHMS LUN
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Riccardi A, 2011, MED PHYS, V38, P1962, DOI 10.1118/1.3560427
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Santos AM, 2014, ENG APPL ARTIF INTEL, V36, P27, DOI 10.1016/j.engappai.2014.07.007
   Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809
   Sharma S, 2020, VISUAL COMPUT, V36, P1755, DOI 10.1007/s00371-019-01768-6
   Shen Wei, 2015, Inf Process Med Imaging, V24, P588, DOI 10.1007/978-3-319-19992-4_46
   Sivakumar S, 2013, INT J ENG TECHNOL, V5, P179
   Tariq A, 2013, PROCEEDINGS OF THE 2013 FOURTH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE IN MEDICAL IMAGING (CIMI), P49, DOI 10.1109/CIMI.2013.6583857
   Wu SY, 2012, GLOB CONGRESS INTELL, P257, DOI 10.1109/GCIS.2012.46
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   YANG JJ, 2017, MATH MODEL APPL
   ZHANG T, 2018, RES DETECTION DIAGNO
   [赵鹏飞 Zhao Pengfei], 2018, [计算机科学, Computer Science], V45, P162
   Zhu WT, 2018, IEEE WINT CONF APPL, P673, DOI 10.1109/WACV.2018.00079
NR 47
TC 13
Z9 14
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1343
EP 1356
DI 10.1007/s00371-020-01869-7
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000545456500001
DA 2024-07-18
ER

PT J
AU Guclu, O
   Can, AB
AF Guclu, Oguzhan
   Can, Ahmet Burak
TI Integrating global and local image features for enhanced loop closure
   detection in RGB-D SLAM systems
SO VISUAL COMPUTER
LA English
DT Article
DE SLAM; Loop closure; Autocorrelogram; Histogram; Keypoint matching
ID SCALE; WORDS; BAGS
AB Loop closure detection is essential for simultaneous localization and mapping systems to decrease accumulating drift of trajectory estimations. Robust loop closure detection is specifically important in large-scale mapping, but it gets more challenging as the mapping environment grows. This paper proposes a SLAM system utilizing a two-pass loop closure detection method to improve mapping accuracy in large-scale environments. The proposed system finds loop closure candidates by employing global and local image features together. After selecting a group of candidates by similarity of global features, the system applies keypoint matching on this group to improve scene matching accuracy and determines loop closure candidates. We extensively evaluate the system on the widely used TUM RGB-D dataset, which contains sequences of small to large-scale indoor environments, with respect to different parameter combinations. The results show that the proposed method increases accuracy substantially and achieves large-scale mapping with acceptable overhead.
C1 [Guclu, Oguzhan; Can, Ahmet Burak] Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
C3 Hacettepe University
RP Can, AB (corresponding author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
EM guclu.oguzhan@outlook.com; abc@hacettepe.edu.tr
RI Guclu, Oguzhan/AAA-2207-2020; Can, Ahmet B/M-8712-2018
OI Guclu, Oguzhan/0000-0002-3914-1359; 
CR Agrawal M, 2008, LECT NOTES COMPUT SC, V5305, P102, DOI 10.1007/978-3-540-88693-8_8
   Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514
   [Anonymous], 2010, P INT S EXP ROB ISER
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2008, LECT NOTES COMPUT SC, V5302, P58, DOI 10.1007/978-3-540-88682-2_6
   Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483
   Endres F, 2014, IEEE T ROBOT, V30, P177, DOI 10.1109/TRO.2013.2279412
   Endres F, 2012, IEEE INT CONF ROBOT, P1691, DOI 10.1109/ICRA.2012.6225199
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu ZH, 2018, INT C PATT RECOG, P1689, DOI 10.1109/ICPR.2018.8545135
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   Gao X, 2017, AUTON ROBOT, V41, P1, DOI 10.1007/s10514-015-9516-2
   Glocker B, 2015, IEEE T VIS COMPUT GR, V21, P571, DOI 10.1109/TVCG.2014.2360403
   Grisetti G, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P3478
   Guclu O, 2019, J INTELL ROBOT SYST, V93, P495, DOI 10.1007/s10846-017-0718-z
   Guclu O, 2016, IEEE INT CONF AUTON, P174, DOI 10.1109/ICARSC.2016.42
   Guclu O, 2015, LECT NOTES COMPUT SC, V9164, P297, DOI 10.1007/978-3-319-20801-5_32
   Gutierrez-Gomez D, 2016, ROBOT AUTON SYST, V75, P571, DOI 10.1016/j.robot.2015.09.026
   Henry P, 2012, INT J ROBOT RES, V31, P647, DOI 10.1177/0278364911434148
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Konolige K., 2010, BMVC, V10, P102, DOI DOI 10.5244/C.24.102
   Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607
   Labbé M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maier R, 2014, LECT NOTES COMPUT SC, V8753, P54, DOI 10.1007/978-3-319-11752-2_5
   Merrill N., 2018, ROBOTICS SCI SYSTEMS
   Mur- Artal Raul, 2017, IEEE T ROBOT, DOI [DOI 10.1109/TR0.2017.2705103, DOI 10.1109/TRO.2017.2705103]
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nicosevici T, 2012, IEEE T ROBOT, V28, P886, DOI 10.1109/TRO.2012.2192013
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Rühle S, 2009, J RENEW SUSTAIN ENER, V1, DOI 10.1063/1.3081510
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Stückler J, 2014, J VIS COMMUN IMAGE R, V25, P137, DOI 10.1016/j.jvcir.2013.02.008
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Whelan T., 2012, Proceedings of the RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
NR 42
TC 14
Z9 14
U1 7
U2 56
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1271
EP 1290
DI 10.1007/s00371-019-01720-8
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400015
DA 2024-07-18
ER

PT J
AU Bi, HB
   Wang, K
   Lu, D
   Wu, CL
   Wang, W
   Yang, LN
AF Bi, Hongbo
   Wang, Kang
   Lu, Di
   Wu, Chenlei
   Wang, Wei
   Yang, Lina
TI C<SUP>2</SUP>Net: a complementary co-saliency detection network
SO VISUAL COMPUTER
LA English
DT Article
DE Co-saliency detection; Complementary information; Short connection; Deep
   learning
ID VISUAL-ATTENTION; DEEP; MODEL
AB The main purpose of image Co-saliency detection is to detect objects with similar properties or prospects in a group of images. In this paper, we propose a novel Converged Complementary Co-saliency detection framework in an end-to-end manner for Co-saliency detection. Specifically, considering the consistency and the difference between the features of the intra-image and the inter-image, our network contains two parts [forward information transfer module and complementary information enhancement module (CIEM)]. Each part is a double-branch network with short connections to avoid incomplete information transmission, including single-saliency detection branch (SSDB) and Co-saliency detection branch (CSDB). In FITM, the SSDB and CSDB extract the corresponding features, respectively. In order to produce more accurate results, we feed the SSDB and CSDB into the holistic attention and perform further feature extraction. In CIEM, we extract the useful information of the non-salient area. At the end of the network, we fuse the FITM and CIEM to generate the final Co-saliency map. Furthermore, we rearranged a new data set based on the existing public data set. Compared with other Co-saliency methods, the experimental results show that our proposed model achieves state-of-the-art performance.
C1 [Bi, Hongbo; Wang, Kang; Lu, Di; Wu, Chenlei; Wang, Wei; Yang, Lina] Northeast Petr Univ, Daqing, Peoples R China.
C3 Northeast Petroleum University
RP Bi, HB (corresponding author), Northeast Petr Univ, Daqing, Peoples R China.
EM bhbdq@126.com; kangwangwww@163.com; 18810463560@163.com;
   wcl_master@126.com; 15776556110@163.com; linayangScience@126.com
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2017, IEEE T PATTERN ANAL
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chang K.Y., 2011, 24 IEEE C COMP VIS P, P22
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Du SZ, 2015, IEEE SIGNAL PROC LET, V22, P145, DOI 10.1109/LSP.2014.2347333
   Fan D.-P., 2019, ARXIV190706781
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fanman Meng, 2012, 2012 International Symposium on Intelligent Signal Processing and Communications Systems (ISPACS 2012), P781, DOI 10.1109/ISPACS.2012.6473597
   Fu H., 2014, 2014 IEEE CONFERENCE
   Fu H., 2015, 2015 IEEE C COMP VIS
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jacobs D.E., 2010, ACM S USER INTERFACE, P219
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li M., 2018, BRIT MACH VIS C
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Papushoy A, 2015, DIGIT SIGNAL PROCESS, V36, P156, DOI 10.1016/j.dsp.2014.09.005
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Quan HL, 2017, IEEE ACCESS, V5, P23519, DOI 10.1109/ACCESS.2017.2764503
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Nguyen VK, 2019, IEEE CONF COMPUT, P957, DOI 10.1109/infcomw.2019.8845260
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang W, 2019, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2019.2905607
   Wang W., 2017, IEEE T CIRCUITS SYST, V28, P1727, DOI DOI 10.1109/TCSVT.2017.2701279
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Winn J., 2005, 10 IEEE INT C COMP V
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zheng XJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P959, DOI 10.1145/3240508.3240648
NR 61
TC 4
Z9 4
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 911
EP 923
DI 10.1007/s00371-020-01842-4
EA APR 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000528657100002
DA 2024-07-18
ER

PT J
AU Yoo, I
   Fiser, M
   Hu, KM
   Benes, B
AF Yoo, Innfarn
   Fiser, Marek
   Hu, Kaimo
   Benes, Bedrich
TI Character motion in function space
SO VISUAL COMPUTER
LA English
DT Article
DE Character motion; Functional principal component analysis; Orthonormal
   basis functions
ID CAPTURE DATA; COMPRESSION
AB We address the problem of animated character motion representation and approximation by introducing a novel form of motion expression in a function space. For a given set of motions, our method extracts a set of orthonormal basis (ONB) functions. Each motion is then expressed as a vector in the ONB space or approximated by a subset of the ONB functions. Inspired by the static PCA, our approach works with the time-varying functions. The set of ONB functions is extracted from the input motions by using functional principal component analysis and it has an optimal coverage of the input motions for the given input set. We show the applications of the novel compact representation by providing a motion distance metric, motion synthesis algorithm, and a motion level of detail. Not only we can represent a motion by using the ONB; a new motion can be synthesized by optimizing connectivity of reconstructed motion functions, or by interpolating motion vectors. The quality of the approximation of the reconstructed motion can be set by defining a number of ONB functions, and this property is also used to level of detail. Our representation provides compression of the motion. Although we need to store the generated ONB that are unique for each set of input motions, we show that the compression factor of our representation is higher than for commonly used analytic function methods. Moreover, our approach also provides lower distortion rate.
C1 [Yoo, Innfarn; Fiser, Marek] Purdue Univ, W Lafayette, IN 47907 USA.
   [Hu, Kaimo] Purdue Univ, Dept Comp Graph, W Lafayette, IN 47907 USA.
   [Benes, Bedrich] Purdue Univ, Technol, W Lafayette, IN 47907 USA.
   [Benes, Bedrich] Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; Purdue University System; Purdue University; Purdue
   University System; Purdue University
RP Benes, B (corresponding author), Purdue Univ, Technol, W Lafayette, IN 47907 USA.; Benes, B (corresponding author), Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
EM bbenes@purdue.edu
RI Benes, Bedrich/A-8150-2016
OI Benes, Bedrich/0000-0002-5293-2112
FU National Science Foundation [10001387, 10001364]
FX This research was funded in part by National Science Foundation Grants
   #10001387, Functional Proceduralization of 3D Geometric Models and
   #10001364, Multimodal Affective Pedagogical Agents for Different Types
   of Learners.
CR Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   Alon J, 2003, PROC CVPR IEEE, P375
   [Anonymous], 2009, P 2009 ACM SIGGRAPH
   [Anonymous], 2004, P INT C VERY LARGE D
   Arikan O, 2006, ACM T GRAPHIC, V25, P890, DOI 10.1145/1141911.1141971
   Aristidou A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275038
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Beaudoin P., 2008, P 2008 ACM SIGGRAPHE, P117
   Bernard J, 2013, IEEE T VIS COMPUT GR, V19, P2257, DOI 10.1109/TVCG.2013.178
   Bertschek I, 2006, CONTRIB STAT, P130, DOI 10.1007/3-7908-1701-5_9
   Chao MW, 2012, IEEE T VIS COMPUT GR, V18, P729, DOI 10.1109/TVCG.2011.53
   Coffey N, 2011, HUM MOVEMENT SCI, V30, P1144, DOI 10.1016/j.humov.2010.11.005
   Courrieu P, 2008, ARXIV08044809 CORR
   Du H, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P134, DOI [10.1109/CIS.2016.38, 10.1109/CIS.2016.0039]
   Forbes N, 2005, IMITATION OF LIFE: HOW BIOLOGY IS INSPIRING COMPUTING, P67
   Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Hou J., 2014, ARXIV14104730 CORR
   Ikemoto L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477927
   Ikemoto Leslie, 2006, P 2006 S INT 3D GRAP, P49
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Lai Yu-Chi., 2005, SCA 2005: Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P281, DOI DOI 10.1145/1073368.1073409
   Lau M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618517
   Lee J, 1999, COMP GRAPH, P39
   Lee K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275071
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Levine S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185524
   Liu Guodong., 2006, SCA 06, P127
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Mordatch I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778808
   Ormoneit D, 2005, IMAGE VISION COMPUT, V23, P1264, DOI 10.1016/j.imavis.2005.09.004
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3197517.3201311, 10.1145/3450626.3459670]
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Ramsay JO., 2006, Functional Data Analysis
   Reitsma PSA, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289609
   Safonova A., 2007, Construction and optimal search of interpolated motion graphs, DOI DOI 10.1145/1275808.1276510
   Shin HJ, 2006, COMPUT ANIMAT VIRT W, V17, P219, DOI 10.1002/cav.125
   Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260
   Tournier M, 2009, COMPUT GRAPH FORUM, V28, P355, DOI 10.1111/j.1467-8659.2009.01375.x
   Unuma M., 1995, P 22 ANN C COMPUTER, P91, DOI DOI 10.1145/218380.218419
   Vogele A, 2014, 2014 ACM SIGGRAPH EU
   Wang H., 2019, IEEE T VISUALIZATION
   Wang Zhiyong, 2019, IEEE T VISUALIZATION
   Wei XL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966398
   Yao F, 2005, ANN STAT, V33, P2873, DOI 10.1214/009053605000000660
   Yoo I, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP), VOL 1, P110, DOI 10.5220/0007456401100121
   Yoo I, 2015, COMPUT GRAPH-UK, V47, P59, DOI 10.1016/j.cag.2014.11.001
   Yoo I, 2014, VISUAL COMPUT, V30, P213, DOI 10.1007/s00371-013-0797-1
   Zhao LM, 2009, GRAPH MODELS, V71, P139, DOI 10.1016/j.gmod.2009.04.001
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
   Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812
NR 54
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 735
EP 748
DI 10.1007/s00371-020-01840-6
EA APR 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000523400600001
DA 2024-07-18
ER

PT J
AU Doyle, L
   Mould, D
AF Doyle, Lars
   Mould, David
TI Augmenting photographs with textures using the Laplacian pyramid
SO VISUAL COMPUTER
LA English
DT Article
DE Image stylization; Laplacian pyramid; Texture synthesis
ID IMAGE
AB We introduce a method to stylize photographs with auxiliary textures, by means of the Laplacian pyramid. Laplacian pyramid coefficients from a synthetic texture are combined with the coefficients from the original image by means of a smooth maximum function. The final result is a stylized image which maintains the structural characteristics from the input, including edges, color, and existing texture, while enhancing the image with additional fine-scale details. Further, we extend patch-based texture synthesis to include a guidance channel so that texture structures are aligned with an orientation field, obtained through the image structure tensor.
C1 [Doyle, Lars; Mould, David] Carleton Univ, Sch Comp Sci, Graph Imaging & Games Lab, Ottawa, ON, Canada.
C3 Carleton University
RP Doyle, L (corresponding author), Carleton Univ, Sch Comp Sci, Graph Imaging & Games Lab, Ottawa, ON, Canada.
EM larsdoyle@cmail.carleton.ca; mould@scs.carleton.ca
OI Doyle, Lars/0000-0002-1674-3795
FU NSERC; Carleton University
FX We would like to thank the anonymous reviewers for many insightful
   comments. Thanks also to Eric Paquette and members of the Graphics,
   Imaging and Games Lab for productive comments and discussions. Funding
   for this work was provided by NSERC and by Carleton University. We used
   many images from Flickr under a CreativeCommons license. Thanks to the
   numerous photographers who provided material: Bill Showalter (barn), Jim
   Sorbie (cabin), Alana Sise (rocky hill), Sergio Sakakibara (cameraman),
   Harry Rose (grass), delta ! (girl), bananaana04 (parrot), Maria Morri
   (earring), Donald Lammers (ruin), Tim Adams (walkway), Ryan Basilio
   (berries), Gabor Lengyel (portrait), Stephanie Kroos (lion), and Martin
   Pettitt (car).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Akl A, 2015, IEEE T IMAGE PROCESS, V24, P4082, DOI 10.1109/TIP.2015.2458701
   [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], ACM T GRAPH
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 2016, ARXIV161207919
   [Anonymous], ACM T GRAPH
   [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], ACM T GRAPH
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Brox T, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P17, DOI 10.1007/3-540-31272-2_2
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151
   Cook J. D., 2011, UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Fang H, 2004, ACM T GRAPHIC, V23, P354, DOI 10.1145/1015706.1015728
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan EA, 2006, ACM T GRAPHIC, V25, P654, DOI 10.1145/1141911.1141937
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee H, 2011, COMPUT GRAPH-UK, V35, P81, DOI 10.1016/j.cag.2010.11.008
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Okura F, 2015, COMPUT GRAPH FORUM, V34, P53, DOI 10.1111/cgf.12678
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Semmo A., 2015, P WORKSHOP COMPUTATI, P149
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Zhou Y, 2017, COMPUT GRAPH FORUM, V36, P199, DOI 10.1111/cgf.13119
NR 43
TC 5
Z9 5
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1489
EP 1500
DI 10.1007/s00371-018-1513-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900011
DA 2024-07-18
ER

PT J
AU Abbasi, A
   Kalkan, S
   Sahillioglu, Y
AF Abbasi, Ali
   Kalkan, Sinan
   Sahillioglu, Yusuf
TI Deep 3D semantic scene extrapolation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D scenes; Extrapolation; Convolutional neural networks
ID COMPLETION; OPTIMIZATION; REGISTRATION; GEOMETRY
AB Scene extrapolation is a challenging variant of the scene completion problem, which pertains to predicting the missing part(s) of a scene. While the 3D scene completion algorithms in the literature try to fill the occluded part of a scene such as a chair behind a table, we focus on extrapolating the available half-scene information to a full one, a problem that, to our knowledge, has not been studied yet. Our approaches are based on convolutional neural networks (CNN). As input, we take the half of 3D voxelized scenes, then our models complete the other half of scenes as output. Our baseline CNN model consisting of convolutional and ReLU layers with multiple residual connections and Softmax classifier with voxel-wise cross-entropy loss function at the end. We train and evaluate our models on the synthetic 3D SUNCG dataset. We show that our trained networks can predict the other half of the scenes and complete the objects correctly with suitable lengths. With a discussion on the challenges, we propose scene extrapolation as a challenging test bed for future research in deep learning. We made our models available on https://github.com/aliabbasi/d3dsse.
C1 [Abbasi, Ali; Kalkan, Sinan; Sahillioglu, Yusuf] Middle East Tech Univ, Ankara, Turkey.
C3 Middle East Technical University
RP Abbasi, A (corresponding author), Middle East Tech Univ, Ankara, Turkey.
EM abbasi.ali.tab@gmail.com; skalkan@ceng.metu.edu.tr; ys@ceng.metu.edu.tr
RI KALKAN, Sinan/AAC-3625-2019
OI Sahillioglu, Yusuf/0000-0002-7997-4232; Kalkan,
   Sinan/0000-0003-0915-5917
FU Scientific and Technological Research Council of Turkey (TuBTAK)
   [EEEAG-215E255]
FX This work has been supported by the Scientific and Technological
   Research Council of Turkey (TuBTAK) under the Project EEEAG-215E255. We
   also thank NVIDIA for their donation of a Titan X graphics card.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   [Anonymous], binvox
   Averbuch-Elor H, 2017, VISUAL COMPUT, V52, P1
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Gal R., 2007, Symposium on Geometry Processing, number EPFL-CONF-149318, P253
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Harary G, 2014, COMPUT GRAPH FORUM, V33, P45, DOI 10.1111/cgf.12430
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang YJ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982401
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kelly B., 2017, ARXIV170900584
   Kermani ZS, 2016, COMPUT GRAPH FORUM, V35, P197, DOI 10.1111/cgf.12976
   [Краевский В.В. Kraevsky V.V.], 2005, [Педагогика, Pedagogika], P13
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li DP, 2017, IEEE T VIS COMPUT GR, V23, P1809, DOI 10.1109/TVCG.2016.2553102
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liepa Peter, 2003, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P200, DOI DOI 10.2312/SGP/SGP03/200-206
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu H, 2012, VISUAL COMPUT, V28, P279, DOI 10.1007/s00371-011-0638-z
   Mavridis P, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12741
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mnih A, 2014, PR MACH LEARN RES, V32, P1791
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sahillioglu Y, 2010, COMPUT VIS IMAGE UND, V114, P334, DOI 10.1016/j.cviu.2009.12.003
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu JJ, 2016, ADV NEUR IN, V29
   Xia C, 2017, COMPUT AIDED DES APP, V4, P1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang B, 2017, IEEE INT CONF COMP V, P679, DOI 10.1109/ICCVW.2017.86
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yang L, 2017, VISUAL COMPUT, V33, P385, DOI 10.1007/s00371-016-1208-1
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu F., 2015, ARXIV
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
   Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402
NR 52
TC 10
Z9 10
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 271
EP 279
DI 10.1007/s00371-018-1586-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Prantl, M
   Vása, L
AF Prantl, M.
   Vasa, L.
TI Estimation of differential quantities using Hermite RBF interpolation
SO VISUAL COMPUTER
LA English
DT Article
DE Curvature; RBF; HRBF; Discrete Differential Geometry; Computer Graphics
ID MESHES
AB Curvature is an important geometric property in computer graphics that provides information about the character of object surfaces. The exact curvature can only be calculated for a limited set of surface descriptions. Most of the time, we deal with triangles, point sets or some other discrete representation of the surface. For those, curvature can only be estimated. However, surfaces can be fitted by some kind of interpolation function and from it, curvature can be calculated directly. This paper proposes a method for curvature estimation and normal vector re-estimation based on surface fitting using Hermite Radial Basis Function interpolation. Hermite variation uses not only control points, but normal vectors at those points as well. This leads to a better and more robust interpolation than if only control points are used. Once the interpolant is obtained, the curvature and other possible properties can be directly computed using known approaches. The proposed algorithm was tested on several explicit and implicit functions, and it outperforms current state-of-the-art methods if exact normals are available. For normals calculated directly from a triangle mesh, the proposed algorithm works on par with existing state-of-the-art methods.
C1 [Prantl, M.; Vasa, L.] Univ West Bohemia, Dept Comp Sci & Engn, Fac Appl Sci, Univ 8, Plzen 30614, Czech Republic.
   [Vasa, L.] Univ West Bohemia, NTIS, Univ 8, Plzen 30614, Czech Republic.
C3 University of West Bohemia Pilsen; University of West Bohemia Pilsen
RP Prantl, M (corresponding author), Univ West Bohemia, Dept Comp Sci & Engn, Fac Appl Sci, Univ 8, Plzen 30614, Czech Republic.
EM perry@kiv.zcu.cz; lvasa@kiv.zcu.cz
RI Prantl, Martin/AAR-5491-2020; Vasa, Libor/F-6706-2011
OI Prantl, Martin/0000-0002-7900-5028; Vasa, Libor/0000-0002-0213-3769
FU UWB grant Advanced Graphical and Computing Systems [SGS-2016-013]
FX This work was supported by the UWB grant SGS-2016-013 Advanced Graphical
   and Computing Systems.
CR [Anonymous], 2003, PROC 19 ANN S COMPUT, DOI DOI 10.1145/777792.777839
   Barequet G, 2001, J ALGORITHMS, V38, P91, DOI 10.1006/jagm.2000.1127
   Batagelo HC, 2007, VISUAL COMPUT, V23, P803, DOI 10.1007/s00371-007-0133-8
   Bridson Robert., 2007, SIGGRAPH sketches, V10, DOI DOI 10.1145/1278780.1278807
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Goldman R, 2005, COMPUT AIDED GEOM D, V22, P632, DOI 10.1016/j.cagd.2005.06.005
   Gray A., 1997, MODERN DIFFERENTIAL, P394
   Guennebaud G., 2010, Eigen
   Hildebrandt K, 2011, COMPUT AIDED GEOM D, V28, P321, DOI 10.1016/j.cagd.2011.05.001
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kalogerakis E., 2007, P EUROGRAPHICSSIGGRA, P13
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   Macêdo I, 2009, SIBGRAPI, P1, DOI 10.1109/SIBGRAPI.2009.11
   Mao ZH, 2011, COMPUT AIDED DESIGN, V43, P1561, DOI 10.1016/j.cad.2011.06.006
   Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Pottmann H, 2009, COMPUT AIDED GEOM D, V26, P37, DOI 10.1016/j.cagd.2008.01.002
   Pouget Marc, 2016, CGAL USER REFERENCE
   Razdan A, 2005, COMPUT AIDED DESIGN, V37, P1481, DOI 10.1016/j.cad.2005.03.003
   Rusinkiewicz S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P486, DOI 10.1109/TDPVT.2004.1335277
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Theisel H, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P288, DOI 10.1109/PCCGA.2004.1348359
   Vaillant R, 2013, ELECTRONIC
   Vása L, 2016, COMPUT GRAPH FORUM, V35, P271, DOI 10.1111/cgf.12982
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Yang P., 2007, EUR S POINT BAS GRAP, DOI [10. 2312/SPBG/SPBG07/029-036, DOI 10.2312/SPBG/SPBG07/029-036]
NR 29
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1645
EP 1659
DI 10.1007/s00371-017-1438-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400003
DA 2024-07-18
ER

PT J
AU Barendrecht, PJ
   Luinstra, M
   Hogervorst, J
   Kosinka, J
AF Barendrecht, Pieter J.
   Luinstra, Martijn
   Hogervorst, Jonathan
   Kosinka, Jiri
TI Locally refinable gradient meshes supporting branching and sharp colour
   transitions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Vector graphics; Gradient meshes; Colour interpolation; Local refinement
AB We present a local refinement approach for gradient meshes, a primitive commonly used in the design of vector illustrations with complex colour propagation. Local refinement allows the artist to add more detail only in the regions where it is needed, as opposed to global refinement which often clutters the workspace with undesired detail and potentially slows down the workflow. Moreover, in contrast to existing implementations of gradient mesh refinement, our approach ensures mathematically exact refinement. Additionally, we introduce a branching feature that allows for a wider range of mesh topologies, as well as a feature that enables sharp colour transitions similar to diffusion curves, which turn the gradient mesh into a more versatile and expressive vector graphics primitive.
C1 [Barendrecht, Pieter J.; Luinstra, Martijn; Hogervorst, Jonathan; Kosinka, Jiri] Univ Groningen, Bernoulli Inst, Nijenborgh 9, NL-9747 Groningen, Netherlands.
C3 University of Groningen
RP Barendrecht, PJ (corresponding author), Univ Groningen, Bernoulli Inst, Nijenborgh 9, NL-9747 Groningen, Netherlands.
EM p.j.barendrecht@rug.nl
RI Luinstra, Martijn/JHT-7851-2023
OI Kosinka, Jiri/0000-0002-8859-2586; , Pieter/0000-0002-1556-6213
CR Adobe Systems, 1998, AD ILL 8 0 CLASSR BO
   [Anonymous], 2010, P INT S NONPH AN REN
   Barla P., 2013, Image and Video-Based Artistic Stylisation, P149
   Batra V, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766968
   Bowers JC, 2011, COMPUT GRAPH FORUM, V30, P1345, DOI 10.1111/j.1467-8659.2011.01994.x
   Boyé S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366192
   Coburn F. D., 1999, CORELDRAW9 OFFICIAL
   Dalstein B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766913
   DeRose T. D., 1990, Computer-Aided Geometric Design, V7, P165, DOI 10.1016/0167-8396(90)90028-P
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Finch M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024200
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   Hjelmervik J. M., 2015, EUROGRAPHICS SHORT P, P65
   Inkscape Wiki, 2018, MESH GRAD
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Li XY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461917
   Liao ZC, 2012, IEEE T VIS COMPUT GR, V18, P1858, DOI 10.1109/TVCG.2012.76
   Lieng H, 2017, COMPUT GRAPH FORUM, V36, P112, DOI 10.1111/cgf.12862
   Lieng H, 2015, COMPUT GRAPH FORUM, V34, P228, DOI 10.1111/cgf.12532
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   PDF Association, 2017, 320002 ISO
   Price B, 2006, VISUAL COMPUT, V22, P661, DOI 10.1007/s00371-006-0051-1
   Reinhard E., 2008, Color Imaging: Fundamentals and Applications
   Richardt C, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12408
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Sun X, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185570
   W3C, 2018, SCAL VECT GRAPH SVG, V2
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
   Xiao Y, 2015, COMPUT GRAPH FORUM, V34, P123, DOI 10.1111/cgf.12524
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Zhou HL, 2014, IEEE T IMAGE PROCESS, V23, P3268, DOI 10.1109/TIP.2014.2327807
NR 31
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 949
EP 960
DI 10.1007/s00371-018-1547-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400018
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Barut, O
   Haciomeroglu, M
   Sezer, EA
AF Barut, Oner
   Haciomeroglu, Murat
   Sezer, Ebru Akcapinar
TI Perceptual evaluation of maneuvering motion illusion for virtual
   pedestrians
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Perception; Motion illusion; Crowd simulation; Ambient crowd
ID CROWD; HUMANS
AB Crowd simulations span a wide spectrum of application domains, most notably video games, evacuation scenarios, and the movie industry. However, it is not obligatory that all virtual populace applications have the primary objective of realistic simulation. In most instances, it is necessary and sufficient that viewers perceive the crowd as plausible. Even for a crowd consisting of agents navigating on linear trajectories without any maneuvers, visual motion illusion elicited by these trajectories might appear to be a natural consequence, causing them to be perceived as wriggling rather than straight. In this respect, we evaluate in this study whether simulated 3D human agents walking with constant, collision-free velocities, induce such a maneuvering motion illusion, aiming toward an efficient real-time crowd simulation. For this purpose, we recorded videos of virtual human crowds with different parameter combinations, such as the agent walking speed, crowd density, camera tilt angle, and camera distance. These videos were watched by human subjects who were instructed to mark the virtual agents who they thought had changed their gait directions. The analyzed results revealed that participants claimed the presence of maneuvering virtual agents in the videos, even though there were none in any of them. Spatial grouping of the markings highlighted that the participants mainly focused on the central area of the simulation environment, and spatiotemporal analysis of the click data also showed stronger evidence to such an illusion (see accompanying video). Furthermore, we found that all of the referred parameters have statistically significant main effects on the number of marked agents per watched video.
C1 [Barut, Oner; Sezer, Ebru Akcapinar] Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   [Haciomeroglu, Murat] Gazi Univ, Dept Comp Engn, Ankara, Turkey.
C3 Hacettepe University; Gazi University
RP Barut, O (corresponding author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
EM barutoner@gmail.com; murath@gazi.edu.tr; ebru@hacettepe.edu.tr
RI Sezer, Ebru Akcapinar/H-5566-2011; Barut, Oner/K-3319-2018
OI Barut, Oner/0000-0003-3442-1586
CR Ahn J, 2006, COMPUT ANIMAT VIRT W, V17, P155, DOI 10.1002/cav.119
   Azahar M.A.B.M., 2008, 3 INT C P CHAP SURV, P573, DOI [10.1007/978-3-540-69736-7_61, DOI 10.1007/978-3-540-69736-7_61]
   Barut O, 2015, VISUAL COMPUT, V31, P843, DOI 10.1007/s00371-015-1105-z
   Beacco A, 2016, COMPUT GRAPH FORUM, V35, P32, DOI 10.1111/cgf.12774
   Bohannon RW, 1997, AGE AGEING, V26, P15, DOI 10.1093/ageing/26.1.15
   Browning RC, 2006, J APPL PHYSIOL, V100, P390, DOI 10.1152/japplphysiol.00767.2005
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Dobbyn S., 2005, ACM SIGGRAPH 2005 S, P95, DOI DOI 10.1145/1053427.1053443
   Ennis C, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870078
   Hamill J, 2005, COMPUT GRAPH FORUM, V24, P623, DOI 10.1111/j.1467-8659.2005.00887.x
   Ijaz K., 2015, 17 UKSIMAMSS INT C M, P111, DOI [10.1109/UKSim.2015.46, DOI 10.5555/2867552.2868182]
   Izquierdo Torrent M., 2014, THESIS
   Jarabo A, 2012, COMPUT GRAPH FORUM, V31, P565, DOI 10.1111/j.1467-8659.2012.03057.x
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Kulpa R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024172
   Kuwahara M, 2012, J VISION, V12, DOI 10.1167/12.12.4
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   McDonnell R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531361
   McDonnell R, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P259
   McHugh JE, 2010, EXP BRAIN RES, V204, P361, DOI 10.1007/s00221-009-2037-5
   Peters C, 2009, IEEE COMPUT GRAPH, V29, P54, DOI 10.1109/MCG.2009.69
   Ryder G, 2005, COMPUT GRAPH FORUM, V24, P203, DOI 10.1111/j.1467-8659.2005.00844.x
   Thalmann D, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P1, DOI 10.1109/CW.2009.23
   Xu ML, 2014, J COMPUT SCI TECH-CH, V29, P799, DOI 10.1007/s11390-014-1469-y
   Yarbus A. L., 1967, Eye Movements and Vision
   Zhou SP, 2010, ACM T MODEL COMPUT S, V20, DOI 10.1145/1842722.1842725
NR 27
TC 2
Z9 2
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1119
EP 1128
DI 10.1007/s00371-018-1557-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400033
DA 2024-07-18
ER

PT J
AU Okabe, M
   Dobashi, Y
   Anjyo, K
AF Okabe, Makoto
   Dobashi, Yoshinori
   Anjyo, Ken
TI Animating pictures of water scenes using video retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Single image; Interactive design; Video database; Video
   analysis/synthesis; Fluid animation; Texture analysis/synthesis
ID FLUID ANIMATION
AB We present a system to quickly and easily create an animation of water scenes in a single image. Our method relies on a database of videos of water scenes and video retrieval technique. Given an input image, alpha masks specifying regions of interest, and sketches specifying flow directions, our system first retrieves appropriate video candidates from the database and create the candidate animations for each region of interest as the composite of the input image and the retrieved videos: this process spends less than one minute by taking advantage of parallel distributed processing. Our system then allows the user to interactively control the speed of the desired animation and select the appropriate animation. After selecting the animation for all the regions, the resulting animation is completed. Finally, the user optionally applies a texture synthesis algorithm to recover the appearance of the input image. We demonstrate that our system allows the user to create a variety of animations of water scenes.
C1 [Okabe, Makoto] Shizuoka Univ, Naka Ku, 5th Engn Bldg,Room 619,3-5-1 Johoku, Hamamatsu, Shizuoka 4328561, Japan.
   [Dobashi, Yoshinori] Hokkaido Univ, Grad Sch Informat Sci & Technol, Kita Ku, Kita 14,Nishi 9, Sapporo, Hokkaido 0600814, Japan.
   [Anjyo, Ken] OLM Digital Inc, Setagaya Ku, Dens Kono Bldg,Room 302,1-8-8 Wakabayashi, Tokyo 1540023, Japan.
C3 Shizuoka University; Hokkaido University
RP Okabe, M (corresponding author), Shizuoka Univ, Naka Ku, 5th Engn Bldg,Room 619,3-5-1 Johoku, Hamamatsu, Shizuoka 4328561, Japan.
EM m.o@acm.org; doba@ime.ist.hokudai.ac.jp; anjyo@acm.org
FU JSPS KAKENHI [JP15H05924, JP25730071]; Japan Science and Technology
   Agency, CREST; Institute of Mathematics for Industry, Kyushu University;
   UEI Research; Grants-in-Aid for Scientific Research [15H05924] Funding
   Source: KAKEN
FX We would like to thank the anonymous reviewers for their insightful and
   constructive comments. Many thanks also go to Ayumi Kimura for
   discussions and encouragements. This work was supported by JSPS KAKENHI
   Grant Numbers JP15H05924 and JP25730071. This work was supported by
   Japan Science and Technology Agency, CREST. This work was partially
   supported by the Joint Research Program (Short-term Collaborative
   Research) of the Institute of Mathematics for Industry, Kyushu
   University. Yoshinori Dobashi was partially supported by UEI Research.
CR Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Gui Y, 2012, J ZHEJIANG U-SCI C, V13, P510, DOI 10.1631/jzus.C1100342
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645
   Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Joshi N, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P251
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Lalonde JF, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618477
   Liao Z, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461950
   Lin ZC, 2007, IEEE T VIS COMPUT GR, V13, P562, DOI 10.1109/TVCG.2007.1005
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma CY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618456
   Okabe M, 2011, COMPUT GRAPH FORUM, V30, P1973, DOI 10.1111/j.1467-8659.2011.02062.x
   Okabe M, 2009, COMPUT GRAPH FORUM, V28, P677, DOI 10.1111/j.1467-8659.2009.01408.x
   Prashnani E, 2017, COMPUT GRAPH FORUM, V36, P303, DOI 10.1111/cgf.12940
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Ramanan D., 2007, Advances in Neural Information Processing Systems, V19, P1129
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419
   Sun M, 2009, PROC CVPR IEEE, P1247, DOI 10.1109/CVPRW.2009.5206723
   Wang YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P213, DOI 10.1109/ICCV.2003.1238343
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Werlberger M, 2010, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2010.5539945
   Xu XM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409070
NR 31
TC 13
Z9 13
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 347
EP 358
DI 10.1007/s00371-016-1337-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900004
DA 2024-07-18
ER

PT J
AU Michailidis, GT
   Pajarola, R
AF Michailidis, Georgios-Tsampikos
   Pajarola, Renato
TI Bayesian graph-cut optimization for wall surfaces reconstruction in
   indoor environments
SO VISUAL COMPUTER
LA English
DT Article
DE Scene reconstruction; Point cloud processing; LiDAR reconstruction; Wall
   surface reconstruction; Wall openings detection
AB In this paper, a new method capable to extract the wall openings (windows and doors) of interior scenes from point clouds under cluttered and occluded environments is presented. For each wall surface extracted by the polyhedral model of a room, our method constructs a cell complex representation, which is used for the wall object segmentation using a graph-cut method. We evaluate the results of the proposed approach on real-world 3D scans of indoor environments and demonstrate its validity.
C1 [Michailidis, Georgios-Tsampikos; Pajarola, Renato] Univ Zurich, Dept Informat, Binzmhlestr 14, CH-8050 Zurich, Switzerland.
C3 University of Zurich
RP Michailidis, GT (corresponding author), Univ Zurich, Dept Informat, Binzmhlestr 14, CH-8050 Zurich, Switzerland.
EM gtmichail@ifi.uzh.ch
OI Pajarola, Renato/0000-0002-6724-526X
CR Adan A., 2011, 2011 INT C 3D IM MOD, P275, DOI [DOI 10.1109/3DIMPVT.2011.42, 10.1109/3DIMPVT.2011.42]
   [Anonymous], 2014, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci
   [Anonymous], 2011, P NSF ENG RES INN C
   Boykov Y, 1998, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.1998.698673
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Budroni A, 2010, INT ARCH PHOTOGRAMM, V38, P115
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Diaz-Vilarino L., 2014, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, V40, P203
   Dumitru RC, 2013, INT ARCH PHOTOGRAMM, V40-5-W1, P65
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Frome A., 2006, P ADV NEUR INF PROC, V19, P417
   Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867
   Hinneburg A., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P58
   Mura C, 2014, COMPUT GRAPH-UK, V44, P20, DOI 10.1016/j.cag.2014.07.005
   Oesau S, 2014, ISPRS J PHOTOGRAMM, V90, P68, DOI 10.1016/j.isprsjprs.2014.02.004
   Okorn B, 2010, P S 3D DAT PROC VIS, V2, P17
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Tang PB, 2010, AUTOMAT CONSTR, V19, P829, DOI 10.1016/j.autcon.2010.06.007
   Vicente S., 2008, PROC IEEE C COMPUT V, P1, DOI DOI 10.1109/CVPR.2008.4587440
   Volk R, 2014, AUTOMAT CONSTR, V38, P109, DOI 10.1016/j.autcon.2013.10.023
   Xiong XH, 2013, AUTOMAT CONSTR, V31, P325, DOI 10.1016/j.autcon.2012.10.006
   Yusuf A., 2007, AUTOMAT CONSTR, V16, P816
   Zhang R, 2014, IEEE WINT CONF APPL, P107, DOI 10.1109/WACV.2014.6836112
NR 24
TC 30
Z9 33
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1347
EP 1355
DI 10.1007/s00371-016-1230-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000011
OA Green Published
DA 2024-07-18
ER

PT J
AU Lalos, AS
   Vasilakis, AA
   Dimas, A
   Moustakas, K
AF Lalos, Aris S.
   Vasilakis, Andreas A.
   Dimas, Anastasios
   Moustakas, Konstantinos
TI Adaptive compression of animated meshes by exploiting orthogonal
   iterations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE 3D animated meshes; Orthogonal iterations; Online compression
ID DYNAMIC MESHES; SEGMENTATION; SEQUENCES; ALGORITHM; MATRICES; TRACKING
AB We introduce a novel approach to support fast and efficient lossy compression of arbitrary animation sequences ideally suited for real-time scenarios, such as streaming and content creation applications, where input is not known a priori and is dynamically generated. The presented method exploits temporal coherence by altering the principal component analysis (PCA) procedure from a batch- to an adaptive-basis aiming to simultaneously support three important objectives: fast compression times, reduced memory requirements and high-quality reproduction results. A dynamic compression pipeline is presented that can efficiently approximate the k-largest PCA bases based on the previous iteration (frame block) at a significantly lower complexity than directly computing the singular value decomposition. To avoid errors when a fixed number of basis vectors are used for all frame blocks, a flexible solution that automatically identifies the optimal subspace size for each one is also offered. An extensive experimental study is finally offered, showing that the proposed methods are superior in terms of performance as compared to several direct PCA-based schemes while, at the same time, achieves plausible reconstruction output despite the constraints posed by arbitrarily complex animated scenarios.
C1 [Lalos, Aris S.; Dimas, Anastasios; Moustakas, Konstantinos] Univ Patras, Elect & Comp Engn Dept, Patras, Greece.
   [Vasilakis, Andreas A.] Ctr Res & Technol Hellas, Informat Technol Inst, Athens, Greece.
C3 University of Patras; Centre for Research & Technology Hellas
RP Lalos, AS (corresponding author), Univ Patras, Elect & Comp Engn Dept, Patras, Greece.
EM aris.lalos@ece.upatras.gr
RI Dimas, Anastasios/AAM-2106-2021; Lalos, Aris/W-6443-2019
OI Dimas, Anastasios/0000-0003-2325-8525; Lalos, Aris/0000-0003-0511-9302;
   Vasilakis, Andreas A./0000-0001-6895-3324; Moustakas,
   Konstantinos/0000-0001-7617-227X
FU H2020-PHC RIA project MyAirCoach [643607]; H2020 Societal Challenges
   Programme [643607] Funding Source: H2020 Societal Challenges Programme
FX This work has been supported by the H2020-PHC-2014 RIA project
   MyAirCoach (Grant No. 643607).
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Amjoun R, 2007, JOURNAL WSCG, V15, P99
   [Anonymous], 1996, J HOPKINS STUDIES MA
   [Anonymous], 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'02
   [Anonymous], 3DTV C
   Cheng ZQ, 2007, VISUAL COMPUT, V23, P651, DOI 10.1007/s00371-007-0128-5
   COMON P, 1990, P IEEE, V78, P1327, DOI 10.1109/5.58320
   Hua Y, 2004, IEEE SIGNAL PROC MAG, V21, P56, DOI 10.1109/MSP.2004.1311143
   Ibarria L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P126
   Jacobson A., 2014, ACM SIGGRAPH 2014 CO, P24
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Lalas A, 2016, IEEE INT C BIOINFORM, P606, DOI 10.1109/BIBM.2016.7822588
   Lalos AS, 2017, IEEE T MULTIMEDIA, V19, P41, DOI 10.1109/TMM.2016.2605927
   Lee DY, 2014, VISUAL COMPUT, V30, P1077, DOI 10.1007/s00371-013-0779-3
   Luo GL, 2013, COMPUT ANIMAT VIRT W, V24, P365, DOI 10.1002/cav.1522
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Mamou K, 2006, COMPUT ANIMAT VIRT W, V17, P337, DOI 10.1002/cav.137
   Mamou K, 2009, COMPUT ANIMAT VIRT W, V20, P343, DOI 10.1002/cav.319
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Rus J, 2010, LECT NOTES COMPUT SC, V6169, P55, DOI 10.1007/978-3-642-14061-7_6
   Saad Y, 2016, SIAM J MATRIX ANAL A, V37, P103, DOI 10.1137/141002037
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Strobach P, 1997, SIGNAL PROCESS, V59, P73, DOI 10.1016/S0165-1684(97)00039-X
   Tian J, 2012, VISUAL COMPUT, V28, P819, DOI 10.1007/s00371-012-0700-5
   Vása L, 2014, COMPUT GRAPH FORUM, V33, P145, DOI 10.1111/cgf.12304
   Vása L, 2009, COMPUT GRAPH FORUM, V28, P1529, DOI 10.1111/j.1467-8659.2008.01304.x
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Vása L, 2010, COMPUT GRAPH FORUM, V29, P1921, DOI 10.1111/j.1467-8659.2010.01659.x
   Vasilakis A.A., 2016, P 33 COMP GRAPH INT, P53, DOI [10.1145/2949035.2949049, DOI 10.1145/2949035.2949049]
   Vasilakis AA, 2014, COMPUT GRAPH FORUM, V33, P293, DOI 10.1111/cgf.12327
   XU GH, 1994, SIAM J MATRIX ANAL A, V15, P974, DOI 10.1137/S0895479890183848
NR 33
TC 8
Z9 9
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 811
EP 821
DI 10.1007/s00371-017-1395-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800013
DA 2024-07-18
ER

PT J
AU Lu, SF
   Huang, Y
   Jin, XG
   Jaffer, A
   Kaplan, CS
   Mao, XY
AF Lu, Shufang
   Huang, Yue
   Jin, Xiaogang
   Jaffer, Aubrey
   Kaplan, Craig S.
   Mao, Xiaoyang
TI Marbling-based creative modelling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Computational geometry; Shape modelling; Volume-preserving vector
   fields; Marbling art
ID DEFORMATION; SURFACES; SYSTEM
AB Most mathematical marbling simulations generate patterns for texture mapping and surface decoration. We explore the application of three-dimensional deformations inspired by mathematical marbling as a suite of tools to enable creative shape design. Our tools are expressed as analytical functions of space and are volume-preserving vector fields, meaning that the modelling process preserves volumes and avoids self-intersections. Complicated deformations are easily combined to create complex objects from simple ones. To achieve smooth and high-quality shapes, we also present a mesh refinement and simplification algorithm adapted to our deformations. We show a number of examples of shapes created with our technique in order to demonstrate its power and expressiveness.
C1 [Lu, Shufang] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Huang, Yue; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Jaffer, Aubrey] Digilant, Boston, MA USA.
   [Kaplan, Craig S.] Univ Waterloo, Comp Sci, Waterloo, ON, Canada.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp & Media Engn, Kofu, Yamanashi, Japan.
C3 Zhejiang University of Technology; Zhejiang University; University of
   Waterloo; University of Yamanashi
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM jin@cad.zju.edu.cn
OI mao, xiaoyang/0000-0001-9531-3197; Kaplan, Craig/0000-0002-3168-6805
FU National Natural Science Foundation of China [61402410, 61472351]; China
   Scholarship Council
FX Shufang Lu was supported by the National Natural Science Foundation of
   China (Grant No. 61402410) and the China Scholarship Council. Xiaogang
   Jin was supported by the National Natural Science Foundation of China
   (Grant No. 61472351).
CR Acar R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289606
   Akgun BT, 2004, LEONARDO, V37, P49, DOI 10.1162/002409404772828120
   Alexis AA, 2006, GRAPH MODELS, V68, P2, DOI 10.1016/j.gmod.2005.08.002
   Ando R, 2011, COMPUT GRAPH-UK, V35, P148, DOI 10.1016/j.cag.2010.11.002
   [Anonymous], 2014, COMPUTER GRAPHICS PR
   [Anonymous], ULTIMATE MARBLING HD
   Barr A. H., 1984, Computers & Graphics, V18, P21
   BLANC C, 1995, GRAPHICS GEMS, V5, P249
   Brochu T, 2009, SIAM J SCI COMPUT, V31, P2472, DOI 10.1137/080737617
   Cohen-Or D, 2016, VISUAL COMPUT, V32, P7, DOI 10.1007/s00371-015-1193-9
   Cui YM, 2015, COMPUT AIDED GEOM D, V35-36, P69, DOI 10.1016/j.cagd.2015.03.002
   Gain J, 2005, IEEE T VIS COMPUT GR, V11, P217, DOI 10.1109/TVCG.2005.36
   Gain J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409629
   Guo XK, 2014, GRAPH MODELS, V76, P376, DOI 10.1016/j.gmod.2014.03.019
   HSU WM, 1992, COMP GRAPH, V26, P177, DOI 10.1145/142920.134036
   Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694
   Jin XG, 2007, IEEE COMPUT GRAPH, V27, P78, DOI 10.1109/MCG.2007.28
   Kil YJ, 2006, COMPUT GRAPH-UK, V30, P610, DOI 10.1016/j.cag.2006.03.014
   Lu SF, 2017, IEEE COMPUT GRAPH, V37, P90, DOI 10.1109/MCG.2016.42
   Lu SF, 2012, IEEE COMPUT GRAPH, V32, P26, DOI 10.1109/MCG.2011.51
   Nealen A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276429
   PENTLAND A., 1989, COMPUT GRAPHICS-US, V23, P207
   Schmitt Benjamin., 2003, GRAPHITE P 1 INT C C, P127
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sieger D., 2015, NEW CHALLENGES GRID, P281, DOI DOI 10.1007/978-3-319-06053-8_14
   Singh K., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P405, DOI 10.1145/280814.280946
   von Funck W, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P291, DOI 10.1109/PG.2007.26
   von Funck W, 2006, ACM T GRAPHIC, V25, P1118, DOI 10.1145/1141911.1142002
   Xie XH, 2013, COMPUT GRAPH FORUM, V32, P233, DOI 10.1111/cgf.12200
   Xu JY, 2008, IEEE COMPUT GRAPH, V28, P35, DOI 10.1109/MCG.2008.36
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185553
   Zhao HL, 2009, MULTIMED TOOLS APPL, V44, P187, DOI 10.1007/s11042-009-0290-z
NR 32
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 913
EP 923
DI 10.1007/s00371-017-1396-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800022
DA 2024-07-18
ER

PT J
AU Andersen, D
   Popescu, V
   Cabrera, ME
   Shanghavi, A
   Gomez, G
   Marley, S
   Mullis, B
   Wachs, J
AF Andersen, Daniel
   Popescu, Voicu
   Cabrera, Maria Eugenia
   Shanghavi, Aditya
   Gomez, Gerardo
   Marley, Sherri
   Mullis, Brian
   Wachs, Juan
TI Virtual annotations of the surgical field through an augmented reality
   transparent display
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented reality; Telementoring; Telemedicine; Annotation anchoring
ID EXPERIENCE
AB Existing telestrator-based surgical telementoring systems require a trainee surgeon to shift focus frequently between the operating field and a nearby monitor to acquire and apply instructions from a remote mentor. We present a novel approach to surgical telementoring where annotations are superimposed directly onto the surgical field using an augmented reality (AR) simulated transparent display. We present our first steps towards realizing this vision, using two networked conventional tablets to allow a mentor to remotely annotate the operating field as seen by a trainee. Annotations are anchored to the surgical field as the trainee tablet moves and as the surgical field deforms or becomes occluded. The system is built exclusively from compact commodity-level components-all imaging and processing are performed on the two tablets.
C1 [Andersen, Daniel; Popescu, Voicu; Cabrera, Maria Eugenia; Shanghavi, Aditya; Wachs, Juan] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Gomez, Gerardo; Marley, Sherri; Mullis, Brian] Indiana Univ, Sch Med, Indianapolis, IN USA.
C3 Purdue University System; Purdue University; Indiana University System;
   Indiana University Indianapolis
RP Andersen, D (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM andersed@purdue.edu
RI MULLIS, BRIAN/Q-3123-2019; Paredes, Gerardo Gómez/W-4070-2017;
   Shanghavi, Aditya/F-5056-2019
OI Shanghavi, Aditya/0000-0001-8628-2026; Andersen,
   Daniel/0000-0002-5640-1845
FU Office of the Assistant Secretary of Defense for Health Affairs
   [W81XWH-14-1-0042]
FX This work was supported by the Office of the Assistant Secretary of
   Defense for Health Affairs under Award No. W81XWH-14-1-0042. Opinions,
   interpretations, conclusions and recommendations are those of the author
   and are not necessarily endorsed by the Department of Defense.
CR Agarwal R, 2007, UROLOGY, V70, DOI 10.1016/j.urology.2007.09.053
   Ballantyne GH, 2002, SURG ENDOSC, V16, P1389, DOI 10.1007/s00464-001-8283-7
   Baricevic D., 2014, Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology, P87
   Bashshur R L, 1995, Telemed J, V1, P19, DOI 10.1089/tmj.1.1995.1.19
   Bogen EM, 2014, WORLD J GASTRO ENDOS, V6, P148, DOI 10.4253/wjge.v6.i5.148
   Chou WS, 2004, IEEE SYS MAN CYBERN, P2901
   Ereso AQ, 2010, J AM COLL SURGEONS, V211, P400, DOI 10.1016/j.jamcollsurg.2010.05.014
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Guo Yanbo, 2014, Stud Health Technol Inform, V196, P147
   Loescher T, 2014, IEEE SYS MAN CYBERN, P2341, DOI 10.1109/SMC.2014.6974276
   Marescaux J., 2014, Robotics in General Surgery, P479, DOI 10.1007/978-1-4614-8739-5_37
   Marescaux Jacques, 2003, Curr Urol Rep, V4, P109, DOI 10.1007/s11934-003-0036-9
   Occipital I., 2014, OCCIPITAL
   Ponce Brent A, 2014, J Bone Joint Surg Am, V96, pe84, DOI 10.2106/JBJS.M.00928
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Satava RM, 1996, SURG ENDOSC, V10, P173, DOI 10.1007/BF00188366
   Schulam PG, 1997, SURG ENDOSC, V11, P1001, DOI 10.1007/s004649900511
   Shenai MB, 2011, NEUROSURGERY, V68, DOI 10.1227/NEU.0b013e3182077efd
   Smurro J.P., 2014, US Patent App., Patent No. [14/138,045, 14138045]
   Tomioka M, 2013, INT SYM MIX AUGMENT, P21, DOI 10.1109/ISMAR.2013.6671760
   Treter S, 2013, ANN SURG ONCOL, V20, P2754, DOI 10.1245/s10434-013-2894-9
   Unuma Yuko., 2014, Proceedings of the companion publication of the 19th international conference on Intelligent User Interfaces, P17
   Vera AM, 2014, SURG ENDOSC, V28, P3467, DOI 10.1007/s00464-014-3625-4
NR 24
TC 21
Z9 40
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1481
EP 1498
DI 10.1007/s00371-015-1135-6
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000010
DA 2024-07-18
ER

PT J
AU Zank, M
   Kunz, A
AF Zank, Markus
   Kunz, Andreas
TI Where are you going? Using human locomotion models for target estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Redirection; Human locomotion; Prediction
ID MOTION COMPRESSION; WALKING; REAL
AB To explore virtual environments that are larger than the available physical tracking space by real walking, it is necessary to use so-called redirected walking. Redirection techniques allowthe user to explore an unlimited virtual environment in a limited tracking space by introducing a small mismatch between a user's real and virtual movement, thus preventing the user from colliding with the physical walls of the tracking space. Steering algorithms are used to select the most suitable redirection technique at any given time, depending on the geometry of the real and virtual environment. Together with prediction of a user's future walking path, these algorithms select the best redirection strategy by an optimal control scheme. In this paper, a new approach for the prediction of a person's locomotion target is presented. We use various models of human locomotion together with a set of possible targets to create a set of expected paths. These paths are then compared to the real path the user already traveled to calculate the probability of a certain target being the one the user is heading for. A new approach for comparing paths with each other is introduced and is compared to three others. For describing the human's path to a given target, four different models are used and compared. To gather data for the comparison of the models against the real path, a user study was conducted. Based on the results of the user study, the paper concludes with a discussion on the prediction performance of the different approaches.
C1 [Zank, Markus; Kunz, Andreas] Swiss Fed Inst Technol, IWF, Innovat Ctr Virtual Real, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Zank, M (corresponding author), Swiss Fed Inst Technol, IWF, Innovat Ctr Virtual Real, Zurich, Switzerland.
EM zank@iwf.mavt.ethz.ch; kunz@iwf.mavt.ethz.ch
RI Kunz, Andreas/B-9241-2008
OI Kunz, Andreas/0000-0002-6495-4327; Zank, Markus/0009-0008-5446-8289
FU Swiss National Science Foundation [205121 153243]; Swiss National
   Science Foundation (SNF) [205121_153243] Funding Source: Swiss National
   Science Foundation (SNF)
FX The authors would like to thank the Swiss National Science Foundation
   (Project Number 205121 153243) for funding this work.
CR [Anonymous], S 3D US INT
   [Anonymous], 2008, P IEEE WORKSH ADV RO
   [Anonymous], S 3D US INT
   [Anonymous], 2004, P INT C ROB AUT
   [Anonymous], 2007, IEEE S 3D US INT 200
   [Anonymous], 2001, P EUROGRAPHICS
   [Anonymous], 1971, ICA
   Arechavaleta G, 2008, IEEE T ROBOT, V24, P5, DOI 10.1109/TRO.2008.915449
   Arechavaleta G, 2006, P IEEE RAS-EMBS INT, P49
   Cirio G, 2013, IEEE T VIS COMPUT GR, V19, P671, DOI 10.1109/TVCG.2013.34
   Fink PW, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1227134.1227136
   Hicheur H, 2007, EUR J NEUROSCI, V26, P2376, DOI 10.1111/j.1460-9568.2007.05836.x
   Larrue F, 2014, J COGN PSYCHOL, V26, P906, DOI 10.1080/20445911.2014.965714
   Mombaur K, 2010, AUTON ROBOT, V28, P369, DOI 10.1007/s10514-009-9170-7
   Nescher T, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P15, DOI 10.1109/CW.2012.10
   Nitzsche N, 2004, PRESENCE-TELEOP VIRT, V13, P44, DOI 10.1162/105474604774048225
   Ruddle RA, 2011, ACM T COMPUT-HUM INT, V18, DOI 10.1145/1970378.1970384
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Steinicke F, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P217, DOI 10.1109/CW.2008.53
   Su JB, 2007, PRESENCE-VIRTUAL AUG, V16, P385, DOI 10.1162/pres.16.4.385
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Zank M, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P229, DOI 10.1109/CW.2015.20
   Zmuda MA, 2013, IEEE T VIS COMPUT GR, V19, P1872, DOI 10.1109/TVCG.2013.88
NR 23
TC 10
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1323
EP 1335
DI 10.1007/s00371-016-1229-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800010
OA Green Published
DA 2024-07-18
ER

PT J
AU Bao, YT
   Qi, Y
AF Bao, Yongtang
   Qi, Yue
TI Realistic hair modeling from a hybrid orientation field
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Image-based hair modeling; Hybrid orientation field; Fractional
   anisotropy; Preserving structural details; Tracing hair strand
AB Image-based hair modeling methods enable artists to produce abundant 3D hair models. However, the reconstructed hair models could not preserve the structural details, such as uniformly distributed hair roots, interior strands growing in line with real distribution and exterior strands similar to images. In this paper, we propose a novel approach to construct a realistic 3D hair model from a hybrid orientation field. Our hybrid orientation field is generated from four fields. The first field makes the surface structure of a hairstyle be similar to the input images as much as possible. The second field makes the hair roots and interior hair strands be consistent with actual distribution. The tracing hair strands can be confined to the hair volume according to the third field. And the fourth field makes the growing direction of one point at a strand be compatible with its predecessor. To generate these fields, we construct high-confidence 3D strand segments from the orientation field of point cloud and 2D traced strands. Hair strands automatically grow from uniformly distributed hair roots according to the hybrid orientation field. We use energy minimization strategy to optimize the entire 3D hair model. We demonstrate that our approach can preserve structural details of 3D hair models.
C1 [Bao, Yongtang; Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Qi, Y (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM qy@buaa.edu.cn
RI qi, yue/KLE-0386-2024
CR [Anonymous], ACM T GRAPH
   [Anonymous], 2014, P 18 M ACM SIGGRAPH
   Aras R, 2008, VISUAL COMPUT, V24, P577, DOI 10.1007/s00371-008-0238-8
   Beeler T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185613
   Bertails F, 2006, ACM T GRAPHIC, V25, P1180, DOI 10.1145/1141911.1142012
   Bonneel N, 2009, COMPUT GRAPH FORUM, V28, P1171, DOI 10.1111/j.1467-8659.2009.01494.x
   Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112
   Chai ML, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461990
   Chai ML, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185612
   Coulon O, 2004, MED IMAGE ANAL, V8, P47, DOI 10.1016/j.media.2003.06.002
   Echevarria JI, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601133
   Fu HB, 2007, SKETCH-BASED INTERFACES AND MODELING 2007, P31
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Herrera TL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366165
   Hu LW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766931
   Jakob W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618510
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Luo LJ, 2013, PROC CVPR IEEE, P265, DOI 10.1109/CVPR.2013.41
   Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paris S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360629
   Selle A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360663
   Wang LD, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531362
   Wang WJ, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-015-5360-4
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Wei YC, 2005, ACM T GRAPHIC, V24, P816, DOI 10.1145/1073204.1073267
   Xu ZX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661284
   Yamaguchi T, 2009, LECT NOTES COMPUT SC, V5414, P585, DOI 10.1007/978-3-540-92957-4_51
   Yan LQ, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818080
   YORUK E, 2005, P EUR SIGN PROC C EU
   Zinke A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360631
NR 32
TC 6
Z9 7
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 729
EP 738
DI 10.1007/s00371-016-1240-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600006
DA 2024-07-18
ER

PT J
AU Mukherjee, R
   Debattista, K
   Bashford-Rogers, T
   Waterfield, B
   Chalmers, A
AF Mukherjee, Ratnajit
   Debattista, Kurt
   Bashford-Rogers, Thomas
   Waterfield, Brian
   Chalmers, Alan
TI A study on user preference of high dynamic range over low dynamic range
   video
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE HDR video; Subjective experiment; Ranking; Rating
ID TONE MAPPING OPERATORS; EXPOSURE
AB The increased interest in high dynamic range (HDR) video over existing low dynamic range (LDR) video during the past decade or so was primarily due to its inherent capability to capture, store and display the full range of real-world lighting visible to the human eye with increased precision. This has led to an inherent assumption that HDR video would be preferable by the end-user over LDR video due to the more immersive and realistic visual experience provided by HDR. This assumption has led to a considerable body of research into efficient capture, processing, storage and display of HDR video. Although this is beneficial for scientific research and industrial purposes, very little research has been conducted to test the veracity of this assumption. In this paper, we conduct two subjective studies by means of a ranking and a rating-based experiment where 60 participants in total, 30 in each experiment, were tasked to rank and rate several reference HDR video scenes along with three mapped LDR versions of each scene on an HDR display, in order of their viewing preference. Results suggest that given the option, end-users prefer the HDR representation of the scene over its LDR counterpart.
C1 [Mukherjee, Ratnajit; Debattista, Kurt; Bashford-Rogers, Thomas; Chalmers, Alan] Univ Warwick, WMG, Coventry, W Midlands, England.
   [Waterfield, Brian] Jaguar Land Rover Automot PLC, Coventry, W Midlands, England.
C3 University of Warwick; Jaguar Land Rover
RP Mukherjee, R (corresponding author), Univ Warwick, WMG, Coventry, W Midlands, England.
EM ratnajitmukherjee@gmail.com
FU EPSRC [EP/I006192/1, EP/K014056/1] Funding Source: UKRI
CR Akyuz A. O., 2007, SIGGRAPH 07 ACM SIGG, DOI [10.1145/1275808.1276425, DOI 10.1145/1275808.1276425]
   [Anonymous], 2013, Colour Appearance Models
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Debattista K, 2015, VISUAL COMPUT, V31, P1089, DOI 10.1007/s00371-015-1121-z
   Drago Frederic., 2003, Proceedings of ACM SIGGRAPH 2003 Sketches Applications, P1
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Ebner F, 1998, SIXTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P8
   Eilertsen G, 2013, COMPUT GRAPH FORUM, V32, P275, DOI 10.1111/cgf.12235
   FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868
   Hirakawa K, 2010, IEEE IMAGE PROC, P3137, DOI 10.1109/ICIP.2010.5654059
   Kuang JT, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1265957.1265958
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Mantiuk R., 2008, SIGGRAPH'08 Int. Conf. Comput. Graph. Interact. Tech. ACM SIGGRAPH 2008 Pap, DOI DOI 10.1145/1399504.1360667
   Melo M, 2015, COMPUT GRAPH FORUM, V34, P38, DOI 10.1111/cgf.12606
   Moroney N., 2002, P COL IM C, P23
   Narwaria M, 2014, EUR SIGNAL PR CONF, P2140
   Schlick C., 1995, Photorealistic Rendering Techniques, P7
   Urbano C, 2010, COMPUT GRAPH FORUM, V29, P2469, DOI 10.1111/J.1467-8659.2010.01758.x
NR 21
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 825
EP 834
DI 10.1007/s00371-016-1239-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600015
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Wang, Z
   Yoon, S
   Xie, SJ
   Lu, Y
   Park, DS
AF Wang, Zhihui
   Yoon, Sook
   Xie, Shan Juan
   Lu, Yu
   Park, Dong Sun
TI Visual tracking with semi-supervised online weighted multiple instance
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Multiple instance learning; Semi-supervised learning; Weak classifier;
   Unlabeled sample; Inconsistency function
ID OBJECT TRACKING; SELECTION
AB Adaptive discriminative tracking is a new research topic that has attracted broad attention due to its extensive application value. To take full advantage of the information about targets and their surrounding background, we propose a novel single object tracking-by-detection tracker in this paper, combining semi-supervised learning, multiple instance learning and the Bayesian theorem. The tracker uses a block-based inconsistency function of the labeled and unlabeled training samples in the selection of optimal weak classifiers during the parameter updating phase of each frame. Experimental results showed that the proposed tracker has excellent performance over other eight state-of-the-art trackers for thirteen open-access video sequences.
C1 [Wang, Zhihui; Lu, Yu] Chonbuk Natl Univ, Dept Elect Engn, Jeonju 561756, South Korea.
   [Yoon, Sook] Mokpo Natl Univ, Dept Multimedia, Jeonnam, South Korea.
   [Xie, Shan Juan] Hangzhou Normal Univ, Inst Remote Sensing & Earth Sci, Hangzhou, Zhejiang, Peoples R China.
   [Park, Dong Sun] Chonbuk Natl Univ, Div Elect Engn, Jeonju 561756, South Korea.
C3 Jeonbuk National University; Mokpo National University; Hangzhou Normal
   University; Jeonbuk National University
RP Park, DS (corresponding author), Chonbuk Natl Univ, Div Elect Engn, Jeonju 561756, South Korea.
EM zhihuiwangjl@gmail.com; syoon@mokpo.ac.kr; shanj_x@hotmail.com;
   luyu0311@gmail.com; dspark@jbnu.ac.kr
RI xie, shan/HIZ-6686-2022
OI Lu, Yu/0000-0002-3685-6228
CR [Anonymous], VIS COMPUT
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2008, COMPUT SCI
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Babu RV, 2010, COMPUT VIS IMAGE UND, V114, P297, DOI 10.1016/j.cviu.2009.10.004
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Okuma K, 2004, LECT NOTES COMPUT SC, V3021, P28, DOI 10.1007/978-3-540-24670-1_3
   Quan W, 2014, VISUAL COMPUT, V30, P351, DOI 10.1007/s00371-013-0860-y
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Tian M, 2007, LECT NOTES COMPUT SC, V4843, P355
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P4454, DOI 10.1109/TIP.2012.2205700
   Wu HF, 2014, VISUAL COMPUT, V30, P229, DOI 10.1007/s00371-013-0823-3
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie Y, 2012, PATTERN RECOGN LETT, V33, P1075, DOI 10.1016/j.patrec.2012.01.020
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zhang SP, 2013, NEUROCOMPUTING, V100, P31, DOI 10.1016/j.neucom.2011.11.031
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zheng-Biao LI, 2014, Matéria (Rio J.), V19, P1
NR 32
TC 13
Z9 13
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 307
EP 320
DI 10.1007/s00371-015-1067-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FK
UT WOS:000371666200004
DA 2024-07-18
ER

PT J
AU Rao, AS
   Gubbi, J
   Marusic, S
   Palaniswami, M
AF Rao, Aravinda S.
   Gubbi, Jayavardhana
   Marusic, Slaven
   Palaniswami, Marimuthu
TI Estimation of crowd density by clustering motion cues
SO VISUAL COMPUTER
LA English
DT Article
DE Video surveillance; Crowd; Density estimation; People counting; Crowd
   monitoring; Optical flow; Clustering
ID OPTICAL-FLOW; DISTORTION CORRECTION; MULTIPLE HUMANS; TRACKING;
   SEGMENTATION; SURVEILLANCE; COMPUTATION; FRAMEWORK; PEOPLE; MODEL
AB Understanding crowd behavior using automated video analytics is a relevant research problem in recent times due to complex challenges in monitoring large gatherings. From an automated video surveillance perspective, estimation of crowd density in particular regions of the video scene is an indispensable tool in understanding crowd behavior. Crowd density estimation provides the measure of number of people in a given region at a specified time. While most of the existing computer vision methods use supervised training to arrive at density estimates, we propose an approach to estimate crowd density using motion cues and hierarchical clustering. The proposed method incorporates optical flow for motion estimation, contour analysis for crowd silhouette detection, and clustering to derive the crowd density. The proposed approach has been tested on a dataset collected at the Melbourne Cricket Ground (MCG) and two publicly available crowd datasets-Performance Evaluation of Tracking and Surveillance (PETS) 2009 and University of California, San Diego (UCSD) Pedestrian Traffic Database-with different crowd densities (medium- to high-density crowds) and in varied environmental conditions (in the presence of partial occlusions). We show that the proposed approach results in accurate estimates of crowd density. While the maximum mean error of was received for MCG and PETS datasets, it was for UCSD dataset. The proposed approach delivered superior performance in of the cases on PETS dataset when compared with existing methods.
C1 [Rao, Aravinda S.; Gubbi, Jayavardhana; Marusic, Slaven; Palaniswami, Marimuthu] Univ Melbourne, Dept Elect & Elect Engn, ISSNIP, Parkville, Vic 3010, Australia.
C3 University of Melbourne
RP Rao, AS (corresponding author), Univ Melbourne, Dept Elect & Elect Engn, ISSNIP, Parkville, Vic 3010, Australia.
EM aravinda@student.unimelb.edu.au
RI Gubbi, Jayavardhana/ABF-9315-2020; Rao, Aravinda/I-6941-2016;
   Palaniswami, Marimuthu/AAE-2179-2022
OI Rao, Aravinda/0000-0003-2319-6539; Palaniswami, Marimuthu
   Swami/0000-0002-3635-4252
FU Australian Research Council (ARC) [LP100200430]; Australian Research
   Council [LP100200430] Funding Source: Australian Research Council
FX This work is partially supported by the Australian Research Council
   (ARC) linkage project LP100200430, partnering the University of
   Melbourne, Melbourne Cricket Club and ARUP. Authors would like to thank
   representatives and staff of ARUP and MCG.
CR Acampora G, 2011, IEEE INT CONF FUZZY, P139
   Aggarwal J. K., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P2, DOI 10.1109/MNRAO.1994.346261
   Aijun S., 2009, 2009 INT S COMP NETW, P1
   Albiol A., 2009, 3International_Workshop_on Performance_Evaluation_of_Tracking_and_Surveillance, P31
   ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167
   [Anonymous], 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, DOI [10.1109/CVPR.2006.312, DOI 10.1109/CVPR.2006.312]
   [Anonymous], 2011, P INT C IM INF PROC
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], PERF EV TRACK SURV P
   [Anonymous], P IEEE C COMPUTER VI
   [Anonymous], 1995, P MUSTERERKENNUNG 19
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2008, CVPR
   [Anonymous], 2005, Computer Vision and Pattern Recognition-Workshops
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Batur AU, 2005, IEEE T IMAGE PROCESS, V14, P1707, DOI 10.1109/TIP.2005.854473
   Bhaskar H, 2013, NEUROCOMPUTING, V100, P58, DOI 10.1016/j.neucom.2011.12.039
   Chan A., 2008, PEDESTRIAN TRAFFIC D
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Clark A.J., 2008, J BRIT STUD, P1
   Conde C, 2013, NEUROCOMPUTING, V100, P19, DOI 10.1016/j.neucom.2011.12.037
   Conte D., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P225, DOI 10.1109/AVSS.2010.78
   Da Xu RY, 2009, IEEE IMAGE PROC, P2577, DOI 10.1109/ICIP.2009.5413999
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Nguyen DT, 2009, 2009 24TH INTERNATIONAL CONFERENCE IMAGE AND VISION COMPUTING NEW ZEALAND (IVCNZ 2009), P357, DOI 10.1109/IVCNZ.2009.5378380
   Ferryman, 2009, PETS 2009 BENCHMARK
   FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772
   Gao XB, 2010, IEEE T SYST MAN CY C, V40, P145, DOI 10.1109/TSMCC.2009.2035631
   Gavrila DM, 2007, IEEE T PATTERN ANAL, V29, P1408, DOI 10.1109/TPAMI.2007.1062
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Grimson WEL, 1998, PROC CVPR IEEE, P22, DOI 10.1109/CVPR.1998.698583
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Haritaoglu I, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P222, DOI 10.1109/AFGR.1998.670952
   Haritaoglu I, 2000, INT C PATT RECOG, P179, DOI 10.1109/ICPR.2000.902890
   Hartley R, 2007, IEEE T PATTERN ANAL, V29, P1309, DOI 10.1109/TPAMI.2007.1147
   HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455
   HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568
   Horn B.K.P., 1980, TECHNICAL REPORT
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hou YL, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS, VOLS 1-6, P464, DOI 10.1109/ICAL.2008.4636196
   Huazhong X., 2010, 2010 INT C COMP DES, V1
   Ishii Y, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P403, DOI 10.1109/AFGR.2004.1301566
   JAIN R, 1979, IEEE T PATTERN ANAL, V1, P206, DOI 10.1109/TPAMI.1979.4766907
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Kambhamettu C., 1994, HDB PATTERN RECOGNIT, P405
   Kilger M., 1992, Proceedings. IEEE Workshop on Applications of Computer Vision (Cat. No.92TH0446-5), P11, DOI 10.1109/ACV.1992.240332
   Leibo J.Z., 2011, Conference on Neural Information Processing Systems, P711
   Li M., 2008, 2008 19 INT C PATTER, P1, DOI DOI 10.1109/ICPR.2008.4761705
   Li M, 2009, IEEE IMAGE PROC, P2545, DOI 10.1109/ICIP.2009.5414008
   Lin Z, 2010, IEEE T PATTERN ANAL, V32, P604, DOI 10.1109/TPAMI.2009.204
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Ma L., 2004, International Journal of Information Acquisition, V1, P135
   Mao YB, 2010, 2010 8TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P6295, DOI 10.1109/WCICA.2010.5554367
   Merad D., 2010, AVSS IEEE, P151
   Milan A., 2011, PETS 2009 GROUND TRU
   NAGEL HH, 1987, ARTIF INTELL, V33, P299, DOI 10.1016/0004-3702(87)90041-5
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Oren M, 1997, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.1997.609319
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Park J, 2009, IEEE T CONSUM ELECTR, V55, P987, DOI 10.1109/TCE.2009.5278053
   Rahmalan H., 2006, Institution of Engineering and Technology Conference on Crime and Security, P540
   Rao AS, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P494, DOI 10.1109/ICACCI.2013.6637221
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P2423, DOI 10.1109/ICCV.2011.6126526
   Salti S, 2012, IEEE T IMAGE PROCESS, V21, P4334, DOI 10.1109/TIP.2012.2206035
   Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6
   Seki M, 2003, PROC CVPR IEEE, P65
   Jacques JCS, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.937394
   SINGH A, 1992, CVGIP-IMAG UNDERSTAN, V56, P152, DOI 10.1016/1049-9660(92)90037-4
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Song M, 2013, COMPUTING RES REPOSI
   Srivastava S., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P60, DOI 10.1109/AVSS.2011.6027295
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Subburaman VB, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P470, DOI 10.1109/AVSS.2012.87
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tosato D, 2010, IEEE IMAGE PROC, P3469, DOI 10.1109/ICIP.2010.5650076
   URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang L, 2012, IEEE T INTELL TRANSP, V13, P691, DOI 10.1109/TITS.2011.2179536
   Wang R., 2013, PRIMARY VISUAL CORTE
   Waxman A. M., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P717, DOI 10.1109/CVPR.1988.196313
   Wei-Lieh Hsu, 2011, 2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing, P348, DOI 10.1109/IIHMSP.2011.49
   Wenhua Ma, 2010, Proceedings of the 5th International Conference on Computer Sciences and Convergence Information Technology (ICCIT 2010), P170, DOI 10.1109/ICCIT.2010.5711051
   WILLICK D, 1991, CVGIP-IMAG UNDERSTAN, V54, P206, DOI 10.1016/1049-9660(91)90063-U
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Wren C, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P51, DOI 10.1109/AFGR.1996.557243
   Wu XY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-3, P214, DOI 10.1109/ROBIO.2006.340379
   Xiao JJ, 2006, LECT NOTES COMPUT SC, V3951, P211
   Yang SJ, 2012, THIRD INTERNATIONAL CONFERENCE ON INFORMATION SECURITY AND INTELLIGENT CONTROL (ISIC 2012), P198, DOI 10.1109/ISIC.2012.6449740
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yongjun Ma, 2010, Proceedings 2010 IEEE Youth Conference on Information, Computing and Telecommunications (YC-ICT 2010), P234, DOI 10.1109/YCICT.2010.5713088
   Yoon H, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4558, DOI 10.1109/IROS.2006.282159
   Yu-Ting Chen, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1177, DOI 10.1109/ICCVW.2009.5457475
   Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4
   Zhao T, 2004, IEEE T PATTERN ANAL, V26, P1208, DOI 10.1109/TPAMI.2004.73
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
   Zhao T, 2001, PROC CVPR IEEE, P194
   Zhao T, 2008, IEEE T PATTERN ANAL, V30, P1198, DOI 10.1109/TPAMI.2007.70770
NR 98
TC 26
Z9 30
U1 0
U2 48
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1533
EP 1552
DI 10.1007/s00371-014-1032-4
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600009
DA 2024-07-18
ER

PT J
AU Lee, J
   Kim, YJ
   Kim, MS
   Elber, G
AF Lee, Jaewook
   Kim, Yong-Joon
   Kim, Myung-Soo
   Elber, Gershon
TI Comparison of three bounding regions with cubic convergence to planar
   freeform curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Planar freeform curves; Monotone spiral curve; Bounding circular arcs;
   Dynamic BVH
ID COLLISION DETECTION; DISTANCE COMPUTATION; FAT BIARCS; ARCS;
   APPROXIMATION
AB We compare the relative performance of bounding regions generated by three different curve-bounding methods with cubic convergence to planar freeform curves: spiral fat arcs (SFA) (Barton and Elber in Graph Models 73(2): 50-57, 2011), bilens (Kumosenko in Comput Aided Geom Des 30(3): 310-330, 2013), and bounding circular arcs (BCA) (Meek and Walton in J Comput Appl Math 59(2): 221231, 1995). For quantitative comparison, we consider three different criteria: geometric complexity (the number of circular arcs and line segments), construction time, and numerical stability. The BCA construction after one-step refinement (producing four circular arcs) is almost comparable to the other two methods in geometric complexity: the SFA with two circular arcs and two line segments, and the bilens with four circular arcs. In other comparison criteria, the BCA approach is more efficient and stable than the other two methods in producing a hierarchy of bounding regions that approximate a family of freeform planar curves within a given error bound.
C1 [Lee, Jaewook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151744, South Korea.
   [Kim, Yong-Joon; Elber, Gershon] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
   [Kim, Myung-Soo] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 151744, South Korea.
C3 Seoul National University (SNU); Technion Israel Institute of
   Technology; Seoul National University (SNU)
EM mskim@snu.ac.kr
FU European Union under REA [PIAP-GA-2011-286426]; Israel Science
   Foundation [278/13]; Korean MCST; KOCCA in the CT RD Program
   [R2014060001]; NRF [2013R1A1A2010085]
FX This work was supported in part by the People Programme (Marie Curie
   Actions) of the European Unions Seventh Framework Programme
   FP7/2007-2013/ under REA Grant agreement PIAP-GA-2011-286426, in part by
   the Israel Science Foundation (Grant No. 278/13), and also in part by
   the Korean MCST and KOCCA in the CT R&D Program 2014 (No. R2014060001),
   and in part by NRF Research Grants (No. 2013R1A1A2010085).
CR Aichholzer O, 2009, COMPUT AIDED DESIGN, V41, P339, DOI 10.1016/j.cad.2008.08.008
   Aichholzer O, 2010, COMP GEOM-THEOR APPL, V43, P688, DOI 10.1016/j.comgeo.2010.04.004
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], 1997, J GRAPH TOOLS, DOI DOI 10.1080/10867651.1997.10487480
   [Anonymous], 1999, TR99018 UNC DEP COMP
   Barequet G, 1996, COMPUT GRAPH FORUM, V15, pC387, DOI 10.1111/1467-8659.1530387
   Barton M, 2011, GRAPH MODELS, V73, P50, DOI 10.1016/j.gmod.2010.10.005
   Coons S.A., 1964, TECHNICAL REPORT
   Ericson C., 2005, Real-time collision detection
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   HUBBARD PM, 1995, IEEE T VIS COMPUT GR, V1, P218, DOI 10.1109/2945.466717
   Kim YJ, 2014, COMPUT AIDED DESIGN, V46, P252, DOI 10.1016/j.cad.2013.08.041
   Kim YJ, 2013, COMPUT AIDED DESIGN, V45, P270, DOI 10.1016/j.cad.2012.10.010
   Kim YJ, 2012, COMPUT AIDED GEOM D, V29, P555, DOI 10.1016/j.cagd.2012.03.014
   Kim YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024203
   Kim YJ, 2011, COMPUT GRAPH-UK, V35, P698, DOI 10.1016/j.cag.2011.03.028
   Kim YJ, 2010, VISUAL COMPUT, V26, P1007, DOI 10.1007/s00371-010-0477-3
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Kumosenko A., 2013, COMPUT AIDED GEOM D, V30, P310
   Lee J, 2015, COMPUT AIDED DESIGN, V58, P248, DOI 10.1016/j.cad.2014.08.031
   Lin Q, 2002, COMPUT AIDED DESIGN, V34, P969, DOI 10.1016/S0010-4485(01)00141-5
   MEEK DS, 1995, J COMPUT APPL MATH, V59, P221, DOI 10.1016/0377-0427(94)00029-Z
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   QUINLAN S, 1994, IEEE INT CONF ROBOT, P3324, DOI 10.1109/ROBOT.1994.351059
   Sederberg T. W., 1989, Computer-Aided Geometric Design, V6, P205, DOI 10.1016/0167-8396(89)90024-1
   Sír Z, 2006, COMPUT AIDED DESIGN, V38, P608, DOI 10.1016/j.cad.2006.02.003
   Yong JH, 2006, COMPUT AIDED DESIGN, V38, P515, DOI 10.1016/j.cad.2006.01.003
NR 27
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 809
EP 818
DI 10.1007/s00371-015-1093-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500007
DA 2024-07-18
ER

PT J
AU Ozcan, CY
   Haciomeroglu, M
AF Ozcan, Cumhur Yigit
   Haciomeroglu, Murat
TI A path-based multi-agent navigation model
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Crowd simulation; Path planning; A*
ID CROWD; SIMULATION
AB The quality of a crowd simulation model is determined by its agents' local and global trajectory efficiency. While an agent-based model can accurately handle the local trajectories, global decisions usually are handled by a global path planner. However, most of the global path planning techniques do not consider other agents and their possible paths and the future global flow in the environment. In this paper, we propose a composite system that takes future agent configurations into account via a modified A* algorithm to create a global path plan and combines the global path plan with a local navigation model. We show that the agents using the proposed model intelligently plan their paths based on the dynamic configuration of the environment. In order to balance the performance vs. trajectory quality trade-off, we propose a hierarchical grid structure and discuss its effects on both trajectory quality and computational performance.
C1 [Ozcan, Cumhur Yigit] Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
   [Haciomeroglu, Murat] Gazi Univ, Dept Comp Engn, Ankara, Turkey.
C3 Hacettepe University; Gazi University
RP Ozcan, CY (corresponding author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkey.
EM cumhuryigitozcan@cs.hacettepe.edu.tr
RI Ozcan, Cumhur Yigit/M-7747-2018
OI Ozcan, Cumhur Yigit/0000-0002-7394-9212
CR [Anonymous], S ROB RES
   [Anonymous], P 2013 INT C AUT AG
   [Anonymous], 2013, P SCA 2013 12 ACM SI, DOI DOI 10.1145/2485895.2485910
   [Anonymous], 1999, P GAM DEV C
   [Anonymous], 2004, Journal of Game Development
   [Anonymous], EUR TUT VIENN AUSTR
   Chen D, 2013, FUTURE GENER COMP SY, V29, P1309, DOI 10.1016/j.future.2012.03.006
   Golas A, 2014, IEEE T VIS COMPUT GR, V20, P1022, DOI 10.1109/TVCG.2013.235
   Guy S., 2009, EUR ACM SIGGRAPH S C, P177
   Guy S.J., 2010, P 9 INT C AUTONOMOUS, V2, P575
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Haciomeroglu M, 2008, COMPUT ANIMAT VIRT W, V19, P307, DOI 10.1002/cav.232
   Haciomeroglu M, 2013, COMPUT GRAPH-UK, V37, P862, DOI 10.1016/j.cag.2013.05.006
   Kapadia M., 2013, P 12 ACM SIG GRAPHEU, P115, DOI DOI 10.1145/2485895.2485909
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Morini F, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P144, DOI 10.1109/CW.2007.23
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Patil S, 2011, IEEE T VIS COMPUT GR, V17, P244, DOI 10.1109/TVCG.2010.33
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Pelechano Nuria., 2008, Virtual Crowds: Methods, Simulation, and Control
   Ren CJ, 2009, L N INST COMP SCI SO, V5, P1451
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Ryder G, 2005, COMPUT GRAPH FORUM, V24, P203, DOI 10.1111/j.1467-8659.2005.00844.x
   Thalmann D, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P1, DOI 10.1109/CW.2009.23
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Turkay C, 2011, COMPUT J, V54, P1810, DOI 10.1093/comjnl/bxr014
   van Toll WG, 2012, COMPUT ANIMAT VIRT W, V23, P59, DOI 10.1002/cav.1424
   Xiong M., 2012, Computational Intelligence and Intelligent Systems
NR 29
TC 2
Z9 3
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 863
EP 872
DI 10.1007/s00371-015-1110-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500012
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Zheng, CW
AF Zheng, Quan
   Zheng, Chang-Wen
TI Visual importance-based adaptive photon tracing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Visual importance; Adaptive photon tracing; Photorealistic rendering;
   Global illumination
ID METROPOLIS; VISIBILITY; CHAIN
AB This paper proposes an adaptive photon tracing approach based on a novel importance function, which combines visual importance and photon path visibility. The generation of photon path is guided by sampling this function to trace more photons to visible and more contributive regions. As a first step, a hierarchy of visual importance maps is constructed. Next, photon paths are produced using a new hybrid mutation strategy, which consists of large mutation and small mutation. The mutation parameter used in small mutation is automatically adjusted using the adaptive Markov chain sampling method. Meanwhile, to find a suitable initial parameter, a mutation parameter initialization method is developed. Experiments show that, compared with previous methods, this approach yields results with better visual quality and smaller numerical error.
C1 [Zheng, Quan; Zheng, Chang-Wen] Chinese Acad Sci, Sci & Technol Integrated Informat Syst Lab, Inst Software, Beijing, Peoples R China.
   [Zheng, Quan] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Zheng, Q (corresponding author), Chinese Acad Sci, Sci & Technol Integrated Informat Syst Lab, Inst Software, Beijing, Peoples R China.
EM zhengquan@acm.org; cwzheng@ieee.org
RI Zheng, Quan/HZH-4993-2023
OI Zheng, Quan/0000-0001-5053-5511
CR Andrieu C., 2001, CONTROLLED MCMC OPTI
   [Anonymous], 2013, ACM T GRAPHIC, DOI DOI 10.1145/2461912.2461943
   Atchadé YF, 2005, BERNOULLI, V11, P815, DOI 10.3150/bj/1130077595
   Bashford-Rogers T, 2014, IEEE T VIS COMPUT GR, V20, P907, DOI 10.1109/TVCG.2013.258
   Chen JT, 2011, COMPUT GRAPH FORUM, V30, P1205, DOI 10.1111/j.1467-8659.2011.01979.x
   Christensen PH, 2003, IEEE T VIS COMPUT GR, V9, P329, DOI 10.1109/TVCG.2003.1207441
   Collin C, 2013, VISUAL COMPUT, V29, P849, DOI 10.1007/s00371-013-0845-x
   Craiu RV, 2009, J AM STAT ASSOC, V104, P1454, DOI 10.1198/jasa.2009.tm08393
   FAN S., 2005, RENDERING TECHNIQUES, P127
   Hachisuka T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019633
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409083
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Hoberock J, 2010, COMPUT GRAPH FORUM, V29, P1993, DOI 10.1111/j.1467-8659.2010.01713.x
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kelemen C, 2002, COMPUT GRAPH FORUM, V21, P531, DOI 10.1111/1467-8659.t01-1-00703
   KESTEN H, 1958, ANN MATH STAT, V29, P41, DOI 10.1214/aoms/1177706705
   Kitaoka S, 2009, COMPUT GRAPH FORUM, V28, P2330, DOI 10.1111/j.1467-8659.2009.01540.x
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Peter I., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P269
   Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Vorba J, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601203, 10.1145/2801097.2801203]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 24
TC 6
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1001
EP 1010
DI 10.1007/s00371-015-1104-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500025
DA 2024-07-18
ER

PT J
AU Chen, JJ
   Jin, XG
   Deng, ZG
AF Chen, Junjie
   Jin, Xiaogang
   Deng, Zhigang
TI GPU-based polygonization and optimization for implicit surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Implicit surface; Polygonization; Mesh optimization; GPU parallelization
ID MESHES
AB Despite the popularity of polygonization of implicit surfaces in graphics applications, an efficient solution to both polygonize and optimize meshes from implicit surfaces on modern GPUs has not been developed to date. In this paper, we introduce a practical GPU-based approach to efficiently polygonize and optimize iso-surface meshes for implicit surfaces. Specifically, we design new schemes to maximally exploit the parallel features of the GPU hardware, by optimizing both the geometry (vertex position, vertex distribution, triangle shape, and triangle normal) and the topology (connectivity) aspects of a mesh. Our experimental results show that, besides significant improvement on the resultant mesh quality, our GPU-based approach is approximately an order of magnitude faster than its CPU counterpart and faster than or comparable to other GPU iso-surface extraction methods. Furthermore, the achieved speedup becomes even higher if the resolution of the iso-surface is increased.
C1 [Chen, Junjie; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Deng, Zhigang] Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.
C3 Zhejiang University; University of Houston System; University of Houston
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM chenjunjie@cad.zju.edu.cn; jin@cad.zju.edu.cn; zdeng4@uh.edu
OI Deng, Zhigang/0000-0003-2571-5865; Deng, Zhigang/0000-0002-0452-8676
FU National Natural Science Foundation of China [61272298]; Zhejiang
   Provincial Natural Science Foundation of China [Z1110154]; Joint
   Research Fund for Overseas Chinese, Hong Kong and Macao Young Scientists
   of the National Natural Science Foundation of China [61328204]
FX Xiaogang Jin was supported by the National Natural Science Foundation of
   China (Grant No. 61272298) and Zhejiang Provincial Natural Science
   Foundation of China (Grant No. Z1110154). Zhigang Deng was supported by
   the Joint Research Fund for Overseas Chinese, Hong Kong and Macao Young
   Scientists of the National Natural Science Foundation of China (Grant
   No. 61328204).
CR Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   [Anonymous], VIS COMPUT
   [Anonymous], P VIS MOD VIS WORKSH
   [Anonymous], POLYGON MESH PROCESS
   Bischoff S, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P246, DOI 10.1109/PCCGA.2002.1167868
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Dias Sergio., 2010, Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing, HPDC '10, P531, DOI DOI 10.1145/1851476.1851553
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gomes AbelJ P., 2009, Implicit curves and surfaces: Mathematics, data structures and algorithms
   Griffin W, 2012, IEEE T VIS COMPUT GR, V18, P1603, DOI 10.1109/TVCG.2012.113
   Hartmann E, 1998, VISUAL COMPUT, V14, P95, DOI 10.1007/s003710050126
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Lempitsky V, 2010, PROC CVPR IEEE, P1197, DOI 10.1109/CVPR.2010.5539832
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Liu YJ, 2003, VISUAL COMPUT, V19, P565, DOI 10.1007/s00371-003-0222-2
   Liu YJ, 2003, VISUAL COMPUT, V19, P23, DOI 10.1007/s00371-002-0162-2
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Meyer MD, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/SMI.2005.41
   Meyer M, 2007, IEEE T VIS COMPUT GR, V13, P1704, DOI 10.1109/TVCG.2007.70604
   Nielson G. M., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P83, DOI 10.1109/VISUAL.1991.175782
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   Ohtake Y, 2001, COMPUT GRAPH FORUM, V20, pC368, DOI 10.1111/1467-8659.00529
   Rong G., 2010, Proceedings of Symposium of Solid and Physical Modeling (SPM 2010), P117
   Rong GD, 2011, IEEE T VIS COMPUT GR, V17, P345, DOI 10.1109/TVCG.2010.53
   Satish N., 2009, PROCESSING IEEE INT, P1
   Schmitz LA, 2009, SIBGRAPI, P64, DOI 10.1109/SIBGRAPI.2009.19
   Shirazian P., 2012, Eurographics Symposium on Parallel Graphics and Visualization (EGPGV), P89
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Tatarchuk N., 2007, ACM SIGGRAPH 2007 Courses, SIGGRAPH '07, P122
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Taubin G., 2001, RC22213W0110051 IBM
   Wang CCL, 2013, COMPUT AIDED DESIGN, V45, P321, DOI 10.1016/j.cad.2012.10.015
   Wang CCL, 2010, COMPUT AIDED DESIGN, V42, P535, DOI 10.1016/j.cad.2010.02.001
   Zhong ZC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461946
NR 37
TC 10
Z9 11
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 119
EP 130
DI 10.1007/s00371-014-0924-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800003
DA 2024-07-18
ER

PT J
AU Liu, XD
   Zheng, CW
AF Liu, Xiao Dan
   Zheng, Chang Wen
TI Adaptive cluster rendering via regression analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Cluster sampling; Adaptive rendering; Feature vector; Polynomial
   function
AB Monte Carlo ray tracing suffers noise and aliasing because of low sampling rate. We show that sparse samples can be used to generate high quality images based on feature cluster and regression analysis. Our algorithm has two main stages: adaptive sampling and polynomial reconstruction. In sampling stage, rendering space are organized into clusters based on their features. A feature vector is used to distinguish the different features, which contains gradient, variance and position. Clusters are progressively modified by adaptive sampling. In reconstruction stage, we model each cluster by smooth polynomial functions using regression analysis. The final image is synthesized by integrating these functions. The experiments show that our algorithm generates higher quality images than the previous methods.
C1 [Liu, Xiao Dan; Zheng, Chang Wen] Chinese Acad Sci, Inst Software, Integrated Informat Syst Technol Lab, Beijing, Peoples R China.
   [Liu, Xiao Dan] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, XD (corresponding author), Chinese Acad Sci, Inst Software, Integrated Informat Syst Technol Lab, Beijing, Peoples R China.
EM lxdfigo@163.com
CR [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], 2006, Geometric partial differential equations and image analysis
   Bala K, 2003, ACM T GRAPHIC, V22, P631, DOI 10.1145/882262.882318
   CROW FC, 1977, COMMUN ACM, V20, P799, DOI 10.1145/359863.359869
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   DURAND F, 1999, THESIS GRENOBLE U
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lepage G. P., 1980, CLNS80447 CORN U
   Li TM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366213
   Liu XD, 2012, VISUAL COMPUT, V28, P613, DOI 10.1007/s00371-012-0709-9
   MITCHELL DP, 1991, COMP GRAPH, V25, P157, DOI 10.1145/127719.122736
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Ragan-Kelley J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966396
   Rigau J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P260
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Sen Pradeep, 2004, HWWS '04: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware, P65
   Szecsi L., 2010, P 26 SPRING C COMP G, P69, DOI [10.1145/1925059.1925073, DOI 10.1145/1925059.1925073]
   [No title captured]
NR 22
TC 5
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 105
EP 114
DI 10.1007/s00371-013-0914-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600009
DA 2024-07-18
ER

PT J
AU Sfikas, K
   Theoharis, T
   Pratikakis, I
AF Sfikas, Konstantinos
   Theoharis, Theoharis
   Pratikakis, Ioannis
TI Pose normalization of 3D models via reflective symmetry on panoramic
   views
SO VISUAL COMPUTER
LA English
DT Article
DE Pose normalization; Rotation normalization; Panoramic view
   representation; Reflective symmetry; 3D object retrieval
ID SHAPE BENCHMARK; RETRIEVAL; APPROXIMATE
AB A novel pose normalization method, based on reflective symmetry computed on panoramic views, is presented. Qualitative and experimental investigation in 3D data-sets has led us to the observation that most objects possess a single plane of symmetry. Our approach is thus guided by this observation. Initially, through an iterative procedure, the symmetry plane of a 3D model is estimated, thus computing the first axis of the model. This is achieved by rotating the 3D model and computing reflective symmetry scores on panoramic view images. The other principal axes of the 3D model are estimated by computing the variance of the 3D model's panoramic views. The proposed method is incorporated in a hybrid scheme, that serves as the pose normalization method in a state-of-the-art 3D object retrieval system. The effectiveness of this system, using the hybrid pose normalization scheme, is evaluated in terms of retrieval accuracy and the results clearly show improved performance against current approaches.
C1 [Sfikas, Konstantinos; Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Comp Graph Lab, Athens, Greece.
   [Theoharis, Theoharis] NTNU, IDI, Trondheim, Norway.
   [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
   [Pratikakis, Ioannis] IRISA, Rennes, France.
   [Pratikakis, Ioannis] Demokritos Natl Ctr Sci Res, Inst Informat & Telecommun, GR-15310 Athens, Greece.
C3 National & Kapodistrian University of Athens; Norwegian University of
   Science & Technology (NTNU); Democritus University of Thrace; Universite
   de Rennes; National Centre of Scientific Research "Demokritos"
RP Sfikas, K (corresponding author), Univ Athens, Dept Informat & Telecommun, Comp Graph Lab, Athens, Greece.
EM ksfikas@di.uoa.gr; theotheo@idi.ntnu.no; ipratika@ee.duth.gr
RI Theoharis, Theoharis/AAN-2555-2020; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Sfikas,
   Konstantinos/0000-0002-9173-4557
FU European Union (European Social Fund-ESF); Greek national funds through
   the Operational Program "Education and Lifelong Learning" of the
   National Strategic Reference Framework (NSRF)-Research Funding Program:
   THALES-3DOR [MIS 379516]
FX This research has been co-financed by the European Union (European
   Social Fund-ESF) and Greek national funds through the Operational
   Program "Education and Lifelong Learning" of the National Strategic
   Reference Framework (NSRF)-Research Funding Program: THALES-3DOR (MIS
   379516). Investing in knowledge society through the European Social
   Fund.
CR Alizadeh F, 2012, CBMI, P1, DOI DOI 10.1109/CBMI.2012.6269797
   [Anonymous], 3D SHAPE BASED RETRI
   [Anonymous], EUR WORKSH 3D OBJ RE, DOI DOI 10.2312/3DOR/3DOR08/009-016
   [Anonymous], 2012, P 3DOR
   [Anonymous], 2009, P ACM INT C IM VID R, DOI DOI 10.1145/1646396.1646430
   Axenopoulos A, 2011, ICMR, P41, DOI [10.1145/1991996.1992037, DOI 10.1145/1991996.1992037]
   Chaouch M, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P599
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   CHEN DY, 2002, P INT COMP S WORKSH, P1436
   Elad M, 2002, SPRING EUROGRAP, P107
   Fang R, 2008, LECT NOTES COMPUT SC, V5358, P381, DOI 10.1007/978-3-540-89639-5_37
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kazhdan M, 2002, LECT NOTES COMPUT SC, V2351, P642
   Kazhdan M, 2007, IEEE T PATTERN ANAL, V29, P1221, DOI 10.1109/TPAMI.2007.1032
   Kazhdan Michael., 2004, COMPUT GRAPH FORUM, P115, DOI DOI 10.1145/1057432.1057448
   Kim DH, 2004, LECT NOTES COMPUT SC, V3332, P238
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lv D, 2013, APPL OPTICS, V52, P8073, DOI 10.1364/AO.52.008073
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Ohbuchi Ryutarou, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P63, DOI 10.1109/ICCVW.2009.5457716
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P., 2011, COMP VIS PATT REC WO, P23, DOI [10.1109/CVPRW.2011.5981714, DOI 10.1109/CVPRW.2011.5981714]
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Rustamov RM, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P13, DOI 10.1109/SMI.2007.6
   Sfikas K, 2013, 3DOR, P41
   Sfikas K, 2011, INT J COMPUT VISION, V91, P262, DOI 10.1007/s11263-010-0395-x
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2
   Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Vranic DV, 2004, THESIS
   Yuan H, 2011, KEY ENG MATER, V464, P453, DOI 10.4028/www.scientific.net/KEM.464.453
   Zaharia T, 2004, PROC SPIE, V5298, P47, DOI 10.1117/12.533092
   Zhang J, 2002, VDI BERICHT, V1679, P263
NR 46
TC 12
Z9 14
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1261
EP 1274
DI 10.1007/s00371-014-0935-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300006
DA 2024-07-18
ER

PT J
AU Toyoura, M
   Aruga, H
   Turk, M
   Mao, XY
AF Toyoura, Masahiro
   Aruga, Haruhito
   Turk, Matthew
   Mao, Xiaoyang
TI Mono-spectrum marker: an AR marker robust to image blur and defocus
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented reality; Spectrum analysis; Planar marker
AB Planar markers enable an augmented reality (AR) system to estimate the pose of objects from images containing them. However, conventional markers are difficult to detect in blurred or defocused images. We propose a new marker and a new detection and identification method that is designed to work under such conditions. The problem of conventional markers is that their patterns consist of high-frequency components such as sharp edges which are attenuated in blurred or defocused images. Our marker consists of a single low-frequency component. We call it a mono-spectrum marker. The mono-spectrum marker can be detected in real time with a GPU. In experiments, we confirm that the mono-spectrum marker can be accurately detected in blurred and defocused images in real time. Using these markers can increase the performance and robustness of AR systems and other vision applications that require detection or tracking of defined markers.
C1 [Toyoura, Masahiro; Aruga, Haruhito; Mao, Xiaoyang] Univ Yamanashi, Yamanashi 4008511, Japan.
   [Turk, Matthew] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 University of Yamanashi; University of California System; University of
   California Santa Barbara
RP Toyoura, M (corresponding author), Univ Yamanashi, 4-3-11 Kofu, Yamanashi 4008511, Japan.
EM mtoyoura@yamanashi.ac.jp
OI mao, xiaoyang/0000-0001-9531-3197; Toyoura,
   Masahiro/0000-0002-5897-7573; Turk, Matthew/0000-0002-4198-8401
CR [Anonymous], 2007, INT S MIX AUGM REAL
   Anqi Xu, 2011, 2011 Canadian Conference on Computer and Robot Vision (CRV), P40, DOI 10.1109/CRV.2011.13
   Aruga H., 2011, M IM REC UND MIRU, P496
   Asai H., 2010, IEEE INT C IM PROC I
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bichlmeier C., 2007, INT S MIX AUGM REAL, P129, DOI DOI 10.1109/ISMAR.2007.4538837
   Dixon J. D., 1996, GRAD TEXTS MATH, V163
   Fiala M, 2005, PROC CVPR IEEE, P590
   Ito E., 2011, INT S MIX AUGM REAL
   Kanatani K, 2007, COMPUT STAT DATA AN, V52, P1208, DOI 10.1016/j.csda.2007.05.013
   Kato H., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P85, DOI 10.1109/IWAR.1999.803809
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mohan A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531404
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Okumura B, 2006, INT SYM MIX AUGMENT, P126
   Owen C., 2002, IEEE INT WORKSH AUGM
   Park Y, 2009, INT SYM MIX AUGMENT, P163, DOI 10.1109/ISMAR.2009.5336480
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Tateno K, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P259
   Uchiyama H., 2011, IEEE VIRTUAL REALITY
   Uematsu Y, 2007, 17TH INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE, ICAT 2007, PROCEEDINGS, P183, DOI 10.1109/ICAT.2007.16
NR 21
TC 6
Z9 7
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 1035
EP 1044
DI 10.1007/s00371-013-0910-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chi, MT
   Yao, CY
   Zhang, E
   Lee, TY
AF Chi, Ming-Te
   Yao, Chih-Yuan
   Zhang, Eugene
   Lee, Tong-Yee
TI Optical illusion shape texturing using repeated asymmetric patterns
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Illusory motion; Line field; Repeated asymmetric patterns (RAP); Optical
   illusion art
ID VISUALIZATION; MOTION
AB Illusory motions refer to the phenomena in which static images composed of certain colors and patterns lead to the illusion of motions. This paper presents an approach for generating illusory motions on 3D surfaces which can be used for shape illustration as well as artistic visualization of line fields on surfaces. Our method extends previous work on generating illusory motions in the plane, which we adapt to 3D surfaces. In addition, we propose novel volume texture of repeated asymmetric patterns (RAPs) to visualize bidirectional flows, thus enabling the visualization of line fields in the plane and on the surface. We demonstrate the effectiveness of our method with applications in shape illustration as well as line field visualization on surfaces. For the design of optical illusion art, it is a tough case to arrange the distribution of RAP. However, we provide a semi-automatic algorithm to help users design flow direction. Finally, this technique applies to the design of street art and user could easily set the perspective effect and flow motion for illustration.
C1 [Chi, Ming-Te] Natl Chengchi Univ, Taipei 11623, Taiwan.
   [Yao, Chih-Yuan] Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
   [Zhang, Eugene] Oregon State Univ, Corvallis, OR 97331 USA.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan 70101, Taiwan.
C3 National Chengchi University; National Taiwan University of Science &
   Technology; Oregon State University; National Cheng Kung University
RP Yao, CY (corresponding author), Natl Taiwan Univ Sci & Technol, Taipei, Taiwan.
EM mtchi@cs.nccu.edu.tw; cyuan.yao@csie.ntust.edu.tw;
   zhange@eecs.oregonstate.edu; tonylee@mail.ncku.edu.tw
OI Chi, Ming-Te/0000-0002-9014-7561
FU National Science Council, Taiwan [NSC-102-2221-E-004 -008,
   NSC-102-2221-E-011-130, NSC-100-2221-E-006-188-MY3,
   NSC-100-2628-E-006-031-MY3]; Div Of Information & Intelligent Systems;
   Direct For Computer & Info Scie & Enginr [0917308] Funding Source:
   National Science Foundation
FX This work is supported by the National Science Council, Taiwan under
   NSC-102-2221-E-004 -008, NSC-102-2221-E-011-130,
   NSC-100-2221-E-006-188-MY3 and NSC-100-2628-E-006-031-MY3.
CR Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151
   Chi M.T., 2008, P ACM SIGGRAPH 08 LO, P1
   Conway BR, 2005, J NEUROSCI, V25, P5651, DOI 10.1523/JNEUROSCI.1084-05.2005
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Foley J.D., 1990, Computer graphics: Principles and practice
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Girshick Ahna., 2000, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering 2000, P43, DOI DOI 10.1145/340916.340922
   Gooch A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P447, DOI 10.1145/280814.280950
   Gossett N., 2004, SELF ANIMATING LINE
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Inglis TC, 2012, COMPUT GRAPH-UK, V36, P607, DOI 10.1016/j.cag.2012.03.003
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Kitaoka A., 2005, TRICK EYES GRAPHICS
   Kitaoka A., 2003, Vision, V15, P261, DOI [DOI 10.11247/JSSDJ.58, DOI 10.24636/VISION.15.4_261]
   Kitaoka Akiyoshi, 2006, J 3 DIMENSIONAL IMAG, V20, P9
   Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110
   Lee Y., 2007, ACM SIGGRAPH 2007 SI
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Li YY, 2011, IEEE T VIS COMPUT GR, V17, P231, DOI 10.1109/TVCG.2010.36
   Luft T, 2006, ACM T GRAPHIC, V25, P1206, DOI 10.1145/1141911.1142016
   Lum EB, 2003, IEEE T VIS COMPUT GR, V9, P115, DOI 10.1109/TVCG.2003.1196000
   Palacios J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276446, 10.1145/1239451.1239506]
   Palacios J, 2011, IEEE T VIS COMPUT GR, V17, P947, DOI 10.1109/TVCG.2010.121
   Potmesil M., 1983, Computer Graphics, V17, P389, DOI 10.1145/964967.801169
   Ray N, 2006, ACM T GRAPHIC, V25, P1460, DOI 10.1145/1183287.1183297
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   VAN WIJK J.J., 2003, VIS 03, P17
   VANWIJK JJ, 2002, SIGGRAPH 02 P 29 ANN, V21, P745
   Wei L. Y., 2006, MSRTR200682
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Yoshizawa Shin., 2005, S SOLID PHYS MODELIN, P227, DOI [10.1145/1060244.1060270, DOI 10.1145/1060244.1060270]
   Zhang E, 2007, IEEE T VIS COMPUT GR, V13, P94, DOI 10.1109/TVCG.2007.16
NR 32
TC 8
Z9 8
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 809
EP 819
DI 10.1007/s00371-014-0989-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700023
DA 2024-07-18
ER

PT J
AU Li, XW
   Xin, Q
   Wu, ZN
   Zhang, MS
   Zhang, Q
AF Li, Xiaowu
   Xin, Qiao
   Wu, Zhinan
   Zhang, Mingsheng
   Zhang, Qian
TI A geometric strategy for computing intersections of two spatial
   parametric curves
SO VISUAL COMPUTER
LA English
DT Article
DE Spatial parametric curve; Rectifying plane; Intersection; Tangent line
ID NONLINEAR EQUATIONS; ALGORITHMS; PROJECTION
AB An effective geometrically motivated intersection algorithm for two spatial parametric curves is presented. Examples are shown to illustrate the efficiency and robustness of the new method.
C1 [Li, Xiaowu; Zhang, Mingsheng] Guizhou Minzu Univ, Comp & Informat Engn Coll, Guiyang 550025, Peoples R China.
   [Xin, Qiao] Yili Normal Univ, Coll Math & Stat, Yining 835000, Peoples R China.
   [Wu, Zhinan] Yichun Univ, Sch Math & Comp Sci, Yichun 336000, Peoples R China.
   [Zhang, Qian] Guizhou Minzu Univ, Coll Sci, Guiyang 550025, Peoples R China.
C3 Guizhou Minzu University; Yili Normal University; Yichun University;
   Guizhou Minzu University
RP Li, XW (corresponding author), Guizhou Minzu Univ, Comp & Informat Engn Coll, Guiyang 550025, Peoples R China.
EM lixiaowu002@126.com
RI Xin, Qiao/HGA-9157-2022
FU Scientific and Technology Foundation Funded Project of Guizhou Province
   [[2012]2193]; Introduced Talents Scientific Research Foundation Funded
   Project of Guizhou Minzu University; Key Laboratory of Pattern
   Recognition and Intelligent System of Construction Project of Guizhou
   Province [[2009]4002]; Information Processing and Pattern Recognition
   for Graduate Education Innovation Base of Guizhou Province; Yili Normal
   University [2012YB016]; Key Disciplines in the General Colleges and
   Universities of Xin Jiang Uygur Autonomous Region [2012ZDXK08]
FX We take the opportunity to thank the reviewers for their thoughtful and
   meaningful comments. This work was supported by Scientific and
   Technology Foundation Funded Project of Guizhou Province ([2012]2193),
   Introduced Talents Scientific Research Foundation Funded Project of
   Guizhou Minzu University, Key Laboratory of Pattern Recognition and
   Intelligent System of Construction Project of Guizhou Province
   ([2009]4002) and Information Processing and Pattern Recognition for
   Graduate Education Innovation Base of Guizhou Province. The second
   author was supported by the general project of Yili Normal University
   (2012YB016) and the Fund of the Key Disciplines in the General Colleges
   and Universities of Xin Jiang Uygur Autonomous Region (2012ZDXK08).
CR Barton M, 2007, COMPUT AIDED GEOM D, V24, P125, DOI 10.1016/j.cagd.2007.01.003
   COLLINS GE, 1992, P INT S SYMB ALG COM, P189
   Cordero A, 2007, APPL MATH COMPUT, V190, P686, DOI 10.1016/j.amc.2007.01.062
   Elber G., 1990, Computer Graphics, V24, P95, DOI 10.1145/97880.97890
   Farouki R.T., 1990, J SYMB COMPUT, V9, P457
   Hoffmann C., 1989, Geometric and Solid Modeling: An Introduction
   Hu CY, 1996, COMPUT AIDED DESIGN, V28, P495, DOI 10.1016/0010-4485(95)00063-1
   Hu SM, 2005, COMPUT AIDED GEOM D, V22, P251, DOI 10.1016/j.cagd.2004.12.001
   Kajiya J. T., 1982, Computer Graphics, V16, P245, DOI 10.1145/965145.801287
   Kallay M, 2001, COMPUT AIDED GEOM D, V18, P797, DOI 10.1016/S0167-8396(01)00070-X
   KOPARKAR PA, 1983, COMPUT AIDED DESIGN, V15, P41, DOI 10.1016/S0010-4485(83)80050-5
   LANE JM, 1980, IEEE T PATTERN ANAL, V2, P150
   Li XW, 2011, NUMER ALGORITHMS, V57, P389, DOI 10.1007/s11075-010-9434-5
   Li XW, 2010, APPL MATH COMPUT, V215, P3754, DOI 10.1016/j.amc.2009.11.016
   Ma YL, 2003, COMPUT AIDED GEOM D, V20, P79, DOI 10.1016/S0167-8396(03)00021-9
   MANOCHA D, 1994, ACM T GRAPHIC, V13, P73, DOI 10.1145/174462.174617
   Morken K, 2009, COMPUT AIDED GEOM D, V26, P351, DOI 10.1016/j.cagd.2008.07.005
   Noor MA, 2009, COMPUT MATH APPL, V57, P101, DOI 10.1016/j.camwa.2008.10.067
   Palaj V., P S COMP GEOM SCG 20
   Schulz C, 2009, COMPUT AIDED GEOM D, V26, P61, DOI 10.1016/j.cagd.2007.12.006
   Sederberg T. W., 1989, Computer-Aided Geometric Design, V6, P205, DOI 10.1016/0167-8396(89)90024-1
   Sederberg Thomas Warren, 1983, Implicit and parametric curves and surfaces for computer aided geometric design
   SEDERBERG TW, 1989, COMPUT AIDED DESIGN, V21, P547, DOI 10.1016/0010-4485(89)90015-8
   SEDERBERG TW, 1986, COMPUT AIDED DESIGN, V18, P58, DOI 10.1016/S0010-4485(86)80013-6
   SEDERBERG TW, 1990, COMPUT AIDED DESIGN, V22, P538, DOI 10.1016/0010-4485(90)90039-F
   Wilkinson J. H., 1959, Numer. Math., V1, P150
   Wilkinson J.H., 1959, NUMER MATH, V1, P150, DOI DOI 10.1007/BF01386381
NR 27
TC 6
Z9 6
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1151
EP 1158
DI 10.1007/s00371-012-0758-0
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300004
DA 2024-07-18
ER

PT J
AU Wardhana, NM
   Johan, H
   Seah, HS
AF Wardhana, Nicholas Mario
   Johan, Henry
   Seah, Hock Soon
TI Enhanced waypoint graph for surface and volumetric path planning in
   virtual worlds
SO VISUAL COMPUTER
LA English
DT Article
DE Path planning; Automatic waypoint graph generation; Heterogeneous
   characters; Surface and volumetric motions; Virtual worlds
AB Our research focuses on the problem of path planning in 3D virtual world applications. The characters we consider are heterogeneous, as they have different sizes, and can perform surface or volumetric motion. In this paper, we propose an enhanced waypoint graph, which consists of point nodes equipped with radius, as well as edges connecting those nodes. Each edge is labeled with the motion type it can support. Given a polygon soup representation of a virtual world, the proposed algorithm starts by subdividing the virtual world into regions. This enables us to process large virtual worlds. Each region is then locally voxelized, one at a time. Two kinds of waypoints are generated: local waypoints using corner detection on the voxelization, and border waypoints at the region boundary. Waypoints are then sparsely connected to form a local waypoint graph, and local graphs are finally connected via the border waypoints to create the final global enhanced waypoint graph. To plan paths between arbitrary points using this graph, the points are connected to the graph using nearest neighbor search and traversability test, then Dijkstra/A* algorithm is used to calculate the final path, taking into account the appropriate size and motion type.
C1 [Wardhana, Nicholas Mario] Nanyang Technol Univ, Sch Comp Engn, GameLAB, Singapore 639798, Singapore.
   [Johan, Henry; Seah, Hock Soon] Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Wardhana, NM (corresponding author), Nanyang Technol Univ, Sch Comp Engn, GameLAB, N4-B1B-13 Nanyang Ave, Singapore 639798, Singapore.
EM mario.wardhana@ntu.edu.sg
RI Seah, Hock Soon/AAK-9900-2020; Wardhana, Nicholas Mario/AAH-8527-2019
OI Seah, Hock Soon/0000-0003-2699-7147; 
FU Ministry of Education Singapore
FX Our sincere gratitude goes to Quah Chee Kwang, Anthony Chansavang, and
   Budianto Tandianus for the fruitful personal discussions. The small
   outdoor scene as well as the two-story house models were created by Max
   Lim Tze Yuen, whereas Kampong Glam were created by the Media Development
   Authority (MDA) Singapore. These models are used in this research with
   permissions. This research is supported in part by the Ministry of
   Education Singapore, Academic Research Fund (AcRF) Tier 1 for project
   "Volumetric Path Planning in Real-Time 3D Virtual Environments".
CR Amanatides John, 1987, P EUROGRAPHICS, P3
   [Anonymous], 2004, Journal of Game Development
   CGAL Open Source Project, 2012, CGAL COMP GEOM ALG L
   Clarkson K., 1987, P 19 ANN ACM S THEOR, P56, DOI DOI 10.1145/28395.28402
   Coumans E, 2012, BULLET PHYS LIB VERS
   Coumans E, 2010, SIGGRAPH 2010 TALKS
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Galassi M., 2009, GNU Scientific Library Reference Manual, V3rd edn
   Gärtner B, 1999, LECT NOTES COMPUT SC, V1643, P325
   Geraerts R, 2007, IEEE INT CONF ROBOT, P1023, DOI 10.1109/ROBOT.2007.363119
   Harris C., 1988, ALVEY VISION C, P147151
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Hjaltason GR, 1995, LECT NOTES COMPUT SC, V951, P83
   Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439
   KEIL JM, 1988, LECT NOTES COMPUT SC, V318, P208
   LOZANOPEREZ T, 1979, COMMUN ACM, V22, P560, DOI 10.1145/359156.359164
   LOZANOPEREZ T, 1983, IEEE T COMPUT, V32, P108, DOI 10.1109/TC.1983.1676196
   Pettre J., 2005, First Intl. Work- shop on Crowd Simulation, P81
   Pinter M., 2001, MORE REALISTIC PATHF
   Siek J., 2012, BOOST GRAPH LIB VERS
   Tozour P, 2004, AI GAME PROGRAMMING, P113
   Wardhana N. M., 2012, P 2012 INT WORKSH AD, P450
   Wardhana NM, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P69, DOI 10.1109/CW.2012.17
   Zagar M, 2010, SIGNALS COMMUN TECHN, P331, DOI 10.1007/978-3-642-12802-8_14
   Zhang L, 2007, VISUAL COMPUT, V23, P783, DOI 10.1007/s00371-007-0149-0
NR 25
TC 6
Z9 7
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1051
EP 1062
DI 10.1007/s00371-013-0837-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400007
DA 2024-07-18
ER

PT J
AU Celikcan, U
   Cimen, G
   Kevinc, EB
   Capin, T
AF Celikcan, Ufuk
   Cimen, Gokcen
   Kevinc, E. Bengu
   Capin, Tolga
TI Attention-Aware Disparity Control in interactive environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Stereoscopic 3D; Disparity control; Interactive 3D; User attention;
   Real-time graphics; Accommodation/convergence conflict
AB Our paper introduces a novel approach for controlling stereo camera parameters in interactive 3D environments in a way that specifically addresses the interplay of binocular depth perception and saliency of scene contents. Our proposed Dynamic Attention-Aware Disparity Control (DADC) method produces depth-rich stereo rendering that improves viewer comfort through joint optimization of stereo parameters. While constructing the optimization model, we consider the importance of scene elements, as well as their distance to the camera and the locus of attention on the display. Our method also optimizes the depth effect of a given scene by considering the individual user's stereoscopic disparity range and comfortable viewing experience by controlling accommodation/convergence conflict. We validate our method in a formal user study that also reveals the advantages, such as superior quality and practical relevance, of considering our method.
C1 [Celikcan, Ufuk; Cimen, Gokcen; Kevinc, E. Bengu; Capin, Tolga] Bilkent Univ, Bilgisayar Muhendisligi Bolumu, TR-06800 Ankara, Turkey.
C3 Ihsan Dogramaci Bilkent University
RP Celikcan, U (corresponding author), Bilkent Univ, Bilgisayar Muhendisligi Bolumu, TR-06800 Ankara, Turkey.
EM celikcan@acm.org; gokcen.cimen@cs.bilkent.edu.tr;
   kevinc@cs.bilkent.edu.tr; tcapin@cs.bilkent.edu.tr
RI Çapın, Tolga Kurtuluş/G-6172-2018; Celikcan, Ufuk/H-1191-2017
OI Celikcan, Ufuk/0000-0001-6421-185X
FU Scientific and Technical Research Council of Turkey (TUBITAK) [110E029]
FX We would like to thank Dr. Ugur Gudukbay, Aytek Aman and Ates Akaydin
   for supplying some of the 3D human models used in our scenes; Sami Arpa
   and the 3dios Productions for providing the 3D display equipment; and
   also the anonymous reviewers for their valuable suggestions. This work
   is supported by the Scientific and Technical Research Council of Turkey
   (TUBITAK, project number 110E029).
CR [Anonymous], 2012, RECOMMENDATION ITU R
   DIDYK P., 2012, ACM T GRAPH, V31
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Gress A, 2006, COMPUT GRAPH FORUM, V25, P497, DOI 10.1111/j.1467-8659.2006.00969.x
   Guttmann M, 2009, IEEE I CONF COMP VIS, P136, DOI 10.1109/ICCV.2009.5459158
   Heinzle S., 2011, ACM T GRAPH, V30, P94
   Howard I. P., 2002, SEEING DEPTH DEPTH P, V2
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Johnson Steven G., 2011, NLOPT NONLINEAR OPTI
   Jones G. R., 2001, CONTROLLING PERCEIVE
   Koppal SJ, 2011, IEEE COMPUT GRAPH, V31, P20, DOI 10.1109/MCG.2010.37
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Liu C., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), P253, DOI DOI 10.1145/2072298.2072332
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Milgram P., 1992, P SPIE STEREOSCOPIC, V1669-13
   Oskam T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024223
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Runarsson TP, 2000, IEEE T EVOLUT COMPUT, V4, P284, DOI 10.1109/4235.873238
   Shamir A., 2009, ACM SIGGRAPH ASIA 20, P11
   Zilly F., 2011, Electronic Media Technology (CEMT), 2011 14th ITG Conference on, P1
   Zilly F, 2011, P IEEE, V99, P590, DOI 10.1109/JPROC.2010.2095810
NR 21
TC 14
Z9 19
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 685
EP 694
DI 10.1007/s00371-013-0804-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400022
DA 2024-07-18
ER

PT J
AU Vais, A
   Berger, B
   Wolter, FE
AF Vais, Alexander
   Berger, Benjamin
   Wolter, Franz-Erich
TI Complex line bundle Laplacians
SO VISUAL COMPUTER
LA English
DT Article
DE Laplace operator; Complex vector bundles; Spectral decomposition; Finite
   elements
ID BELTRAMI OPERATORS; SURFACES; SPECTRA; LOOPS
AB In the present work, we extend the theoretical and numerical discussion of the well-known Laplace-Beltrami operator by equipping the underlying manifolds with additional structure provided by vector bundles. Focusing on the particular class of flat complex line bundles, we examine a whole family of Laplacians including the Laplace-Beltrami operator as a special case.
   To demonstrate that our proposed approach is numerically feasible, we describe a robust and efficient finite-element discretization, supplementing the theoretical discussion with first numerical spectral decompositions of those Laplacians.
   Our method is based on the concept of introducing complex phase discontinuities into the finite element basis functions across a set of homology generators of the given manifold. More precisely, given an m-dimensional manifold M and a set of n generators that span the relative homology group H (m-1)(M,a,M), we have the freedom to choose n phase shifts, one for each generator, resulting in a n-dimensional family of Laplacians with associated spectra and eigenfunctions. The spectra and absolute magnitudes of the eigenfunctions are not influenced by the exact location of the paths, depending only on their corresponding homology classes.
   Employing our discretization technique, we provide and discuss several interesting computational examples highlighting special properties of the resulting spectral decompositions. We examine the spectrum, the eigenfunctions and their zero sets which depend continuously on the choice of phase shifts.
C1 [Vais, Alexander; Berger, Benjamin; Wolter, Franz-Erich] Leibniz Univ Hannover, Div Comp Graph, Welfenlab, D-30167 Hannover, Germany.
C3 Leibniz University Hannover
RP Vais, A (corresponding author), Leibniz Univ Hannover, Div Comp Graph, Welfenlab, D-30167 Hannover, Germany.
EM vais@welfenlab.de; bberger@welfenlab.de; few@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz-Erich/AAV-3008-2020;
   Berger, Benjamin/HJA-7645-2022; Wolter, Franz - Erich/B-1672-2014
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494; Berger, Benjamin/0000-0003-2654-2898; 
CR Alexa M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964997
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   de Verdière ÉC, 2005, DISCRETE COMPUT GEOM, V33, P507, DOI 10.1007/s00454-004-1150-2
   Dey TK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360644
   Diaz-Gutierrez P, 2009, COMPUT GRAPH FORUM, V28, P2015, DOI 10.1111/j.1467-8659.2009.01580.x
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   DODZIUK J, 1976, AM J MATH, V98, P79, DOI 10.2307/2373615
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   DZIUK G, 1988, LECT NOTES MATH, V1357, P142
   Erickson J, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1038
   Frankel T., 2011, GEOMETRY PHYS INTRO
   Gross P. W., 2004, Electromagnetic Theory and Computation: A Topological Approach
   Hatcher A., 2003, ALGEBRAIC TOPOLOGY
   Hernandez V, 2005, ACM T MATH SOFTWARE, V31, P351, DOI 10.1145/1089014.1089019
   Hildebrandt K, 2010, LECT NOTES COMPUT SC, V6130, P296, DOI 10.1007/978-3-642-13411-1_20
   Hou TB, 2012, IEEE T VIS COMPUT GR, V18, P1268, DOI 10.1109/TVCG.2011.267
   KOTIUGA PR, 1989, IEEE T MAGN, V25, P4129, DOI 10.1109/20.42544
   Levy B, 2006, INT C SHAP MOD APPL
   Memoli Facundo, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P256, DOI 10.1109/ICCVW.2009.5457690
   Niethammer M., 2007, Medical Image Computing and Computer Assisted Intervention
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Peinecke N, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P409, DOI 10.1109/CW.2007.49
   Peinecke N, 2007, COMPUT AIDED DESIGN, V39, P460, DOI 10.1016/j.cad.2007.01.014
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M., 2005, P 2005 ACM S SOLID P, P101, DOI DOI 10.1145/1060244.1060256
   Reuter M, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P417, DOI 10.1109/CW.2007.42
   Reuter M, 2010, INT J COMPUT VISION, V89, P287, DOI 10.1007/s11263-009-0278-1
   Reuter M, 2009, COMPUT AIDED DESIGN, V41, P739, DOI 10.1016/j.cad.2009.02.007
   Reuter M, 2009, COMPUT GRAPH-UK, V33, P381, DOI 10.1016/j.cag.2009.03.005
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Solin P., 2005, Partial Differential Equations and the Finite Element Method
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Vais A, 2012, COMPUT GRAPH-UK, V36, P398, DOI 10.1016/j.cag.2012.03.027
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wardetzky M., 2007, P EUROGRAPHICS S GEO, P33, DOI [DOI 10.2312/SGP/SGP07/033-037, 10.2312/SGP/SGP07/033-037]
   Wolter F-E, LAPLACE SPEKTRA ANWE
   Wolter F-E, 2009, US Patent, Patent No. [20090169050 US, 20090169050]
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
   Xin SQ, 2011, LECT NOTES COMPUT SC, V6892, P384, DOI 10.1007/978-3-642-23629-7_47
   Xu GL, 2004, COMPUT AIDED GEOM D, V21, P767, DOI 10.1016/j.cagd.2004.07.007
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
NR 43
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 345
EP 357
DI 10.1007/s00371-012-0737-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200003
DA 2024-07-18
ER

PT J
AU Guenther, D
   Reininghaus, J
   Wagner, H
   Hotz, I
AF Guenther, David
   Reininghaus, Jan
   Wagner, Hubert
   Hotz, Ingrid
TI Efficient computation of 3D Morse-Smale complexes and persistent
   homology using discrete Morse theory
SO VISUAL COMPUTER
LA English
DT Article
DE Persistent homology; Morse-Smale complex; Discrete Morse theory; Large
   data
AB We propose an efficient algorithm that computes the Morse-Smale complex for 3D gray-scale images. This complex allows for an efficient computation of persistent homology since it is, in general, much smaller than the input data but still contains all necessary information. Our method improves a recently proposed algorithm to extract the Morse-Smale complex in terms of memory consumption and running time. It also allows for a parallel computation of the complex. The computational complexity of the Morse-Smale complex extraction solely depends on the topological complexity of the input data. The persistence is then computed using the Morse-Smale complex by applying an existing algorithm with a good practical running time. We demonstrate that our method allows for the computation of persistent homology for large data on commodity hardware.
C1 [Guenther, David] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
   [Reininghaus, Jan; Hotz, Ingrid] Zuse Inst Berlin, D-14165 Berlin, Germany.
   [Wagner, Hubert] Jagiellonian Univ, Inst Comp Sci, PL-30348 Krakow, Poland.
C3 Max Planck Society; Zuse Institute Berlin; Jagiellonian University
RP Guenther, D (corresponding author), Max Planck Inst Informat, Stuhlsatzenhausweg 85, D-66123 Saarbrucken, Germany.
EM dguenther@mpi-inf.mpg.de; reininghaus@zib.de;
   hubert.wagner@ii.uj.edu.pl; hotz@zib.de
FU MPI of Biochemistry; MPI for Informatics; DFG Emmy-Noether research
   program; Foundation for Polish Science Geometry and Topology in Physical
   Models program
FX This work was supported by the MPI of Biochemistry, the MPI for
   Informatics, the DFG Emmy-Noether research program, and Foundation for
   Polish Science Geometry and Topology in Physical Models program. We
   thank Daniel Baum for providing the molecule data set. We also thank
   Herbert Edelsbrunner and Chao Chen for many fruitful discussions.
CR [Anonymous], 2002, SEM LOTHAR COMBIN
   [Anonymous], 2005, THESIS PUC RIO
   Bauer U., DISCRETE COMPUTATION, V47, P1
   Bendich P, 2010, IEEE T VIS COMPUT GR, V16, P1251, DOI 10.1109/TVCG.2010.139
   Chari MK, 2000, DISCRETE MATH, V217, P101, DOI 10.1016/S0012-365X(99)00258-7
   Chen C, 2011, 27 EUR WORKSH COMP G
   Chen C, 2011, COMPUTATIONAL GEOMETRY (SCG 11), P207
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H., 2010, AM MATH SOC, DOI DOI 10.1007/978-3-540-33259-6_7
   Forman R, 1998, ADV MATH, V134, P90, DOI 10.1006/aima.1997.1650
   Guenther D., 2012, PROC TOPOINVIS, P15
   Gunther D., 2011, P C GRAPH PATT IM SI, V24, P25
   Gyulassy A, 2008, IEEE T VIS COMPUT GR, V14, P1619, DOI 10.1109/TVCG.2008.110
   Hatcher A., 2003, ALGEBRAIC TOPOLOGY
   Kaczynski T., 2004, Applied mathematical sciences, V157
   Lewiner T, 2003, COMP GEOM-THEOR APPL, V26, P221, DOI 10.1016/S0925-7721(03)00014-2
   Milnor J. W., 1965, Topology from the differentiable viewpoint
   Milosavljevic N, 2011, COMPUTATIONAL GEOMETRY (SCG 11), P216
   Morozov Dmitriy, 2005, BioGeometry News
   Mrozek M, 2010, COMPUT MATH APPL, V60, P2812, DOI 10.1016/j.camwa.2010.09.036
   Reininghaus J, 2010, LECT NOTES COMPUT SC, V6327, P198, DOI 10.1007/978-3-642-15582-6_35
   Robins V, 2011, IEEE T PATTERN ANAL, V33, P1646, DOI 10.1109/TPAMI.2011.95
   Volvis, 2010, VOX DAT REP
   Wagner H., 2012, Topological methods in data analysis and visualization II, P91, DOI [10.1007/978-3-642-23175-9_7, 10.1007/978-3-642-23175-97, DOI 10.1007/978-3-642-23175-97]
NR 25
TC 46
Z9 50
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 959
EP 969
DI 10.1007/s00371-012-0726-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900002
DA 2024-07-18
ER

PT J
AU Cha, S
   Park, J
   Hwang, J
   Noh, J
AF Cha, Seunghoon
   Park, Jinho
   Hwang, Jonghyun
   Noh, Junyong
TI An efficient diffusion model for viscous fingering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Natural phenomena; Viscous fingering; Physically-based modeling;
   Efficient simulation
ID CUBIC-POLYNOMIAL INTERPOLATION; HYPERBOLIC-EQUATIONS; REALISTIC
   ANIMATION; UNIVERSAL SOLVER; FLUID; SIMULATION
AB Viscous fingering is one of the most important factors to produce realistic diffusion among two miscible fluids with differing viscosities. Diffusion-limited Aggregation (DLA) has been a popular choice for the synthesis of the viscous fingering effect. However, as DLA provides a mere description of aggregation process, it is not clear how to apply the DLA model into conventional 3D fluid simulation equations. The DLA model first generates a shape description of the viscous fingering effect. The shape description is changed to a fluid flow region by the application of dilation and erosion operators. The flow region is then filled with the directions which will guide the fluid motion in a simulation. The directions are converted into a form of external force by means of a linear feedback system. Our results show that the DLA model can generate the viscous fingering effect effectively in a single phase simulation without relying on a high resolution grid. Our method is semi-physical due to the employment of DLA and is easy to implement, as the underlying concept is simple. Computational overhead is also negligible from the conventional fluid simulation.
C1 [Cha, Seunghoon; Hwang, Jonghyun; Noh, Junyong] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
   [Park, Jinho] Namseoul Univ, Dept Multimedia, Cheonan, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Namseoul
   University
RP Noh, J (corresponding author), Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
EM junyongnoh@kaist.ac.kr
RI Noh, Junyong/C-1663-2011
CR Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Bogoyavlenskiy VA, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.066303
   CHAN DYC, 1986, PHYS REV A, V34, P4079, DOI 10.1103/PhysRevA.34.4079
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Gunzburger MD, 2000, COMPUT METHOD APPL M, V189, P803, DOI 10.1016/S0045-7825(99)00344-8
   Hong JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360647
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Kang N, 2010, COMPUT GRAPH FORUM, V29, P685, DOI 10.1111/j.1467-8659.2009.01638.x
   Kim B., 2005, P 1 EUROGRAPHICS C N, DOI DOI 10.2312/NPH/NPH05/051-056
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P467, DOI 10.1111/j.1467-8659.2008.01144.x
   KIM T, 2004, P 2004 ACM SIGGRAPH, P305, DOI DOI 10.1145/1028523.1028564
   Kim T, 2007, IEEE T VIS COMPUT GR, V13, P390, DOI 10.1109/TVCG.2007.38
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   LEONARD BP, 1979, COMPUT METHOD APPL M, V19, P59, DOI 10.1016/0045-7825(79)90034-3
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   MONAGHAN JJ, 1988, COMPUT PHYS COMMUN, V48, P89, DOI 10.1016/0010-4655(88)90026-4
   Park J, 2008, COMPUT ANIMAT VIRT W, V19, P455, DOI 10.1002/cav.256
   SAFFMAN PG, 1958, PROC R SOC LON SER-A, V245, P312, DOI 10.1098/rspa.1958.0085
   Sethian J., 1999, LEVEL SET METHODS FA
   Shin SH, 2007, COMPUT ANIMAT VIRT W, V18, P447, DOI 10.1002/cav.202
   Shin SH, 2010, COMPUT GRAPH FORUM, V29, P675, DOI 10.1111/j.1467-8659.2009.01637.x
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J., 2003, Real-time fluid dynamics for games
   Takahashi T, 2003, COMPUT GRAPH FORUM, V22, P391, DOI 10.1111/1467-8659.00686
   Tamas Vicsek., 1992, Fractal growth phenomena
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   WITTEN TA, 1981, PHYS REV LETT, V47, P1400, DOI 10.1103/PhysRevLett.47.1400
   Xu S., 2011, P 10 INT C VIRT REAL, P109
   YABE T, 1991, COMPUT PHYS COMMUN, V66, P219, DOI 10.1016/0010-4655(91)90071-R
   YABE T, 1991, COMPUT PHYS COMMUN, V66, P233, DOI 10.1016/0010-4655(91)90072-S
NR 36
TC 2
Z9 3
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 563
EP 571
DI 10.1007/s00371-012-0690-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500005
DA 2024-07-18
ER

PT J
AU Kim, PR
   Lee, HY
   Kim, JH
   Kim, CH
AF Kim, Po-Ram
   Lee, Ho-Young
   Kim, Jong-Hyun
   Kim, Chang-Hun
TI Controlling shapes of air bubbles in a multi-phase fluid simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Fluid simulation; Air bubble control; Ellipsoidal bubble shape
ID ANIMATION; SPH
AB Controlling shapes is a challenging problem in a multi-phase fluid simulation. Bubble particles enable the details of air bubbles to be represented within a simulation based on an Euler grid. We control the target shapes of bubbles by the gradient vectors of the signed distance field and attraction forces associated with control particles. Our hybrid approach enables to simulate physically plausible movements of bubbles while preserving the details of a target shape. Furthermore, we control the paths of moving bubbles using user-defined curves and the shape of an air bubbles by drag force. An accurate model of the drag force near the fluid surface means that bubbles have realistic ellipsoidal shapes.
C1 [Kim, Chang-Hun] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
   [Kim, Po-Ram; Kim, Jong-Hyun] Korea Univ, Dept Visual Informat Proc, Seoul, South Korea.
   [Lee, Ho-Young] Korea Univ, Dept Comp & Radio Commun, Seoul, South Korea.
C3 Korea University; Korea University; Korea University
RP Kim, CH (corresponding author), Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
EM poramkim@korea.ac.kr; flymist@korea.ac.kr; gogogoscv@korea.ac.kr;
   chkim@korea.ac.kr
CR [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   [Anonymous], ACM SIGGRAPH 2007 PA
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Cleary P.W., 2007, ACM SIGGRAPH 2007 Papers
   Coleman P., 2002, THESIS OHIO STATE U
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   Ellingsen K, 2001, J FLUID MECH, V440, P235, DOI 10.1017/S0022112001004761
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Eunchan J., 2011, J VIS
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   Hong J.-M., 2008, ACM SIGGRAPH 2008 PA, p[48, 1, 4]
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Kim B., 2005, P 1 EUROGRAPHICS C N, DOI DOI 10.2312/NPH/NPH05/051-056
   Kim B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239549
   Kim B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866197
   Kim D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778807
   Lee HY, 2009, VISUAL COMPUT, V25, P713, DOI 10.1007/s00371-009-0339-z
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Robinson-Mosher A., 2008, ACM SIGGRAPH 2008, P46
   Shew WL, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/01/P01009
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Shin SH, 2007, COMPUT ANIMAT VIRT W, V18, P447, DOI 10.1002/cav.202
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
NR 27
TC 9
Z9 10
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 597
EP 602
DI 10.1007/s00371-012-0696-x
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500008
DA 2024-07-18
ER

PT J
AU Ling, Y
   Yan, CP
   Liu, CX
   Wang, X
   Li, H
AF Ling, Yun
   Yan, Caiping
   Liu, Chunxiao
   Wang, Xun
   Li, Hong
TI Adaptive tone-preserved image detail enhancement
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Image detail enhancement; Multi-scale decomposition; Adaptive synthesis;
   Tone correction
AB An adaptive tone-preserved algorithm for image detail enhancement is proposed to retain the tonal distribution of the input image and avoid experiential manipulation. First of all, domain transform based multi-scale image decomposition is carried out to quickly divide the input image into a base image which contains the coarse-scale image information, and the detail layers which contain the fine-scale details. Then, during the process of detail enhancement and synthesis, we construct an adaptive detail enhancement function based on the edge response, to prevent the exaggeration of strong edges and increase the enhancing magnitude of small details. Finally, in order to keep the color values of the input image and the gradient values of the detail enhanced image, a tonal correction algorithm based on energy optimization is presented to eliminate the distinct tonal differences of the enhanced image from the input image. Our experimental results show that tone-consistent image detail enhancement effect is available for arbitrary input images with unified parameters setting, which is superior to the state-of-the-art methods.
C1 [Ling, Yun; Yan, Caiping; Liu, Chunxiao; Wang, Xun; Li, Hong] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China.
   [Liu, Chunxiao] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 10091, Peoples R China.
C3 Zhejiang Gongshang University; Beihang University
RP Liu, CX (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China.
EM cxliu@mail.zjgsu.edu.cn
RI Leon, Leon/K-8790-2019
CR Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Feng Xiao, 2011, 2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC 2011), P1061, DOI 10.1109/AIMSEC.2011.6010635
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Hanika J, 2011, COMPUT GRAPH FORUM, V30, P1879, DOI 10.1111/j.1467-8659.2011.02054.x
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Piroddi R, 2004, ADV IMAG ELECT PHYS, V132, P109, DOI 10.1016/S1076-5670(04)32003-3
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zheng JH, 2010, C IND ELECT APPL, P436
NR 18
TC 9
Z9 10
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 733
EP 742
DI 10.1007/s00371-012-0691-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500021
DA 2024-07-18
ER

PT J
AU Thielhelm, H
   Vais, A
   Brandes, D
   Wolter, FE
AF Thielhelm, Hannes
   Vais, Alexander
   Brandes, Daniel
   Wolter, Franz-Erich
TI Connecting geodesics on smooth surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Shortest paths; Distance computation; Geodesics; Homotopy method; Focal
   curves
ID MANIFOLD
AB In this paper, we present a novel method for computing multiple geodesic connections between two arbitrary points on a smooth surface. Our method is based on a homotopy approach that is able to capture the ambiguity of geodesic connections in the presence of positive Gaussian curvature that generates focal curves.
   Contrary to previous approaches, we exploit focal curves to gain theoretical insights on the number of connecting geodesics and a practical algorithm for collecting these.
   We consider our method as a contribution to the contemporary debate regarding the calculation of distances in general situations, applying continuous concepts of classical differential geometry which are not immediately transferable in purely discrete settings.
C1 [Thielhelm, Hannes; Vais, Alexander; Brandes, Daniel; Wolter, Franz-Erich] Leibniz Univ Hannover, Div Comp Graph, Welfenlab, Hannover, Germany.
C3 Leibniz University Hannover
RP Thielhelm, H (corresponding author), Leibniz Univ Hannover, Div Comp Graph, Welfenlab, Hannover, Germany.
EM thielhel@welfenlab.de; vais@welfenlab.de; dbrandes@welfenlab.de;
   few@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz-Erich/AAV-3008-2020;
   Wolter, Franz - Erich/B-1672-2014
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494; 
CR [Anonymous], 1990, Numerical Continuation Methods
   [Anonymous], 1992, RIEMANNIAN GEOMETRY
   Bauer E., 2010, THESIS LEIBNIZ U HAN
   Blaschke W., 1973, ELEMENTARE DIFFERENT
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Kunze R, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P230, DOI 10.1109/CGI.1997.601311
   Meyer M., 2002, Visualization and mathematics, V3, P34
   NASS H, 2007, THESIS LEIBNIZ U HAN
   Nass H., 2007, CYBERWORLDS, P376
   Nass H., 2007, CYBERWORLDS, P386
   POLTHIER K, 1998, MATH VISUALIZATION
   RAUSCH T, 1997, MATH SURFACES, V7, P43
   RAUSCH T, 1999, THESIS LEIBNIZ U HAN
   Savage Leonard J., 1943, Bull. Amer. Math. Soc., V49, P467, DOI DOI 10.1090/S0002-9904-1943-07960-8,9101
   Schmidt R, 2006, ACM T GRAPHIC, V25, P605, DOI 10.1145/1141911.1141930
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Wolter F.-E., 1992, MIT DESIGN LAB MEMOR, V92
   Wolter F.- E., 1979, THESIS FU BERLIN
   WOLTER FE, 1979, ARCH MATH, V32, P92, DOI 10.1007/BF01238473
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
   WOLTER FE, 1985, THESIS TU BERLIN
   Wolter FE, 2011, LECT NOTES APPL COMP, V57, P211
   Ying LX, 2004, ACM T GRAPHIC, V23, P271, DOI 10.1145/1015706.1015714
NR 24
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 529
EP 539
DI 10.1007/s00371-012-0681-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500002
DA 2024-07-18
ER

PT J
AU Wang, H
   Chen, HY
   Su, ZX
   Cao, JJ
   Liu, FS
   Shi, XQ
AF Wang, Hui
   Chen, Hongyin
   Su, Zhixun
   Cao, Junjie
   Liu, Fengshan
   Shi, Xiquan
TI Versatile surface detail editing via Laplacian coordinates
SO VISUAL COMPUTER
LA English
DT Article
DE Detail editing; Laplacian coordinates; Smoothing; Enhancing
ID MESH; DIFFUSION; FILTER
AB This paper presents a versatile detail editing approach for triangular meshes based on filtering the Laplacian coordinates. More specifically, we first compute the Laplacian coordinates of the mesh vertices, then filter the Laplacian coordinates, and finally reconstruct the mesh from the filtered Laplacian coordinates by solving a linear least square system. The proposed detail editing method includes not only feature preserving smoothing but also enhancing. Furthermore, the proposed approach allows interactive editing of some user-specified frequencies and regions. Experimental results demonstrate that our method is much more versatile and faster than the existing methods.
C1 [Wang, Hui; Su, Zhixun; Cao, Junjie] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Chen, Hongyin] Neusoft Corp, Elect Power Div, Dalian 116085, Peoples R China.
   [Cao, Junjie] Dalian Univ Technol, Dept Engn Mech, State Key Lab Struct Anal Ind Equipment, Dalian 116024, Peoples R China.
   [Liu, Fengshan; Shi, Xiquan] Delaware State Univ, Appl Math Res Ctr, Dover, DE 19901 USA.
C3 Dalian University of Technology; Dalian University of Technology;
   Delaware State University
RP Su, ZX (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM zxsu@dlut.edu.cn
FU Fundamental Research Funds for the Central Universities; National
   Natural Science Foundation of China-Guangdong Joint Fund [U0935004];
   DEPSCoR [W911NF-07-1-0422]
FX We would like to express our deepest gratitude to the anonymous
   reviewers for their extensive help in improving this article. We are
   also grateful to Prof. Hongbo Fu for providing the software of Cholmod
   Wrapper, to Prof. Ligang Liu for insightful discussions and advices, to
   Prof. Igor Guskov for providing their mesh filtering software used in
   Fig. 11, and to Prof. Bruno Levy for their software of Manifold
   Harmonics. This work has been done at the Lab of Computational Geometry,
   Graphics and Image, Dalian University of Technology, and is supported by
   the Fundamental Research Funds for the Central Universities, National
   Natural Science Foundation of China-Guangdong Joint Fund (No. U0935004),
   and DEPSCoR (W911NF-07-1-0422). Models are courtesy of the AIM@SHAPE
   repository, except for the Dinosaur and Bunny.
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   Alexa M, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P51, DOI 10.1109/SMI.2002.1003528
   [Anonymous], P ECCV 1996
   AU OKC, 2005, IEEE T VIS COMPUT GR, V12, P191
   AU OKC, 2005, P EUR S GEOM PROC, P191
   Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   Botsch M., 2008, Eurographics Tutorial
   Chen CY, 2005, COMPUT AIDED GEOM D, V22, P376, DOI 10.1016/j.cagd.2005.04.003
   Choudhury, 2005, ACM SIGGRAPH 2005 Courses, P5, DOI 10.1145/1198555.1198565
   DAVIS T, 2005, CHOLMOD ALGORITHM
   Desbrun M, 2000, PROC GRAPH INTERF, P145
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Eigensatz M, 2008, COMPUT GRAPH FORUM, V27, P241, DOI 10.1111/j.1467-8659.2008.01121.x
   El Ouafdi AF, 2008, COMPUT GRAPH FORUM, V27, P1357, DOI 10.1111/j.1467-8659.2008.01275.x
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   FU H, 2008, CHOLMODWRAPPER
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   HIROKAZU Y, 2002, P GEOM MOD PROC, P2006
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kim B, 2005, COMPUT GRAPH FORUM, V24, P295, DOI 10.1111/j.1467-8659.2005.00854.x
   KLAUS H, 2004, COMPUT GRAPH FORUM, V23, P391
   LEE KW, 2006, P 9 INT C COMP AC DE, P275
   Li Z, 2009, VISUAL COMPUT, V25, P139, DOI 10.1007/s00371-008-0210-7
   Lipman Y., 2005, International Journal of Shape Modeling, V11, P43, DOI 10.1142/S0218654305000724
   Liu LG, 2007, COMPUT AIDED DESIGN, V39, P772, DOI 10.1016/j.cad.2007.03.004
   Liu XG, 2001, COMPUT GRAPH FORUM, V20, P115, DOI 10.1111/1467-8659.00483
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Sorkine O, 2005, IEEE T VIS COMPUT GR, V11, P171, DOI 10.1109/TVCG.2005.33
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   SORKINE O, 2003, P EUR ACM SIGGRAPH S, P42
   Sorkine O, 2006, COMPUT GRAPH FORUM, V25, P789, DOI 10.1111/j.1467-8659.2006.00999.x
   Su ZX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P1, DOI 10.1109/SMI.2009.5170156
   Sun XF, 2008, COMPUT AIDED GEOM D, V25, P437, DOI 10.1016/j.cagd.2007.12.008
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   TAUBIN G, 2001, RC2213 IBM RES
   TAUBIN G, 2000, P EUROGRAPHICS 2000
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Yagou H., 2003, J 3 DIMENSIONAL IMAG, V17, P170
   Yoshizawa S, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P38
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhang H, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P502
   Zhao HX, 2006, J COMPUT APPL MATH, V195, P300, DOI 10.1016/j.cam.2005.03.094
   Zhou K, 2004, COMPUT AIDED DESIGN, V36, P363, DOI 10.1016/S0010-4485(03)00098-8
NR 45
TC 9
Z9 12
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2011
VL 27
IS 5
BP 401
EP 411
DI 10.1007/s00371-011-0558-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745ZI
UT WOS:000289209800006
DA 2024-07-18
ER

PT J
AU Tian, FL
   Hua, W
   Dong, ZL
   Bao, HJ
AF Tian, Fenglin
   Hua, Wei
   Dong, Zilong
   Bao, Hujun
TI Adaptive voxels: interactive rendering of massive 3D models
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive display; Hardware occlusion culling; Level-of-detail;
   Out-of-core
ID VISIBILITY
AB We present a novel approach for interactive rendering of massive 3D models. Our approach integrates adaptive sampling-based simplification, visibility culling, out-of-core data management and level-of-detail. We use a unified scene graph representation for acceleration techniques. In preprocessing, we subdivide large objects, and build a BVH clustering hierarchy. We make use of a novel adaptive sampling method to generate LOD models: AdaptiveVoxels. The AdaptiveVoxels reduces the preprocessing cost and our out-of-core rendering algorithm improves rendering efficiency. We have implemented our algorithm on a desktop PC. We can render massive CAD and isosurface models, consisting of hundreds of millions of triangles interactively with little loss in image quality.
C1 [Tian, Fenglin; Hua, Wei; Dong, Zilong; Bao, Hujun] Zhejiang Univ, CADCG State Key Lab, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Hua, W (corresponding author), Zhejiang Univ, CADCG State Key Lab, Hangzhou 310003, Zhejiang, Peoples R China.
EM fltian@cad.zju.edu.cn; huawei@cad.zju.edu.cn; zldong@cad.zju.edu.cn;
   bao@cad.zju.edu.cn
FU National Natural Science Foundation of China [60773184]; National
   Program on Key Basic Research Project (973 Program) [2009CB320802];
   National High-tech Research and Development Program of China (863
   Program) [2009A A12Z229]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 60773184), National Program on Key Basic Research
   Project (973 Program) (Grant No. 2009CB320802), National High-tech
   Research and Development Program of China (863 Program) (Grant No. 2009A
   A12Z229).
CR Aila T, 2004, IEEE COMPUT GRAPH, V24, P86, DOI 10.1109/MCG.2004.1274066
   ALIAGA D, 1999, 1999 ACM S INT 3D GR, P199
   Aliaga DG, 1999, COMP GRAPH, P307, DOI 10.1145/311535.311574
   Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Bittner J, 2003, ENVIRON PLANN B, V30, P729, DOI 10.1068/b2957
   Bittner J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531400
   BORGEAT L, 2005, SIGGRAPH 05, P869
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   Cignoni P, 2003, IEEE T VIS COMPUT GR, V9, P525, DOI 10.1109/TVCG.2003.1260746
   CIGNONI P, 2003, VIS 03, P20
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Debevec P., 1998, Efficient view-dependent image-based rendering with projective texture-mapping
   Décoret X, 2003, ACM T GRAPHIC, V22, P689, DOI 10.1145/882262.882326
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   GOBBETTI E, 2005, SIGGRAPH 05, P878
   GUTHE M, 2004, RENDERING TECHNIQUES, P69
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   ISENBURG M, 2003, SIGGRAPH 2003 P, P935
   Isenburg Martin., 2005, IEEE VISUALIZATION, P30
   JESCHKE S, 2002, EGRW 02 P 13 EUR WOR, P181
   LINDSTROM P, 2003, I3D 03, P93
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacDonald J. D., 1990, Visual Computer, V6, P153, DOI 10.1007/BF01911006
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   RUSINKIEWICZ S, 2001, I3D 01, P63
   Sainz M, 2004, COMPUT GRAPH-UK, V28, P869, DOI 10.1016/j.cag.2004.08.014
   SCHAUFLER G, 1996, COMPUT GRAPH FORUM, P227
   Schroeder WJ, 1997, VISUALIZATION '97 - PROCEEDINGS, P205, DOI 10.1109/VISUAL.1997.663883
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   SHADE J, 1996, SIGGRAPH 96 C P, P75
   Wald I, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P33, DOI 10.1109/RT.2007.4342588
   WILLIAM V, 2002, EGRW 02, P203
   Wilson A, 2003, ACM T GRAPHIC, V22, P678, DOI 10.1145/882262.882325
   Wonka P, 2006, ACM T GRAPHIC, V25, P494, DOI 10.1145/1141911.1141914
   Yoon SE, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P131, DOI 10.1109/VISUAL.2004.86
   YOON SE, 2005, SIGGRAPH 05 ACM SIGG, P886
NR 38
TC 6
Z9 9
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 409
EP 419
DI 10.1007/s00371-010-0465-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800003
DA 2024-07-18
ER

PT J
AU Bulbul, A
   Cipiloglu, Z
   Capin, T
AF Bulbul, Abdullah
   Cipiloglu, Zeynep
   Capin, Tolga
TI A color-based face tracking algorithm for enhancing interaction with
   mobile devices
SO VISUAL COMPUTER
LA English
DT Article
DE Face tracking; Human computer interaction; Mobile devices; Motion
   parallax
AB A color-based face tracking algorithm is proposed to be used as a human-computer interaction tool on mobile devices. The solution provides a natural means of interaction enabling a motion parallax effect in applications. The algorithm considers the characteristics of mobile use-constrained computational resources and varying environmental conditions. The solution is based on color comparisons and works on images gathered from the front camera of a device. In addition to color comparisons, the coherency of the facial pixels is considered in the algorithm. Several applications are also demonstrated in this work, which use the face position to determine the viewpoint in a virtual scene, or for browsing large images. The accuracy of the system is tested under different environmental conditions such as lighting and background, and the performance of the system is measured in different types of mobile devices. According to these measurements the system allows for accurate (7% RMS error) face tracking in real time (20-100 fps).
C1 [Bulbul, Abdullah; Cipiloglu, Zeynep; Capin, Tolga] Bilkent Univ, Dept Comp Engn, Ankara, Turkey.
C3 Ihsan Dogramaci Bilkent University
RP Bulbul, A (corresponding author), Bilkent Univ, Dept Comp Engn, Ankara, Turkey.
EM bulbul@cs.bilkent.edu.tr; zeynep@cs.bilkent.edu.tr;
   tcapin@cs.bilkent.edu.tr
RI Bulbul, Abdullah/AAC-6616-2020; Yıldız, Zeynep/KBA-3063-2024; Yildiz,
   Zeynep Cipiloglu/AAF-6305-2020; Yildiz, Zeynep Cipiloglu/T-3389-2017;
   Çapın, Tolga Kurtuluş/G-6172-2018; Capin, Tolga K/K-2683-2012
OI Bulbul, Abdullah/0000-0002-2527-2729; Yildiz, Zeynep
   Cipiloglu/0000-0003-4129-591X; Yildiz, Zeynep
   Cipiloglu/0000-0003-4129-591X; 
FU European Commission [FP7-213349]; TUBITAK
FX This work is supported by the European Commission FP7-213349 All 3D
   Imaging Phone project and TUBITAK.
CR [Anonymous], 2007, P 1 INT CONVENTION R
   BARNARD M, 2007, P 5 INT C COMP VIS S
   BRADSKI GR, 1998, INTEL TECHNOL J, P95
   Brand J, 2000, INT C PATT RECOG, P1056, DOI 10.1109/ICPR.2000.905653
   Bulbul A, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P385, DOI 10.1109/CW.2009.9
   Capin T, 2008, IEEE COMPUT GRAPH, V28, P74, DOI 10.1109/MCG.2008.83
   Çapin T, 2006, LECT NOTES COMPUT SC, V4263, P765
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   HANNUKSELA J, 2007, P 4 EUR C VIS MED PR
   HANNUKSELA J, 2005, P IEEE C COMP VIS PA, V6, P71
   HARO A, 2005, P ICCV HCI 2005 OCT, P79
   HJELMAS BKL, 2001, COMPUTER VISION IMAG, V3, P236
   HUNKE M, 1994, CONF REC ASILOMAR C, P1277, DOI 10.1109/ACSSC.1994.471664
   JAIMES A, 2005, P 11 IEEE INT WORKSH
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   Shirley P., 2002, FUNDAMENTALS COMPUTE, V1st
   Vezhnevets V., 2003, GRAPHICON03, P85
   WARE C, 2004, SPACE PERCEPTION DIS
   Yee K.P., 2003, Proc. CHI'03, P571, DOI DOI 10.1145/642611.642613
NR 19
TC 6
Z9 10
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 311
EP 323
DI 10.1007/s00371-010-0419-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800003
DA 2024-07-18
ER

PT J
AU Lien, JM
   Kurillo, G
   Bajcsy, R
AF Lien, Jyh-Ming
   Kurillo, Gregorij
   Bajcsy, Ruzena
TI Multi-camera tele-immersion system with real-time model driven data
   compression
SO VISUAL COMPUTER
LA English
DT Article
DE 3D tele-immersion; Multi-camera tele-immersion system; Point data
   compression; Model-based compression
ID VIRTUAL-REALITY; 3D VIDEO; ANIMATION
AB Vision-based full-body 3D reconstruction for tele-immersive applications generates large amount of data points, which have to be sent through the network in real time. In this paper, we introduce a skeleton-based compression method using motion estimation where kinematic parameters of the human body are extracted from the point cloud data in each frame. First we address the issues regarding the data capturing and transfer to a remote site for the tele-immersive collaboration. We compare the results of the existing compression methods and the proposed skeleton-based compression technique. We examine the robustness and efficiency of the algorithm through experimental results with our multi-camera tele-immersion system. The proposed skeleton-based method provides high and flexible compression ratios from 50:1 to 5000:1 with reasonable reconstruction quality (peak signal-to-noise ratio from 28 to 31 dB) while preserving real-time (10+ fps) processing.
C1 [Lien, Jyh-Ming] George Mason Univ, Fairfax, VA 22030 USA.
   [Kurillo, Gregorij; Bajcsy, Ruzena] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 George Mason University; University of California System; University of
   California Berkeley
RP Lien, JM (corresponding author), George Mason Univ, Fairfax, VA 22030 USA.
EM jmlien@gmu.edu; gregorij@eecs.berkeley.edu; bajcsy@eecs.berkeley.edu
FU Directorate For Engineering [0941382] Funding Source: National Science
   Foundation; Div Of Electrical, Commun & Cyber Sys [0941382] Funding
   Source: National Science Foundation
CR *AD, 2006, QUICKT 7 0 H 264 IMP
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   [Anonymous], P INT WORKSH IMM TEL
   [Anonymous], GRAPH LAB MOT CAPT D
   [Anonymous], J MULTIMEDIA
   Arikan O, 2006, ACM T GRAPHIC, V25, P890, DOI 10.1145/1141911.1141971
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   CHEUNG KM, 2003, P IEEE C COMP VIS PA
   Demirdjian D, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P267, DOI 10.1109/ICMI.2002.1167005
   DEWAELE G, 2004, ECCV, V1, P495
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Gross M, 2003, ACM T GRAPHIC, V22, P819, DOI 10.1145/882262.882350
   GUMHOLD S, 2005, SIGGRAPH 2005 SKETCH
   Hasenfratz J.-M., 2004, Proceedings of the Tenth Eurographics Conference on Virtual Environments, EGVE'04, (Aire-la-Ville, Switzerland, Switzerland), P147
   Holden MK, 2005, CYBERPSYCHOL BEHAV, V8, P187, DOI 10.1089/cpb.2005.8.187
   Ibarria L., 2003, Dans SCA '03, P126
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kalra P, 1998, IEEE COMPUT GRAPH, V18, P42, DOI 10.1109/38.708560
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Keshner EA, 2004, ASSIST TECHNOL, V16, P54, DOI 10.1080/10400435.2004.10132074
   Keshner Emily A, 2004, J Neuroeng Rehabil, V1, P8, DOI 10.1186/1743-0003-1-8
   KNOOP RDS, 2006, P IEEE INT C ROB AUT
   Kum SU, 2005, ACM T MULTIM COMPUT, V1
   LAMBORAY E, 2004, VR 04, P91
   LANIER J, 2001, SCI AM, V4, P52
   LENGYEL JE, 1999, SI3D 99, P89, DOI DOI 10.1145/300523.300533
   LI L, 2005, VIRTUAL REALITY, V8, P194
   LIEN JM, 2007, P INT S VIS COMP ISV, P347
   LOPEZ EJL, 2006, TRUST NSF SIT VIS
   MASON H, 2006, 2 LIF ED WORKSH SAN, P30
   McComas J., 1998, Virtual Environments in Clinical Psychology and Neuroscience
   MORENCY LP, 2002, P INT C PATT REC
   Mulligan J, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P959, DOI 10.1109/ICIP.2001.958284
   OCHOTTA T, 2004, S POINT BAS GRAPH, P103
   Patel K., 2006, Proceedings of the 9th Annual International Workshop on Presence, Ohio, USA, P87
   PENNEBAKER JB, 1992, JPEG STILL IMAGE DAT
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Robb R, 2002, J VISUAL-JAPAN, V5, P317, DOI 10.1007/BF03182346
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Sattler Mirko, 2005, P ACM SIGGRAPH EUR S, P209
   SIMON DA, 1994, IEEE INT CONF ROBOT, P2235, DOI 10.1109/ROBOT.1994.350953
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Würmlin S, 2004, COMPUT GRAPH-UK, V28, P3, DOI 10.1016/j.cag.2003.10.015
   Yang YJ, 2002, COMPUT SCI ENG, V4, P86, DOI 10.1109/5992.976440
   YANG ZY, 2006, P SPIE ACM MULT COMP
   ZHANG D, 1991, P IEEE RSJ INT WORKS, P292
   Zhang JH, 2004, IEEE DATA COMPR CONF, P508
   Zhao WY, 1996, PATTERN RECOGN, V29, P2115, DOI 10.1016/S0031-3203(96)00051-9
NR 50
TC 23
Z9 24
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2010
VL 26
IS 1
BP 3
EP 15
DI 10.1007/s00371-009-0367-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 526WD
UT WOS:000272325300001
DA 2024-07-18
ER

PT J
AU Maurice, X
   Sandholm, A
   Pronost, N
   Boulic, R
   Thalmann, D
AF Maurice, Xavier
   Sandholm, Anders
   Pronost, Nicolas
   Boulic, Ronan
   Thalmann, Daniel
TI A subject-specific software solution for the modeling and the
   visualization of muscles deformations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE Muscle deformation; Finite element modeling; Anatomical human
   visualization
ID FINITE-ELEMENT MODEL; SIMULATION; CONTACT; KNEE
AB Today, to create and to simulate a virtual anatomical version of a subject is useful in the decision process of surgical treatments. The muscular activity is one of the factors which can contribute to abnormal movements such as in spasticity or static contracture. In this paper, we propose a numerical solution, based on the Finite Element (FE) method, able to estimate muscles deformations during contraction. Organized around a finite element solver and a volumetric environment, this solution is made of all the modeling and simulation processes from the discretization of the studied domain to the visualization of the results. The choices of materials and properties of the FE model are also presented such as the hyperelasticity, the contention model based on inter-meshes neighboring nodes pairing, and the estimation of nodal forces based on the subject-specific muscular forces and action lines.
C1 [Maurice, Xavier; Sandholm, Anders; Pronost, Nicolas; Boulic, Ronan; Thalmann, Daniel] Ecole Polytech Fed Lausanne, VRLab, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Pronost, N (corresponding author), Ecole Polytech Fed Lausanne, VRLab, Lausanne, Switzerland.
EM nicolas.pronost@epfl.ch
RI Thalmann, Daniel/A-4347-2008; BOULIC, RONAN/A-9108-2008; Thalmann,
   Daniel/AAL-1097-2020
OI BOULIC, RONAN/0000-0001-9176-6877; Thalmann, Daniel/0000-0002-0451-7491;
   PRONOST, NICOLAS/0000-0003-4499-509X
CR Ansari MZ, 2007, KEY ENG MATER, V345-346, P1241, DOI 10.4028/www.scientific.net/KEM.345-346.1241
   Beillas P, 2001, Stapp Car Crash J, V45, P469
   Belytschko T., 2007, Meshfree and particle methods
   Belytschko Ted., 2006, Nonlinear Finite Elements for Continua and Structures
   BLANKEVOORT L, 1991, J BIOMECH, V24, P1019, DOI 10.1016/0021-9290(91)90019-J
   Büchler P, 2002, CLIN BIOMECH, V17, P630, DOI 10.1016/S0268-0033(02)00106-7
   Crisfield M., 1997, Nonlinear finite element analysis of solids and structures, V1
   DELP SL, 1990, IEEE T BIO-MED ENG, V37, P757, DOI 10.1109/10.102791
   Donzelli PS, 1999, J BIOMECH, V32, P1037, DOI 10.1016/S0021-9290(99)00106-2
   Hirokawa S, 1997, MED ENG PHYS, V19, P637, DOI 10.1016/S1350-4533(96)00077-X
   JAMES D, 2002, ACM T GRAPH SIGGRAPH, V21
   Jonkers I, 2002, J BIOMECH, V35, P609, DOI 10.1016/S0021-9290(01)00240-8
   KEYAK JH, 1992, J BIOMED ENG, V14, P483, DOI 10.1016/0141-5425(92)90100-Y
   MacDonald B.J., 2007, PRACTICAL STRESS ANA
   Macosko C.W., 1994, Rheology Principles, Measurements, and Applications
   Mow VC., 2000, Orthopaedic basic science. Biology of the musculoskeletal system, P133
   Muller M., 2001, Comput._Anim._and_Sim._'01, P99, DOI [10.1145/634067.634128, DOI 10.1145/634067.634128]
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   Papaioannou G, 2008, J BIOMECH, V41, P2633, DOI 10.1016/j.jbiomech.2008.06.027
   Scheepers F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P163, DOI 10.1145/258734.258827
   SCHMID J, 2009, MUSCULOSKELETAL SIMU
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Tang CY, 2007, INT J MECH SCI, V49, P1179, DOI 10.1016/j.ijmecsci.2007.02.002
   THALMANN D, 1996, P COMP GRAPH INT
   Winters J.M., 1990, Multiple Muscle Systems: Biomechanics and Movement Organization, P69
NR 25
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 835
EP 842
DI 10.1007/s00371-009-0313-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100003
DA 2024-07-18
ER

PT J
AU Trebien, F
   Oliveira, MM
AF Trebien, Fernando
   Oliveira, Manuel M.
TI Realistic real-time sound re-synthesis and processing for interactive
   virtual worlds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Recursive filters; Real-time audio processing; Linear digital filters;
   GPU-based techniques
ID ALGORITHMS
AB We present new GPU-based techniques for implementing linear digital filters for real-time audio processing. Our solution for recursive filters is the first presented in the literature. We demonstrate the relevance of these algorithms to computer graphics by synthesizing realistic sounds of colliding objects made of different materials, such as glass, plastic, and wood, in real time. The synthesized sounds can be parameterized by the object materials, velocities, and collision angles. Despite its flexibility, our approach uses very little memory, since it essentially requires a set of coefficients representing the impulse response of each material sound. Such features make our approach an attractive alternative to traditional CPU-based techniques that use playback of pre-recorded sounds.
C1 [Trebien, Fernando; Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Oliveira, MM (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Av Bento Goncalves 9500,Caixa Postal 15-064, Porto Alegre, RS, Brazil.
EM ftrebien@inf.ufrgs.br; oliveira@inf.ufrgs.br
RI Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Menezes de Oliveira Neto, Manuel/0000-0003-4957-9984
CR [Anonymous], 2008, NVIDIA CUDA Programming Guide
   ANSARI M, 2003, GAM DEV C
   Antoniou A., 1980, DIGITAL FILTERS ANAL
   *ASIO, ALL YOU NEED KNOW AS
   BARCELLOS A, 2009, ALFREDS WORLD 3D PHY
   Begault DurandR., 1994, 3-D sound for virtual reality and multimedia
   Bierens L, 1998, J VLSI SIG PROCESS S, V18, P51, DOI 10.1023/A:1007993310185
   Bonneel N., 2008, P ACM SIGGRAPH, DOI [10.1145/1399504.1360623, DOI 10.1145/1399504.1360623]
   DELSARTE P, 1986, IEEE T ACOUST SPEECH, V34, P470, DOI 10.1109/TASSP.1986.1164830
   Eyre J, 2000, IEEE SIGNAL PROC MAG, V17, P43, DOI 10.1109/79.826411
   GALLO E, 2003, BREAKING 64 SPATIALI
   GALLO E, 2004, EL P ACM WORKSH GEN, pC42
   GARCIA G, 2002, P AUD ENG SOC
   Govindaraju NK, 2007, PARALLEL COMPUT, V33, P663, DOI 10.1016/j.parco.2007.09.006
   HARRINGTON J, 1999, TECHNIQUES SPEECH AC, P211
   Harris M., 2005, GPU GEMS, V2, P493
   HOUSTON M, 2007, GPGPU COURSE
   JARGSTORFF F, 2004, GPU GEMS, P445
   JEDRZEJEWSKI M, 2004, INT C COMP VIS GRAPH
   Moreland Kenneth, 2003, P ACM SIGGRAPH EUROG, V117, P112, DOI DOI 10.2312/EGGH.EGGH03.112-119
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   ROBELLY J, 2004, IEEE INT C AC SPEECH, V5, P165
   SPITZER J, NVID COURS SIGGRAPH
   Sumanaweera T., 2005, GPU GEMS 2, P765
   Trebien F., 2008, REAL TIME AUDIO PROC, P583
   ZHANG Q, 2005, LNCS, P328
NR 27
TC 9
Z9 13
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 469
EP 477
DI 10.1007/s00371-009-0341-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300011
DA 2024-07-18
ER

PT J
AU Jin, XG
   Tai, CL
   Zhang, HL
AF Jin, Xiaogang
   Tai, Chiew-Lan
   Zhang, Hailin
TI Implicit modeling from polygon soup using convolution
SO VISUAL COMPUTER
LA English
DT Article
DE Convolution surfaces; Implicit surfaces; Ray tracing
ID SURFACES
AB We present a novel method for creating implicit surfaces from polygonal models. The implicit function is defined by convolving a kernel with the triangles in the polygonal model. By adopting a piecewise quartic polynomial kernel function with a finite support, we derive a convolution model that has a closed-form solution, and thus can be efficiently evaluated. The user only needs to specify an effective radius of influence to generate an implicit surface of desired closeness to the polygonal model. The resulting implicit surface is fast to evaluate, not requiring accumulating evaluation results using any hierarchical data structure, and can be efficiently ray-traced to reveal the detailed features.
C1 [Jin, Xiaogang; Zhang, Hailin] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Tai, Chiew-Lan] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM jin@cad.zju.edu.cn; taicl@cse.ust.hk
FU Natural Science Foundation of Zhejiang Province [R105431]; National
   Natural Science Foundation of China [60573153]; China 863 Program
   [2006AA01Z314]; Research Grant Council of the Hong Kong Special
   Administrative Region, China [HKUST6295/04E]
FX We would like to thank Hui Zhao for his help in making the examples in
   Fig. 5 and reporting the rendering times in Table 1. This work is
   supported in part by grants from the Natural Science Foundation of
   Zhejiang Province (R105431), the National Natural Science Foundation of
   China (60573153), the China 863 Program (2006AA01Z314) and the Research
   Grant Council of the Hong Kong Special Administrative Region, China
   (HKUST6295/04E).
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   Bloomenthal J, 1997, COMPUT GRAPH FORUM, V16, P31, DOI 10.1111/1467-8659.114
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   CaniGascuel MP, 1997, IEEE T VIS COMPUT GR, V3, P39, DOI 10.1109/2945.582343
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   Hornus S, 2003, VISUAL COMPUT, V19, P94, DOI 10.1007/s00371-002-0179-6
   Jin XG, 2002, VISUAL COMPUT, V18, P530, DOI 10.1007/s00371-002-0161-3
   Kalra D., 1989, P SIGGRAPH 89, P297
   Kanai T., 2006, P 4 EUR S GEOM PROC, P21
   McCormack J, 1998, COMPUT GRAPH FORUM, V17, P113, DOI 10.1111/1467-8659.00232
   Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585
   Nishimura H., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P718
   NISHITA T, 1994, COMPUT GRAPH FORUM, V13, pC271, DOI 10.1111/1467-8659.1330271
   Oeltze S, 2005, IEEE T MED IMAGING, V24, P540, DOI 10.1109/TMI.2004.843196
   Ohtake Y, 2003, P SHAP MOD INT, P292
   SEDERBERG TW, 1989, P SIGGRAPH 89, P145
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Sherstyuk A, 1999, VISUAL COMPUT, V15, P171, DOI 10.1007/s003710050170
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Wyvill B., 1989, Visual Computer, V5, P75, DOI 10.1007/BF01901483
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
   Yngve G, 2002, IEEE T VIS COMPUT GR, V8, P346, DOI 10.1109/TVCG.2002.1044520
NR 25
TC 18
Z9 21
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 279
EP 288
DI 10.1007/s00371-008-0267-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200007
DA 2024-07-18
ER

PT J
AU Tang, M
   Yoon, SE
   Manocha, D
AF Tang, Min
   Yoon, Sung-Eui
   Manocha, Dinesh
TI Adjacency-based culling for continuous collision detection
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE adjacency-based culling; continuous collision detection; elementary
   test; duplication elimination
ID OBJECTS; MODELS
AB We present an efficient approach to reduce the number of elementary tests for continuous collision detection between rigid and deformable models. Our algorithm exploits connectivity information and uses the adjacency relationships between triangles to perform hierarchical culling. This can be combined with table-based lookups to eliminate duplicate elementary tests. In practice, our approach can reduce the number of elementary tests by two orders of magnitude. We demonstrate the performance of our algorithm on various challenging rigid body and deformable simulations.
C1 [Tang, Min] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Yoon, Sung-Eui] Korea Adv Inst Sci & Technol, Taejon 305701, South Korea.
   [Tang, Min; Manocha, Dinesh] Univ N Carolina, Chapel Hill, NC USA.
C3 Zhejiang University; Korea Advanced Institute of Science & Technology
   (KAIST); University of North Carolina; University of North Carolina
   Chapel Hill
RP Tang, M (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM tang_m@zju.edu.cn; sungeui@cs.kaist.ac.kr; dm@cs.unc.edu
RI Tang, Min/KOC-3090-2024; Yoon, Sung-eui/C-1678-2011
OI Manocha, Dinesh/0000-0001-7047-9801
CR [Anonymous], 2007, P 18 EUROGRAPHICS C
   [Anonymous], 1997, J GRAPH TOOLS, DOI DOI 10.1080/10867651.1997.10487480
   [Anonymous], P IEEE C ROB AUT
   BARAFF D, 2003, P ACM SIGGRAPH, P862
   CURTIS S, 2008, SI3D 08, P61
   FOSKEY M, 2001, P IEEE RSJ INT C INT, P67
   GOTTSCHALK S, 1996, P ACM SIGGR, V96, P171, DOI DOI 10.1145/237170.237244
   Govindaraju NK, 2005, ACM T GRAPHIC, V24, P991, DOI 10.1145/1073204.1073301
   HUTTER M, 2007, P WSCG 07, P25
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Larsson T, 2006, COMPUT GRAPH-UK, V30, P450, DOI 10.1016/j.cag.2006.02.011
   Lauterbach C, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P39
   LaValle S. M., 2006, Planning algorithms
   Lin M., 2003, HDB DISCRETE COMPUTA
   PISULA C, 2000, P WORKSH ALG FDN ROB, P279
   Provot X, 1997, GRAPHICS INTERFACE, P177, DOI DOI 10.1007/978-3-7091-6874-5_13
   Redon S, 2002, COMPUT GRAPH FORUM, V21, P279, DOI 10.1111/1467-8659.t01-1-00587
   Redon S., 2004, P 9 ACM S SOLID MODE, P145
   Sud A, 2004, COMPUT GRAPH FORUM, V23, P557, DOI 10.1111/j.1467-8659.2004.00787.x
   TANG M, 2008, P SPM08 ACM SOL PHYS
   Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x
   VOLINO P, 1994, COMPUT GRAPH FORUM, V13, pC155, DOI 10.1111/1467-8659.1330155
   Wong W. S. -K., 2006, P ACM VRCIA, P181
   Wong WSK, 2005, IEEE T VIS COMPUT GR, V11, P329, DOI 10.1109/TVCG.2005.44
   ZHANG L, 2008, 08001 TR DEP COMP SC
   Zhang XY, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239466
NR 26
TC 16
Z9 21
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 545
EP 553
DI 10.1007/s00371-008-0235-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Reniers, D
   Telea, A
AF Reniers, Dennie
   Telea, Alexandru
TI Hierarchical part-type segmentation using voxel-based curve skeletons
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 9th International Conference on Shape Modeling and Applications
CY JUN 13-15, 2007
CL Lyon, FRANCE
SP ACM SIGRAPT, CNRS, Groupement Rech Informat Mathemat, Reg Rhone Alpes, Univ Claude Bernard Lyon 1
DE shape segmentation; curve skeletons
AB We present an effective framework for segmenting 3D shapes into meaningful components using the curve skeleton. Our algorithm identifies a number of critical points on the efficiently computed curve skeleton, either fully automatically as the junctions of the curve skeleton, or based on user input. We use these points to construct a partitioning of the object surface using geodesics. Because the segmentation is based on the curve skeleton, it intrinsically reflects the shape symmetry and articulation, and can handle shapes with tunnels. We describe a voxel-based implementation of our method which is robust and noise resistant, able to handle shapes of complex articulation and topology, produces smooth segment borders, and delivers hierarchical level-of-detail segmentations. We demonstrate the framework on various real-world 3D shapes. Additionally, we discuss the use of both curve and surface skeletons to produce part-type and patch-type, respectively, segmentations of 3D shapes.
C1 [Reniers, Dennie] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5600 MB Eindhoven, Netherlands.
   [Telea, Alexandru] Univ Groningen, Inst Math & Comp Sci, NL-9700 AB Groningen, Netherlands.
C3 Eindhoven University of Technology; University of Groningen
RP Reniers, D (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, POB 513, NL-5600 MB Eindhoven, Netherlands.
EM d.reniers@tue.nl; a.c.telea@rug.nl
CR Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Clarenz U, 2004, VISUAL COMPUT, V20, P329, DOI 10.1007/s00371-004-0245-3
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Costa L., 2001, SHAPE ANAL CLASSIFIC
   Dey TK, 2006, S GEOMETRY PROCESSIN, P143
   Giblin P, 2004, IEEE T PATTERN ANAL, V26, P238, DOI 10.1109/TPAMI.2004.1262192
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KIRYATI N, 1993, PATTERN RECOGN, V26, P1623, DOI 10.1016/0031-3203(93)90018-R
   KONG TY, 1989, COMPUT VISION GRAPH, V48, P357, DOI 10.1016/0734-189X(89)90147-3
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   MULLIKIN JC, 1992, CVGIP-GRAPH MODEL IM, V54, P526, DOI 10.1016/1049-9652(92)90072-6
   Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267
   RENIERS D, 2008, ADV COMPUTER GRAPHIC, V4, P187
   Reniers D, 2008, IEEE T VIS COMPUT GR, V14, P355, DOI 10.1109/TC.2007.70786
   Reniers D, 2008, LECT NOTES COMPUT SC, V4992, P262, DOI 10.1007/978-3-540-79126-3_24
   Reniers D, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P179, DOI 10.1109/SMI.2007.33
   Shamir A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P82
   Shilane P., 2004, Shape Modeling International
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   West D.B., 1996, Introduction to Graph Theory, V2
NR 24
TC 9
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2008
VL 24
IS 6
BP 383
EP 395
DI 10.1007/s00371-008-0220-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 299FP
UT WOS:000255741800002
DA 2024-07-18
ER

PT J
AU Di Fiore, F
   Van Reeth, F
   Patterson, J
   Willis, P
AF Di Fiore, Fabian
   Van Reeth, Frank
   Patterson, John
   Willis, Philip
TI Highly stylised animation
SO VISUAL COMPUTER
LA English
DT Article
DE paint systems; interaction techniques; highly stylised modelling and
   animation
AB In this paper we argue for our NPAR system as an effective 2D alternative to most NPR research, which is focused on frame coherent stylised rendering of 3D models. Our approach gives a highly stylised look to images without the support of 3D models. Nevertheless, they still behave as though they are animated by drawing, which they are.
   First, a stylised brush tool is used to freely draw extreme poses of characters. Each character is built of 2D drawn brush strokes which are manually grouped into layers. Each layer is assigned its place in a drawing hierarchy called a hierarchical display model (HDM). Next, multiple HDMs are created for the same character, each corresponding to a specific view. A collection of HDMs essentially reintroduces some correspondence information to the 2D drawings needed for inbetweening and, in effect, eliminates the need for a true 3D model.
   Once the models are composed the animator starts by defining keyframes from extreme poses in time. Next, brush stroke trajectories defined by the keyframe HDMs are inbetweened automatically across intermediate frames. Finally, each HDM of each generated inbetween frame is traversed and all elements are drawn one on another from back to front.
   Our techniques support highly rendered styles which are particularly difficult to animate by traditional means including the 'airbrushed', scraperboard, watercolour, Gouache, 'ink-wash', pastel, and the 'crayon' styles. In addition, we describe the data path to be followed to create highly stylised animations by incorporating real footage.
   We believe our system offers a new fresh perspective on computer-aided animation production and associated tools.
C1 [Di Fiore, Fabian; Van Reeth, Frank] Hasselt Univ, Expertise Ctr Digital Media, Transnationale Univ Limburg, B-3590 Diepenbeek, Belgium.
   [Patterson, John] Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Willis, Philip] Univ Bath, Dept Comp Sci, Media Technol Res Ctr, Bath BA2 7AY, Avon, England.
C3 Hasselt University; University of Glasgow; University of Bath
RP Di Fiore, F (corresponding author), Hasselt Univ, Expertise Ctr Digital Media, Transnationale Univ Limburg, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM fabian.difiore@uhasselt.be; frank.vanreeth@uhasselt.be;
   jwp@dcs.gla.ac.uk; P.J.Willis@bath.ac.uk
OI VAN REETH, Frank/0000-0002-3705-7807; Di Fiore,
   Fabian/0000-0003-4908-0673
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764
   Blair P., 1994, CARTOON ANIMATION
   BURTNYK N, 1971, J SMPTE, V80, P149
   CATMULL E, 1978, PROBLEMS COMPUTER AS, V12, P348
   COHEN JM, 2000, NPAR 2000, P83
   DANIELS E, 1999, C ABSTR APPL SIGGRAP, P200, DOI DOI 10.1145/311625.312010
   DECAUDIN P, 1996, THESIS U TECHNOLOGIE
   DIFIORE F, 2003, P GRAPHICON INT C CO, P124
   FLERACKERS C, 2002, THESIS TRANSNATIONAL
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Hertzmann A., 2000, NPAR, P7
   Kalnins R., 2002, P 29 ANN C COMPUTER, P755, DOI DOI 10.1145/566570.566648
   KORT A, 2002, NPAR2002 S NONPH AN, P125
   Lake C., 2000, Proceedings of the first international symposium on Non-photorealistic animation and rendering-NPAR'00, P13, DOI 10.1145/340916.3409185[27]M.S.
   MEIER BJ, 1996, P SIGGRAPH 1996, V25, P477
   Northrup J. D., 2000, Proceedings of the 1st International Symposium on Non-photorealistic Animation and Rendering, P31, DOI DOI 10.1145/340916.340920
   PATTERSON JW, 1994, COMPUT J, V37, P829, DOI 10.1093/comjnl/37.10.829
   Rademacher P, 1999, COMP GRAPH, P439, DOI 10.1145/311535.311612
   Ranjan V, 1996, COMPUT GRAPH FORUM, V15, pC129, DOI 10.1111/1467-8659.1530129
   Reeves W. T., 1981, Computer Graphics, V15, P263, DOI 10.1145/965161.806814
   SEDERBERG TW, 1992, COMP GRAPH, V26, P25, DOI 10.1145/142920.134001
   SHAPIRA M, 1995, IEEE COMPUT GRAPH, V15, P44, DOI 10.1109/38.365005
   VANDENBERGH J, 2002, P 1 IB AM S COMP GRA, P315
   VANHAEVRE W, 2007, VISUAL COMPUTER COMP
   VANLAERHOEVEN T, 2005, COMPUTER ANIMATION V, P429
   Vansichem G., 2001, Proceedings of the IASTED International Conference Computer Graphics and Imaging, P44
   YU JH, 1996, P EUR UK CHAPT 14 AN, P75
NR 28
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 105
EP 123
DI 10.1007/s00371-007-0189-5
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200004
DA 2024-07-18
ER

PT J
AU Huang, X
   Li, S
   Wang, G
AF Huang, Xin
   Li, Sheng
   Wang, Guoping
TI Displacement modeling: Hardware-accelerated interactive feature modeling
   on subdivision surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE displacement modeling; view-dependent adaptive subdivision; interactive
   design; fine level features; GPU
ID MESHES
AB Feature modeling on subdivision surfaces remains a challenging task for interactive 3D design. This paper presents the idea of displacement modeling, which utilizes displacement mapping as an interactive modeling method to design fine level features by exploiting the computation power of modern programmable graphics hardware (GPU). We also propose a view-dependent adaptive subdivision method according to the error metric in view space. It can highly reduce the number of refined faces and points while maintaining the same visual quality as uniform subdivision. Furthermore, several feature adjustment tools are introduced for flexible design and manipulation of created features. Since the displacement modeling approach is fully implemented on graphics hardware, it can substantially alleviate the computing load on CPU and significantly reduce the data transmission on the graphics channel.
C1 Peking Univ, HCI, Beijing, Peoples R China.
   Peking Univ, Multimedia Lab, Beijing, Peoples R China.
C3 Peking University; Peking University
RP Wang, G (corresponding author), Peking Univ, HCI, Beijing, Peoples R China.
EM hx@graphics.pku.edu.cn; lisheng@graphics.pku.edu.cn;
   wgp@graphics.pku.edu.cn
CR Biermann H, 2002, GRAPH MODELS, V64, P61, DOI 10.1006/gmod.2002.0570
   BIERMANN H, 2002, P ACM SIGGRAPH 02, P312, DOI DOI 10.1145/566570.566583
   Bischoff S, 2004, COMPUT AIDED DESIGN, V36, P1483, DOI 10.1016/j.cad.2003.11.007
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   Bolz J., 2002, Proc. Web3D '02, P11
   Bolz J., EVALUATION SUBDIVISI, P2
   Bunnell M., 2005, GPU GEMS 2, P109
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   CHENG FH, 2006, P 24 COMP GRAPH INT, P404
   COOK RL, 1984, P 11 ANN C COMP GRAP, P223, DOI DOI 10.1145/800031.808602
   Elber G, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P156, DOI 10.1109/PCCGA.2002.1167850
   HARRIS M, 2005, 39 SIGGRAPH
   Hirche J, 2004, PROC GRAPH INTERF, P153
   Ji JF, 2006, VISUAL COMPUT, V22, P424, DOI 10.1007/s00371-006-0020-8
   Kähler KA, 2003, VISUAL COMPUT, V19, P310, DOI 10.1007/s00371-002-0185-8
   KANAI T, 2004, ACM WORKSH GEN PURP
   KHODAKOVSKY A., 1999, Proceedings of the 1999 ACM Symposium on Solid and Physical Modeling, P203
   Lai SH, 2005, INT C COMP AID DES C, P125
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Pernot J-P., 2003, P 8 ACM S SOLID MODE, P270
   SETTGAST V, 2003, P OPENSG S DARMST GE, P39
   SHIUE LJ, 2005, P SIGGRAPH 2005, P1010
   SMITH J, 2004, THESIS  U CALIFORNIA
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Wang X., 2004, EUR WORKSH REND, DOI [10.2312/EGWR/EGSR04/227-233, DOI 10.2312/EGWR/EGSR04/227-233]
   WU X, 2005, PANSYSTEMS THEORY HI, P1
   Yasui Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P129, DOI 10.1109/SMI.2004.1314500
   ZORIN D, 2000, SIGGRAPH 2000 COURSE, V23, P11
NR 28
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 861
EP 872
DI 10.1007/s00371-007-0138-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600026
DA 2024-07-18
ER

PT J
AU Ignacio, UA
   Jung, CR
AF Ignacio, Ubirata A.
   Jung, CIdudio R.
TI Block-based image inpainting in the wavelet domain
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE image inpainting; wavelets; texture synthesis; image processing
ID COMPLETION
AB This paper introduces a new model for block-based image inpainting in the wavelet domain. The proposed technique separates the inpainting process into two different and important steps, both using the energy of wavelet coefficients. First, the model explores wavelet detail coefficients to estimate the image gradient vector, weighting each vector with the energy of wavelet coefficients. Such information is then used to determine which block belonging to the inpainting region should be filled first. After that, an adapted method for texture synthesis in the wavelet domain is applied in order to successfully fill this block. These two steps are applied successively, until the inpainting region is completely filled. Experimental results indicate that the proposed algorithm can fill large inpainting regions with good visual quality, presenting results comparable to or better than other competitive approaches for image inpainting.
C1 Unisinos Univ Vale Rio Dos Sinos, Grad Sch Appl Comp, BR-93022000 Sao Leopoldo, Brazil.
C3 Universidade do Vale do Rio dos Sinos (Unisinos)
RP Ignacio, UA (corresponding author), Unisinos Univ Vale Rio Dos Sinos, Grad Sch Appl Comp, Av Unisinos 950, BR-93022000 Sao Leopoldo, Brazil.
EM biraai@terra.com.br; crjung@unisinos.br
RI Jung, Claudio R/G-2439-2012
CR [Anonymous], 1979, Organization in vision
   [Anonymous], P ACM SIGGRAPH
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   CHAN TF, 2001, TOTAL VARIATION WAVE
   Chen Y, 2006, IEEE IMAGE PROC, P1997, DOI 10.1109/ICIP.2006.312890
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V4, P1509, DOI 10.1109/83.469932
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016
   Meyer Y., 2001, Memoirs of the American Mathematical Society
   Nitzberg M., 1993, Filtering, Segmentation and Depth
   Patwardhan KA, 2003, IEEE IMAGE PROC, P857
   Pessoa L, 1998, BEHAV BRAIN SCI, V21, P723
   Rane SD, 2002, IEEE IMAGE PROC, P309
   SARKAR S, 1993, IEEE T PATTERN ANAL, V15, P256, DOI 10.1109/34.204907
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Tonietto L, 2006, COMPUT GRAPH FORUM, V25, P675, DOI 10.1111/j.1467-8659.2006.00989.x
   Tonietto L, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P383
   Tschumperlé D, 2005, IEEE T PATTERN ANAL, V27, P506, DOI 10.1109/TPAMI.2005.87
   Vese LA, 2003, J SCI COMPUT, V19, P553, DOI 10.1023/A:1025384832106
NR 22
TC 17
Z9 17
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 733
EP 741
DI 10.1007/s00371-007-0139-2
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600013
DA 2024-07-18
ER

PT J
AU Egges, A
   Papagiannakis, G
   Magnenat-Thalmann, N
AF Egges, Arjan
   Papagiannakis, George
   Magnenat-Thalmann, Nadia
TI Presence and interaction in mixed reality environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW 2006)
CY NOV 28-29, 2006
CL Lausanne, SWITZERLAND
SP EPFL, VRlab
DE presence; interaction; animation; real-time rendering; mixed reality
AB In this paper, we present a simple and robust mixed reality (MR) framework that allows for real-time interaction with virtual humans in mixed reality environments under consistent illumination. We will look at three crucial parts of this system: interaction, animation and global illumination of virtual humans for an integrated and enhanced presence. The interaction system comprises of a dialogue module, which is interfaced with a speech recognition and synthesis system. Next to speech output, the dialogue system generates face and body motions, which are in turn managed by the virtual human animation layer. Our fast animation engine can handle various types of motions, such as normal key-frame animations, or motions that are generated on-the-fly by adapting previously recorded clips. Real-time idle motions are an example of the latter category. All these different motions are generated and blended on-line, resulting in a flexible and realistic animation. Our robust rendering method operates in accordance with the previous animation layer, based on an extended for virtual humans precomputed radiance transfer (PRT) illumination model, resulting in a realistic rendition of such interactive virtual characters in mixed reality environments. Finally, we present a scenario that illustrates the interplay and application of our methods, glued under a unique framework for presence and interaction in MR.
C1 Univ Utrecht, Dept Informat & Comp Sci, Ctr Adv Gaming & Simulat, NL-3508 TC Utrecht, Netherlands.
   Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
C3 Utrecht University; University of Geneva
RP Egges, A (corresponding author), Univ Utrecht, Dept Informat & Comp Sci, Ctr Adv Gaming & Simulat, NL-3508 TC Utrecht, Netherlands.
EM egges@cs.uu.nl; george@miralab.unige.ch; thalmann@miralab.unige.ch
RI papagiannakis, george/AAI-7973-2020; Thalmann, Nadia/AAK-5195-2021
OI papagiannakis, george/0000-0002-2977-9850; Thalmann,
   Nadia/0000-0002-1459-5960
CR Amanatides J., 1987, EUROGRAPHICS, V87, P3
   [Anonymous], P AISB 2004 S LANG S
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   BALCISOY SS, 2001, THESIS EPFL
   Cassell J, 2001, COMP GRAPH, P477, DOI 10.1145/383259.383315
   CAVAZZA M, 2003, P 2 INT C VIRT STOR, P189
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   Egges A, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P121, DOI 10.1109/PCCGA.2004.1348342
   Egges A, 2004, COMPUT ANIMAT VIRT W, V15, P1, DOI 10.1002/cav.3
   Egges A., 2005, Proceedings of the First International Workshop on Crowd Simulation, P31
   GARCHERY S, 2004, THESIS U GENEVA
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   *H AN HUM AN WORK, SPEC STAND HUM
   Hartmann B, 2002, COMP ANIM CONF PROC, P111, DOI 10.1109/CA.2002.1017516
   Inui T., 1990, Group theory and its applications in physics
   Ivanic J, 1998, J PHYS CHEM A, V102, P9099, DOI 10.1021/jp9833350
   KAUTZ J, 2005, ACM SIGGRAPH 05 COUR
   Kopp S, 2004, COMPUT ANIMAT VIRT W, V15, P39, DOI 10.1002/cav.6
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KOVAR L, 2003, P ACM SIGGRAPH EUR S, P214
   Kshirsagar S, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P38, DOI 10.1109/CGI.2001.934656
   *MICR, MICR SPEECH SDK VERS
   MUELLER M, 2005, P SIGGRAPH 05, P677
   PAPAGIANNAKIS G, 2005, P VIRT SYST MULT 200, P189
   PAPAGIANNAKIS G, 2005, IEEE VR2005 WORKSH V
   PERLIN K, 1995, IEEE T VIS COMPUT GR, V1, P5, DOI 10.1109/2945.468392
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Poggi I, 2005, TEXT SPEECH LANG TEC, V27, P3, DOI 10.1007/1-4020-3051-7_1
   Ponder M, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P96
   RAMAMOORTHI R, 2001, P SIGGRAPH 01
   Ren Z, 2006, ACM T GRAPHIC, V25, P977, DOI 10.1145/1141911.1141982
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   ROSENWASSER LJ, 1996, LUNG BIOL HEALTH DIS, V96, P147
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Tamura H, 2001, IEEE COMPUT GRAPH, V21, P64, DOI 10.1109/38.963462
   Thomas B, 2000, FOURTH INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, DIGEST OF PAPERS, P139, DOI 10.1109/ISWC.2000.888480
   Unuma M., 1995, P 22 ANN C COMPUTER, P91, DOI DOI 10.1145/218380.218419
   VACCHETTI L, 2004, VIRTUAL REALITY AUGM
   Vlahakis V, 2002, IEEE COMPUT GRAPH, V22, P52, DOI 10.1109/MCG.2002.1028726
   Wiley DJ, 1997, IEEE COMPUT GRAPH, V17, P39, DOI 10.1109/38.626968
   ALICE CHAT BOT
   VIRTUAL HUMAN MARKUP
NR 42
TC 12
Z9 13
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2007
VL 23
IS 5
BP 317
EP 333
DI 10.1007/s00371-007-0113-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 154JS
UT WOS:000245503600004
OA hybrid
DA 2024-07-18
ER

PT J
AU Jin, XG
   Lin, JC
   Wang, CCL
   Feng, JQ
   Sun, HQ
AF Jin, XG
   Lin, JC
   Wang, CCL
   Feng, JQ
   Sun, HQ
TI Mesh fusion using functional blending on topologically incompatible
   sections
SO VISUAL COMPUTER
LA English
DT Article
DE mesh fusion; functional blending; interactive modeling tool
ID RECONSTRUCTION; OPERATIONS; SURFACES
AB Three-dimensional mesh fusion provides an easy and fast way to create new mesh models from existing ones. We introduce a novel approach of mesh fusion in this paper based on functional blending. Our method has no restriction of disk-like topology or one-ring opening on the meshes to be merged. First of all, sections with boundaries of the under-fusing meshes are converted into implicit representations. An implicit transition surface, which joins the sections together while keeping smoothness at the boundaries, is then created based on cubic Hermite functional blending. Finally, the implicit surface is tessellated to form the resultant mesh. Our scheme is both efficient and simple, and with it users can easily construct interesting, complex 3D models.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   Chinese Univ Hong Kong, Dept Automat & Comp Aided Engn, Shatin, Hong Kong, Peoples R China.
   Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Zhejiang University; Chinese University of Hong Kong; Chinese University
   of Hong Kong
RP Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM jin@cad.zju.edu.cn; linjuncong@cad.zju.edu.cn; cwang@acae.cuhk.edu.hk;
   jqfeng@cad.zju.edu.cn; hanqiu@cse.cuhk.edu.hk
RI Wang, Charlie C. L./B-3730-2010
OI Wang, Charlie C. L./0000-0003-4406-8480
CR Adams B, 2003, ACM T GRAPHIC, V22, P651, DOI 10.1145/882262.882320
   [Anonymous], 2017, Elementary Numerical Analysis: an Algorithmic Approach
   [Anonymous], 1960, Introduction to Matrix Analysis
   Bajaj CL, 1996, GRAPH MODEL IM PROC, V58, P524, DOI 10.1006/gmip.1996.0044
   Barequet G, 2000, VISUAL COMPUT, V16, P116, DOI 10.1007/s003710050201
   Barequet G, 1996, COMPUT VIS IMAGE UND, V63, P251, DOI 10.1006/cviu.1996.0018
   BEDI S, 1992, COMPUT AIDED DESIGN, V24, P505, DOI 10.1016/0010-4485(92)90030-E
   Biermann H, 2002, ACM T GRAPHIC, V21, P312, DOI 10.1145/566570.566583
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Elber G, 2005, GRAPH MODELS, V67, P189, DOI 10.1016/j.gmod.2004.06.005
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Fu HB, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P173
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gong G, 2001, VISUAL COMPUT, V17, P199, DOI 10.1007/PL00013409
   Kanai T, 1998, VISUAL COMPUT, V14, P166, DOI 10.1007/s003710050132
   Kanai T, 1999, PROC GRAPH INTERF, P148
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   Singh K, 2001, VISUAL COMPUT, V17, P415
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
NR 26
TC 11
Z9 15
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2006
VL 22
IS 4
BP 266
EP 275
DI 10.1007/s00371-006-0004-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 032DG
UT WOS:000236753500004
DA 2024-07-18
ER

PT J
AU Mao, ZH
   Ma, LH
   Zhao, MX
   Xiao, XZ
AF Mao, ZH
   Ma, LH
   Zhao, MX
   Xiao, XZ
TI SUSAN structure preserving filtering for mesh denoising
SO VISUAL COMPUTER
LA English
DT Article
DE mesh processing; mesh denoising; feature preserving smoothing; SUSAN
   operator; Laplacian smoothing
AB Motivated by the impressive effect of the SUSAN operator for low level image processing and its usage simplicity, we extend it to denoise the 3D mesh. We use the angle between the normals on the surface to determine the SUSAN area; each point has associated itself with the SUSAN area that is has a similar continuity feature to the point. The SUSAN area avoids the feature to be taken as noise effectively, so the SUSAN operator gives the maximal number of suitable neighbors with which to take an average, whilst no neighbors from unrelated regions are involved. Thus, the entire structure can be preserved. We also extend the SUSAN operator to two-ring neighbors by a squared umbrella-operator to improve the surface smoothness with little loss of detailed features. Details of the SUSAN structure preserving noise reduction algorithm are discussed along with the test results in this paper.
C1 Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University
RP Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.
EM mzh_yu@stju.edu.cn; ma-lz@sjtu.edu.cn; zhaomx@sjtu.edu.cn;
   shaw@sjtu.edu.cn
CR Alexa M, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P51, DOI 10.1109/SMI.2002.1003528
   Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   CHOUDHURY P, 2003, P EUR S REND, P186
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Desbrun M, 2000, PROC GRAPH INTERF, P145
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   FLEISHMAN S, 2005, P SIGGRAPH 05
   Halstead M., 1993, P SIGGRAPH 93, P35
   HILDEBRANDT K, 2004, P EUROGRAPHICS 2004, V23
   Jones T., 2003, THESIS MIT
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   Ohtake Y, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P203
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   OHTAKE Y, 1999, J 3 DIMENSIONAL IMAG, V13, P19
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rusinkiewicz S, 2002, ACM T GRAPHIC, V21, P438, DOI 10.1145/566570.566600
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   TAUBIN G, 2000, P EUROGRAPHICS 00
   TAUBIN G, 1996, 4 EUR C COMP VIS ECC, P283
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   WELCH W, 1992, COMP GRAPH, V26, P157, DOI 10.1145/142920.134033
   WELCH W, 1994, P SIGGRAPH 94, P247
NR 28
TC 15
Z9 20
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2006
VL 22
IS 4
BP 276
EP 284
DI 10.1007/s00371-006-0005-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 032DG
UT WOS:000236753500005
DA 2024-07-18
ER

PT J
AU Ferre, M
   Puig, A
   Tost, D
AF Ferre, M
   Puig, A
   Tost, D
TI Decision trees for accelerating unimodal, hybrid and multimodal
   rendering models
SO VISUAL COMPUTER
LA English
DT Article
DE volume rendering; multimodal rendering; hybrid rendering; decision
   trees; run-length encoding
AB This paper deals with the rendering of segmented unimodal, hybrid and aligned multimodal voxel models. We propose a data structure that classifies the segmented voxels into categories, so that whenever the model has to be traversed, only the selected categories are visited and the empty and non-selected voxels are skipped. This strategy is based on: (i) a decision tree, called the rendering decision tree (RDT), which represents the hierarchy of the classification process and (ii) an intermediate run-length encoding (RLE) of the classified voxel model. The traversal of the voxel model given a user query consists of two steps: first, the RDT is traversed and the set of selected categories computed; next, the RLE is visited, but the non-selected runs are skipped and only the voxels of the original model that are codified are accessed in selected runs of the RLE. This strategy has been used to render a voxel model by back-to-front traversal and splatting as well as to construct 3D textures for hardware-driven 3D texture mapping. The results show that the voxel model traversal is significantly accelerated.
C1 URV, Dept Ingn Informat & Matemat, Tarragona, Spain.
   UB, Dept Matemat Aplicades & Informat, Barcelona, Spain.
   UPC, ETSEIB, CREB, Biomed Engn Res Ctr, Barcelona, Spain.
C3 Universitat Rovira i Virgili; University of Barcelona; Universitat
   Politecnica de Catalunya
RP URV, Dept Ingn Informat & Matemat, Tarragona, Spain.
EM maria.ferre@urv.net; anna@maia.ub.es; dani@lsi.upc.es
RI Puig, Anna/ADI-9599-2022; Tost, Dani/H-6289-2015
OI Puig, Anna/0000-0002-2184-2800; Tost, Dani/0000-0001-9619-605X
CR [Anonymous], 2010, ARTIF INTELL
   Cai WL, 1999, COMPUT GRAPH FORUM, V18, pC359, DOI 10.1111/1467-8659.00356
   DOLEISCH H, 2003, P 5 JOINT IEEE TCVG, P239
   Freund J, 1997, VISUALIZATION '97 - PROCEEDINGS, P191, DOI 10.1109/VISUAL.1997.663880
   Garofalakis M., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P335, DOI 10.1145/347090.347163
   GOODMAN RM, 1988, IEEE T INFORM THEORY, V34, P979, DOI 10.1109/18.21221
   Hauser H, 2001, IEEE T VIS COMPUT GR, V7, P242, DOI 10.1109/2945.942692
   Kadosh A, 2003, IEEE T VIS COMPUT GR, V9, P580, DOI 10.1109/TVCG.2003.1260750
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Kreeger K. A., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P191, DOI 10.1109/VISUAL.1999.809887
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   LACROUTE P, 1994, ACM COMPUTER GRAPHIC, V28, P451
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Li W, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P317, DOI 10.1109/VISUAL.2003.1250388
   Livnat Y, 1996, IEEE T VIS COMPUT GR, V2, P73, DOI 10.1109/2945.489388
   MEISSNER M, 1999, ENABLING CLASSIFICAT, P207
   Mroz L, 2001, IEEE VISUAL, P279, DOI 10.1109/VISUAL.2001.964522
   MUELLER K, 2005, SCI VISUALIZATION VI, P131
   Neophytou N, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P97, DOI 10.1109/SWG.2002.1226515
   Puig A, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P489
   Puig A, 2000, SPRING COMP SCI, P125
   Quinlan J.R., 1993, C4.5 : programs for machine learning
   SUBRAMANIAN KR, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P150, DOI 10.1109/VISUAL.1990.146377
   Tiede U, 1998, VISUALIZATION '98, PROCEEDINGS, P255, DOI 10.1109/VISUAL.1998.745311
   TZENG F.Y., 2003, VISUALIZATION 2003, P16
   UDUPA JK, 1993, IEEE COMPUT GRAPH, V13, P58, DOI 10.1109/38.252558
   VEGA F, 2005, P SPIE MED IM 2005, P13
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
   WILHEMS J, 1994, P ACM S VOLUME VISUA, V11, P27
   ZUIDERVELD KJ, 1992, P SOC PHOTO-OPT INS, V1808, P324, DOI 10.1117/12.131088
NR 31
TC 5
Z9 6
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2006
VL 22
IS 3
BP 158
EP 167
DI 10.1007/s00371-006-0373-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 028UP
UT WOS:000236514600002
DA 2024-07-18
ER

PT J
AU Cornea, ND
   Silver, D
   Yuan, XS
   Balasubramanian, R
AF Cornea, ND
   Silver, D
   Yuan, XS
   Balasubramanian, R
TI Computing hierarchical curve-skeletons of 3D objects
SO VISUAL COMPUTER
LA English
DT Article
DE 3D curve-skeleton; repulsive force field
ID TOPOLOGY; REGISTRATION; GENERATION; SHAPE
AB A curve-skeleton of a 3D object is a stick-like figure or centerline representation of that object. It is used for diverse applications, including virtual colonoscopy and animation. In this paper, we introduce the concept of hierarchical curve-skeletons and describe a general and robust methodology that computes a family of increasingly detailed curve-skeletons. The algorithm is based upon computing a repulsive force field over a discretization of the 3D object and using topological characteristics of the resulting vector field, such as critical points and critical curves, to extract the curve-skeleton. We demonstrate this method on many different types of 3D objects (volumetric, polygonal and scattered point sets) and discuss various extensions of this approach.
C1 Rutgers State Univ, Dept Elect & Comp Engn, Piscataway, NJ 08855 USA.
C3 Rutgers University System; Rutgers University New Brunswick
RP Rutgers State Univ, Dept Elect & Comp Engn, Piscataway, NJ 08855 USA.
EM cornea@ece.rutgers.edu; silver@ece.rutgers.edu;
   xiaosong@ece.rutgers.edu; balaiitm@ece.rutgers.edu
RI Raman, Balasubramanian/D-1282-2012
CR ABDELHAMID GH, 1994, IEEE IMAGE PROC, P949, DOI 10.1109/ICIP.1994.413249
   Ahuja N, 1997, IEEE T PATTERN ANAL, V19, P169, DOI 10.1109/34.574801
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 2002, P 2002 ACM SIGGRAPHE
   Aylward SR, 2003, INT J COMPUT VISION, V55, P123, DOI 10.1023/A:1026126900358
   Aylward SR, 2002, IEEE T MED IMAGING, V21, P61, DOI 10.1109/42.993126
   BERGER MJ, 1984, J COMPUT PHYS, V53, P484, DOI 10.1016/0021-9991(84)90073-1
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Bouix S, 2000, LECT NOTES COMPUT SC, V1842, P603
   Brennecke A., 2004, SIMULATION VISUALIZA, P299
   Chuang JH, 2000, IEEE T PATTERN ANAL, V22, P1241
   DICKINSON RR, 1991, IBM J RES DEV, V35, P59, DOI 10.1147/rd.351.0059
   *DISCR, 3D STUD MAX 3DSMAX
   Fang SF, 2000, COMPUT GRAPH-UK, V24, P433, DOI 10.1016/S0097-8493(00)00038-8
   Gagvani N, 2001, GRAPH MODELS, V63, P443, DOI 10.1006/gmod.2001.0557
   Gagvani N, 1999, GRAPH MODEL IM PROC, V61, P149, DOI 10.1006/gmip.1999.0495
   Globus A., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P33, DOI 10.1109/VISUAL.1991.175773
   Grigorishin T, 1998, PATTERN ANAL APPL, V1, P163, DOI 10.1007/BF01259366
   Hameiri E, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P258, DOI 10.1109/TDPVT.2002.1024070
   He TS, 2001, IEEE T VIS COMPUT GR, V7, P333, DOI 10.1109/2945.965347
   HELMAN JL, 1991, IEEE COMPUT GRAPH, V11, P36, DOI 10.1109/38.79452
   Kanitsar A, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P37, DOI 10.1109/VISUAL.2002.1183754
   Kanitsar A, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P43, DOI 10.1109/VISUAL.2003.1250353
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KONG TY, 1989, COMPUT VISION GRAPH, V48, P357, DOI 10.1016/0734-189X(89)90147-3
   LEYMARIE F, 1992, IEEE T PATTERN ANAL, V14, P56, DOI 10.1109/34.107013
   Leymarie F. F., 2003, THESIS BROWN U PROVI
   Liu PC, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P409
   Ma WC, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P207
   Manzanera A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P337, DOI 10.1109/ICCV.1999.791239
   Palágyi K, 1999, LECT NOTES COMPUT SC, V1568, P325
   PERRY AE, 1987, ANNU REV FLUID MECH, V19, P125, DOI 10.1146/annurev.fl.19.010187.001013
   Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267
   Pizer SM, 1999, IEEE T MED IMAGING, V18, P851, DOI 10.1109/42.811263
   SCHIRMACHER H, 1998, P IMDSP 98 ALPB AUST, P25
   SEBASTIAN T, 2002, P EUR C COMP VIS, V3, P731
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Svensson S, 2002, PATTERN RECOGN LETT, V23, P1419, DOI 10.1016/S0167-8655(02)00102-2
   TELEA A, 2003, EUR IEEE S DAT VIS, P185
   Verroust A, 2000, VISUAL COMPUT, V16, P15, DOI 10.1007/PL00007210
   Wade L, 2002, VISUAL COMPUT, V18, P97, DOI 10.1007/s003710100139
   WU FC, 2003, COMP GRAPH WORKSH 20
   Zhou Y, 1998, VISUAL COMPUT, V14, P303, DOI 10.1007/s003710050142
NR 43
TC 150
Z9 183
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 945
EP 955
DI 10.1007/s00371-005-0308-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600006
DA 2024-07-18
ER

PT J
AU De Sapio, V
   Warren, J
   Khatib, O
   Delp, S
AF De Sapio, V
   Warren, J
   Khatib, O
   Delp, S
TI Simulating the task-level control of human motion: a methodology and
   framework for implementation
SO VISUAL COMPUTER
LA English
DT Article
DE human animation; musculoskeletal dynamics; robotics; task-level control
ID MUSCLE; MOVEMENT; MODELS
AB A task-level control framework is proposed for providing feedback control in the simulation of goal-directed human motion. An operational space approach, adapted from the field of robotics, is used for this purpose. This approach is augmented by a significant new extension directed at addressing the control of muscle-driven systems. Task/posture decomposition is intrinsically exploited, allowing human musculoskeletal properties to direct postural behavior during the performance of a task. This paper also describes a simulation architecture for generating musculoskeletal simulations of human characters. The evolving capabilities of the collective environment are directed toward autonomously generating realistic motion control for virtual actors in interactive computer graphics applications, as well as synthesizing the control of human-like motion in robotic systems.
C1 Stanford Univ, Dept Comp Sci, Artificial Intelligence Lab, Stanford, CA 94305 USA.
   Stanford Univ, Neuromuscular Biomech Lab, Dept Mech Engn, Stanford, CA 94305 USA.
   Stanford Univ, Neuromuscular Biomech Lab, Dept Bioengn, Stanford, CA 94305 USA.
C3 Stanford University; Stanford University; Stanford University
RP Clark Ctr Room E100,318 Campus Dr, Stanford, CA 94305 USA.
EM vdesap@robotics.stanford.edu; warren@robotics.stanford.edu;
   khatib@robotics.stanford.edu
OI Delp, Scott/0000-0002-9643-7551; Khatib, Oussama/0000-0003-0482-1415
CR [Anonymous], 1984, Muscles, reflexes, and locomotion
   CHANG KS, 2000, P IEEE INT C ROB AUT, V1, P850
   DELP SL, 1995, COMPUT BIOL MED, V25, P21, DOI 10.1016/0010-4825(95)98882-E
   Delp SL, 2000, COMPUT SCI ENG, V2, P46, DOI 10.1109/5992.877394
   DESAPIO V, 2005, IN PRESS P 2005 IEEE
   Faloutsos P, 2003, IEEE INT CONF ROBOT, P917
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Featherstone R, 1987, Robot Dynamics Algorithms, P65, DOI DOI 10.1007/978-0-387-74315-8
   Hase K, 2003, J VISUAL COMP ANIMAT, V14, P73, DOI 10.1002/vis.306
   Hollars Michael G, 1991, SD FAST USERS MANUAL
   Ijspeert AJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1398, DOI 10.1109/ROBOT.2002.1014739
   Jessell TM., 2000, PRINCIPLES NEURAL SC
   Khatib O, 2004, ON ADVANCES IN ROBOT KINEMATICS, P145
   Khatib O, 2002, COMMUN ACM, V45, P46, DOI 10.1145/504729.504753
   KHATIB O, 1987, IEEE T ROBOTIC AUTOM, V3, P43, DOI 10.1109/JRA.1987.1087068
   KHATIB O, 1995, INT J ROBOT RES, V14, P19, DOI 10.1177/027836499501400103
   KHATIB O., 2004, Int J Humanoid Robot, V01, P29, DOI [DOI 10.1142/S0219843604000058, 10.1142/S0219843604000058]
   Komura T, 1999, J VISUAL COMP ANIMAT, V10, P57, DOI 10.1002/(SICI)1099-1778(199904/06)10:2<57::AID-VIS196>3.0.CO;2-R
   Komura T, 2000, VISUAL COMPUT, V16, P254, DOI 10.1007/s003719900065
   Kuroki Y, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1394
   LILLY KW, 1992, EFFICIENT DYNAMICS S
   PANDY MG, 1992, J BIOMECH ENG-T ASME, V114, P450, DOI 10.1115/1.2894094
   RASMUSSEN J, 2003, P INT S COMP SIM BIO
   Ruspini D, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P1322, DOI 10.1109/IROS.2000.893204
   Sakagami Y, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P2478, DOI 10.1109/IRDS.2002.1041641
   Schutte LM, 1992, THESIS STANFORD U
   Thelen DG, 2003, J BIOMECH, V36, P321, DOI 10.1016/S0021-9290(02)00432-3
   THELEN DG, 2001, P 5 INT S COMP METH
   Yamane K., 2004, Simulating and Generating Motions of Human Figures
   ZAJAC FE, 1989, CRIT REV BIOMED ENG, V17, P359
   ZAJAC FE, 1993, J BIOMECH, V26, P109, DOI 10.1016/0021-9290(93)90083-Q
NR 31
TC 36
Z9 46
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2005
VL 21
IS 5
BP 289
EP 302
DI 10.1007/s00371-005-0284-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 937OJ
UT WOS:000229935100002
DA 2024-07-18
ER

PT J
AU Zhong, YM
   Shirinzadeh, B
   Ma, WY
AF Zhong, YM
   Shirinzadeh, B
   Ma, WY
TI Solid modelling in a virtual reality environment
SO VISUAL COMPUTER
LA English
DT Article
DE constraint-based manipulations; model representation; solid modelling;
   virtual reality
ID GEOMETRIC CONSTRAINT SYSTEMS; CONSTRUCTIVE APPROACH; DESIGN
AB This paper presents a constraint-based methodology for intuitive and precise solid modelling in a virtual reality (VR) environment. A hierarchically structured and constraint-based data model is developed to support solid modelling in the VR environment. A constraint reasoning engine is also developed to automatically deduce allowable motions for precise constraint-based 3D manipulations. A prototype system of product modelling has been successfully developed, and experimental results demonstrate the advantage of precise solid modelling through constraint-based manipulation in virtual environments.
C1 Monash Univ, Dept Mech Engn, Robot & Mechatron Res Lab, Clayton, Vic 3800, Australia.
   City Univ Hong Kong, Dept Mfg Engn & Engn Management, Hong Kong, Hong Kong, Peoples R China.
C3 Monash University; City University of Hong Kong
RP Monash Univ, Dept Mech Engn, Robot & Mechatron Res Lab, POB 31,Clayton Campus, Clayton, Vic 3800, Australia.
EM yongmin.zhong@eng.monash.edu.au
RI MA, Weiyin/K-9155-2015; Zhong, Yongmin/L-3100-2017
OI MA, Weiyin/0000-0001-9760-7789; Zhong, Yongmin/0000-0002-0105-9296
CR ALDEFELD B, 1988, COMPUT AIDED DESIGN, V20, P117, DOI 10.1016/0010-4485(88)90019-X
   ANATHA R, 1996, COMPUT AIDED DESIGN, V28, P707
   Balázs A, 2004, COMPUT GRAPH-UK, V28, P79, DOI 10.1016/j.cag.2003.10.007
   Balmelli L, 2003, COMP GEOM-THEOR APPL, V25, P171, DOI 10.1016/S0925-7721(02)00166-9
   Bao JS, 2002, J MATER PROCESS TECH, V129, P592, DOI 10.1016/S0924-0136(02)00655-6
   Bier E. A., 1986, Computer Graphics, V20, P233, DOI 10.1145/15886.15912
   BLOOR MS, 1994, P IFIP WG 5 10 WORKS, P69
   Bruderlin B, 1998, GEOMETRIC CONSTRAINT
   Brunetti G., 1995, Proceedings. Third Symposium on Solid Modeling and Applications, P95, DOI 10.1145/218013.218039
   BUTTERWORTH J, 1992, ACM COMPUTER GRAPHIC, V25, P197
   Chang CF, 1997, COMPUT INTEGR MANUF, V10, P205, DOI 10.1016/S0951-5240(97)00009-8
   Choi SH, 2004, COMPUT AIDED DESIGN, V36, P401, DOI 10.1016/S0010-4485(03)00110-6
   DANI T, 1997, ASME 1997 COMP ENG C
   DASILVA RE, 1990, ASME DES ENG DIV, V23, P1
   FA M, 1993, P 2 ACM S SOL MOD AP, P243
   FERNANDO T, 1995, EUR WORKSH VIRT ENV, P185
   FERNANDO T, 1999, P ACM S VIRT REAL SO, P147
   FIGUEIREDO M, 1994, P IFIP WG 5 10 WORKS, P99
   FLORIANI LD, 2004, COMPUT AIDED DESIGN, V36, P141
   Fudos I, 1997, ACM T GRAPHIC, V16, P179, DOI 10.1145/248210.248223
   Gao SM, 2000, COMPUT GRAPH-UK, V24, P191, DOI 10.1016/S0097-8493(99)00154-5
   Gao XS, 1998, COMPUT AIDED DESIGN, V30, P47, DOI 10.1016/S0010-4485(97)00052-3
   GLEICHER M, 1993, P UIST 93, P109
   GUI JK, 1994, COMPUT AIDED DESIGN, V26, P435, DOI 10.1016/0010-4485(94)90066-3
   Hoffmann CM, 1997, J SYMB COMPUT, V23, P287, DOI 10.1006/jsco.1996.0089
   Hsu C., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P168, DOI 10.1145/267734.267779
   Ji P, 2002, ASSEMBLY AUTOM, V22, P337
   Joan-Arinyo R, 1997, COMPUT GRAPH, V21, P599, DOI 10.1016/S0097-8493(97)00038-1
   Kan HY, 2001, COMPUT IND, V45, P197, DOI 10.1016/S0166-3615(01)00093-8
   Kim J, 2000, INT J ADV MANUF TECH, V16, P843, DOI 10.1007/s001700070019
   Kiyokawa K, 2000, IEEE MULTIMEDIA, V7, P22, DOI 10.1109/93.839308
   KIYOKAWA K, 1998, ELECTR COMMUN 3, V8, P1517
   Kramer G. A., 1991, Proceedings. Symposium on Solid Modeling Foundations and CAD/CAM Applications, P371, DOI 10.1145/112515.112566
   Kwaiter G., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P265, DOI 10.1145/267734.267803
   Lau HYK, 2003, J MATER PROCESS TECH, V139, P402, DOI 10.1016/S0924-0136(03)00510-7
   Li YT, 2002, COMPUT AIDED DESIGN, V34, P97, DOI 10.1016/S0010-4485(01)00054-9
   LIANG JD, 1994, COMPUT GRAPH, V18, P499, DOI 10.1016/0097-8493(94)90062-0
   Marcelino L, 2003, COMPUT GRAPH-UK, V27, P19, DOI 10.1016/S0097-8493(02)00228-5
   Nishino H., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P81, DOI 10.1109/ICSMC.1999.816460
   NOORT A, 1998, GEOMETRIC CONSTRAINT, P107
   *PAR TECHN CORP, 1999, DIV REAL MAN
   Shah J. J., 1993, Research in Engineering Design, V5, P218, DOI 10.1007/BF01608364
   SHAH JJ, 1988, COMPUT AIDED DESIGN, V20, P515, DOI 10.1016/0010-4485(88)90041-3
   Shimizu S, 1997, ARTIF INTELL, V91, P51, DOI 10.1016/S0004-3702(96)00057-4
   Stork A., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P181, DOI 10.1145/267734.267782
   Sutherland I.E., 1963, P MAY 21 23 1963 SPR, P329, DOI DOI 10.1145/1461551.1461591
   van Emmerik M. J. G. M., 1990, Computer Graphics Forum, V9, P355, DOI 10.1111/j.1467-8659.1990.tb00427.x
   You CF, 1996, INT J ADV MANUF TECH, V12, P280, DOI 10.1007/BF01239615
   Zhong YM, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P389, DOI 10.1109/IV.2002.1028804
NR 49
TC 8
Z9 8
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 17
EP 40
DI 10.1007/s00371-004-0268-9
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300002
DA 2024-07-18
ER

PT J
AU Pasko, G
   Pasko, A
AF Pasko, G
   Pasko, A
TI Trimming implicit surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE geometric modeling; implicit surfaces; trimming; function
   representation; polygonization
AB Algorithms for trimming implicit surfaces yielding surface sheets and stripes are presented. These two-dimensional manifolds with boundaries result from set-theoretic operations on an implicit surface and a solid or another implicit surface. The algorithms generate adaptive polygonal approximation of the trimmed surfaces by extending our original implicit surface polygonization algorithm. The presented applications include modeling several spiral shaped surface sheets and stripes (based on M. Escher's artworks) and extraction of ridges on implicit surfaces. Another promising application of the presented algorithms is modeling heterogeneous objects as implicit complexes.
C1 Kanazawa Inst Technol, IT Inst, Tokyo, Japan.
   Hosei Univ, Tokyo, Japan.
C3 Kanazawa Institute of Technology; Hosei University
RP Kanazawa Inst Technol, IT Inst, Tokyo, Japan.
EM gip@tokyo.com; pasko@k.hosei.ac.jp
RI Pasko, Alexander/H-9344-2017
OI Pasko, Alexander/0000-0002-4785-7066
CR Adzhiev V., 2002, PROC 7 ACM S SOLID M, P192
   [Anonymous], 1997, Introduction to Implicit Surfaces
   Belyaev AG, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P530, DOI 10.1109/CGI.1998.694306
   BLOOMENTHAL J, 1995, SIGGRAPH 95, P309
   BOWYER A, 1994, SVLIS SET THEORETIC
   Hosaka M., 1992, Modeling of Curves and Surfaces in CAD/CAM
   Kunii T. L., 1999, International Journal of Shape Modeling, V5, P123, DOI 10.1142/S0218654399000137
   Ohtake Y, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P74, DOI 10.1109/SMA.2001.923377
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   PASKO A, 1986, P1086310 JOINT I NUC
   PASKO AA, 1988, COMPUT GRAPH, V12, P457, DOI 10.1016/0097-8493(88)90070-2
   RossWatt DAJ, 1996, J S AFR I MIN METALL, V96, P1
   RVACHEV VL, 1987, THEORY R FUNCTIONS S
   Wyvill B., 1996, International Journal of Shape Modeling, V2, P257, DOI 10.1142/S0218654396000142
NR 14
TC 4
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2004
VL 20
IS 7
BP 437
EP 447
DI 10.1007/s00371-004-0250-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 863IZ
UT WOS:000224556200001
DA 2024-07-18
ER

PT J
AU Yu, F
   Chen, ZX
   Cao, JC
   Jiang, MH
AF Yu, Feng
   Chen, Zhaoxiang
   Cao, Jiacheng
   Jiang, Minghua
TI Redundant same sequence point cloud registration
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud registration; 3D point matching; Computer vision
AB Many point cloud registration methods rely on establishing correspondence pairs in order to solve the registration problem. However, their performance significantly deteriorates when the given point clouds contain only a limited number of inliers. In this regard, we propose an innovative approach called Redundant Same Sequence point cloud Registration (RSSR) that effectively addresses the challenge of registering point clouds with a limited number of inliers. RSSR achieves this by reconstructing the correspondence relation constraint to allow the discovery of more potential correspondence pairs. It allows one point to correspond with an arbitrary number of points and then generates redundant same sequence point clouds based on the correspondence relation. It explores more potential correspondence pairs at the cost of introducing more erroneous correspondence pairs. Finally, for the introduced erroneous correspondence pairs, we design a rigid transformation solver to register the redundant same sequence point clouds. Our experiments demonstrate that while the performance of other methods significantly drops as the number of inliers decreases, our method maintains stable performance. The codes are available in (https://github.com/ChenPointCloud/RSSR).
C1 [Yu, Feng; Chen, Zhaoxiang; Cao, Jiacheng; Jiang, Minghua] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
   [Yu, Feng; Jiang, Minghua] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University
RP Jiang, MH (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.; Jiang, MH (corresponding author), Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Hubei, Peoples R China.
EM yufeng@wtu.edu.cn; pclchenzx@yeah.net; caojiachengjc@gmail.com;
   minghuajiang@wtu.edu.cn
RI Yu, Feng/JTD-1798-2023
OI Yu, Feng/0000-0001-8252-5131
FU National Natural Science Foundation of China [62202346]; National
   natural science foundation of China [2021BAA042]; Hubei key research and
   development program [2022HBCI01, 2022013988065212]; Open project of
   engineering research center of Hubei province for clothing information;
   MIIT's AI Industry Innovation Task unveils flagship projects; Hubei
   science and technology project of safe production special fund (Scene
   control platform based on proprioception information computing of
   artificial intelligence)
FX This work was supported by national natural science foundation of China
   (No. 62202346), Hubei key research and development program
   (No.2021BAA042), open project of engineering research center of Hubei
   province for clothing information (No. 2022HBCI01), Wuhan applied basic
   frontier research project (No. 2022013988065212), MIIT's AI Industry
   Innovation Task unveils flagship projects (Key technologies, equipment,
   and systems for flexible customized and intelligent manufacturing in the
   clothing industry), and Hubei science and technology project of safe
   production special fund (Scene control platform based on proprioception
   information computing of artificial intelligence).
CR Bai XY, 2021, PROC CVPR IEEE, P15854, DOI 10.1109/CVPR46437.2021.01560
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Chen W, 2022, PROC CVPR IEEE, P6338, DOI 10.1109/CVPR52688.2022.00624
   Chen Z, 2022, AAAI CONF ARTIF INTE, P401
   Chi P, 2024, VISUAL COMPUT, V40, P2889, DOI 10.1007/s00371-023-02992-x
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Deschaud JE, 2018, IEEE INT CONF ROBOT, P2480
   Ding L, 2019, PROC CVPR IEEE, P8642, DOI 10.1109/CVPR.2019.00885
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Ginzburg D, 2022, AAAI CONF ARTIF INTE, P706
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Gu CJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3039641
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Jiang HB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6108, DOI 10.1109/ICCV48922.2021.00607
   Kadam P, 2022, IEEE T IMAGE PROCESS, V31, P2710, DOI 10.1109/TIP.2022.3160609
   Kamel A, 2019, INT J HUM-COMPUT INT, V35, P427, DOI 10.1080/10447318.2018.1543081
   Kingma D. P., 2014, arXiv
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Li X., 2020, arXiv
   Li Y., 2018, P ADV NEUR INF PROC, VVolume 31, P1
   Liu YY, 2021, IEEE T IMAGE PROCESS, V30, P7486, DOI 10.1109/TIP.2021.3106799
   Pais GD, 2020, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR42600.2020.00722
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qin Z, 2022, PROC CVPR IEEE, P11133, DOI 10.1109/CVPR52688.2022.01086
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shen Y., 2022, AAAI, P1
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1630, DOI 10.1145/3503161.3548023
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu H, 2022, AAAI CONF ARTIF INTE, P2848
   Xu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3112, DOI 10.1109/ICCV48922.2021.00312
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhang J., 2014, ROBOTICS SCI SYSTEMS, P9, DOI 10.15607/RSS.2014.X.007
   Zhang ZY, 2022, AAAI CONF ARTIF INTE, P3399
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 42
TC 1
Z9 1
U1 9
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 19
PY 2023
DI 10.1007/s00371-023-03203-3
EA DEC 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ6V8
UT WOS:001126758000001
DA 2024-07-18
ER

PT J
AU Lin, LY
   Zhang, S
   Ji, SL
   Zhao, SX
   Wen, AL
   Yan, JP
   Zhou, Y
   Zhou, WB
AF Lin, Liyuan
   Zhang, Shun
   Ji, Shulin
   Zhao, Shuxian
   Wen, Aolin
   Yan, Jingpeng
   Zhou, Yuan
   Zhou, Weibin
TI TMGAN: two-stage multi-domain generative adversarial network for
   landscape image translation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial networks (GAN); Image-to-image translation; Image
   generation; Style transfer
AB Chinese landscape paintings, realistic landscape photographs, and oil paintings each possess unique artistic characteristics and painting features. Image-to-image translation between these three domains is an extremely challenging task. Existing image-to-image translation networks suffer from deficiencies in preserving content or conveying style, posing difficulties in achieving this task. To address this issue, we propose a novel two-stage multi-domain generative adversarial network approach (TMGAN). We add edge maps as additional guidance input and implement content control to better retain content information. In addition, we design the IOST (In/Out module for Style Transfer) module to better assist the style transfer task. By employing a clever design, we decompose the image translation task into two stages: content extraction and style injection. In the content extraction stage, TMGAN extracts high-resolution edge images from content images. In the style injection stage, TMGAN takes the high-resolution edge image as input and injects the specified style for generation. Notably, we accomplish this two-stage task using only a single multi-domain generator network. Extensive qualitative and quantitative experiments conducted against the baseline model validate the exceptional performance of TMGAN. Furthermore, to facilitate further research, we release MLHQ, a high-quality multi-domain landscape dataset.
C1 [Lin, Liyuan; Zhang, Shun; Zhao, Shuxian; Wen, Aolin; Yan, Jingpeng; Zhou, Weibin] Tianjin Univ Sci & Technol, Coll Elect Informat & Automat, Tianjin 300222, Peoples R China.
   [Ji, Shulin] Canaan Creat Co Ltd, Applicat Dept, Beijing 100094, Beijing, Peoples R China.
   [Zhou, Yuan] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University Science & Technology; Tianjin University
RP Zhou, WB (corresponding author), Tianjin Univ Sci & Technol, Coll Elect Informat & Automat, Tianjin 300222, Peoples R China.
EM linly@tust.edu.cn; zhangshun1120@126.com; 448315782@qq.com;
   zhao_shuxian777y@163.com; wenaolin@mail.tust.edu.cn;
   Yanjp@mail.tust.edu.cn; zhouyuan@tju.edu.cn; zhouweibin@tust.edu.cn
CR Bi ZW, 2022, IEEE T IMAGE PROCESS, V31, P6664, DOI 10.1109/TIP.2022.3214336
   Bosquet B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108998
   Cao B, 2023, INT J COMPUT VISION, V131, P1995, DOI 10.1007/s11263-023-01791-0
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du Z., 2022, IEEE Trans. Instrum. Measure.
   Gao CY, 2020, PROC CVPR IEEE, P5173, DOI 10.1109/CVPR42600.2020.00522
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Z., 2023, Visual Comput., P1
   He B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1172, DOI 10.1145/3240508.3240655
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2023, INT J DIGIT EARTH, V16, P1491, DOI 10.1080/17538947.2023.2202422
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Lai Y, 2023, VISUAL COMPUT, V39, P4133, DOI 10.1007/s00371-022-02580-5
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li LY, 2022, VISUAL COMPUT, V38, P3577, DOI 10.1007/s00371-021-02188-1
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Liu MY, 2017, ADV NEUR IN, V30
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Peng ZX, 2023, COMPUT VIS MEDIA, V9, P619, DOI 10.1007/s41095-022-0295-3
   Richter SR, 2023, IEEE T PATTERN ANAL, V45, P1700, DOI 10.1109/TPAMI.2022.3166687
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan WR, 2017, IEEE IMAGE PROC, P3760, DOI 10.1109/ICIP.2017.8296985
   Tang H, 2023, IEEE T PATTERN ANAL, V45, P6055, DOI 10.1109/TPAMI.2022.3212915
   Tumanyan N, 2022, PROC CVPR IEEE, P10738, DOI 10.1109/CVPR52688.2022.01048
   Varghese S, 2023, ADV ENG INFORM, V56, DOI 10.1016/j.aei.2023.101940
   Wang H., 2023, IEEE Trans. Instrum. Measure.
   Wang L., 2022, VISUAL COMPUT
   Wu B, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401448
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu S, 2021, IEEE INT WORKS MACH, DOI 10.1109/MLSP52302.2021.9596481
   Xue A, 2021, IEEE WINT CONF APPL, P3862, DOI 10.1109/WACV48630.2021.00391
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang J., 2023, IEEE Trans. Multim.
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XQ, 2023, IEEE T IND INFORM, V19, P3144, DOI 10.1109/TII.2022.3160705
   Zhang YB, 2023, VISUAL COMPUT, V39, P1283, DOI 10.1007/s00371-022-02404-6
   Zhao YP, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3093887
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu M., 2023, P IEEE CVF INT C COM, P23109
   Zhu MR, 2021, INT J COMPUT VISION, V129, P1820, DOI 10.1007/s11263-021-01442-2
NR 49
TC 0
Z9 0
U1 7
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 7
PY 2023
DI 10.1007/s00371-023-03171-8
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2V2
UT WOS:001115674200005
DA 2024-07-18
ER

PT J
AU Wu, B
   Dong, QS
   Sun, WQ
AF Wu, Bing
   Dong, Qingshuang
   Sun, Wenqing
TI Fast continuous patch-based artistic style transfer for videos
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Style transfer; Deep neural networks; Patch matching; Video generation
AB Convolutional neural network-based image style transfer models often suffer from temporal inconsistency when applied to video. Although several video style transfer models have been proposed to improve temporal consistency, they often trade off processing speed, perceptual style quality, and temporal consistency. In this work, we propose a novel approach for fast continuous patch-based arbitrary video style transfer that achieves high-quality transfer results while maintaining temporal coherence. Our approach begins with stylizing the first frame as a standalone single image using patch propagation within the content activation. Subsequent frames are computed based on the key insight that optical flow field evaluated from neighboring content activations provides meaningful information to preserve temporal coherence efficiently. To address the problems introduced from optical flow stage, we additionally incorporate a correction procedure as a post-process to ensure a high-quality stylized video. Finally, we demonstrate our method can transfer arbitrary styles on a set of examples and illustrate that our approach exhibits superior performance both qualitatively and quantitatively.
C1 [Wu, Bing; Dong, Qingshuang; Sun, Wenqing] Qufu Normal Univ, Sch Commun, Rizhao 276826, Peoples R China.
C3 Qufu Normal University
RP Dong, QS (corresponding author), Qufu Normal Univ, Sch Commun, Rizhao 276826, Peoples R China.
EM dariadong2010@qfnu.edu.cn
FU Humanities and Social Science Fund of Ministry of Education of China
FX No Statement Available
CR Gatys LA, 2015, Arxiv, DOI arXiv:1508.06576
   Abualigah L, 2021, COMPUT METHOD APPL M, V376, DOI 10.1016/j.cma.2020.113609
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen JC, 2022, J SYST ARCHITECT, V129, DOI 10.1016/j.sysarc.2022.102598
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Dumoulin V, 2017, Arxiv, DOI [arXiv:1610.07629, DOI 10.48550/ARXIV.1610.07629]
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2020, AAAI CONF ARTIF INTE, V34, P4369
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kong XY, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3230084
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li DJ, 2023, VISUAL COMPUT, V39, P5359, DOI 10.1007/s00371-022-02664-2
   Li WB, 2019, LECT NOTES COMPUT SC, V11361, P232, DOI 10.1007/978-3-030-20887-5_15
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li YH, 2017, Arxiv, DOI [arXiv:1701.01036, DOI 10.48550/ARXIV.1701.01036]
   Li YT, 2017, ADV NEUR IN, V30
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Ning X, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108873
   Chen TQ, 2016, Arxiv, DOI arXiv:1612.04337
   Ruder M, 2018, INT J COMPUT VISION, V126, P1199, DOI 10.1007/s11263-018-1089-z
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wang CS, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3170493
   Wang C, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108498
   Wang GJ, 2022, IEEE T NEUR NET LEAR, V33, P3264, DOI 10.1109/TNNLS.2021.3051430
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu B, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401448
   Wu B, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9163304
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Ye WJ, 2023, VISUAL COMPUT, V39, P609, DOI 10.1007/s00371-021-02361-6
   Yu XM, 2024, VISUAL COMPUT, V40, P1369, DOI 10.1007/s00371-023-02855-5
   Zhang Hang, 2018, P EUROPEAN C COMPUTE, P0
NR 41
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 30
PY 2023
DI 10.1007/s00371-023-03157-6
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ4K2
UT WOS:001122252600001
DA 2024-07-18
ER

PT J
AU Hao, ZW
   Wang, SS
   Long, SF
   Li, YY
   Chai, H
AF Hao, Zhiwei
   Wang, Shengsheng
   Long, Sifan
   Li, Yiyang
   Chai, Hao
TI Bidirectional feature enhancement transformer for unsupervised domain
   adaptation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Domain adaptation; Transformer; Feature enhancement; Image
   classification
AB Unsupervised domain adaptation (UDA) aims to generalize knowledge learned from one labeled source domain to another unlabeled target domain. To extract domain-invariant feature representations, most existing UDA approaches leverage convolution neural network-based frameworks either at the domain level or the category level. Compared with coarse-grained domain-level alignment methods, fine-grained category-level UDA approaches facilitates more precise alignment. However, a basic difficulty with category-level-based UDA is that the construction of pseudo-labels for unlabeled target domain is frequently too noisy for appropriate domain alignment, hence compromising the UDA performance. Clustering methods can be used to extract prototype representations across and within domains, which can be used to augment features in source and target domains. As a result, correlative features can be constructed in the feature space and more robust pseudo-labels can be obtained, allowing for more precise feature alignment. With the significant application of the transformer in visual tasks, the cross-attention mechanism of the transformer is more resistant to noise, making it possible to achieve more accurate feature alignment. In our paper, we propose a bidirectional feature enhancement transformer (BFET) as a solution to the difficult UDA tasks. Specifically, we propose a bidirectional cross-attention transformer architecture with an equilibrium factor to achieve feature representation learning and domain alignment. And we design a bidirectional cross-domain homogeneous prototype feature enhancement algorithm to produce proper Source-Target pairs. Besides, we propose a Feature Enhancement Module to utilize the knowledge of prototypes to provide more accurate pseudo-labels. The proposed BFET can mandate the framework to concurrently learn discriminative representations that are domain-specific as well as feature representations that are domain-invariant. In addition, detailed and systematic experiments illustrate that our BFET achieves noteworthy performance.
C1 [Hao, Zhiwei; Wang, Shengsheng; Long, Sifan; Li, Yiyang; Chai, Hao] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.
   [Hao, Zhiwei; Wang, Shengsheng; Long, Sifan; Li, Yiyang; Chai, Hao] Jilin Univ, Minist Educ, Key Lab Symbol Computat & Knowledge Engn, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Wang, SS (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.; Wang, SS (corresponding author), Jilin Univ, Minist Educ, Key Lab Symbol Computat & Knowledge Engn, Changchun 130012, Peoples R China.
EM haozw21@mails.jlu.edu.cn; wss@jlu.edu.cn; summitlsf@outlook.com;
   yiyangl21@mails.jlu.edu.cn; chaihao21@mails.jlu.edu.cn
FU Jilin Province Development and Reform Commission [62376106]; National
   Natural Science Foundation of China [2021FGWCXNLJSSZ10, 2019C053-3];
   Innovation Capacity Construction Project of Jilin Province Development
   and Reform Commission [2020YFA0714103]; National Key Research and
   Development Program of China; Fundamental Research Funds for the Central
   Universities
FX This work is supported by the National Natural Science Foundation of
   China (62376106), the Innovation Capacity Construction Project of Jilin
   Province Development and Reform Commission (2021FGWCXNLJSSZ10,
   2019C053-3), the National Key Research and Development Program of China
   (No. 2020YFA0714103) and the Fundamental Research Funds for the Central
   Universities, JLU.
CR Bo Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P567, DOI 10.1007/978-3-030-58555-6_34
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen L, 2022, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR52688.2022.00704
   Chen XY, 2019, PR MACH LEARN RES, V97
   Csurka G, 2017, Arxiv, DOI [arXiv:1702.05374, DOI 10.48550/ARXIV:1702.05374]
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng MF, 2024, VISUAL COMPUT, V40, P1053, DOI 10.1007/s00371-023-02831-z
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du ZK, 2021, PROC CVPR IEEE, P3936, DOI 10.1109/CVPR46437.2021.00393
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Gretton A., 2006, A kernel method for the two-sample-problem, P19
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Han L, 2021, INT J COMPUT VISION, V129, P2927, DOI 10.1007/s11263-021-01507-2
   Hinton G., 2015, COMPUT SCI, V2
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Jiang X, 2020, PR MACH LEARN RES, V119
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kurmi VK, 2019, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2019.00058
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li GR, 2021, PROC CVPR IEEE, P9752, DOI 10.1109/CVPR46437.2021.00963
   Li JC, 2021, PROC CVPR IEEE, P2505, DOI 10.1109/CVPR46437.2021.00253
   Li JY, 2022, INFORM SCIENCES, V609, P257, DOI 10.1016/j.ins.2022.07.068
   Li S, 2021, AAAI CONF ARTIF INTE, V35, P1949
   Li S, 2020, AAAI CONF ARTIF INTE, V34, P11386
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li XY, 2022, IEEE T COGN DEV SYST, V14, P246, DOI 10.1109/TCDS.2020.3048883
   Li XY, 2021, NEURAL COMPUT APPL, V33, P8031, DOI 10.1007/s00521-020-05545-8
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Liang J, 2021, PROC CVPR IEEE, P16627, DOI 10.1109/CVPR46437.2021.01636
   Liu Y., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Morerio P, 2020, IEEE WINT CONF APPL, P3119, DOI [10.1109/WACV45572.2020.9093579, 10.1109/wacv45572.2020.9093579]
   Munir F, 2021, IEEE INT C INT ROBOT, P206, DOI 10.1109/IROS51168.2021.9636353
   Na J, 2022, LECT NOTES COMPUT SC, V13694, P92, DOI 10.1007/978-3-031-19830-4_6
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Oza P., 2021, ARXIV PREPRINT, Vabs/2105.13502
   Pan LR, 2022, BIOMED SIGNAL PROCES, V77, DOI 10.1016/j.bspc.2022.103824
   Peng X., 2017, ARXIV
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Rangwani H, 2022, PR MACH LEARN RES
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2017, Adversarial dropout regularization
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Snell J, 2017, ADV NEUR IN, V30
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang X., 2019, Transferable normalization: towards improving transferability of deep neural networks, P32
   Westfechtel T, 2023, IEEE WINT CONF APPL, P392, DOI 10.1109/WACV56688.2023.00047
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Xu T., 2021, Cdtrans: Cross-domain transformer for unsupervised domain adaptation
   Yang Guanglei, 2021, Transformer-based source-free domain adaptation, P1
   Yang J, 2022, EXP HEAT TRANSFER, V35, P577, DOI 10.1080/08916152.2021.1919246
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Yu ZT, 2021, IEEE SIGNAL PROC LET, V28, P1290, DOI 10.1109/LSP.2021.3089908
   Zhang QM, 2019, ADV NEUR IN, V32
   Zhang YB, 2022, IEEE T PATTERN ANAL, V44, P2775, DOI 10.1109/TPAMI.2020.3036956
   Zhang YF, 2023, Arxiv, DOI arXiv:2302.00194
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
NR 75
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 27
PY 2023
DI 10.1007/s00371-023-03164-7
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB1Q4
UT WOS:001122703200003
DA 2024-07-18
ER

PT J
AU Moutafidou, A
   Toulatzis, V
   Fudos, I
AF Moutafidou, Anastasia
   Toulatzis, Vasileios
   Fudos, Ioannis
TI Deep fusible skinning of animation sequences
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Animation; Skinning; Deep learning; Linear Blend Skinning; Rigging
AB Animation compression is a key process in replicating and streaming animated 3D models. Linear Blend Skinning (LBS) facilitates the compression of an animated sequence while maintaining the capability of real-time streaming by deriving vertex to proxy bone assignments and per frame bone transformations. We introduce a innovative deep learning approach that learns how to assign vertices to proxy bones with persistent labeling. This is accomplished by learning how to correlate vertex trajectories to bones of fully rigged animated 3D models. Our method uses these pretrained networks on dynamic characteristics (vertex trajectories) of an unseen animation sequence (a sequence of meshes without skeleton or rigging information) to derive an LBS scheme that outperforms most previous competent approaches by offering better approximation of the original animation sequence with fewer bones, therefore offering better compression and smaller bandwidth requirements for streaming. This is substantiated by a thorough comparative performance evaluation using several error metrics, and compression/bandwidth measurements. In this paper, we have also introduced a persistent bone labeling scheme that (i) improves the efficiency of our method in terms of lower error values and better visual outcome and (ii) facilitates the fusion of two (or more) LBS schemes by an innovative algorithm that combines two arbitrary LBS schemes. To demonstrate the usefulness and potential of this fusion process, we have combined the outcome of our deep skinning method with that of Rignet-which is a state-of-the-art method that performs rigging on static meshes-with impressive results.
C1 [Moutafidou, Anastasia; Toulatzis, Vasileios; Fudos, Ioannis] Univ Ioannina, Dept Comp Sci & Engn, Ioannina, Greece.
C3 University of Ioannina
RP Fudos, I (corresponding author), Univ Ioannina, Dept Comp Sci & Engn, Ioannina, Greece.
EM fudos@uoi.gr
FU European Union (European Social Fund- ESF) through the Operational
   Programme 'Human Resources Development, Education and Lifelong Learning'
   [MIS-5000432]
FX This research is co-financed by Greece and the European Union (European
   Social Fund- ESF) through the Operational Programme 'Human Resources
   Development, Education and Lifelong Learning' in the context of the
   project 'Strengthening Human Resources Research Potential via Doctorate
   Research' (MIS-5000432), implemented by the State Scholarships
   Foundation (IKY).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Andreou N, 2022, COMPUT GRAPH FORUM, V41, P155, DOI 10.1111/cgf.14632
   [Anonymous], 2022, Mixamo: empowering creativity with animated 3d characters
   [Anonymous], 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'02
   [Anonymous], 2022, TurboSquid: 3d models for professionals
   [Anonymous], 2022, Sketchfab: publish and find 3d models online
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Avril Q, 2016, COMPUT GRAPH FORUM, V35, P115, DOI 10.1111/cgf.12816
   Bailey SW, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201300
   CGTrader, 2022, 3d models for vr/ar and cg projects
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Fan CH, 2022, CONNECT SCI, V34, P2845, DOI 10.1080/09540091.2022.2151569
   Feng Andrew, 2015, P 8 ACM SIGGRAPH C M, P57
   Hasler N., 2010, P ACM SIGGRAPH S INT, P23, DOI DOI 10.1145/1730804.1730809
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jacobson A., 2014, ACM SIGGRAPH 2014 CO, P24
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Kakadiaris I, 1998, COMP ANIM CONF PROC, P144, DOI 10.1109/CA.1998.681919
   Kavan L, 2010, COMPUT GRAPH FORUM, V29, P327, DOI 10.1111/j.1467-8659.2009.01602.x
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P53
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Kingma D. P., 2014, arXiv
   Kokkevis E., 1996, Proceedings. Computer Animation '96, P16, DOI 10.1109/CA.1996.540484
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Laperriere Richard, 1989, P GRAPH INT, P26
   Le BH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601161
   Le BH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366218
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322969
   Luo R, 2020, IEEE T VIS COMPUT GR, V26, P1745, DOI 10.1109/TVCG.2018.2881451
   Mikhailov A., 2019, Google AI Blog
   Moutafidou A, 2021, LECT NOTES COMPUT SC, V13002, P3, DOI 10.1007/978-3-030-89029-2_1
   Papagiannakis G., 2013, SIGGRAPH ASIA 2013 T, DOI [10.1145/2542355.2542369, DOI 10.1145/2542355.2542369]
   Santesteban I, 2020, COMPUT GRAPH FORUM, V39, P65, DOI 10.1111/cgf.13912
   Sattler Mirko, 2005, P ACM SIGGRAPH EUR S, P209
   Shen WW, 2022, IET IMAGE PROCESS, V16, P681, DOI 10.1049/ipr2.12286
   Vaillant R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461960
   Vasilakis A.A., 2016, P 33 COMP GRAPH INT, P53, DOI [10.1145/2949035.2949049, DOI 10.1145/2949035.2949049]
   Vasilakis AA, 2011, COMPUT ANIMAT VIRT W, V22, P27, DOI 10.1002/cav.382
   Wareham R, 2008, LECT NOTES COMPUT SC, V5098, P63, DOI 10.1007/978-3-540-70517-8_7
   Xu Z, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392379
   Yuksel Can, 2007, Symposium on Geometry Processing, P153, DOI DOI 10.2312/SGP/SGP07/153-162
   Zell E., 2013, Symposium on Non-Photorealistic Animation and Rendering, P15
NR 44
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 6
PY 2023
DI 10.1007/s00371-023-03130-3
EA NOV 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X2OZ7
UT WOS:001096914400002
OA hybrid
DA 2024-07-18
ER

PT J
AU Lan, TY
   Dou, FR
   Feng, ZL
   Zhang, CF
AF Lan, Tianye
   Dou, Furong
   Feng, Ziliang
   Zhang, Chengfang
TI Efficient real-time semantic segmentation: accelerating accuracy with
   fast non-local attention
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Semantic segmentation; Fast non-local attention; Attentional feature
   fusion; Real-time speed; Encoder-decoder structure
ID NETWORK
AB As an essential aspect of semantic segmentation, real-time semantic segmentation poses significant challenge in achieving trade-off between segmentation accuracy and inference speed. Standard non-local block can effectively capture the long-range dependencies that are critical to semantic segmentation, while its huge computational cost is unacceptable for real-time semantic segmentation. To confront this issue, we propose fast non-local attention network (FNANet) with encoder-decoder structure for real-time semantic segmentation. FNANet relies on the utilization of fast non-local attention module and fast non-local attention fusion module. These modules serve the dual purpose of reducing computational demands and capturing essential contextual information, thereby achieving an equilibrium between enhanced segmentation accuracy and minimized computational overhead. Furthermore, improved non-local attention is incorporated to augment feature representation, consequently facilitating precise class label prediction. Experimental results demonstrate that FNANet outperforms state-of-the-art methods in terms of segmentation accuracy and speed on Cityscapes and CamVid.
C1 [Lan, Tianye; Dou, Furong; Feng, Ziliang] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
   [Zhang, Chengfang] Sichuan Police Coll, Intelligent Policing Key Lab Sichuan Prov, Luzhou, Peoples R China.
C3 Sichuan University; Sichuan Police College
RP Zhang, CF (corresponding author), Sichuan Police Coll, Intelligent Policing Key Lab Sichuan Prov, Luzhou, Peoples R China.
EM chengfangzhang@scpolicec.edu.cn
RI zhang, chengfang/AAB-5298-2022
FU Sichuan Science and Technology Program [2023NSFSC0495]; Luzhou Municipal
   Peopleapos;s Government Strategic cooperation projects [ZNJW2022ZZMS001,
   ZNJW2023ZZQN004]; Colleague Project of Intelligent Policing Key
   Laboratory of Sichuan Province
FX This work is supported by Sichuan Science and Technology Program
   (2023NSFSC0495), Sichuan University and Luzhou Municipal People's
   Government Strategic cooperation projects (2020CDLZ-10), and Colleague
   Project of Intelligent Policing Key Laboratory of Sichuan Province
   (ZNJW2022ZZMS001, ZNJW2023ZZQN004).
CR Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Cao H., 2021, arXiv
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Deng WX, 2022, IEEE T CYBERNETICS, V52, P10735, DOI 10.1109/TCYB.2021.3065247
   Ding P., 2023, J. Real-Time Image Process., V20
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao GW, 2022, Arxiv, DOI arXiv:2109.00699
   Hao SJ, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3154443
   Hao Y., 2022, arXiv
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu P, 2021, IEEE ROBOT AUTOM LET, V6, P263, DOI 10.1109/LRA.2020.3039744
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Liang C, 2022, Arxiv, DOI arXiv:2210.02025
   Lin PW, 2020, PROC CVPR IEEE, P4202, DOI 10.1109/CVPR42600.2020.00426
   Liu C, 2020, 2020 INTERNATIONAL SYMPOSIUM ON AUTONOMOUS SYSTEMS (ISAS), P249, DOI 10.1109/ISAS49493.2020.9378857
   Liu RR, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14133109
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Marin D, 2019, IEEE I CONF COMP VIS, P2131, DOI 10.1109/ICCV.2019.00222
   Niu R. G., 2022, Ieee Trans. Geosci. Remote Sens., V60
   Paszke A, 2016, Arxiv, DOI [arXiv:1606.02147, 10.48550/arXiv.1606.02147, DOI 10.48550/ARXIV.1606.02147]
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Song Q, 2021, AAAI CONF ARTIF INTE, V35, P2567
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sun P, 2021, INT J COMPUT VISION, V129, P1506, DOI 10.1007/s11263-021-01433-3
   Sun YP, 2022, Arxiv, DOI arXiv:2206.06122
   Tang YP, 2022, MULTIMED TOOLS APPL, V81, P21547, DOI 10.1007/s11042-022-12519-6
   Tiwari T, 2023, MULTIMED TOOLS APPL, V82, P3605, DOI 10.1007/s11042-022-13230-2
   Tsai TH, 2023, NEUROCOMPUTING, V532, P33, DOI 10.1016/j.neucom.2023.02.025
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang X., Laser Optoelectronics Progress, V59
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2486, DOI 10.1145/3219819.3219944
   Weng X, 2022, IEEE T CIRC SYST VID, V32, P4444, DOI 10.1109/TCSVT.2021.3121680
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu GA, 2023, IEEE T INTELL TRANSP, V24, P15897, DOI 10.1109/TITS.2023.3248089
   Yin H, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01373-7
   Yu C., 2021, INT J COMPUT VISION, V129, P3051, DOI [DOI 10.1007/s11263-021-01515-2, 10.1007/s11263-021-01515-2]
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang XL, 2022, NEURAL COMPUT APPL, V34, P3573, DOI 10.1007/s00521-022-06932-z
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zhuang MX, 2021, NEUROCOMPUTING, V459, P349, DOI 10.1016/j.neucom.2021.07.019
NR 61
TC 0
Z9 0
U1 16
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 23
PY 2023
DI 10.1007/s00371-023-03135-y
EA OCT 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U9GM1
UT WOS:001087814000001
DA 2024-07-18
ER

PT J
AU Chang, QJ
   Ma, WY
   Deng, CY
AF Chang, Qingjun
   Ma, Weiyin
   Deng, Chongyang
TI Constrained least square progressive and iterative approximation
   (CLSPIA) for B-spline curve and surface fitting
SO VISUAL COMPUTER
LA English
DT Article
DE B-spline; Interpolation and approximation; Data fitting; Progressive and
   iterative approximation (PIA); Least square progressive and iterative
   approximation (LSPIA)
ID INEXACT UZAWA ALGORITHM; CONVERGENCE ANALYSIS; INTERPOLATION
AB Combining the Lagrange multiplier method, the Uzawa algorithm, and the least square progressive and iterative approximation (LSPIA), we proposed the constrained least square progressive and iterative approximation (CLSPIA) to solve the problem of B-spline curve and surface fitting with constraint on data interpolation, i.e., computing the control points of a B-spline curve or surface which interpolates one set of input points while approximating the other set of given points. Compared with the method of solving the linear system directly, CLSPIA has some advantages as it inherits all the nice properties of LSPIA. Because of the data reuse property of LSPIA, CLSPIA reduces a great amount of computation. Using the local property of LSPIA, we can get shape preserving fitting curves by CLSPIA. CLSPIA is efficient for fitting large-scale data sets due to the fact that its computational complexity is linear to the scale of the input data. The many numerical examples in this paper show the efficiency and effectiveness of CLSPIA.
C1 [Chang, Qingjun] Univ Svizzera italiana, Fac Informat, CH-6900 Lugano, Switzerland.
   [Chang, Qingjun; Deng, Chongyang] Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
   [Ma, Weiyin] City Univ Hong Kong, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
C3 Universita della Svizzera Italiana; Hangzhou Dianzi University; City
   University of Hong Kong
RP Deng, CY (corresponding author), Hangzhou Dianzi Univ, Sch Sci, Hangzhou 310018, Peoples R China.
EM dcy@hdu.edu.cn
RI MA, Weiyin/K-9155-2015; Deng, Chongyang/E-4422-2017
OI MA, Weiyin/0000-0001-9760-7789; Chang, Qingjun/0000-0002-9426-2110;
   Deng, Chongyang/0000-0002-8725-4622
FU This work was supported by the Swiss National Science Foundation (SNF
   Grant No. 188577), the National Natural Science Foundation of China
   (Grant No. 61872121) and City University of Hong Kong (SRG Grant No.
   7004605). [188577]; Swiss National Science Foundation (SNF) [61872121];
   National Natural Science Foundation of China [7004605]; City University
   of Hong Kong
FX This work was supported by the Swiss National Science Foundation (SNF
   Grant No. 188577), the National Natural Science Foundation of China
   (Grant No. 61872121) and City University of Hong Kong (SRG Grant No.
   7004605).
CR [Anonymous], 1986, Curve and Surface Fitting: An Introduction
   [Anonymous], 1999, Geometry and the Imagination
   Axelsson O, 1996, ITERATIVE SOLUTION M
   Bai ZZ, 2008, LINEAR ALGEBRA APPL, V428, P2900, DOI 10.1016/j.laa.2008.01.018
   Benzi M, 2005, ACTA NUMER, V14, P1, DOI 10.1017/S0962492904000212
   [陈杰 Chen Jie], 2012, [自动化学报, Acta Automatica Sinica], V38, P135
   Chen J, 2011, COMPUT AIDED DESIGN, V43, P889, DOI 10.1016/j.cad.2011.03.012
   Chen Sugen, 2014, Computer Engineering and Applications, V50, P152, DOI 10.3778/j.issn.1002-8331.1309-0015
   Chen Z., 2006, FINITE ELEMENT METHO
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Cheng FH, 2009, J COMPUT SCI TECH-CH, V24, P39, DOI 10.1007/s11390-009-9199-2
   Cheng F, 2008, GEOMETRIC MODELING & IMAGING: MODERN TECHNIQUES AND APPLICATIONS, P27, DOI 10.1109/GMAI.2008.15
   Cheng F, 2008, LECT NOTES COMPUT SC, V4975, P526
   Cheng XL, 2000, SIAM J NUMER ANAL, V37, P1930, DOI 10.1137/S0036142998349266
   de Boor C, 1979, ARO report 79-3, P299
   De Boor C., 1978, A practical guide to splines, DOI [10.1007/978-1-4612-6333-3, DOI 10.1007/978-1-4612-6333-3]
   Delgado J, 2010, LECT NOTES COMPUT SC, V5862, P136, DOI 10.1007/978-3-642-11620-9_10
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   ELMAN HC, 1994, SIAM J NUMER ANAL, V31, P1645, DOI 10.1137/0731085
   Fan F., 2008, COMPUT AIDED DESIGN, V5, P539, DOI 10.3722/cadaps.2008.539-547
   Golub G. H., 1983, MATRIX COMPUTATIONS
   He SS, 2015, J COMPUT DES ENG, V2, P218, DOI 10.1016/j.jcde.2015.06.002
   J Flanigan F., 1998, Calculus Two: Linear and Nonlinear Functions
   Ke YL, 2006, COMPUT AIDED DESIGN, V38, P101, DOI 10.1016/j.cad.2005.07.004
   Kineri Y, 2012, COMPUT AIDED DESIGN, V44, P697, DOI 10.1016/j.cad.2012.02.011
   Lin HW, 2018, J SYST SCI COMPLEX, V31, P1618, DOI 10.1007/s11424-018-7443-y
   Lin HW, 2018, COMPUT AIDED DESIGN, V95, P40, DOI 10.1016/j.cad.2017.10.002
   Lin HW, 2011, COMPUT GRAPH-UK, V35, P967, DOI 10.1016/j.cag.2011.07.003
   Lin HW, 2010, COMPUT AIDED DESIGN, V42, P505, DOI 10.1016/j.cad.2010.01.006
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Lin YQ, 2006, CALCOLO, V43, P65, DOI 10.1007/s10092-006-0117-5
   Loucera C., 2018, LEVY FLIGHT DRIVEN S, P149
   Lu JF, 2014, APPL MATH SER B, V29, P29, DOI 10.1007/s11766-014-2828-8
   Lu JF, 2010, SIAM J MATRIX ANAL A, V31, P1934, DOI 10.1137/090756235
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Maekawa T, 2007, COMPUT AIDED DESIGN, V39, P313, DOI 10.1016/j.cad.2006.12.008
   Nishiyama Y., 2008, P PAC GRAPH 2008, V2008, P67
   [庞宏奎 Pang Hongkui], 2009, [计算数学, Mathematica Numerica Sinica], V31, P231
   Park H, 2004, COMPUT AIDED GEOM D, V21, P479, DOI 10.1016/j.cagd.2004.03.003
   Park H, 2007, COMPUT AIDED DESIGN, V39, P439, DOI 10.1016/j.cad.2006.12.006
   Pereyra V, 2003, APPL NUMER MATH, V44, P225, DOI 10.1016/S0168-9274(02)00147-2
   Piegl L., 1997, The NURBS Book, V2, DOI DOI 10.1007/978-3-642-59223-2
   Qi D, 1975, Acta Math Sin, V18, P173
   ROGERS DF, 1989, COMPUT AIDED DESIGN, V21, P641, DOI 10.1016/0010-4485(89)90162-0
   Saad Y., 2003, Iterative methods for sparse linear systems, V2, DOI DOI 10.1137/1.9780898718003
   [史利民 SHI Limin], 2006, [数学研究与评论, Journal of Mathematical Research and Exposition], V26, P735
   Smith RE., 1974, SMOOTHING ALGORITHM
   Uyar K, 2017, APPL MATH MODEL, V52, P320, DOI 10.1016/j.apm.2017.07.047
   Uzawa H., 1958, STUDIES LINEAR NONLI, DOI DOI 10.1017/S0008439500025522
   Vandergraft JS., 1983, INTRO NUMERICAL COMP, P89, DOI DOI 10.1016/B978-0-12-711356-2.50010-X
   Xiong YH, 2012, COMPUT GRAPH-UK, V36, P884, DOI 10.1016/j.cag.2012.07.002
   Yamaguchi F., 1977, J JAPAN SOC PRECIS E, V43, P168
   Yu Zhao, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P239, DOI 10.1109/CAD/Graphics.2011.73
   Zhang L., 2015, J INF COMPUT SCI, V12, P865, DOI [10.12733/jics20105302, DOI 10.12733/JICS20105302]
   Zhang L, 2016, VISUAL COMPUT, V32, P1109, DOI 10.1007/s00371-015-1170-3
   [张莉 Zhang Li], 2014, [中国图象图形学报, Journal of Image and Graphics], V19, P275
   Zheng WN, 2012, COMPUT AIDED GEOM D, V29, P448, DOI 10.1016/j.cagd.2012.03.004
NR 58
TC 0
Z9 0
U1 6
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4427
EP 4439
DI 10.1007/s00371-023-03090-8
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001079888400005
DA 2024-07-18
ER

PT J
AU Su, SZ
   Tang, ZF
   Zhu, YM
AF Su, Shuzhi
   Tang, Zefang
   Zhu, Yanmin
TI High-density foreground object detection in optical remote sensing
   images via semantic fusion and box alignment
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Bounding box loss functions; Convolutional modules;
   Small objects; Auxiliary-point balancing IoU
AB Accuracy and effectiveness towards multiscale and dense remote sensing multivariate 2D information with object detection of bi-directional learning method remains challenging. Most methods require the design of complex network structures or bounding box loss functions, thus neglecting computational cost and training noise. To facilitate practical applications, a novel optical remote sensing of the bi-directional learning object detection (ORS-BLOD) is proposed in this paper. In the method, the positive direction mechanism contains two feature re-identification convolutional modules, which can effectively distinguish complex internal texture features and improve the accuracy of small objects. The method further designs a novel auxiliary-point balancing IoU (ABIoU) loss in the reverse direction mechanism. The novel loss not only can avoid the local optimum solutions of Euclidean distance term in single-pair points regression but also can avoid IoU loss non-converging for local aspect ratio, which can realize the stability of the loss values and the direct measure of the side length. During the training phase, ABIoU loss does not produce additional parameters and improves the accuracy of box position and the integrity of aspect ratio. mAP50 of our method can, respectively, reach 73.3%, 87.03% and 56.84% on DIOR, DIOR6 and VOC2007 object detection data sets, and the high-precision and portability of our method are revealed by extensive experiment results and analysis.
C1 [Su, Shuzhi; Tang, Zefang] Anhui Univ Sci & Technol, Sch Comp Sci & Engn, Huainan 232001, Peoples R China.
   [Zhu, Yanmin] Anhui Univ Sci & Technol, Sch Mech Engn, Huainan 232001, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology
RP Zhu, YM (corresponding author), Anhui Univ Sci & Technol, Sch Mech Engn, Huainan 232001, Peoples R China.
EM Zyanmin1988@163.com
RI 不计后果, Tonze/HNT-0176-2023
FU The work was supported by the Natural Science Research Project of
   Colleges and Universities in Anhui Province (No. 2022AH040113), the
   University Synergy Innovation Program of Anhui Province (No.
   GXXT-2021-006), the Postdoctoral Science Foundation of China
   [2022AH040113]; Natural Science Research Project of Colleges and
   Universities in Anhui Province [GXXT-2021-006]; University Synergy
   Innovation Program of Anhui Province [2019M660149]; Postdoctoral Science
   Foundation of China [52374155]; National Natural Science Foundation of
   China [2308085MF218]; Anhui Provincial Natural Science Foundation
FX The work was supported by the Natural Science Research Project of
   Colleges and Universities in Anhui Province (No. 2022AH040113), the
   University Synergy Innovation Program of Anhui Province (No.
   GXXT-2021-006), the Postdoctoral Science Foundation of China (No.
   2019M660149), the National Natural Science Foundation of China Grant
   (No. 52374155) and Anhui Provincial Natural Science Foundation (No.
   2308085MF218).
CR Assunçao E, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174217
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen JR, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119407
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Ciampiconi L., 2023, ARXIV
   Cui M, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3240428
   Dong C, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109256
   Dong XD, 2022, ENG APPL ARTIF INTEL, V113, DOI 10.1016/j.engappai.2022.104914
   Dong YF, 2021, APPL ACOUST, V174, DOI 10.1016/j.apacoust.2020.107740
   Duan K. etal, 2021, ARXIV
   Dubey S. R., 2022, Neurocomputing
   Ghandorh H, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14030613
   Girshick R., 2015, IEEE I CONF COMP VIS, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Li LY, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14071534
   Li PL, 2022, EXPERT SYST APPL, V205, DOI 10.1016/j.eswa.2022.117296
   Li RH, 2023, SIGNAL PROCESS, V208, DOI 10.1016/j.sigpro.2023.108962
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Zhuang, 2022, P IEEECVF C COMPUTER, P11976, DOI [DOI 10.48550/ARXIV.2201.03545, 10.1109/CVPR52688.2022.01167, DOI 10.1109/CVPR52688.2022.01167]
   Obeso AM, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108411
   Qian XL, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15051259
   Rashed H, 2021, IEEE WINT CONF APPL, P2271, DOI 10.1109/WACV48630.2021.00232
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen YY, 2022, NEUROCOMPUTING, V500, P99, DOI 10.1016/j.neucom.2022.05.052
   Sun X, 2021, ISPRS J PHOTOGRAMM, V173, P50, DOI 10.1016/j.isprsjprs.2020.12.015
   Tian D, 2022, NEUROCOMPUTING, V500, P1029, DOI 10.1016/j.neucom.2022.06.018
   Tong Z., 2023, ARXIV
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Chen, 2021, arXiv
   Wang XW, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107035
   Woo S. etal, 2023, ARXIV
   Wu YM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3137903
   Xia Z., 2022, VISION TRANSFORMER D, P4794
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Zand M, 2022, LECT NOTES COMPUT SC, V13670, P390, DOI 10.1007/978-3-031-20080-9_23
   Zeng NY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3153997
   Zhang YY, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105453
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zhao C, 2023, MEASUREMENT, V214, DOI 10.1016/j.measurement.2023.112776
   Zheng T., 2022, SCALOSS SIDE CORNER, V36, P3535
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou DF, 2019, INT CONF 3D VISION, P85, DOI 10.1109/3DV.2019.00019
NR 44
TC 0
Z9 0
U1 5
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4355
EP 4371
DI 10.1007/s00371-023-03086-4
EA OCT 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001079888400002
DA 2024-07-18
ER

PT J
AU Ho, CC
   Chiao, YC
   Su, E
AF Ho, Chao-Ching
   Chiao, Yuan-Cheng
   Su, Eugene
TI Reinforcement learning-based approach for plastic texture surface
   defects inspection
SO VISUAL COMPUTER
LA English
DT Article
DE Automated optical inspection; Texture synthesis; Reinforcement learning;
   Transfer learning; Deep learning
AB This paper proposes a novel data-enhanced virtual texture generation network for use in deep learning detection systems. The current methods of data enhancement, such as image flipping, scaling ratios, or Generative Adversarial Networks, have limitations as they cannot determine characteristics beyond the training data. The proposed system uses the texture characteristics of a learning surface to generate surface textures through the Open Graphics Library, which can simulate material textures, light sources, and shadow effects. This enables the generation of required texture parameters for the Reinforcement Learning Network to conduct parameter search. The generated image is authenticated by a discriminator, and the reward score is fed back into the critic network to update the value network. The proposed system can complement the imbalance of defective data types, generate large quantities of random and non-defective data, and automatically classify and label during the generation process, reducing labor consumption and improving labeling accuracy. The study found that the proposed data enhancement method can increase the diversity of data characteristics, and the generated data can increase the recall rate of test and verification data sets. Specifically, the proposed system increased the recall rate of test data sets with different distributions from 78.21% to 82.40% and the recall rate of verification data sets with the same distribution from 81.64 to 91.94%.
C1 [Ho, Chao-Ching; Chiao, Yuan-Cheng; Su, Eugene] Natl Taipei Univ Technol, Grad Inst Mfg Technol, Dept Mech Engn, Taipei, Taiwan.
C3 National Taipei University of Technology
RP Ho, CC (corresponding author), Natl Taipei Univ Technol, Grad Inst Mfg Technol, Dept Mech Engn, Taipei, Taiwan.
EM hochao@mail.ntut.edu.tw
CR Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   Alonso-Monsalve S, 2020, IEEE T NEUR NET LEAR, V31, P5645, DOI 10.1109/TNNLS.2020.2969327
   Andresini G, 2021, FUTURE GENER COMP SY, V123, P108, DOI 10.1016/j.future.2021.04.017
   [Anonymous], 2008, ELCVIA Electron. Lett. Comput. Vis. Image Anal, DOI DOI 10.5565/REV/ELCVIA.268
   Bi S, 2019, IEEE I CONF COMP VIS, P2730, DOI 10.1109/ICCV.2019.00282
   Chen J., 2019, NEUROCOMPUTING, V25
   Chen Y.-F., 2019, INT C ENG SCI IND AP, P22, DOI DOI 10.1109/ICCV.2019.01012
   Chung-Chi Huang, 2018, MATEC Web of Conferences, V201, DOI 10.1051/matecconf/201820101010
   Gutierrez J, 2020, COMPUT GRAPH FORUM, V39, P511, DOI 10.1111/cgf.13889
   Ho CC, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3144224
   Inoue T., 2017, ARXIV
   Jian CX, 2017, APPL SOFT COMPUT, V52, P348, DOI 10.1016/j.asoc.2016.10.030
   Jiang Liming, 2021, ADV NEURAL INF PROCE, V34, P21655
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Lin L., 2020, INT C ADV HYBR INF P, P320
   Marceau L., 2019, ARXIV
   Mujeeb A, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P391, DOI 10.1109/CW.2018.00076
   Sharma V, 2023, VISUAL COMPUT, V39, P6503, DOI 10.1007/s00371-022-02742-5
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Tao X, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091575
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   Wang H., 2022, P IEEE C COMP VIS PA, P1
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   Yu YZ, 1999, COMP GRAPH, P215
   Zhang S, 2023, VISUAL COMPUT, V39, P5375, DOI 10.1007/s00371-022-02665-1
   Zhou F, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153159
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 27
TC 0
Z9 0
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4201
EP 4220
DI 10.1007/s00371-023-03077-5
EA SEP 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001072285000002
DA 2024-07-18
ER

PT J
AU Zhang, W
   Fan, WS
   Yang, X
   Zhang, Q
   Zhou, DS
AF Zhang, Wei
   Fan, Wanshu
   Yang, Xin
   Zhang, Qiang
   Zhou, Dongsheng
TI Lightweight single-image super-resolution via multi-scale feature fusion
   CNN and multiple attention block
SO VISUAL COMPUTER
LA English
DT Article
DE Lightweight single-image super-resolution; Multi-scale feature fusion;
   Multiple attention block; Transformer
AB In recent years, single-image super-resolution (SISR) has acquired tremendous progresswith the development of deep learning. However, the majority of SISR methods based on deep learning focus on building more complex networks, which inevitably lead to the problems of computational and memory costs. Thus, these methods may fail to be applied in real-world scenarios. To solve this problem, this paper proposes a lightweight convolution network combined with transformer for SISR named as MMSR. Specifically, an efficient convolutional neural network (CNN) based on multi-scale feature fusion is designed for local feature extraction, which is called MFF-CNN. In addition, we propose a simple and efficient multiple attention block (MAB) to further utilize the context information in features. MAB incorporates channel attention and transformer to help network obtain similar features at a long-term dependence, making full use of global information to further refine texture details. Finally, this paper provides comprehensive results for different settings of the entire network. Experimental results on common used datasets demonstrate that the proposed method can achieve better performances at the 2x, 3x and 4x scales than other state-of-the-art lightweight methods.
C1 [Zhang, Wei; Fan, Wanshu; Zhang, Qiang; Zhou, Dongsheng] Dalian Univ, Sch Software Engn, Natl & Local Joint Engn Lab Comp Aided Design, Dalian 116622, Peoples R China.
   [Yang, Xin; Zhang, Qiang; Zhou, Dongsheng] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
C3 Dalian University; Dalian University of Technology
RP Fan, WS; Zhou, DS (corresponding author), Dalian Univ, Sch Software Engn, Natl & Local Joint Engn Lab Comp Aided Design, Dalian 116622, Peoples R China.; Zhou, DS (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
EM fan921amber@163.com; zhouds@dlu.edu.cn
RI CHEN, BING/KHX-6659-2024; WANG, YUHAO/KBB-0213-2024; ren,
   jing/JXN-8411-2024; yu, xiao/KFT-1725-2024; Li, Chun/KBC-9591-2024; Yao,
   Chen/JVD-6226-2023; tian, ye/KGL-6485-2024; liu, zhao/KGM-5884-2024;
   wang, shuo/KCL-3379-2024
OI Fan, Wanshu/0000-0001-6299-2795
FU National Key Research and Development Program of China [2021ZD0112400];
   National Natural Science Foundation of China [U1908214]; Program for
   Innovative Research Team in University of Liaoning Province [LT2020015];
   Support Plan for Key Field Innovation Team of Dalian [2021RT06]; Support
   Plan for Leading Innovation Team of Dalian University [XLJ202010];
   Program for the Liaoning Province Doctoral Research Starting Fund
   [2022-BS-336]; Key Laboratory of Advanced Design and Intelligent
   Computing (Dalian University), Ministry of Education [ADIC2022003];
   Interdisciplinary project of Dalian University [DLUXK-2023-QN-015]
FX This work was supported in part by the National Key Research and
   Development Program of China (Grant No. 2021ZD0112400), National Natural
   Science Foundation of China (Grant No. U1908214), the Program for
   Innovative Research Team in University of Liaoning Province (Grant No.
   LT2020015), the Support Plan for Key Field Innovation Team of Dalian
   (2021RT06), the Support Plan for Leading Innovation Team of Dalian
   University (XLJ202010), Program for the Liaoning Province Doctoral
   Research Starting Fund (Grant No. 2022-BS-336), Key Laboratory of
   Advanced Design and Intelligent Computing (Dalian University), Ministry
   of Education (Grant No. ADIC2022003), Interdisciplinary project of
   Dalian University (Grant No. DLUXK-2023-QN-015).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L., 2022, ARXIV
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Dai XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2968, DOI 10.1109/ICCV48922.2021.00298
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Gao G., 2022, P INT JOINT C ART IN, P913, DOI DOI 10.24963/IJCAI.2022/128
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Li C, 2016, IEEE INT SEMICONDUCT
   Li W., 2020, ADV NEURAL INF PROCE, V33, P20343
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J., 2021, arXiv
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soh JW, 2020, IEEE ACCESS, V8, P35383, DOI 10.1109/ACCESS.2020.2974876
   Sun B., 2022, arXiv
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, ARXIV
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xiao T, 2021, ADV NEUR IN, V34
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 48
TC 3
Z9 3
U1 4
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3519
EP 3531
DI 10.1007/s00371-023-03021-7
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035766800004
OA Bronze
DA 2024-07-18
ER

PT J
AU Xiang, N
   Liang, HN
   Yu, LY
   Yang, XS
   Zhang, JJ
AF Xiang, Nan
   Liang, Hai-Ning
   Yu, Lingyun
   Yang, Xiaosong
   Zhang, Jian J.
TI A mixed reality framework for microsurgery simulation with
   visual-tactile perception
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual surgery; Mixed reality; 3D tracking; 3D modeling
ID VISION-BASED TRACKING; VIRTUAL-REALITY; TRAINING MODEL; MOTION;
   INSTRUMENTS; VALIDATION
AB Microsurgery is a general term for surgery combining surgical microscope and specialized precision instruments during operation. Training in microsurgery requires considerable time and training resources. With the rapid development of computer technologies, virtual surgery simulation has gained extensive attention over the past decades. In this work, we take advantage of mixed reality (MR) that creates an interactive environment where physical and digital objects coexist, and present an MR framework for the microsurgery simulation. It enables users to practice anastomosis skills with real microsurgical instruments rather than additional haptic feedback devices that are typically used in virtual reality-based systems, and to view a realistic rendering intra-operative scene at the same time, thus creating an immersive training experience with such a visual-tactile interactive environment. A vision-based tracking system is proposed to simultaneously track microsurgical instruments and artificial blood vessels, and a learning-based anatomical modeling approach is introduced to facilitate the development of simulations in different microsurgical specialities by rapidly creating virtual assets. Moreover, we build a prototype system for the simulation specializing in microvascular hepatic artery reconstruction to demonstrate the feasibility and applicability of our framework.
C1 [Xiang, Nan; Liang, Hai-Ning; Yu, Lingyun] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.
   [Yang, Xiaosong; Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Poole, England.
C3 Xi'an Jiaotong-Liverpool University; Bournemouth University
RP Xiang, N (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.; Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole, England.
EM Nan.Xiang@xjtlu.edu.cn; HaiNing.Liang@xjtlu.edu.cn;
   Lingyun.Yu@xjtlu.edu.cn; xyang@bournemouth.ac.uk;
   jzhang@bournemouth.ac.uk
RI Yang, Ning/KHD-1133-2024
OI Liang, Hai-Ning/0000-0003-3600-8955; XIANG, NAN/0000-0003-4028-2287
FU XJTLU Research Development Fund [RDF-21-02-065]; High Education
   Innovation Fund (UK-HEIF)
FX AcknowledgementsThis work was supported by XJTLU Research Development
   Fund No. RDF-21-02-065 and Neuravatar Project funded by High Education
   Innovation Fund (UK-HEIF).
CR Alaraj A, 2015, OPER NEUROSURG, V11, P52, DOI 10.1227/NEU.0000000000000583
   Antiga L, 2008, MED BIOL ENG COMPUT, V46, P1097, DOI 10.1007/s11517-008-0420-1
   Atlan M, 2018, J SURG EDUC, V75, P182, DOI 10.1016/j.jsurg.2017.06.008
   Bhalodia R, 2018, LECT NOTES COMPUT SC, V11167, P244, DOI 10.1007/978-3-030-04747-4_23
   Birbara NS, 2019, HEART LUNG CIRC, V28, P302, DOI 10.1016/j.hlc.2017.10.017
   Brown J, 2002, MED IMAGE ANAL, V6, P289, DOI 10.1016/S1361-8415(02)00086-5
   Bucking TM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178540
   Byvaltsev VA, 2018, MINIM INVASIVE SURG, V2018, DOI 10.1155/2018/6130286
   Chan WY, 2007, MICROSURG, V27, P494, DOI 10.1002/micr.20393
   Chen WF, 2014, J PLAST RECONSTR AES, V67, P973, DOI 10.1016/j.bjps.2014.03.024
   Creighton FX, 2017, LARYNGOSCOPE INVEST, V2, P471, DOI 10.1002/lio2.94
   Crouch G, 2021, ANZ J SURG, V91, P1110, DOI 10.1111/ans.16721
   De Virgilio A, 2020, EUR ARCH OTO-RHINO-L, V277, P2589, DOI 10.1007/s00405-020-06014-7
   deGiacomoCarneiro C., 2009, RABBIT EXPT MODEL LA
   Erel E, 2003, MICROSURG, V23, P147, DOI 10.1002/micr.10106
   Fanua SP, 2001, MICROSURG, V21, P379, DOI 10.1002/micr.21812
   Gadwe A, 2019, IEEE SENS J, V19, P2338, DOI 10.1109/JSEN.2018.2886418
   Grober ED, 2003, MICROSURG, V23, P317, DOI 10.1002/micr.10152
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   Hosain M., 2021, J ADV MED MED RES, P97
   Hoshyarmanesh H, 2021, MECHATRONICS, V73, DOI 10.1016/j.mechatronics.2020.102481
   Javid P, 2019, MICROSURG, V39, P655, DOI 10.1002/micr.30513
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kazemi H, 2010, MICROSURG, V30, P479, DOI 10.1002/micr.20766
   Kong F., 2021, ARXIV
   Liu MY, 2010, PROC CVPR IEEE, P1696, DOI 10.1109/CVPR.2010.5539837
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Liu XY, 2021, J MED IMAGING, V8, DOI 10.1117/1.JMI.8.1.015001
   Microsurgeon.org, 2021, MICR DEF
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Neal ML, 2010, BRIEF BIOINFORM, V11, P111, DOI 10.1093/bib/bbp049
   O'Toole RV, 1999, J AM COLL SURGEONS, V189, P114, DOI 10.1016/S1072-7515(99)00076-9
   Pharr M., 2016, Physically Based Rendering: From Theory to Implementation, V3rd ed.
   Rangarajan K, 2020, J SURG EDUC, V77, P337, DOI 10.1016/j.jsurg.2019.09.006
   Remie R, 2001, LAB ANIMAL, V30, P48
   rfwireless-world, 2021, WIRELESSWORLD ADV XR
   Richa R, 2010, INT J ROBOT RES, V29, P218, DOI 10.1177/0278364909356600
   Robu M., 2020, COMP METHOD BIOMEC, P1
   Rodríguez A, 2007, J RECONSTR MICROSURG, V23, P251, DOI 10.1055/s-2007-985204
   Roohi, 2020, CURRENT PERSPECTIVES
   Ryu J, 2013, ARTIF ORGANS, V37, P107, DOI 10.1111/j.1525-1594.2012.01543.x
   Sánchez-González P, 2011, MINIM INVASIV THER, V20, P311, DOI 10.3109/13645706.2010.541921
   Schoob A, 2017, MED IMAGE ANAL, V40, P80, DOI 10.1016/j.media.2017.06.004
   Senior MA, 2001, ANN ROY COLL SURG, V83, P358
   Song JW, 2018, IEEE ROBOT AUTOM LET, V3, P155, DOI 10.1109/LRA.2017.2735487
   StandfordMedicine, 2021, MICR ESS
   Tahir W., 2021, BME FRONTIERS, V2021
   Tamai Susumu, 2009, Plast Reconstr Surg, V124, pe282, DOI 10.1097/PRS.0b013e3181bf825e
   Taylor CA, 2009, ANNU REV BIOMED ENG, V11, P109, DOI 10.1146/annurev.bioeng.10.061807.160521
   Wanderer S, 2020, JOVE-J VIS EXP, DOI 10.3791/61157
   Wong WK, 2013, IEEE-ASME T MECH, V18, P1472, DOI 10.1109/TMECH.2012.2203919
   Wu PC, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P365, DOI 10.1145/3126594.3126664
   Xiang N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4454, DOI 10.1145/3474085.3475597
   Xiang N, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1939
   Xiao ZY, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84619-6
   Xu L, 2018, INT J COMPUT ASS RAD, V13, P1019, DOI 10.1007/s11548-018-1739-1
   Yamamoto T, 2017, MICROSURG, V37, P57, DOI 10.1002/micr.22335
   Yang B, 2014, PATTERN RECOGN, V47, P2962, DOI 10.1016/j.patcog.2014.03.020
   Yang L, 2015, COMPUT MED IMAG GRAP, V40, P205, DOI 10.1016/j.compmedimag.2014.09.003
   Yang LJ, 2020, IET CYBER-SYST ROBOT, V2, P107, DOI 10.1049/iet-csr.2020.0013
   Yu P, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1940
   Yu Q, 2017, PATTERN RECOGN, V65, P82, DOI 10.1016/j.patcog.2016.11.020
NR 63
TC 2
Z9 2
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3661
EP 3673
DI 10.1007/s00371-023-02964-1
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019508700002
DA 2024-07-18
ER

PT J
AU Yang, AL
   Jin, ZD
   Guo, S
   Wu, DK
   Chen, L
AF Yang, Aolei
   Jin, Zhouding
   Guo, Shuai
   Wu, Dakui
   Chen, Ling
TI Unconstrained human gaze estimation approach for medium-distance scene
   based on monocular vision
SO VISUAL COMPUTER
LA English
DT Article
DE Human gaze estimation; Monocular vision; Recurrent convolution;
   Medium-distance scene
ID TRACKING
AB This paper proposes an unconstrained human gaze estimation approach for medium-distance scene based on monocular vision. A recurrent convolution neural network for gaze estimation is designed to construct the mapping relationship from the face image to the 3D gaze vector in the camera space, so as to complete the estimation of the human gaze. Firstly, based on the VICON system and a monocular camera, the face image sequence and gaze vector sequence of human gaze behavior are collected in different situations of medium-distance scenes to construct the time-synchronized gaze estimation dataset. Secondly, the recurrent convolutional gaze estimation neural network is designed, and the head pose feature and eye appearance feature are extracted from single frame image with monocular vision image sequences as input. Thirdly, the extracted gaze features are fused in the spatial dimension and the temporal dimension to estimate the direction of the human gaze. Finally, the cross-validation and experimental evaluation of the estimation errors of the constructed gaze estimation model are carried out. The results show that the Cross-Person experimental average error is 7.65 degrees, and the Person-Specific experimental error is 3.88 degrees. Compared with the Gaze360 method, the proposed approach reduces the average evaluation error of the gaze angle by 5.0% in the Cross-Person experiments and 16.6% with the Person-Specific experiments. These experiments verify the effectiveness of the proposed approach, and it has better generalization ability and robustness.
C1 [Yang, Aolei; Jin, Zhouding; Guo, Shuai; Wu, Dakui] Shanghai Univ, Shanghai, Peoples R China.
   [Chen, Ling] Hunan Normal Univ, Coll Engn & Design, Changsha, Peoples R China.
C3 Shanghai University; Hunan Normal University
RP Yang, AL (corresponding author), Shanghai Univ, Shanghai, Peoples R China.
EM ayang02@qub.ac.uk; dkwu306@shu.edu.cn
OI Yang, Aolei/0000-0002-5077-9150
FU Natural Science Foundation of Shanghai [22ZR1424200]
FX AcknowledgmentsThis work is supported by Natural Science Foundation of
   Shanghai under Grant 22ZR1424200.
CR Baltrusaitis T, 2014, LECT NOTES COMPUT SC, V8692, P593, DOI 10.1007/978-3-319-10593-2_39
   Bao J, 2022, IEEE T IMAGE PROCESS, V31, P3322, DOI 10.1109/TIP.2022.3171416
   Cazzato D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133739
   Cheng Y.C., 2021, ARXIV
   Cheng YH, 2022, AAAI CONF ARTIF INTE, P436
   Cheng YH, 2020, AAAI CONF ARTIF INTE, V34, P10623
   Cheng YH, 2020, IEEE T IMAGE PROCESS, V29, P5259, DOI 10.1109/TIP.2020.2982828
   Harezlak K, 2018, COMPUT MED IMAG GRAP, V65, P176, DOI 10.1016/j.compmedimag.2017.04.006
   Huang JH, 2021, WIREL COMMUN MOB COM, V2021, DOI 10.1155/2021/8213946
   Huang Q, 2017, MACH VISION APPL, V28, P445, DOI 10.1007/s00138-017-0852-4
   Kellnhofer P, 2019, IEEE I CONF COMP VIS, P6911, DOI 10.1109/ICCV.2019.00701
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li JF, 2016, IEEE T HUM-MACH SYST, V46, P414, DOI 10.1109/THMS.2015.2477507
   Liu G, 2021, IEEE T PATTERN ANAL, V43, P1092, DOI 10.1109/TPAMI.2019.2957373
   Lu H., 2008, P INT WORKSH MOB DEV, P1
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   Ranjan R, 2018, IEEE COMPUT SOC CONF, P2237, DOI 10.1109/CVPRW.2018.00290
   Sesma-Sanchez L, 2012, IEEE T BIO-MED ENG, V59, P2235, DOI 10.1109/TBME.2012.2201716
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   TSAI RY, 1989, IEEE T ROBOTIC AUTOM, V5, P345, DOI 10.1109/70.34770
   Yang AL, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03399-z
   Yoon HS, 2019, MULTIMED TOOLS APPL, V78, P7155, DOI 10.1007/s11042-018-6490-7
   Zhang X., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI 10.1109/CVPR.2015.7299081
   Zhang XC, 2019, IEEE T PATTERN ANAL, V41, P162, DOI 10.1109/TPAMI.2017.2778103
   Zhou XL, 2020, IEEE ACCESS, V8, P82142, DOI 10.1109/ACCESS.2020.2990685
NR 26
TC 1
Z9 1
U1 4
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 73
EP 85
DI 10.1007/s00371-022-02766-x
EA FEB 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000934214900001
DA 2024-07-18
ER

PT J
AU Hassan, MF
   Adam, T
   Rajagopal, H
   Paramesran, R
AF Hassan, Mohd Fikree
   Adam, Tarmizi
   Rajagopal, Heshalini
   Paramesran, Raveendran
TI A hue preserving uniform illumination image enhancement via triangle
   similarity criterion in HSI color space
SO VISUAL COMPUTER
LA English
DT Article
DE Hue preservation; Low light image; Image enhancement; HSI color space;
   Model-based method
AB Color images are essential in computer vision applications such as surveillance and security systems. However, due to the low light conditions, the color of the obtained image deviates from the original color. Moreover, it would produce inaccurate results that limit the performance of these applications. Therefore, this paper proposes a hue preserving uniform illumination image enhancement via triangle similarity criterion in hue-saturation-intensity (HSI) color space. Based on this, the proposed method develops translation and scaling operations to enhance the intensity and saturation. These enhancement processes maintain the hue and features while having minimal effect on the mean brightness. The proposed method and five model-based enhancement methods are evaluated from five perspectives: subjective visual evaluation, the hue and features preservation capabilities, the ability to enhance the contrast and visual information, mean brightness preservation capability, and computational complexity. Results indicate that the proposed method produces better-enhanced images than the other five model-based enhancement methods.
C1 [Hassan, Mohd Fikree] Int Univ Malaya Wales, Fac Arts & Sci, Jalan Tun Ismail, Kuala Lumpur 50480, Malaysia.
   [Adam, Tarmizi] Univ Teknol Malaysia, Fac Comp, Skudai 81310, Johor, Malaysia.
   [Rajagopal, Heshalini] UCSI Univ, Inst Comp Sci & Digital Innovat, Jalan Puncak Menara Gading, Taman Connaught, Cheras 56000, Kuala Lumpur, Malaysia.
   [Paramesran, Raveendran] Univ Malaya, Fac Engn, Dept Elect Engn, Jalan Prof Diraja Ungku Aziz, Kuala Lumpur 50603, Malaysia.
C3 Universiti Malaya; Universiti Teknologi Malaysia; UCSI University;
   Universiti Malaya
RP Hassan, MF (corresponding author), Int Univ Malaya Wales, Fac Arts & Sci, Jalan Tun Ismail, Kuala Lumpur 50480, Malaysia.
EM fikree@iumw.edu.my
RI BIN ADAM, TARMIZI/GPX-5943-2022; Hassan, Mohd Fikree/AAD-8538-2021
OI BIN ADAM, TARMIZI/0000-0002-5599-5071; Hassan, Mohd
   Fikree/0000-0003-4878-4695
FU International University of Malaya-Wales (IUMW), under IUMW Internal
   Research Grant; Universiti Teknologi Malaysia, under the UTM
   Encouragement Research (UTMER) grant [RMC/2022-23/03];  [PY/2021/01263]
FX The research work is financially supported by the International
   University of Malaya-Wales (IUMW), under IUMW Internal Research Grant
   2022, Grant Number: RMC/2022-23/03. In addition, the research of Tarmizi
   Adam is financially supported by Universiti Teknologi Malaysia, under
   the UTM Encouragement Research (UTMER) grant PY/2021/01263.
CR Akhand MAH, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091036
   [Anonymous], 2005, 1 INT WORKSHOP VIDEO
   [Anonymous], 2016, Journal of Telecommunication, Electronic and Computer Engineering
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen BW, 2021, IEEE T GEOSCI REMOTE, V59, P3567, DOI 10.1109/TGRS.2020.3006577
   Chien CL, 2014, IEEE T GEOSCI REMOTE, V52, P651, DOI 10.1109/TGRS.2013.2243157
   Dai Q., 2019, SYMMETRY, V11, P1
   Dixit A.K., 2019, INT J COMPUT SCI ENG, V7, P263
   Dong X, 2011, IEEE INT CON MULTI
   Economopoulos TL, 2010, IMAGE VISION COMPUT, V28, P45, DOI 10.1016/j.imavis.2009.04.011
   Gonzalez R.C., 2018, Digital Image Processing
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hassan MF, 2022, MULTIMED TOOLS APPL, V81, P26331, DOI 10.1007/s11042-022-12429-7
   Hu P., 2022, IEEE T PATTERN ANALM, V1
   Hu P, 2022, IEEE T CYBERNETICS, V52, P12954, DOI 10.1109/TCYB.2021.3093626
   Huang CY, 2022, IEEE SIGNAL PROC LET, V29, P1417, DOI 10.1109/LSP.2022.3182143
   Katircioglu F, 2020, IET IMAGE PROCESS, V14, P3202, DOI 10.1049/iet-ipr.2020.0393
   Kim D, 2017, IEEE SIGNAL PROC LET, V24, P804, DOI 10.1109/LSP.2017.2687945
   Ko S, 2017, SIGNAL PROCESS-IMAGE, V58, P99, DOI 10.1016/j.image.2017.06.016
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li GL, 2020, MULTIMED TOOLS APPL, V79, P30883, DOI 10.1007/s11042-020-09586-y
   Li J, 2021, ARXIV
   Li M., 2021, VISUAL COMPUT, P1
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liu S, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14133232
   Liu XB, 2022, J KING SAUD UNIV-COM, V34, P6179, DOI 10.1016/j.jksuci.2021.07.014
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma SP, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103583
   Maurya L, 2022, J KING SAUD UNIV-COM, V34, P7247, DOI 10.1016/j.jksuci.2021.07.008
   Mu Q, 2021, COMPUT VIS MEDIA, V7, P529, DOI 10.1007/s41095-021-0232-x
   Naik SK, 2003, IEEE T IMAGE PROCESS, V12, P1591, DOI 10.1109/TIP.2003.819231
   Ooi CH, 2010, IEEE T CONSUM ELECTR, V56, P2543, DOI 10.1109/TCE.2010.5681139
   Petrol AB, 2014, IMAGE PROCESS ON LIN, V4, P71, DOI 10.5201/ipol.2014.107
   Plataniotis K., 2000, DIGITAL SIGNAL PROC, P25
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Singh A. K., 2021, IEEE Trans. Instrum.Meas., V70, P1
   Singh K, 2014, PATTERN RECOGN LETT, V36, P10, DOI 10.1016/j.patrec.2013.08.024
   Tao Rui, 2022, 2022 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI), P226, DOI 10.1109/ICCEAI55464.2022.00055
   Tian H., 2020, ACTA PHOTONICA SINIC, V49, P173
   Wang FJ, 2021, AD HOC NETW, V113, DOI 10.1016/j.adhoc.2020.102398
   Wang WC, 2020, IEEE ACCESS, V8, P87884, DOI 10.1109/ACCESS.2020.2992749
   Wang WC, 2019, INFORM SCIENCES, V496, P25, DOI 10.1016/j.ins.2019.05.015
   Xu Y, 2016, IEEE ACCESS, V4, P165, DOI 10.1109/ACCESS.2015.2511558
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yu XY, 2023, VISUAL COMPUT, V39, P4165, DOI 10.1007/s00371-022-02582-3
   Yue S.S., 2022, P MECH ENG RES DAY M, P101
   Zhang L, 2016, IET IMAGE PROCESS, V10, P840, DOI 10.1049/iet-ipr.2015.0844
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XC, 2022, IEEE T INTELL TRANSP, V23, P14148, DOI 10.1109/TITS.2022.3147770
NR 52
TC 2
Z9 2
U1 5
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6755
EP 6766
DI 10.1007/s00371-022-02761-2
EA DEC 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000906115500001
DA 2024-07-18
ER

PT J
AU Guo, L
   Li, J
   Wan, P
   Liu, L
   Wang, YF
AF Guo, Ling
   Li, Jie
   Wan, Ping
   Liu, Ling
   Wang, Yifan
TI Tea-cake CBIR: a tea-cake content-based image retrieval model with rich
   and intensive feature extraction
SO VISUAL COMPUTER
LA English
DT Article
DE Tea-cake image retrieval; Convolutional neural networks; Feature
   distance measurement; Normalization flow
ID OPTIMIZATION ALGORITHM; SCALE
AB Tea-cake content-based image retrieval (CBIR) is an essential issue in tea-cake traceability. Popular CBIR methods work on medical and social networking, whereas tea-cake images have intensive and parallel representations among classes. While facing tea-cake CBIR, low inter-class and high intra-class distances increase retrieval difficulties using traditional CBIR methods. Thus, this paper proposes a tea-cake CBIR approach based on deep neural networks to retrieve tea-cakes. In the model, we establish a feature extraction model with designed dense blocks, where a cross-entropy loss function is explored to train the model and catch detailed features. Furthermore, to decrease intra-class and expand inter-class interval, a masked autoregressive discriminative normalization flow (MADNF) is presented to map the gained features in high dimensions to corresponding representations in Gaussian spaces. Particularly, a maximum likelihood function is developed to train MADNF for avoiding non-convergence. Extensive experiments on the tea-cake dataset show our method has significant performance compared with current competitors. Furthermore, experiments on the bird species dataset further demonstrate the effectiveness of our proposed approach.
C1 [Guo, Ling; Wan, Ping] Army Logist Acad, Chongqing 401000, Peoples R China.
   [Li, Jie; Wang, Yifan] Chongqing Univ Sci & Technol, Chongqing 401331, Peoples R China.
   [Liu, Ling] Chongqing Vocat Inst Engn, Chongqing 402260, Peoples R China.
C3 Chongqing University of Science & Technology; Chongqing Vocational
   Institute of Engineering
RP Wang, YF (corresponding author), Chongqing Univ Sci & Technol, Chongqing 401331, Peoples R China.
EM guoling118@163.com; jieli@cqust.edu.cn; 1035380280@qq.com;
   719246598@qq.com; yifanwang1207@163.com
RI Wang, Yifan/GRO-1003-2022
OI Wang, Yifan/0000-0002-6401-2428
CR Arulmozhi P, 2021, VISUAL COMPUT, V37, P2391, DOI 10.1007/s00371-020-01993-4
   Bhunia AK, 2019, IEEE WINT CONF APPL, P609, DOI 10.1109/WACV.2019.00070
   Cai YQ, 2021, IEEE-ACM T AUDIO SPE, V29, P733, DOI 10.1109/TASLP.2020.3039573
   Dehghani M., 2020, Int. J. Intell. Eng. Syst., V13
   Dehghani M., 2020, Int. J. Intell. Eng. Syst, V13, P286, DOI [10.22266/ijies2020.1031.26, DOI 10.22266/IJIES2020.1031.26]
   Dehghani M., 2019, International Journal of Innovative Technology and Exploring Engineering, V9, P5306, DOI [10.35940/ijitee.A4215.119119, DOI 10.35940/IJITEE.A4215.119119]
   Dehghani M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186173
   Dhiman G, 2021, KNOWL-BASED SYST, V211, DOI 10.1016/j.knosys.2020.106560
   Dhiman G, 2021, J AMB INTEL HUM COMP, V12, P8457, DOI 10.1007/s12652-020-02580-0
   Dhiman G, 2021, ENG COMPUT-GERMANY, V37, P323, DOI 10.1007/s00366-019-00826-w
   Dhiman G, 2019, ENG APPL ARTIF INTEL, V82, P148, DOI 10.1016/j.engappai.2019.03.021
   Dhiman G, 2019, KNOWL-BASED SYST, V165, P169, DOI 10.1016/j.knosys.2018.11.024
   Dhiman G, 2018, KNOWL-BASED SYST, V159, P20, DOI 10.1016/j.knosys.2018.06.001
   Dhiman G, 2017, ADV ENG SOFTW, V114, P48, DOI 10.1016/j.advengsoft.2017.05.014
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Ghose S, 2021, INT C PATT RECOG, P4766, DOI 10.1109/ICPR48806.2021.9412703
   Gupta S, 2020, PATTERN ANAL APPL, V23, P1569, DOI 10.1007/s10044-020-00879-4
   Gupta VK., 2022, Int. J. Mod. Res, V2, P1, DOI DOI 10.1109/INDISCON53343.2021.9582222
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kanaparthi SK, 2020, MULTIMED TOOLS APPL, V79, P34875, DOI 10.1007/s11042-019-08029-7
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kaur S, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103541
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Khwildi R, 2020, VISUAL COMPUT, V36, P1111, DOI 10.1007/s00371-019-01719-1
   Kumar R., 2021, Int. J. Modern Res, V1, P1, DOI DOI 10.1109/ICMLC.2007.4370325
   Lande Milind V., 2022, Proceedings of Data Analytics and Management: ICDAM 2021. Lecture Notes on Data Engineering and Communications Technologies (91), P165, DOI 10.1007/978-981-16-6285-0_14
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Palubinskas G, 2014, IEEE IMAGE PROC, P575, DOI 10.1109/ICIP.2014.7025115
   Pirlo G, 2013, IET BIOMETRICS, V2, P151, DOI 10.1049/iet-bmt.2013.0012
   Rudin W, 1987, REAL COMPLEX ANAL
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Vaishnav P.K., 2021, Int. J. Mod. Res, V1, P22, DOI DOI 10.31838/IJPR/2021.13.01.268
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang L, 2020, IEEE T CYBERNETICS, V50, P3330, DOI 10.1109/TCYB.2019.2894498
   Yu BS, 2019, IEEE I CONF COMP VIS, P6499, DOI 10.1109/ICCV.2019.00659
NR 40
TC 0
Z9 0
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5625
EP 5636
DI 10.1007/s00371-022-02685-x
EA DEC 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000901683800001
DA 2024-07-18
ER

PT J
AU Xiong, K
   Hong, K
   Li, J
   Li, WY
   Liao, WD
   Liu, QG
AF Xiong, Kuan
   Hong, Kai
   Li, Jin
   Li, Wanyun
   Liao, Weidong
   Liu, Qiegen
TI Joint intensity-gradient guided generative modeling for colorization
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic colorization; Unsupervised learning; Generative model;
   Intensity-gradient domain; Gradient constraint
ID IMAGE; COLOR
AB This paper proposes an iterative score-based generative model for solving the automatic colorization problem. Although unsupervised learning methods have shown the capability to generate plausible color, inadequate exploration of detailed information and data dimensions still limit the performance of the colorization model. Considering that the number of samples in score-based generative model has influence on estimating the target gradients and the gradient map possesses important latent information of the image, the inference process of the generative modeling is conducted in joint intensity-gradient domain for colorization. Specifically, a set of intensity-gradient formed high-dimensional tensors are trained, via the score matching, to attain the gradient of data distribution in joint intensity-gradient domain. As the score function is determined, data samples are generated by means of annealed Langevin dynamics, forming an iterative colorization procedure. Furthermore, the joint intensity-gradient constraint in data-fidelity term is proposed to limit the degree of freedom within generative model at the iterative colorization stage, thus being conducive to edge-preserving colorization effect. Experimental results conveyed the remarkable performance and diversity of our proposed method.
C1 [Xiong, Kuan; Liao, Weidong] Jiangxi Univ Finance & Econ, Sch Econ, Nanchang 330013, Peoples R China.
   [Hong, Kai; Li, Jin; Li, Wanyun; Liu, Qiegen] Nanchang Univ, Dept Elect Informat Engn, Nanchang 330031, Peoples R China.
C3 Jiangxi University of Finance & Economics; Nanchang University
RP Liu, QG (corresponding author), Nanchang Univ, Dept Elect Informat Engn, Nanchang 330031, Peoples R China.
EM liuqiegen@ncu.edu.cn
RI li, wanyun/HZH-5867-2023
FU National Natural Science Foundation of China;  [61871206];  [61601450]
FX AcknowledgementsThe authors sincerely thank the anonymous reviewers for
   their valuable comments and constructive suggestions that are very
   helpful in the improvement of this paper. This work was supported by
   National Natural Science Foundation of China (61871206, 61601450).
CR Antic J., 2019, A deep learning based project for colorizing and restoring old images (and video!)
   Anwar S., 2020, ARXIV
   Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27
   Baig MH, 2017, COMPUT VIS IMAGE UND, V164, P111, DOI 10.1016/j.cviu.2017.01.010
   Bishop C. M., 1994, NCRG94004 AST U, DOI DOI 10.1007/978-3-322-81570-58
   Block A., 2020, ARXIV
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Cao Y, 2017, LECT NOTES ARTIF INT, V10534, P151, DOI 10.1007/978-3-319-71249-9_10
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cho TS, 2012, IEEE T PATTERN ANAL, V34, P683, DOI 10.1109/TPAMI.2011.166
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Frans K., 2017, In Arxiv
   Gong YH, 2015, LECT NOTES COMPUT SC, V9009, P47, DOI 10.1007/978-3-319-16631-5_4
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guadarrama Sergio, 2017, BRIT MACHINE VISION
   Guo JY, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050483
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jayaram V., 2020, ARXIV
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Kingma D. P., 2014, arXiv
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Lei CY, 2019, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2019.00387
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li B, 2019, IEEE T IMAGE PROCESS, V28, P4606, DOI 10.1109/TIP.2019.2912291
   Limmer M, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P61, DOI [10.1109/ICMLA.2016.0019, 10.1109/ICMLA.2016.114]
   Liu QG, 2020, MAGN RESON MED, V83, P322, DOI 10.1002/mrm.27921
   Liu QG, 2019, VISUAL COMPUT, V35, P205, DOI 10.1007/s00371-017-1464-8
   Liu QG, 2019, INFORM FUSION, V46, P114, DOI 10.1016/j.inffus.2018.05.007
   Manjunatha Varun, 2018, NAACL HLT 2018, V2, P764
   Messaoud S, 2018, LECT NOTES COMPUT SC, V11210, P603, DOI 10.1007/978-3-030-01231-1_37
   MI Z, 2018, P OCEANS MTSIEEE KOB, P1
   Morimoto Y., 2009, P SIGGRAPH, P59
   Narayanan Hariharan, 2010, INT C NEUR INF PROC
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Rifai S., 2011, Advances in Neural Information Processing Systems, P2294
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sheikh HR, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, P975, DOI 10.1016/B978-012119792-6/50120-0
   Song Y, 2019, ADV NEUR IN, V32
   Sun ZB, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.5.053006
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Wang HC, 2006, IEEE IMAGE PROC, P2893, DOI 10.1109/ICIP.2006.313034
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wu JL, 2012, VISUAL COMPUT, V28, P723, DOI 10.1007/s00371-012-0683-2
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Yu F., 2015, ARXIV
   [张黎明 ZHANG Liming], 2011, [生态环境学报, Ecology and Environmental Sciences], V20, P1
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
   Zhao Jieyu, 2018, ARXIV
   Zhongyou Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9360, DOI 10.1109/CVPR42600.2020.00938
   Zhou JJ, 2020, IEEE SIGNAL PROC LET, V27, P2054, DOI 10.1109/LSP.2020.3037690
NR 61
TC 1
Z9 1
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6537
EP 6552
DI 10.1007/s00371-022-02747-0
EA DEC 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000899147500001
DA 2024-07-18
ER

PT J
AU Zhang, S
   Yu, DB
   Zhou, YQ
   Wu, Y
   Ma, YP
AF Zhang, Shan
   Yu, Dabing
   Zhou, Yaqin
   Wu, Yi
   Ma, Yunpeng
TI Enhanced visual perception for underwater images based on multistage
   generative adversarial network
SO VISUAL COMPUTER
LA English
DT Article
DE Underwater images enhancement; Image processing; Visual perception;
   Multistage; Generative adversarial network
ID LIGHT
AB Underwater images often suffer from color distortion and low contrast, which dramatically affects the target detection and measurement tasks in the underwater context. In this paper, we present a multistage generative adversarial network for better visual perception of underwater images. Extensive multi-scale context feature learning and high-precision restoration of spatial details are implemented stage by stage. Rich context features are learned based on the encoder and decoder architecture. Spatial details are restored through a pixel restoration module based on original images. Through channel attention module used between multistages, cross-stage feature utilization is realized. More notably, we introduce Gaussian noise into the generator, which enriches the details of images, and the relative discriminator, which promotes the generated image to have more realistic edges and textures. Experimental results demonstrate the superiority of our method over state-of-the-art methods in terms of both quantitative metrics and visual quality. In particular, we applied our method to natural underwater scenes. The results confirm that our method can effectively improve the efficiency of downstream tasks.
C1 [Zhang, Shan; Yu, Dabing; Zhou, Yaqin; Wu, Yi; Ma, Yunpeng] Hohai Univ, Coll Internet Things Engn, Changzhou 213022, Peoples R China.
C3 Hohai University
RP Ma, YP (corresponding author), Hohai Univ, Coll Internet Things Engn, Changzhou 213022, Peoples R China.
EM yunpengma_hhu@163.com
OI Yu, Dabing/0000-0003-1500-0276
FU National Key Research and Development Program [2018YFC0406900];
   Fundamental Research Funds for the Central Universities [B220201037];
   Jiangsu Provincial Key Research and Development Program [BE2020649,
   BE2020092]; National Natural Science Foundation of China [62001156]
FX This study was funded by National Key Research and Development Program
   (2018YFC0406900), the Fundamental Research Funds for the Central
   Universities (B220201037), Jiangsu Provincial Key Research and
   Development Program (BE2020649, BE2020092), and National Natural Science
   Foundation of China (62001156).
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2017, IEEE COMPUT SOC CONF, P997, DOI 10.1109/CVPRW.2017.136
   [Anonymous], 2015, An Improved Histogram Equalization Technique for Image Contrast Enhancement
   Bhatia N, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES FOR SMART NATION (SMARTTECHCON), P815, DOI 10.1109/SmartTechCon.2017.8358486
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chowdhury D, 2019, 2019 INTERNATIONAL CONFERENCE ON OPTO-ELECTRONICS AND APPLIED OPTICS (OPTRONIX 2019), DOI 10.1109/optronix.2019.8862330
   Deng XY, 2021, J OPT SOC AM A, V38, P181, DOI 10.1364/JOSAA.400199
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fan TH, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P410, DOI 10.1109/ICIVC.2017.7984588
   Fu Z., 2021, ARXIV
   Ghani ASA, 2015, APPL SOFT COMPUT, V27, P219, DOI 10.1016/j.asoc.2014.11.020
   Gunawan AAS, 2016, INT C ADV COMP SCI I, P470, DOI 10.1109/ICACSIS.2016.7872757
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Hanmante B. P., 2018, 2018 4 INT C COMPUTI, P1
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jia YX, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1797, DOI 10.1109/CompComm.2017.8322849
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Khan A, 2016, 2016 IEEE 6TH INTERNATIONAL CONFERENCE ON UNDERWATER SYSTEM TECHNOLOGY: THEORY AND APPLICATIONS, P83, DOI 10.1109/USYS.2016.7893927
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Li CY, 2020, IEEE IMAGE PROC, P1083, DOI [10.1109/icip40778.2020.9191157, 10.1109/ICIP40778.2020.9191157]
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Liu XD, 2020, IEEE GEOSCI REMOTE S, V17, P1488, DOI 10.1109/LGRS.2019.2950056
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Panetta K, 2022, IEEE J OCEANIC ENG, V47, P59, DOI 10.1109/JOE.2021.3086907
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Parihar AS, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P619, DOI 10.1109/ICISC.2018.8398874
   Qiao N., 2022, VISUAL COMPUT, P1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao GF, 2020, CHIN AUTOM CONGR, P6294, DOI 10.1109/CAC51589.2020.9327565
   Wang X., 2018, EUROPEAN C COMPUTER
   Wang Y, 2018, IEEE T CIRCUITS-I, V65, P992, DOI 10.1109/TCSI.2017.2751671
   Wu M, 2017, OCEANS-IEEE
   Wu Y, 2021, APPL OPTICS, V60, P7754, DOI 10.1364/AO.428502
   Yang H.-H., 2021, IET IMAGE PROCESS
   Zamir Syed Waqas, 2021, 2021 IEEE CVF C COMP
   Zhao Y., 2020, ECCV, P800
NR 43
TC 4
Z9 4
U1 3
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5375
EP 5387
DI 10.1007/s00371-022-02665-1
EA SEP 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000857806300001
DA 2024-07-18
ER

PT J
AU Cheng, PP
   Wang, JP
   Zeng, XY
   Bruniaux, P
   Tao, XY
   Chen, DL
AF Cheng, Pengpeng
   Wang, Jianping
   Zeng, Xianyi
   Bruniaux, Pascal
   Tao, Xuyuan
   Chen, Daoling
TI Research on winter sportswear comfort and its visual model
SO VISUAL COMPUTER
LA English
DT Article
DE Tight sportswear; Sport comfort; Intelligent prediction model;
   Visualization
ID THERMAL COMFORT; LOWER-BODY; GARMENT; FABRICS; SETS; FIT
AB In order to study the comfort of tight-fitting sportswear in winter, this paper designed a series of motions, and explored the distribution of comfort perception under different sports conditions by evaluating the wearing perception of human body. Finally, through the acquired experimental data, intelligent prediction models were established, and the prediction results were visualized, which makes the comfort distribution more intuitive. The results show that there are great differences in the parts that affect the overall comfort perception under different sports conditions; different ages subjects have different perceptions of comfort; Particle Swarm Optimization-Cuckoo Search-Adaptive Network-based Fuzzy Inference System has better prediction accuracy, and could replace the wearing trials.
C1 [Cheng, Pengpeng; Wang, Jianping] Donghua Univ, Coll Fash & Design, Shanghai, Peoples R China.
   [Cheng, Pengpeng; Zeng, Xianyi; Bruniaux, Pascal; Tao, Xuyuan] Cent Lille, Ensait, Gemtex, F-59000 Roubaix, France.
   [Wang, Jianping] Donghua Univ, Key Lab Clothing Design & Technol, Minist Educ, Shanghai 200051, Peoples R China.
   [Wang, Jianping] Shanghai Belt & Rd Joint Lab Text Intelligent Mfg, Shanghai 200051, Peoples R China.
   [Chen, Daoling] Minjiang Univ, Clothing & Design Fac, Fuzhou, Peoples R China.
C3 Donghua University; Universite de Lille; Ecole Nationale Superieure des
   Arts et Industries Textiles (ENSAIT); Donghua University; Minjiang
   University
RP Cheng, PP (corresponding author), Donghua Univ, Coll Fash & Design, Shanghai, Peoples R China.; Cheng, PP (corresponding author), Cent Lille, Ensait, Gemtex, F-59000 Roubaix, France.
EM cppcd13344@163.com
RI Zeng, Xianyi/AAQ-1183-2021
OI Zeng, Xianyi/0000-0002-3236-6766
FU China Scholarship Council; Fujian Province Social Science Planning
   Project [FJ2020C049]; national key research and development plan
   "science and technology in Winter Olympic Games" [2019YFF0302100];
   International Cooperation Fund of Science and Technology Commission of
   Shanghai Municipality [21130750100]
FX This paper was financially supported by China Scholarship Council and
   Fujian Province Social Science Planning Project (FJ2020C049), national
   key research and development plan "science and technology in Winter
   Olympic Games" (2019YFF0302100) and International Cooperation Fund of
   Science and Technology Commission of Shanghai Municipality
   (21130750100).
CR Al-Adwan AS, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13169453
   Atalie D, 2021, J NAT FIBERS, V18, P1699, DOI 10.1080/15440478.2019.1697989
   Atasagun HG, 2019, TEXT RES J, V89, P4425, DOI 10.1177/0040517519834609
   Barker R, 2014, AATCC J RES, V1, P13, DOI 10.14504/ajr.1.1.3
   Basra SA, 2020, J ENG FIBER FABR, V15, DOI 10.1177/1558925020963009
   Çeven EK, 2021, FIBER POLYM, V22, P567, DOI 10.1007/s12221-021-0246-0
   Chen JF, 2015, ALGORITHMS, V8, P292, DOI 10.3390/a8020292
   Cheng PP, 2021, THERM SCI, V25, P2589, DOI 10.2298/TSCI190310229C
   Cheng PP, 2020, SOFT COMPUT, V24, P13219, DOI 10.1007/s00500-020-04735-9
   Choi J, 2015, APPL ERGON, V48, P186, DOI 10.1016/j.apergo.2014.11.016
   COTTER JD, 1995, EUR J APPL PHYSIOL O, V71, P549, DOI 10.1007/BF00238559
   Daukantiene V, 2018, INT J CLOTH SCI TECH, V30, P839, DOI 10.1108/IJCST-02-2018-0021
   de Britto MA, 2017, J STRENGTH COND RES, V31, P2480, DOI 10.1519/JSC.0000000000001620
   Ding WP, 2020, KNOWL-BASED SYST, V198, DOI 10.1016/j.knosys.2020.105945
   Doan BK, 2003, J SPORT SCI, V21, P601, DOI 10.1080/0264041031000101971
   El Gmili N, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/8925165
   Ertekin G, 2022, J NAT FIBERS, V19, P810, DOI 10.1080/15440478.2021.1944428
   Eryuruk SH, 2019, INT J CLOTH SCI TECH, V31, P243, DOI 10.1108/IJCST-01-2018-0009
   Ghodrati A, 2012, LECT NOTES ARTIF INT, V7198, P89, DOI 10.1007/978-3-642-28493-9_11
   Gu C., 2010, J ANHUI POLYTECH U, V25, P15
   Gu M., 2008, SHANDONG TEXTILE SCI, V4, P51
   HERTZMAN AB, 1957, J APPL PHYSIOL, V10, P242, DOI 10.1152/jappl.1957.10.2.242
   Jalal M, 2013, NEURAL COMPUT APPL, V23, P455, DOI 10.1007/s00521-012-0941-2
   Kaplan S, 2022, FIBER POLYM, V23, P537, DOI 10.1007/s12221-021-0045-7
   Kara S, 2020, IND TEXTILA, V71, P105
   Karasawa Y, 2021, INT J AFFECT ENG, V20, P21, DOI 10.5057/ijae.TJSKE-D-20-00025
   Kuang C., 2014, BIOTECHNOL INDIAN J, V10, P10431
   Kurek K A., 2022, Quality Quantity, V56, P1371, DOI DOI 10.1007/S11135-021-01181-Z
   LI Y, 1991, J TEXT I, V82, P277, DOI 10.1080/00405009108659210
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Machado-Moreira CA, 2008, EUR J APPL PHYSIOL, V104, P265, DOI 10.1007/s00421-007-0646-x
   Maji P, 2013, APPL SOFT COMPUT, V13, P3968, DOI 10.1016/j.asoc.2012.09.006
   Mert E, 2016, INT J BIOMETEOROL, V60, P1995, DOI 10.1007/s00484-016-1258-0
   Moshagen M, 2018, PSYCHOL METHODS, V23, P318, DOI 10.1037/met0000122
   Mousavi G, 2022, J IND TEXT, V51, p3593S, DOI 10.1177/1528083720988089
   Narges S, 2021, J ENVIRON HEALTH SCI, V19, P1543, DOI 10.1007/s40201-021-00710-0
   Ning, 2017, STUDY HUMAN THERMAL
   Pandit Amiya, 2021, Arabian Journal of Geosciences, V14, DOI 10.1007/s12517-021-07594-2
   Park S, 2008, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION 2007, VOL 5, P1, DOI 10.1145/13606121360695
   Saenroth D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0142131
   Shahri MM, 2021, J INTELL FUZZY SYST, V41, P199, DOI 10.3233/JIFS-201407
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Shen Q, 2004, PATTERN RECOGN, V37, P1351, DOI 10.1016/j.patcog.2003.10.016
   Singh S, 2021, WIREL NETW, V27, P4151, DOI 10.1007/s11276-021-02679-y
   Smith CJ, 2011, EUR J APPL PHYSIOL, V111, P1391, DOI 10.1007/s00421-010-1744-8
   Terliksiz S, 2016, INT J CLOTH SCI TECH, V28, P105, DOI 10.1108/IJCST-02-2015-0028
   Uren N, 2019, TEXT RES J, V89, P4842, DOI 10.1177/0040517519840634
   Wang L, 2021, SOFT COMPUT, V25, P973, DOI 10.1007/s00500-020-05193-z
   Xi C., 2016, YOUTH SPORT, V7, P45
   Xu G., 2021, J XIAN U SCI TECHNOL, V41, P55
   Yang Y, 2020, TEXT RES J, V90, P2385, DOI 10.1177/0040517520920950
   Zamporri J, 2018, ORTHOP J SPORTS MED, V6, DOI 10.1177/2325967118789955
   [张又升 Zhang Yousheng], 2016, [东华大学学报. 自然科学版, Journal of Donghua University. Natural Science Edition], V42, P268
   Zhou SM, 2006, FUZZY SET SYST, V157, P1057, DOI 10.1016/j.fss.2005.08.004
NR 54
TC 1
Z9 1
U1 13
U2 51
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4371
EP 4389
DI 10.1007/s00371-022-02596-x
EA JUL 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000830367500002
DA 2024-07-18
ER

PT J
AU Yu, XY
   Li, HX
   Yang, HD
AF Yu, Xinyi
   Li, Hanxiong
   Yang, Haidong
TI Two-stage image decomposition and color regulator for low-light image
   enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light image enhancement; A two-stage decomposition network; Flexible
   joint loss function; Color regulator
ID ADAPTIVE HISTOGRAM EQUALIZATION; RETINEX; VISIBILITY; ERROR
AB Low-lighting is a common condition in data collection due to environmental restrictions. However, high-level pattern recognition tasks such as object detection require the datasets to be more clear. Thus, low-light image enhancement is necessary. Noise and color distortion are two major problems of the existing enhancement algorithms. This paper has proposed a low-light image enhancement algorithm that integrates denoising and color restoration. First, we propose a two-stage hybrid decomposition network, which can perform modified Retinex-decomposition on paired images, and then extract principal components of the decomposed low-light images to handle the nonlinear residuals, thereby obtaining reliable reflectance and illumination maps. Then, in order not to over-smooth the details and edges of the image, we use a flexible joint function to train the hybrid network. Finally, we create a color regulator in the HSI (Hue-Saturation-Intensity) space to correct the distortion in RGB space caused by coupling between pixels. Experimental results on public datasets show that the proposed method greatly enhanced the quality of low-light images.
C1 [Yu, Xinyi] Cent South Univ, State Key Lab High Performance Complex Mfg, Changsha 410083, Hunan, Peoples R China.
   [Yu, Xinyi] Cent South Univ, Coll Mech & Elect Engn, Changsha 410083, Hunan, Peoples R China.
   [Li, Hanxiong] City Univ Hong Kong, Dept Adv Design & Syst Engn, Kowloon, Hong Kong 999077, Peoples R China.
   [Yang, Haidong] Guangdong Univ Technol, Guangdong Engn Res Ctr Green Mfg & Energy Efficie, Guangzhou 510006, Guangdong, Peoples R China.
C3 Central South University; Central South University; City University of
   Hong Kong; Guangdong University of Technology
RP Li, HX (corresponding author), City Univ Hong Kong, Dept Adv Design & Syst Engn, Kowloon, Hong Kong 999077, Peoples R China.
EM yuxinyi@csu.edu.cn; mehxli@cityu.edu.hk; yanghd@yeah.net
RI Li, Han-Xiong/F-5276-2012
FU Guangdong-Hong Kong joint Project [2020A0505090005]
FX The work presented in this paper is partially supported by the
   Guangdong-Hong Kong joint Project under Grant 2020A0505090005.
CR ALLEN DM, 1971, TECHNOMETRICS, V13, P469, DOI 10.2307/1267161
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen BH, 2018, IEEE T NEUR NET LEAR, V29, P3828, DOI 10.1109/TNNLS.2017.2741975
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo S, 2023, VISUAL COMPUT, V39, P1363, DOI 10.1007/s00371-022-02412-6
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jiang ZQ, 2021, NEUROCOMPUTING, V454, P361, DOI 10.1016/j.neucom.2021.05.025
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Lee C, 2012, IEEE T IMAGE PROCESS, V21, P80, DOI 10.1109/TIP.2011.2159387
   Li M., 2021, VISUAL COMPUT, V1
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Sharma V, 2018, PROC CVPR IEEE, P4033, DOI 10.1109/CVPR.2018.00424
   Song XJ, 2023, VISUAL COMPUT, V39, P489, DOI 10.1007/s00371-021-02343-8
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.106949
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   YU N., VISUAL COMPUT, V1-20
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zheng M., 2022, J ARTIF INTELL TECHN
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 41
TC 6
Z9 5
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4165
EP 4175
DI 10.1007/s00371-022-02582-3
EA JUL 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000828925700002
DA 2024-07-18
ER

PT J
AU Netto, GM
   Oliveira, MM
AF Netto, Gustavo Marques
   Oliveira, Manuel M.
TI Robust point-cloud registration based on dense point matching and
   probabilistic modeling
SO VISUAL COMPUTER
LA English
DT Article
DE Robust point-cloud registration; Rigid and non-rigid registration
ID SET REGISTRATION; SURFACES
AB We present 3D point-cloud registration techniques suited for scenarios where robustness to outliers and missing regions is necessary, besides being applicable to both rigid and non-rigid configurations. Our techniques exploit advantages from deep learning models for dense point matching and from recent advances in probabilistic modeling of point-cloud registration. Such a combination produces context awareness and resilience to outliers and missing information. We demonstrate their effectiveness by comparing them to state-of-the-art methods and showing that ours achieve superior results on existing and proposed datasets.
C1 [Netto, Gustavo Marques; Oliveira, Manuel M.] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Oliveira, MM (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
EM gmnetto@inf.ufrgs.br; oliveira@inf.ufrgs.br
RI Menezes de Oliveira Neto, Manuel/H-1508-2011
OI Menezes de Oliveira Neto, Manuel/0000-0003-4957-9984
FU CNPq-Brazil [312975/2018-0, 423673/2016-5]; Coordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior -Brasil (CAPES) [001]
FX This work was funded by CNPq-Brazil (fellowships and grants
   312975/2018-0 and 423673/2016-5), and financed in part by the
   Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior -Brasil
   (CAPES) -Finance Code 001.
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Bresson G, 2017, IEEE T INTELL VEHICL, V2, P194, DOI 10.1109/TIV.2017.2749181
   Bronstein AM, 2007, IEEE T VIS COMPUT GR, V13, P902, DOI 10.1109/TVCG.2007.1041
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Cai H., 2021, RECURRENT MULTIVIEW
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Eisenberger M, 2021, PROC CVPR IEEE, P7469, DOI 10.1109/CVPR46437.2021.00739
   Fitzgibbon AW, 2003, IMAGE VISION COMPUT, V21, P1145, DOI 10.1016/j.imavis.2003.09.004
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Gao W, 2019, PROC CVPR IEEE, P11087, DOI 10.1109/CVPR.2019.01135
   Ge S, 2015, IEEE COMPUT SOC CONF
   Ge S, 2014, IEEE COMPUT SOC CONF, P245, DOI 10.1109/CVPRW.2014.45
   Granger S, 2002, LECT NOTES COMPUT SC, V2353, P418
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2858, DOI 10.1109/TPAMI.2020.3043769
   Hirose O, 2021, IEEE T PATTERN ANAL, V43, P2269, DOI 10.1109/TPAMI.2020.2971687
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Kittenplon Y, 2021, PROC CVPR IEEE, P4112, DOI 10.1109/CVPR46437.2021.00410
   Liu JJ, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107378
   Liu XY, 2019, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2019.00062
   Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Naseer M, 2019, IEEE ACCESS, V7, P1859, DOI 10.1109/ACCESS.2018.2886133
   Ouyang BJ, 2021, IEEE COMPUT SOC CONF, P2799, DOI 10.1109/CVPRW53098.2021.00315
   Paszke A, 2019, ADV NEUR IN, V32
   Pomerleau Francois, 2015, Found. Trends Robot., V4, P1
   Puy Gilles, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P527, DOI 10.1007/978-3-030-58604-1_32
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tishchenko I, 2020, INT CONF 3D VISION, P150, DOI 10.1109/3DV50981.2020.00025
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Cao VT, 2014, 2014 PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP 2014), P43
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang L., 2019, ARXIVPREPRINT ARXIV1
   Wang L., 2019, ARXIV190401428
   Wang RZ, 2019, IEEE I CONF COMP VIS, P3056, DOI 10.1109/ICCV.2019.00315
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang ZJ, 2020, IEEE ACCESS, V8, P2847, DOI 10.1109/ACCESS.2019.2962554
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Wu Wenxuan, 2020, EUROPEAN C COMPUTER, P88
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yao Y, 2020, P IEEE CVF C COMP VI, P7600
   Zeng Y., CVPR
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 56
TC 4
Z9 4
U1 8
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3217
EP 3230
DI 10.1007/s00371-022-02525-y
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000810352600002
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Merras, M
   El Hazzat, S
   Bouazi, A
   Chana, I
   El Akkad, N
   Satori, K
AF Merras, Mostafa
   El Hazzat, Soulaiman
   Bouazi, Aziz
   Chana, Idriss
   El Akkad, Nabil
   Satori, Khalid
TI Camera self-calibration with varying parameters based on planes basis
   using particle swarm optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Camera self-calibration; Homography; Planes basis; Particle swarm
   optimization; Nonlinear optimization
ID 3D RECONSTRUCTION; OBJECT RECOGNITION; STEREO VISION; MOTION; SEQUENCES
AB This work proposes an approach of cameras self-calibration with varying intrinsic parameters from two images of any 3D scene. The present method is based on the projection of several unknown planes of the scene by their homographies in the two images taken with different viewpoints. Each plane is characterized by a Euclidean reference which is defined on two points of the scene whose their projections are well localized, then a global reference of the scene is chosen from the considered references. Then, the self-calibration equations are formulated by the expression of each homography which is defined by the intrinsic cameras parameters and the coordinates of the two points on which the Euclidean references of the planes are chosen. These equations are introduced into a nonlinear cost function, the optimization of this cost function allows to estimate the intrinsic cameras parameters. The strong points of the present approach are: a minimum images number (two images are sufficient), only five matching points and a new formulation of the cost function which does not require too much computing to converge towards an optimal solution of the cameras parameters. Extensive experiments on synthetic and real data are presented to demonstrate the performance of the proposed technique in terms of simplicity, precision, stability and convergence.
C1 [Merras, Mostafa; Bouazi, Aziz; Chana, Idriss] Moulay Ismail Univ Meknes, High Sch Technol, IMAGE Lab, BP 3103, Toulal, Morocco.
   [El Hazzat, Soulaiman] Sidi Mohamed Ben Abdellah Univ, Polydisciplinary Fac Taza, Dept Math Phys & Comp Sci, LSI, BP 1223, Taza, Morocco.
   [El Akkad, Nabil] Sidi Mohamed Ben Abellah Univ, Lab Engn Syst & Applicat LISA, ENSA, Fes, Morocco.
   [Satori, Khalid] Sidi Mohamed Ben Abdellah Univ, Fac Sci, Dhar Mahraz FSDM, LISAC, Fes, Morocco.
C3 Moulay Ismail University of Meknes; Sidi Mohamed Ben Abdellah University
   of Fez; Sidi Mohamed Ben Abdellah University of Fez
RP Merras, M (corresponding author), Moulay Ismail Univ Meknes, High Sch Technol, IMAGE Lab, BP 3103, Toulal, Morocco.
EM merras.mostafa@gmail.com; nabil.elakkad@usmba.ac.ma
RI satori, khalid/GSE-3077-2022; Chana, Idriss/HHZ-9718-2022; Merras,
   Mostafa/AAJ-4405-2020; AKKAD, Nabil EL/AAL-4049-2020
OI Chana, Idriss/0000-0001-6989-4709; Merras, Mostafa/0000-0002-3020-726X;
   AKKAD, Nabil EL/0000-0003-0277-8003; SATORI, khalid/0000-0001-6055-4169;
   El Hazzat, Soulaiman/0000-0002-6647-1767
CR AhmadFarizHasan AliAbuassal., 2013, INT J SCI ENG RES, V4, P56
   Alemán-Flores M, 2014, PATTERN RECOGN, V47, P89, DOI 10.1016/j.patcog.2013.05.011
   Baataoui A., 2014, INT J SOFTWARE ENG I, V8, P23
   Cao XC, 2006, COMPUT VIS IMAGE UND, V102, P227, DOI 10.1016/j.cviu.2006.01.004
   El Akkad N, 2018, MULTIMED TOOLS APPL, V77, P14055, DOI 10.1007/s11042-017-5012-3
   El Akkad N, 2016, 3D RES, V7, DOI 10.1007/s13319-016-0082-y
   El Akkad N, 2014, VISUAL COMPUT, V30, P519, DOI 10.1007/s00371-013-0877-2
   El Hazzat S, 2019, MULTIMED TOOLS APPL, V78, P14251, DOI 10.1007/s11042-018-6828-1
   El Hazzat S, 2018, VISUAL COMPUT, V34, P1443, DOI 10.1007/s00371-017-1451-0
   Furukawa Y, 2009, INT J COMPUT VISION, V84, P257, DOI 10.1007/s11263-009-0232-2
   Gunen MA, 2020, NEURAL COMPUT APPL, V32, P17681, DOI 10.1007/s00521-020-04944-1
   Ha JE, 2005, PATTERN RECOGN, V38, P143, DOI 10.1016/j.patcog.2004.05.005
   Habed A, 2008, PATTERN RECOGN, V41, P2484, DOI 10.1016/j.patcog.2007.12.010
   Halloran B, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107058
   Häne C, 2017, IMAGE VISION COMPUT, V68, P14, DOI 10.1016/j.imavis.2017.07.003
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Jiang ZT, 2012, J COMPUT, V7, P774, DOI 10.4304/jcp.7.3.774-778
   Kang L, 2017, PATTERN RECOGN, V69, P251, DOI 10.1016/j.patcog.2017.04.006
   Kanheer NK., 2008, VISION BASED DETECTI
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Li Y, 2008, IMAGE VISION COMPUT, V26, P731, DOI 10.1016/j.imavis.2007.12.007
   Li ZX, 2022, VISUAL COMPUT, V38, P1589, DOI 10.1007/s00371-021-02090-w
   Liu D, 2016, PHOTOGRAMM ENG REM S, V82, P325, DOI 10.14358/PERS.82.5.325
   Liu SG, 2019, J COMPUT SCI-NETH, V31, P45, DOI 10.1016/j.jocs.2018.12.002
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Manolis I.A., 2000, 3911 INRIA
   Merras M, 2018, SOFT COMPUT, V22, P6271, DOI 10.1007/s00500-017-2966-z
   Merras M, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0039-6
   Mostafa Merras, 2013, Journal of Theoretical and Applied Information Technology, V51, P363
   Peng E, 2010, PATTERN RECOGN, V43, P1188, DOI 10.1016/j.patcog.2009.08.003
   Pereira M, 2016, ROBOT AUTON SYST, V83, P326, DOI 10.1016/j.robot.2016.05.010
   Pollefeys M, 1999, INT J COMPUT VISION, V32, P7, DOI 10.1023/A:1008109111715
   Priya L, 2018, CLUSTER COMPUT, V21, P29, DOI 10.1007/s10586-017-0891-7
   Qu HZ, 2021, VISUAL COMPUT, V37, P2331, DOI 10.1007/s00371-020-01989-0
   Ramalingam S, 2010, COMPUT VIS IMAGE UND, V114, P210, DOI 10.1016/j.cviu.2009.07.007
   Saravanan C., 2013, INT J ENG ADV TECHNO, V2, P930
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   Sochor J, 2017, COMPUT VIS IMAGE UND, V161, P87, DOI 10.1016/j.cviu.2017.05.015
   Sturm P, 2002, IMAGE VISION COMPUT, V20, P415, DOI 10.1016/S0262-8856(02)00012-4
   Sturm P, 2000, IEEE T PATTERN ANAL, V22, P1199, DOI 10.1109/34.879804
   Tang Z, 2019, IEEE ACCESS, V7, P10754, DOI 10.1109/ACCESS.2019.2891224
   Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552
   Urban S, 2017, INT J COMPUT VISION, V121, P234, DOI 10.1007/s11263-016-0935-0
   Van den Bergh F., 2001, THESIS U PRETORIA PR
   van Dijck H, 2003, PATTERN RECOGN LETT, V24, P137, DOI 10.1016/S0167-8655(02)00206-4
   Wang DS, 2018, SOFT COMPUT, V22, P387, DOI 10.1007/s00500-016-2474-6
   Xu LJ, 2018, MEAS SCI TECHNOL, V29, DOI 10.1088/1361-6501/aac747
   Yanliang Shang, 2012, Information Technology Journal, V11, P376, DOI 10.3923/itj.2012.376.379
   Yue Zhao, 2012, Information Technology Journal, V11, P926, DOI 10.3923/itj.2012.926.930
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zhang XC, 2017, ROBOT AUTON SYST, V93, P43, DOI 10.1016/j.robot.2017.04.001
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao Y., 2012, Information Technology Journal, V11, P276, DOI 10.3923/itj.2012.276.282
NR 53
TC 2
Z9 2
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3109
EP 3122
DI 10.1007/s00371-022-02516-z
EA MAY 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000805468000002
DA 2024-07-18
ER

PT J
AU Mühlenbrock, A
   Fischer, R
   Schröder-Dering, C
   Weller, R
   Zachmann, G
AF Muehlenbrock, Andre
   Fischer, Roland
   Schroeder-Dering, Christoph
   Weller, Rene
   Zachmann, Gabriel
TI Fast, accurate and robust registration of multiple depth sensors without
   need for RGB and IR images
SO VISUAL COMPUTER
LA English
DT Article
DE Point clouds; Registration; Extrinsic calibration; Depth sensors
ID D CAMERAS; CALIBRATION; ICP
AB Registration is an essential prerequisite for many applications when a multiple-camera setup is used. Due to the noise in depth images, registration procedures for depth sensors frequently rely on the detection of a target object in color or infrared images. However, this prohibits use cases where color and infrared images are not available or where there is no mapping between the pixels of different image types, e.g., due to separate sensors or different projections. We present our novel registration method that requires only the point cloud resulting from the depth image of each camera. For feature detection, we propose a combination of a custom-designed 3D registration target and an algorithm that is able to reliably detect that target and its features in noisy point clouds. Our evaluation indicates that our lattice detection is very robust (with a precision of more than 0.99) and very fast (on average about 20 ms with a single core). We have also compared our registration method with known methods: Our registration method achieves an accuracy of 1.6 mm at a distance of 2 m using only the noisy depth image, while the most accurate registration method achieves an accuracy of 0.7 mm requiring both the infrared and depth image.
C1 [Muehlenbrock, Andre; Fischer, Roland; Schroeder-Dering, Christoph; Weller, Rene; Zachmann, Gabriel] Univ Bremen, Comp Graph & Virtual Real, Bremen, Germany.
C3 University of Bremen
RP Mühlenbrock, A (corresponding author), Univ Bremen, Comp Graph & Virtual Real, Bremen, Germany.
EM muehlenb@cs.uni-bremen.de; rfischer@cs.uni-bremen.de;
   schroeder.c@cs.uni-bremen.de; weller@cs.uni-bremen.de;
   zach@cs.uni-bremen.de
RI Zachmann, Gabriel/AAI-9685-2020
OI Zachmann, Gabriel/0000-0001-8155-1127; Muhlenbrock,
   Andre/0000-0002-7836-3341; Schroder-Dering,
   Christoph/0000-0003-0722-9222
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   Avetisyan R., 2014, CALIBRATION DEPTH CA, V06
   Beck Stephan, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P89, DOI 10.1109/3DUI.2015.7131731
   Beck S., 2017, 2017 IEEE VIRTUAL RE
   Bradski G., 2000, Opencv. Dr. Dobb's journal of software tools
   Chen C, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020328
   Darwish W, 2019, IEEE ACCESS, V7, P8824, DOI 10.1109/ACCESS.2018.2890713
   Deng T, 2017, IMAGE VISION COMPUT, V62, P1, DOI 10.1016/j.imavis.2017.03.005
   Deng T, 2014, IEEE INT CON MULTI
   Duda A., 2018, 29 BRIT MACHINE VISI, P126
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Furrer F., 2017, EVALUATION COMBINED
   Han JD, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16020228
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Li P, 2020, IEEE ACCESS, V8, P68030, DOI 10.1109/ACCESS.2020.2986470
   Liu HY, 2020, OPT EXPRESS, V28, P19058, DOI 10.1364/OE.392414
   Macknojia R., 2013, CALIBRATION NETWORK
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Morell-Gimenez V., 2014, SENSORS-BASEL, V14
   Mühlenbrock A, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P41, DOI 10.1109/CW52790.2021.00014
   Papachristou A., 2018, MARKERLESS STRUCTURE
   Reyes-Aviles F., 2020, J WSCG, V28, P105, DOI [10.24132/JWSCG.2020.28.13, DOI 10.24132/JWSCG.2020.28.13]
   Rufli M., 2008, IEEERSJ INT C INTELL
   Song XB, 2018, MULTIMED TOOLS APPL, V77, P14951, DOI 10.1007/s11042-017-5081-3
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Staranowicz AN, 2015, COMPUT VIS IMAGE UND, V137, P102, DOI 10.1016/j.cviu.2015.03.013
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Su PC, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010235
NR 30
TC 1
Z9 1
U1 5
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 3995
EP 4008
DI 10.1007/s00371-022-02505-2
EA MAY 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000796846900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Ding, Y
   Duan, ZK
   Li, SR
AF Ding, Yi
   Duan, Zhikui
   Li, Shiren
TI 2D arcsine and sine combined logistic map for image encryption
SO VISUAL COMPUTER
LA English
DT Article
DE 2D arcsine and sine combined logistic mapping; A chaotic tree transform;
   Image encryption algorithm; Chaotic mapping
ID KOLMOGOROV-ENTROPY; ALGORITHM; SCHEME; CHAOS; PERMUTATION; KEYS
AB Chaotic system is widely applied in field of information security because of its excellent properties of unpredictability, no periodicity, pseudo-randomness, and high sensitivity to initial parameters. In this paper, we propose a new chaotic map, named 2D arcsine and sine combined logistic map (2D-ASLM), which is a combination of arcsine, Sine, and Logistic maps. Compared with the existing chaotic maps, the proposed one has the better pseudo-randomness and ergodicity from the viewpoint of chaotic performance. Besides, it also has a more unpredictable chaotic rang as well as a relatively low implementation cost. A chaotic tree transform (CTT) is introduced to effectively disrupt the image pixel positions. Then, the image can be encrypted by combining 2D-ASLM and CTT. Extensive results on encryption simulation and security analysis have shown the effectiveness of the proposed algorithm from different aspects.
C1 [Ding, Yi] Hunan Univ Arts & Sci, Changde, Peoples R China.
   [Duan, Zhikui] Foshan Univ, Foshan, Peoples R China.
   [Li, Shiren] Sun Yat Sen Univ, Guangzhou, Peoples R China.
C3 Hunan University of Arts & Science; Foshan University; Sun Yat Sen
   University
RP Ding, Y (corresponding author), Hunan Univ Arts & Sci, Changde, Peoples R China.
EM mrtbs99@163.com
FU Foshan University [2019A1515110127]
FX This work was supported by Foshan University in part of the funding
   called Analyzing and hardening single event effects in LDO
   (2019A1515110127).
CR Abanda Y, 2016, IET IMAGE PROCESS, V10, P742, DOI 10.1049/iet-ipr.2015.0244
   Ai XX, 2014, ACTA PHYS SIN-CH ED, V63, DOI 10.7498/aps.63.120511
   [Anonymous], 1994, Chaos and Nonlinear Dynamics: An Introduction for Scientists and Engineers: An Introduction for Scientists and Engineers
   Bassham L. E., 2010, SPECIAL PUBLICATION
   Behnia S, 2008, CHAOS SOLITON FRACT, V35, P408, DOI 10.1016/j.chaos.2006.05.011
   Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Çavusoglu Ü, 2017, CHAOS SOLITON FRACT, V95, P92, DOI 10.1016/j.chaos.2016.12.018
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen JX, 2015, OPT LASER ENG, V67, P191, DOI 10.1016/j.optlaseng.2014.11.017
   Enayatifar R, 2015, OPT LASER ENG, V71, P33, DOI 10.1016/j.optlaseng.2015.03.007
   Faure P, 1998, PHYSICA D, V122, P265, DOI 10.1016/S0167-2789(98)00177-8
   Fu C, 2012, OPT EXPRESS, V20, P2363, DOI 10.1364/OE.20.002363
   Fu C, 2011, OPT COMMUN, V284, P5415, DOI 10.1016/j.optcom.2011.08.013
   GALLAS JAC, 1993, PHYS REV LETT, V70, P2714, DOI 10.1103/PhysRevLett.70.2714
   GRASSBERGER P, 1983, PHYS REV A, V28, P2591, DOI 10.1103/PhysRevA.28.2591
   Haroun MF, 2015, NONLINEAR DYNAM, V82, P1523, DOI 10.1007/s11071-015-2258-z
   Hsiao HI, 2015, SIGNAL PROCESS, V117, P281, DOI 10.1016/j.sigpro.2015.06.007
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Kanso A, 2012, COMMUN NONLINEAR SCI, V17, P2943, DOI 10.1016/j.cnsns.2011.11.030
   Liao XF, 2010, SIGNAL PROCESS, V90, P2714, DOI 10.1016/j.sigpro.2010.03.022
   Liu HJ, 2017, IET IMAGE PROCESS, V11, P324, DOI 10.1049/iet-ipr.2016.0040
   Liu HJ, 2011, OPT COMMUN, V284, P3895, DOI 10.1016/j.optcom.2011.04.001
   Liu HJ, 2010, COMPUT MATH APPL, V59, P3320, DOI 10.1016/j.camwa.2010.03.017
   Liu LL, 2012, COMPUT ELECTR ENG, V38, P1240, DOI 10.1016/j.compeleceng.2012.02.007
   Liu LF, 2016, IET SIGNAL PROCESS, V10, P1096, DOI 10.1049/iet-spr.2015.0522
   Liu WH, 2016, OPT LASER ENG, V84, P26, DOI 10.1016/j.optlaseng.2016.03.019
   MAY RM, 1976, NATURE, V261, P459, DOI 10.1038/261459a0
   Miller, DOUBLE PRECISION FLO
   ROSSLER OE, 1979, PHYS LETT A, V71, P155, DOI 10.1016/0375-9601(79)90150-6
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Shen CW, 2014, IEEE T CIRCUITS-I, V61, P2380, DOI 10.1109/TCSI.2014.2304655
   Shevchenko II, 2014, PHYS LETT A, V378, P34, DOI 10.1016/j.physleta.2013.10.035
   Solak E, 2011, INFORM SCIENCES, V181, P227, DOI 10.1016/j.ins.2010.09.009
   Wang, J XIAN U POST TELECO, V1
   Wang LY, 2016, OPT LASER ENG, V77, P118, DOI 10.1016/j.optlaseng.2015.07.015
   Wang XY, 2010, NONLINEAR DYNAM, V62, P615, DOI 10.1007/s11071-010-9749-8
   Wang XY, 2015, OPT LASER ENG, V66, P10, DOI 10.1016/j.optlaseng.2014.08.005
   Wang XY, 2014, NONLINEAR DYNAM, V75, P345, DOI 10.1007/s11071-013-1070-x
   Wang XY, 2013, COMMUN NONLINEAR SCI, V18, P3075, DOI 10.1016/j.cnsns.2013.04.008
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Wu Y, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.013014
   Xu SJ, 2012, PHYS LETT A, V376, P1003, DOI 10.1016/j.physleta.2012.01.040
   Yao W, 2015, NONLINEAR DYNAM, V81, P151, DOI 10.1007/s11071-015-1979-3
   Ye RS, 2011, OPT COMMUN, V284, P5290, DOI 10.1016/j.optcom.2011.07.070
   Yue Wu, 2011, Proceedings of the 2011 International Conference on System Science and Engineering (ICSSE), P23, DOI 10.1109/ICSSE.2011.5961867
   Zhang YQ, 2014, INFORM SCIENCES, V273, P329, DOI 10.1016/j.ins.2014.02.156
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
   Zhou YC, 2013, SIGNAL PROCESS, V93, P3039, DOI 10.1016/j.sigpro.2013.04.021
   Zhu CX, 2013, NONLINEAR DYNAM, V71, P25, DOI 10.1007/s11071-012-0639-0
   Zhu ZL, 2011, INFORM SCIENCES, V181, P1171, DOI 10.1016/j.ins.2010.11.009
NR 51
TC 14
Z9 14
U1 7
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1517
EP 1532
DI 10.1007/s00371-022-02426-0
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000785702200001
DA 2024-07-18
ER

PT J
AU Mousa, A
   Badran, Y
   Salama, G
   Mahmoud, T
AF Mousa, Aiman
   Badran, Yasser
   Salama, Gouda
   Mahmoud, Tarek
TI Regression layer-based convolution neural network for synthetic aperture
   radar images: de-noising and super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Single image super-resolution; Synthetic aperture radar; Convolution
   neural network; Regression layer
AB Nowadays, the increasing demands for synthetic aperture radar images are of great importance in both marine and terrestrial applications, due to their availability day and night and in all-weather conditions. Moreover, synthetic aperture radar images are characterized by much more important information than that introduced by optical images. The single image super-resolution is considered a challenging process, especially for synthetic aperture radar images which are usually degraded by high level of speckle noise. Due to the success of convolutional neural networks in many super-resolution optical image applications, a proposed regression layer-based framework is exploited for synthetic aperture radar image enhancement. The proposed regression layer is used during the learning phase to drive the back-propagation error based on the structure similarity index loss function. The structure similarity index is a quality assessment based on comparing the structural information. Consequently, the proposed loss function compares the statistical values of the network output images with that of the optimal target images. Moreover, the proposed framework introduces the concept of adjusting the proportions of the structure similarity index components (luminance, contrast and structure) according to their impacts, by changing their exponents. In addition to that, the proposed layer modifies the statistical properties of the target images, in such a way that drives learning to enhance the resulting image structure and eliminate the speckle noise. This layer is useful for noise reduction and super-resolution applications, especially for those dedicated to synthetic aperture radar images.
C1 [Mousa, Aiman; Badran, Yasser; Salama, Gouda; Mahmoud, Tarek] Egyptian Armed Forces, Cairo, Egypt.
RP Mahmoud, T (corresponding author), Egyptian Armed Forces, Cairo, Egypt.
EM aimanmsaad2000@gmail.com; ronanran@windowslive.com;
   a_gouda94@hotmail.com; tarek.mahmoud.mtc@gmail.com
RI Mahmoud, Tarek/ABB-9830-2021
OI Mahmoud, Tarek/0000-0002-5714-4596
CR Abergel R, 2019, INT GEOSCI REMOTE SE, P608, DOI [10.1109/IGARSS.2019.8900036, 10.1109/igarss.2019.8900036]
   Ahmed A, 2019, IEEE ACCESS, V7, P121350, DOI 10.1109/ACCESS.2019.2936455
   Chen SW, 2018, IEEE J-STARS, V11, P2657, DOI 10.1109/JSTARS.2018.2818939
   Chen SW, 2018, IEEE GEOSCI REMOTE S, V15, P627, DOI 10.1109/LGRS.2018.2799877
   Chen SW, 2016, IEEE T GEOSCI REMOTE, V54, P6919, DOI 10.1109/TGRS.2016.2588325
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HC, 2013, IEEE T GEOSCI REMOTE, V51, P2388, DOI 10.1109/TGRS.2012.2211366
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Massinas BA., 2016, P SPACE C, V5353, P1, DOI [10.2514/6.2016-5353, DOI 10.2514/6.2016-5353]
   Mei SH, 2020, IEEE T GEOSCI REMOTE, V58, P4590, DOI 10.1109/TGRS.2020.2964288
   Mikaeli E, 2020, VISUAL COMPUT, V36, P1573, DOI 10.1007/s00371-019-01756-w
   Penna PAA, 2019, IEEE T GEOSCI REMOTE, V57, P7194, DOI 10.1109/TGRS.2019.2912153
   Rohith G, 2021, VISUAL COMPUT, V37, P1965, DOI 10.1007/s00371-020-01957-8
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yahia M, 2020, IEEE J-STARS, V13, P859, DOI 10.1109/JSTARS.2020.2973920
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang YJ, 2017, INT GEOSCI REMOTE SE, P3082, DOI 10.1109/IGARSS.2017.8127650
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zhang YQ, 2015, IEEE T IMAGE PROCESS, V24, P2797, DOI 10.1109/TIP.2015.2431435
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou DY, 2022, VISUAL COMPUT, V38, P119, DOI 10.1007/s00371-020-02007-z
NR 28
TC 1
Z9 1
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1295
EP 1306
DI 10.1007/s00371-022-02405-5
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000761308900001
DA 2024-07-18
ER

PT J
AU Gao, JX
   Liu, XK
   Liu, RS
   Fan, X
AF Gao, Jiaxin
   Liu, Xiaokun
   Liu, Risheng
   Fan, Xin
TI Learning adaptive hyper-guidance via proxy-based bilevel optimization
   for image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; Bilevel learning; Low-level vision; Hyper-guidance
ID OPTIMALITY CONDITIONS; DECONVOLUTION; RESTORATION; FRAMEWORK
AB In recent years, image enhancement based on deep network plays a vital role and has become the mainstream research. However, current approaches are generally limited to the manual embedding of auxiliary components (e.g., hyper-parameters, appended modules) to train the network; thus, they can often lack flexibility, adaptability, or even fail to achieve the optimal settings. Moreover, the straightforward learning-based architectures cannot adequately handle the complex latent image distributions in real-world scenarios. To partially address the above issues, in this work, a generic adaptive hyper-training scheme based on bilevel optimization is established. Specifically, we propose a completely new bilevel deep-unfolded strategy to collaboratively optimize the inner-level task-related hyper-guidance and the outer-level image reconstruction. The process can embed the differentiable proxy-based network with parameters to automatically learn the appended control mechanism. Instead of constructing the empirically manual interventions, our strategy can proactively learn to learn self-adaptive auxiliary modules. Extensive experiments demonstrate the superiority of our strategy to address different image enhancement tasks (i.e., image restoration, image rain removal and image haze removal) in terms of flexibility and effectiveness.
C1 [Gao, Jiaxin; Liu, Xiaokun] Dalian Univ Technol, Sch Software Technol, Dalian, Peoples R China.
   [Liu, Risheng; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Gao, Jiaxin; Liu, Xiaokun; Liu, Risheng; Fan, Xin] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Liu, RS (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Liu, RS (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM rsliu@dlut.edu.cn
OI Liu, Risheng/0000-0002-9554-0565
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cai J., 2019, ARXIV190300763
   Cheng JT, 2019, SIGNAL IMAGE VIDEO P, V13, P155, DOI 10.1007/s11760-018-1340-7
   Dempe S, 2007, OPTIMIZATION, V56, P577, DOI 10.1080/02331930701617551
   Dempe S., 2002, FDN BILEVEL PROGRAMM
   Du YJ, 2020, IEEE WINT CONF APPL, P2395, DOI 10.1109/WACV45572.2020.9093393
   Fan Z., 2018, ARXIV180407493
   Franceschi L., 2018, ICML
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   King DB, 2015, ACS SYM SER, V1214, P1
   Kohli B, 2012, J OPTIMIZ THEORY APP, V152, P632, DOI 10.1007/s10957-011-9941-0
   Krishnan D., 2009, NEURIPS
   Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491
   Lampariello L, 2020, COMPUT OPTIM APPL, V76, P277, DOI 10.1007/s10589-020-00178-y
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liu D, 2018, ADV NEUR IN, V31
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu R., VALUE FUNCTION BASED
   Liu RS, 2020, IEEE T NEUR NET LEAR, V31, P1653, DOI 10.1109/TNNLS.2019.2921597
   Liu RS, 2020, IEEE T PATTERN ANAL, V42, P3027, DOI 10.1109/TPAMI.2019.2920591
   Liu RS, 2019, IEEE T IMAGE PROCESS, V28, P5013, DOI 10.1109/TIP.2019.2913536
   Liu RS, 2019, IEEE T IMAGE PROCESS, V28, P1528, DOI 10.1109/TIP.2018.2875568
   Liu RS, 2019, IEEE T NEUR NET LEAR, V30, P2973, DOI 10.1109/TNNLS.2018.2862631
   Liu RS, 2010, LECT NOTES COMPUT SC, V6311, P115
   Liu Risheng, 2021, ICML
   Liu Risheng, 2020, ICML
   Liu R, 2018, ADV NEUR IN, V31
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Lorraine J., 2018, ARXIV180209419
   MacKay M, 2019, ARXIV190303088
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren W., 2018, NEURIPS, P297
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simoes M, 2016, IEEE T IMAGE PROCESS, V25, P5266, DOI 10.1109/TIP.2016.2603920
   Sun LB, 2013, IEEE INT CONF COMPUT
   Swersky K., 2014, ARXIV14063896
   Tai X. C., 2006, P INT C PDE BAS IM P
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tirer T, 2019, IEEE T IMAGE PROCESS, V28, P1220, DOI 10.1109/TIP.2018.2875569
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
NR 66
TC 2
Z9 2
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1471
EP 1484
DI 10.1007/s00371-022-02423-3
EA FEB 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000758114500001
DA 2024-07-18
ER

PT J
AU Varga, LG
   Lékó, G
   Balázs, P
AF Varga, Laszlo G.
   Leko, Gabor
   Balazs, Peter
TI Grayscale uncertainty and errors of tomographic reconstructions based on
   projection geometries and projection sets
SO VISUAL COMPUTER
LA English
DT Article
DE Uncertainty; Computed tomography; Algebraic reconstruction; Intensity
   bounds; Reconstruction error
ID SELECTION
AB In certain cases of computed tomography, the projection acquisition process is limited, and thus one cannot gain a sufficient number of projections for an acceptable reconstruction. In this case, the low number of projections yields a lack of information, and uncertainty in the reconstructions. In practice, this means that the pixel values of the reconstruction are not uniquely determined by the measured data and thus can have variable values. In this paper, we provide a theoretically proven uncertainty measure that can be used for measuring the variability of pixel values in grayscale reconstructions. The uncertainty values are based on linear algebra and measure the slopes of the hyperplane of solutions in the algebraic formulation of tomography. The method can also be applied for any linear equation system that satisfies a given set of conditions. Using the uncertainty measure, we also derive upper and lower bounds on the possible pixel values in tomographic reconstructions. Finally, we show how our results can be used for modelling reconstruction error. All of the results are supported by both theoretical proofs and experimental evaluations
C1 [Varga, Laszlo G.; Leko, Gabor; Balazs, Peter] Univ Szeged, Dept Image Proc & Comp Graph, Arpad Ter 2, H-6720 Szeged, Hungary.
C3 Szeged University
RP Varga, LG (corresponding author), Univ Szeged, Dept Image Proc & Comp Graph, Arpad Ter 2, H-6720 Szeged, Hungary.
EM vargalg@inf.u-szeged.hu; leko@inf.u-szeged.hu; pbalazs@infu-szeged.hu
RI Varga, László G./M-2975-2018; Balázs, Péter/M-4393-2018
FU New National Excellence Program of the Ministry for Innovation and
   Technology from National Research, Development and Innovation Fund
   [UNKP-20-4]; project "Integrated program for training new generation of
   scientists in the fields of computer science"
   [EFOP-3.6.3-VEKOP16-2017-00002]; Ministry for Innovation and Technology,
   Hungary [NKFIH-1279-2/2020]; Ministry of Innovation and Technology of
   Hungary from the National Research, Development and Innovation Fund
   [TKP2021-NVA-09]
FX Gabor Leko was supported by the UNKP-20-4 New National Excellence
   Program of the Ministry for Innovation and Technology from the source of
   the National Research, Development and Innovation Fund. This research
   was supported by the project "Integrated program for training new
   generation of scientists in the fields of computer science", no.
   EFOP-3.6.3-VEKOP16-2017-00002. This research was supported by grant
   NKFIH-1279-2/2020 of theMinistry for Innovation and Technology, Hungary.
   This research was supported by Project TKP2021-NVA-09. Project no.
   TKP2021-NVA-09 has been implemented with the support provided by the
   Ministry of Innovation and Technology of Hungary from the National
   Research, Development and Innovation Fund, financed under the
   TKP2021-NVA funding scheme.
CR DEROSIER DJ, 1968, NATURE, V217, P130, DOI 10.1038/217130a0
   Frenkel, 2020, 10 C IND COMP TOM IC, P1
   Frost A, 2013, SENSORS-BASEL, V13, P137, DOI 10.3390/s130100137
   Haque MA, 2013, IEEE T IMAGE PROCESS, V22, P5085, DOI 10.1109/TIP.2013.2280185
   Herman G.T., 1999, Discrete Tomography: Foundations, Algorithms, and Applications
   Herman GT, 2009, ADV PATTERN RECOGNIT, P1
   Herman GT, 2007, APPL NUMER HARMON AN, P1, DOI 10.1007/978-0-8176-4543-4
   Hong QQ, 2016, VISUAL COMPUT, V32, P1251, DOI 10.1007/s00371-015-1160-5
   Hyvönen N, 2010, INVERSE PROBL IMAG, V4, P257, DOI 10.3934/ipi.2010.4.257
   Kak A. C., 1988, Principles of computerized tomographic imaging, DOI 10.1118/1.1455742
   Lékó G, 2018, LECT NOTES COMPUT SC, V10882, P3, DOI 10.1007/978-3-319-93000-8_1
   Liu XL, 2015, VISUAL COMPUT, V31, P1431, DOI 10.1007/s00371-014-1024-4
   Lukic T, 2022, VISUAL COMPUT, V38, P695, DOI 10.1007/s00371-020-02044-8
   Mueller JL, 2012, COMPUT SCI ENG SER, V10, P3, DOI 10.1137/1.9781611972344
   Niklason LT, 1997, RADIOLOGY, V205, P399, DOI 10.1148/radiology.205.2.9356620
   PLACIDI G, 1995, J MAGN RESON SER B, V108, P50, DOI 10.1006/jmrb.1995.1101
   Reiser, 2009, ARXIV09082610V1 U CH
   VANDERSLUIS A, 1990, LINEAR ALGEBRA APPL, V130, P257, DOI 10.1016/0024-3795(90)90215-X
   Varga Laszlo G., 2020, Combinatorial Image Analysis. 20th International Workshop, IWCIA 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12148), P123, DOI 10.1007/978-3-030-51002-2_9
   Varga L, 2011, GRAPH MODELS, V73, P365, DOI 10.1016/j.gmod.2011.06.006
   Varga L, 2010, LECT NOTES COMPUT SC, V6474, P390, DOI 10.1007/978-3-642-17688-3_37
   Varga LG, 2014, COMPUT VIS IMAGE UND, V129, P52, DOI 10.1016/j.cviu.2014.05.006
NR 22
TC 2
Z9 2
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1557
EP 1569
DI 10.1007/s00371-022-02428-y
EA FEB 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000758950400003
DA 2024-07-18
ER

PT J
AU Chen, CW
   Zhou, XF
AF Chen, Changwei
   Zhou, Xiaofeng
TI Collaborative representation-based fuzzy discriminant analysis for Face
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Feature extraction; Fuzzy discriminant analysis;
   Collaborative representation
ID FEATURE-EXTRACTION; PRESERVING PROJECTIONS; DIRECT LDA; SIMILARITY;
   EIGENFACES
AB In face recognition, the dimensionality reduction (DR) method is usually used to extract the discriminative features of the image. However, the performance is easily affected by varying facial poses, expressions and illumination. To solve this problem, a novel DR algorithm, namely collaborative representation-based fuzzy discriminant analysis (CRFDA), is proposed in this paper. In CRFDA, each training sample is firstly collaboratively represented by the overall training samples, and the fuzzy membership degrees of each sample are computed in terms of the representation coefficients. Secondly, the fuzzy means of different classes are computed using the membership degrees. Thirdly, the between-class and within-class scatter matrices are calculated to model the separability and compactness of samples, respectively. Finally, the feature extraction standard is improved by maximizing the ratio of fuzzy between-class scatter to fuzzy within-class scatter. A large number of experiments on publicly available facial datasets demonstrate the effectiveness of the proposed method.
C1 [Chen, Changwei] Nanjing Xiaozhuang Univ, Coll Informat & Engn, Nanjing 211171, Peoples R China.
   [Chen, Changwei; Zhou, Xiaofeng] Hohai Univ, Coll Comp & Informat, Nanjing 210098, Peoples R China.
C3 Nanjing Xiaozhuang University; Hohai University
RP Chen, CW (corresponding author), Nanjing Xiaozhuang Univ, Coll Informat & Engn, Nanjing 211171, Peoples R China.; Chen, CW (corresponding author), Hohai Univ, Coll Comp & Informat, Nanjing 210098, Peoples R China.
EM chenchangwei8285@126.com
RI zhou, Xiaofeng/HII-3631-2022
FU National Natural Science Foundation of China [11101216]; University
   Level Scientific Research Project of Nanjing Xiaozhuang University
   [2019NXY25]; Training Objects of HighLevel Talents of Jiangsu Province
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant No.11101216), the University Level Scientific
   Research Project of Nanjing Xiaozhuang University (Grant No. 2019NXY25)
   and the Training Objects of HighLevel Talents of Jiangsu Province.
CR Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Chen HT, 2005, PROC CVPR IEEE, P846
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Gui J, 2012, PATTERN RECOGN, V45, P2884, DOI 10.1016/j.patcog.2012.02.005
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Huang P, 2019, SOFT COMPUT, V23, P7015, DOI 10.1007/s00500-018-3340-5
   Huang P, 2018, DIGIT SIGNAL PROCESS, V76, P84, DOI 10.1016/j.dsp.2018.02.009
   Huang P, 2017, IEEE ACCESS, V5, P4340, DOI 10.1109/ACCESS.2017.2680437
   Huang P, 2015, COMPUT ELECTR ENG, V46, P231, DOI 10.1016/j.compeleceng.2015.03.013
   Huang P, 2015, AEU-INT J ELECTRON C, V69, P1724, DOI 10.1016/j.aeue.2015.08.009
   Huang P, 2014, NEUROCOMPUTING, V140, P104, DOI 10.1016/j.neucom.2014.03.031
   Huang P, 2014, NEUROCOMPUTING, V139, P180, DOI 10.1016/j.neucom.2014.02.047
   Huang P, 2014, J VIS COMMUN IMAGE R, V25, P296, DOI 10.1016/j.jvcir.2013.11.007
   Kwak KC, 2005, PATTERN RECOGN, V38, P1717, DOI 10.1016/j.patcog.2005.01.018
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Song FX, 2007, NEUROCOMPUTING, V71, P191, DOI 10.1016/j.neucom.2007.01.003
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wan MH, 2017, FUZZY SET SYST, V318, P120, DOI 10.1016/j.fss.2016.06.001
   Wang XG, 2004, PROC CVPR IEEE, P564
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang WK, 2015, PATTERN RECOGN, V48, P20, DOI 10.1016/j.patcog.2014.07.009
   Ye J, 2004, ADV NEURAL INFORM PR, P1569
   Ye JP, 2005, IEEE T PATTERN ANAL, V27, P929, DOI 10.1109/TPAMI.2005.110
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
NR 35
TC 4
Z9 4
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1383
EP 1393
DI 10.1007/s00371-021-02325-w
EA FEB 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000751592300001
DA 2024-07-18
ER

PT J
AU Lin, ZK
   Sun, W
   Tang, B
   Li, JD
   Yao, XY
   Li, Y
AF Lin, Zhongkang
   Sun, Wei
   Tang, Bo
   Li, Jinda
   Yao, Xinyuan
   Li, Yu
TI Semantic segmentation network with multi-path structure, attention
   reweighting and multi-scale encoding
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Multi-scale information; Attention mechanism;
   Multi-path networks
AB Semantic segmentation is an active field of computer vision. It provides semantic information for many applications. In semantic segmentation tasks, spatial information, context information, and high-level semantic information play an important role in improving segmentation accuracy. In this paper, a semantic segmentation network with multi-path structure, attention reweighting, and multi-scale encoding structure is proposed. Firstly, three parallel structures were designed, including a pyramid spatial path with a pyramid image input, a context path composed of a lightweight backbone network, and a semantic graph path composed of spatial graph convolutional layers. Secondly, a feature fusion module was designed to perform a weighted fusion of the output features of different paths based on the channel attention mechanism. Then, the semantic segmentation dataset CamVid and Cityscapes were used for network training. Finally, ablation experiments were carried out to verify the effectiveness of the proposed network components, and analyze the computational efficiency and segmentation accuracy of the model. The experimental results show that the semantic segmentation network can improve the accuracy of semantic segmentation by combining multi-scale information, high-level semantic information, and global context information while ensuring high computational efficiency.
C1 [Lin, Zhongkang; Sun, Wei; Tang, Bo; Li, Jinda; Yao, Xinyuan; Li, Yu] Wuhan Univ Sci & Technol, Key Lab Met Equipment & Control Technol, Wuhan 430081, Peoples R China.
   [Lin, Zhongkang; Sun, Wei; Tang, Bo; Li, Jinda; Yao, Xinyuan; Li, Yu] Wuhan Univ Sci & Technol, Engn Res Ctr Met Automat & Measurement Technol, Wuhan 430081, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University of Science &
   Technology
RP Sun, W; Tang, B (corresponding author), Wuhan Univ Sci & Technol, Key Lab Met Equipment & Control Technol, Wuhan 430081, Peoples R China.; Sun, W; Tang, B (corresponding author), Wuhan Univ Sci & Technol, Engn Res Ctr Met Automat & Measurement Technol, Wuhan 430081, Peoples R China.
EM mclin1997@foxmail.com; sw@wust.edu.cn; tangbo@wust.edu.cn;
   1402040956@qq.com; 1450683493@qq.com; 745234332@qq.com
OI WEI, SUN/0000-0002-3424-9643
FU National Natural Science Foundation of China [51874217]; Foundation of
   Hubei Provincial Education Department [B2020011]; WUST National Defense
   Pre-research Foundation [GF202008]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 51874217), Foundation of Hubei Provincial
   Education Department (No. B2020011), WUST National Defense Pre-research
   Foundation (No. GF202008).
CR [Anonymous], 2016, ENET DEEP NEURAL NET
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Boulch A, 2018, COMPUT GRAPH-UK, V71, P189, DOI 10.1016/j.cag.2017.11.010
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng BW, 2019, IEEE I CONF COMP VIS, P5217, DOI 10.1109/ICCV.2019.00532
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fu J, 2021, IEEE T NEUR NET LEAR, V32, P2547, DOI 10.1109/TNNLS.2020.3006524
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hegde S, 2021, COMPUT GRAPH-UK, V95, P13, DOI 10.1016/j.cag.2021.01.004
   Howard A. G., 2017, arXiv
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kipf TN, 2017, INT C LEARN REPR
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Ku T, 2020, COMPUT GRAPH-UK, V93, P13, DOI 10.1016/j.cag.2020.09.006
   Li C, 2020, COMPUT GRAPH-UK, V90, P11, DOI 10.1016/j.cag.2020.05.003
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Li Y, 2018, ADV NEUR IN, V31
   Liang XD, 2018, 32 C NEURAL INFORM P, V31
   Liu F, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.014501
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu YS, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.016501
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Michieli Umberto, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P397, DOI 10.1007/978-3-030-58598-3_24
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang CL, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.051223
   Wang DY, 2021, J APPL REMOTE SENS, V15, DOI 10.1117/1.JRS.15.016505
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang PY, 2018, COMPUT GRAPH-UK, V76, P182, DOI 10.1016/j.cag.2018.07.011
   Wang Y, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.6.063005
   Wang YQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392443
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z., 2017, ARXIVABS171200213
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yuan D, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.5.053004
   Zhang QM, 2019, ADV NEUR IN, V32
   Zhang R, 2017, IEEE I CONF COMP VIS, P2050, DOI 10.1109/ICCV.2017.224
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao YF, 2019, IEEE I CONF COMP VIS, P9176, DOI 10.1109/ICCV.2019.00927
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 58
TC 12
Z9 12
U1 9
U2 171
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 597
EP 608
DI 10.1007/s00371-021-02360-7
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000749116600001
DA 2024-07-18
ER

PT J
AU Ottoni, ALC
   Novo, MS
   Costa, DB
AF Carvalho Ottoni, Andre Luiz
   Novo, Marcela Silva
   Costa, Dayana Bastos
TI Hyperparameter tuning of convolutional neural networks for building
   construction image classification
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Convolutional neural networks; Hyperparameter tuning;
   Scott-Knott method; Building construction image classification
ID LEARNING APPROACH; DEEP; SYSTEM
AB Deep Learning models have important applications in image processing. However, one of the challenges in this field is the definition of hyperparameters. Thus, the objective of this work is to propose a rigorous methodology for hyperparameter tuning of Convolutional Neural Network for building construction image classification, especially in roofs structure analysis. For this, the HyperTuningSK algorithm was developed, intended to create recommendation rankings for two hyperparameters: learning rate and optimizer. The approach uses concepts from the statistical design of experiments, such as Analysis of Variance and the Scott-Knott clustering algorithm. In addition, the adopted database includes images of inspections on buildings roofs made with unmanned aerial vehicles. The images are divided into two classes: (i) roofs with clean gutters and (ii) roofs with dirty gutters. The methods recommended by the HyperTuningSK algorithm achieved good results in comparison to the hyperparameters adopted in the literature. In this respect, adagrad015 achieved the highest average values of accuracy in the validation (100%) and testing steps (90%) for Convolutional Neural Network architecture with 12 layers. In addition, the hyperparameters recommended by the HyperTuningSK algorithm achieved the best test results for other two literature architectures: Densenet121 (85.7%) and VGG16 (84.4%).
C1 [Carvalho Ottoni, Andre Luiz] Univ Fed Reconcavo Bahia, Technol & Exact Ctr, Cruz Das Almas, Brazil.
   [Novo, Marcela Silva] Univ Fed Bahia, Dept Elect & Comp Engn, Salvador, BA, Brazil.
   [Costa, Dayana Bastos] Univ Fed Bahia, Dept Struct & Construct Engn, Salvador, BA, Brazil.
C3 Universidade Federal do Reconcavo da Bahia; Universidade Federal da
   Bahia; Universidade Federal da Bahia
RP Ottoni, ALC (corresponding author), Univ Fed Reconcavo Bahia, Technol & Exact Ctr, Cruz Das Almas, Brazil.
EM andre.ottoni@ufrb.edu.br; marcela.novo@ufba.br; dayanabcosta@ufba.br
RI Ottoni, André Luiz Carvalho/HNR-9085-2023; Bastos Costa,
   Dayana/Z-4428-2019
OI Ottoni, André Luiz Carvalho/0000-0003-2136-9870; Bastos Costa,
   Dayana/0000-0002-1457-6401; Novo, Marcela/0000-0003-2742-3145
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Aguiar GJ, 2019, PATTERN RECOGN LETT, V128, P480, DOI 10.1016/j.patrec.2019.10.018
   Bang S, 2019, COMPUT-AIDED CIV INF, V34, P713, DOI 10.1111/mice.12440
   Bartlett MS, 1937, PROC R SOC LON SER-A, V160, P0268, DOI 10.1098/rspa.1937.0109
   Basgalupp M.P., 2020, IN PRESS, P1
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bhosle K, 2019, J INDIAN SOC REMOTE, V47, P1949, DOI 10.1007/s12524-019-01041-2
   Braun A, 2019, AUTOMAT CONSTR, V106, DOI 10.1016/j.autcon.2019.102879
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen SP, 2019, PROC INT C TOOLS ART, P170, DOI 10.1109/ICTAI.2019.00032
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Cheng SL, 2019, VISUAL COMPUT, V35, P1255, DOI 10.1007/s00371-018-1583-x
   Chollet F., 2018, Towards Data Science
   Conceiçao J, 2017, J PERFORM CONSTR FAC, V31, DOI 10.1061/(ASCE)CF.1943-5509.0001094
   Czerniawski T, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101131
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elgendy M., 2020, Deep Learning for Vision Systems
   Feurer M, 2015, ADV NEUR IN, V28
   Garcez N, 2012, CONSTR BUILD MATER, V35, P1034, DOI 10.1016/j.conbuildmat.2012.06.047
   Gökstorp SGE, 2022, VISUAL COMPUT, V38, P2033, DOI 10.1007/s00371-021-02264-6
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110
   Guo JJ, 2021, COMPUT-AIDED CIV INF, V36, P302, DOI 10.1111/mice.12632
   Guo JJ, 2020, COMPUT-AIDED CIV INF, V35, P1403, DOI 10.1111/mice.12578
   Hammerla N.Y., 2016, P 25 INT JOINT C ART
   Hertel L, 2020, SOFTWAREX, V12, DOI 10.1016/j.softx.2020.100591
   Hijam D, 2022, VISUAL COMPUT, V38, P525, DOI 10.1007/s00371-020-02032-y
   Hu YQ, 2020, INT J MACH LEARN CYB, V11, P795, DOI 10.1007/s13042-020-01062-1
   Hutter F, 2019, SPRING SER CHALLENGE, P1, DOI 10.1007/978-3-030-05318-5
   Jaafra Y, 2019, IMAGE VISION COMPUT, V89, P57, DOI 10.1016/j.imavis.2019.06.005
   Jelihovschi E.G., 2014, TEMA (São Carlos), V15, P3
   Kim B, 2021, NEURAL COMPUT APPL, V33, P9289, DOI 10.1007/s00521-021-05690-8
   Kingma D. P., 2014, arXiv
   Kouzehgar M, 2019, AUTOMAT CONSTR, V108, DOI 10.1016/j.autcon.2019.102959
   Lakshmi L, 2021, WIRELESS PERS COMMUN, V118, P3549, DOI 10.1007/s11277-021-08196-7
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li SM, 2019, AM J ROENTGENOL, V212, P950, DOI 10.2214/AJR.18.20414
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Mantovani RG, 2019, INFORM SCIENCES, V501, P193, DOI 10.1016/j.ins.2019.06.005
   Mantovani RG, 2016, PROCEEDINGS OF 2016 5TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS 2016), P37, DOI [10.1109/BRACIS.2016.62, 10.1109/BRACIS.2016.018]
   Monshi MMA, 2021, COMPUT BIOL MED, V133, DOI 10.1016/j.compbiomed.2021.104375
   Montgomery D. C., 2020, DESIGN ANAL EXPT
   Nahhas FH, 2018, J SENSORS, V2018, DOI 10.1155/2018/7212307
   Ni XP, 2020, HORTIC RES-ENGLAND, V7, DOI 10.1038/s41438-020-0323-3
   Nutter F, 2014, PR MACH LEARN RES, V32
   Ottoni ALC, 2021, IEEE LAT AM T, V19, P2062, DOI 10.1109/TLA.2021.9480148
   Ottoni ALC, 2022, COMPLEX INTELL SYST, V8, P2001, DOI 10.1007/s40747-021-00444-4
   Ottoni ALC, 2020, SOFT COMPUT, V24, P4441, DOI 10.1007/s00500-019-04206-w
   Ottoni ALC, 2018, J CONTROL AUTOM ELEC, V29, P350, DOI 10.1007/s40313-018-0374-y
   Pirotti F, 2019, INT ARCH PHOTOGRAMM, V42-2, P975, DOI 10.5194/isprs-archives-XLII-2-W11-975-2019
   Postalcioglu S, 2020, INT J PATTERN RECOGN, V34, DOI 10.1142/S0218001420510039
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Rangarajan AK, 2018, PROCEDIA COMPUT SCI, V133, P1040, DOI 10.1016/j.procs.2018.07.070
   Razali N. M., 2011, J. Stat. Model. and Anal., V2, P21, DOI DOI 10.1515/BILE-2015-0008
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Russell S., 2016, Artificial intelligence a modern approach
   SCOTT AJ, 1974, BIOMETRICS, V30, P507, DOI 10.2307/2529204
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Silva I.N. d., 2016, Artificial Neural Networks: A Practical Course
   Silveira Bruno, 2021, Proceedings of the 18th International Conference on Computing in Civil and Building Engineering. ICCCBE 2020. Lecture Notes in Civil Engineering (LNCE 98), P1055, DOI 10.1007/978-3-030-51295-8_73
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh R, 2021, VISUAL COMPUT, V37, P2157, DOI 10.1007/s00371-020-01977-4
   Staffa L.B, 2020, ENTAC NAT M BUILT EN
   Strohm H, 2020, INT J COMPUT ASS RAD, V15, P1487, DOI 10.1007/s11548-020-02197-w
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Uçkun FA, 2020, SIG PROCESS COMMUN, DOI 10.1109/siu49456.2020.9302448
   Ünlü R, 2022, VISUAL COMPUT, V38, P685, DOI 10.1007/s00371-020-02043-9
   Wadhawan A, 2020, NEURAL COMPUT APPL, V32, P7957, DOI 10.1007/s00521-019-04691-y
   Wang W, 2020, SENS IMAGING, V21, DOI 10.1007/s11220-020-00285-4
   Yadav O, 2018, IEEE INT C BIOINFORM, P2368, DOI 10.1109/BIBM.2018.8621525
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yang M., 2021, VISUAL COMPUT, P1
   Younis MC, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.046510
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhou SL, 2020, AUTOMAT CONSTR, V114, DOI 10.1016/j.autcon.2020.103171
NR 79
TC 6
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 847
EP 861
DI 10.1007/s00371-021-02350-9
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741262500001
DA 2024-07-18
ER

PT J
AU Chen, YM
   Xiang, ZY
   Du, WT
AF Chen, Yiman
   Xiang, Zhiyu
   Du, Wentao
TI Improving lane detection with adaptive homography prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Lane detection; Curve fitting; Homography prediction
ID TRACKING
AB Lane marks regulate the routes and define the prior drivable areas for a vehicle. Robust detection of lanes plays a vital role in intelligent vehicle navigation. Lane detection algorithms are usually composed of two steps: lane candidate generation and lane curve fitting. The latter is not only used for fitting lane mark candidates with concise curve forms but also for removal of the outliers produced in the former step. Therefore, lane curve fitting is crucial for lane detection. In this step, a common way is carrying out the curve fitting on the bird's-eye view (BEV), which can mitigate the distortion caused by the perspective projection and improve the fitting results. However, due to the sloping road surfaces in real scenarios, the relative pose between the camera and the ground can change frequently, where using a fixed pre-calibrated projection matrix could bring extra errors in curve fitting. In this paper, we propose a homography prediction network named HP-Net for robust lane mark fitting under various sloping roads. The network can adaptively predict the homographic projection matrix for each input image, producing a suitable BEV for lane fitting. Considering the parallel nature of multiple lanes, the HP-Net could skillfully be trained by reusing the lane labels originally for the task of lane mark segmentation, without introducing any extra manpower. Our method has been verified on a large dataset CULane and another dataset acquired by ourselves. Experiment results show that the proposed model can effectively improve the robustness and accuracy of lane detection.
C1 [Chen, Yiman; Du, Wentao] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.
   [Xiang, Zhiyu] Zhejiang Univ, Zhejiang Prov Key Lab Informat Proc Commun & Netw, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Xiang, ZY (corresponding author), Zhejiang Univ, Zhejiang Prov Key Lab Informat Proc Commun & Netw, Hangzhou, Peoples R China.
EM chenyiman@zju.edu.cn; xiangzy@zju.edu.cn
OI Chen, Yiman/0000-0001-6809-4291; Xiang, Zhiyu/0000-0002-3329-7037
FU NSFC-Zhejiang Joint Fund for the Integration of Industrialization and
   Informatization [U1709214]; Key Research & Development Plan of Zhejiang
   Province [2021C01196]
FX The work is supported by NSFC-Zhejiang Joint Fund for the Integration of
   Industrialization and Informatization under grant No.U1709214 and Key
   Research & Development Plan of Zhejiang Province (2021C01196).
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Aly M, 2008, IEEE INT VEH SYM, P165, DOI 10.1109/ivs.2008.4621152
   Borkar A, 2009, IEEE IMAGE PROC, P3261, DOI 10.1109/ICIP.2009.5413980
   Bruls T, 2019, IEEE INT VEH SYM, P302, DOI [10.1109/ivs.2019.8814056, 10.1109/IVS.2019.8814056]
   Chanawangsa P, 2012, INT CONF CONNECT VEH, P166, DOI 10.1109/ICCVE.2012.38
   Chiu KY, 2005, 2005 IEEE INTELLIGENT VEHICLES SYMPOSIUM PROCEEDINGS, P706
   Das D.K., VISUAL COMPUT
   Ding D, 2013, TENCON IEEE REGION
   Gackstatter C, 2010, ADVANCED MICROSYSTEMS FOR AUTOMOTIVE APPLICATIONS 2010: SMART SYSTEMS FOR GREEN CARS AND SAFE MOBILITY, P133, DOI 10.1007/978-3-642-16362-3_14
   Hang Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P689, DOI 10.1007/978-3-030-58555-6_41
   He B, 2016, IEEE INT VEH SYM, P1041, DOI 10.1109/IVS.2016.7535517
   Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110
   Hsu Y.-C., 2018, P 2018 INT JOINT C N, P1, DOI DOI 10.1109/IVCNZ.2018.8634799
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kim J, 2014, LECT NOTES COMPUT SC, V8834, P454, DOI 10.1007/978-3-319-12637-1_57
   Ko YeongMin, 2020, ABS200206604 CORR
   Kumar Ammu M., 2015, International Journal of Computer Science & Information Technology, V7, P65, DOI 10.5121/ijcsit.2015.7406
   Lee C, 2018, IEEE T INTELL TRANSP, V19, P4043, DOI 10.1109/TITS.2018.2791572
   Lee S, 2017, IEEE I CONF COMP VIS, P1965, DOI 10.1109/ICCV.2017.215
   Li J, 2017, IEEE T NEUR NET LEAR, V28, P690, DOI 10.1109/TNNLS.2016.2522428
   Li X, 2020, IEEE T INTELL TRANSP, V21, P248, DOI 10.1109/TITS.2019.2890870
   Liu GL, 2010, IEEE INT VEH SYM, P993, DOI 10.1109/IVS.2010.5548021
   Liu L., 2021, ARXIV210505003
   Liu RJ, 2021, IEEE WINT CONF APPL, P3693, DOI 10.1109/WACV48630.2021.00374
   Loose H, 2009, IEEE INT VEH SYM, P60, DOI 10.1109/IVS.2009.5164253
   Neven D, 2018, IEEE INT VEH SYM, P286
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Qu GX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P283, DOI 10.1145/3240508.3240553
   Qu Zhan, 2021, CVPR
   Son J, 2015, EXPERT SYST APPL, V42, P1816, DOI 10.1016/j.eswa.2014.10.024
   Srivastava S., 2014, J ADV RES COMPUT SCI, V4, P30
   Su J., 2021, P INT JOINT C ART IN, P1
   Tabelini L., 2021, P IEEE C COMP VIS PA, P294
   Tabelini L., 2020, ARXIV PREPRINT ARXIV
   Wang Y, 2004, IMAGE VISION COMPUT, V22, P269, DOI 10.1016/j.imavis.2003.10.003
   Yang S., VISUAL COMPUT
   Yoo S, 2020, IEEE COMPUT SOC CONF, P4335, DOI 10.1109/CVPRW50498.2020.00511
   Zequn Qin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P276, DOI 10.1007/978-3-030-58586-0_17
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
NR 39
TC 9
Z9 9
U1 3
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 581
EP 595
DI 10.1007/s00371-021-02358-1
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600011
DA 2024-07-18
ER

PT J
AU Li, XL
   Hua, Z
   Li, JJ
AF Li, Xiaoling
   Hua, Zhen
   Li, Jinjiang
TI Attention-based adaptive feature selection for multi-stage image
   dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Single image dehazing; Multi-stage network; Self-calibration attention;
   Adaptive feature selection
ID U-NET; NETWORK
AB Removing haze, especially non-homogeneous and in various concentrations, is quite challenging. Existing dehazing methods are usually used to deal with homogeneous haze or just to deal with non-homogeneous haze. Few methods can be used in dealing with differently distributed haze. To address this problem, we propose an attention-based adaptive feature selection for multi-stage image dehazing network (ASNet). Rich and effective detail features are extracted by adaptive selection and aggregation, which in turn leads to better recovery of haze images with different concentrations and non-uniform distribution. Specifically, our model first learns different scale contextual feature information using an encoder-decoder architecture and then fuses the original resolution imaged features with spatial information through convolution and cascade operations. We introduce a new adaptive feature selection module, which selects features at different stages through different attention mechanisms to extract more effective image features. For ASNet, the exchange of information between stages is crucial. Therefore, we designed the self-calibration attention module between the two stages to supervise the features under the guidance of ground-truth, which can effectively avoid the loss and redundancy of information while realizing the information transfer between the stages. This inter-stage modular design enhances the connectivity of the modules, reduces the network burden, and improves the processing power of the network. Extensive experiments have shown that the proposed ASNet outperforms advanced methods in both uniform and non-uniform haze datasets with excellent dehazing and satisfactory visual effects.
C1 [Li, Xiaoling] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Inst Network Technol, Yantai, Peoples R China.
   [Hua, Zhen] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
   [Li, Jinjiang] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University; Shandong Technology & Business University
RP Hua, Z (corresponding author), Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
EM 1828162238@qq.com; huazhen66@foxmail.com; lijinjiang@gmail.com
RI Hua, Zhen/AGN-6068-2022; wang, xiao/HZI-9156-2023
FU National Natural Science Foundation of China [61772319, 62002200,
   61976125, 61976124, 12001327]; Shandong Natural Science Foundation of
   China [ZR2020QF012]
FX The authors acknowledge the National Natural Science Foundation of China
   (Grant Nos. 61772319, 62002200, 61976125, 61976124, 12001327) and
   Shandong Natural Science Foundation of China (Grant No. ZR2020QF012).
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Anvari, 2020, ARXIV PREPRINT ARXIV
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen WT, 2020, IEEE T IMAGE PROCESS, V29, P6773, DOI 10.1109/TIP.2020.2993407
   Cheng BW, 2019, IEEE I CONF COMP VIS, P5217, DOI 10.1109/ICCV.2019.00532
   Chu C, 2021, COMPUT METH PROG BIO, V199, DOI 10.1016/j.cmpb.2020.105906
   Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   Ghosh P, 2020, IEEE WINT CONF APPL, P565, DOI 10.1109/WACV45572.2020.9093361
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu DC, 2020, ADV INTELL SYST COMP, V1038, P432, DOI 10.1007/978-3-030-29513-4_31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua Z, 2020, IEEE ACCESS, V8, P167693, DOI 10.1109/ACCESS.2020.3023906
   Huang SQ, 2019, IEEE ACCESS, V7, P104179, DOI 10.1109/ACCESS.2019.2929591
   Huang XN, 2021, IEEE ACCESS, V9, P33756, DOI 10.1109/ACCESS.2021.3061078
   Kar A, ARXIV PREPRINT ARXIV
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li B., 2017, ARXIV PREPRINT ARXIV
   Li B, 2020, SIGNAL IMAGE VIDEO P, V14, P1245, DOI 10.1007/s11760-020-01665-9
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li JJ, 2018, IEEE ACCESS, V6, P26831, DOI 10.1109/ACCESS.2018.2833888
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li SJ, 2023, IEEE T PATTERN ANAL, V45, P6647, DOI 10.1109/TPAMI.2020.3021756
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mei KF, 2019, LECT NOTES COMPUT SC, V11361, P203, DOI 10.1007/978-3-030-20887-5_13
   Mingzhao Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P1832, DOI 10.1109/CVPRW50498.2020.00233
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Ngo D, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142233
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shi J, 2021, IEEE ACCESS, V9, P13304, DOI 10.1109/ACCESS.2021.3052224
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Tangsakul S, 2020, IEEE ACCESS, V8, P103181, DOI 10.1109/ACCESS.2020.2999076
   Wang CS, 2020, IEEE ACCESS, V8, P9488, DOI 10.1109/ACCESS.2020.2964271
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu HY, 2020, IEEE COMPUT SOC CONF, P1975, DOI 10.1109/CVPRW50498.2020.00247
   Wu QB, 2020, IEEE T IMAGE PROCESS, V29, P2583, DOI 10.1109/TIP.2019.2949392
   Xiao JS, 2020, NEUROCOMPUTING, V389, P108, DOI 10.1016/j.neucom.2020.01.007
   Yan JJ, 2020, IEEE ACCESS, V8, P25431, DOI 10.1109/ACCESS.2020.2971092
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yeh CH, 2020, IEEE T IMAGE PROCESS, V29, P3153, DOI 10.1109/TIP.2019.2957929
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang JM, 2021, IEEE ACCESS, V9, P10858, DOI 10.1109/ACCESS.2021.3050628
   Zhang Y, 2017, IEEE C COMP VIS PATT, P5287, DOI DOI 10.1109/CVPR.2017.537
   Zhu, 2014, BMVC
NR 58
TC 10
Z9 10
U1 3
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 663
EP 678
DI 10.1007/s00371-021-02365-2
EA JAN 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600008
DA 2024-07-18
ER

PT J
AU Chen, XY
   Jiang, SF
   Guo, LT
   Chen, Z
   Zhang, CX
AF Chen, Xingyan
   Jiang, Shaofeng
   Guo, Lanting
   Chen, Zhen
   Zhang, Congxuan
TI Whole brain segmentation method from 2.5D brain MRI slice image based on
   Triple U-Net
SO VISUAL COMPUTER
LA English
DT Article
DE Triple U-Net; 2; 5D slice image; Whole brain segmentation; Inter-layer
   constraints
ID SKULL STRIPPING PROBLEM
AB With the application of magnetic resonance imaging (MRI) in the diagnosis of brain diseases, MRI has become a powerful tool for clinical neuroimaging analysis, and whole brain segmentation is a very important step of neuroimaging analysis. In order to promote segmentation accuracy, this paper makes full use of the spatial constraint information between adjacent slice of MRI brain image sequence, and combines three consecutive images into one image named 2.5D slice image. Through 2.5D slice image, a triple U-Net-based whole brain segmentation method is proposed, which is composed of one main U-Net and two auxiliary U-Nets. Moreover, this network introduces three auxiliary outputs to provide inter-layer constraints for the final prediction, and uses a multi-auxiliary hybrid loss function based on binary cross-entropy loss and dice loss to optimize the training model. In this paper, a comprehensive comparative experiment is carried out on LPBA40 dataset and IBSR18 dataset. The experimental results show that the dice coefficient, specificity and sensitivity of this method are 98.23%, 99.62% and 98.54%, respectively, on LPBA40 dataset, and 97.01%, 99.45% and 98%, respectively, on IBSR18 dataset. The experiments show that the accuracy of whole brain segmentation is greatly improved by the proposed triple U-Net.
C1 [Chen, Xingyan; Jiang, Shaofeng; Guo, Lanting; Chen, Zhen; Zhang, Congxuan] Nanchang Hangkong Univ, Sch Measuring & Opt Engn, Nanchang 330063, Jiangxi, Peoples R China.
   [Jiang, Shaofeng; Chen, Zhen; Zhang, Congxuan] Nanchang Hangkong Univ NCHU, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
C3 Nanchang Hangkong University; Nanchang Hangkong University
RP Jiang, SF (corresponding author), Nanchang Hangkong Univ, Sch Measuring & Opt Engn, Nanchang 330063, Jiangxi, Peoples R China.; Jiang, SF (corresponding author), Nanchang Hangkong Univ NCHU, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
EM 1908080400122@stu.nchu.edu.cn; jsphone@163.com; 913925986@qq.com;
   dr_chenzhen@163.com; zcxdsg@163.com
FU National Natural Science Foundation of China [61162023]; Natural Science
   Foundation of Jiangxi Province [20192BAB205083]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61162023, was supported by the Natural Science
   Foundation of Jiangxi Province, Grant Number 20192BAB205083.
CR Chang HH, 2009, NEUROIMAGE, V47, P122, DOI 10.1016/j.neuroimage.2009.03.068
   Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Hahn HK, 2000, LECT NOTES COMPUT SC, V1935, P134
   Holland O, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500442
   Huang H, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9148748
   Hwang H, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9030569
   Isensee F, 2019, HUM BRAIN MAPP, V40, P4952, DOI 10.1002/hbm.24750
   Jadon S, 2020, 2020 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB), P115, DOI 10.1109/cibcb48159.2020.9277638
   Kim J., 2020, ICLR, P1
   Liang Y, 2019, LECT NOTES COMPUT SC, V11766, P292, DOI 10.1007/978-3-030-32248-9_33
   Lucena O, 2019, ARTIF INTELL MED, V98, P48, DOI 10.1016/j.artmed.2019.06.008
   Ma YD, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P743
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Rohlfing T, 2012, IEEE T MED IMAGING, V31, P153, DOI 10.1109/TMI.2011.2163944
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salehi SSM, 2017, IEEE T MED IMAGING, V36, P2319, DOI 10.1109/TMI.2017.2721362
   Sandor S, 1997, IEEE T MED IMAGING, V16, P41, DOI 10.1109/42.552054
   Ségonne F, 2004, NEUROIMAGE, V22, P1060, DOI 10.1016/j.neuroimage.2004.03.032
   Shattuck DW, 2008, NEUROIMAGE, V39, P1064, DOI 10.1016/j.neuroimage.2007.09.031
   Shattuck DW, 2001, NEUROIMAGE, V13, P856, DOI 10.1006/nimg.2000.0730
   Smith SM, 2002, HUM BRAIN MAPP, V17, P143, DOI 10.1002/hbm.10062
   Ward BDJBRI., 1999, MED COLL WISCONSIN
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Zhang HH, 2019, LECT NOTES COMPUT SC, V11766, P338, DOI 10.1007/978-3-030-32248-9_38
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 27
TC 3
Z9 4
U1 3
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 255
EP 266
DI 10.1007/s00371-021-02326-9
EA NOV 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000714996100001
DA 2024-07-18
ER

PT J
AU Xiao, BL
   Da, FP
AF Xiao, Bingling
   Da, Feipeng
TI Three-stage generative network for single-view point cloud completion
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape completion; Point cloud; Deep learning
ID SHAPE
AB 3D shape completion from single-view scan is an important task for follow-up applications such as recognition and segmentation, but it is challenging due to the critical sparsity and structural incompleteness of single-view point clouds. In this paper, a three-stage generative network (TSGN) is proposed for single-view point cloud completion, which generates fine-grained dense point clouds step by step and effectively overcomes the ubiquitous problem-the imbalance between general and individual characteristics. In the first stage, an encoder-decoder network consumes a partial point cloud and generates a rough sparse point cloud inferring the complete geometric shape. Then, a bi-channel residual network is designed to refine the preliminary result with assistance of the original partial input. A local-based folding network is introduced in the last stage to extract local context information from the revised result and build a dense point cloud with finer-grained details. Experiments on ShapeNet dataset and KITTI dataset validate the effectiveness of TSGN. The results on ShapeNet demonstrate the competitive performance on both CD and EMD.
C1 [Xiao, Bingling; Da, Feipeng] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Xiao, Bingling; Da, Feipeng] Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.
   [Da, Feipeng] Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Da, FP (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.; Da, FP (corresponding author), Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.; Da, FP (corresponding author), Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
EM dafp@seu.edu.cn
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province of China [BK20192004C]; Natural Science Foundation of
   Jiangsu Province of China [BK20181269]
FX This work is supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province of China (Grant No.
   BK20192004C) and Natural Science Foundation of Jiangsu Province of China
   (Grant No. BK20181269).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fan YL, 2018, VISUAL COMPUT, V34, P659, DOI 10.1007/s00371-017-1405-6
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Li DP, 2017, IEEE T VIS COMPUT GR, V23, P1809, DOI 10.1109/TVCG.2016.2553102
   Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202
   Liu M., 2019, Morphing and Sampling Network for Dense Point Cloud Completion *
   Luciano L, 2019, VISUAL COMPUT, V35, P1171, DOI 10.1007/s00371-019-01668-9
   Marini S, 2011, VISUAL COMPUT, V27, P1005, DOI 10.1007/s00371-011-0612-9
   Mitliagkas I., 2017, Learning representations and generative models for 3d point clouds
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Qi CR, 2017, ADV NEUR IN, V30
   Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863
   Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20
   Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481
   Smith Edward J., 2017, P MACH LEARN RES, V78, P87
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Thrun S, 2005, IEEE I CONF COMP VIS, P1824
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
NR 27
TC 3
Z9 3
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4373
EP 4382
DI 10.1007/s00371-021-02301-4
EA OCT 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000704958400001
DA 2024-07-18
ER

PT J
AU Tulsulkar, G
   Mishra, N
   Thalmann, NM
   Lim, HE
   Lee, MP
   Cheng, SK
AF Tulsulkar, Gauri
   Mishra, Nidhi
   Thalmann, Nadia Magnenat
   Lim, Hwee Er
   Lee, Mei Ping
   Cheng, Siok Khoong
TI Can a humanoid social robot stimulate the interactivity of cognitively
   impaired elderly? A thorough study based on computer vision methods
SO VISUAL COMPUTER
LA English
DT Article
DE Social Assistive Robotics (SAR); Robot companions; Social robots; Social
   intelligence for Robots; Human-humanoid interaction; Computer vision;
   Observational scales
ID OLDER-ADULTS; DEMENTIA SYMPTOMS; COMPANION ROBOT; NURSING-HOMES; CARE;
   SEAL; THERAPY; PEOPLE; ENGAGEMENT; RESIDENTS
AB Social Assistive Robotics is increasingly being used in care settings to provide psychosocial support and interventions for the elderly with cognitive impairments. Most of these social robots have provided timely stimuli to the elderly at home and in care centres, including keeping them active and boosting their mood. However, previous investigations have registered shortcomings in these robots, particularly in their ability to satisfy an essential human need: the need for companionship. Reports show that the elderly tend to lose interests in these social robots after the initial excitement as the novelty wears out and the monotonous familiarity becomes all too familiar. This paper presents our research facilitating conversations between a social humanoid robot, Nadine, and cognitively impaired elderly at a nursing home. We analysed the effectiveness of human-humanoid interactions between our robot and 14 elderly over 29 sessions. We used both objective tools (based on computer vision methods) and subjective tools (based on observational scales) to evaluate the recorded videos. Our findings showed that our subjects engaged positively with Nadine, suggesting that their interaction with the robot could improve their well-being by compensating for some of their emotional, cognitive, and psychosocial deficiencies. We detected emotions associated with cognitively impaired elderly during these interactions. This study could help understand the expectations of the elderly and the current limitations of Social Assistive Robots. Our research is aligned with all the ethical recommendations by the NTU Institutional Review Board.
C1 [Tulsulkar, Gauri; Mishra, Nidhi; Thalmann, Nadia Magnenat] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
   [Thalmann, Nadia Magnenat] Univ Geneva, MIRALab, Geneva, Switzerland.
   [Lim, Hwee Er; Lee, Mei Ping] Goshen Consultancy Serv Pte Ltd, Singapore, Singapore.
   [Cheng, Siok Khoong] Bright Hill Evergreen Home, Singapore, Singapore.
C3 Nanyang Technological University; University of Geneva
RP Tulsulkar, G (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
EM gauri.rt09@gmail.com; nidhimishra2906@gmail.com;
   nadia.thalmann@miralab.ch; hweeer@consultgoshen.com;
   meiping@consultgoshen.com; siokkhoong.cheng@bheh.org
RI Thalmann, Nadia/AAK-5195-2021; Mishra, Nidhi/AAK-1794-2020
OI Thalmann, Nadia/0000-0002-1459-5960; 
FU National Research Foundation, Singapore, under its International
   Research Centres in Singapore Funding Initiative; Institute for Media
   Innovation, Nanyang Technological University (IMI-NTU)
FX This research is partly supported by the National Research Foundation,
   Singapore, under its International Research Centres in Singapore Funding
   Initiative, and Institute for Media Innovation, Nanyang Technological
   University (IMI-NTU). Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not reflect the views of National Research Foundation, Singapore.
   We are very thankful to the institutional review board (IRB) for their
   guidelines and support in ethical issues. Special thanks to Chai Kooi
   Foong, Linda Lim, and Sau Wai from Bright Hill Evergreen Home for their
   continuous support during this research. We would like to thank our
   colleagues Tham Yiep Soon for his strong technical support during the
   study and Ashwini Lawate for her help in pre-processing the video
   material.
CR Abdollahi H, 2017, 2017 IEEE-RAS 17TH INTERNATIONAL CONFERENCE ON HUMANOID ROBOTICS (HUMANOIDS), P541, DOI 10.1109/HUMANOIDS.2017.8246925
   Agrigoroaie R, 2016, LECT NOTES ARTIF INT, V9979, P735, DOI 10.1007/978-3-319-47437-3_72
   Alves-Oliveira P, 2015, LECT NOTES ARTIF INT, V9388, P11, DOI 10.1007/978-3-319-25554-5_2
   Ananto R.A., 2020, ROBOT PETS EVERYONE
   Bajones M, 2020, ACM T HUM-ROBOT INTE, V9, DOI 10.1145/3368554
   Baka E, 2019, LECT NOTES COMPUT SC, V11542, P240, DOI 10.1007/978-3-030-22514-8_20
   Banks MR, 2008, J AM MED DIR ASSOC, V9, P173, DOI 10.1016/j.jamda.2007.11.007
   Barata A., 2015, INT J ROBOT ENG, V2, P002
   Bates Danika Passler, 2020, HAI '20: Proceedings of the 8th International Conference on Human-Agent Interaction, P260, DOI 10.1145/3406499.3418772
   Beck A., 2016, CONTEXT AWARE HUMAN, P237, DOI DOI 10.1007/978-3-319-19947-411
   Bradwell HL, 2020, BMC GERIATR, V20, DOI 10.1186/s12877-020-01641-5
   Bradwell HL, 2019, BMJ OPEN, V9, DOI 10.1136/bmjopen-2019-032468
   Broekens Joost, 2009, Gerontechnology, V8, P94, DOI 10.4017/gt.2009.08.02.002.00
   Carros F, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376402
   Céspedes N, 2020, IEEE T NEUR SYS REH, V28, P1299, DOI 10.1109/TNSRE.2020.2987428
   Chang WL, 2014, 2014 IEEE WORKSHOP ON ADVANCED ROBOTICS AND ITS SOCIAL IMPACTS (ARSO), P32, DOI 10.1109/ARSO.2014.7020976
   Chang WL, 2013, ACMIEEE INT CONF HUM, P101, DOI 10.1109/HRI.2013.6483521
   Chen K, 2020, J AM MED DIR ASSOC, V21, P1724, DOI 10.1016/j.jamda.2020.05.036
   Chu MT, 2017, ASSIST TECHNOL, V29, P8, DOI 10.1080/10400435.2016.1171807
   Cohen-Mansfield J, 2009, AM J GERIAT PSYCHIAT, V17, P299, DOI 10.1097/JGP.0b013e31818f3a52
   Cosar S, 2020, INT J SOC ROBOT, V12, P779, DOI 10.1007/s12369-019-00614-y
   Cruz-Sandoval D, 2020, ACMIEEE INT CONF HUM, P161, DOI 10.1145/3319502.3374840
   Cylkowska-Nowak M, 2015, INT MULTIDDISCIP SCI, P1007
   De Carolis B, 2017, MULTIMED TOOLS APPL, V76, P5073, DOI 10.1007/s11042-016-3797-0
   Eyal N., 2014, Hooked: How to Build Habit-Forming Products
   Fasola J, 2010, 2010 IEEE RO-MAN, P416, DOI 10.1109/ROMAN.2010.5598658
   Ferland F., 2016, RO MAN WORKSH BEH AD
   Fernaeus Y, 2010, 9TH INTERNATIONAL CONFERENCE ON INTERACTION DESIGN AND CHILDREN (IDC2010), P39
   Fischer K, 2011, AI MAG, V32, P31, DOI 10.1609/aimag.v32i4.2377
   Fischinger D, 2016, ROBOT AUTON SYST, V75, P60, DOI 10.1016/j.robot.2014.09.029
   Giusti L., 2006, ROMAN 2006 15 IEEE I, P111, DOI [10.1109/ROMAN.2006.314403, DOI 10.1109/ROMAN.2006.314403]
   Guiot Denis., 2019, 28th IEEE RO-MAN Internet Of Intelligent Robotic Things For Healthy Living and Active Ageing, International Conference on Robot Human Interactive Communication, P23
   Hamada T, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P174, DOI 10.1109/ROMAN.2008.4600662
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henschel A, 2020, TRENDS NEUROSCI, V43, P373, DOI 10.1016/j.tins.2020.03.013
   Homes S., 2006, SMART HOMES ICOST, V2006, P4
   Huisman C, 2019, HEALTHCARE-BASEL, V7, DOI 10.3390/healthcare7010031
   Judge K.S., 2000, AM J ALZHEIMERS DIS, V15, P42, DOI [DOI 10.1177/153331750001500105, https://doi.org/10.1177/153331750001500105]
   Karunarathne D, 2019, INT J SOC ROBOT, V11, P343, DOI 10.1007/s12369-018-0503-6
   Ke C, 2020, INT J MED INFORM, V141, DOI 10.1016/j.ijmedinf.2020.104241
   Khosla R., 2013, ACM Transactions on Management Information Systems (TMIS), V4, P18, DOI DOI 10.1145/2544104
   Kim GH, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123251
   Klamer T, 2011, L N INST COMP SCI SO, V59, P74
   Lane GW, 2016, PSYCHOL SERV, V13, P292, DOI 10.1037/ser0000080
   Law M, 2019, BMJ OPEN, V9, DOI 10.1136/bmjopen-2019-031937
   Lawton M., 1999, J MENTAL HLTH AGING, V5, P69
   Lee HR, 2016, IEEE ROMAN, P312, DOI 10.1109/ROMAN.2016.7745148
   Li J, 2016, 2016 IEEE INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS (IRIS), P109, DOI 10.1109/IRIS.2016.8066075
   Libin Alexander, 2004, Am J Alzheimers Dis Other Demen, V19, P111, DOI 10.1177/153331750401900209
   Lohse M., 2008, DOMESTIC APPL SOCIAL
   Ludwiczuk B., 2016, Tech. Rep. CMU-CS-16-118
   Magnenat-Thalmann N, 2014, INT C DIGITAL HOME, P1, DOI 10.1109/ICDH.2014.8
   Martinez-Martin E, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9020367
   Melkas H, 2020, INT J MED INFORM, V134, DOI 10.1016/j.ijmedinf.2019.104041
   Miseikis J, 2020, IEEE ROBOT AUTOM LET, V5, P5339, DOI 10.1109/LRA.2020.3007462
   Mishra NK, 2019, 2019 INTERNATIONAL CONFERENCE ON POWER ELECTRONICS, CONTROL AND AUTOMATION (ICPECA-2019), P592, DOI [10.1109/SUMMA48161.2019.8947500, 10.1109/icpeca47973.2019.8975605]
   Moharana S, 2019, ACMIEEE INT CONF HUM, P458, DOI [10.1109/HRI.2019.8673206, 10.1109/hri.2019.8673206]
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Moyle W, 2017, J AM MED DIR ASSOC, V18, P766, DOI 10.1016/j.jamda.2017.03.018
   Moyle W, 2016, INT J SOC ROBOT, V8, P145, DOI 10.1007/s12369-015-0326-7
   Neven L, 2010, SOCIOL HEALTH ILL, V32, P335, DOI 10.1111/j.1467-9566.2009.01218.x
   Nishio T, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.633045
   Obayashi K, 2020, GERIATR GERONTOL INT, V20, P373, DOI 10.1111/ggi.13890
   Panou M., 2015, USE CASES OPTIMISING
   Perugia G, 2018, AM J ALZHEIMERS DIS, V33, P112, DOI 10.1177/1533317517739700
   Pigini L., 2012, Technol. Disabil., V24, P303, DOI [DOI 10.3233/TAD-120361, 10.3233/TAD-120361]
   Pineau J, 2003, ROBOT AUTON SYST, V42, P271, DOI 10.1016/S0921-8890(02)00381-0
   Polak RF, 2020, ACMIEEE INT CONF HUM, P151, DOI 10.1145/3319502.3374797
   Pollack M.E., 2002, AAAI WORKSH AUT ELD
   Pripfl J, 2016, ACMIEEE INT CONF HUM, P497, DOI 10.1109/HRI.2016.7451824
   Pu LH, 2019, GERONTOLOGIST, V59, pE37, DOI 10.1093/geront/gny046
   Ramanathan M, 2019, LECT NOTES COMPUT SC, V11542, P490, DOI 10.1007/978-3-030-22514-8_49
   Robinson H, 2013, J AM MED DIR ASSOC, V14, P661, DOI 10.1016/j.jamda.2013.02.007
   Rouaix N, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00950
   Rudzicz F., 2014, Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies, P20
   Sabanovic Selma, 2013, IEEE Int Conf Rehabil Robot, V2013, P6650427, DOI 10.1109/ICORR.2013.6650427
   Sabanovic S, 2016, AI SOC, V31, P537, DOI 10.1007/s00146-015-0636-1
   Sabelli AM, 2011, ACMIEEE INT CONF HUM, P37, DOI 10.1145/1957656.1957669
   Sakairi K, 2004, SICE 2004 ANNUAL CONFERENCE, VOLS 1-3, P2092
   Salatino C, 2017, STUD HEALTH TECHNOL, V242, P484, DOI 10.3233/978-1-61499-798-6-484
   Sánchez ML, 2015, LECT NOTES ARTIF INT, V9513, P264, DOI 10.1007/978-3-319-29339-4_22
   Sato M, 2020, ENFERM CLIN, V30, P32, DOI 10.1016/j.enfcli.2019.09.021
   Schüssler S, 2020, JMIR RES PROTOC, V9, DOI 10.2196/14927
   Shen Z., 2016, Proceedings of the Fourth International Conference on Human Agent Interaction, P63, DOI [10.1145/2974804.2980485, DOI 10.1145/2974804.2980485]
   Shibata T, 2012, P IEEE, V100, P2527, DOI 10.1109/JPROC.2012.2200559
   Simoens P, 2018, INT J ADV ROBOT SYST, V15, DOI 10.1177/1729881418759424
   Studies B.C., 2020, MEET STEVIE SOCIAL R
   Tamura T, 2004, J GERONTOL A-BIOL, V59, P83
   Tapus A., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P297
   Tapus A., 2009, IJCAI WORKSH INT SYS
   Tapus A, 2009, INT C REHAB ROBOT, P1077
   Tapus A, 2009, AT-EQUAL 2009: 2009 ECSIS SYMPOSIUM ON ADVANCED TECHNOLOGIES FOR ENHANCED QUALITY OF LIFE: LAB-RS AND ARTIPED 2009, P81, DOI 10.1109/AT-EQUAL.2009.26
   Teed Z., 2020, ECCV 2020
   Thodberg K, 2016, ANTHROZOOS, V29, P107, DOI 10.1080/08927936.2015.1089011
   Thunberg S, 2020, LECT NOTES ARTIF INT, V12483, P616, DOI 10.1007/978-3-030-62056-1_51
   Tisseron S, 2015, INT J SOC ROBOT, V7, P97, DOI 10.1007/s12369-014-0268-5
   Tobis S, 2017, OCCUP THER INT, DOI 10.1155/2017/9592405
   Tsujimura M, 2020, J ENABLING TECHNOL, V14, P201, DOI 10.1108/JET-03-2020-0008
   Tuisku O, 2019, INFORM TECHNOL PEOPL, V32, P47, DOI 10.1108/ITP-06-2018-0277
   Turkle S, 2006, CONNECT SCI, V18, P347, DOI 10.1080/09540090600868912
   Ujjwal KC, 2019, BIOMIMETICS-BASEL, V4, DOI 10.3390/biomimetics4040074
   Vincze M., 2016, P ISR 2016 47 INT S, P1
   Wada K., 2006, RO-MAN 2006: The 15th IEEE International Symposium on Robot and Human Interactive Communication (IEEE Cat No. 06TH8907)
   Wada K, 2006, IEEE INT CONF ROBOT, P3966, DOI 10.1109/ROBOT.2006.1642310
   Wada K, 2007, IEEE T ROBOT, V23, P972, DOI 10.1109/TRO.2007.906261
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Xiao Y, 2014, PRESENCE-VIRTUAL AUG, V23, P133, DOI 10.1162/PRES_a_00176
   Zhang X, 2017, IEEE COMPUT SOC CONF, P2299, DOI 10.1109/CVPRW.2017.284
NR 108
TC 13
Z9 13
U1 8
U2 47
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3019
EP 3038
DI 10.1007/s00371-021-02242-y
EA JUL 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XP1DP
UT WOS:000679625200002
PM 34345091
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Das, DK
   Shit, S
   Ray, DN
   Majumder, S
AF Das, Dibyendu Kumar
   Shit, Sahadeb
   Ray, Dip Narayan
   Majumder, Somajyoti
TI CGAN: closure-guided attention network for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Visual attention; Saliency; Encoder-decoder network; Gestalt;
   Convolution neural network; Multi-level fusion
ID VISUAL-ATTENTION; CONTRAST; MODEL; ROBUST
AB In recent years, salient object detection (SOD) has achieved significant progress with the help of convolution neural network (CNN). Most of the state-of-the-art methods segment the salient object by either aggregating the multilevel features from the CNN module or introducing the refinement module along with the baseline network. However, these models suffer from simplicity bias, where neural networks converge to global minima using the simple feature and remain invariant to complex predictive features. Very few methods concentrate on the neurophysiological behaviour of visual attention. As per Gestalt psychology, humans tend to perceive the objects as a whole rather than focus on the discrete elements of that object. The law of Closure (closed contour) is one of the Gestalt axioms that states that if there is a discontinuity in the object's contour, we perceive the object as continuous in a smooth pattern. This paper proposes a two-way learning network, where Closure-guided Attention Network (CGAN) and the Coarse Saliency Networks (CSN) jointly supervise the feature-channel to mitigate the simplicity bias. Furthermore, a channel-wise attention residual network is incorporated in the Closure Guided module to alleviate the scale-space problem and generate smooth object contour. Finally, the closure map from CGAN fused with the coarse saliency map of the Coarse Saliency Network generates a salient object. Experimental result on five benchmark datasets demonstrates the significant improvements in our approach over the state-of-the-art method.
C1 [Das, Dibyendu Kumar; Shit, Sahadeb; Ray, Dip Narayan] CSIR CMERI, AcSIR, Durgapur, W Bengal, India.
   [Ray, Dip Narayan; Majumder, Somajyoti] CSIR CMERI, Design Management & Syst Engn, Durgapur, W Bengal, India.
C3 Academy of Scientific & Innovative Research (AcSIR); Council of
   Scientific & Industrial Research (CSIR) - India; CSIR - Central
   Mechanical Engineering Research Institute (CMERI); Council of Scientific
   & Industrial Research (CSIR) - India; CSIR - Central Mechanical
   Engineering Research Institute (CMERI)
RP Das, DK (corresponding author), CSIR CMERI, AcSIR, Durgapur, W Bengal, India.
EM dibs06.it@gmail.com
OI Kumar Das, Dibyendu/0000-0002-3328-7883
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2005, Advances in neural information processing systems, DOI DOI 10.5555/2976248.2976268
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Borji A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247706
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   DUNCAN J, 1984, J EXP PSYCHOL GEN, V113, P501, DOI 10.1037/0096-3445.113.4.501
   EGLY R, 1994, J EXP PSYCHOL GEN, V123, P161, DOI 10.1037/0096-3445.123.2.161
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Guo F, 2018, IEEE T CYBERNETICS, V48, P3159, DOI 10.1109/TCYB.2017.2761361
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Hess R.F., 2013, Oxford handbook of perceptual organization
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Ji YZ, 2019, NEUROCOMPUTING, V323, P188, DOI 10.1016/j.neucom.2018.09.081
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Koffka Kurt, 2013, PRINCIPLES GESTALT P
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li, 2009, VISUAL SALIENCY BASE
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Lu SJ, 2014, IEEE T PATTERN ANAL, V36, P195, DOI 10.1109/TPAMI.2013.158
   Marini F, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00446
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506
   Ni WP, 2018, IEEE GEOSCI REMOTE S, V15, P1392, DOI 10.1109/LGRS.2018.2838151
   Niu J, 2017, MULTIMED TOOLS APPL, V76, P10427, DOI 10.1007/s11042-016-3430-2
   Niu YZ, 2018, IEEE ACCESS, V6, P56170, DOI 10.1109/ACCESS.2018.2873022
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ramachandran P., 2019, STAND ALONE SELF ATT
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shi YJ, 2015, VISUAL COMPUT, V31, P1191, DOI 10.1007/s00371-014-1005-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang Q, 2013, IEEE T CIRC SYST VID, V23, P1150, DOI 10.1109/TCSVT.2012.2226528
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wei, 2019, ABS191111445 ARXIV
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu X., 2018, ECCV WORKSH
   Yang HJ, 2019, KNOWL-BASED SYST, V164, P21, DOI 10.1016/j.knosys.2018.09.033
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang Q, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107484
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou, 2016, MATH PROBL ENG, V2016, P1
   Zhou Q, 2014, ELECTRON LETT, V50, P997, DOI 10.1049/el.2014.0903
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 68
TC 13
Z9 13
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3803
EP 3817
DI 10.1007/s00371-021-02222-2
EA JUL 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000673137400001
DA 2024-07-18
ER

PT J
AU Grosso, R
   Zint, D
AF Grosso, Roberto
   Zint, Daniel
TI A parallel dual marching cubes approach to quad only surface
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Dual marching cubes; Quad meshing; Parallel mesh reconstruction; GPU
   programming
ID TOPOLOGICALLY CORRECT; MESH
AB We present a novel method that reconstructs surfaces from volume data using a dual marching cubes approach without lookup tables. The method generates quad only meshes which are consistent across cell borders, i.e., they are manifold and watertight. Vertices are positioned exactly on the reconstructed surface almost everywhere, leading to higher accuracy than other reconstruction methods. A halfedge data structure is used for storing the meshes which is convenient for further processing. The method processes elements in parallel and therefore runs efficiently on GPU. Due to the transition between layers in volume data, meshes have numerous vertices with valence three. We use simplification patterns for eliminating quads containing these vertices wherever possible which reduces the number of elements and increases quality. We briefly describe a CUDA implementation of our method, which allows processing huge amounts of data on GPU at almost interactive time rates. Finally, we present runtime and quality results of our method on medical and synthetic data sets.
C1 [Grosso, Roberto; Zint, Daniel] Friedrich Alexander Univ Erlangen Nurnberg, Visual Comp, Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Zint, D (corresponding author), Friedrich Alexander Univ Erlangen Nurnberg, Visual Comp, Erlangen, Germany.
EM roberto.grosso@fau.de; daniel.zint@fau.de
RI Zint, Daniel/JNT-1002-2023
OI Zint, Daniel/0000-0003-4491-1685; Grosso, Roberto/0000-0001-5965-5325
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR [Anonymous], 2021, NTR
   [Anonymous], 2017, NTR
   [Anonymous], 2020, NTR
   [Anonymous], 2020, REG APPR AUT QUAD ME, DOI [10.5281/zenodo.3653416, DOI 10.5281/ZENODO.3653416]
   Attali D., 2005, P 3 EUR S GEOM PROC
   Canann SA, 1998, ENG COMPUT-GERMANY, V14, P168, DOI 10.1007/BF01213591
   Chen JJ, 2015, VISUAL COMPUT, V31, P119, DOI 10.1007/s00371-014-0924-7
   Chernyaev E., 1995, Technical report
   Cignoni P, 2000, COMPUT GRAPH-UK, V24, P399, DOI 10.1016/S0097-8493(00)00036-4
   Custodio L, 2013, COMPUT GRAPH-UK, V37, P840, DOI 10.1016/j.cag.2013.04.004
   D'Agostino D, 2015, J COMPUT APPL MATH, V273, P383, DOI 10.1016/j.cam.2014.05.019
   de Bruin PW, 2000, LECT NOTES COMPUT SC, V1935, P804
   Dupuy G, 2010, COMPUT AIDED DESIGN, V42, P129, DOI 10.1016/j.cad.2009.04.016
   Durst MartinJ., 1988, ACM SIGGRAPH Computer Graphics, V22, P243, DOI DOI 10.1145/378267.378271
   Etiene T, 2012, IEEE T VIS COMPUT GR, V18, P952, DOI 10.1109/TVCG.2011.109
   Freitag L, 1999, SIAM J SCI COMPUT, V20, P2023, DOI 10.1137/S1064827597323208
   Gibson SFF, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P23, DOI 10.1109/SVV.1998.729581
   Gibson SFF, 1998, LECT NOTES COMPUT SC, V1496, P888, DOI 10.1007/BFb0056277
   Grosso R., 2016, TMC
   Grosso R, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095179
   Grosso R, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P102, DOI 10.5220/0008948701020112
   Grosso R, 2016, COMPUT GRAPH FORUM, V35, P187, DOI 10.1111/cgf.12975
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kazhdan M., 2007, Proceedings of the fth Eurographics symposium on Geometry processing, P125
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Loffler F., 2012, VMV
   Lopes A, 2003, IEEE T VIS COMPUT GR, V9, P16, DOI 10.1109/TVCG.2003.1175094
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Matveyev SV, 1999, P SOC PHOTO-OPT INS, V3643, P220, DOI 10.1117/12.342838
   Montani C., 1994, Visual Computer, V10, P353, DOI 10.1007/BF01900830
   NATARAJAN BK, 1994, VISUAL COMPUT, V11, P52, DOI 10.1007/BF01900699
   Nielson GM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P489, DOI 10.1109/VISUAL.2004.28
   Nielson GM, 2003, IEEE T VIS COMPUT GR, V9, P283, DOI 10.1109/TVCG.2003.1207437
   NIELSON GM, 1991, VISUALIZATION 91, P83
   PASKO AA, 1988, COMPUT GRAPH, V12, P457, DOI 10.1016/0097-8493(88)90070-2
   Rangarajan R, 2017, SIAM J SCI COMPUT, V39, pA2438, DOI 10.1137/16M1089101
   Rashid T, 2016, PROCEDIA ENGINEER, V163, P136, DOI 10.1016/j.proeng.2016.11.037
   Remacle JF, 2012, INT J NUMER METH ENG, V89, P1102, DOI 10.1002/nme.3279
   Schaefer S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2004.1348336
   Schaefer S, 2007, IEEE T VIS COMPUT GR, V13, P610, DOI 10.1109/TVCG.2007.1012
   Stimpson C., 2007, VERDICT LIBRARY REFE, V9
   Tarini M, 2010, COMPUT GRAPH FORUM, V29, P407, DOI 10.1111/j.1467-8659.2009.01610.x
   Ulrich C, 2014, P WSCG, P361
   Xia RB, 2005, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY - PROCEEDINGS, P565, DOI 10.1109/CIT.2005.44
NR 44
TC 7
Z9 7
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1301
EP 1316
DI 10.1007/s00371-021-02139-w
EA JUN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000658239900001
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Gupta, N
   Garg, H
   Agarwal, R
AF Gupta, Neeraj
   Garg, Hitendra
   Agarwal, Rohit
TI A robust framework for glaucoma detection using CLAHE and EfficientNet
SO VISUAL COMPUTER
LA English
DT Article
DE Glaucoma; Convolutional neural network (CNN); Deep learning;
   EfficientNet; CLAHE; U-Net
ID OPTIC CUP SEGMENTATION; DISC
AB Glaucoma disease is affecting a large community worldwide. It gradually affects the optic nerve and may cause partial or complete vision loss. Glaucoma happens due to an increase in the fluid pressure inside the optic nerve, which is also known as intraocular pressure (IOP). Therefore, it is essential to detect it in the early stage to prevent blindness. Recently, deep neural networks have been applied to analyse medical imagery. This paper proposes a framework for glaucoma detection using the deep convolution neural network. In this framework, a preprocessing step uses the CLAHE to enhance the local contrast. Further, we have utilized two segmentation models (EfficientNet + U-Net) for segmenting the optic cup and disc mask from retinal fundus images. Moreover, the CDR ratio is computed from the segmented optic cup and disc masks. The framework detects whether the inputted image is glaucoma infected or not based on the CDR ratio. The accuracy of the proposed framework is compared to various baseline models. A qualitative and quantitative assessment has been done on various benchmark datasets (DRISHTI-GS1 and RIM-ONE). The experimental outcomes illustrate that the proposed framework outperformed the other state-of-the-art methods for glaucoma detection in the retinal fundus image.
C1 [Gupta, Neeraj; Garg, Hitendra; Agarwal, Rohit] GLA Univ, Dept Comp Engn & Applicat, Mathura, Uttar Pradesh, India.
C3 GLA University
RP Agarwal, R (corresponding author), GLA Univ, Dept Comp Engn & Applicat, Mathura, Uttar Pradesh, India.
EM neeraj.gupta@gla.ac.in; hitendra.garg@gmail.com; rohit.agrwal@gla.ac.in
RI Garg, Hitendra/AAV-6756-2020; Gupta, Neeraj/AAQ-6313-2021
OI Garg, Dr. Hitendra/0000-0002-4273-2328
CR Abdullah M, 2016, PEERJ, V4, DOI 10.7717/peerj.2003
   Al-Bander B, 2017, INT MULTICONF SYST, P207, DOI 10.1109/SSD.2017.8166974
   Almazroa A, 2017, CLIN OPHTHALMOL, V11, P841, DOI 10.2147/OPTH.S117157
   Arnay R, 2017, APPL SOFT COMPUT, V52, P409, DOI 10.1016/j.asoc.2016.10.026
   Bharkad S, 2017, BIOMED SIGNAL PROCES, V31, P483, DOI 10.1016/j.bspc.2016.09.009
   Chen XY, 2015, IEEE ENG MED BIO, P715, DOI 10.1109/EMBC.2015.7318462
   Chen XY, 2015, LECT NOTES COMPUT SC, V9351, P669, DOI 10.1007/978-3-319-24574-4_80
   Cheng J, 2013, IEEE T MED IMAGING, V32, P1019, DOI 10.1109/TMI.2013.2247770
   Chrástek R, 2005, P SOC PHOTO-OPT INS, V5747, P1604, DOI 10.1117/12.594492
   Christopher M, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35044-9
   Fu HZ, 2018, IEEE T MED IMAGING, V37, P2493, DOI 10.1109/TMI.2018.2837012
   Fumero F, 2011, COMP MED SY
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Joshi GD, 2010, I S BIOMED IMAGING, P948, DOI 10.1109/ISBI.2010.5490144
   Juneja M, 2020, MULTIMED TOOLS APPL, V79, P15531, DOI 10.1007/s11042-019-7460-4
   Kande GB, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P535
   Kumar Singh Law, 2019, 2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS), P397, DOI 10.1109/ICCCIS48478.2019.8974539
   Li HQ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P394
   Li ZX, 2018, OPHTHALMOLOGY, V125, P1199, DOI 10.1016/j.ophtha.2018.01.023
   Liu SP, 2011, J MED BIOL ENG, V31, P405, DOI 10.5405/jmbe.773
   Lowell J, 2004, IEEE T MED IMAGING, V23, P256, DOI 10.1109/TMI.2003.823261
   Lu SJ, 2011, IEEE T MED IMAGING, V30, P2126, DOI 10.1109/TMI.2011.2164261
   Mehdizadeh M., 2009, Research Journal of Biological Sciences, V4, P922
   Pallawala PMDS, 2004, LECT NOTES COMPUT SC, V3022, P139
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Quigley HA, 2006, BRIT J OPHTHALMOL, V90, P262, DOI 10.1136/bjo.2005.081224
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sevastopolsky A., 2017, Pattern Recognition and Image Analysis, V27, P618, DOI 10.1134/S1054661817030269
   Shibata N, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-33013-w
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh LK, 2021, MED BIOL ENG COMPUT, V59, P333, DOI 10.1007/s11517-020-02307-5
   Singh LK, 2020, INT J INF SYST MODEL, V11, P37, DOI 10.4018/IJISMD.2020010103
   Sivaswamy J., 2015, JSM Biomedical Imaging Data Papers, V2, P1004
   Sivaswamy J, 2014, I S BIOMED IMAGING, P53, DOI 10.1109/ISBI.2014.6867807
   Soorya M, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1260-2
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tessier-Lavigne Marc., 2000, Principles of Neural Science, V4th, P507
   Thakur N, 2017, CURR MED IMAGING REV, V13, P99, DOI 10.2174/1573405612666160606124044
   Tobin KW, 2006, PROC SPIE, V6144, DOI 10.1117/12.641670
   Walter T, 2002, IEEE T MED IMAGING, V21, P1236, DOI 10.1109/TMI.2002.806290
   Wong DWK, 2008, IEEE ENG MED BIO, P2266, DOI 10.1109/IEMBS.2008.4649648
   Zahoor MN, 2017, IEEE ACCESS, V5, P12293, DOI 10.1109/ACCESS.2017.2723320
   Zhu XL, 2010, J DIGIT IMAGING, V23, P332, DOI 10.1007/s10278-009-9189-5
   Zilly Julian G., 2015, Machine Learning in Medical Imaging. 6th International Workshop, MLMI 2015, held in conjunction with MICCAI 2015. Proceedings: LNCS 9352, P136, DOI 10.1007/978-3-319-24888-2_17
   Zilly J, 2017, COMPUT MED IMAG GRAP, V55, P28, DOI 10.1016/j.compmedimag.2016.07.012
NR 47
TC 16
Z9 17
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2315
EP 2328
DI 10.1007/s00371-021-02114-5
EA MAY 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000647506500001
DA 2024-07-18
ER

PT J
AU Yang, MM
   Ma, TH
   Tian, Q
   Tian, Y
   Al-Dhelaan, A
   Al-Dhelaan, M
AF Yang, Mingming
   Ma, Tinghuai
   Tian, Qing
   Tian, Yuan
   Al-Dhelaan, Abdullah
   Al-Dhelaan, Mohammed
TI Aggregated squeeze-and-excitation transformations for densely connected
   convolutional networks
SO VISUAL COMPUTER
LA English
DT Article
DE Image classification; Attention mechanism; Residual learning; Aggregated
   transformations
AB Recently, convolutional neural networks (CNNs) have achieved great success in computer vision, but suffer from parameter redundancy in large-scale networks. DenseNet is a typical CNN architecture, which connects each layer to every other layer to maximize feature reuse and network efficiency, but it can become parametrically expensive with the potential risk of overfitting in deep networks. To address these problems, we propose a lightweight Densely Connected and Inter-Sparse Convolutional Networks with aggregated Squeeze-and-Excitation transformations (DenisNet-SE) in this paper. First, Squeeze-and-Excitation (SE) blocks are introduced in different locations of the dense model to adaptively recalibrate channel-wise feature responses. Meanwhile, we propose the Squeeze-Excitation-Residual (SERE) block, which applies residual learning to construct identity mapping. Second, to construct the densely connected and inter-sparse structure, we further apply the sparse three-layer bottleneck layer and grouped convolutions, which increase the cardinality of transformations. Our proposed network is evaluated on three highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, and ImageNet) and achieves better performance than the state-of-the-art networks while requiring fewer parameters.
C1 [Yang, Mingming; Ma, Tinghuai; Tian, Qing] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Tian, Yuan] Nanjing Inst Technol, Nanjing 211167, Jiangsu, Peoples R China.
   [Al-Dhelaan, Abdullah; Al-Dhelaan, Mohammed] KingSaud Univ, Dept Comp Sci, Riyadh 11362, Saudi Arabia.
C3 Nanjing University of Information Science & Technology; Nanjing
   Institute of Technology
RP Ma, TH (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
EM thma@nuist.edu.cn; tianqing@nuist.edu.cn; ytian@nit.edu.cn;
   dhelaan@ksu.edu.sa; mdhelaan@ksu.edu.sa
RI tian, qing/JMQ-8820-2023; Tian, Yuan/AFV-8924-2022; , 马廷淮/AAL-3878-2020;
   Al-Dhelaan, Abdullah M/B-4159-2015
OI , 马廷淮/0000-0003-2320-1692; 
FU National key research and development program (International Technology
   Cooperation Project) [2019YFE0130700]; Deanship of Scientific Research
   at King Saud University [RGP-264]
FX This work was supported in part by National key research and development
   program (International Technology Cooperation Project) (No.
   2019YFE0130700). The authors extend their appreciation to the Deanship
   of Scientific Research at King Saud University for funding this work
   through research group no. RGP-264.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Alipour N, 2020, MULTIMED TOOLS APPL, V79, P8249, DOI 10.1007/s11042-019-08597-8
   [Anonymous], 2016, ARXIVABS160306765
   Bao WT, 2020, IEEE T IMAGE PROCESS, V29, P2753, DOI 10.1109/TIP.2019.2952201
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Collobert R., 2002, Torch: a modular machine learning software library
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hao SJ, 2020, NEUROCOMPUTING, V406, P302, DOI 10.1016/j.neucom.2019.11.118
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Y., 2018, ARXIVABS180708920
   Huang, P 11 EUROPEAN C COMP
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaderberg M, 2015, ADV NEUR IN, V28
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Son LH, 2019, IEEE ACCESS, V7, P23319, DOI 10.1109/ACCESS.2019.2899260
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Liu YQ, 2022, VEHICLE SYST DYN, V60, P2686, DOI 10.1080/00423114.2021.1918728
   Ma TH, 2021, NEUROCOMPUTING, V447, P224, DOI 10.1016/j.neucom.2021.03.055
   Ma TH, 2021, NEUROCOMPUTING, V423, P639, DOI 10.1016/j.neucom.2020.10.060
   Ma TH, 2022, IEEE T AFFECT COMPUT, V13, P60, DOI 10.1109/TAFFC.2019.2932061
   Ma TH, 2020, FUTURE GENER COMP SY, V105, P533, DOI 10.1016/j.future.2019.12.022
   Ma TH, 2019, EXPERT SYST APPL, V115, P346, DOI 10.1016/j.eswa.2018.08.010
   Ma TH, 2018, APPL SOFT COMPUT, V66, P525, DOI 10.1016/j.asoc.2017.08.027
   Ma TH, 2018, NEUROCOMPUTING, V296, P33, DOI 10.1016/j.neucom.2018.03.029
   Meng M, 2020, IEEE T IMAGE PROCESS, V29, P186, DOI 10.1109/TIP.2019.2926774
   Pérez AF, 2020, IEEE WINT CONF APPL, P2843, DOI 10.1109/WACV45572.2020.9093307
   Rong H, 2019, INFORM SCIENCES, V488, P158, DOI 10.1016/j.ins.2019.03.023
   Sehovac L, 2020, IEEE ACCESS, V8, P36411, DOI 10.1109/ACCESS.2020.2975738
   Shen YH, 2020, NEURAL COMPUT APPL, V32, P9495, DOI 10.1007/s00521-019-04462-9
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava R. K., 2015, ADV NEURAL INFORM PR, P2377, DOI DOI 10.48550/ARXIV.1505.00387
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tong M, 2020, NEURAL COMPUT APPL, V32, P5285, DOI 10.1007/s00521-019-04030-1
   Tong M, 2019, NEUROCOMPUTING, V325, P90, DOI 10.1016/j.neucom.2018.09.086
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang H, 2020, IEEE T MED IMAGING, V39, P1306, DOI 10.1109/TMI.2019.2948026
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu CY, 2019, IEEE ACCESS, V7, P183604, DOI 10.1109/ACCESS.2019.2960315
   Zagoruyko, 2016, P BRIT MACH VIS C
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
NR 52
TC 4
Z9 5
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2661
EP 2674
DI 10.1007/s00371-021-02144-z
EA MAY 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000646101500001
DA 2024-07-18
ER

PT J
AU Yu, J
   Zhao, J
   Shao, HL
   Lu, Y
   He, YJ
   Zhang, LJ
AF Yu, Jun
   Zhao, Jing
   Shao, Huili
   Lu, Yu
   He, Yongjun
   Zhang, Lejun
TI Illumination compensation for microscope images based on illumination
   difference estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Illumination difference; Image segmentation; Background filling;
   Illumination compensation
AB The DNA ploidy analysis which measures the relative content of DNA in cells by image processing has a wide range of applications in cancer diagnosis. However, the measured results of the same cell in different positions are different because of uneven illumination, which may reduce the measurement accuracy and the diagnosis performance. Many methods are proposed to compensate for uneven illumination, but they are generally aimed at image enhancement, segmentation, or recognition and therefore not suitable for cell measurement. To solve this problem, a compensation method without using white-referencing images is proposed in this paper. This method first grabs images with cells and then removes the cells after locating them on the slide by image segmentation. Next, the regions of removed cells are filled by the thin-plate spline interpolation to obtain background images. Then, two methods used for estimating illumination difference from the background images are provided. Finally, the illumination compensation is made by adding the input image and the illumination difference image. Experiments show that the methods proposed can remove uneven illumination without using white-referencing images.
C1 [Yu, Jun; Zhao, Jing; Shao, Huili; Lu, Yu; He, Yongjun] Harbin Univ Sci & Technol, Sch Comp Sci & Technol, Harbin 150080, Peoples R China.
   [Zhang, Lejun] Yangzhou Univ, Coll Informat Engn, Yangzhou 225009, Jiangsu, Peoples R China.
C3 Harbin University of Science & Technology; Yangzhou University
RP Zhang, LJ (corresponding author), Yangzhou Univ, Coll Informat Engn, Yangzhou 225009, Jiangsu, Peoples R China.
EM zhanglejun@yzu.edu.cn
RI zhang, lejun/IST-9774-2023
OI He, Yongjun/0000-0002-5156-651X; zhang, lejun/0000-0002-3458-7431
FU National Natural Science Foundation of China [61673142]; Foundation of
   Education Department of Heilongjiang Province [12511096]; Natural
   Science Foundation of HeiLongjiang Province of China [JJ2019JQ0013,
   F2017013]; University Nursing Program for Young Scholars with Creative
   Talents in Heilongjiang Province [UNPYSCT-2016034]; Outstanding Youth
   Talent Foundation of Harbin of China [2017RAYXJ013]; Research Fund for
   the Doctoral Program of Higher Education of China [20132303120003];
   Science Funds for the Young Innovative Talents of HUST [20152]
FX This research is partly supported by The National Natural Science
   Foundation of China (61673142), the Foundation of Education Department
   of Heilongjiang Province (12511096), Natural Science Foundation of
   HeiLongjiang Province of China (F2017013), Natural Science Foundation of
   HeiLongjiang Province of China (JJ2019JQ0013), University Nursing
   Program for Young Scholars with Creative Talents in Heilongjiang
   Province (UNPYSCT-2016034), OutstandingYouthTalent Foundation of Harbin
   of China (2017RAYXJ013), and the Research Fund for the Doctoral Program
   of Higher Education of China (20132303120003) and the Science Funds for
   the Young Innovative Talents of HUST (20152).
CR [Anonymous], 2001, CURRENT PROTOCOLS CY
   Chen XL, 2016, IEEE T ANTENN PROPAG, V64, P4482, DOI 10.1109/TAP.2016.2587743
   Coster AD, 2014, NAT METHODS, V11, P602, DOI 10.1038/nmeth.2971
   Goldman DB, 2010, IEEE T PATTERN ANAL, V32, P2276, DOI 10.1109/TPAMI.2010.55
   Kang S. B., 2000, PROC EUR C COMPUT VI, P640, DOI [10.1007/3-540-45053-X_41, DOI 10.1007/3-540-45053-X_41]
   Leong FJWM, 2003, J CLIN PATHOL, V56, P619, DOI 10.1136/jcp.56.8.619
   McNamara George, 2017, Curr Protoc Hum Genet, V94, DOI 10.1002/cphg.42
   Model, 2014, CURR PROTOC CYTOM, V10, P1
   Model M, 2014, Curr. Protoc. Cytom., V68, P10
   Ng CA., 2016, DIGIT IMAGE COMPUT T, V1, P1
   Peng TY, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14836
   Piccinini F, 2012, J MICROSC-OXFORD, V248, P6, DOI 10.1111/j.1365-2818.2012.03645.x
   Poon SSS, 2008, CYTOM PART A, V73A, P904, DOI 10.1002/cyto.a.20624
   Schindelin J, 2012, NAT METHODS, V9, P676, DOI [10.1038/NMETH.2019, 10.1038/nmeth.2019]
   SCHULTZ ML, 1974, J HISTOCHEM CYTOCHEM, V22, P751, DOI 10.1177/22.7.751
   Singh S, 2014, J MICROSC-OXFORD, V256, P231, DOI 10.1111/jmi.12178
   Smith K, 2015, NAT METHODS, V12, P404, DOI [10.1038/NMETH.3323, 10.1038/nmeth.3323]
   Varga VS, 2004, CYTOM PART A, V60A, P53, DOI 10.1002/cyto.a.20027
   Wolf DE, 2007, METHOD CELL BIOL, V81, P365, DOI 10.1016/S0091-679X(06)81017-4
NR 19
TC 3
Z9 3
U1 3
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1775
EP 1786
DI 10.1007/s00371-021-02104-7
EA MAR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000633745000001
DA 2024-07-18
ER

PT J
AU Yang, F
   Zhang, Q
AF Yang, Fei
   Zhang, Qian
TI Depth aware image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Depth aware; Generative adversarial network; Feature
   fusion
AB Image dehazing aims to remove the haze noise and restore the image content from hazy images. It is a challenging task because of the unbalanced distribution of the haze noise and the variety of the image contents. Most existing methods apply convolutional neural networks to learn the dehazing process by blind end-to-end training, which relies on the noise distribution of the training datasets. Indeed, the distribution of the haze noise is much related to the image depth, which indicates the distances of the scenes from the viewer. In this work, based on the structure of conventional generative adversarial network, we propose a depth aware method to estimate the depth maps and provide the depth features for dehazing within one joint framework. By fusing the depth feature to the dehazing network, the dehazing model is able to better separate various extent of haze against the image content. The experiments demonstrate that the proposed depth aware module can significantly improve the performance of the dehazing model and is able to be implemented into traditional CNN-based dehazing models conveniently.
C1 [Yang, Fei] Univ Nottingham Ningbo China, Int Doctoral Innovat Ctr, Taikang East Rd 199, Ningbo, Peoples R China.
   [Zhang, Qian] Univ Nottingham Ningbo China, Sch Comp Sci, Taikang East Rd 199, Ningbo, Peoples R China.
C3 University of Nottingham Ningbo China; University of Nottingham Ningbo
   China
RP Zhang, Q (corresponding author), Univ Nottingham Ningbo China, Sch Comp Sci, Taikang East Rd 199, Ningbo, Peoples R China.
EM fei.yang@nottingham.edu.cn; qian.zhang@nottingham.edu.cn
FU International Doctoral Innovation Centre; Ningbo Education Bureau;
   Ningbo Science and Technology Bureau; University of Nottingham; UK
   Engineering and Physical Sciences Research Council [EP/L015463/1];
   National Natural Science Foundation of China [72071116]; Science and
   Technology Innovation 2025 Major Project, Ningbo Science and Technology
   Bureau [2019B10026]
FX The author acknowledges the financial support from the International
   Doctoral Innovation Centre, Ningbo Education Bureau, Ningbo Science and
   Technology Bureau, and the University of Nottingham. This work was also
   supported by the UK Engineering and Physical Sciences Research Council
   [grant number EP/L015463/1]. We are grateful for accessing to
   theUniversity ofNottinghamNingbo China High Performance Computing
   Facility.; This work is part of the project of `Model and Data Driven
   HyperHeuristics for Combinatorial Optimisation and Their Applications in
   Port Operation Integrated Scheduling', supported by the National Natural
   Science Foundation of China [Project code: 72071116]. It is also part of
   the project of `KeyTechnological Enhancement and Applications for Ningbo
   Port Terminal Operating System', supported by the Science and Technology
   Innovation 2025 Major Project, Ningbo Science and Technology Bureau
   [Project code: 2019B10026].
CR Alhashim I., 2018, High quality monocular depth estimation via transfer learning
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Chen WT, 2019, PROC CVPR IEEE, P11673, DOI 10.1109/CVPR.2019.01195
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Das B, 2022, VISUAL COMPUT, V38, P179, DOI 10.1007/s00371-020-02010-4
   Fan X, 2019, VISUAL COMPUT, V35, P565, DOI 10.1007/s00371-018-1485-y
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Guo TT, 2020, INT CONF ACOUST SPEE, P8891, DOI [10.1109/ICASSP40776.2020.9054504, 10.1109/icassp40776.2020.9054504]
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li R., 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00856
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shu X., 2020, HOST PARASITE GRAPH
   Shu X., 2019, SPATIOTEMPORAL COATT
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Sulami M, 2014, IEEE INT CONF COMPUT
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang JW, 2010, VISUAL COMPUT, V26, P761, DOI 10.1007/s00371-010-0444-z
   Zhang Q, 2018, AS C COMP VIS ACCV
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 44
TC 17
Z9 17
U1 8
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1579
EP 1587
DI 10.1007/s00371-021-02089-3
EA MAR 2021
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000630631000001
DA 2024-07-18
ER

PT J
AU Fan, JC
   Chen, M
   Mo, JQ
   Wang, SG
   Liang, QH
AF Fan, Jiacheng
   Chen, Min
   Mo, Jinqiu
   Wang, Shigang
   Liang, Qinghua
TI Variational formulation of a hybrid perspective shape from shading model
SO VISUAL COMPUTER
LA English
DT Article
DE Shape from shading; Variational formulation; Cook&#8211; Torrance BRDF
   reflectance model; Featureless 3D reconstruction
AB Over a long period of time, it is challenging to solve the completed shape from shading (SFS) problem which has no limitations on light position and surface roughness. Many existing researches developed Hamilton-Jacobi equation (HJE)-based method of such SFS problem. However, HJE-based methods have one major weakness of boundary condition requirement. Finding a proper algorithm for calculating direct depth in a completed SFS model is desirable. In this paper, a completed hybrid perspective shape from shading model based on Cook-Torrance BRDF reflectance model is established. The depth is expressed and calculated directly on the Cartesian coordinates rather than spherical coordinates, which prevents exponential explosion risks caused by substitution of natural exponential. A direct variational formulation method is employed to solve the complex partial differential equation (PDE) of the image irradiance model. The major contribution of the method is that depth is considered as the only variable to deduce iterative equation. After discretizing the partial derivative terms of the iterative equation, the variational method could be programed conveniently and intuitively. Simulations and experiments demonstrate the accuracy and robustness of the method.
C1 [Fan, Jiacheng; Chen, Min; Mo, Jinqiu; Wang, Shigang; Liang, Qinghua] Shanghai Jiao Tong Univ, Sch Mech Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University
RP Liang, QH (corresponding author), Shanghai Jiao Tong Univ, Sch Mech Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
EM qhliang@sjtu.edu.cn
RI Wang, Shigang/AAF-5686-2020
FU National Key R&D Program of China [2017YFB1302901]
FX This study was funded by National Key R&D Program of China (No.
   2017YFB1302901).
CR Ahmed A, 2007, 2007 IEEE INT C IM P, V2, pII
   Ahmed A.H., 2007, IEEE Conference on Computer Vision and Pattern Recognition, P1
   [Anonymous], 1989, Shape from shading
   Bruhn A, 2013, P INT C SCAL SPAC VA, P222, DOI DOI 10.1007/978-3-642-38267-3_19
   Dacorogna B., 2007, Direct Methods in the Calculus of Variations, Vvol 78
   Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003
   Horn B K P, 1970, Tech. Rep.
   HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3
   IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0
   Jin YR, 2021, MEASUREMENT, V173, DOI 10.1016/j.measurement.2020.108500
   Kelemen C., 2001, EUROGRAPHICS SHORT P, V2, P4
   Lee KM, 1997, COMPUT VIS IMAGE UND, V67, P143, DOI 10.1006/cviu.1997.0522
   Lei L, 2016, ACTA OTO-LARYNGOL, V136, P1190, DOI 10.1080/00016489.2016.1193895
   Malti A, 2014, IEEE T BIO-MED ENG, V61, P1684, DOI 10.1109/TBME.2014.2300237
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Maurer D, 2018, INT J COMPUT VISION, V126, P1342, DOI 10.1007/s11263-018-1079-1
   Nie XY, 2020, VISUAL COMPUT, V36, P1247, DOI 10.1007/s00371-019-01735-1
   Prados E, 2005, PROC CVPR IEEE, P870
   Prasath VBS, 2012, IEEE ENG MED BIO, P4014, DOI 10.1109/EMBC.2012.6346847
   Qin CJ, 2021, MECH SYST SIGNAL PR, V151, DOI 10.1016/j.ymssp.2020.107386
   Ron G., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P350, DOI 10.1109/CVPR.1989.37871
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Wang C, 2017, VISUAL COMPUT, V33, P1211, DOI 10.1007/s00371-016-1284-2
   Wang GH, 2016, OPTIK, V127, P7740, DOI 10.1016/j.ijleo.2016.05.120
   Wang GH, 2009, OPT ENG, V48, DOI 10.1117/1.3257283
   Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987
   Wu CY, 2010, INT J COMPUT VISION, V86, P211, DOI 10.1007/s11263-009-0207-3
   Yong Chul Ju, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P538, DOI 10.1007/978-3-319-18461-6_43
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 30
TC 4
Z9 4
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1469
EP 1482
DI 10.1007/s00371-021-02081-x
EA MAR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000626807900001
DA 2024-07-18
ER

PT J
AU Rolet, A
   Seguy, V
AF Rolet, Antoine
   Seguy, Vivien
TI Fast optimal transport regularized projection and application to
   coefficient shrinkage and filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Optimal transport; Wasserstein distance; Coefficient shrinkage; Sparse
   decomposition; Wavelet thresholding; Denoising
ID ALGORITHM; ADAPTATION
AB This paper explores solutions to the problem of regularized projections with respect to the optimal transport metric. Expanding recent works on optimal transport dictionary learning and non-negative matrix factorization, we derive general purpose algorithms for projecting on any set of vectors with any regularization, and we further propose fast algorithms for the special cases of projecting onto invertible or orthonormal bases. Noting that pass filters and coefficient shrinkage can be seen as regularized projections under the Euclidean metric, we show how to use our algorithms to perform optimal transport pass filters and coefficient shrinkage. We give experimental evidence that using the optimal transport distance instead of the Euclidean distance for filtering and coefficient shrinkage leads to reduced artifacts and improved denoising results.
C1 [Rolet, Antoine] Kyoto Univ, Grad Sch Informat, Yoshida Honmachi, Kyoto, Japan.
   [Seguy, Vivien] Nomad AI OU, Tallinn, Estonia.
C3 Kyoto University
RP Rolet, A (corresponding author), Kyoto Univ, Grad Sch Informat, Yoshida Honmachi, Kyoto, Japan.
EM antoine.rolet@iip.ist.i.kyoto-u.ac.jp
OI Rolet, Antoine/0000-0002-3712-7481
FU JSPS KAKENHI [17H01788]; Grants-in-Aid for Scientific Research
   [17H01788] Funding Source: KAKEN
FX This work was partly supported by JSPS KAKENHI Grant Number 17H01788.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Ataee Z, 2020, VISUAL COMPUT, V36, P1679, DOI 10.1007/s00371-019-01766-8
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BEYLKIN G, 1991, COMMUN PUR APPL MATH, V44, P141, DOI 10.1002/cpa.3160440202
   Blondel M., 2018, INT C ARTIFICIAL INT, P880
   Cazelles E, 2018, SIAM J SCI COMPUT, V40, pB429, DOI 10.1137/17M1143459
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633
   COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502
   Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600
   Cuturi M, 2014, PR MACH LEARN RES, V32, P685
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Dehda B, 2017, J APPL MATH COMPUT, V16, P55, DOI 10.17512/jamcm.2017.2.05
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626
   Emiya V., 2016, P ADV NEUR INF PROC, P703
   Feydy J., 2019, 22 INT C ART INT STA, P2681
   Frogner C., 2015, Advances in Neural Information Processing Systems, P2053
   Gramfort A, 2015, Inf Process Med Imaging, V24, P261, DOI 10.1007/978-3-319-19992-4_20
   Kaur Lakhwinder., 2002, ICVGIP, V2, P16
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lorenz DA, 2015, J MATH IMAGING VIS, V51, P311, DOI 10.1007/s10851-014-0523-2
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Orlin JB, 1997, MATH PROGRAM, V78, P109, DOI 10.1007/BF02614365
   Peyré G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073
   Rabin Julien, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P256, DOI 10.1007/978-3-319-18461-6_21
   Redko I, 2019, PR MACH LEARN RES, V89, P849
   Rolet A, 2018, EURASIP J ADV SIG PR, DOI 10.1186/s13634-018-0576-2
   Rolet A, 2016, JMLR WORKSH CONF PRO, V51, P630
   Sandler R, 2009, PROC CVPR IEEE, P1873, DOI 10.1109/CVPRW.2009.5206834
   Seguy V., 2018, ICLR
   Seguy V., 2015, ADV NEURAL INFORM PR, V2, P3312
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Tartavel G, 2016, SIAM J IMAGING SCI, V9, P1726, DOI 10.1137/16M1067494
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 40
TC 2
Z9 2
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 477
EP 491
DI 10.1007/s00371-020-02029-7
EA FEB 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000615564300001
DA 2024-07-18
ER

PT J
AU Peng, YP
   Ge, JH
   Qin, H
   Ge, XJ
   Xiao, CY
AF Peng, Yiping
   Ge, Junhui
   Qin, Hui
   Ge, Xiaojun
   Xiao, Changyan
TI A corner-clustering method for detection of slab management numbers
   sprayed on steel slabs
SO VISUAL COMPUTER
LA English
DT Article
DE SMN detection; Computer vision; Steel slabs; DBSCAN cluster; Corner
   detection
ID SCENE TEXT DETECTION; HUMAN FACE; RECOGNITION; IDENTIFICATION;
   SEGMENTATION; IMAGES
AB To achieve manufacturing and logistics informatization management for steelworks, it is of crucial importance to automatically recognize the slab management numbers (SMNs) sprayed on the steel slabs. However, due to the poor quality of spraying and various interferences, SMN detection is a major challenge for subsequent recognition in the steel-slab product line. This paper proposes a corner-clustering method, which can extract the SMN from a changeable background precisely and promptly. In our method, the FAST algorithm is modified to extract the image corners by adaptively adjusting the local threshold of corner detecting with the change of image contrast. Then, the DBSCAN algorithm is implemented to group the corners into several clusters, which includes the SMN regions and interference regions. Finally, a classifier based on HOG features and SVM is applied to discriminate SMN and non-SMN regions. For experimental validation, the proposed method was implemented to a substantial amount of acquired images. A good performance has been achieved as the detection accuracy can reach as high as 98.96% for SMN on the steel slabs.
C1 [Peng, Yiping; Ge, Junhui; Qin, Hui; Ge, Xiaojun; Xiao, Changyan] Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha, Peoples R China.
C3 Hunan University
RP Xiao, CY (corresponding author), Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha, Peoples R China.
EM c.xiao@hnu.edu.cn
CR Astafiev AV, 2016, DYN SYST MECH MACH
   Baskan S, 2002, PATTERN RECOGN LETT, V23, P1623, DOI 10.1016/S0167-8655(02)00037-5
   Chen XR, 2004, PROC CVPR IEEE, P366
   Choi S, 2012, EXPERT SYST APPL, V39, P7621, DOI 10.1016/j.eswa.2012.01.124
   Choi S, 2009, OPT ENG, V48, DOI 10.1117/1.3083340
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fang YJ, 2004, IEEE T VEH TECHNOL, V53, P1679, DOI 10.1109/TVT.2004.834875
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Freeman W. T., 1995, P INT WORKSH AUT FAC, V12, P296
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Hong Hanyu, 2012, 2012 Third International Conference on Digital Manufacturing and Automation (ICDMA), P331, DOI 10.1109/ICDMA.2012.80
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jiang W, 2013, ADV MATER RES-SWITZ, V764, P161, DOI 10.4028/www.scientific.net/AMR.764.161
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Koo G, 2019, ISIJ INT, V59, P98, DOI 10.2355/isijinternational.ISIJINT-2018-506
   Koo HI, 2013, IEEE T IMAGE PROCESS, V22, P2296, DOI 10.1109/TIP.2013.2249082
   Kriegel H.-P., 1996, KNOWLEDGE DISCOVERY, P226, DOI DOI 10.5555/3001460
   Lee SJ, 2019, IEEE ACCESS, V7, P23177, DOI 10.1109/ACCESS.2019.2899109
   Lee SJ, 2018, ISIJ INT, V58, P696, DOI 10.2355/isijinternational.ISIJINT-2017-695
   Lee SJ, 2017, EXPERT SYST APPL, V77, P34, DOI 10.1016/j.eswa.2017.01.026
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin H, 2020, ARCH COMPUT METHOD E, V27, P433, DOI 10.1007/s11831-019-09315-1
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XT, 2016, VISUAL COMPUT, V32, P501, DOI 10.1007/s00371-015-1084-0
   Mosleh A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.63
   Noh S, 2016, IEEE T INTELL TRANSP, V17, P323, DOI 10.1109/TITS.2015.2466652
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rowley HA, 1996, ADV NEUR IN, V8, P875
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wu H, 2017, VISUAL COMPUT, V33, P113, DOI 10.1007/s00371-015-1156-1
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yi CC, 2013, COMPUT VIS IMAGE UND, V117, P182, DOI 10.1016/j.cviu.2012.11.002
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Zhao QJ, 2017, ADV MANUF, V5, P261, DOI 10.1007/s40436-017-0190-9
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
NR 40
TC 0
Z9 0
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 357
EP 369
DI 10.1007/s00371-020-02019-9
EA NOV 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000592624500001
DA 2024-07-18
ER

PT J
AU Chen, C
   Li, CH
   Qi, YN
   Wang, CB
AF Chen, Chen
   Li, Chenhui
   Qi, Yannan
   Wang, Changbo
TI VEFP: visual evaluation of flight procedure in airport terminal
SO VISUAL COMPUTER
LA English
DT Article
DE Airspace planning; Visual analysis; Airport terminal utilization rate;
   Intelligent transportation system
ID VISUALIZATION; TRAJECTORIES; PATTERNS; MOBILITY
AB The optimization of terminal airspace can provide more airport capacity to meet the growing aviation demand. Improving flight procedures is an important prerequisite for optimizing airspace. The existing air-route network visualization cannot fully meet the assessment needs of the terminal flight procedure. In this research, we introduce an analysis tool, VEFP, that provides multiple visualizations based on unlabeled flight trajectory data. The system can help domain experts to evaluate the terminal flight procedure from multiple perspectives, respectively. First, we provide a time series-based statistical information view to help determine the usage status of the flight procedure per unit time. Second, we evaluated the controller's use of space for flight procedures based on the location information in the data. Third, combined with the visualization method after data processing, the visual complexity is reduced and necessary details are displayed. Then, the users can directly observe the actual flight procedure. We evaluate flight procedures in actual use through cases and experiments and then discuss with users about the observations provided by the system. These results confirm that our system can help domain experts evaluate flight procedures.
C1 [Chen, Chen; Li, Chenhui; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Qi, Yannan] Civil Aviat Univ China, Sch Air Traff Management, Tianjin 300300, Peoples R China.
C3 East China Normal University; Civil Aviation University of China
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM starla.cchen@gmail.com; chli@cs.ecnu.edu.cn; yannan.qi@yahoo.com;
   cbwang@cs.ecnu.edu.cn
RI wang, toby/AFU-6392-2022; Li, Chenhui/AAR-3682-2020
OI Li, Chenhui/0000-0001-9835-2650
FU National Science Foundation Project of China [61672237, 61802128]
FX We would like to thank experts in Civil Aviation University of China for
   helpful feedback and accurate datasets. This paper is supported by
   National Science Foundation Project of China (Grant No. 61672237 and No.
   61802128).
CR Andrienko G., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P161, DOI 10.1109/VAST.2011.6102454
   Andrienko G., 2014, CARTOGRAPHY POLE POL, P157
   Andrienko G, 2018, IEEE T VIS COMPUT GR, V24, P34, DOI 10.1109/TVCG.2017.2744322
   Andrienko G, 2017, IEEE T INTELL TRANSP, V18, P2232, DOI 10.1109/TITS.2017.2683539
   Andrienko G, 2013, IEEE T VIS COMPUT GR, V19, P1078, DOI 10.1109/TVCG.2012.311
   Bach B., 2014, EUROVIS
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Buschmann S, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P174, DOI 10.1109/CW.2014.32
   Cai Z, 2019, VISUAL COMPUT, V35, P739, DOI 10.1007/s00371-018-1509-7
   Chen W, 2017, IEEE T VIS COMPUT GR, V99, P1
   Chen W, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2436897
   Cherniavsky E.A., 2000, Aviation System Performance Metrics
   Demsar U, 2010, INT J GEOGR INF SCI, V24, P1527, DOI 10.1080/13658816.2010.511223
   Doraiswamy H, 2014, IEEE T VIS COMPUT GR, V20, P2634, DOI 10.1109/TVCG.2014.2346449
   Ferstl F, 2017, IEEE T VIS COMPUT GR, V23, P831, DOI 10.1109/TVCG.2016.2598868
   Holten D, 2009, COMPUT GRAPH FORUM, V28, P983, DOI 10.1111/j.1467-8659.2009.01450.x
   Hong S, 2017, IEEE PAC VIS SYMP, P81, DOI 10.1109/PACIFICVIS.2017.8031582
   Hurter C., 2016, DIG AV SYST C DASC, P1, DOI [10.1109/DASC.2016.7777947, DOI 10.1109/DASC.2016.7777947]
   Hurter C, 2009, IEEE T VIS COMPUT GR, V15, P1017, DOI 10.1109/TVCG.2009.145
   Johansson J, 2016, IEEE T VIS COMPUT GR, V22, P579, DOI 10.1109/TVCG.2015.2466992
   Klein T, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P104
   Lhuillier A, 2017, IEEE PAC VIS SYMP, P190, DOI 10.1109/PACIFICVIS.2017.8031594
   Li XM, 2016, ADV ENG SOFTW, V93, P1, DOI 10.1016/j.advengsoft.2015.11.003
   Li Y, 2016, IEEE PHOTONICS J, V8, DOI 10.1109/JPHOT.2016.2619063
   Liu RC, 2016, IEEE PAC VIS SYMP, P96, DOI 10.1109/PACIFICVIS.2016.7465256
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Lu HP, 2015, DISCRETE DYN NAT SOC, V2015, DOI 10.1155/2015/284906
   Ma YX, 2016, IEEE T INTELL TRANSP, V17, P2627, DOI 10.1109/TITS.2015.2498187
   Mahboubi Z, 2017, LEARNING TRAFFIC PAT
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nguyen M. K., 2017, IEEE T POWER ELECTR, P1
   Organization I.C.A, 2008, PERF BAS NAV PBN MAN
   Peysakhovich V, 2015, IEEE PAC VIS SYMP, P39, DOI 10.1109/PACIFICVIS.2015.7156354
   Saini R, 2019, VISUAL COMPUT, V35, P415, DOI 10.1007/s00371-018-1473-2
   Salvador S, 2004, Fastdtw: Toward accurate dynamic time warping in linear time and space
   Scheepens R, 2016, IEEE T VIS COMPUT GR, V22, P379, DOI 10.1109/TVCG.2015.2467112
   Senaratne H., 2017, IEEE T INTELL TRANSP
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Vuckovic A, 2013, HUM FACTORS, V55, P946, DOI 10.1177/0018720813481803
   Wang F, 2017, IEEE T INTELL TRANSP, V18, P2250, DOI 10.1109/TITS.2017.2711644
   Wang ZC, 2013, IEEE T VIS COMPUT GR, V19, P2159, DOI 10.1109/TVCG.2013.228
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Zheng XH, 2016, IEEE T INTELL TRANSP, V17, P620, DOI 10.1109/TITS.2015.2480157
NR 43
TC 1
Z9 1
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2139
EP 2155
DI 10.1007/s00371-020-01975-6
EA OCT 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000574329900001
DA 2024-07-18
ER

PT J
AU Jia, JQ
   Elezovikj, S
   Fan, H
   Yang, SJ
   Liu, J
   Guo, W
   Tan, CC
   Ling, HB
AF Jia, Jianqing
   Elezovikj, Semir
   Fan, Heng
   Yang, Shuojin
   Liu, Jing
   Guo, Wei
   Tan, Chiu C.
   Ling, Haibin
TI Semantic-aware label placement for augmented reality in street view
SO VISUAL COMPUTER
LA English
DT Article
DE Label placement; Augmented reality; Guidance map; Street view;
   Image-based layout
ID MANAGEMENT; ANNOTATIONS
AB In an augmented reality (AR) application, placing labels in a manner that is clear and readable without occluding the critical information from the real world can be a challenging problem. This paper introduces a label placement technique for AR used in street view scenarios. We propose a semantic-aware task-specific label placement method by identifying potentially important image regions through a novel feature map, which we refer to asguidance map. Given an input image, its saliency information, semantic information and the task-specific importance prior are integrated in the guidance map for our labeling task. To learn the task prior, we created a label placement dataset with the users' labeling preferences, as well as use it for evaluation. Our solution encodes the constraints for placing labels in an optimization problem to obtain the final label layout, and the labels will be placed in appropriate positions to reduce the chances of overlaying important real-world objects in street view AR scenarios. The experimental validation shows clearly the benefits of our method over previous solutions in the AR street view navigation and similar applications.
C1 [Jia, Jianqing; Yang, Shuojin; Liu, Jing; Guo, Wei] Hebei Normal Univ, Coll Math & Informat Sci, Key Lab Augmented Real, Shijiazhuang 050024, Hebei, Peoples R China.
   [Elezovikj, Semir; Fan, Heng; Tan, Chiu C.] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
   [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Hebei Normal University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Temple University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook
RP Guo, W (corresponding author), Hebei Normal Univ, Coll Math & Informat Sci, Key Lab Augmented Real, Shijiazhuang 050024, Hebei, Peoples R China.; Ling, HB (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM guowei@chmiot.net; hling@cs.stonybrook.edu
RI Jia, Jianqing/KVB-4237-2024
OI Jia, Jianqing/0000-0001-5008-3391
FU National Natural Science Foundation of China [61802109, 61902109]
FX This study was funded by the National Natural Science Foundation of
   China under Grants 61802109 and 61902109.
CR [Anonymous], 2008, PROC 6 INT S NONPHOT
   [Anonymous], 2009, P IEEE INT C COMP VI
   Azuma R, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P66, DOI 10.1109/ISMAR.2003.1240689
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bell B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/502348.502363
   Carmigniani J, 2011, MULTIMED TOOLS APPL, V51, P341, DOI 10.1007/s11042-010-0660-6
   Chen L.-C., 2017, IEEE C COMP VIS PATT
   Cmolík L, 2010, COMPUT GRAPH-UK, V34, P378, DOI 10.1016/j.cag.2010.05.002
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Grasset R, 2012, INT SYM MIX AUGMENT, P177, DOI 10.1109/ISMAR.2012.6402555
   Hartmann K, 2005, LECT NOTES COMPUT SC, V3638, P115
   Ichihashi K, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040939
   Iwai D, 2013, IEEE T VIS COMPUT GR, V19, P1415, DOI 10.1109/TVCG.2012.321
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jianqing Jia, 2018, 2018 IEEE 4th International Conference on Computer and Communications (ICCC). Proceedings, P1654, DOI 10.1109/CompComm.2018.8780965
   Lauber F., 2013, P IEEE INT S MIX AUG, P1
   Leykin A, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P224, DOI 10.1109/ISMAR.2004.22
   Lhuillier A, 2019, VISUAL COMPUT, V35, P1041, DOI 10.1007/s00371-019-01686-7
   Li G., 2017, P 29 AUSTR C COMP HU, P266, DOI DOI 10.1145/3152771.3152800
   Li G, 2018, PROCEEDINGS OF THE 16TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2018), DOI 10.1145/3284398.3284422
   Makita K, 2009, IEEE INT CON MULTI, P982, DOI 10.1109/ICME.2009.5202661
   Orlosky J, 2013, INT SYM MIX AUGMENT, P281, DOI 10.1109/ISMAR.2013.6671805
   Prince CN, 2009, PROCEEDINGS OF THE ASME SUMMER BIOENGINEERING CONFERENCE - 2009, PT A AND B, P437
   Rakholia N, 2018, IEEE IMAGE PROC, P604, DOI 10.1109/ICIP.2018.8451052
   Rosten E, 2005, LECT NOTES COMPUT SC, V3804, P294
   Sato M, 2014, J VISUAL LANG COMPUT, V25, P891, DOI 10.1016/j.jvlc.2014.10.030
   Tanaka K, 2008, INT SYM MIX AUGMENT, P139, DOI 10.1109/ISMAR.2008.4637340
   Tatzgern M, 2014, 2014 IEEE VIRTUAL REALITY (VR), P27, DOI 10.1109/VR.2014.6802046
   Tatzgern M, 2013, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2013.6549347
   Tenmoku R, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P192
   Vollick I, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P221
   Wang W., 2019, ARXIV190409146
   Zhang B., 2010, P 18 SIGSPATIAL INT, P260
   Zheng JB, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON BIG KNOWLEDGE (IEEE ICBK 2017), P320, DOI 10.1109/ICBK.2017.58
NR 34
TC 3
Z9 4
U1 6
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1805
EP 1819
DI 10.1007/s00371-020-01939-w
EA AUG 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000554423100001
DA 2024-07-18
ER

PT J
AU Kaddah, W
   Elbouz, M
   Ouerhani, Y
   Alfalou, A
   Desthieux, M
AF Kaddah, Wissam
   Elbouz, Marwa
   Ouerhani, Yousri
   Alfalou, Ayman
   Desthieux, Marc
TI Automatic darkest filament detection (ADFD): a new algorithm for crack
   extraction on two-dimensional pavement images
SO VISUAL COMPUTER
LA English
DT Article
DE Segmentation; Unsupervised method; Road crack detection; Performance
   assessment; 2D and 3D pavement images; Image processing
AB Pavement condition information is a significant component in pavement management systems. Precise extraction of road degradations particularly cracks is a critical task for surface safety. Manual surveys, which are labor intensive and costly, have induced several researchers to investigate the use of image processing to achieve automated pavement distress ratings. In the context of fine structures extraction, we present in this paper a novel approach for road crack detection under real conditions using several systems installed differently on a vehicle. It is such an automatic and effective approach that relies on both photometric and geometric characteristics of cracks. Based on an edge detection technique to avoid the bad conditions of image acquisition and an examination algorithm to verify the presence of high concentration of cracking pixels, this approach allows in a first step to select pixels that have great probability of belonging to a crack. Indeed, the originality of this approach stems from the proposed way to compute a set of thin filaments connecting the pixels selected at the first step between them. Finally, a post-processing step is applied to refine the obtained result and confirm either the presence or the absence of cracks in the image. Our proposed approach provides very robust and precise results on 2D pavement images in a wide range of situations and in a fully unsupervised manner. Furthermore, its innovative aspect is reflected in its ability to analyze easily both 2D and 3D pavement images.
C1 [Kaddah, Wissam; Elbouz, Marwa; Alfalou, Ayman] Yncrea Ouest, L Bisen, AIDE Lab, 20 Rue Cuirasse Bretagne, F-29200 Brest, France.
   [Kaddah, Wissam; Ouerhani, Yousri; Desthieux, Marc] Actris, F-29803 Brest 9, France.
RP Kaddah, W (corresponding author), Yncrea Ouest, L Bisen, AIDE Lab, 20 Rue Cuirasse Bretagne, F-29200 Brest, France.; Kaddah, W (corresponding author), Actris, F-29803 Brest 9, France.
EM wissam.kaddah@isen-ouest.yncrea.fr
OI Kaddah, Wissam/0000-0001-9014-3053
CR Amhaz R, 2016, IEEE T INTELL TRANSP, V17, P2718, DOI 10.1109/TITS.2015.2477675
   [Anonymous], THESIS
   [Anonymous], MACH VISION APPL
   [Anonymous], P IEEE INT JOINT C N
   [Anonymous], VIS COMPUTER
   [Anonymous], 2006, SIGN PROC C 2006 14
   Chambon S, IMAGE DATABASE AVAIL
   Coudray N., 2010, INT JOINT C COMP VIS
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Fernández C, 2017, J ADV TRANSPORT, DOI 10.1155/2017/7090549
   HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941
   Kaddah W, 2016, OPT COMMUN, V371, P117, DOI 10.1016/j.optcom.2016.03.065
   Kaddah W, 2018, THESIS
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Li HF, 2019, IEEE T INTELL TRANSP, V20, P2025, DOI 10.1109/TITS.2018.2856928
   MALLOT HA, 1991, BIOL CYBERN, V64, P177, DOI 10.1007/BF00201978
   McRobbie S.G., 2004, TTS INITIAL REV REV
   Muad AM, 2004, TENCON IEEE REGION, pA207
   Nieto M, 2007, 2007 IEEE INTELLIGENT VEHICLES SYMPOSIUM, VOLS 1-3, P862
   Oliveira Henrique, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P622
   Oliveira H, 2013, IEEE T INTELL TRANSP, V14, P155, DOI 10.1109/TITS.2012.2208630
   Ross Arun, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1221
   Ryu SK, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/968361
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Subirats P, 2006, THESIS
   Sultani W, 2018, IEEE T INTELL TRANSP, V19, P2076, DOI 10.1109/TITS.2017.2728680
   Tang JS, 2013, IEEE SYS MAN CYBERN, P3026, DOI 10.1109/SMC.2013.516
   Tien Sy Nguyen, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P617
   Wang T, 2011, IEEE INT SYMP ELEC, P156, DOI 10.1109/ISEMC.2011.6038302
   Yamaguchi T, 2008, C IND ELECT APPL, P1875, DOI 10.1109/ICIEA.2008.4582845
   Zhang L, 2016, IEEE IMAGE PROC, P3708, DOI 10.1109/ICIP.2016.7533052
   Zou Q, 2019, IEEE T IMAGE PROCESS, V28, P1498, DOI 10.1109/TIP.2018.2878966
   Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004
NR 33
TC 7
Z9 9
U1 1
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1369
EP 1384
DI 10.1007/s00371-019-01742-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000006
DA 2024-07-18
ER

PT J
AU Asad, M
   Yang, J
   He, J
   Shamsolmoali, P
   He, XJ
AF Asad, Mujtaba
   Yang, Jie
   He, Jiang
   Shamsolmoali, Pourya
   He, Xiangjian
TI Multi-frame feature-fusion-based model for violence detection
SO VISUAL COMPUTER
LA English
DT Article
DE Violence detection; Autonomous Video Surveillance; CNN-LSTM; Feature
   fusion; Spatio-temporal features
ID RECOGNITION; HISTOGRAMS
AB Human behavior detection is essential for public safety and monitoring. However, in human-based surveillance systems, it requires continuous human attention and observation, which is a difficult task. Detection of violent human behavior using autonomous surveillance systems is of critical importance for uninterrupted video surveillance. In this paper, we propose a novel method to detect fights or violent actions based on learning both the spatial and temporal features from equally spaced sequential frames of a video. Multi-level features for two sequential frames, extracted from the convolutional neural network's top and bottom layers, are combined using the proposed feature fusion method to take into account the motion information. We also proposed Wide-Dense Residual Block to learn these combined spatial features from the two input frames. These learned features are then concatenated and fed to long short-term memory units for capturing temporal dependencies. The feature fusion method and use of additional wide-dense residual blocks enable the network to learn combined features from the input frames effectively and yields better accuracy results. Experimental results evaluated on four publicly available datasets: HockeyFight, Movies, ViolentFlow and BEHAVE show the superior performance of the proposed model in comparison with the state-of-the-art methods.
C1 [Asad, Mujtaba; Yang, Jie; He, Jiang; Shamsolmoali, Pourya] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
   [He, Xiangjian] Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW, Australia.
C3 Shanghai Jiao Tong University; University of Technology Sydney
RP Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
EM asadmujtaba@sjtu.edu.cn; jieyang@sjtu.edu.cn
RI He, Xiangjian/CAA-1461-2022; Yang, Jie/JCD-9867-2023
OI He, Xiangjian/0000-0001-8962-540X; Asad, Mujtaba/0000-0003-0318-7379
FU NSFC, China [61876107, U1803261]; Committee of Science and Technology,
   Shanghai, China [19510711200]
FX This research is partly supported by NSFC, China (No: 61876107,
   U1803261) and Committee of Science and Technology, Shanghai, China (No.
   19510711200).
CR Asad M, 2019, LECT NOTES COMPUT SC, V11953, P405, DOI 10.1007/978-3-030-36708-4_33
   Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Blunsden S., 2010, ANN BMVA, V4, P4, DOI DOI 10.5465/19416521003654160
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen Ming-Yu., MOSIFT RECOGNIZING H
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Datta A, 2002, INT C PATT RECOG, P433, DOI 10.1109/ICPR.2002.1044748
   de Souza F. D. M., 2010, Proceedings of the 23rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2010), P224, DOI 10.1109/SIBGRAPI.2010.38
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deniz O, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P478
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   DU C, ARXIV181106295
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Farahbakhsh R, 2017, IEEE TRUST BIG, P9, DOI 10.1109/Trustcom/BigDataSE/ICESS.2017.214
   Gao Y, 2016, IMAGE VISION COMPUT, V48-49, P37, DOI 10.1016/j.imavis.2016.01.006
   Giannakopoulos T., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3244, DOI 10.1109/ICPR.2010.793
   Giannakopoulos T, 2006, LECT NOTES COMPUT SC, V3955, P502
   Hanson A., 2018, P EUR C COMP VIS ECC
   Hassner T., 2012, 2012 IEEE COMP SOC C, P1, DOI [DOI 10.1109/CVPRW.2012.6239348, 10.1109/CVPRW.2012.6239348]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kang FG, 2014, INT CONF CARTOGR GIS, P517
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Kumar N, 2020, VISUAL COMPUT, V36, P1809, DOI 10.1007/s00371-019-01777-5
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Liu JX, 2021, VISUAL COMPUT, V37, P359, DOI 10.1007/s00371-020-01804-w
   Long Xu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3538, DOI 10.1109/ICASSP.2014.6854259
   MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872
   Mohammadi S, 2015, 2015 12TH INTERNATIONAL IRANIAN SOCIETY OF CRYPTOLOGY CONFERENCE ON INFORMATION SECURITY AND CRYPTOLOGY (ISCISC), P6, DOI 10.1109/ISCISC.2015.7387890
   Nam J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P353, DOI 10.1109/ICIP.1998.723496
   Rashid M, 2013, VISUAL COMPUT, V29, P1269, DOI 10.1007/s00371-012-0768-y
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Senst T, 2017, IEEE T INF FOREN SEC, V12, P2945, DOI 10.1109/TIFS.2017.2725820
   Gracia IS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120448
   Shi X., 2015, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1506.04214
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sudhakaran S., 2017, 2017 14 IEEE INT C A, P1, DOI DOI 10.1109/AVSS.2017.8078468
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Zagoruyko S., ARXIV160507146
   Zaremba W., Recurrent neural network regularization. arXiv preprint arXiv:1409.2329
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P696, DOI 10.1109/TCSVT.2016.2589858
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 56
TC 35
Z9 37
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1415
EP 1431
DI 10.1007/s00371-020-01878-6
EA JUN 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000543177800001
DA 2024-07-18
ER

PT J
AU Yang, KN
   Wang, CF
   Sarsenbayeva, Z
   Tag, B
   Dingler, T
   Wadley, G
   Goncalves, J
AF Yang, Kangning
   Wang, Chaofan
   Sarsenbayeva, Zhanna
   Tag, Benjamin
   Dingler, Tilman
   Wadley, Greg
   Goncalves, Jorge
TI Benchmarking commercial emotion detection systems using realistic
   distortions of facial image datasets
SO VISUAL COMPUTER
LA English
DT Article
DE Affective computing; Facial emotion recognition; Commercial emotion
   recognition systems; Non-ideal conditions; Validation analysis
ID EXPRESSION RECOGNITION; VALIDATION; FACE; CLASSIFICATION; SET
AB Currently, there are several widely used commercial cloud-based services that attempt to recognize an individual's emotions based on their facial expressions. Most research into facial emotion recognition has used high-resolution, front-oriented, full-face images. However, when images are collected in naturalistic settings (e.g., using smartphone's frontal camera), these images are likely to be far from ideal due to camera positioning, lighting conditions, and camera shake. The impact these conditions have on the accuracy of commercial emotion recognition services has not been studied in full detail. To fill this gap, we selected five prominent commercial emotion recognition systems-Amazon Rekognition, Baidu Research, Face++, Microsoft Azure, and Affectiva-and evaluated their performance via two experiments. In Experiment 1, we compared the systems' accuracy at classifying images drawn from three standardized facial expression databases. In Experiment 2, we first identified several common scenarios (e.g., partially visible face) that can lead to poor-quality pictures during smartphone use, and manipulated the same set of images used in Experiment 1 to simulate these scenarios. We used the manipulated images to again compare the systems' classification performance, finding that the systems varied in how well they handled manipulated images that simulate realistic image distortion. Based on our findings, we offer recommendations for developers and researchers who would like to use commercial facial emotion recognition technologies in their applications.
C1 [Yang, Kangning; Wang, Chaofan; Sarsenbayeva, Zhanna; Tag, Benjamin; Dingler, Tilman; Wadley, Greg; Goncalves, Jorge] Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Vic, Australia.
C3 University of Melbourne
RP Yang, KN (corresponding author), Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Vic, Australia.
EM kangning.yang@student.unimelb.edu.au; jorge.goncalves@unimelb.edu.au
RI Tag, Benjamin/S-6178-2019; Dingler, Tilman/AAU-4851-2021
OI Tag, Benjamin/0000-0002-7831-2632; Dingler, Tilman/0000-0001-6180-7033;
   Goncalves, Jorge/0000-0002-0117-0322; Sarsenbayeva,
   Zhanna/0000-0002-1247-6036; YANG, KANGNING/0000-0002-7106-0022
FU Australian Research Council [DP190102627]
FX This work is supported by the Australian Research Council DP190102627).
CR Albohn DN, 2016, NEUROIMAGING PERSONALITY, SOCIAL COGNITION AND CHARACTER, P159, DOI 10.1016/B978-0-12-800935-2.00008-7
   Alm CO, 2005, P C HUM LANG TECHN E, P579, DOI DOI 10.3115/1220575.1220648
   AlZoubi O, 2009, LECT NOTES ARTIF INT, V5866, P52, DOI 10.1007/978-3-642-10439-8_6
   [Anonymous], 2003, P 2003 C COMP VIS PA, DOI DOI 10.1109/CVPRW.2003.10057
   [Anonymous], 2019, VIS DER IM INS VIA M
   [Anonymous], 2018, P 2018 CHI C HUM FAC
   [Anonymous], 2020, P 2020 CHI C HUM FAC
   [Anonymous], 2000, FACIAL EXPRESSION EM
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 1978, PALO ALTO
   [Anonymous], CASC CLASS TRAIN
   [Anonymous], 2016, ARXIV161006756
   [Anonymous], 2006, 2006 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2006.207
   [Anonymous], 2004, P 42 ANN M ASS COMPU, DOI DOI 10.3115/1218955.1219000
   [Anonymous], 1965, The expression of emotions in man and animal
   [Anonymous], 1983, EMFACS EMOTIONAL FAC
   [Anonymous], 2019, HOM AFF AFF
   [Anonymous], FACIAL ACTION CODING
   [Anonymous], 2019, FAC API FAC REC SOFT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2013, BEHAV MED
   [Anonymous], 2019, AAAIACM C ETHICS SOC, DOI DOI 10.1145/3306618.3314284
   [Anonymous], 2018, ARXIV180508660
   [Anonymous], 2003, UNMASKING FACE GUIDE
   [Anonymous], 2017, IEEE IPCCC, DOI DOI 10.1109/TCYB.2017.2662199
   [Anonymous], 2007, INT J COMPUTER SCI
   [Anonymous], 2017, INT C HUM ASP IT AG
   [Anonymous], 2012, ARXIV12036722
   [Anonymous], 2006, P SPEECH PROS 2006 D
   [Anonymous], 2006, Software testing
   [Anonymous], 1971, The face of emotion
   [Anonymous], 2015, SIGNAL IMAGE PROCESS
   [Anonymous], 2017, 2017 26th international conference on computer communication and networks (ICCCN), DOI [10.1109/ICCCN.2017.8038465, DOI 10.1109/ICCCN.2017.8038465]
   [Anonymous], 2017, 2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO), DOI DOI 10.1109/ARSO.2017.8025197
   [Anonymous], 2015, P INT C ART INT ICAI
   [Anonymous], 2019, AM REK VID IM AWS
   [Anonymous], 2016, CoRR, DOI DOI 10.1109/QOMEX.2016.7498955
   [Anonymous], 2018, P 12 INT WORKSH SEM
   [Anonymous], 2007, P ANN M COGN SCI SOC
   [Anonymous], 2012, P ESOMAR C ATL
   [Anonymous], 2019, EMOTION RECOGNITION
   [Anonymous], 1984, APPROACHES EMOTION
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/d15-1303, 10.18653/v1/D15-1303]
   [Anonymous], 1972, Emotion in the Human Face: guidelines for Research and an Integration of Findings
   BACHMANN T, 1991, European Journal of Cognitive Psychology, V3, P87, DOI 10.1080/09541449108406221
   Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930
   Bernin A, 2017, 10TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2017), P37, DOI 10.1145/3056540.3056546
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Bourel F, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P113, DOI 10.1109/AFGR.2002.1004141
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Carroll JM, 1996, J PERS SOC PSYCHOL, V70, P205, DOI 10.1037/0022-3514.70.2.205
   Celma O, 2010, MUSIC RECOMMENDATION AND DISCOVERY, P43, DOI 10.1007/978-3-642-13287-2_3
   Chen Y, 2015, PSYCHIAT RES, V225, P619, DOI 10.1016/j.psychres.2014.11.035
   Cheng Y, 2014, 2014 TENTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2014), P211, DOI 10.1109/IIH-MSP.2014.59
   Coulson M, 2004, J NONVERBAL BEHAV, V28, P117, DOI 10.1023/B:JONB.0000023655.25550.be
   Dodge S, 2017, IEEE INT CONF COMP V, P2798, DOI 10.1109/ICCVW.2017.329
   Dupre Damien, 2018, 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P627, DOI 10.1109/PERCOMW.2018.8480127
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Ekman P., 1982, HDB METHODS NONVERBA, P45, DOI DOI 10.1017/S0142716400004641
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   FRIDLUND AJ, 1991, BIOL PSYCHOL, V32, P3, DOI 10.1016/0301-0511(91)90003-Y
   Gedraite ES, 2011, ELMAR PROC, P393
   Goeleven E, 2008, COGNITION EMOTION, V22, P1094, DOI 10.1080/02699930701626582
   Goncalves J, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P487, DOI 10.1145/2632048.2636067
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Gross JJ, 1999, COGNITION EMOTION, V13, P551, DOI 10.1080/026999399379186
   Gu Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P537, DOI 10.1145/3240508.3240714
   Gu Yue, 2018, Proc Conf Assoc Comput Linguist Meet, V2018, P2379
   Hou LK, 2013, SIAM J IMAGING SCI, V6, P2213, DOI 10.1137/120888302
   Huang D, 2010, LECT NOTES COMPUT SC, V6312, P364, DOI 10.1007/978-3-642-15552-9_27
   Jack RE, 2012, P NATL ACAD SCI USA, V109, P7241, DOI 10.1073/pnas.1200155109
   Kempe J, 2008, ANN IEEE CONF COMPUT, P211, DOI 10.1109/CCC.2008.6
   Kheradpisheh SR, 2016, SCI REP-UK, V6, DOI 10.1038/srep32672
   Kim Y, 2013, INT CONF ACOUST SPEE, P3687, DOI 10.1109/ICASSP.2013.6638346
   Kowalska M., 2017, HDB COGNITION EMOTIO, P1, DOI [https://doi.org/10.1007/978-3-319-28099-8495-1, DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Le HuyViet., 2016, P 2016 CHI C EXTENDE, P2576
   Lewinski P, 2014, J NEUROSCI PSYCHOL E, V7, P227, DOI 10.1037/npe0000028
   Lewis M., 2010, HDB EMOTIONS
   Ma CL, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON ACTIVE MEDIA TECHNOLOGY (AMT 2005), P546
   Maalej Ahmed, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4129, DOI 10.1109/ICPR.2010.1003
   Mao X, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P113, DOI 10.1109/WIAMIS.2009.5031445
   Matsumoto D., 2008, Scholarpedia, V3, P4237, DOI DOI 10.4249/SCHOLARPEDIA.4237
   Matthews O, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC'18 ADJUNCT), P1144, DOI 10.1145/3267305.3274759
   Nelson NL, 2013, EMOT REV, V5, P8, DOI 10.1177/1754073912457227
   Olszanowski M, 2015, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01516
   Panigrahi SK, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P1771, DOI 10.1109/ICCSP.2016.7754471
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   RUSSELL JA, 1994, PSYCHOL BULL, V115, P102, DOI 10.1037/0033-2909.115.1.102
   Sander D., 2014, OXFORD COMPANION EMO
   Sarsenbayeva Z, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC '17 ADJUNCT), P494, DOI 10.1145/3123024.3124438
   Singh P, 2006, 2006 IFIP INTERNATIONAL CONFERENCE ON WIRELESS AND OPTICAL COMMUNICATIONS NETWORKS, P529
   Stöckli S, 2018, BEHAV RES METHODS, V50, P1446, DOI 10.3758/s13428-017-0996-1
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Towner H, 2007, LECT NOTES COMPUT SC, V4738, P36
   Valstar Michel F., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374
   van Berkel N, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300281
   van der Haar Dustin Terence, 2019, Learning and Collaboration Technologies. Designing Learning Experiences. 6th International Conference, LCT 2019 Held as Part of the 21st HCI International Conference, HCII 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11590), P301, DOI 10.1007/978-3-030-21814-0_23
   van der Schalk J, 2011, EMOTION, V11, P907, DOI 10.1037/a0023853
   Violante MG, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9112218
   Visuri A, 2016, UBICOMP'16 ADJUNCT: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P535, DOI 10.1145/2968219.2968317
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang LG, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3158369
   Zhang Y, 2017, FUTURE GENER COMP SY, V66, P30, DOI 10.1016/j.future.2015.12.001
NR 110
TC 25
Z9 27
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1447
EP 1466
DI 10.1007/s00371-020-01881-x
EA JUN 2020
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SO2LW
UT WOS:000542515600002
DA 2024-07-18
ER

PT J
AU Madhuranga, D
   Madushan, R
   Siriwardane, C
   Gunasekera, K
AF Madhuranga, Danushka
   Madushan, Rivindu
   Siriwardane, Chathuranga
   Gunasekera, Kutila
TI Real-time multimodal ADL recognition using convolution neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Activity recognition; Depth images; Video classification; Data fusion;
   Silhouette extraction
AB Activities of daily living (ADLs) are the activities which humans perform every day of their lives. Walking, sleeping, eating, drinking and sleeping are examples for ADLs. Compared to RGB videos, depth video-based activity recognition is less intrusive and eliminates many privacy concerns, which are crucial for applications such as life-logging and ambient assisted living systems. Existing methods rely on handcrafted features for depth video classification and ignore the importance of audio stream. In this paper, we propose an ADL recognition system that relies on both audio and depth modalities. We propose to adopt popular convolutional neural network (CNN) architectures used for RGB video analysis to classify depth videos. The adaption poses two challenges: (1) depth data are much nosier and (2) our depth dataset is much smaller compared RGB video datasets. To tackle those challenges, we extract silhouettes from depth data prior to model training and alter deep networks to be shallower. As per our knowledge, we used CNN to segment silhouettes from depth images and fused depth data with audio data to recognize ADLs for the first time. We further extended the proposed techniques to build a real-time ADL recognition system.
C1 [Madhuranga, Danushka; Madushan, Rivindu; Siriwardane, Chathuranga; Gunasekera, Kutila] Univ Moratuwa, Dept Comp Sci & Engn, Katubedda, Sri Lanka.
C3 University Moratuwa
RP Madhuranga, D (corresponding author), Univ Moratuwa, Dept Comp Sci & Engn, Katubedda, Sri Lanka.
EM danushka.15@cse.mrt.ac.lk; rivindum.15@cse.mrt.ac.lk;
   chathuranga.15@cse.mrt.ac.lk; kutila@cse.mrt.ac.lk
OI Edirimanna, Danushka/0000-0002-5652-161X
CR [Anonymous], 2015, ARXIVABS150504597
   [Anonymous], 2016, INT C INF COMM TECHN
   [Anonymous], 2016, ARXIVABS160400494 CO
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 2020, WACV 2020 IEEE WINT
   [Anonymous], 2010, J COMPUT, DOI DOI 10.5120/20312-2362
   [Anonymous], MOVO USB COMP LAV MI
   [Anonymous], NOISEREDUCE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2011, P 5 IEEE INT C AUTOM
   [Anonymous], 2014, ARXIVARXIVABS1406219
   [Anonymous], KIN WIND APP DEV
   [Anonymous], 2017, P 2017 IEEE 13 INT C, DOI DOI 10.1109/CSPA.2017.8064918
   [Anonymous], 2017, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615
   [Anonymous], 2010, PRIM SENS NIT 1 3 AL
   [Anonymous], 2014, P 5 S INF COMM TECHN, DOI DOI 10.1145/2676585.2676624
   Arshad S, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON FUTURE NETWORKS AND DISTRIBUTED SYSTEMS (ICFNDS '17), DOI 10.1145/3102304.3102345
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Bingbing Ni, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1147, DOI 10.1109/ICCVW.2011.6130379
   Chen JF, 2005, LECT NOTES COMPUT SC, V3468, P47, DOI 10.1385/1-59259-820-x:047
   Cheng JY, 2016, PERVASIVE MOB COMPUT, V30, P97, DOI 10.1016/j.pmcj.2016.01.007
   Chollet F, 2015, KERAS
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gasparrini S, 2014, SENSORS-BASEL, V14, P2756, DOI 10.3390/s140202756
   Grushin A, 2013, IEEE IJCNN
   Hou JC, 2018, IEEE TETCI, V2, P117, DOI 10.1109/TETCI.2017.2784878
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Kamal S, 2016, J ELECTR ENG TECHNOL, V11, P1857, DOI 10.5370/JEET.2016.11.6.1857
   Mainetti L, 2016, INT CONF SOFTW, P436
   Ordóñez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pieropan A, 2014, IEEE INT C INT ROBOT, P3045, DOI 10.1109/IROS.2014.6942983
   Ronao CA, 2016, EXPERT SYST APPL, V59, P235, DOI 10.1016/j.eswa.2016.04.032
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Simonyan K., 2014, 14091556 ARXIV
   Stork J. A., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P509, DOI 10.1109/ROMAN.2012.6343802
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wang W, 2017, IEEE J SEL AREA COMM, V35, P1118, DOI 10.1109/JSAC.2017.2679658
   Wu QX, 2013, IEEE T SYST MAN CY-S, V43, P875, DOI 10.1109/TSMCA.2012.2226575
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yang X., 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yu YB, 2019, IEEE C EVOL COMPUTAT, P3102, DOI [10.1109/CEC.2019.8789896, 10.1109/cec.2019.8789896]
NR 48
TC 16
Z9 19
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1263
EP 1276
DI 10.1007/s00371-020-01864-y
EA JUN 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539878400001
DA 2024-07-18
ER

PT J
AU Khosravanian, A
   Rahmanimanesh, M
   Keshavarzi, P
   Mozaffari, S
AF Khosravanian, Asieh
   Rahmanimanesh, Mohammad
   Keshavarzi, Parviz
   Mozaffari, Saeed
TI Fuzzy local intensity clustering (FLIC) model for automatic medical
   image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic segmentation; Level set; Medical images; Intensity
   inhomogeneity; Fuzzy C-means clustering
ID SCALABLE FITTING ENERGY; ACTIVE CONTOURS DRIVEN; C-MEANS; LEVEL SET;
   ENTROPY; INFORMATION; ALGORITHM; FCM
AB Intensity inhomogeneity is one of the main challenges in automatic medical image segmentation. In this paper, fuzzy local intensity clustering (FLIC), which is based on the combination of level set algorithm and fuzzy clustering, is proposed to mitigate the effect of intensity variation and noise contamination. For the FLIC method, the segmentation and bias modification are carried out in a fully automatic and simultaneous manner through the local clustering of intensity and selection of the initial contour by the fuzzy method. Besides, the local entropy is integrated into the FLIC function to improve the contour evolution. Experimental results on inhomogeneous medical images indicate the superiority of the FLIC model over the other state-of-the-art segmentation methods in terms of accuracy, robustness, and computational time.
C1 [Khosravanian, Asieh; Rahmanimanesh, Mohammad; Keshavarzi, Parviz; Mozaffari, Saeed] Semnan Univ, Fac Elect & Comp Engn, Semnan, Iran.
C3 Semnan University
RP Rahmanimanesh, M (corresponding author), Semnan Univ, Fac Elect & Comp Engn, Semnan, Iran.
EM a.khosravanian@semnan.ac.ir; rahmanimanesh@semnan.ac.ir;
   pkeshavarzi@semnan.ac.ir; mozaffari@semnan.ac.ir
RI Khosravanian, Asieh/AAV-2819-2021; Keshavarzi, Parviz/M-2641-2017
OI zy, P/0009-0006-2820-8788
CR Ali H, 2018, IEEE T IMAGE PROCESS, V27, P3729, DOI 10.1109/TIP.2018.2825101
   Alipour S, 2014, MACH VISION APPL, V25, P1469, DOI 10.1007/s00138-014-0606-5
   [Anonymous], Simulated Brain Database
   [Anonymous], 2007, 2007 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2007.383014
   [Anonymous], 2012, MICCAI GRAND CHALLEN
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   Cai Q, 2018, PATTERN RECOGN, V82, P79, DOI 10.1016/j.patcog.2018.05.008
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Chack S., 2015, INT J SIGNAL PROCESS, V8, P115, DOI [10.14257/ijsip.2015.8.1.12., DOI 10.14257/IJSIP.2015.8.1.12]
   Chan T.F., 2000, 0014 COMP APPL MATH
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chatterjee A, 2012, ENG APPL ARTIF INTEL, V25, P1698, DOI 10.1016/j.engappai.2012.02.007
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   Chen YF, 2016, PROCEEDINGS OF THE 2016 SAI COMPUTING CONFERENCE (SAI), P259, DOI 10.1109/SAI.2016.7555992
   Dunn J. C., 1973, Journal of Cybernetics, P32, DOI 10.1080/01969727308546046
   Feng CL, 2017, NEUROCOMPUTING, V219, P107, DOI 10.1016/j.neucom.2016.09.008
   George MM, 2017, MAGN RESON IMAGING, V42, P43, DOI 10.1016/j.mri.2017.05.005
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Guo FF, 2016, IET IMAGE PROCESS, V10, P272, DOI 10.1049/iet-ipr.2015.0236
   Han B, 2019, J INDIAN SOC REMOTE, V47, P201, DOI 10.1007/s12524-018-0909-5
   He CJ, 2012, SIGNAL PROCESS, V92, P587, DOI 10.1016/j.sigpro.2011.09.004
   Huang GP, 2018, MAGN RESON IMAGING, V52, P33, DOI 10.1016/j.mri.2018.05.011
   Javaran TA, 2017, VISUAL COMPUT, V33, P151, DOI 10.1007/s00371-015-1166-z
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Lei T., 2019, AUTOMATIC FUZZY CLUS
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Li CY, 2013, IEEE T IMAGE PROCESS, V22, P3296, DOI 10.1109/TIP.2013.2263808
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Li CM, 2011, IEEE T IMAGE PROCESS, V20, P2007, DOI 10.1109/TIP.2011.2146190
   Liu SG, 2012, PATTERN RECOGN, V45, P2769, DOI 10.1016/j.patcog.2011.11.019
   Meziou L., 2012, FRACTIONAL ENTROPY B
   Min H, 2018, PATTERN RECOGN, V73, P15, DOI 10.1016/j.patcog.2017.07.002
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Rastgarpour M, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0068-3
   Sethian J., 1999, LEVEL SET METHODS FA
   SHIOZAKI A, 1986, COMPUT VISION GRAPH, V36, P1, DOI 10.1016/S0734-189X(86)80025-1
   Song Y, 2019, INT J HYPERTHER, V36, P1, DOI 10.1080/02656736.2018.1526416
   Szilágyi L, 2003, P ANN INT IEEE EMBS, V25, P724, DOI 10.1109/IEMBS.2003.1279866
   Wang L, 2018, SIGNAL PROCESS, V149, P27, DOI 10.1016/j.sigpro.2018.02.025
   Wang L, 2018, PATTERN RECOGN, V74, P145, DOI 10.1016/j.patcog.2017.08.031
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Yang Y., 2019, VISUAL COMPUT, P1
   Zhang K., 2013, ARXIV13057053
   Zhang KH, 2016, IEEE T CYBERNETICS, V46, P546, DOI 10.1109/TCYB.2015.2409119
   Zhang Y, 2018, INTL CONF POWER SYST, P2350, DOI 10.1109/POWERCON.2018.8601628
   Zhao ZX, 2014, IET IMAGE PROCESS, V8, P150, DOI 10.1049/iet-ipr.2011.0128
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
   Zong JJ, 2019, COMPUT MATH APPL, V78, P929, DOI 10.1016/j.camwa.2019.03.022
NR 52
TC 16
Z9 16
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1185
EP 1206
DI 10.1007/s00371-020-01861-1
EA JUN 2020
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000537657900001
DA 2024-07-18
ER

PT J
AU Fan, XM
   Pan, GF
   Mao, Y
   He, W
AF Fan, Xinmiao
   Pan, Gaofeng
   Mao, Yan
   He, Wu
TI A personalized traffic simulation integrating emotion using a driving
   simulator
SO VISUAL COMPUTER
LA English
DT Article
DE Traffic simulation; Driving style; Change lanes; Driver model;
   Microscopic model
ID MODEL
AB Since driver's behavior is affected by driving environment and driver's individual characteristics, it is important to understand driver's behavior from these viewpoints. We present a novel driver's behavior simulation that considers both the stable personality of individuals and the changes in driving behavior when the environment changed. In this study, driving styles are classified as aggressive, careful, and anxious. The driving styles are analyzed from the perspectives of personality traits and emotion, and the driving parameters of each driving style are learned through simulator experiments. Given individual personality, traffic conditions, and driving environment as input, our approach can generate traffic flow that reflects driver's personalized driving behavior.
C1 [Fan, Xinmiao; He, Wu] Sichuan Normal Univ, Coll Movie & Media, Chengdu 610068, Peoples R China.
   [Pan, Gaofeng] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Mao, Yan] Sichuan Normal Univ, Coll Business, Chengdu, Sichuan, Peoples R China.
C3 Sichuan Normal University; Beijing Institute of Technology; Sichuan
   Normal University
RP He, W (corresponding author), Sichuan Normal Univ, Coll Movie & Media, Chengdu 610068, Peoples R China.
EM paofeng.Pan.CN@ieee.org; maoyy85@outlook.com; wuhe83@163.com
RI Pan, Gaofeng/AAG-9186-2021; joy, shone/AAR-3728-2020
OI Pan, Gaofeng/0000-0003-1008-5717; 
CR Aljamal MA, 2018, IEEE INT C INTELL TR, P2321, DOI 10.1109/ITSC.2018.8569290
   Basak AE, 2018, COMPUT GRAPH-UK, V72, P70, DOI 10.1016/j.cag.2018.02.004
   Cao MX, 2017, PHYSICA A, V483, P250, DOI 10.1016/j.physa.2017.04.137
   Chao QW, 2013, GRAPH MODELS, V75, P305, DOI 10.1016/j.gmod.2013.07.003
   Clara MM, 2018, IEEE T INTELL TRANSP, V19, P1, DOI [10.1109/TITS.2018.2806498, DOI 10.1109/TITS.2018.2806498]
   Durst Dirk, 2014, LARGE SCALE MULTIMOD
   He W, 2019, COMPUT SCI ENG, V21, P35, DOI 10.1109/MCSE.2018.2873885
   Itkonen TH, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185856
   Kato T, 2011, IEEE INT C INTELL TR, P1217, DOI 10.1109/ITSC.2011.6083113
   Kesting A, 2007, TRANSPORT RES REC, P86, DOI 10.3141/1999-10
   Kesting A, 2010, PHILOS T R SOC A, V368, P4585, DOI 10.1098/rsta.2010.0084
   Kostovasili M, 2017, SIMUL MODEL PRACT TH, V70, P135, DOI 10.1016/j.simpat.2016.10.010
   Lanatà A, 2015, IEEE T INTELL TRANSP, V16, P1505, DOI 10.1109/TITS.2014.2365681
   Lee KW, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18040957
   Li X, 2017, PHYSICA A, V467, P41, DOI 10.1016/j.physa.2016.09.022
   Liu YK, 2018, PROCEEDINGS OF THE FIRST WORKSHOP ON RADICAL AND EXPERIENTIAL SECURITY (RESEC'18), P1, DOI 10.1145/3203422.3203429
   Lu XQ, 2014, COMPUT ANIMAT VIRT W, V25, P363, DOI 10.1002/cav.1575
   Mao Y, 2019, VISUAL COMPUT, V35, P1725, DOI 10.1007/s00371-018-1568-9
   Martin T., 2000, PHYS REV E, V62, P1805, DOI DOI 10.1103/PhysRevE.62.1805
   Mccrae RR, 2003, PERSONALITY ADULTHOO, DOI [10.1007/978-1-4615-0763-5_15, DOI 10.1007/978-1-4615-0763-5_15]
   Melnikov VR, 2016, PROCEDIA COMPUT SCI, V80, P2030, DOI 10.1016/j.procs.2016.05.523
   Montanino M, 2015, TRANSPORT RES B-METH, V80, P82, DOI 10.1016/j.trb.2015.06.010
   Nyame-Baafi E, 2018, J TRAFFIC TRANSP ENG, V5, P417, DOI 10.1016/j.jtte.2018.01.005
   Orozco H, 2011, VISUAL COMPUT, V27, P275, DOI 10.1007/s00371-011-0549-z
   Ruan X, 2017, TRANSPORT RES C-EMER, V78, P63, DOI 10.1016/j.trc.2017.02.023
   Sagberg F, 2015, HUM FACTORS, V57, P1248, DOI 10.1177/0018720815591313
   Saini R, 2019, VISUAL COMPUT, V35, P415, DOI 10.1007/s00371-018-1473-2
   Shen JJ, 2012, GRAPH MODELS, V74, P265, DOI 10.1016/j.gmod.2012.04.002
   Sider TMN, 2014, CAN J CIVIL ENG, V41, P856, DOI 10.1139/cjce-2013-0536
   Syed W.J., 2017, INT MULT C, DOI [10.1109/INMIC.2017.8289478, DOI 10.1109/INMIC.2017.8289478]
   Taubman-Ben-Ari O, 2004, ACCIDENT ANAL PREV, V36, P323, DOI 10.1016/S0001-4575(03)00010-1
   Taubman-Ben-Ari O, 2012, ACCIDENT ANAL PREV, V45, P416, DOI 10.1016/j.aap.2011.08.007
   Wang H, 2018, COMPUT GRAPH-UK, V70, P235, DOI 10.1016/j.cag.2017.07.004
   Wang J, 2016, INT J MOD PHYS C, V27, DOI 10.1142/S0129183116500133
   Yang X, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1740
   Yuan SC, 2017, TRANSPORT RES C-EMER, V78, P129, DOI 10.1016/j.trc.2017.03.001
   Yujie Sun, 2015, Applied Mechanics and Materials, V738-739, P204, DOI 10.4028/www.scientific.net/AMM.738-739.204
NR 37
TC 4
Z9 4
U1 6
U2 47
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1203
EP 1218
DI 10.1007/s00371-019-01732-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LI0TO
UT WOS:000529199400009
DA 2024-07-18
ER

PT J
AU Jia, SY
   Zhang, WZ
   Pan, ZK
   Wang, GD
   Yu, XK
AF Jia, Shiyu
   Zhang, Weizhong
   Pan, Zhenkuan
   Wang, Guodong
   Yu, Xiaokang
TI Using pseudo voxel octree to accelerate collision between cutting tool
   and deformable objects modeled as linked voxels
SO VISUAL COMPUTER
LA English
DT Article
DE Deformable object; Physically based modeling; Interactive cutting;
   Octree
ID SIMULATION; CUTS
AB For deformable objects modeled as a uniform grid of voxels connected by links, an octree for the voxels is constructed. Cutting is performed by disconnecting links swept by the cutting tool and reconstructing cut surface mesh using the dual contour method. The cubes of the voxel octree are not directly used because their edges generally do not remain straight when the objects deform. Instead, the voxel octree is used to mark active voxels and links and is therefore called "pseudo." Voxels and links located in the interiors of voxel octree cubes are deactivated. For collision between the cutting tool and the deformable objects, only active voxels and links are considered. Then, voxel octree cubes with newly cut links on their boundaries are recursively subdivided, and new voxels and links are activated accordingly. These algorithms are implemented with multi-threading techniques. Simulation tests show that when compared to previous methods using a uniform grid of voxels, our voxel octree method can increase cutting tool collision speed by 11-96% and can increase overall simulation speed by 7-43%.
C1 [Jia, Shiyu; Zhang, Weizhong; Pan, Zhenkuan; Wang, Guodong; Yu, Xiaokang] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
C3 Qingdao University
RP Zhang, WZ (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
EM zhangwz_01@aliyun.com
RI Zhang, Weizhong/S-9082-2019; jia, shi/HHS-4172-2022
OI Zhang, Weizhong/0000-0002-1831-0862; 
FU National Key Technology Support Program of China during the Twelfth
   Five-year Plan Period [2013BAI01B03]
FX This study was funded by the National Key Technology Support Program of
   China during the Twelfth Five-year Plan Period (Grant Number
   2013BAI01B03).
CR Berndt I, 2017, IEEE COMPUT GRAPH, V37, P24, DOI 10.1109/MCG.2017.45
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P1663, DOI 10.1109/TVCG.2010.268
   Hahn D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925902
   Hahn D, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766896
   Jerábková L, 2010, PROG BIOPHYS MOL BIO, V103, P217, DOI 10.1016/j.pbiomolbio.2010.09.012
   Jerabkova L, 2009, IEEE COMPUT GRAPH, V29, P61, DOI 10.1109/MCG.2009.32
   Jia SY, 2018, COMPUT GRAPH FORUM, V37, P45, DOI 10.1111/cgf.13162
   Jia SY, 2015, INT J COMPUT ASS RAD, V10, P1477, DOI 10.1007/s11548-014-1147-0
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kaufmann P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531356
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   Pan JJ, 2018, VISUAL COMPUT, V34, P105, DOI 10.1007/s00371-016-1317-x
   Paulus CJ, 2015, VISUAL COMPUT, V31, P831, DOI 10.1007/s00371-015-1123-x
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Seiler M, 2011, VISUAL COMPUT, V27, P519, DOI 10.1007/s00371-011-0561-3
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   Steinemann D, 2009, GRAPH MODELS, V71, P209, DOI 10.1016/j.gmod.2008.12.004
   Turkiyyah GM, 2011, COMPUT AIDED DESIGN, V43, P809, DOI 10.1016/j.cad.2010.10.005
   Wang Y., 2014, Proc ACM SIGGRAPH/Eurographics Symp Comp Anim, SCA '14, P77
   Wu J, 2015, COMPUT GRAPH FORUM, V34, P161, DOI 10.1111/cgf.12528
   Wu J, 2013, VISUAL COMPUT, V29, P739, DOI 10.1007/s00371-013-0810-8
   Zhu YF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766942
NR 23
TC 2
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1017
EP 1028
DI 10.1007/s00371-019-01716-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100011
DA 2024-07-18
ER

PT J
AU Gupta, S
   Thakur, K
   Kumar, M
AF Gupta, Surbhi
   Thakur, Kutub
   Kumar, Munish
TI 2D-human face recognition using SIFT and SURF descriptors of face's
   feature regions
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; SURF; SIFT; Decision tree; Random forest
ID GENDER; IMAGE; AGE
AB Face recognition is the process of identifying people through facial images. It has become vital for security and surveillance applications and required everywhere including institutions, organizations, offices, and social places. There are a number of challenges faced in face recognition which includes face pose, age, gender, illumination, and other variable condition. Another challenge is that the database size for these applications is usually small. So, training and recognition become difficult. Face recognition methods can be divided into two major categories, appearance-based method and feature-based method. In this paper, the authors have presented the feature-based method for 2D face images. speeded up robust features (SURF) and scale-invariant feature transform (SIFT) are used for feature extraction. Five public datasets, namely Yale2B, Face 94, M2VTS, ORL, and FERET, are used for experimental work. Various combinations of SIFT and SURF features with two classification techniques, namely decision tree and random forest, have experimented in this work. A maximum recognition accuracy of 99.7% has been reported by the authors with a combination of SIFT (64-components) and SURF (32-components).
C1 [Gupta, Surbhi] Gokaraju Rangaraju Inst Engn & Technol, Dept Comp Sci & Engn, Hyderabad, Telangana, India.
   [Thakur, Kutub] New Jersey City Univ, Dept Profess Secur Studies, Cyber Secur, Jersey City, NJ USA.
   [Kumar, Munish] Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda, Punjab, India.
C3 Gokaraju Rangaraju Institute of Engineering & Technology; New Jersey
   City University
RP Kumar, M (corresponding author), Maharaja Ranjit Singh Punjab Tech Univ, Dept Computat Sci, Bathinda, Punjab, India.
EM munishcse@gmail.com
RI Gupta, Surbhi/AAS-3570-2020; Gupta, Surbhi/HPF-6704-2023; Kumar,
   Munish/P-7756-2018; gupta, surbhi/AAY-7465-2020
OI Gupta, Surbhi/0000-0003-0618-8369; Gupta, Surbhi/0000-0003-0618-8369;
   Kumar, Munish/0000-0003-0115-1620; 
CR Abdurrahim SH, 2018, VISUAL COMPUT, V34, P1617, DOI 10.1007/s00371-017-1428-z
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Carro Raul Cid, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P316, DOI 10.1007/978-3-319-22180-9_31
   Cedillo-Hernandez M, 2014, P I CONF MEC EL AUT, P38, DOI 10.1109/ICMEAE.2014.16
   Chhabra P., 2018, Neural Computing and Applications, P1
   Geng Du, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7496, DOI 10.1117/12.832636
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Guntupalli JS, 2017, TRENDS COGN SCI, V21, P915, DOI 10.1016/j.tics.2017.09.007
   Hassner T, 2016, IEEE COMPUT SOC CONF, P127, DOI 10.1109/CVPRW.2016.23
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Huang ZH, 2015, INFORM FUSION, V22, P95, DOI 10.1016/j.inffus.2014.06.001
   Huibin Tan, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P548, DOI 10.1007/978-3-319-14442-9_59
   Karczmarek P, 2017, PATTERN RECOGN, V65, P26, DOI 10.1016/j.patcog.2016.12.008
   Ke JC, 2018, J MOD OPTIC, V65, P367, DOI 10.1080/09500340.2017.1380854
   Klemm Soeren, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P447
   Kotropoulos C, 1997, INT CONF ACOUST SPEE, P2537, DOI 10.1109/ICASSP.1997.595305
   LI G, 2015, OPEN AUTOM CONTROL S, V7, P1721
   Li J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072080
   Liong VE, 2013, 2013 9TH INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING (ICICS)
   Liu BD, 2016, NEUROCOMPUTING, V204, P198, DOI 10.1016/j.neucom.2015.08.128
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4042, DOI 10.1109/TIP.2017.2713940
   Naik MK, 2016, APPL SOFT COMPUT, V38, P661, DOI 10.1016/j.asoc.2015.10.039
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Pigeon S, 1997, LECT NOTES COMPUT SC, V1206, P403, DOI 10.1007/BFb0016021
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Vinay A, 2015, PROCEDIA COMPUT SCI, V70, P185, DOI 10.1016/j.procs.2015.10.070
   Vinay A, 2015, PROCEDIA COMPUT SCI, V70, P174, DOI 10.1016/j.procs.2015.10.068
   Wang WH, 2015, LECT NOTES COMPUT SC, V8944, P812, DOI 10.1007/978-3-319-15554-8_73
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Werghi N, 2016, IEEE T INF FOREN SEC, V11, P964, DOI 10.1109/TIFS.2016.2515505
   Wu SF, 2019, J VIS COMMUN IMAGE R, V60, P116, DOI 10.1016/j.jvcir.2019.01.013
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhang C, 2014, IEEE WINT CONF APPL, P1036, DOI 10.1109/WACV.2014.6835990
NR 37
TC 102
Z9 108
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 447
EP 456
DI 10.1007/s00371-020-01814-8
EA FEB 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000513032100001
DA 2024-07-18
ER

PT J
AU Zhang, SD
   He, FZ
   Ren, WQ
   Yao, J
AF Zhang, Shengdong
   He, Fazhi
   Ren, Wenqi
   Yao, Jian
TI Joint learning of image detail and transmission map for single image
   dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Joint learning; Dehazing; Image detail estimating; Non-local
   regularization; Transmission estimating
AB Single image haze removal is an important task in computer vision. However, haze removal is an extremely challenging problem due to its massively ill-posed, which is that at each pixel we must estimate the transmission and the global atmospheric light from a single color measurement. In this paper, we propose a new deep learning-based method for removing haze from single input image. First, we estimate a transmission map via joint estimation of clear image detail and transmission map, which is different from traditional methods only estimating a transmission map for a hazy image. Second, we use a global regularization method to eliminate the halos and artifacts. Experimental results on synthetic dataset and real-world images show our method outperforms the other state-of-the-art methods.
C1 [Zhang, Shengdong] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [He, Fazhi] Wuhan Univ, Sch Comp Sci, State Key Lab Software Engn, Wuhan, Peoples R China.
   [Ren, Wenqi] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
   [Yao, Jian] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan, Peoples R China.
C3 Wuhan University; Wuhan University; Chinese Academy of Sciences;
   Institute of Information Engineering, CAS; Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, State Key Lab Software Engn, Wuhan, Peoples R China.
EM fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018; Ren, Wenqi/L-8724-2019
FU National Natural Science Foundation of China [61472289, 41571436]
FX This study was funded by National Natural Science Foundation of China
   (Grant Numbers 61472289 and 41571436).
CR Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai B., 2016, ARXIV160107661
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1
   Chen YL, 2017, PATTERN RECOGN, V67, P139, DOI 10.1016/j.patcog.2017.02.013
   Fan X, 2016, VISUAL COMPUT, V32, P137, DOI 10.1007/s00371-015-1083-1
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gibson KB, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-37
   Harald K, 1924, THEORIE HORIZONTALEN
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Ju MY, 2017, VISUAL COMPUT, V33, P1613, DOI 10.1007/s00371-016-1305-1
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li K, 2019, FRONT COMPUT SCI-CHI, V13, P1116, DOI 10.1007/s11704-018-6442-4
   Li K, 2018, J COMPUT SCI TECH-CH, V33, P223, DOI 10.1007/s11390-017-1764-5
   Li K, 2017, APPL MATH SER B, V32, P294, DOI 10.1007/s11766-017-3466-8
   Li W., 2018, BRIT MACH VIS C
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Ling ZG, 2016, VISUAL COMPUT, V32, P653, DOI 10.1007/s00371-015-1081-3
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sulami M, 2014, IEEE INT CONF COMPUT
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yu HP, 2019, MULTIMED TOOLS APPL, V78, P11779, DOI 10.1007/s11042-018-6735-5
   Yu HP, 2018, MULTIMED TOOLS APPL, V77, P24097, DOI 10.1007/s11042-018-5697-y
   Zhang DJ, 2017, INTEGR COMPUT-AID E, V24, P261, DOI 10.3233/ICA-170544
   Zhang S., 2017, PAC RIM C MULT, P315
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 37
TC 76
Z9 76
U1 3
U2 63
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 305
EP 316
DI 10.1007/s00371-018-1612-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300007
DA 2024-07-18
ER

PT J
AU Men, QH
   Leung, H
AF Men, Qianhui
   Leung, Howard
TI Retrieval of spatial-temporal motion topics from 3D skeleton data
SO VISUAL COMPUTER
LA English
DT Article
DE Skeleton-based motion retrieval; Spatial-temporal descriptors; Motion
   documents; Latent Dirichlet allocation
AB Retrieval of a specific human motion from 3D skeleton data is intractable because of its articulated complexity. We propose a context-based motion document formation method to reflect geometric variations by calculating covariance descriptors among skeletal joint locations and joint relative distances, and temporal variations by performing a coarse-to-fine segmentation on the motion sequence. The descriptors of query motion traverse all the motion categories to lock its motion words, which can be regarded as the basic units of a motion document. The discrete motion words of different spatiotemporal descriptors are also mapped to divergent index ranges to add prior knowledge of motion with temporal order to latent Dirichlet allocation (LDA). The similarity matching is based on motion-topic distributions from LDA with semantic meanings. The experiments on public datasets show the effectiveness and robustness of the proposed method over existing models.
C1 [Men, Qianhui; Leung, Howard] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Leung, H (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM qianhumen2-c@my.cityu.edu.hk; howard@cityu.edu.hk
OI MEN, Qianhui/0000-0002-0059-5484
FU City University of Hong Kong [7004681, 7004916]
FX The work described in this paper was fully supported by Grants from City
   University of Hong Kong (Project No. 7004681 and 7004916).
CR [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   [Anonymous], BRIT MACH VIS C
   [Anonymous], 2014, P 2014 ACM SIGGRAPHE
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bremaud P., 2012, INTRO PROBABILISTIC
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Chao MW, 2012, IEEE T VIS COMPUT GR, V18, P729, DOI 10.1109/TVCG.2011.53
   Chen C, 2011, IEEE T VIS COMPUT GR, V17, P1676, DOI 10.1109/TVCG.2010.272
   Chiu CY, 2004, J VIS COMMUN IMAGE R, V15, P446, DOI 10.1016/j.jvcir.2004.04.004
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Sucar LE, 2008, COMM COM INF SC, V25, P531
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   Ho ESL, 2009, IEEE T VIS COMPUT GR, V15, P481, DOI 10.1109/TVCG.2008.199
   Hussein, 2013, INT JOINT C ART INT
   Jia ZH, 2007, CIS WORKSHOPS 2007: INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY WORKSHOPS, P191, DOI 10.1109/CIS.Workshops.2007.116
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kitagawa M., 2012, MoCap for Artists: Workflow and Techniques for Motion Capture
   Komura T, 2005, COMPUT ANIMAT VIRT W, V16, P213, DOI 10.1002/cav.101
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Lan R., 2012, INT C INT SCI INT DA, P72
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li M, 2016, COMPUT GRAPH-UK, V54, P104, DOI 10.1016/j.cag.2015.07.005
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu Xinxin, 2017, IEEE T IND ELECTRON, V99, P1
   Lv N, 2018, IEEE T VIS COMPUT GR, V24, P1969, DOI 10.1109/TVCG.2017.2702620
   MacKay D., 2003, INFORM THEORY INFERE
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Muller M., 2007, Information retrieval for music and motion, P69, DOI [10.1007/978-3-540-74048-3_4, DOI 10.1007/978-3-540-74048-3_4]
   Muller M., 2007, Tech. Rep. CG-2007-2
   Qi T, 2013, COMPUT ANIMAT VIRT W, V24, P399, DOI 10.1002/cav.1505
   Sedmidubsky J, 2018, MULTIMED TOOLS APPL, V77, P12073, DOI 10.1007/s11042-017-4859-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Valcik J, 2016, COMPUT ANIMAT VIRT W, V27, P484, DOI 10.1002/cav.1674
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Wang PJ, 2014, COMPUT GRAPH-UK, V38, P255, DOI 10.1016/j.cag.2013.11.008
   Wang Z, 2016, SIGNAL PROCESS, V120, P691, DOI 10.1016/j.sigpro.2014.11.015
   Wu S., 2009, P 16 ACM S VIRTUAL R, P207
   Xiao J, 2015, SIGNAL PROCESS, V113, P1, DOI 10.1016/j.sigpro.2015.01.004
   Xiao QK, 2017, SOFT COMPUT, V21, P255, DOI 10.1007/s00500-016-2059-4
   Xiao QK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0164610
   Yang S, 2015, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2015.7298769
   Yoo I, 2014, VISUAL COMPUT, V30, P213, DOI 10.1007/s00371-013-0797-1
   Yoshitaka A, 1999, IEEE T KNOWL DATA EN, V11, P81, DOI 10.1109/69.755617
   Yu T, 2005, COMPUT ANIMAT VIRT W, V16, P273, DOI 10.1002/cav.89
   Zhou LY, 2014, VISUAL COMPUT, V30, P845, DOI 10.1007/s00371-014-0957-y
   Zhu MY, 2012, COMPUT ANIMAT VIRT W, V23, P469, DOI 10.1002/cav.432
NR 51
TC 4
Z9 4
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 973
EP 984
DI 10.1007/s00371-019-01690-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200017
OA Bronze
DA 2024-07-18
ER

PT J
AU Gong, H
   Finlayson, GD
   Fisher, RB
   Fang, FF
AF Gong, Han
   Finlayson, Graham D.
   Fisher, Robert B.
   Fang, Fufu
TI 3D color homography model for photo-realistic color transfer re-coding
SO VISUAL COMPUTER
LA English
DT Article
DE Color transfer; Color grading; Color homography; Tone mapping
ID LINEAR-MODELS; SURFACE
AB Color transfer is an image editing process that naturally transfers the color theme of a source image to a target image. In this paper, we propose a 3D color homography model which approximates photo-realistic color transfer algorithm as a combination of a 3D perspective transform and a mean intensity mapping. A key advantage of our approach is that the re-coded color transfer algorithm is simple and accurate. Our evaluation demonstrates that our 3D color homography model delivers leading color transfer re-coding performance. In addition, we also show that our 3D color homography model can be applied to color transfer artifact fixing, complex color transfer acceleration, and color-robust image stitching.
C1 [Gong, Han; Finlayson, Graham D.; Fang, Fufu] Univ East Anglia, Sch Comp Sci, Norwich, Norfolk, England.
   [Fisher, Robert B.] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
C3 University of East Anglia; University of Edinburgh
RP Gong, H (corresponding author), Univ East Anglia, Sch Comp Sci, Norwich, Norfolk, England.
EM h.gong@uea.ac.uk
FU Engineering and Physical Sciences Research Council (EPSRC)
   [EP/M001768/1]; EPSRC [EP/M00189X/1, EP/M001768/1] Funding Source: UKRI
FX This work was supported by Engineering and Physical Sciences Research
   Council (EPSRC) (EP/M001768/1). We also acknowledge the constructive
   suggestions from all reviewers.
CR An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   [Anonymous], BRIT MACH VIS C
   [Anonymous], 2015, ACM T GRAPH
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   Finalyson G. D., 2016, COL IM C
   Finlayson G. D., 2017, IEEE T PATTERN ANAL
   Finlayson GD, 2015, COLOR RES APPL, V40, P232, DOI 10.1002/col.21889
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   Ilie A, 2005, IEEE I CONF COMP VIS, P1268, DOI 10.1109/ICCV.2005.88
   MALONEY LT, 1986, J OPT SOC AM A, V3, P1673, DOI 10.1364/JOSAA.3.001673
   MARIMONT DH, 1992, J OPT SOC AM A, V9, P1905, DOI 10.1364/JOSAA.9.001905
   Nguyen RMH, 2014, COMPUT GRAPH FORUM, V33, P319, DOI 10.1111/cgf.12500
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Pitie A., 2007, 4 EUR C VIS MED PROD, DOI DOI 10.1049/CP:20070055
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Pouli T., 2010, Proceedings ofNPAR, P81
   Rabin J, 2011, IEEE T IMAGE PROCESS, V20, P3073, DOI 10.1109/TIP.2011.2142318
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Robertson A.R., 1977, Color Res. Appl., V2, P7, DOI [10.1002/j.1520-6378.1977.tb00104.x, DOI 10.1002/J.1520-6378.1977.TB00104.X]
   Tai YW, 2005, PROC CVPR IEEE, P747
   TAKANE Y, 1977, PSYCHOMETRIKA, V42, P7, DOI 10.1007/BF02293745
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu FZ, 2013, COMPUT GRAPH FORUM, V32, P190, DOI 10.1111/cgf.12008
NR 28
TC 10
Z9 13
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 323
EP 333
DI 10.1007/s00371-017-1462-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600003
OA hybrid, Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Eren, MT
   Balcisoy, S
AF Eren, Mustafa Tolga
   Balcisoy, Selim
TI Evaluation of X-ray visualization techniques for vertical depth
   judgments in underground exploration
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented reality; Depth perception; User study; X-ray visualization
ID HELD AUGMENTED REALITY
AB This paper investigates depth judgment-related performances of X-ray visualization techniques for rendering fully occluded geometries in augmented reality. The techniques we selected for this evaluation are careless overlay (CO), edge overlay (EO), excavation box (EB) and a cross-sectional visualization technique (CS). We have designed and conducted a comprehensive user study with 16 participants to examine and analyze the effects related to visualization techniques, having additional virtual objects and the scale of the vertical depths. To the best of our knowledge, this is the first user study on judged vertical depth distances that these techniques were compared against each other. We report our findings using four dependent variables: accuracy, signed error, absolute error and response time to shed some light into real-world performances and also to reveal estimation tendencies of each technique. Our findings suggest similar and better performance for EB, CS compared to CO and EO. We also observed significantly better results for EB and CS techniques when judging Top and Bottom distances compared to Middle distances. Derived from our findings, we proposed a new visualization technique for underground investigation with multiple views. The multi-view technique is our own implementation inspired by magic lens and cross-sectional visualizations with correlating displays.
C1 [Eren, Mustafa Tolga; Balcisoy, Selim] Sabanci Univ, Istanbul, Turkey.
C3 Sabanci University
RP Eren, MT (corresponding author), Sabanci Univ, Istanbul, Turkey.
EM tolgaeren@gmail.com; balcisoy@sabanciuniv.edu
RI , Selim/Y-3196-2019
OI Balcisoy, Selim/0000-0002-6495-7341
CR Avery B, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P79
   Bane R, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P231, DOI 10.1109/ISMAR.2004.36
   Behzadan AH, 2005, PROCEEDINGS OF THE 2005 WINTER SIMULATION CONFERENCE, VOLS 1-4, P1914, DOI 10.1109/WSC.2005.1574469
   Bier E. A., 1993, Computer Graphics Proceedings, P73, DOI 10.1145/166117.166126
   Coffin C, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P25, DOI 10.1109/TRIDUI.2006.1618266
   Dey A., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology (VRST), P211, DOI DOI 10.1145/1889863.1889911
   Dey A, 2014, INT J HUM-COMPUT ST, V72, P704, DOI 10.1016/j.ijhcs.2014.04.001
   Dey A, 2012, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR.2012.6402556
   Diepstraten J, 2003, COMPUT GRAPH FORUM, V22, P523, DOI 10.1111/1467-8659.t01-3-00700
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Goldstein E., 2013, SENSATION PERCEPTION
   Jansen Y, 2016, IEEE T VIS COMPUT GR, V22, P479, DOI 10.1109/TVCG.2015.2467951
   Jones JA, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P9
   Kalkofen D, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P71, DOI 10.1109/VR.2009.4811001
   King GR, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P52
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   Kruijff Ernst, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P3, DOI 10.1109/ISMAR.2010.5643530
   LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M
   Lappin JS, 2006, PERCEPT PSYCHOPHYS, V68, P571, DOI 10.3758/BF03208759
   Li W, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239482
   Livingston M. A., P 2 IEEE ACM INT S M, P101
   Livingston M. A., 2013, PURSUIT XRAY VISION
   Livingston MA, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P55, DOI 10.1109/VR.2009.4810999
   Lo Monte L, 2009, NAECON: PROCEEDINGS OF THE IEEE 2009 NATIONAL AEROSPACE & ELECTRONICS CONFERENCE, P182, DOI 10.1109/NAECON.2009.5426629
   Lo Monte L, 2010, IEEE T GEOSCI REMOTE, V48, P1128, DOI 10.1109/TGRS.2009.2029341
   Looser Julian., 2004, P 2 INT C COMPUTER G, P204, DOI DOI 10.1145/988834.988870
   Mendez E., 2009, Proc. ACM Sym. Vir. Real- ity Softw. Technol., P247, DOI [10.1145/1643928.1643988, DOI 10.1145/1643928.1643988]
   Roberts JC, 1998, IEEE INFOR VIS, P8, DOI 10.1109/IV.1998.694193
   Schall G., 2010, GEOHYDROINFORMATICS, V1, P1
   Schall G, 2009, PERS UBIQUIT COMPUT, V13, P281, DOI 10.1007/s00779-008-0204-5
   Simi A, 2008, 2008 IEEE RAD C ROM, P1, DOI [10.1109/RADAR.2008.4720763, DOI 10.1109/RADAR.2008.4720763]
   Swan JE, 2006, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2006.13
   Viega John., 1996, UIST 96, P51, DOI [10.1145/237091.237098, DOI 10.1145/237091.237098]
   Zollmann Stefanie, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P19, DOI 10.1109/ISMAR.2010.5643546
   [No title captured]
NR 35
TC 14
Z9 15
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 405
EP 416
DI 10.1007/s00371-016-1346-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900008
DA 2024-07-18
ER

PT J
AU Wang, C
   Wang, CB
   Qin, H
   Zhang, TY
AF Wang, Chao
   Wang, Changbo
   Qin, Hong
   Zhang, Tai-you
TI Video-based fluid reconstruction and its coupling with SPH simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Video-based reconstruction; Height field; Shallow water; SPH; Two-way
   coupling; Physically based simulation
ID LATTICE BOLTZMANN METHOD
AB Conventional methods to create fluid animation primarily resort to physically based simulation via numerical integration, whose performance is dominantly hindered by large amount of numerical calculation and low efficiency. Alternatively, video-based methods could easily reconstruct fluid surfaces from videos, yet they are not able to realize two-way dynamic interaction with their surrounding environment in a physically correct manner. In this paper, we propose a hybrid method that combines video-based fluid surface reconstruction and popular fluid animation models to compute and re-animate fluid surface. First, the fluid surface's height field corresponding to each video frame is estimated by using the shape-from-shading method. After denoising, hole-filling, and smoothing operations, the height field is utilized to calculate the velocity field, where the shallow water model is adopted. Then we treat the height field and velocity field as real data to drive the simulation. Still, only one layer of surface particles is not capable of driving the smoothed particle hydrodynamics (SPH) system. The surface particles (including 3D position and its velocity) are then employed to guide the spatial sampling of the entire volume underneath. Second, the volume particles corresponding to each video frame are imported into the SPH system to couple with other possible types of particles (used to define interacting objects), whose movement is dictated by the direct forcing method, and fluid particles' geometry information is then corrected by both physical models and real video data. The resulting animation approximates the reconstruction surface from the input video, and new physically based coupling behaviors are also appended. We document our system's detailed implementation and showcase visual performance across a wide range of scenes.
C1 [Wang, Chao; Wang, Changbo; Zhang, Tai-you] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
   [Wang, Changbo] East China Normal Univ, MOE Int Joint Lab Trustworthy Software, Shanghai, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; East China Normal University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.; Wang, CB (corresponding author), East China Normal Univ, MOE Int Joint Lab Trustworthy Software, Shanghai, Peoples R China.
EM cbwangcg@gmail.com
FU Natural Science Foundation of China [61532002, 61272199]; National
   High-tech R& D Program of China (863 Program) [2015AA016404];
   Specialized Research Fund for Doctoral Program of Higher Education
   [20130076110008]; Open Funding Project of State Key Laboratory of
   Virtual Reality Technology and Systems of Beihang University
   [BUAA-VR-15KF-14]
FX This paper is partially supported by Natural Science Foundation of China
   under Grant Nos. 61532002, 61272199, National High-tech R& D Program of
   China (863 Program) under Grant 2015AA016404, the Specialized Research
   Fund for Doctoral Program of Higher Education under Grant
   20130076110008, and Open Funding Project of State Key Laboratory of
   Virtual Reality Technology and Systems of Beihang University under Grant
   BUAA-VR-15KF-14. The authors would like to thank all reviewers for their
   very helpful and constructive comments and suggestions. We also thank
   Dyntex Dataset for the support of rich fluid videos for our study.
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Barbara Solenthaler., 2011, 106 Proceedings of 8th Workshop on Virtual Reality Interaction and Physical Simulation 2011, VRIPHYS'11, P39
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chao JH, 2011, INT J NUMER METH FL, V66, P622, DOI 10.1002/fld.2276
   Chen S, 1998, ANNU REV FLUID MECH, V30, P329, DOI 10.1146/annurev.fluid.30.1.329
   Chentanez N., 2011, ACM T GRAPHIC, V30
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   He XW, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682630
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Kutulakos KN, 2008, INT J COMPUT VISION, V76, P13, DOI 10.1007/s11263-007-0049-9
   Kwatra V, 2008, COMPUT GRAPH FORUM, V27, P487, DOI 10.1111/j.1467-8659.2008.01146.x
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Lentine M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778851
   Li C, 2013, IEEE T VIS COMPUT GR, V19, P1242, DOI 10.1109/TVCG.2012.302
   Low K.-L., 2004, Chapel Hill, Univ. North Carolina, V4, P1
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Ojeda J., 2013, EUROGRAPHICS, V2013, P25
   Pan ZR, 2012, COMPUT GRAPH FORUM, V31, P2029, DOI 10.1111/j.1467-8659.2012.03195.x
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Pickup D, 2011, LECT NOTES COMPUT SC, V6495, P189, DOI 10.1007/978-3-642-19282-1_16
   Quan H, 2015, VISUAL COMPUT, P1
   Robert Bridson A.K.P., 2008, FLUID SIMULATION COM
   Shao X, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12467
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Thürey N, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P39, DOI 10.1109/PG.2007.33
   TSAI PS, 1994, IMAGE VISION COMPUT, V12, P487, DOI 10.1016/0262-8856(94)90002-7
   Wang H., 2009, ACM T GRAPHIC, V28, P341
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhu B, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461999
NR 34
TC 9
Z9 12
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1211
EP 1224
DI 10.1007/s00371-016-1284-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600010
DA 2024-07-18
ER

PT J
AU Yu, DS
   Kanai, T
AF Yu, Duosheng
   Kanai, Takashi
TI Data-driven subspace enrichment for elastic deformations with collisions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Elastic deformation; Subspace; Collision; Cubature
AB We propose an efficient data-driven enrichment approach to adaptively enhance the expressivity of subspaces for elastic deformations with novel collisions. In general, subspace integration method (also known as model reduction) for elastic deformations can greatly increase simulation speed. However, when the deformations are beyond the expressivity of subspaces such as novel external collisions, obvious artifacts will appear. First, we construct a position-based database of subspaces through full-space collided simulations. We then select small sets of basis vectors to enrich existing subspaces for incoming collided deformations. We also demonstrate that cubature can easily be exploited by our subspace database, and we propose a novel post-processing scheme for refining the cubature weights for more accurate and faster deformations. Our method can achieve well-approximated full-space deformations when novel collisions occur. From our experiment results, we further show that our method is applicable to large deformations and large steps in real time.
C1 [Yu, Duosheng; Kanai, Takashi] Univ Tokyo, Meguro Ku, 3-8-1 Komaba, Tokyo 1538902, Japan.
C3 University of Tokyo
RP Kanai, T (corresponding author), Univ Tokyo, Meguro Ku, 3-8-1 Komaba, Tokyo 1538902, Japan.
EM kanai@graco.c.u-tokyo.ac.jp
OI Kanai, Takashi/0000-0002-1635-3818
FU Grants-in-Aid for Scientific Research [16K00169] Funding Source: KAKEN
CR An SS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409118
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Barbic J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185566
   Barbic J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964986
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Hahn Fabian, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601160
   Harmon D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461922
   Hauser KK, 2003, PROC GRAPH INTERF, P247
   James DL, 2002, ACM T GRAPHIC, V21, P582, DOI 10.1145/566570.566621
   Kim T., 2013, ACM T GRAPHIC, V32
   Kim T, 2012, IEEE T VIS COMPUT GR, V18, P1228, DOI 10.1109/TVCG.2012.78
   Kim T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618469
   Li SW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601217
   PENTLAND A., 1989, COMPUT GRAPHICS-US, V23, P207
   Shabana A.A., 2012, THEORY VIBRATION
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, DOI [10.1145/2343483.2343501, DOI 10.1145/2343483.2343501]
   Tang M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185603
   Teng Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766904
   Treuille A, 2006, ACM T GRAPHIC, V25, P826, DOI 10.1145/1141911.1141962
   von Tycowicz C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2729972
   von Tycowicz C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508392
   Xu HY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925916
NR 22
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 779
EP 788
DI 10.1007/s00371-017-1376-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800010
DA 2024-07-18
ER

PT J
AU Quan, HY
   Wang, CB
   Song, YH
AF Quan, Hongyan
   Wang, Changbo
   Song, Yahui
TI Fluid re-simulation based on physically driven model from video
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid; Re-simulation; Non-normalized geometry; Physically driven model;
   Video; Geometry scale factor
ID RECONSTRUCTION; REFRACTION
AB Customizing a desired naturalistic fluid simulation result from video to obtain similar artistic effect is significant in practice. But art-directed customizing is challengeable due to the chaotic nature of the physics contained in it, and this still remains to be a difficult task in spite of rapid advancements of computer graphics during the last two decades. This paper focuses on the problem of physically based fluid re-simulation which is foundational to customize desired naturalistic simulation result from video example. In the previous achievements, conventional algorithms primarily recover 3D geometry of fluid surface or obtain the velocity of fluid particles in video. However, to launch new derivative results, just geometry and velocity are not enough, and physically based driven models are promising. We present a novel method that is capable of efficiently recovering physically driven model from an existing video. We advocate a new approach to calculate the velocity and non-normalized surface geometry under the constraints of the appearance and dynamic behavior of the example fluid in multi-scale framework, and use the calculated physical properties quickly to recover the fluid-driven model. In particular, to calculate the surface geometry more accurately, we introduce a scale factor between normalized geometry and non-normalized geometry to acquire a more accurate result. We propose a novel recovery algorithm, in which the particle densities of lattice Boltzmann method can be recovered more accurately from matching fluid advection geometry with the calculated non-normalized geometry. Fluid re-simulations with different types of density can be achieved, including constrained particle density, auto-advection density, and enhanced particle density. We demonstrate our results in several challenging scenarios and provide qualitative evaluation to our method. Some applications based on our approach are also demonstrated in the implementation.
C1 [Quan, Hongyan; Wang, Changbo; Song, Yahui] East China Normal Univ, Shanghai, Peoples R China.
C3 East China Normal University
RP Quan, HY (corresponding author), East China Normal Univ, Shanghai, Peoples R China.
EM ynyn888@163.com; cbwang@sei.ecnu.edu.cn; 51131500033@sei.ecnu.edu.cn
FU Natural Science Foundation of China [61532002, 61272199]; National
   High-tech R&D Program of China (863 Program) [2015AA016404]; Specialized
   Research Fund for Doctoral Program of Higher Education [20130076110008];
   Open Funding Project of State Key Laboratory of Virtual Reality
   Technology and Systems of Beihang University [BUAA-VR-15KF-14]
FX This paper is partially supported by Natural Science Foundation of China
   under Grant Nos. 61532002, 61272199, National High-tech R&D Program of
   China (863 Program) under Grant 2015AA016404, the Specialized Research
   Fund for Doctoral Program of Higher Education under Grant
   20130076110008, and Open Funding Project of State Key Laboratory of
   Virtual Reality Technology and Systems of Beihang University under Grant
   BUAA-VR-15KF-14.
CR [Anonymous], P 10 AS C COMP VIS A
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Balschbach G., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P170, DOI 10.1007/BFb0054740
   Chetverikov D., 2001, Computer Analysis of Images and Patterns. 9th International Conference, CAIP 2001. Proceedings (Lecture Notes in Computer Science Vol.2124), P325
   Chuan Li, 2011, 2011 Conference for Visual Media Production, P109, DOI 10.1109/CVMP.2011.19
   Corpetti T, 2006, EXP FLUIDS, V40, P80, DOI 10.1007/s00348-005-0048-y
   Corpetti T, 2002, IEEE T PATTERN ANAL, V24, P365, DOI 10.1109/34.990137
   Doshi Ashish., 2007, BMVC, P1
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   HORN BKP, 1981, P SOC PHOTO-OPT INST, V281, P319
   Kim T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461987
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Lentine M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778851
   Li C, 2013, IEEE T VIS COMPUT GR, V19, P1242, DOI 10.1109/TVCG.2012.302
   Meinhardt-Llopis E, 2013, IMAGE PROCESS ON LIN, V3, P151, DOI 10.5201/ipol.2013.20
   Morris NJW, 2005, IEEE I CONF COMP VIS, P1573
   Mullen P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531344
   MURASE H, 1992, IEEE T PATTERN ANAL, V14, P1045, DOI 10.1109/34.159906
   Naoya I., 2011, P ACM SIGGRAPH, DOI [10.1145/2037715.2037719, DOI 10.1145/2037715.2037719]
   Paris S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360629
   Patel Sanjit, 2009, P ACM EUR S COMP AN
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   QIAN YH, 1992, EUROPHYS LETT, V17, P479, DOI 10.1209/0295-5075/17/6/001
   Quan H.Y., 2013, CHIN J COMPUT, V36, P1834
   Ray N, 2011, IEEE T IMAGE PROCESS, V20, P2925, DOI 10.1109/TIP.2011.2142005
   Sakaino H., 2008, IEEE Conference on Computer Vision and Pattern Recognition, P1
   Tan P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409061
   Thomas G, 2011, IEEE T INSTRUM MEAS, V60, P1565, DOI 10.1109/TIM.2010.2089110
   Wang Changbo, 2011, Journal of Computer Aided Design & Computer Graphics, V23, P104
   Wang HM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531396
   Yang M, 2012, VISUAL COMPUT, V28, P425, DOI 10.1007/s00371-011-0624-5
   Yu M.Q., 2013, AIDED DES COMPUT GRA, V25, P622
   Yu MQ, 2013, COMPUT ANIMAT VIRT W, V24, P497, DOI 10.1002/cav.1526
   Zhang GJ, 2011, VISUAL COMPUT, V27, P199, DOI 10.1007/s00371-010-0526-y
NR 35
TC 3
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 85
EP 98
DI 10.1007/s00371-015-1154-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200009
DA 2024-07-18
ER

PT J
AU Guo, J
   Pan, JG
AF Guo, Jie
   Pan, Jin-Gui
TI Real-time rendering of refracting transmissive objects with multi-scale
   rough surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Rough surface; Multi-scale; Refraction; Microfacet-based BTDF; Real-time
   rendering
ID MODEL
AB This paper presents an efficient approach to render refracting transmissive objects with multi-scale surface roughness, under distant illumination. To correctly capture both fine-scale surface details and large-scale appearances, and enable real-time processing at various viewing resolutions, we first divide the surface roughness into three levels, namely, micro-scale, meso-scale and macro-scale. Each scale of roughness is modeled and evaluated using different strategies, and the overall roughness is approximated by their spherical convolution. Then, this representation is incorporated into a microfacet-based BTDF model, and multi-scale rough refractions are simulated on both front and back sides of an object as light enters and exits the object. In particular, non-linear filtering methods are applied to both macro-scale geometries and meso-scale bumps to reduce aliasing when viewed across a range of distances. Finally, experimental results illustrate that our approach produces resolution-dependent refraction effects that match super-sampled ground truth, while achieving a speed up of several orders of magnitude with hardware acceleration.
C1 [Guo, Jie; Pan, Jin-Gui] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing University
RP Guo, J (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
EM guojie_022@163.com; panjg@nju.edu.cn
CR [Anonymous], P GRAPH INT MONTR CA
   [Anonymous], 2009, AUTOMOT TECHNOL, DOI DOI 10.1016/J.JDENT.2009.10.001
   [Anonymous], 2014, ACM T GRAPHICS
   [Anonymous], 1982, ACM T GRAPHIC, DOI DOI 10.1145/357290.357293
   Banerjee A, 2005, J MACH LEARN RES, V6, P1345
   Bruneton E, 2012, IEEE T VIS COMPUT GR, V18, P242, DOI 10.1109/TVCG.2011.81
   Bruneton E, 2010, COMPUT GRAPH FORUM, V29, P487, DOI 10.1111/j.1467-8659.2009.01618.x
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chan B, 2005, VISUAL COMPUT, V21, P579, DOI 10.1007/s00371-005-0312-4
   Colbert M., 2007, GPU GEMS, V3, P459
   Dai Q, 2009, COMPUT GRAPH FORUM, V28, P1917, DOI 10.1111/j.1467-8659.2009.01570.x
   De Rousiers C., 2011, S INTERACTIVE 3D GRA, P111
   de Rousiers C, 2012, IEEE T VIS COMPUT GR, V18, P1591, DOI 10.1109/TVCG.2011.282
   DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008
   Dupuy J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508422
   Eisemann Elmar, 2006, P 2006 S INTERACTIVE, P71, DOI DOI 10.1145/1111411.1111424
   Fournier A., 1992, GRAPHICS INTERFACE W, P45
   Genevaux O., 2006, I3D '06, P145
   Han C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239479
   Heidrich W., 1998, P ACM SIGGRAPH EUROG, P39
   Heitz Eric, 2012, EGGH HPG 12
   Hensley J, 2005, COMPUT GRAPH FORUM, V24, P547, DOI 10.1111/j.1467-8659.2005.00880.x
   Hu W, 2007, IEEE T VIS COMPUT GR, V13, P46, DOI 10.1109/TVCG.2007.14
   Ihrke L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451,1239510
   Mardia K.V., 2000, DIRECTIONAL STAT, DOI DOI 10.1162/089976600300015349
   OLANO M, 2010, LEAN MAPPING, P181, DOI [10.1145/1730804.1730834, DOI 10.1145/1730804.1730834]
   Oliveira MM, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P89
   Pharr M., 2010, PHYS BASED RENDERING
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Stam J, 2001, SPRING EUROGRAP, P39
   Sun X, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360634
   Svensson L., 2011, 14 INT C INF FUS, P1
   Tan P., 2005, EUROGRAPHICS S RENDE, P111
   Tan P, 2008, IEEE T VIS COMPUT GR, V14, P412, DOI 10.1109/TVCG.2007.70439
   Toksvig M., 2005, Journal of Graphics Tools, V10, P65
   Walter B., 2007, EUROGRAPHICS C RENDE
   Walter B., 2009, ACM T GRAPHIC, V28
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Wu HZ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024179
   Wu HZ, 2009, COMPUT GRAPH FORUM, V28, P1227, DOI 10.1111/j.1467-8659.2009.01500.x
   Wyman C, 2005, ACM T GRAPHIC, V24, P1050, DOI 10.1145/1073204.1073310
   Wyman Chris., 2005, Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia (GRAPHITE '05), P205
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 48
TC 1
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1579
EP 1592
DI 10.1007/s00371-015-1141-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200007
DA 2024-07-18
ER

PT J
AU Hong, QQ
   Li, Y
   Li, QD
   Wang, BZ
   Yao, JF
   Wu, QQ
   She, YY
AF Hong, Qingqi
   Li, Yan
   Li, Qingde
   Wang, Beizhan
   Yao, Junfeng
   Wu, Qingqiang
   She, Yingying
TI An implicit skeleton-based method for the geometry reconstruction of
   vasculatures
SO VISUAL COMPUTER
LA English
DT Article
DE Geometry reconstruction; Vasculature; Implicit modeling; Segmentation
ID VISUALIZATION
AB Due to the high complexity of vascular system network, the geometry reconstruction of vasculatures from raw medical datasets remains a very challenging task. In this paper, we present a novel skeleton-based method for the geometry reconstruction of vascular structures from standard 3D medical datasets. With the proposed techniques, the geometry of vascular structures with high level of smoothness and accuracy can be reconstructed from the raw medical datasets. The experimental results and comparison with other techniques demonstrate that our method can achieve faithful and smooth vascular structures. In addition, quantitative validation has been conducted to evaluate the accuracy and smoothness of the reconstructed vessel geometry based on the proposed method.
C1 [Hong, Qingqi] Xiamen Univ, Software Sch, Comp Sci, Xiamen, Peoples R China.
   [Wang, Beizhan; Yao, Junfeng; Wu, Qingqiang; She, Yingying] Xiamen Univ, Software Sch, Xiamen, Peoples R China.
   [Li, Yan] Xiamen Univ, Sch Management, Xiamen, Peoples R China.
   [Li, Qingde] Univ Hull, Dept Comp Sci, Kingston Upon Hull HU6 7RX, N Humberside, England.
C3 Xiamen University; Xiamen University; Xiamen University; University of
   Hull
RP Li, Y (corresponding author), Xiamen Univ, Sch Management, Xiamen, Peoples R China.
EM hongqq@xmu.edu.cn; liyangigi@xmu.edu.cn
FU National Natural Science Foundation of China [61502402]; Natural Science
   Foundation of Fujian Province of China [2015J05129]; Scientific Research
   Foundation for the Returned Overseas Chinese Scholars, China
   Postdoctoral Science Foundation [2015M57191]; MOE (Ministry of Education
   in China) Project of Humanities and Social Sciences [11YJC870027];
   Special and Major Subject Project of the Industrial Science and
   Technology in Fujian Province [2013HZ0004-1]; Key Project of Anhui
   Science and Technology BureauCDevelopment and Application of Energy
   Efficiency Monitoring, Evaluating and Optimizing Control System for
   Flash Copper Smelting Enterprises [1301021018]
FX The authors would like to thank all the anonymous reviewers for their
   constructive comments. This work is supported by the National Natural
   Science Foundation of China (Grant No. 61502402), the Natural Science
   Foundation of Fujian Province of China (No. 2015J05129), the Scientific
   Research Foundation for the Returned Overseas Chinese Scholars, China
   Postdoctoral Science Foundation (Project No. 2015M57191), and MOE
   (Ministry of Education in China) Project of Humanities and Social
   Sciences (Grant No. 11YJC870027). The author would also acknowledge the
   supports of the Special and Major Subject Project of the Industrial
   Science and Technology in Fujian Province 2013 (the Special Subject
   Project No. is 2013HZ0004-1), and the 2014 Key Project of Anhui Science
   and Technology BureauCDevelopment and Application of Energy Efficiency
   Monitoring, Evaluating and Optimizing Control System for Flash Copper
   Smelting Enterprises (Number: 1301021018).
CR Abeysinghe SS, 2009, VISUAL COMPUT, V25, P627, DOI 10.1007/s00371-009-0325-5
   [Anonymous], 2002, SURFACES
   [Anonymous], 2007, 2007 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2007.383014
   [Anonymous], 1997, Introduction to Implicit Surfaces
   Bartz D., 1999, BIOS 99 INT BIOM OPT, P1
   Bloomenthal J., 1995, THESIS
   Bornik A., 2005, Proc. of Winter School of Computer Graphics (WSCG) 2005, P61
   Boskamp T, 2005, MEDICAL IMAGING SYSTEMS TECHNOLOGY: METHODS IN CARDIOVASCULAR AND BRAIN SYSTEMS, VOL 5, P1
   Dong Chen-shi., 2005, Journal of Zhejiang University SCIENCE, V6, P128, DOI DOI 10.1631/JZUS.2005.AS0128
   Felkel P, 2002, BIOSIG BRNO, P252
   Felkel P, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P70, DOI 10.1109/CGI.2004.1309194
   Gerig G., 1993, Information Processing in Medical Imaging. 13th International Conference, IPMI '93 Proceedings, P94, DOI 10.1007/BFb0013783
   Gobbetti E, 1998, VISUALIZATION '98, PROCEEDINGS, P435, DOI 10.1109/VISUAL.1998.745337
   Hahn HK, 2001, IEEE VISUAL, P395, DOI 10.1109/VISUAL.2001.964538
   Höhne KH, 2000, LECT NOTES COMPUT SC, V1935, P776
   Hong QQ, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P686, DOI 10.1109/CISP.2013.6745253
   Hong QQ, 2014, PROC SPIE, V9069, DOI 10.1117/12.2050060
   Hong QQ, 2012, IEEE T MED IMAGING, V31, P543, DOI 10.1109/TMI.2011.2172455
   Jin XG, 2009, VISUAL COMPUT, V25, P279, DOI 10.1007/s00371-008-0267-3
   Joshi A, 2008, IEEE T VIS COMPUT GR, V14, P1603, DOI 10.1109/TVCG.2008.123
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kretschmer J, 2013, IEEE T VIS COMPUT GR, V19, P2828, DOI 10.1109/TVCG.2013.169
   Li Q, 2007, COMPUT GRAPH FORUM, V26, P157, DOI 10.1111/j.1467-8659.2007.01011.x
   Li QD, 2011, COMPUT AIDED DESIGN, V43, P394, DOI 10.1016/j.cad.2011.01.007
   Luboz V, 2014, VISUAL COMPUT, V30, P341, DOI 10.1007/s00371-013-0859-4
   Luboz V, 2009, VISUAL COMPUT, V25, P827, DOI 10.1007/s00371-009-0312-x
   Masutani Y, 1996, LECT NOTES COMPUT SC, V1131, P161
   Oeltze S, 2005, IEEE T MED IMAGING, V24, P540, DOI 10.1109/TMI.2004.843196
   Orkisz MM, 1997, MAGNET RESON MED, V37, P914, DOI 10.1002/mrm.1910370617
   Pekkan K, 2008, MED BIOL ENG COMPUT, V46, P1139, DOI 10.1007/s11517-008-0377-0
   POMMERT A, 1992, IEEE COMPUT GRAPH, V12, P12
   Preimi B, 2008, MATH VIS, P39, DOI 10.1007/978-3-540-72630-2_3
   Qingqi Hong, 2010, Proceedings of the 2010 IEEE 10th International Conference on Computer and Information Technology (CIT 2010), P1397, DOI 10.1109/CIT.2010.250
   Schumann C., 2007, Proceedings of Joint Eurographics - IEEE VGTC Symposium on Visualization, P283, DOI DOI 10.2312/VISSYM/EUROVIS07/283-290
   Schumann C, 2008, INT J COMPUT ASS RAD, V2, P275, DOI 10.1007/s11548-007-0137-x
   Sethian J., 1999, LEVEL SET METHODS FA
   Tang W, 2010, VISUAL COMPUT, V26, P1157, DOI 10.1007/s00371-010-0442-1
   Wu XL, 2011, MED IMAGE ANAL, V15, P22, DOI 10.1016/j.media.2010.06.006
   Zhang Y, 2008, MEDIVIS 2008: FIFTH INTERNATIONAL CONFERENCE BIOMEDICAL VISUALIZATION - INFORMATION VISUALIZATION IN MEDICAL AND BIOMEDICAL INFORMATICS, PROCEEDINGS, P71, DOI 10.1109/MediVis.2008.12
NR 39
TC 5
Z9 5
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1251
EP 1262
DI 10.1007/s00371-015-1160-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800004
DA 2024-07-18
ER

PT J
AU Wang, RM
   Zhou, F
   Yang, F
AF Wang, Ruomei
   Zhou, Fan
   Yang, Fei
TI Retiling scheme: a novel approach of direct anisotropic quad-dominant
   remeshing
SO VISUAL COMPUTER
LA English
DT Article
DE Remeshing; Retiling; Quadrilateral; Feature preserved
ID MESH SEGMENTATION METHOD; PARAMETERIZATION
AB Remeshing has been an active research topic in Digital Geometry Processing. In this paper, a novel approach of direct anisotropic quad-dominant remeshing is proposed. We apply the retiling method to the particular problem of quad-dominant remeshing. Compared with other methods, this method can simply partition the surface of an original triangular mesh into connected quads with the mesh edges aligning to the principal directions. The first step in this method is to estimate and smooth the curvature tensor field of the surface at the vertices, and then the quad-dominant mesh is obtained by retiling the quad surface so that quadrilateral edges are parallel to the local principal curvature directions. In addition, to preserve the sharp feature information during remeshing processes, the feature lines can be extracted using mesh segmentation method, and the intersections between the feature lines and the orthogonal planes can be found during the process of retiling. A feature fusion process is presented to join the feature edges and feature points into the quad-dominant mesh. The experiment results show that this new remeshing method is simple and easy to implement. The resolution of the quadrilateral mesh can be controlled during the remeshing. It is applicable to arbitrary genus meshes and can generate high-quality quad-dominant mesh.
C1 [Wang, Ruomei; Zhou, Fan; Yang, Fei] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Wang, RM (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM isswrm@mail.sysu.edu.cn
RI Zhou, fan/KIL-4066-2024
FU National Natural Science Foundation of China [61272192, 61379112,
   61432003]
FX We would like to thank the reviewers for their valuable comments. This
   work is supported by the National Natural Science Foundation of China
   (Nos. 61272192, 61379112, 61432003).
CR Alexa M, 2000, VISUAL COMPUT, V16, P26, DOI 10.1007/PL00007211
   Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Alliez P, 2002, ACM T GRAPHIC, V21, P347, DOI 10.1145/566570.566588
   Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   [Anonymous], 2003, PROC 19 ANN S COMPUT, DOI DOI 10.1145/777792.777839
   [Anonymous], 2006, PROC EUROGRAPHICS S
   Attene M, 2003, ACM T GRAPHIC, V22, P982, DOI 10.1145/944020.944022
   Bommes D., 2009, TOG, V09, P137
   Bommes D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531383
   Botsch M, 2001, COMPUT GRAPH FORUM, V20, pC402, DOI 10.1111/1467-8659.00533
   Botsch M., 2002, OpenSG Symp
   Campen M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185606
   Chiang CH, 2011, VISUAL COMPUT, V27, P811, DOI 10.1007/s00371-011-0555-1
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Dong S, 2005, COMPUT AIDED GEOM D, V22, P392, DOI 10.1016/j.cagd.2005.04.004
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Hormann K., 2000, Proceedings of Vision, Modeling and Vizualization, 2000, P153
   Huang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409100
   Kälberer F, 2007, COMPUT GRAPH FORUM, V26, P375, DOI 10.1111/j.1467-8659.2007.01060.x
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P137
   Lavoué G, 2005, COMPUT AIDED DESIGN, V37, P975, DOI 10.1016/j.cad.2004.09.001
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Marinov M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P207, DOI 10.1109/PCCGA.2004.1348351
   Ray N, 2006, ACM T GRAPHIC, V25, P1460, DOI 10.1145/1183287.1183297
   Surazhsky V., 2003, Symposium on Geometry Processing, P20
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Yang F, 2014, APPL MATH SER B, V29, P468, DOI 10.1007/s11766-014-3240-0
NR 33
TC 3
Z9 3
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1179
EP 1189
DI 10.1007/s00371-016-1210-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400010
DA 2024-07-18
ER

PT J
AU Gkaravelis, A
   Papaioannou, G
AF Gkaravelis, Anastasios
   Papaioannou, Georgios
TI Inverse lighting design using a coverage optimization strategy
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Light hierarchy; Lighting optimization; Inverse lighting problem
AB Lighting design is an essential process in computer cinematography, games, architectural design and various other applications for correctly illuminating or highlighting parts of a scene and enhancing storytelling. When targeting specific illumination goals and constraints, this process can be tedious and counter-intuitive even for experienced users and thus automatic, goal-driven methods have emerged for the estimation of a lighting configuration to match the desired result. We present a general automatic approach to such an inverse lighting design problem, where the number of light sources along with their position and emittance are computed given a set of user-specified lighting goals. To this end, we employ a special hierarchical light clustering that operates in the lighting goal coverage domain and overcomes limitations of previous approaches in environments with high occlusion or structural complexity. Our approach is independent of the underlying light transport model and can quickly converge to usable solutions. We validate our results and provide comparative evaluation with the current state of the art.
C1 [Gkaravelis, Anastasios; Papaioannou, Georgios] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
C3 Athens University of Economics & Business
RP Gkaravelis, A (corresponding author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.
EM agkar@aueb.gr; gepap@aueb.gr
RI Papaioannou, Georgios/AAH-9642-2021
OI Papaioannou, Georgios/0000-0003-4774-0746; Gkaravelis,
   Anastasios/0000-0002-9673-2462
CR [Anonymous], 1987, SIAM CLASSICS APPL M
   Castro F, 2012, ENG APPL ARTIF INTEL, V25, P566, DOI 10.1016/j.engappai.2011.11.009
   Castro F, 2009, LECT NOTES COMPUT SC, V5531, P128
   Costa AC, 1999, SPRING EUROGRAP, P317
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   Fernández E, 2012, COMPUT GRAPH-UK, V36, P1096, DOI 10.1016/j.cag.2012.09.003
   Gkaravelis A., 2015, P 10 INT S SMART GRA, P128
   Kawai J. K., 1993, Computer Graphics Proceedings, P147, DOI 10.1145/166117.166136
   Lin WC, 2013, COMPUT GRAPH FORUM, V32, P133, DOI 10.1111/cgf.12159
   Lon V., 2014, PACIFIC GRAPHICS SHO
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Patow G, 2003, COMPUT GRAPH FORUM, V22, P663, DOI 10.1111/j.1467-8659.2003.00716.x
   PELLACINI F., 2007, ACM T GRAPH, V26, P2
   Poulin P., 1997, Proceedings. Computer Graphics International (Cat. No.97TB100104), P56, DOI 10.1109/CGI.1997.601272
   Poulin Pierre., 1992, Proc. of I3D, P31, DOI DOI 10.1145/147156.147160
   Schmidt T.-W., 2014, EUROGRAPHICS 2014 ST
   Schoeneman C., 1993, Computer Graphics Proceedings, P143, DOI 10.1145/166117.166135
   Schwarz M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629573
   Shacked R., 2001, COMP GRAPH FORUM
NR 19
TC 6
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 771
EP 780
DI 10.1007/s00371-016-1237-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600010
DA 2024-07-18
ER

PT J
AU Hou, XN
   Ding, SH
   Ma, LZ
   Wang, CJ
   Li, JL
   Huang, FY
AF Hou, Xiao-Nan
   Ding, Shou-Hong
   Ma, Li-Zhuang
   Wang, Cheng-Jie
   Li, Ji-Lin
   Huang, Fei-Yue
TI Similarity metric learning for face verification using sigmoid decision
   function
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Face verification; Sigmoid function; Similarity metric learning;
   Mahalanobis distance; Bilinear similarity
AB In this paper, we consider the face verification problem, which is to determine whether two face images belong to the same subject or not. Although many research efforts have been focused on this problem, it still remains a challenging problem due to large intra-personal variations in imaging conditions, such as illumination, pose, expression, and occlusion. Our proposed method is based on the idea that we would like the similarity between positive pairs larger than negative pairs, and obtain a similarity estimation of two images. We construct our decision function by incorporating bilinear similarity and Mahalanobis distance to the sigmoid function. The constructed decision function makes our method discriminative for inter-personal differences and invariant to intra-personal variations such as pose/lighting/expression. What is more, our formulated objective function is convex, which guarantees global minimum. Our method belongs to nonlinear metric which is more robust to handle heterogeneous data than linear metric. We evaluate our proposed verification method on the challenging labeled faces in the wild (LFW) database. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods such as Joint Bayesian under the unrestricted setting of LFW.
C1 [Hou, Xiao-Nan; Ding, Shou-Hong] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Ma, Li-Zhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Digital Media Technol & Data Reconstruct Lab, Shanghai, Peoples R China.
   [Wang, Cheng-Jie; Li, Ji-Lin; Huang, Fei-Yue] Tencent Inc, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Tencent
RP Hou, XN (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM xnhou1989@gmail.com; dingsh1987@gmail.com; ma-lz@cs.sjtu.edu.cn
RI Sun, Peng/KDO-4243-2024
FU Tencent; Shanghai Jiao Tong University; National Basic Research 973
   Program of China [2011CB302203]; National Natural Science Foundation of
   China [61133009]; Science and Technology Commission of Shanghai
   Municipality [13511505000]; Tencent; Shanghai Jiao Tong University;
   National Basic Research 973 Program of China [2011CB302203]; National
   Natural Science Foundation of China [61133009]; Science and Technology
   Commission of Shanghai Municipality [13511505000]
FX This work is supported by a joint project of Tencent and Shanghai Jiao
   Tong University. It is also partially supported by the National Basic
   Research 973 Program of China under Grant No. 2011CB302203, the National
   Natural Science Foundation of China under Grant No. 61133009, the
   Science and Technology Commission of Shanghai Municipality Program under
   Grant No. 13511505000.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2006, P 2006 IEEE COMPUTER, DOI DOI 10.1109/CVPR.2006.167
   [Anonymous], VIS COMPUT
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Bellet A., 2013, CoRR
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Cox D., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Davis J. V., 2007, ICML, P209
   Goldberger J., 2004, P INT C NEUR INF PRO, V17, P513
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33
   HOSMER DW, 2000, WILEY PS TX, P1, DOI 10.1002/0471722146
   Jain P., 2008, NIPS, P761, DOI 10.5555/2981780.2981875
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104
   Li YL, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4856-z
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X
   Nguyen Hieu V., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P709, DOI 10.1007/978-3-642-19309-5_55
   Noh YK, 2010, ADV NEURAL INFORM PR, V23, P1822
   Qi Guo-Jun., 2009, ICML, P106
   Shalev-Shwartz S., 2004, ICML, P94, DOI DOI 10.1145/1015330.1015376
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Swami MSSKR, 2013, J COMPUT SCI TECH-CH, V28, P322, DOI 10.1007/s11390-013-1333-5
   Taigman Y., 2009, British Machine Vision Conference, P1
   Torresani Lorenzo., 2006, ADV NEURAL INFORM PR, P1385
   Vieira TF, 2014, VISUAL COMPUT, V30, P1333, DOI 10.1007/s00371-013-0884-3
   Wang XG, 2004, IEEE T PATTERN ANAL, V26, P1222, DOI 10.1109/TPAMI.2004.57
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Weinberger K.Q., 2008, Proceedings of the 25th international conference on Machine learning, P1160, DOI DOI 10.1145/1390156.1390302
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Ying YM, 2012, J MACH LEARN RES, V13, P1
NR 41
TC 10
Z9 10
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 479
EP 490
DI 10.1007/s00371-015-1079-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200006
DA 2024-07-18
ER

PT J
AU Hou, F
   Qin, H
   Qi, Y
AF Hou, Fei
   Qin, Hong
   Qi, Yue
TI Procedure-based component and architecture modeling from a single image
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DOR Workshop
CY APR 06, 2014
CL Strasbourg, FRANCE
DE Architectural modeling; Image-based modeling; Procedural modeling
AB This paper advocates a new component-aware framework to reconstruct 3D architecture from a single image. Different from existing work, our motivation is to obtain a complete set of semantically correct 3D architectural components, which enables part reusability towards rapid model reproduction and facilitates model variation. The core of our system is a novel algorithm to adaptively segment repeated curved stripes (e. g., roof tiles, building floors) into individual elements, based on which 3D dimensions as well as architectural components are derived from a single image. Specially for Chinese architectures, we further devise an interactive method to identify outer columns based on userspecified inner columns. Finally, 3D components are generated and shape rules are derived, from which the buildings and their variants are constructed. Our newcomponent-aware framework minimizes the use of data resource (i. e., one single image) and emphasizes component utility during rapid 3D architecture reproduction by advocating a component-aware approach.
C1 [Hou, Fei; Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Hou, Fei] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Qin, Hong] SUNY Stony Brook, NYU, Dept Comp Sci, New York, NY USA.
C3 Beihang University; Nanyang Technological University; State University
   of New York (SUNY) System; State University of New York (SUNY) Stony
   Brook; New York University
RP Hou, F (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Hou, F (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM houfei@buaa.edu.cn; qin@cs.sunysb.edu; qy@vrlab.buaa.edu.cn
RI qi, yue/KLE-0386-2024
NR 0
TC 13
Z9 14
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2016
VL 32
IS 2
BP 151
EP 166
DI 10.1007/s00371-015-1061-7
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FU
UT WOS:000371667200002
DA 2024-07-18
ER

PT J
AU Duan, Q
   Cai, JF
   Zheng, JM
AF Duan, Qi
   Cai, Jianfei
   Zheng, Jianmin
TI Compressive environment matting
SO VISUAL COMPUTER
LA English
DT Article
DE Picture/image generation; Modeling and recovery of physical attributes
ID ROBUST; MRI
AB The existing high-quality environment matting methods usually require the capturing of a few thousand sample images and spend a few hours in data acquisition. In this paper, a novel environment matting algorithm is proposed to capture and extract the environment matte data effectively and efficiently. First, the recently developed compressive sensing theory is incorporated to reformulate the environment matting problem and simplify the data acquisition process. Next, taking into account the special properties of light refraction and reflection effects of transparent objects, two advanced priors, group clustering and Gaussian priors, as well as other basic constraints are introduced during the matte data recovery process to combat with the limited image samples, suppress the effects of the measurement noise resulted from data acquisition, and faithfully recover the sparse environment matte data. Compared with most of the existing environment matting methods, our algorithm significantly simplifies and accelerates the environment matting extraction process while still achieving high-accurate composition results.
C1 [Duan, Qi; Cai, Jianfei; Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Cai, Jianfei] Nanyang Technol Univ, Sch Comp Engn, Visual & Interact Comp Div, Singapore 639798, Singapore.
   [Cai, Jianfei] Nanyang Technol Univ, Sch Comp Engn, Comp Commun Div, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University
RP Cai, JF (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM duanqi1983@gmail.com; asjfcai@ntu.edu.sg; asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Cai, Jianfei/A-3691-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Cai, Jianfei/0000-0002-9444-3763
CR [Anonymous], ACM T GR
   [Anonymous], 2005, P 16 EUROGRAPHICS C
   [Anonymous], 2011, 2011 IEEE INT C MULT
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chuang YY, 2000, COMP GRAPH, P121, DOI 10.1145/344779.344844
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Fuchs M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243984
   Garg Gaurav, 2006, Eurographics Symposium on Rendering (EGSR'06), P251
   Goyal VK, 2008, IEEE SIGNAL PROC MAG, V25, P48, DOI 10.1109/MSP.2007.915001
   Gu JW, 2008, LECT NOTES COMPUT SC, V5305, P845
   Haldar JP, 2011, IEEE T MED IMAGING, V30, P893, DOI 10.1109/TMI.2010.2085084
   Huang JZ, 2009, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2009.5459202
   Ihrke L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451,1239510
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Masselus V, 2003, ACM T GRAPHIC, V22, P613, DOI 10.1145/882262.882315
   Matusik W., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P267
   Matusik W., 2004, Eurographics Symposium on Rendering (EGSR'04), V1, P299
   Peers P., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P157
   Peyré G, 2010, IEEE T SIGNAL PROCES, V58, P2613, DOI 10.1109/TSP.2010.2042490
   Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257
   Sen P, 2011, IEEE T VIS COMPUT GR, V17, P487, DOI 10.1109/TVCG.2010.46
   Sen P, 2010, COMPUT GRAPH FORUM, V29, P1355, DOI 10.1111/j.1467-8659.2010.01731.x
   Sen P, 2009, COMPUT GRAPH FORUM, V28, P609, DOI 10.1111/j.1467-8659.2009.01401.x
   Wakin MB, 2006, IEEE IMAGE PROC, P1273, DOI 10.1109/ICIP.2006.312577
   Wexler Y., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P279
   Xu WW, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531341
   Zhang ST, 2012, MED IMAGE ANAL, V16, P265, DOI 10.1016/j.media.2011.08.004
   Zhu JY, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P402
   Zongker DE, 1999, COMP GRAPH, P205, DOI 10.1145/311535.311558
NR 34
TC 5
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1587
EP 1600
DI 10.1007/s00371-014-1035-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900002
DA 2024-07-18
ER

PT J
AU Kim, J
   Park, H
   Lee, J
   Kwon, T
AF Kim, Jongmin
   Park, Hwangpil
   Lee, Jehee
   Kwon, Taesoo
TI Human motion control with physically plausible foot contact models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Physics-based simulation; Character animation; Data-driven animation
ID INTERACTIVE SIMULATION
AB The foot-to-ground contact model plays an important role in the simulation of highly dynamic motions, such as turns and kicks. In this paper, we propose a method for solving dynamically cumbersome slipping contact problems, which are frequently observed in highly dynamic motions. We employ and modify a combination of two different types of cones representing the inequality constraints of a contact model: the friction cone and the velocity cone. The friction cone makes character animation physically plausible while the velocity cone allows a character to perform a sharp turn without foot-to-ground penetration. Our system effectively simulates human behavior using an inverted pendulum on a cart (IPC) model and motion capture data. In the preprocessing step, we analyze motion capture data to extract meaningful information for the IPC model. At run-time, our system produces a physically simulated character by tracking the desired motion that is predicted by the IPC model. We formulate human motion control as a quadratic programming satisfying the proposed foot-to-ground contact constraints. Our examples show that the proposed system can produce physically plausible character animation without noticeable foot-to-ground contact artifacts.
C1 [Kim, Jongmin; Kwon, Taesoo] Hanyang Univ, Div Comp Sci & Software, Seoul 133791, South Korea.
   [Park, Hwangpil; Lee, Jehee] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Hanyang University; Seoul National University (SNU)
RP Kwon, T (corresponding author), Hanyang Univ, Div Comp Sci & Software, Seoul 133791, South Korea.
EM taesoobear@gmail.com
RI Lee, Jehee/V-7545-2019
FU National Research Foundation of Korea (NRF) - Ministry of Science, ICT
   and Future Planning (MSIP) [NRF-2014R1A1A1038386]
FX We thank to the anonymous reviewers for their helpful comments. This
   research was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT and Future Planning (MSIP) (NRF-2014R1A1A1038386).
CR Abe Y, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P249
   BARAFF D, 1995, IEEE COMPUT GRAPH, V15, P63, DOI 10.1109/38.376615
   Baraff D., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P23, DOI 10.1145/192161.192168
   Baraff D., 1989, Computer Graphics, V23, P223, DOI 10.1145/74334.74356
   Baraff D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P137, DOI 10.1145/237170.237226
   BARAFF D, 1993, ALGORITHMICA, V10, P292, DOI 10.1007/BF01891843
   da Silva M, 2008, COMPUT GRAPH FORUM, V27, P371, DOI 10.1111/j.1467-8659.2008.01134.x
   da Silva M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360681
   de Lasa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781157
   Dorato P., 1994, LINEAR QUADRATIC CON
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Geijtenbeek T., 2011, EUROGRAPHICS STATE A, V2
   Glardon P, 2006, VISUAL COMPUT, V22, P194, DOI 10.1007/s00371-006-0376-9
   Hodgins J. K., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P153, DOI 10.1145/258734.258822
   Hwang J, 2014, COMPUT GRAPH FORUM, V33, P21, DOI 10.1111/cgf.12470
   Kajita S, 2004, IEEE INT CONF ROBOT, P629, DOI 10.1109/ROBOT.2004.1307219
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L., 2002, ACM SIGGR S COMP AN
   Kwon Taesoo, 2010, P 2010 ACM SIGGRAPHE, P129
   Lee Jehee, 1999, P 26 ANN C COMP GRAP
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Lee Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661233
   Macchietto A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531386
   Mirtich B., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P181, DOI 10.1145/199404.199436
   Mirtich B., 1994, Impulse-based dynamic simulation
   Mordatch I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778808
   Muico U, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531387
   Prazˇak M., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P287
   Sok KW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276511, 10.1145/1239451.1239558]
   Sugihara T, 2008, IEEE INT CONF ROBOT, P1264
   Wu JC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778809
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
NR 32
TC 1
Z9 1
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 883
EP 891
DI 10.1007/s00371-015-1097-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500014
DA 2024-07-18
ER

PT J
AU Roe, E
   de Mello, CAB
AF Roe, Edward
   Barros de Mello, Carlos Alexandre
TI Restoring images of ancient color postcards
SO VISUAL COMPUTER
LA English
DT Article
DE Color image processing; Color restoration; Color constancy; Image
   restoration; Edge detection; Difference of Gaussians
ID RETINEX
AB This paper proposes an automatic system for the restoration of digital images of vintage colored postcards, employing the combined techniques of image equalization, background segmentation based on edge detection (using an extension of the standard difference of Gaussians filter), color enhancement, and noisy spots removal. Equalization and background segmentation are used to facilitate background spot removal. To enhance colors, the overall document degradation is regarded as an illumination problem, thereby allowing the use of color constancy algorithms. The methodology was applied to a set of postcards dating from the end of the nineteenth century, and satisfactory visual results were achieved.
C1 [Roe, Edward; Barros de Mello, Carlos Alexandre] Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
C3 Universidade Federal de Pernambuco
RP de Mello, CAB (corresponding author), Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
EM er@cin.ufpe.br; cabm@cin.ufpe.br
OI Mello, Carlos/0000-0002-0594-3345
CR [Anonymous], 2011, IRACE PACKAGE ITERAT
   [Anonymous], 2011, Analysis, Restoration, and Reconstruction of Ancient Artworks
   [Anonymous], 1992, R. woods digital image processing
   [Anonymous], 2000, WILEY SERIES PURE AP
   [Anonymous], 2007, Color Constancy
   ASTOLA J, 1989, IEEE T CIRCUITS SYST, V36, P1430, DOI 10.1109/31.41299
   Bandeira D, 2010, VISUAL COMPUT, V26, P965, DOI 10.1007/s00371-010-0495-1
   Beckwith T. D., 1940, Publ. Univ. Calif., biol Sci., V1, P299
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Burger W., 2008, DIGITAL IMAGE PROCES
   Cui M, 2010, VISUAL COMPUT, V26, P1349, DOI 10.1007/s00371-009-0412-7
   Ebner M, 2003, LECT NOTES COMPUT SC, V2781, P60
   Fan Jian, 2007, CBDAR07, P87
   Finlayson G. D., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P475, DOI 10.1007/BFb0055685
   Foley J.D., 1995, COMPUTER GRAPHICS PR, V2nd
   Funt B, 2004, J ELECTRON IMAGING, V13, P48, DOI 10.1117/1.1636761
   Jian Fan, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P561, DOI 10.1109/ICDAR.2009.111
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LAND EH, 1964, AM SCI, V52, P247
   Manso M, 2009, ANAL BIOANAL CHEM, V395, P2029, DOI 10.1007/s00216-009-3142-9
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   MELLO C. A. B., 2012, DIGITAL DOCUMENT ANA
   Porck H.J., 2001, PRESERVATION SCI SUR
   Qingyun Yang, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P239, DOI 10.1109/CMSP.2011.55
   Rizzi A, 2003, PATTERN RECOGN LETT, V24, P1663, DOI 10.1016/S0167-8655(02)00323-9
   Roe E., 2013, P INT C DOC AN REC W
   Roe E, 2012, IEEE SYS MAN CYBERN, P451, DOI 10.1109/ICSMC.2012.6377765
   Stanco F., 2004, P EUR SIGN PROC C EU
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Winnemoller H., 2011, P ACM SIGGRAPH EUR S, P147
   Wu JL, 2012, VISUAL COMPUT, V28, P723, DOI 10.1007/s00371-012-0683-2
   Yin L, 1996, IEEE T CIRCUITS-II, V43, P157, DOI 10.1109/82.486465
   Zhu W, 2014, VISUAL COMPUT, V30, P299, DOI 10.1007/s00371-013-0854-9
NR 35
TC 5
Z9 5
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 627
EP 641
DI 10.1007/s00371-014-0988-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA CH2CF
UT WOS:000353831400010
DA 2024-07-18
ER

PT J
AU Zhan, J
   Su, Z
   Wu, HF
   Luo, XN
AF Zhan, Jin
   Su, Zhuo
   Wu, Hefeng
   Luo, Xiaonan
TI Robust tracking via discriminative sparse feature selection
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Sparse representation; Template dictionary;
   Discriminative sparse feature
ID VISUAL TRACKING; OBJECT TRACKING; MODELS
AB In this paper, we propose a novel generative tracking approach based on discriminative sparse feature selection. The sparse features are the discriminative sparse representation of samples, which are achieved by learning a compact and discriminative dictionary. Besides the target templates, the proposed approach also incorporates the close-background templates to approximate the partial variations. We learn the dictionary and a classifier together, and search the tracking result with the maximum similarity and the minimal reconstruction error criterion using the discrimination of sparse features. In addition, we resample the close-background templates and update the dictionary in an adaptive way during tracking. Experimental results on several challenging video sequences demonstrate that the proposed approach has more favorable performance than the state-of-the-art approaches.
C1 [Zhan, Jin; Su, Zhuo; Wu, Hefeng; Luo, Xiaonan] Sun Yat Sen Univ, Sch Informat Sci & Technol, Natl Engn Res Ctr Digital Life, State Prov Joint Lab Digital Home Interact Applic, Guangzhou 510006, Guangdong, Peoples R China.
   [Su, Zhuo] Sun Yat Sen Univ, Inst Dongguan, Dongguan 523000, Peoples R China.
   [Zhan, Jin] Guangdong Polytech Normal Univ, Inst Comp Sci, Guangzhou 510665, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Guangdong Polytechnic
   Normal University
RP Su, Z (corresponding author), Sun Yat Sen Univ, Inst Dongguan, Dongguan 523000, Peoples R China.
EM jinerzhan@gmail.com; suzhuoi@gmail.com; wuhefeng@gmail.com;
   lnslxn@mail.sysu.edu.cn
RI Su, Zhuo/AAO-4506-2020
OI Su, Zhuo/0000-0002-6090-0110
FU NSFC-Guangdong Joint Fund [U1135005, U1201252]; National Natural Science
   Foundation of China [61320106008]; Science and Technology Planning
   Project of Guangdong Province [2012B010900009, 2012B010900089]
FX This research is supported by NSFC-Guangdong Joint Fund (No. U1135005,
   U1201252), the National Natural Science Foundation of China (No.
   61320106008), Science and Technology Planning Project of Guangdong
   Province (No. 2012B010900009, 2012B010900089).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587652
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hu WM, 2011, INT J COMPUT VISION, V91, P303, DOI 10.1007/s11263-010-0399-6
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li Y, 2008, IEEE T PATTERN ANAL, V30, P1728, DOI 10.1109/TPAMI.2008.73
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Mei X, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1818
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Pang Y, 2013, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2013.346
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Porikli F., 2006, 2006 IEEE COMPUTER S, V1, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]
   Quan W, 2014, VISUAL COMPUT, V30, P351, DOI 10.1007/s00371-013-0860-y
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Tang F, 2007, IEEE I CONF COMP VIS, P992
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wu HF, 2014, VISUAL COMPUT, V30, P229, DOI 10.1007/s00371-013-0823-3
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
NR 35
TC 11
Z9 12
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 575
EP 588
DI 10.1007/s00371-014-0984-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400006
DA 2024-07-18
ER

PT J
AU Wu, H
   Miao, ZJ
   Wang, Y
   Lin, MN
AF Wu, Hao
   Miao, Zhenjiang
   Wang, Yi
   Lin, Manna
TI Optimized recognition with few instances based on semantic distance
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic distance; Object recognition; GIST; SIFT; AP value; AUC value
ID IMAGE RETRIEVAL; OBJECT CLASSES; FEATURES; CLASSIFICATION
AB In this paper, we present a new object recognition model with few instances based on semantic distance. Learning objects with many instances have been studied in computer vision for many years. However, in many cases, not enough positive instances occur, especially for some special categories. We must take full advantage of all instances, including those that do not belong to the category. The main insight is that, given a few positive instances from one category, we can define some other candidate instances as positive instances based on semantic distance to learn this model. Our model responds more strongly to instances with closer semantic distance to positive instances than to instances with farther semantic distance to positive instances. We use a regularized kernel machine algorithm to train the images from the database. The superiority of our method to existing object recognition methods is demonstrated. Experiments using an image database show that our method not only reduces the number of learning instances but also keeps the accurate rate of recognition.
C1 [Wu, Hao; Miao, Zhenjiang; Wang, Yi; Lin, Manna] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
   [Wang, Yi] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
C3 Beijing Jiaotong University; Carnegie Mellon University
RP Wu, H (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
EM 10112056@bjtu.edu.cn
FU National Key Technology R&D Program of China [2012BAH01F03]; National
   Basic Research (973) Program of China [2011CB302203]; research fund of
   Tsinghua - Tencent Joint Laboratory for Internet Innovation Technology
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work is supported by the National Key Technology
   R&D Program of China (2012BAH01F03), National Basic Research (973)
   Program of China (2011CB302203), research fund of Tsinghua - Tencent
   Joint Laboratory for Internet Innovation Technology.
CR An SJ, 2007, PATTERN RECOGN, V40, P2154, DOI 10.1016/j.patcog.2006.12.015
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587658
   [Anonymous], 10 IEEE INT C COMP V
   [Anonymous], P INT C CONT BAS IM
   [Anonymous], 2011, IMAGE CLASSIFICATION
   [Anonymous], P 21 ACM INT C INF K
   [Anonymous], 12 INT C DOC AN REC
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], KDD
   [Anonymous], INT C ADV COMP COMP
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], ACM T GRAPH
   Bart E, 2005, PROC CVPR IEEE, P672
   Bart E., 2008, P IEEE C COMPUTER VI, P1
   Chen Wu, 2010, 2010 2nd International Workshop on Education Technology and Computer Science (ETCS), P452, DOI 10.1109/ETCS.2010.404
   Choi JH, 2003, LECT NOTES COMPUT SC, V2667, P79
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felzenszwalb P., 2008, P IEEE C COMPUTER VI
   Frome A., 2007, P IEEE INT C COMPUTE
   Jacobs DW, 2000, IEEE T PATTERN ANAL, V22, P583, DOI 10.1109/34.862197
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203
   Palatucci M, 2009, Advances in neural information processing systems, P22
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sudderth Erik., 2005, ADV NEURAL INFORM PR, V18, P1297
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   van de Weijer J., 2007, P IEEE C COMPUTER VI, P1
   Wang G, 2009, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2009.5459194
   Wang G, 2010, PROC CVPR IEEE, P3525, DOI 10.1109/CVPR.2010.5539955
   Weinberger K., 2006, [No title captured], P1437
   Zha Zheng-Jun, 2008, CVPR, P1
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zheng YT, 2009, VISUAL COMPUT, V25, P13, DOI 10.1007/s00371-008-0294-0
NR 38
TC 25
Z9 25
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 367
EP 375
DI 10.1007/s00371-014-0931-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600001
DA 2024-07-18
ER

PT J
AU Martinek, M
   Grosso, R
   Greiner, G
AF Martinek, Michael
   Grosso, Roberto
   Greiner, Guenther
TI Interactive partial 3D shape matching with geometric distance
   optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Shape matching; Partial matching; Part-in-whole matching; Object
   alignment
ID OPTIMAL ROTATION ALIGNMENT; REGISTRATION; SIMILARITY; OBJECTS; RETRIEVAL
AB In this paper, we propose an efficient method for partial 3D shape matching based on minimizing the geometric distance between the source and the target geometry. Unlike existing methods, our method does not use a feature-based distance in order to obtain a matching score. Instead, we use a fast, GPU-based method to approximate the true geometric distance between the source and the target by rendering the source object into a distance field which was built around the target. This function behaves smoothly in the space of transformations and allows for an efficient gradient-based local optimization. In order to overcome local minima, we use single point correspondences between surface points on the source and the target respectively employing simple, yet efficient local features based on the distribution of normal vectors around a reference point. The best correspondences define starting positions for a local optimization. The high efficiency of the distance computation allows for robust determination of the global minima in less than a second, which makes our method usable in interactive applications. Our method works for any kind of input data since it only requires point data with normal information at each point. We also demonstrate the capability of our algorithm to perform global alignment of similar 3D objects.
C1 [Martinek, Michael; Grosso, Roberto; Greiner, Guenther] Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Martinek, M (corresponding author), Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
EM michael.martinek@fau.de; grosso@fau.de; greiner@fau.de
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Alt H, 2003, ALGORITHM COMBINAT, V25, P65
   [Anonymous], P 4 EUR S GEOM PROC
   [Anonymous], 2004, TR04004 U N CAR
   Attene M, 2011, VISUAL COMPUT, V27, P991, DOI 10.1007/s00371-011-0622-7
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bronstein AM, 2009, INT J COMPUT VISION, V84, P163, DOI 10.1007/s11263-008-0147-3
   Bronstein E.M., 2008, P EUR C COMP VIS ECC, P143
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Johan H, 2011, VISUAL COMPUT, V27, P565, DOI 10.1007/s00371-011-0590-y
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M, 2007, IEEE T PATTERN ANAL, V29, P1221, DOI 10.1109/TPAMI.2007.1032
   Linda S.R.C., 2003, P INT C COMP VIS ICC, P1126
   Liu Y, 2006, P IEEE C COMP VIS PA
   Martinek M., 2011, P 16 INT WORKSH VIS, P121, DOI [10.2312/PE/VMV/VMV11/121-128, DOI 10.2312/PE/VMV/VMV11/121-128]
   Martinek M., 2012, P VMV 2012 VIS MOD V, P167
   Martinek M, 2009, COMPUT GRAPH-UK, V33, P291, DOI 10.1016/j.cag.2009.03.023
   Mitra Niloy J., 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P22, DOI DOI 10.1145/1057432.1057435
   Qiu DY, 2009, LECT NOTES COMPUT SC, V5815, P194
   Schnabel R, 2008, WSCG 2008, FULL PAPERS, P65
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Zabatani A., 2012, P 5 EUR C 3D OBJ RET, P17
NR 25
TC 12
Z9 14
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 223
EP 233
DI 10.1007/s00371-014-1040-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800010
DA 2024-07-18
ER

PT J
AU Vieira, TF
   Bottino, A
   Laurentini, A
   De Simone, M
AF Vieira, Tiago F.
   Bottino, Andrea
   Laurentini, Aldo
   De Simone, Matteo
TI Detecting siblings in image pairs
SO VISUAL COMPUTER
LA English
DT Article
DE Kinship verification; Siblings; Support vector machines; Feature
   selection
ID KIN RECOGNITION SIGNALS; SELECTION; FEATURES; KINSHIP
AB In everyday life, face similarity is an important kinship clue. Computer algorithms able to infer kinship from pairs of face images could be applied in forensics, image retrieval and annotation, and historical studies. So far, little work in this area has been presented, and only one study, using a small set of low quality images, tackles the problem of identifying siblings pairs. The purpose of our paper is to present a comprehensive investigation on this subject, aimed at understanding which are, on the average, the most relevant facial features, how effective can be computer algorithms for detecting siblings pairs, and if they can outperform human evaluation. To avoid problems due to low quality pictures and uncontrolled imaging conditions, as for the heterogeneous datasets collected for previous researches, we prepared a database of high quality pictures of sibling pairs, shot in controlled conditions and including frontal, profile, expressionless, and smiling faces. Then we constructed various classifiers of image pairs using different types of facial data, based on various geometric, textural, and holistic features. The classifiers were first tested separately, and then the most significant facial data, selected with a two stage feature selection algorithm were combined into a unique classifier. The discriminating ability of the automatic classifier combining features of different nature has been found to outperform that of a panel of human raters. We also show the good generalization capabilities of the algorithm by applying the classifier, in a cross-database experiment, to a low quality database of images collected from the Internet.
C1 [Vieira, Tiago F.; Bottino, Andrea; Laurentini, Aldo; De Simone, Matteo] Politecn Torino, DAUIN, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Bottino, A (corresponding author), Politecn Torino, DAUIN, Cso Duca Abruzzi 24, I-10129 Turin, Italy.
EM tiago.figueiredo@polito.it; andrea.bottino@polito.it;
   aldo.laurentini@hotmail.it; matteo.desimone@polito.it
RI Bottino, Andrea/F-4509-2012; Vieira, Tiago/JJE-5497-2023; Figueiredo,
   Tiago F/A-7740-2018
OI Bottino, Andrea/0000-0002-8894-5089; Vieira, Tiago/0000-0002-5202-2477
CR [Anonymous], 2006, P IEEE COMPUTER SOC, DOI DOI 10.1109/CVPR.2006.95
   [Anonymous], P 20 INT C GRAPH ENG
   [Anonymous], P ACCV2012 WORKSH LB
   Bailenson JN, 2008, PUBLIC OPIN QUART, V72, P935, DOI 10.1093/poq/nfn064
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Bottino A., 2012, Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods. ICPRAM 2012, P405
   Bottino Andrea, 2008, WSEAS Transactions on Computers, V7, P1250
   Bottino A, 2010, LECT NOTES COMPUT SC, V6111, P425, DOI 10.1007/978-3-642-13772-3_43
   Chang C.C., LIBSVM: a library for support vector machines
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dal Martello MF, 2006, J VISION, V6, P1356, DOI 10.1167/6.12.2
   DeBruine LM, 2009, VISION RES, V49, P38, DOI 10.1016/j.visres.2008.09.025
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   GmbH C., FACEVACS SDK VERS 8
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Guo GD, 2012, IEEE T INSTRUM MEAS, V61, P2322, DOI 10.1109/TIM.2012.2187468
   HAMILTON WD, 1964, J THEOR BIOL, V7, P1, DOI 10.1016/0022-5193(64)90039-6
   Kaminski G, 2009, P ROY SOC B-BIOL SCI, V276, P3193, DOI 10.1098/rspb.2009.0677
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Milborrow S., ACTIVE SHAPE MODELS
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Ng A.Y., 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Park JH, 2008, REV GEN PSYCHOL, V12, P215, DOI 10.1037/1089-2680.12.3.215
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Shao M., 2011, CVPR 2011 WORKSH, P60, DOI DOI 10.1109/CVPRW.2011.5981801
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   Somanath G., 2012, IEEE INT C BIOM THEO
   Somanath G, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130517
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410
   Wolf L, 2005, PROC CVPR IEEE, P801
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Zhang L, 2006, VISUAL COMPUT, V22, P43, DOI 10.1007/s00371-005-0352-9
NR 37
TC 55
Z9 59
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1333
EP 1345
DI 10.1007/s00371-013-0884-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400003
DA 2024-07-18
ER

PT J
AU Ricks, BC
   Egbert, PK
AF Ricks, Brian C.
   Egbert, Parris K.
TI Optimal acceleration thresholds for non-holonomic agents
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Non-holonomic agents; Optimization; Crowd simulation
ID CURVATURE; MOTION
AB Finding optimal trajectories for non-accelerating, non-holonomic agents is a well-understood problem. However, in video games, robotics, and crowd simulations non-holonomic agents start and stop frequently. With the vision of improving crowd simulation, we find optimal paths for virtual agents accelerating from a standstill. These paths are designed for the "ideal", initial stage of planning when obstacles are ignored. We analytically derive paths and arrival times using arbitrary acceleration angle thresholds. We use these paths and arrival times to find an agent's optimal ideal path. We then numerically calculate the decision surface that can be used by an application at run-time to quickly choose the optimal path. Finally, we use quantitative error analysis to validate the accuracy of our approach.
C1 [Ricks, Brian C.] Rutgers State Univ, CCICADA Ctr, New Brunswick, NJ 08903 USA.
   [Egbert, Parris K.] Brigham Young Univ, Provo, UT 84602 USA.
C3 Rutgers University System; Rutgers University New Brunswick; Brigham
   Young University
RP Ricks, BC (corresponding author), Rutgers State Univ, CCICADA Ctr, New Brunswick, NJ 08903 USA.
EM brian.ricks@rutgers.edu; egbert@cs.byu.edu
RI Ricks, Brian/AAG-9889-2020
CR Arechavaleta G, 2008, IEEE T ROBOT, V24, P5, DOI 10.1109/TRO.2008.915449
   Arechavaleta G, 2006, IEEE-RAS INT C HUMAN, P131, DOI 10.1109/ICHR.2006.321374
   COCKAYNE EJ, 1975, SIAM J CONTROL, V13, P197, DOI 10.1137/0313012
   DUBINS LE, 1957, AM J MATH, V79, P497, DOI 10.2307/2372560
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hicheur H, 2007, EUR J NEUROSCI, V26, P2376, DOI 10.1111/j.1460-9568.2007.05836.x
   LACQUANITI F, 1983, ACTA PSYCHOL, V54, P115, DOI 10.1016/0001-6918(83)90027-6
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Soueres P, 1996, IEEE T AUTOMAT CONTR, V41, P672, DOI 10.1109/9.489204
   Thalmann Daniel., 2007, CROWD SIMULATION
   Todorov E, 1998, J NEUROPHYSIOL, V80, P696, DOI 10.1152/jn.1998.80.2.696
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
NR 15
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 579
EP 589
DI 10.1007/s00371-014-0972-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700003
DA 2024-07-18
ER

PT J
AU Yu, Z
   Yu, X
   Thorpe, C
   Grauer-Gray, S
   Li, F
   Yu, JY
AF Yu, Zhan
   Yu, Xuan
   Thorpe, Christopher
   Grauer-Gray, Scott
   Li, Feng
   Yu, Jingyi
TI Racking focus and tracking focus on live video streams: a stereo
   solution
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic depth of field; Racking focus; Tracking focus; Belief
   propagation; Cross bilateral Filtering; Light field; CUDA
ID CAMERA
AB The ability to produce dynamic Depth of Field effects in live video streams was until recently a quality unique to movie cameras. In this paper, we present a computational camera solution coupled with real-time GPU processing to produce runtime dynamic Depth of Field effects. We first construct a hybrid-resolution stereo camera with a high-res/low-res camera pair. We recover a low-res disparity map of the scene using GPU-based Belief Propagation, and subsequently upsample it via fast Cross/Joint Bilateral Upsampling. With the recovered high-resolution disparity map, we warp the high-resolution video stream to nearby viewpoints to synthesize a light field toward the scene. We exploit parallel processing and atomic operations on the GPU to resolve visibility when multiple pixels warp to the same image location. Finally, we generate racking focus and tracking focus effects from the synthesized light field rendering. All processing stages are mapped onto NVIDIA's CUDA architecture. Our system can produce racking and tracking focus effects for the resolution of 640x480 at 15 fps.
C1 [Yu, Zhan; Yu, Xuan; Thorpe, Christopher; Grauer-Gray, Scott; Li, Feng; Yu, Jingyi] Univ Delaware, Newark, DE 19716 USA.
C3 University of Delaware
RP Yu, Z (corresponding author), Univ Delaware, Newark, DE 19716 USA.
EM zyu@cis.udel.edu; xyu@cis.udel.edu; thorpe@cis.udel.edu;
   grauerg@cis.udel.edu; feli@cis.udel.edu; yu@cis.udel.edu
FU National Science Foundation [IIS-CAREER-0845268, IIS-RI-1016395]; Air
   Force Office of Scientific Research under the YIP Award; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [0845268] Funding Source: National Science Foundation
FX This project was partially supported by the National Science Foundation
   under Grants IIS-CAREER-0845268 and IIS-RI-1016395, and by the Air Force
   Office of Scientific Research under the YIP Award.
CR [Anonymous], SIGGRAPH
   [Anonymous], CVPR
   [Anonymous], CVPR
   [Anonymous], CVPR
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 3 CAN C COMP ROB VIS
   [Anonymous], LIGHT FIELD CAMERA I
   [Anonymous], 2007, CVPR
   [Anonymous], I3D
   [Anonymous], STANFORD TECH REPORT
   [Anonymous], CVPR
   [Anonymous], 2007, SIGGRAPH
   [Anonymous], SIGGRAPH ASIA
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Grauer-Gray S, 2008, 2008 IAPR WORKSHOP ON PATTERN RECOGNITION IN REMOTE SENSING, P21
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P453, DOI 10.1109/TVCG.2008.106
   Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Sawhney HS, 2001, COMP GRAPH, P451, DOI 10.1145/383259.383312
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Stroebel Leslie., 1986, Photographic materials and processes
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Wang HM, 2007, IEEE T VIS COMPUT GR, V13, P697, DOI 10.1109/TVCG.2007.1019
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Yang J. C., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P77
   Yu XA, 2010, COMPUT GRAPH FORUM, V29, P2099, DOI 10.1111/j.1467-8659.2010.01797.x
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 28
TC 5
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 45
EP 58
DI 10.1007/s00371-013-0778-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500004
DA 2024-07-18
ER

PT J
AU Huang, CY
   Tai, WK
AF Huang, Chun-Yen
   Tai, Wen-Kai
TI Ting tools: interactive and procedural modeling of Chinese ting
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural modeling; Interactive modeling; Chinese architecture; Ting;
   User guidance
AB We propose an interactive and procedural approach to developing a tool for modeling Chinese tings, a featured Chinese architecture. The procedural approach models a detailed 3D Chinese ting model efficiently. The movement of control-point with user guidance and interaction-clue provides a user-friendly GUI for intuitively constructing variant tings flexibly. Also, the summarized scale proportional relationships between parameters used in ting architecture makes the user's interaction simple yet powerful. The user studies and the experimental results show that nonprofessional users can intuitively construct a variant of complex, novel Chinese tings within minutes.
C1 [Huang, Chun-Yen; Tai, Wen-Kai] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Hualien, Taiwan.
C3 National Dong Hwa University
RP Tai, WK (corresponding author), Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Hualien, Taiwan.
EM nschuang.tw@gmail.com; wktai@mail.ndhu.edu.tw
CR Bai L. - J., 2000, STRUCTURE OFFICIAL B
   Bokeloh M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778841
   Chen XJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356684
   Chen YY, 2002, ACM T GRAPHIC, V21, P630, DOI 10.1145/566570.566628
   DOWNING F, 1981, ENVIRON PLANN B, V8, P269, DOI 10.1068/b080269
   Duarte J., 2002, THESIS MIT
   FLEMMING U, 1987, ENVIRON PLANN B, V14, P323, DOI 10.1068/b140323
   Ganster B., 2007, SPRING C COMPUTER GR, P150
   Hou Y.B., 1997, AESTHETICS CHINESE A
   Jiang NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618459
   KONING H, 1981, ENVIRON PLANN B, V8, P295, DOI 10.1068/b080295
   Liang S. - C., 1985, EXAMPLE CONSTRUCTION
   Liu D. - K., 1993, CONSTRUCTION METHODS
   Ma B.C., 1982, CONSTRUCTION TECHNIQ
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Muller P., 2006, Eurographics Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST), P139
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Prusinkiewicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P351, DOI 10.1145/192161.192254
   PRUSINKIEWICZ P, 1997, HDB FORMAL LANGUAGES, V3
   Prusinkiewicz P., 1995, SIGGRAPH'95 Course Notes, P1
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   STINY G, 1978, ENVIRON PLANN B, V5, P5, DOI 10.1068/b050005
   Stiny G., 1976, PICTORIAL FORMAL ASP
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
NR 25
TC 9
Z9 11
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1303
EP 1318
DI 10.1007/s00371-012-0771-3
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200007
DA 2024-07-18
ER

PT J
AU Rashid, M
   Abu-Bakar, SAR
   Mokji, M
AF Rashid, Munaf
   Abu-Bakar, S. A. R.
   Mokji, Musa
TI Human emotion recognition from videos using spatio-temporal and audio
   features
SO VISUAL COMPUTER
LA English
DT Article
DE Human computer interface (HCI); Multimodal system; Human emotions;
   Support vector machines (SVM); Spatio-temporal features
AB In this paper, we present human emotion recognition systems based on audio and spatio-temporal visual features. The proposed system has been tested on audio visual emotion data set with different subjects for both genders. The mel-frequency cepstral coefficient (MFCC) and prosodic features are first identified and then extracted from emotional speech. For facial expressions spatio-temporal features are extracted from visual streams. Principal component analysis (PCA) is applied for dimensionality reduction of the visual features and capturing 97 % of variances. Codebook is constructed for both audio and visual features using Euclidean space. Then occurrences of the histograms are employed as input to the state-of-the-art SVM classifier to realize the judgment of each classifier. Moreover, the judgments from each classifier are combined using Bayes sum rule (BSR) as a final decision step. The proposed system is tested on public data set to recognize the human emotions. Experimental results and simulations proved that using visual features only yields on average 74.15 % accuracy, while using audio features only gives recognition average accuracy of 67.39 %. Whereas by combining both audio and visual features, the overall system accuracy has been significantly improved up to 80.27 %.
C1 [Rashid, Munaf; Abu-Bakar, S. A. R.; Mokji, Musa] Univ Teknol Malaysia, Fac Elect Engn, Comp Vis Video & Image Proc Lab CVVIP, Skudai 81310, Johor Bahru, Malaysia.
   [Rashid, Munaf] Karachi Inst Econ & Technol, Coll Engn COE, Karachi 75190, Pakistan.
C3 Universiti Teknologi Malaysia
RP Rashid, M (corresponding author), Univ Teknol Malaysia, Fac Elect Engn, Comp Vis Video & Image Proc Lab CVVIP, Skudai 81310, Johor Bahru, Malaysia.
EM smunaf_2002@hotmail.com; syed@fke.utm.my; musa@fke.utm.my
RI Abu-Bakar, S.A.R./G-7318-2016
OI Abu-Bakar, S.A.R./0000-0002-4360-6630; Rashid, Sheikh Muhammad
   Munaf/0000-0003-2063-4513
FU Universiti Teknologi Malaysia (UTM); Ministry of Higher Education (MOHE)
FX Authors would like to thank Universiti Teknologi Malaysia (UTM) and
   Ministry of Higher Education (MOHE) for providing financial assistance.
   Authors are also indebted to Dr. Usman Ullah Sheikh for technical help
   and discussions.
CR [Anonymous], ARCH SCI
   Bhatti MW, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P181
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   Busso C, 2009, IEEE T AUDIO SPEECH, V17, P582, DOI 10.1109/TASL.2008.2009578
   Byun KS, 2004, SICE 2004 ANNUAL CONFERENCE, VOLS 1-3, P2483
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen CY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1469
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Devillers L, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P549
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Ekman P., 2003, UNMASKING FACE GUIDE
   Ekman P., 1978, Facial action coding system
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   Jiang RM, 2010, IEEE T SYST MAN CY C, V40, P676, DOI 10.1109/TSMCC.2010.2050476
   Jones S, 2012, PATTERN RECOGN LETT, V33, P446, DOI 10.1016/j.patrec.2011.05.001
   Kharat Govind Ukhandrao, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P653, DOI 10.1109/ICETET.2008.22
   Lee CC, 2011, SPEECH COMMUN, V53, P1162, DOI 10.1016/j.specom.2011.06.004
   Lee CM, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P737, DOI 10.1109/ICME.2002.1035887
   Lien JJ, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P390, DOI 10.1109/AFGR.1998.670980
   Lien JJJ, 1998, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.1998.698704
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Milgram M.C.J., 2006, 10 INT WORKSH FRONT
   Shahdi Seyed Omid, 2010, 2010 10th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA 2010), P85, DOI 10.1109/ISSPA.2010.5605502
   ten Bosch L, 2003, SPEECH COMMUN, V40, P213, DOI 10.1016/S0167-6393(02)00083-3
   Wang YJ, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P15
NR 26
TC 33
Z9 34
U1 1
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1269
EP 1275
DI 10.1007/s00371-012-0768-y
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200004
DA 2024-07-18
ER

PT J
AU Wang, XH
   Jia, J
   Cai, LH
AF Wang, Xiaohui
   Jia, Jia
   Cai, Lianhong
TI Affective image adjustment with a single word
SO VISUAL COMPUTER
LA English
DT Article
DE Affective image adjustment; Color theme; Art theory
ID COLOR; PROPAGATION
AB We present a complete system that automatically adjusts image color to meet a desired emotion. It will be more convenient for users, especially for non-professional users, to adjust an image with a semantic user input, for example, to make it lovelier. The whole algorithm is fully automatic, without any user interactions, and the inputs are simply the original image and an affective word (e.g. lovely). To achieve this goal, we solve several non-trivial problems. First, in order to find the proper color themes (template of colors) to reflect the expression of the affective word, we exploit the theoretical and empirical concepts in famous art theories and build a color theme-affective word relation model allowing efficient selection of candidate themes. Furthermore, we propose a novel strategy to select the most suitable color theme among the candidates. Second, to adjust image colors, we propose the Radial Basis Functions (RBF) based interpolation method, which is more effective in many scenarios as evidenced in experiments. We also evaluate the system with comprehensive user studies and its capability is confirmed by the results.
C1 [Wang, Xiaohui; Jia, Jia; Cai, Lianhong] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Wang, XH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM wangxh09@mails.tsinghua.edu.cn
RI jia, jia/JKJ-5720-2023
FU National Basic Research Program (973 Program) of China [2011CB302201];
   National Natural, and Science Foundation of China [61003094,
   60931160443]; Tsinghua National Laboratory for Information Science and
   Technology (TNList) Cross-discipline Foundation; Innovation Fund of
   Tsinghua-Tencent Joint Laboratory
FX This work is supported by the National Basic Research Program (973
   Program) of China (2011CB302201), National Natural, and Science
   Foundation of China (61003094, 60931160443). This work is also funded by
   Tsinghua National Laboratory for Information Science and Technology
   (TNList) Cross-discipline Foundation, and supported by the Innovation
   Fund of Tsinghua-Tencent Joint Laboratory.
CR [Anonymous], 1991, Color Image Scale
   [Anonymous], 1965, The expression of emotions in man and animal
   [Anonymous], 2006, Hownet and the computation of meaning, DOI 10.1142/5935
   [Anonymous], 1995, ART COLOR COMBINATIO
   [Anonymous], P CTR GEOM COMP 2 AN
   Arnheim R., 1974, Art and Visual Perception, a Psychology of the Creative Eye
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Itten J., 1974, The Art of Color: The Subjective Experience and Objective Rationale of Color
   Lawson C. L., 1995, SOLVING LEAST SQUARE
   Lazebnik S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P649
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Matsuda Y., 1995, COLOR DESIGN
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shin Y., 2010, P ACM INT C IM VID R, P390
   Solli Martin., 2010, Conference on Colour in Graphics, Imaging, and Vision, P353
   Soottitantawat S., 2011, INT J COMPUT SCI, V8
   Wang BY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964959
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang LJ, 2008, IEEE T VIS COMPUT GR, V14, P1739, DOI 10.1109/TVCG.2008.118
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
NR 33
TC 30
Z9 35
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1121
EP 1133
DI 10.1007/s00371-012-0755-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300002
DA 2024-07-18
ER

PT J
AU Weng, YL
   Li, DP
   Tong, YY
AF Weng, Yanlin
   Li, Dongping
   Tong, Yiying
TI Texture mapping subdivision surfaces with hard constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Subdivision surfaces; Parameterization; Texture mapping; Deformation
AB We propose a texture mapping technique that allows user to directly manipulate texture coordinates of subdivision surfaces through adding feature correspondences. After features, or constraints, are specified by user on the subdivision surface, the constraints are projected back to the control mesh and a polygon matching/embedding algorithm is performed to generate polygon regions that embed texture coordinates of control mesh into different regions. After this step, some Steiner points are added to the control mesh. The generated texture coordinates exactly satisfy the input constraints but with high distortions. Then a constrained smoothing algorithm is performed to minimize distortions of the subdivision surface via updating texture coordinates of the control mesh. Finally, an Iterative Closest Point (ICP)-based deformation algorithm is performed to remove subdivision errors caused by the added Steiner points.
C1 [Weng, Yanlin; Li, Dongping] Zhejiang Univ, Hangzhou 310003, Zhejiang, Peoples R China.
   [Tong, Yiying] Michigan State Univ, E Lansing, MI 48824 USA.
C3 Zhejiang University; Michigan State University
RP Weng, YL (corresponding author), Zhejiang Univ, Hangzhou 310003, Zhejiang, Peoples R China.
EM weng@cad.zju.edu.cn; lidongping83@gmail.com; ytong@msu.edu
RI Tong, Yiying/D-9202-2012
OI Tong, Yiying/0000-0002-7929-4333
FU NSF of China [61003145]; National High-Tech R&D program of China
   [2012AA011503]
FX This work was supported by the NSF of China (No. 61003145) and the
   National High-Tech R&D program of China (No. 2012AA011503). An early
   version of the work was presented at Computational Visual Media
   Conference 2012 [22].
CR Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   [Anonymous], 1987, SMOOTH SUBDIVISION S
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Catmull E., 1998, SEMINAL GRAPHICS, P183, DOI DOI 10.1145/280811.280992
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Eckstein I, 2001, COMPUT GRAPH FORUM, V20, pC95, DOI 10.1111/1467-8659.00502
   Guenter B., 2005, ACM SIGGRAPH 2005 CO
   He L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778857
   Hormann K., 2007, Mesh parameterization: Theory and practice
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Kraevoy V, 2003, ACM T GRAPHIC, V22, P326, DOI 10.1145/882262.882271
   Lévy B, 2001, COMP GRAPH, P417, DOI 10.1145/383259.383308
   Ley B., 2002, ACM SIGGRAPH C P
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Popa T., 2005, ACM SIGGRAPH 2005 SI
   Shoemake K., 1992, Proceedings. Graphics Interface '92, P258
   Tutte WT, 1960, Proc. Lond. Math. Soc., Vs3-10, P304, DOI [10.1112/plms/s3-10.1.304, DOI 10.1112/PLMS/S3-10.1.304]
   Weng Y., 2012, P COMP VIS MED C
   Yang Y.-L., IEEE T VIS COMPUT GR, V14
   Zeng W., 2010, ARXIV10054648V2MATHN
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
   ZHOU K, 2007, ACM T GRAPH, V26
NR 26
TC 1
Z9 5
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1231
EP 1241
DI 10.1007/s00371-013-0794-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300011
DA 2024-07-18
ER

PT J
AU Luo, P
   Wu, ZZ
   Xia, CH
   Feng, L
   Ma, T
AF Luo, Pei
   Wu, Zhuangzhi
   Xia, Chunhe
   Feng, Lu
   Ma, Teng
TI Co-segmentation of 3D shapes via multi-view spectral clustering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Co-segmentation; Multi-view clustering; Sparsity; Low-rankness
AB Co-segmentation of 3D shapes in the same category is an intensive topic in computer graphics. In this paper, we present an unsupervised method to segment a set of meshes into corresponding parts in a consistent manner. Given the over-segmented patches as input, the co-segmentation result is generated by grouping them. In contrast to the previous method, we formulate the problem as a multi-view spectral clustering task by co-training a set of affinity matrices derived from different shape descriptors. For each shape descriptor, the affinity matrix is constructed via combining low-rankness and sparse representation. The integration of multiple features makes our method tolerate the large geometry and topology variations among the 3D meshes in a set. Moreover, the low-rank and sparse representation can capture not only the global structure but also the local relationship, which demonstrate robust to outliers. The experimental results show that our approach successfully segments each category in the benchmark dataset into corresponding parts and generates more reliable results compared with the state-of-the-art.
C1 [Luo, Pei; Wu, Zhuangzhi; Xia, Chunhe; Feng, Lu; Ma, Teng] Beihang Univ, Dept Comp Sci, Beijing, Peoples R China.
C3 Beihang University
RP Wu, ZZ (corresponding author), Beihang Univ, Dept Comp Sci, XueYuan Rd 37, Beijing, Peoples R China.
EM areslp@gmail.com; zzwu@buaa.edu.cn; xch@buaa.edu.cn;
   fenglupeter@gmail.com; bhmateng@163.com
CR [Anonymous], 2010, 100920105055 ARXIV
   [Anonymous], 2010, P 26 INT C MACH LEAR
   [Anonymous], EUR WORKSH 3D OBJ RE
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Cheng HD, 2001, PATTERN RECOGN, V34, P2259, DOI 10.1016/S0031-3203(00)00149-7
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Hu RZ, 2012, COMPUT GRAPH FORUM, V31, P1703, DOI 10.1111/j.1467-8659.2012.03175.x
   Huang QX, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024159
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kreavoy A, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P129, DOI 10.1109/PG.2007.40
   Kumar A., 2011, INT C MACH LEARN
   Meng M, 2013, COMPUT AIDED DESIGN, V45, P312, DOI 10.1016/j.cad.2012.10.014
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sidi O, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024160
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P553, DOI 10.1111/j.1467-8659.2011.01893.x
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
NR 23
TC 12
Z9 13
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 587
EP 597
DI 10.1007/s00371-013-0824-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400013
DA 2024-07-18
ER

PT J
AU Zhang, XH
   Tang, M
   Tong, RF
AF Zhang, Xiaohong
   Tang, Min
   Tong, Ruofeng
TI Robust super resolution of compressed video
SO VISUAL COMPUTER
LA English
DT Article
DE Super resolution (SR); Iterative reweighted least square (IRLS); Maximum
   a posteriori (MAP); Bayesian framework
ID SUPERRESOLUTION; RECONSTRUCTION; IMAGES
AB This paper presents a robust algorithm to recover high-frequency information from compressed low-resolution (LR) video sequences. Previous super-resolution (SR) approaches have succeeded in resolution enhancement when the motion in the LR sequence is simple. However, when the motion is complex, new artifacts will be introduced in the SR processing. To solve this problem, we develop a robust Bayesian SR algorithm with two steps. We first isolate the frames individually to get their corresponding initial SR solutions within the Bayesian framework. Secondly, with a robust cost function to reject outliers and noise, final SR images are achieved with multiple LR frames. In the mean time, we impose the constraint that the distribution of high-resolution (HR) image gradient should be equal to one of the corresponding decompressed LR images to sharpen the edges of the results. As a result of these steps, we are able to produce high-quality deblurred results, which show a suppressing of high-frequency artifacts and less ringing artifacts, with a higher peak signal-to-noise ratio (PSNR).
C1 [Tang, Min] Zhejiang Univ, Inst Artificial Intelligence, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
   [Zhang, Xiaohong; Tong, Ruofeng] Zhejiang Univ, Dept Comp Sci & Engn, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Tang, M (corresponding author), Zhejiang Univ, Inst Artificial Intelligence, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
EM zhangxh7877@sina.com; tang_m@zju.edu.cn; trf@zju.edu.cn
RI Tang, Min/KOC-3090-2024; Zhang, Xiaohong/A-3060-2015
FU National Basic Research Program of China [2011CB302205]; Science and
   Technology Bureau of Zhejiang Province, China [2010C13023]; National
   High-Tech Research and Development Program of China [2009AA01Z330]
FX This work is supported by the National Basic Research Program (No.
   2011CB302205) of China, the Science and Technology Bureau (2010C13023)
   of Zhejiang Province, China, and by National High-Tech Research and
   Development Program (No. 2009AA01Z330) of China.
CR Altunbasak Y, 2002, IEEE T CIRC SYST VID, V12, P217, DOI 10.1109/76.999200
   Alvarez LD, 2004, INT J IMAG SYST TECH, V14, P58, DOI 10.1002/ima.20008
   [Anonymous], P IEEE WORKSH STAT C
   [Anonymous], ACM T GRAPH SIGGRAPH
   Belekos SP, 2010, IEEE T IMAGE PROCESS, V19, P1451, DOI 10.1109/TIP.2010.2042115
   BISHOP CM, 2003, P ART INT STAT
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Chantas GK, 2006, IEEE T IMAGE PROCESS, V15, P2987, DOI 10.1109/TIP.2006.877520
   Chen J, 2008, LECT NOTES COMPUT SC, V5018, P1, DOI 10.1007/978-3-540-79723-4_1
   Dedeoglu G, 2004, PROC CVPR IEEE, P151
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Gunturk BK, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P41, DOI 10.1109/ICIP.2001.958419
   Huang H, 2011, IEEE T CIRC SYST VID, V21, P1363, DOI 10.1109/TCSVT.2011.2163461
   Huang H, 2011, IEEE T NEURAL NETWOR, V22, P121, DOI 10.1109/TNN.2010.2089470
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Jiang ZD, 2003, PROC CVPR IEEE, P549
   Katsaggelos A., 1991, DIGITAL IMAGE RESTOR, V23
   Kong D, 2006, INT C PATT RECOG, P619
   Luenberger DG, 1984, LINEAR NONLINEAR PRO
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Ni W, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P13, DOI 10.1109/ICIG.2007.136
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Ratakonda K, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P203, DOI 10.1109/ICIP.1998.727167
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   Segall CA, 2003, IEEE SIGNAL PROC MAG, V20, P37, DOI 10.1109/MSP.2003.1203208
   Segall CA, 2001, SPRING INT SER ENG C, V632, P211
   STARK H, 1989, J OPT SOC AM A, V6, P1715, DOI 10.1364/JOSAA.6.001715
   Su H, 2008, IEEE IMAGE PROC, P1236, DOI 10.1109/ICIP.2008.4711985
   Takeda H, 2009, IEEE T IMAGE PROCESS, V18, P1958, DOI 10.1109/TIP.2009.2023703
   Tekalp A. M., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P169, DOI 10.1109/ICASSP.1992.226249
   Tom BC, 2001, IEEE T IMAGE PROCESS, V10, P278, DOI 10.1109/83.902292
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P2017, DOI 10.1109/TIP.2010.2045707
NR 35
TC 18
Z9 20
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1167
EP 1180
DI 10.1007/s00371-011-0666-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700003
DA 2024-07-18
ER

PT J
AU Liu, XD
   Wu, JZ
   Zheng, CW
AF Liu, Xiao-Dan
   Wu, Jia-Ze
   Zheng, Chang-Wen
TI KD-tree based parallel adaptive rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Multidimensional space; Adaptive technique; Parallelization; Poisson
   disk; Anisotropic sampling
AB Multidimensional adaptive sampling technique is crucial for generating high quality images with effects such as motion blur, depth-of-field and soft shadows, but it costs a lot of memory and computation time. We propose a novel kd-tree based parallel adaptive rendering approach. First, a two-level framework for adaptive sampling in parallel is introduced to reduce the computation time and control the memory cost: in the prepare stage, we coarsely sample the entire multidimensional space and use kd-tree structure to separate it into several multidimensional subspaces; in the main stage, each subspace is refined by a sub kd-tree and rendered in parallel. Second, novel kd-tree based strategies are introduced to measure space's error value and generate anisotropic Poisson disk samples. The experimental results show that our algorithm produces better quality images than previous ones.
C1 [Liu, Xiao-Dan; Wu, Jia-Ze; Zheng, Chang-Wen] Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
   [Liu, Xiao-Dan] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, XD (corresponding author), Chinese Acad Sci, Inst Software, Natl Key Lab Integrated Informat Syst Technol, Beijing, Peoples R China.
EM lxdfigo@163.com; wujiaze05@gmail.com; cwzheng@ieee.org
CR [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], 2010, ACM T GRAPHIC, DOI DOI 10.1145/1778765.1778816
   Chen JT, 2011, COMPUT GRAPH FORUM, V30, P1667, DOI 10.1111/j.1467-8659.2011.01854.x
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   DeCoro C., 2010, PACIFIC GRAPHICS, V29, P7
   Ebeida MS, 2012, COMPUT GRAPH FORUM, V31, P785, DOI 10.1111/j.1467-8659.2012.03059.x
   Egan K., 2001, ACM T GRAPH, V30
   Egan K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531399
   Gamito MN, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640451
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Lehtinen J., 2011, ACM T GRAPHIC, V40, P10
   Li H., 2010, ACM SIGGRAPH ASIA
   McCool MD, 1999, ACM T GRAPHIC, V18, P171, DOI 10.1145/318009.318015
   MITCHELL DP, 1991, COMP GRAPH, V25, P157, DOI 10.1145/127719.122736
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Rigau J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P260
   Rousselle F., 2011, ACM SIGGRAH ASIA
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Wei LY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964945
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360619
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Wu JZ, 2010, VISUAL COMPUT, V26, P555, DOI 10.1007/s00371-010-0459-5
   YELLOTT JI, 1983, SCIENCE, V221, P382, DOI 10.1126/science.6867716
NR 24
TC 11
Z9 13
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 613
EP 623
DI 10.1007/s00371-012-0709-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500010
DA 2024-07-18
ER

PT J
AU Zhang, JW
   Hao, YK
   Li, L
   Sun, D
   Yuan, L
AF Zhang, Jiawan
   Hao, Yukun
   Li, Liang
   Sun, Di
   Yuan, Li
TI StoryWizard: a framework for fast stylized story illustration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Storywizard; Story illustration; Image filtering
AB A fast stylized framework used for creating illustration of a given story is represented. The framework automatically searches proper online images according to the keywords of story, provides users some tools to make the search result precise, and helps users create a picture in every scene without considering the consistency of each character. With the friendly user interface, it provides user abundant interactions, which helps users express their ideas flexibly. Experimental results indicate that this framework allows users, without any art background, to produce personalized picture series with specified story. The fast process, effective interaction and generated delicate pictures can make a story more impressive.
C1 [Zhang, Jiawan; Hao, Yukun; Li, Liang; Sun, Di; Yuan, Li] Tianjin Univ, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Li, L (corresponding author), Tianjin Univ, Tianjin 300072, Peoples R China.
EM allen.liliang@gmail.com
OI Zhang, Jiawan/0000-0002-0667-6744
CR Allen J., 1995, Natural Language Understanding
   [Anonymous], P ACM SIGGRAPH EUR S
   [Anonymous], IEEE T PATTERN ANAL
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Coelho F., 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P241, DOI 10.1109/CBMI.2011.5972552
   Collomosse J.P., 2009, IEEE 12 INT C COMP V
   Curtis C.J., 1997, P 24 ANN C COMP GRAP
   Datta R., 2006, ACM COMPUT SURV, V39
   Delgado D, 2010, P 2010 IEEE 4 INT C
   Ding M., 2010, VISUAL COMPUT, V26
   Eitz M., 2009, SIGGRAPH 2009 TALKS
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Georgescu B., 2003, MEAN SHIFT BASED CLU
   Hanser E., 2009, P 32 ANN GERM C ADV
   Hertzmann A, 2000, P 27 ANN C COMP GRAP
   Jia Jiaya, 2006, ACM T GRAPHIC
   Jin W., 2002, J COMPUT SCI TECHNOL, V17
   Johnson M, 2006, COMPUT GRAPH FORUM, V25, P407, DOI 10.1111/j.1467-8659.2006.00960.x
   Joshi D., 2006, ACM T MULTIM COMPUT, V2
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Liu Z.Q., 2006, SOFT COMPUT, V10
   Madden M., 2008, COMPUT EDUC, V51
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Schwarz K., 2010, P 14 INT C KNOWL B 4
   SEKINE S, 1998, THESIS NEW YORK U
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Wang Changhu., 2010, proceedings of the International Conference on World Wide Web, P1309
   Wang J.Z., 2001, IEEE Transactions on Pattern Analysis and Machine Intelligence, V23
NR 33
TC 2
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 877
EP 887
DI 10.1007/s00371-012-0702-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500035
DA 2024-07-18
ER

PT J
AU Chattopadhyay, A
   Plantinga, S
   Vegter, G
AF Chattopadhyay, A.
   Plantinga, S.
   Vegter, G.
TI Certified meshing of Radial Basis Function based isosurfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Certified meshing; Geometric computing; Radial Basis Functions; Interval
   arithmetic; Affine arithmetic
AB Radial Basis Functions are widely used in scattered data interpolation. The surface-reconstruction method using radial basis functions consists of two steps: (i) computing an interpolating implicit function the zero set of which contains the points in the data set, followed by (ii) extraction of isocurves or isosurfaces. In this paper we focus on the second step, generalizing the work on certified meshing of implicit surfaces based on interval arithmetic (Plantinga and Vegter in Visual Comput. 23:45-58, 2007). It turns out that interval arithmetic, and even the usually faster affine arithmetic, are far too slow in the context of RBF-based implicit surface meshing. We present optimized strategies giving acceptable running times and better space complexity, exploiting special properties of RBF-interpolants. We present pictures and timing results confirming the improved quality of these optimized strategies.
C1 [Chattopadhyay, A.; Plantinga, S.] Univ Groningen, Inst Math & Comp Sci, Groningen, Netherlands.
   [Vegter, G.] Univ Groningen, Johann Bernoulli Inst Math & Comp Sci, Groningen, Netherlands.
C3 University of Groningen; University of Groningen
RP Chattopadhyay, A (corresponding author), Univ Groningen, Inst Math & Comp Sci, Groningen, Netherlands.
EM A.Chattopadhyay@rug.nl; S.Plantinga@rug.nl; G.Vegter@rug.nl
OI Chattopadhyay, Amit/0000-0003-4691-3019
CR [Anonymous], 2003, RADIAL BASIS FUNCTIO
   Boissonnat JD, 2008, DISCRETE COMPUT GEOM, V39, P138, DOI 10.1007/s00454-007-9011-4
   Burr M., 2008, Proc. Int'l Symp. Symbolic and Algebraic Computation (ISSAC'08), P87
   de Figueiredo LH, 2004, NUMER ALGORITHMS, V37, P147, DOI 10.1023/B:NUMA.0000049462.70970.b6
   Iske A, 2002, TUTORIALS ON MULTIRESOLUTION IN GEOMETRIC MODELLING, P205
   Lodha S.K., 1999, Proceedings of the Dagstuhl Conference on Scientific Visualization, P182
   Lopes H, 2002, COMPUT GRAPH-UK, V26, P841, DOI 10.1016/S0097-8493(02)00173-5
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Martin R, 2002, COMPUT AIDED GEOM D, V19, P553, DOI 10.1016/S0167-8396(02)00146-2
   Moore R. E., 1996, INTERVAL ANAL
   Plantinga S, 2007, VISUAL COMPUT, V23, P45, DOI 10.1007/s00371-006-0083-6
   Powell M., 1981, Approximation Theory and Methods
   Schaback R., 2001, Multivariate Approximation and Applications, P1
   SHOU H, 2006, P INT C COMP GRAPH V, P196
   Stander BT, 1997, Proc. SIGGRAPH, P279, DOI DOI 10.1145/258734.258868
   Stol J., 1997, MONOGRAPH 21 BRAZILI
   Wendland H, 2004, CAMBRIDGE MONOGRAPHS, V17
NR 17
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2012
VL 28
IS 5
BP 445
EP 462
DI 10.1007/s00371-011-0627-2
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EJ
UT WOS:000302813800003
DA 2024-07-18
ER

PT J
AU Latorre, P
   Seron, FJ
   Gutierrez, D
AF Latorre, Pedro
   Seron, Francisco J.
   Gutierrez, Diego
TI Birefringence: calculation of refracted ray paths in biaxial crystals
SO VISUAL COMPUTER
LA English
DT Article
DE Computer graphics; Three-dimensional graphics and realism; Simulation of
   physical phenomena; Optics; Physically based rendering
ID DAMPED NEWTON METHOD
AB The phenomenon of birefringence may be observed when light arrives at an anisotropic crystal surface and refracts through it, causing the incident light ray to split into two rays; these become polarized in mutually orthogonal directions, and two images are formed. The principal goal of this paper is the study of the directional issues involved in the behavior of light when refracting through a homogeneous, non-participating medium, including both isotropic and anisotropic media (uniaxial and, for the first time, biaxial). The paper focuses on formulating and solving the non-linear algebraic system that is obtained when the refraction process is simulated using the geometric model of Huygens. The main contribution focuses on the case of biaxial media. In the case of uniaxial media, we rely on symbolic calculus techniques to formulate and solve the problem.
C1 [Latorre, Pedro; Seron, Francisco J.; Gutierrez, Diego] Univ Zaragoza, Zaragoza 50018, Spain.
C3 University of Zaragoza
RP Latorre, P (corresponding author), Univ Zaragoza, Maria de Luna 1, Zaragoza 50018, Spain.
EM platorre@unizar.es; seron@unizar.es; diegog@unizar.es
RI Román, Pedro Ángel Latorre/K-1052-2019; Seron Arbeloa, Francisco
   Jose/L-3146-2014
OI Seron Arbeloa, Francisco Jose/0000-0003-1683-4694
FU Spanish Ministry of Science and Technology [TIN2007-63025]; Aragon
   Government [OTRI 2009/0411, CTPP05/09]
FX This research has been funded by the Spanish Ministry of Science and
   Technology (project TIN2007-63025) and the Aragon Government (projects
   OTRI 2009/0411 and CTPP05/09).
CR Born M., 1999, Principles of Optics, V7th edn.
   Buchanan AM, 2005, PROC CVPR IEEE, P316
   Char B.W., 1992, MAPLE
   Denflhard P., 2004, NUMERICAL METHODS NO
   Dirkse SP, 1996, ANN OPER RES, V68, P211, DOI 10.1007/BF02209613
   Epureanu BI, 1998, SIAM REV, V40, P102, DOI 10.1137/S0036144596310033
   Felippa C.A., 2010, FRACTAL BASINS ATTRA
   Gay P., 1972, CRYSTALLYNE STATE IN
   Guy S, 2004, ACM T GRAPHIC, V23, P231, DOI 10.1145/1015706.1015708
   KAUFMANN S, 1983, COMPUT CHEM ENG, V7, P93, DOI 10.1016/0098-1354(83)85011-X
   Latorre Andres P.M., 2001, THESIS U ZARAGOZA ZA
   Matveev A. N., 1988, OPTICS
   MCCLAIN SC, 1992, P SOC PHOTO-OPT INS, V1746, P107, DOI 10.1117/12.138779
   Ortega J. M., 1970, ITERATIVE SOLUTION N, DOI [10.1137/1.9780898719468, DOI 10.1137/1.9780898719468]
   Press W.H., 1995, ART SCI COMPUTING
   Stone JM., 1963, RAD OPTICS INTRO CLA
   Szivessy G., 1928, HDB PHYS, P840
   Tannenbaum D.C., 1994, P INT WORKSH APPL RO, P221
   Ueberhuber C.W., 1997, NUMERICAL COMPUTATIO
   Weidlich A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330517
   Yi D., 2006, TRENDS MATH, V9, P109
NR 21
TC 15
Z9 15
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 341
EP 356
DI 10.1007/s00371-011-0619-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300002
DA 2024-07-18
ER

PT J
AU Kasap, Z
   Magnenat-Thalmann, N
AF Kasap, Zerrin
   Magnenat-Thalmann, Nadia
TI Building long-term relationships with virtual and robotic characters:
   the role of remembering
SO VISUAL COMPUTER
LA English
DT Article
DE Long-term relationships; Long-term interaction; Episodic memory; Social
   presence
ID PEDAGOGICAL AGENTS; TUTORS
AB With the recent advances, today people are able to communicate with embodied (virtual/robotic) entities using natural ways of communication. In order to use them in our daily lives, they need to be intelligent enough to make long-term relationships with us and this is highly challenging. Previous work on long-term interaction frequently reported that after the novelty effect disappeared, users' interest into the interaction decreased with time. Our primary goal in this study was to develop a system that can still keep the attention of the users after the first interaction.
   Incorporating the notion of time, we think that the key to long-term interaction is the recall of past memories during current conversation. For this purpose, we developed a long-term interaction framework with remembering and dialogue planning capability. In order to see the effect of remembering on users, we designed a tutoring application and measured the changes in social presence and task engagement levels according to the existence of memory. Different from previous work, users' interest in our system did not decrease with time with the important contributions of remembering to the engagement level of users.
C1 [Kasap, Zerrin; Magnenat-Thalmann, Nadia] Univ Geneva, CH-1227 Geneva, Switzerland.
   [Magnenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
C3 University of Geneva; Nanyang Technological University
RP Kasap, Z (corresponding author), Univ Geneva, Battelle Bldg A,7 Route Drize, CH-1227 Geneva, Switzerland.
EM zerrin.kasap@miralab.ch; thalmann@miralab.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
FU EU [ISTFP7 247688]
FX We would like to thank to Nedjma Cadi-Yazli for preparing the
   illustrations and video file accompanying this paper. This research is
   partly funded by the EU Project 3DLife (ISTFP7 247688).
CR ALTMANN E, 2000, HUM FACT ERG SOC ANN, V1, P152
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P 20 FLOR ART INT RE
   Baylor A.L., 2005, International Journal of Artificial Intelligence in Education, V15, P95, DOI DOI 10.1007/BF02504991
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Biocca F, 2003, PRESENCE-VIRTUAL AUG, V12, P456, DOI 10.1162/105474603322761270
   D'Mello S, 2007, IEEE INTELL SYST, V22, P53, DOI 10.1109/MIS.2007.79
   EGGES A, 2006, THESIS U GENEVA GENE
   GOCKLEY R, 2006, P 1 ACM SIGCHI SIGAR, P186
   HARBERS M, 2009, P 8 INT C AUT AG MUL, V2, P1129
   Johnson WL, 1998, PRESENCE-VIRTUAL AUG, V7, P523, DOI 10.1162/105474698565929
   Kanda T, 2004, HUM-COMPUT INTERACT, V19, P61, DOI 10.1207/s15327051hci1901&2_4
   KASAP Z, 2011, COMPUTER GRAPHICS IN
   KASAP Z, 2010, IEEE INT S ROB HUM I, P479
   KIDD C, 2008, ROBOTS HOME UNDERSTA
   Kim Y, 2006, ETR&D-EDUC TECH RES, V54, P569, DOI 10.1007/s11423-006-0637-3
   Leite Iolanda, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P669, DOI 10.1109/ROMAN.2009.5326256
   Lewis PA, 2003, TRENDS COGN SCI, V7, P431, DOI 10.1016/j.tics.2003.08.005
   LOMBARD M, 2000, P 3 INT WORKSH PRES
   MCAULEY E, 1989, RES Q EXERCISE SPORT, V60, P48, DOI 10.1080/02701367.1989.10607413
   NUXOLL AM, 2007, THESIS U MICHIGAN AN
   Sardina S., 2006, P 5 INT JOINT C AUT, P1001, DOI DOI 10.1145/1160633.1160813
   Schank Roger C., 1977, SCRIPTS PLANS GOALS
   Tapus A, 2008, INTEL SERV ROBOT, V1, P169, DOI 10.1007/s11370-008-0017-4
   Tulving E, 1972, ORG MEMORY, P381, DOI DOI 10.1017/S0140525X00047257
   WIXTED JT, 1990, J EXP PSYCHOL LEARN, V16, P927, DOI 10.1037/0278-7393.16.5.927
   Woolf B, 2009, INT J LEARN TECHNOL, V4, P129, DOI 10.1504/IJLT.2009.028804
NR 27
TC 32
Z9 32
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 87
EP 97
DI 10.1007/s00371-011-0630-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000008
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU García, AL
   de Miras, JR
   Feito, FR
AF Garcia, A. L.
   Ruiz de Miras, J.
   Feito, F. R.
TI Evaluation of Boolean operations between free-form solids using extended
   simplicial chains and PN triangles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Free-form solid modeling; Boolean operation; Extended simplicial chain;
   PN triangle
ID SURFACE; DECOMPOSITION; COMBINATIONS; SUBDIVISION; SYSTEM
AB This paper presents a method to evaluate Boolean operations between free-form solids modeled using Extended Simplicial Chains (ESCs). The ESC model is a formal system to represent not only the boundary, but also the volume of free-form solids, that allows the development of simple and robust algorithms. In this implementation of the ESC model, the free-form solids and the results of the operations are bounded by PN triangles and represented by ESCs, and the surface intersection and trimming are computed using adaptive subdivision of the patches and a point in solid test specifically designed for ESCs.
C1 [Garcia, A. L.; Ruiz de Miras, J.; Feito, F. R.] Univ Jaen, Dept Informat, Jaen 23071, Spain.
C3 Universidad de Jaen
RP García, AL (corresponding author), Univ Jaen, Dept Informat, Campus Las Lagunillas S-N, Jaen 23071, Spain.
EM algarcia@ujaen.es
RI García-Fernández, Ángel-Luis/E-4257-2012; Ruiz de Miras,
   Juan/W-6894-2018; Feito, Francisco/M-1672-2014
OI García-Fernández, Ángel-Luis/0000-0002-8183-7130; Ruiz de Miras,
   Juan/0000-0001-7579-8350; Feito, Francisco/0000-0001-8230-6529
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], THESIS U N CAROLINA
   [Anonymous], 1998, COMPUTATIONAL GEOMET
   *ATI TECHN INC, 2001, TRUF WHIT PAP
   AZIZ NM, 1990, IEEE COMPUT GRAPH, V10, P50, DOI 10.1109/38.45810
   Bajaj Chandrajit, 2008, Comput Aided Des Appl, V5, P730
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   Farin G., 1986, Computer-Aided Geometric Design, V3, P83, DOI 10.1016/0167-8396(86)90016-6
   Farouki RT, 2004, COMPUT AIDED GEOM D, V21, P459, DOI 10.1016/j.cagd.2004.03.002
   Feito FR, 1998, COMPUT GRAPH, V22, P611, DOI 10.1016/S0097-8493(98)00067-3
   García AL, 2004, VISUAL COMPUT, V20, P298, DOI 10.1007/s00371-004-0239-1
   García AL, 2003, COMPUT GRAPH-UK, V27, P27, DOI 10.1016/S0097-8493(02)00222-4
   GARCIA AL, 2007, THESIS U GRANADA
   GARCIA AL, 2006, SIACG 2006 P SANT DE, P149
   Garcia-Gutierrez A., 2005, WORLD GEOTHERMAL C, P1
   Grandine TA, 1997, COMPUT AIDED GEOM D, V14, P111, DOI 10.1016/S0167-8396(96)00024-6
   Hable J, 2005, ACM T GRAPHIC, V24, P1024, DOI 10.1145/1073204.1073306
   Hass J, 2007, ADV COMPUT MATH, V27, P1, DOI 10.1007/s10444-005-7539-5
   Heo HS, 1999, COMPUT AIDED DESIGN, V31, P33, DOI 10.1016/S0010-4485(98)00078-5
   JORDAN C, 1887, COURSE ANAL
   Keyser J, 2004, COMPUT AIDED DESIGN, V36, P175, DOI 10.1016/S0010-4485(03)00060-5
   Krishnamurthy A, 2009, IEEE T VIS COMPUT GR, V15, P530, DOI 10.1109/TVCG.2009.29
   Krishnan S, 2001, INT J COMPUT GEOM AP, V11, P105, DOI 10.1142/S0218195901000419
   KRISHNAN S, 1997, THEIS U N CAROLINA C
   Li XY, 2004, COMPUT GRAPH-UK, V28, P527, DOI 10.1016/j.cag.2004.04.008
   Li YQ, 2002, COMPUT GRAPH-UK, V26, P67, DOI 10.1016/S0097-8493(01)00158-3
   Linensen L., 2000, PROC SCCG, P259
   Litke N, 2001, COMPUT AIDED GEOM D, V18, P463, DOI 10.1016/S0167-8396(01)00042-5
   Moller T., 1997, J. Graph. Tools, V2, P25, DOI [DOI 10.1080/10867651.1997.10487472, 10.1080/10867651.1997.10487472]
   MUKUNDAN H, 2005, THESIS MIT
   Pasko G, 2004, VISUAL COMPUT, V20, P437, DOI 10.1007/s00371-004-0250-6
   Patrikalakis N., 2002, Shape Interrogation for Computer Aided Design and Manufacturing
   Patrikalakis N. M., 2004, Computer-Aided Design and Applications, V1, P449
   Plate J, 2007, IEEE T VIS COMPUT GR, V13, P1584, DOI 10.1109/TVCG.2007.70534
   REQUICHA AAG, 1977, 25 U ROCH PROD AUT P
   ROSSIGNAC J, 1996, LECT COURS 29 REPR G
   RUIZ J, 2001, THESIS U GRANADA GRA
   RUIZ J, 1999, WSCG 99, P241
   Schwarz M, 2009, COMPUT GRAPH FORUM, V28, P365, DOI 10.1111/j.1467-8659.2009.01376.x
   Seidel H.-P., 1989, Mathematical Methods in Computer Aided Geometric Design, P573
   Song XW, 2004, COMPUT AIDED GEOM D, V21, P303, DOI 10.1016/j.cagd.2003.11.004
   VLACHOS A, 2001, 2001 ACM S INT 3D GR, P159
   Woo Y, 2003, COMPUT AIDED DESIGN, V35, P969, DOI 10.1016/S0010-4485(02)00144-6
   ZHANG X, 2005, THESIS STATE U
NR 44
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 531
EP 541
DI 10.1007/s00371-011-0566-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600012
DA 2024-07-18
ER

PT J
AU Busking, S
   Botha, CP
   Ferrarini, L
   Milles, J
   Post, FH
AF Busking, Stef
   Botha, Charl P.
   Ferrarini, Luca
   Milles, Julien
   Post, Frits H.
TI Image-based rendering of intersecting surfaces for dynamic comparative
   visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Comparative visualization; Image-based rendering; Surface comparison;
   Nested surfaces
ID SHAPE; EXPLORATION
AB Nested or intersecting surfaces are proven techniques for visualizing shape differences between static 3D objects (Weigle and Taylor II, IEEE Visualization, Proceedings, pp. 503-510, 2005). In this paper we present an image-based formulation for these techniques that extends their use to dynamic scenarios, in which surfaces can be manipulated or even deformed interactively. The formulation is based on our new layered rendering pipeline, a generic image-based approach for rendering nested surfaces based on depth peeling and deferred shading.
   We use layered rendering to enhance the intersecting surfaces visualization. In addition to enabling interactive performance, our enhancements address several limitations of the original technique. Contours remove ambiguity regarding the shape of intersections. Local distances between the surfaces can be visualized at any point using either depth fogging or distance fields: Depth fogging is used as a cue for the distance between two surfaces in the viewing direction, whereas closest-point distance measures are visualized interactively by evaluating one surface's distance field on the other surface. Furthermore, we use these measures to define a three-way surface segmentation, which visualizes regions of growth, shrinkage, and no change of a test surface compared with a reference surface.
   Finally, we demonstrate an application of our technique in the visualization of statistical shape models. We evaluate our technique based on feedback provided by medical image analysis researchers, who are experts in working with such models.
C1 [Busking, Stef; Botha, Charl P.; Post, Frits H.] Delft Univ Technol, Data Visualizat Grp, Delft, Netherlands.
   [Botha, Charl P.; Ferrarini, Luca; Milles, Julien] Leiden Univ, Med Ctr, Div Image Proc LKEB, Dept Radiol, Leiden, Netherlands.
C3 Delft University of Technology; Leiden University; Leiden University
   Medical Center (LUMC); Leiden University - Excl LUMC
RP Busking, S (corresponding author), Delft Univ Technol, Data Visualizat Grp, Delft, Netherlands.
EM s.busking@tudelft.nl; c.p.botha@tudelft.nl; l.ferrarini@lumc.nl;
   j.r.milles@lumc.nl; f.h.post@tudelft.nl
RI Botha, Charl P./E-9506-2010
OI FERRARINI, LUCA/0000-0001-7598-8971; Ferrarini, Luca/0009-0001-8939-5832
FU Netherlands Organization for Scientific Research (NWO) [643.100.503]
FX This research is supported by the Netherlands Organization for
   Scientific Research (NWO), project number 643.100.503 "Multi-Field
   Medical Visualization".
CR [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Bair A, 2007, IEEE T VIS COMPUT GR, V13, P1656, DOI 10.1109/TVCG.2007.70559
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   BRUCKNER S, 2005, EUR IEEE VDTC S VIS, V1, P69
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1559, DOI 10.1109/TVCG.2006.96
   Busking S, 2010, COMPUT GRAPH FORUM, V29, P973, DOI 10.1111/j.1467-8659.2009.01668.x
   Busking S, 2009, COMPUT GRAPH FORUM, V28, P799, DOI 10.1111/j.1467-8659.2009.01471.x
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Deering M., 1988, Computer Graphics, V22, P21, DOI 10.1145/378456.378468
   Di Gesù V, 1999, PATTERN RECOGN LETT, V20, P207, DOI 10.1016/S0167-8655(98)00115-9
   Diefenbach P.J., 1996, THESIS
   Everitt C, 2001, INTERACTIVE ORDER IN
   Ferrarini L, 2006, NEUROIMAGE, V32, P1060, DOI 10.1016/j.neuroimage.2006.05.048
   Gatzke T, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/SMI.2005.13
   GOLDFEATHER J, 1989, IEEE COMPUT GRAPH, V9, P20, DOI 10.1109/38.28107
   GUENNEBAUD G, 2006, IEEE EUR ACM S POINT, P49
   Interrante V, 1997, IEEE T VIS COMPUT GR, V3, P98, DOI 10.1109/2945.597794
   Johnson CR, 2003, IEEE COMPUT GRAPH, V23, P6, DOI 10.1109/MCG.2003.1231171
   LI X, 2006, IEEE SHAP MOD APPL P, P38, DOI DOI 10.1109/SMI.2006.8
   Likert R., 1932, Arch. Psychol., V22, P44, DOI DOI 10.4135/9781412961288.N454
   LIM IS, 2003, IEEE COMP BAS MED SY, P26
   MAMMEN A, 1989, IEEE COMPUT GRAPH, V9, P43, DOI 10.1109/38.31463
   Masuda T, 2003, J VISUAL COMP ANIMAT, V14, P183, DOI 10.1002/vis.316
   Mauch Sean, 2000, A Fast Algorithm for Computing the Closest Point and Distance Transform., DOI DOI 10.1007/978-3-319-15090-1_15
   Miranda PAV, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P139
   NIENHAUS M, 2006, COMP GRAPH VIRT REAL, P91, DOI DOI 10.1145/1108590.1108605
   Pagendarm H.-G., 1995, Visualization in Scientific Computing, P95
   PEIKERT R, 2006, OPTIMIZED BOUNDING P, P65, DOI DOI 10.1007/3-540-30790-7_5
   PICHON E, 2006, SPIE MED IMAGING P, V6141, P373
   Rey D, 2002, MED IMAGE ANAL, V6, P163, DOI 10.1016/S1361-8415(02)00056-7
   Rheingans P, 1996, IEEE VISUAL, P219, DOI 10.1109/VISUAL.1996.568111
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Subsol G, 1997, MAGN RESON IMAGING, V15, P917, DOI 10.1016/S0730-725X(97)00002-7
   Tory M, 2001, PROC SPIE, V4319, P590, DOI 10.1117/12.428104
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Weigle C, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P503
   WEIGLE C, 2006, THESIS
   Wiegand TF, 1996, COMPUT GRAPH FORUM, V15, P249, DOI 10.1111/1467-8659.1540249
   Wilson DL, 1997, INT J COMPUT VISION, V24, P5, DOI 10.1023/A:1007978107063
NR 39
TC 20
Z9 22
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2011
VL 27
IS 5
BP 347
EP 363
DI 10.1007/s00371-010-0541-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745ZI
UT WOS:000289209800002
DA 2024-07-18
ER

PT J
AU Hajder, L
   Pernek, A
   Kazó, C
AF Hajder, Levente
   Pernek, Akos
   Kazo, Csaba
TI Weak-perspective structure from motion by fast alternation
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; 3D reconstruction; Structure from motion; Factorization
ID CLOSED-FORM SOLUTION; SHAPE; FACTORIZATION
AB This paper addresses the problem of moving object reconstruction. Several methods have been published in the past 20 years including stereo reconstruction as well as multi-view factorization methods. In general, reconstruction algorithms compute the 3D structure of the object and the camera parameters in a non-optimal way, and then a nonlinear and numerical optimization algorithm refines the reconstructed camera parameters and 3D coordinates. In this paper, we propose an adjustment method which is the improved version of the well-known Tomasi-Kanade factorization method. The novelty, which yields the high speed of the algorithm, is that the core of the proposed method is an alternation and we give optimal solutions to the subproblems in the alternation. The improved method is discussed here and it is compared to the widely used bundle adjustment algorithm.
C1 [Hajder, Levente; Kazo, Csaba] Hungarian Acad Sci, Comp & Automat Res Inst, H-1111 Budapest, Hungary.
   [Pernek, Akos; Kazo, Csaba] Budapest Univ Technol & Econ, Dept Automat & Appl Informat, H-1111 Budapest, Hungary.
C3 Hungarian Academy of Sciences; Hungarian Research Network; HUN-REN
   Institute for Computer Science & Control; Budapest University of
   Technology & Economics
RP Hajder, L (corresponding author), Hungarian Acad Sci, Comp & Automat Res Inst, Kende 13-17, H-1111 Budapest, Hungary.
EM hajder@sztaki.hu
FU NKTH-OTKA [CK 78409]
FX This research was supported in part by the NKTH-OTKA grant CK 78409.
CR [Anonymous], 2008, CVPR
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Berthilsson R, 1997, PROC CVPR IEEE, P444, DOI 10.1109/CVPR.1997.609363
   Bjorck A., 1996, NUMERICAL METHODS LE
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Buchanan Aeron., 2004, INVESTIGATION MATRIX
   BUCHANAN AM, 2005, CVPR05, V2, P316, DOI DOI 10.1109/CVPR.2005.118
   Delaunay B., 1934, IZV AKAD NAUK SSSR O, V7, P1
   HAJDER L, 2005, 3 HUNG C COMP GRAPH, P30
   Hajder L, 2006, PATTERN RECOGN LETT, V27, P1581, DOI 10.1016/j.patrec.2006.03.007
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   MAHAMUD S, 2000, CVPR, P2430
   MARTINEC D, 2005, CVPR, V1, P198
   Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132
   Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098
   Rousseeuw P. J., 1987, ROBUST REGRESSION OU
   Sturm P., 1996, EUROPEAN C COMPUTER, P709, DOI DOI 10.1007/3-540-61123-1
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Torresani L, 2001, PROC CVPR IEEE, P493
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   TRAJKOVIC M, 1997, P BRIT MACH VIS C
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Vidal R, 2004, PROC CVPR IEEE, P310
   WEINSHALL D, 1995, IEEE T PATTERN ANAL, V17, P512, DOI 10.1109/34.391392
   Xiao J, 2006, INT J COMPUT VISION, V67, P233, DOI 10.1007/s11263-005-3962-9
   Zinsser T., 2005, INT C PATT REC IM PR, P116
NR 27
TC 8
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2011
VL 27
IS 5
BP 387
EP 399
DI 10.1007/s00371-011-0553-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745ZI
UT WOS:000289209800005
DA 2024-07-18
ER

PT J
AU Abbas, A
   Nasri, A
   Maekawa, T
AF Abbas, Abdulwahed
   Nasri, Ahmad
   Maekawa, Takashi
TI Generating B-spline curves with points, normals and curvature
   constraints: a constructive approach
SO VISUAL COMPUTER
LA English
DT Article
DE B-splines curves and surfaces; Recursive subdivision; Limit curve and
   surface; Fitting; Interpolation; Normal; Curvature
ID INTERPOLATION CONSTRAINTS; SUBDIVISION SURFACES; TAXONOMY
AB This paper presents a constructive method for generating a uniform cubic B-spline curve interpolating a set of data points simultaneously controlled by normal and curvature constraints. By comparison, currently published methods have addressed one or two of those constraints (point, normal or cross-curvature interpolation), but not all three constraints simultaneously with C2 continuity. Combining these constraints provides better control of the generated curve in particular for feature curves on free-form surfaces. Our approach is local and provides exact interpolation of these constraints.
C1 [Abbas, Abdulwahed] Univ Balamand, Dept Comp Sci, Tripoli, Lebanon.
   [Nasri, Ahmad] Amer Univ Beirut, Dept Comp Sci, Beirut, Lebanon.
   [Maekawa, Takashi] Yokohama Natl Univ, Dept Mech Engn, Yokohama, Kanagawa 240, Japan.
C3 University Balamand; American University of Beirut; Yokohama National
   University
RP Abbas, A (corresponding author), Univ Balamand, Dept Comp Sci, Tripoli, Lebanon.
EM abbas@balamand.edu.lb; anasri@aub.edu.lb; maekawa@ynu.ac.jp
RI Maekawa, Takashi/AAI-4292-2020
OI Maekawa, Takashi/0000-0003-3896-8431; Nasri, Ahmad/0000-0002-2047-6693
FU University of Balamand; American University of Beirut [DDF
   111135-A88116]
FX This work was supported in part by a an internal (BIRG) grant form the
   University of Balamand, and by a URB grant DDF 111135-A88116 from the
   American University of Beirut.
CR CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Farin G., 2002, CURVES SURFACES COMP
   GOFUKU S, 2009, COMPUT AIDED DES
   HALSTEAD M, 1993, SIGGRAPH 93 C P, P35
   Hoschek J, 1999, SHAPE PRESERVING REPRESENTATIONS IN COMPUTER-AIDED GEOMETRIC DESIGN, P163
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Maekawa T, 2007, COMPUT AIDED DESIGN, V39, P313, DOI 10.1016/j.cad.2006.12.008
   NASRI A, 2003, PACIFIC GRAPHICS
   NASRI A, 2006, COMPUTER GRAPHICS IN, P761
   Nasri AH, 2002, VISUAL COMPUT, V18, P382, DOI 10.1007/s003710100155
   Nasri AH, 2002, VISUAL COMPUT, V18, P259, DOI 10.1007/s003710100154
   Nasri AH, 2002, COMPUT GRAPH-UK, V26, P393, DOI 10.1016/S0097-8493(02)00082-1
   Piegl L., 1997, The Nurbs Book, Vsecond
   Prautzsch H., 2002, Bezier and B-Spline Techniques
   SCHAEFER S, 2004, S GEOM PROC, P105
   Weiss V, 2002, COMPUT AIDED GEOM D, V19, P19, DOI 10.1016/S0167-8396(01)00086-3
   Ye XZ, 1996, COMPUT AIDED DESIGN, V28, P741, DOI 10.1016/0010-4485(95)00080-1
NR 17
TC 11
Z9 18
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 823
EP 829
DI 10.1007/s00371-010-0441-2
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800042
DA 2024-07-18
ER

PT J
AU Miao, YW
   Feng, JQ
AF Miao, Yongwei
   Feng, Jieqing
TI Perceptual-saliency extremum lines for 3D shape illustration
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Shape illustration; Perceptual-saliency
   measure; Extremum lines; Triangular mesh
AB Owing to their efficiency for conveying perceptual information of the underlying shape and their pleasing perceiving in visual aesthetics experience, line drawings are now becoming a widely used technique for illustrating 3D shapes. Using a center-surrounding bilateral filter operator on Gaussian-weighted average of local projection height between mesh vertices and their neighbors, a new perceptual-saliency measure which can depict surface salient features, is proposed in this paper. Due to the definition of perceptual-saliency measure, our perceptual-saliency extremum lines can be considered as the ridge-valley lines of perceptual-saliency measure along the principal curvature directions on triangular meshes. The experimental results demonstrate that these extremum lines effectively capture and depict 3D shape information visually, especially for archaeological artifacts.
C1 [Miao, Yongwei] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Miao, Yongwei; Feng, Jieqing] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University
RP Miao, YW (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM ywmiao@zjut.edu.cn
RI Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060
FU 973 program of China [2009CB320801]; National Natural Science Foundation
   of China [60933007]; Science and Technology Planning Project of Zhejiang
   Province [2009C34005]; Natural Science Foundation of Zhejiang Province
   [Z1090630]
FX This work was supported by the 973 program of China under Grant No.
   2009CB320801, the National Natural Science Foundation of China under
   Grant No. 60933007 and the Science and Technology Planning Project of
   Zhejiang Province under Grant No. 2009C34005. This work was also partly
   supported by the Natural Science Foundation of Zhejiang Province under
   Grant No. Z1090630. The 3D models are courtesy of Stanford University,
   Princeton University, Rutgers University, Haifa University, Hebrew
   University, and the Aim@Shape.
CR Belyaev A., 2005, Mathematics of Surfaces XI 11th IMA International Conference. Proceedings (Lecture Notes in Computer Science Vol. 3604), P50, DOI 10.1007/11537908_4
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DECARLO D, INT S NONPH AN REND, P63
   DECARLO D, INT S NONPH AN REND, P15
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   GOOCH B, P ACM S INT 3D GRAPH, P31
   HERTZMANN A, P ACM SIGGRAPH 2000, P517
   INTERRANTE V, P IEEE VIS 1995, P52
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Kalnins RD, 2003, ACM T GRAPHIC, V22, P856, DOI 10.1145/882262.882355
   Koenderink J. J., 1990, Solid shape
   KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321
   KOLOMENKIN M, IEEE C COMP VIS PATT, P2767
   Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110
   LEE CH, P ACM SIGGRAPH 2005, P659
   LIU YS, P ACM S SOL PHYS MOD, P277
   MAATEN LJP, COMPUTER APPL QUANTI, P112
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   NI X, P ACM SIGGRAPH 2004, P613
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   RUSINKIEWICZ S, P ACM SIGGRAPH 2008
   RUSINKIEWICZ S, S 3D DAT PROC VIS TR, P486
   SAITO T, P ACM SIGGRAPH 1990, P197
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Stylianou G, 2004, IEEE T VIS COMPUT GR, V10, P536, DOI 10.1109/TVCG.2004.24
   TAUBIN G, P ACM SIGGRAPH 1995, P351
   TAUBIN G, P 5 INT C COMP VIS 1, P902
   Todd JT, 2004, TRENDS COGN SCI, V8, P115, DOI 10.1016/j.tics.2004.01.006
   Xie X, 2007, IEEE T VIS COMPUT GR, V13, P1328, DOI 10.1109/TVCG.2007.70538
   YOSHIZAWA S, ACM S SOL PHYS MOD 2, P227
   YOSHIZAWA S, P PAC GRAPH 2007, P231
   YUILLE AL, 1989, COMPUT VISION GRAPH, V45, P68, DOI 10.1016/0734-189X(89)90071-6
   ZHANG L, P ACM S INT 3D GRAPH, P129
NR 37
TC 13
Z9 17
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 433
EP 443
DI 10.1007/s00371-010-0458-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800005
DA 2024-07-18
ER

PT J
AU Silva, P
   Bando, Y
   Chen, BY
   Nishita, T
AF Silva, Paulo
   Bando, Yosuke
   Chen, Bing-Yu
   Nishita, Tomoyuki
TI Curling and clumping fur represented by texture layers
SO VISUAL COMPUTER
LA English
DT Article
DE Fur textures; Fur curling; Fur clumping; Real-time rendering
AB Fur is present in most mammals which are common characters in both movies and video-games, and it is important to model and render fur both realistically and quickly. When the objective is real-time performance, fur is usually represented by texture layers (or 3D textures), which limits the dynamic characteristics of fur when compared with methods that use an explicit representation for each fur strand.
   This paper proposes a method for animating and shaping fur in real-time, adding curling and clumping effects to the existing real-time fur rendering methods on the GPU. Besides fur bending using a mass-spring strand model embedded in the fur texture, we add small scale displacements to layers to represent curls which are suitable for vertex shader implementation, and we also use a fragment shader to compute intra-layer offsets to create fur clumps. With our method, it becomes easy to dynamically add and remove fur curls and clumps, as can be seen in real fur as a result of fur getting wet and drying up.
C1 [Silva, Paulo; Bando, Yosuke; Nishita, Tomoyuki] Univ Tokyo, Tokyo, Japan.
   [Bando, Yosuke] Toshiba Co Ltd, Tokyo, Japan.
   [Chen, Bing-Yu] Natl Taiwan Univ, Taipei 10764, Taiwan.
C3 University of Tokyo; Toshiba Corporation; National Taiwan University
RP Silva, P (corresponding author), Univ Tokyo, Tokyo, Japan.
EM paulo@nis-lab.is.s.u-tokyo.ac.jp; yosuke1.bando@toshiba.co.jp;
   robin@ntu.edu.tw; nis@nis-lab.is.s.u-tokyo.ac.jp
RI Chen, Bing-Yu/E-7498-2016
OI Chen, Bing-Yu/0000-0003-0169-7682
CR BAKAY B, 2002, EUROGRAPHICS 2002 SH, P32
   Banisch S, 2006, JOURNAL WSCG, V14, P25
   Bruderlin A, 2000, J VISUAL COMP ANIMAT, V11, P249, DOI 10.1002/1099-1778(200012)11:5<249::AID-VIS238>3.0.CO;2-G
   BRUDERLIN A, 2003, ACM SIGGRAPH 2003 CO
   CSURI C, 1979, ACM SIGGRAPH 1979 C, P289
   Elber G, 2005, IEEE COMPUT GRAPH, V25, P66, DOI 10.1109/MCG.2005.79
   FLEISCHER KW, 1995, ANN C SERIES SIGGRAP, P239
   GELDER AV, 1997, GRAPHICS INTERFACE 9, P181
   Goldman D. B., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P127, DOI 10.1145/258734.258807
   Gupta R, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P133
   Habel R, 2007, JOURNAL WSCG, V15, P123
   ISIDORO J, 2002, ACM SIGGRAPH 2002 AB, P273
   Jeschke S., 2007, Proceedings of EGSR07, P351
   JIAO S, 2009, P 2009 INT C VIRT RE, P35
   KAJIYA JT, 1989, ACM SIGGRAPH 89 C P, P271
   KLOETZLI J, 2006, THESIS U MARYLAND
   Kowalski MA, 1999, COMP GRAPH, P433, DOI 10.1145/311535.311607
   Lengyel J., 2001, P 2001 S INTERACTIVE, P227, DOI [10.1145/364338.364407, DOI 10.1145/364338.364407]
   Lengyel JE, 2000, SPRING COMP SCI, P243
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   McGuire Morgan., 2004, P 3 INT S NONPHOTORE, P35, DOI [10. 1145/987657. 987663, DOI 10.1145/987657.987663]
   Meyer A., 1998, Interactive Volumetric Textures, P157
   MILLER GS, 1988, GRAPHICS INTERFACE 8, P138
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   OWADA S, 2008, P 2008 EUR WORKSH SK, P1
   Papaioannou G., 2002, SIMPLE FAST TECHNIQU
   PERLIN K, 1989, ANN C SERIES, P253
   SABORET L, 2009, CGAL USER REFERENCE
   Sheng B, 2009, COMPUT ANIMAT VIRT W, V20, P205, DOI 10.1002/cav.289
   TAKEUCHI K, 2009, ACM SIGGRAPH ASIA 20
   Tariq S., 2008, ACM SIGGRAPH 2008 TA, P37
   YANG G, 2006, P 2006 ACM INT C VIR, P343
   Yang G, 2008, IEEE COMPUT GRAPH, V28, P85, DOI 10.1109/MCG.2008.73
NR 33
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 659
EP 667
DI 10.1007/s00371-010-0484-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, JW
   Li, L
   Yang, GQ
   Zhang, Y
   Sun, JZ
AF Zhang, Jiawan
   Li, Liang
   Yang, Guoqiang
   Zhang, Yi
   Sun, Jizhou
TI Local albedo-insensitive single image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Bilateral filter; Visibility restoration; Contrast
   enhancement
ID BILATERAL FILTER; ENHANCEMENT
AB In this paper, we present a new algorithm to remove haze from a single image. The proposed algorithm extracts transmission iteratively under the assumption that large-scale chromaticity variations are due to transmission while small-scale luminance variations are due to scene albedo. A nonlinear edge-preserving filter is introduced to incrementally refine subtle transmission map while still keeping sharp transmission map distinct. The algorithm is verified by both synthetic images and real-scene photographs. The results demonstrate that our method can produce transmission maps without being affected by the local albedo variations and, furthermore, recover haze-free images. On top of haze removal, several applications of the transmission map including refocusing and relighting are also implemented.
C1 [Zhang, Jiawan; Li, Liang; Yang, Guoqiang; Zhang, Yi; Sun, Jizhou] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Zhang, JW (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM yizhang@tju.edu.cn
OI Zhang, Jiawan/0000-0002-0667-6744
CR Aleksic M, 2006, PROC SPIE, V6069, DOI 10.1117/12.643880
   [Anonymous], P IEEE 12 INT C COMP
   [Anonymous], 1995, P MUSTERERKENNUNG 19
   [Anonymous], 2003, IEEE WORKSH COL PHOT
   [Anonymous], P IEEE C CVPR
   Barash D, 2004, IMAGE VISION COMPUT, V22, P73, DOI 10.1016/j.imavis.2003.08.005
   Bennett EP, 2005, ACM T GRAPHIC, V24, P845, DOI 10.1145/1073204.1073272
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   HE K, 2009, 27 IEEE C COMP VIS P, P1956
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   KOSCHMIEDER H, 1924, BEITRAGE PHYS FREIEN, P171
   Liu C, 2006, P IEEE COMP SOC C CV, V1, P901, DOI DOI 10.1109/CVPR.2006.207
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   OH BM, 2001, IMAGE BASED MODELLIN, P433
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   SCHECHNER YY, 2001, P IEEE COMP SOC C CO, V1, P1325
   Tan R.T., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587643
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
NR 23
TC 31
Z9 42
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 761
EP 768
DI 10.1007/s00371-010-0444-z
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800036
DA 2024-07-18
ER

PT J
AU Oshita, M
AF Oshita, Masaki
TI Generating animation from natural language texts and semantic analysis
   for motion search and scheduling
SO VISUAL COMPUTER
LA English
DT Article
DE Computer animation; Motion database; Natural language processing
AB This paper presents an animation system that generates an animation from natural language texts such as movie scripts or stories. It also proposes a framework for a motion database that stores numerous motion clips for various characters. We have developed semantic analysis methods to extract information for motion search and scheduling from script-like input texts. Given an input text, the system searches for an appropriate motion clip in the database for each verb in the input text. Temporal constraints between verbs are also extracted from the input text and are used to schedule the motion clips found. In addition, when necessary, certain automatic motions such as locomotion, taking an instrument, changing posture, and cooperative motions are searched for in the database. An animation is then generated using an external motion synthesis system. With our system, users can make use of existing motion clips. Moreover, because it takes natural language text as input, even novice users can use our system.
C1 Kyushu Inst Technol, Fukuoka 8208502, Japan.
C3 Kyushu Institute of Technology
RP Oshita, M (corresponding author), Kyushu Inst Technol, 680-4 Kawazu, Fukuoka 8208502, Japan.
EM oshita@ces.kyutech.ac.jp
CR [Anonymous], 1972, UNDERSTANDING NATURA
   Baba H, 1996, IEICE T INF SYST, VE79D, P591
   Badler NI, 2000, EMBODIED CONVERSATIONAL AGENTS, P256
   Bindiganavale R., 2000, Proceedings of the Fourth International Conference on Autonomous Agents, P293, DOI 10.1145/336595.337503
   CONWAY MJ, 1997, THESIS U VIRGINIA
   COYNE B, 2000, P SIGGRAPH 2001, P487
   Fillmore C.J., 1968, UNIVERSALS LINGUIST, P1, DOI DOI 10.4236/ENG
   HAYASHI M, 1999, P IM 99, P84
   Klein D, 2002, ADV NEUR IN, V14, P35
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Levine S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618518
   Lu R., 2002, Automatic generation of computer animation: using AI for movie animation
   Oshita M, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P146, DOI 10.1109/CW.2009.46
   Oshita M, 2008, COMPUT GRAPH FORUM, V27, P1909, DOI 10.1111/j.1467-8659.2008.01339.x
   Park S, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P105, DOI 10.1109/MOTION.2002.1182221
   Perlin Ken., 1996, SIGGRAPH 96, P205
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   SHIM H, 2008, P 2008 INT C ADV COM, P115
   SUMI K, 2006, P INT C ADV COMP ENT
   Tokugana T, 2004, LECT NOTES ARTIF INT, V3157, P635
NR 20
TC 8
Z9 11
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 339
EP 352
DI 10.1007/s00371-010-0423-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800005
DA 2024-07-18
ER

PT J
AU Wu, SY
   Xia, SH
   Wang, ZQ
   Li, CP
AF Wu, Shuangyuan
   Xia, Shihong
   Wang, Zhaoqi
   Li, Chunpeng
TI Efficient motion data indexing and retrieval with local similarity
   measure of motion strings
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Motion capture data; Indexing; Retrieval; Self-organizing map;
   Smith-Waterman algorithm
ID IDENTIFICATION
AB Widely used in data-driven computer animation, motion capture data exhibits its complexity both spatially and temporally. The indexing and retrieval of motion data is a hard task that is not totally solved. In this paper, we present an efficient motion data indexing and retrieval method based on self-organizing map and Smith-Waterman string similarity metric. Existing motion clips are first used to train a self-organizing map and then indexed by the nodes of the map to get the motion strings. The Smith-Waterman algorithm, a local similarity measure method for string comparison, is used in clustering the motion strings. Then the motion motif of each cluster is extracted for the retrieval of example-based query. As an unsupervised learning approach, our method can cluster motion clips automatically without needing to know their motion types. Experiment results on a dataset of various kinds of motion show that the proposed method not only clusters the motion data accurately but also retrieves appropriate motion data efficiently.
C1 [Wu, Shuangyuan; Xia, Shihong; Wang, Zhaoqi; Li, Chunpeng] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Wu, Shuangyuan] Chinese Acad Sci, Grad Sch, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Wu, SY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM wushuangyuan@ict.ac.cn; xsh@ict.ac.cn; zqwang@ict.ac.cn; cpli@ict.ac.cn
RI Li, Chunpeng/AAE-6134-2019
CR [Anonymous], 2004, P INT C VERY LARGE D
   [Anonymous], 2005, P ACM SIGGRAPH EUR S, DOI [10.1145/1073368.1073377, DOI 10.1145/1073368.1073377]
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Chiu CY, 2004, J VIS COMMUN IMAGE R, V15, P446, DOI 10.1016/j.jvcir.2004.04.004
   Dame B., 1998, Technical Report, DIKU-TR-98/5
   HAMMING RW, 1950, BELL SYST TECH J, V29, P147, DOI 10.1002/j.1538-7305.1950.tb00463.x
   Heyer LJ, 1999, GENOME RES, V9, P1106, DOI 10.1101/gr.9.11.1106
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kohonen T., 2001, INFORM SCIENCES
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   LI C, 2007, COMPUT COMMUN APPL, V3
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   PULLEN K, 2002, P 29 ANN C COMP GRAP, P501
   SAKAMOTO Y, 2004, P 2004 ACM SIGGRAPH, P259
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Yang K., 2004, ACM INT WORKSHOP MUL, P65, DOI DOI 10.1145/1032604.1032616
   Zordan V. B., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P89
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
NR 23
TC 23
Z9 28
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 499
EP 508
DI 10.1007/s00371-009-0345-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300014
DA 2024-07-18
ER

PT J
AU Navarro, F
   Gutierrez, D
   Serón, FJ
AF Navarro, Fernando
   Gutierrez, Diego
   Seron, Francisco J.
TI Interactive HDR lighting of dynamic participating media
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range; Participating media; Lighting; GPU; Real time;
   Volume density objects
AB In this paper, we present two optimization techniques to light and render volumetric data of inhomogeneous participating media. Both are independent of the lighting model selected. We use an implementation of the ray marching algorithm to approximate the Radiance Transfer Equation. The system can calculate single scattering in time-varying isotropic participating media with the incident field being modeled as a high dynamic range (HDR) environment map. We can use dynamic lighting (with certain restrictions) and free camera movement without using any precomputations while achieving interactive frame rates.
C1 [Navarro, Fernando] Lionhead Studios Microsoft Games Studios, Guildford GU2 7YQ, Surrey, England.
   [Gutierrez, Diego; Seron, Francisco J.] Inst Invest Ingn Aragon, Ctr Politecn Super, Grp Informat Graf, Zaragoza 50018, Spain.
C3 Microsoft
RP Navarro, F (corresponding author), Lionhead Studios Microsoft Games Studios, 1 Occam Court,Surrey Res Pk, Guildford GU2 7YQ, Surrey, England.
EM fnavarrog@gmail.com; diegog@unizar.es; seron@unizar.es
RI ; Seron Arbeloa, Francisco Jose/L-3146-2014
OI Gutierrez Perez, Diego/0000-0002-7503-7022; Seron Arbeloa, Francisco
   Jose/0000-0003-1683-4694
FU University of Zaragoza [UZ2007-TEC06]; Spanish Ministry of Science and
   Technology [TIN2007-63025]
FX The authors would like to express their gratitude to Tim Hawkins, Per
   Einarsson and Paul Debevec from the USC Institute for Creative
   Technologies, for the captured smoke data and hdr images used in this
   paper. The horse data set has been made available by Lin Shi and Yizhou
   Yu from the University of Illinois at Urbana-Champaign. Julian Flores
   from the University of Santiago de Compostela for his support in the
   initial steps of this work. We would also thank the anonymous reviewers
   for their keen insight and meaningful suggestions. This research has
   been funded by the projects UZ2007-TEC06 (University of Zaragoza) and
   TIN2007-63025 (Spanish Ministry of Science and Technology). Diego
   Gutierrez was additionally supported by a mobility grant by the Gobierno
   de Aragon (Ref: MI019/2007).
CR Blasi P., 1993, Computer Graphics Forum, V12, pC201, DOI 10.1111/1467-8659.1230201
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chandrasekhar S., 1950, RAD TRANSFER
   COHEN J, 2001, LIGHTGEN HDR SHOP PL
   CUNTZ N, 2007, EUROGRAPHICS 07
   Donnelly W., 2005, GPU GEMS 2, V1st, P123
   ENGEL K, 2002, EUROGRAPHICS STATE A
   ENTEZARI A, 2002, VVS 2002 P 2002 IEEE, P131
   Goodnight N., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P26
   Gorodnichev E. E., 1995, Journal of Experimental and Theoretical Physics, V80, P112
   GRZESZCZUK R, 1998, ACM SIGGRAPH 1998 CO
   GUTHE S, 2002, IEEE VISUALIZATION
   HARRIS MJ, 2001, EG 2001 P, V20, P76
   HAWKINS T, 2005, SIGGRAPH 2005, P812
   HEGEMAN K, 2005, S INT 3D GRAPH GAM, P117
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   JENSEN H.W., 1998, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, P311
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kalos Malvin, 1986, Monte Carlo methods, VI
   Kay T. L., 1986, Computer Graphics, V20, P269, DOI 10.1145/15886.15916
   Klein T, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P223
   Kollig T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P45
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   LI W, 2003, VIS 03, P42
   Max N. L., 1986, Computer Graphics, V20, P117, DOI 10.1145/15886.15899
   NARASIMHAN SG, 2003, SHEDDING LIGHT WEATH, P665
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   *NVIDIACORPORATION, FRAM OBJ EXT
   *NVIDIACORPORATION, CG LANG SPEC
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   RITSCHE N, 2004, GRAPHITE 06, P265
   RUSHMEIER HE, 1987, SIGGRAPH 87, P293
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
   Stam J, 1995, SPRING COMP SCI, P41
   STAM J, 1994, GRAPH INTER, P51
   Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309
   SZIRMAYKALOS L, 2005, RENDERING TECHNIQUES, P277
   TOST D, 2006, P SPIE
   VOLLRATH J, 2005, VMV 05
   Westermann R, 2001, IEEE VISUAL, P271, DOI 10.1109/VISUAL.2001.964521
   ZHOU K, 2007, FOGSHOP REAL TIME DE
   ZHOU K, 2007, REALTIME SMOKE RENDE
NR 48
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2009
VL 25
IS 4
BP 339
EP 347
DI 10.1007/s00371-008-0299-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 413YH
UT WOS:000263830600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Harm, DL
   Taylor, LC
   Reschke, MF
   Somers, JT
   Bloomberg, JJ
AF Harm, Deborah L.
   Taylor, Laura C.
   Reschke, Millard F.
   Somers, Jeffrey T.
   Bloomberg, Jacob J.
TI Sensorimotor coordination aftereffects of exposure to a virtual
   environment
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Virtual reality training; Eye-head-hand coordination; Gaze-holding;
   Adaptation
ID ADAPTATION
AB Virtual reality environments (VRs) offer unique training opportunities, particularly for training astronauts and preadapting them to microgravity. The purpose of the current research was to compare disturbances in eye-head-hand (EHH) and eye-head (GAZE) sensorimotor coordination produced by repeated exposures to VR systems. In general, we observed significant increases in position errors in manual target acquisition for both horizontal and vertical targets. We also observed a significant decrement in the ability of subjects to maintain gaze on horizontal eccentric targets immediately after exposure to VR. These preliminary findings provide some direction for developing training schedules for VR users that facilitate adaptation and support the idea that VRs may serve as an analog for sensorimotor effects of spaceflight.
C1 [Harm, Deborah L.; Reschke, Millard F.; Bloomberg, Jacob J.] NASA Johnson Space Ctr, Houston, TX 77058 USA.
   [Taylor, Laura C.; Somers, Jeffrey T.] Wyle Labs, Houston, TX 77058 USA.
C3 National Aeronautics & Space Administration (NASA); NASA Johnson Space
   Center
RP Harm, DL (corresponding author), NASA Johnson Space Ctr, 2101 NASA Pkwy, Houston, TX 77058 USA.
EM deborah.harm-1@nasa.gov
RI Somers, Jeffrey/N-4168-2014
OI Somers, Jeffrey/0000-0003-3347-6614
CR Bloomberg JJ, 2000, J VESTIBUL RES-EQUIL, V10, P75
   Ebenholtz S.M., 1992, Teleoperators and Virtual Environments, V1, P302, DOI DOI 10.1162/PRES.1992.1.3.302
   EBENHOLTZ SM, 1988, ADA212699 DEF TECHN
   FLOOK JP, 1977, PERCEPTION, V6, P15, DOI 10.1068/p060015
   HARM DL, 1991, AVIAT SPACE ENV MED, V62, P477
   HETTINGER LJ, 1987, P IMAGE 4 C PHOEN AZ, P320
   KENNEDY RS, 1998, PRESENCE TELEOPER VI
   KOLASINSKI G, 1995, SIMULATOR SICKNESS V, P1027
   Kozlovskaya I.B., 1989, PHYSIOLOGIST, V32, P45
   Martin TA, 1996, BRAIN, V119, P1199, DOI 10.1093/brain/119.4.1199
   McCauley M. E., 1992, Presence: Teleoperators & Virtual Environments, V1, P311, DOI DOI 10.1162/PRES.1992.1.3.311
   MCGONIGLE BO, 1978, NATURE, V272, P364, DOI 10.1038/272364a0
   Reschke MF, 1998, BRAIN RES REV, V28, P102, DOI 10.1016/S0165-0173(98)00031-9
   RESCHKE MF, 1996, SPACE BIOL MED, V3
   RUSHTON S, 1994, DISPLAYS, V15, P255, DOI 10.1016/0141-9382(94)90073-6
   Stanney K, 1998, INT J HUM-COMPUT INT, V10, P135, DOI 10.1207/s15327590ijhc1002_3
   Welch R.B., 1986, HDB PERCEPTION HUMAN, V1, p25
   WELCH RB, 1993, PERCEPT PSYCHOPHYS, V54, P195, DOI 10.3758/BF03211756
NR 18
TC 6
Z9 7
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 995
EP 999
DI 10.1007/s00371-008-0277-1
PG 5
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 359BQ
UT WOS:000259961600010
DA 2024-07-18
ER

PT J
AU Sheng, B
   Wu, EH
   Sun, HQ
AF Sheng, Bin
   Wu, Enhua
   Sun, Hanqiu
TI Sketching freeform meshes using graph rotation functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE freeform surfaces; subdivision; Laplace's equation
AB We present a new approach for sketching free form meshes with topology consistency. Firstly, we interpret the given 2D curve to be the projection of the 3D curve with the minimum curvature. Then we adopt a topology-consistent strategy based on the graph rotation system, to trace the simple faces on the interconnecting 3D curves. With the face tracing algorithm, our system can identify the 3D surfaces automatically. After obtaining the boundary curves for the faces, we apply Delaunay triangulation on these faces. Finally, the shape of the triangle mesh that follows the 3D boundary curves is computed by using harmonic interpolation. Meanwhile our system provides real-time algorithms for both control curve generation and the subsequent surface optimization. With the incorporation of topological manipulation into geometrical modeling, we show that automatically generated models are both beneficial and feasible.
C1 [Sheng, Bin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, Beijing 100864, Peoples R China.
   [Wu, Enhua] Univ Macau, Taipa, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Institute
   of Software, CAS; University of Macau
RP Sheng, B (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM bsheng@cse.cuhk.edu.hk
CR [Anonymous], AUTODESK MAYA
   [Anonymous], AUTODESK 3DS MAX
   Baumgart B., 1972, WINGED EDGE POLYHEDR
   GEORGE PL, 1998, TRIANGULATION MESHIN
   HOFFMANN CM, 1990, MANUFACT AUTIMATION, V48, P157
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Igarashi T., 2003, PROC ACM S INTERACTI, P139
   Karpenko O, 2002, COMPUT GRAPH FORUM, V21, P585, DOI 10.1111/1467-8659.t01-1-00709
   Karpenko OA, 2006, ACM T GRAPHIC, V25, P589, DOI 10.1145/1141911.1141928
   LETNIOWSKI FW, 1992, SIAM J SCI STAT COMP, V13, P765, DOI 10.1137/0913045
   MANTYLA M, 1986, ACM T GRAPHIC, V5, P1, DOI 10.1145/7529.7530
   Nealen A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276429
   OWADA S, 2003, P SMART GRAPH 2003 H
   Shewchuk J., 1996, First Workshop on Applied Computational Geometry, P124
   Sukumar N, 2003, INT J NUMER METH ENG, V57, P1, DOI 10.1002/nme.664
   Tai CL, 2004, COMPUT GRAPH FORUM, V23, P71, DOI 10.1111/j.1467-8659.2004.00006.x
NR 16
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 745
EP 752
DI 10.1007/s00371-008-0256-6
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800030
DA 2024-07-18
ER

PT J
AU Zhang, HX
   Xu, D
   Bao, HJ
AF Zhang, Hongxin
   Xu, Dong
   Bao, Hujun
TI Material-aware differential mesh deformation using sketching interface
SO VISUAL COMPUTER
LA English
DT Article
DE mesh deformation; non-uniform; sketching
AB In this paper, we present a material-aware mesh deformation method using a sketching interface. Guided by user-specified material properties, our method can deform the surface mesh in a non-uniform way, while previous deformation techniques are mainly designed for uniform materials. The non-uniform deformation is achieved by material-dependent gradient field manipulation and Poisson-based reconstruction. Compared with previous material-oblivious deformation techniques, our method supplies better control of the deformation process and can generate more realistic results. We propose a novel detail representation that transforms geometric details between successive surface levels as a combination of dihedral angles and barycentric coordinates. This detail representation is similarity-invariant and fully compatible with material properties. Based on these two methods, we implement a multi-resolution deformation tool, allowing the user to edit a mesh inside a hierarchy in a material-aware manner. We demonstrate the effectiveness and robustness of our methods by several examples with real-world data.
C1 [Zhang, Hongxin; Bao, Hujun] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Xu, Dong] Autodesk, Shang Hai, Peoples R China.
C3 Zhejiang University; Autodesk, Inc.
RP Zhang, HX (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM zhx@cad.zju.edu.cn; dong.xu@autodesk.com; bao@cad.zju.edu.cn
RI Zhang, Hongxin/T-3714-2019
CR Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Botsch M, 2003, COMPUT GRAPH FORUM, V22, P483, DOI 10.1111/1467-8659.00696
   BOTSCH M, 2004, P EUR S GEOM PROC, P189
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Kobbelt Leif., 2000, COMPUT GRAPH FORUM, V19
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   POPA T, 2005, SIGGRAPH 05 ACM SIGG, P5
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Sorkine Olga, 2005, EUROGRAPHICS
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   ZAYER R, HARMONIC GUIDANCE SU
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
   ZORIN D, 1997, P SIGGRAPH 97, P259
NR 18
TC 1
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 85
EP 93
DI 10.1007/s00371-007-0187-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200002
DA 2024-07-18
ER

PT J
AU Dominjon, L
   Perret, J
   Lecuyer, A
AF Dominjon, Lionel
   Perret, Jerome
   Lecuyer, Anatole
TI Novel devices and interaction techniques for human-scale haptics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2006 HAPTEX Workshop
CY NOV 21-23, 2006
CL Natl Commun Assoc, Helsinki, FINLAND
HO Natl Commun Assoc
DE virtual reality; human-scale haptics; force-feedback; haptic device;
   interaction technique; haptic hybrid control
AB Today, most haptic devices are designed for desktop applications. Therefore, when dealing with human-scale virtual environments, haptic interaction usually remains limited to only a small portion of the virtual world. This paper describes novel technologies aimed at addressing this issue. We investigate hardware solutions, with the design of devices offering a large workspace, as well as software solutions, with the description of a new interaction paradigm called haptic hybrid control. First evaluations demonstrate encouraging results and suggest that such solutions are relevant, depending on the targeted application.
C1 ENSAM Angers, P&I Lab, Angers, France.
   INRIA, IRISA, Bunraku Project, Rennes, France.
C3 Arts et Metiers Institute of Technology; Universite de Rennes; Inria
RP Dominjon, L (corresponding author), ENSAM Angers, P&I Lab, Angers, France.
EM dominjon@ingenierium.com; jerome.perret@haption.com;
   anatole.lecuyer@irisa.fr
CR [Anonymous], P IEEE RSJ INT C INT
   BARBAGLI F, 2004, STAR SPRINGER TRACTS, V9
   Borro D, 2004, IEEE COMPUT GRAPH, V24, P70, DOI 10.1109/MCG.2004.45
   BOUGUILA L, 1990, 1 INT WORKSH HAPT HU
   Brooks FP., 1990, P 17 ANN C COMP GRAP, DOI 10.1145/97879.97899
   Dominjon L., 2005, P WORLD HAPT C JOINT
   DOMINJON L, 2006, P IEEE INT C VIRT RE
   Dominjon L, 2006, LECT NOTES COMPUT SC, V4035, P288
   FISCHER A, 2003, P 7 INT IMM PROJ TEC
   Frisoli A, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P195
   Hand C, 1997, COMPUT GRAPH FORUM, V16, P269, DOI 10.1111/1467-8659.00194
   HOLLIS RL, 1993, P INT S ROB RES
   Johnsen E.G. W.R. Corliss., 1971, HUMAN FACTORS APPL T
   Mine M. R, 1995, Tech. Rep. TR95-018
   Nitzsche N, 2003, J ROBOTIC SYST, V20, P549, DOI 10.1002/rob.10105
   Poulton E.C., 1974, TRACKING SKILL MANUA
   POUPYREV I, 2000, P SIGCHI C HUM FACT, P540
   Shadpey F., 2005, CONTROL REDUNDANT RO, V316
   TARRIN N, P EUROGRAPHICS
   ZHAI S, 1993, P VRAIS 93 1 IEEE VI
   Zhai Shumin., 1996, P SIGCHI C HUMAN FAC, P308
   [No title captured]
   2004, HAPTION VIRTUOSE API
NR 23
TC 21
Z9 22
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2007
VL 23
IS 4
BP 257
EP 266
DI 10.1007/s00371-007-0100-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 172ZW
UT WOS:000246844300004
DA 2024-07-18
ER

PT J
AU Chan, B
   Wang, WP
AF Chan, B
   Wang, WP
TI Geocube - GPU accelerated real-time rendering of transparency and
   translucency
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE real-time rendering; GPU; transparency; translucency
AB We present a new method based on GPU acceleration for real-time transparency and translucency rendering. Our method computes refraction at both the front and back sides of a transparent object, as well as internal reflection, thus delivering interactive realistic transparency effects on a commodity PC. The real-time performance is made possible by a new acceleration data structure, called geocube, that enables the use of GPU for fast ray-surface intersection testing. In addition, within the same framework, we introduce the novel use of the mip-map for a hierarchical representation of a sequence of key prefiltered environment maps to simulate translucency. By taking ray depth into account and using GPU to interpolate the key filtered maps to produce the desired blurring effects, we achieve real-time realistic translucency rendering of slightly scattering media that allows show-through of background details.
C1 Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 University of Hong Kong
RP Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM bchan@cs.hku.hk; wenpin@cs.hku.hk
CR Akenine-Moller T., 2003, Real-Time Rendering
   CARR N, 2003, P GRAPH HARDW 03
   CHANDRASEKHAR S, 1964, RAD TRANSFER
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   KAUTZ J, 2000, P EG REND WORKSH 00
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Reinhard E, 2000, SPRING COMP SCI, P299
   Schroeder W., 1998, The visualization toolkit an object-oriented approach to 3D graphics
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Smith A., 1996, PROC C COMPUTER GRAP, P259
   Wald I, 2001, SPRING EUROGRAP, P277
   WALD I, 2003, P EUROPAR 2003, P499
NR 14
TC 3
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 579
EP 590
DI 10.1007/s00371-005-0312-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400010
DA 2024-07-18
ER

PT J
AU Huang, KS
   Chang, CF
   Hsu, YY
   Yang, SN
AF Huang, KS
   Chang, CF
   Hsu, YY
   Yang, SN
TI Key Probe: a technique for animation keyframe extraction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE keyframe extraction; matrix factorization; computer animation; shape
   blending
ID MOTION SYNTHESIS; RETRIEVAL
AB We present a novel constraint-based keyframe extraction technique, Key Probe. Based on animator-specified constraints, the method converts a skeleton-based motion or animated mesh to a keyframe-based representation. In contrast to previous curve simplification or clustering methods, we cast the keyframe extraction problem as a constrained matrix factorization problem and solve the problem based on the least-squares optimization technique. The extracted keyframes have two uses: they could be used for browsing or they may be blended to reconstruct all other frames of an animation. Our approach is general and suitable for both rigid-body and soft-body animations. Experiments on various types of animation examples show that the proposed method produces remarkable results in terms of quality and compression ratio. Empirical tests also show that our algorithm consistently offers better efficiency than those by principal component analysis (PCA) and independent component analysis (ICA).
C1 Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.
C3 National Tsing Hua University
RP Natl Tsing Hua Univ, Dept Comp Sci, 101,Kuang Fu Rd,Sec 2, Hsinchu 30043, Taiwan.
EM hks@glserver.cs.nthu.edu.tw; chunga@cs.nthu.edu.tw;
   mr936708@cs.nthu.edu.tw; snyang@cs.nthu.edu.tw
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Blanz Volker., 1999, P 26 ANN C COMPUTER, P187, DOI DOI 10.1145/311535.311556
   Briceno H. M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P136
   Chao SP, 2004, COMPUT ANIMAT VIRT W, V15, P259, DOI 10.1002/cav.28
   Cooper M, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P25
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Dimitrova N., 1997, Proceedings of the Sixth International Conference on Information and Knowledge Management. CIKM'97, P113, DOI 10.1145/266714.266876
   Girgensohn A, 2000, MULTIMED TOOLS APPL, V11, P347, DOI 10.1023/A:1009630817712
   Golub Gene H, 2012, MATRIX COMPUTATIONS
   GONG Y, 2000, COMPUTER VISION PATT, P174
   Guskov I., 2004, Proc. 2004 ACM SIG- GRAPH/Eurographics Symp. Comput. Animation (SCA '04), P183
   HURRI J, 2004, FASTICA
   Hyvärinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Lim IS, 2001, P ANN INT IEEE EMBS, V23, P1167
   LIM IS, 2001, P IFIP TC5WG510 DEFO, P169
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   Parent Rick., 2001, Computer animation: algorithms and techniques
   Park MJ, 2004, COMPUT ANIMAT VIRT W, V15, P245, DOI 10.1002/cav.27
   Pickering MJ, 2003, COMPUT VIS IMAGE UND, V92, P217, DOI 10.1016/j.cviu.2003.06.002
   Shelton CR, 2000, INT J COMPUT VISION, V38, P75, DOI 10.1023/A:1008170818506
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Whitaker H., 1981, Timing for Animation
NR 29
TC 35
Z9 43
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 532
EP 541
DI 10.1007/s00371-005-0316-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400005
DA 2024-07-18
ER

PT J
AU Lin, HW
   Chen, W
   Wang, GJ
AF Lin, HW
   Chen, W
   Wang, GJ
TI Curve reconstruction based on an interval B-spline curve
SO VISUAL COMPUTER
LA English
DT Article
DE point cloud; curve reconstruction; interval B-spline curve; reverse
   engineering
ID APPROXIMATION; SURFACE; POINTS; MODELS
AB Curve reconstruction that generates a piece of centric curve from a piece of planar strip-shaped point cloud is a fundamental problem in reverse engineering. In this paper, we present a new curve-reconstruction algorithm based on an interval B-spline curve. The algorithm constructs a rectangle sequence approximating the point cloud using a new data clustering technique, which facilitates the determination of curve order implied in the shape of the point cloud. A quasicentric point sequence and two pieces of boundary point sequences are then computed, based on which a piece of interval B-spline curve representing the geometric shape of the point cloud is constructed. Its centric curve is the final reconstructed curve. The whole algorithm is intuitive, simple, and efficient, as demonstrated by experimental results.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM chenwei@cad.zju.edu.cn
CR de Boor C, 1979, ARO report 79-3, P299
   FANG L, 1995, COMPUT AIDED DESIGN, V27, P48, DOI 10.1016/0010-4485(95)90752-2
   Goshtasby AA, 2000, ACM T GRAPHIC, V19, P185, DOI 10.1145/353981.353992
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Lin HW, 2002, COMPUT AIDED DESIGN, V34, P637, DOI 10.1016/S0010-4485(01)00130-0
   OROUKE J, 1994, COMPUTATIONAL GEOMET
   Pottmann H, 1998, COMPUTING, V60, P307, DOI 10.1007/BF02684378
   SEDERBERG TW, 1992, IEEE COMPUT GRAPH, V12, P87, DOI 10.1109/38.156018
   SHEN G, 1998, INT J SHAPE MODEL, V41, P35
   Taubin G, 1996, IEEE T PATTERN ANAL, V18, P321, DOI 10.1109/34.485559
   Tuohy ST, 1997, COMPUT AIDED DESIGN, V29, P791, DOI 10.1016/S0010-4485(97)00025-0
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
NR 15
TC 27
Z9 36
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 418
EP 427
DI 10.1007/s00371-005-0304-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100006
DA 2024-07-18
ER

PT J
AU Lee, J
   Kim, CH
AF Lee, J
   Kim, CH
TI Polygonal space carving with geometric anti-aliasing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY FEB 12-14, 2003
CL Tel Aviv Univ, Tel Aviv, ISRAEL
HO Tel Aviv Univ
DE shape recovery; polygonal carving; image-consistent shape; color
   variance; boundary refinement
AB This paper proposes an algorithm that solves the shape recovery problem from N arbitrary images. By introducing a polygonal carving technique, the proposed algorithm can reconstruct the image-consistent polygonal shape that is patched by input images. This algorithm eliminates the invalid vertices and polygons from the initial polygonal grid space according to the color variance that represents their image consistency. The carved shape is refined by moving the outlier vertices on the boundary of each image. The final reconstructed shape faithfully accounts for the input images, and its textured appearance reflects the similar color property of the target object.
C1 Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Korea University
RP Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM airjung@korea.ac.kr; chkim@korea.ac.kr
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MORRIS DD, 2000, CVPR, V1, P332
   POULIN P, 1998, P EUR WORKSH REND 98, P93, DOI DOI 10.1007/978-3-7091-6453-2_9
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   Seitz SM, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P17, DOI 10.1109/ICCV.1998.710696
   TAYLOR CJ, 1992, 1992 IEEE INTERNATIONAL CONF ON ROBOTICS AND AUTOMATION : PROCEEDINGS, VOLS 1-3, P1615, DOI 10.1109/ROBOT.1992.220021
NR 10
TC 3
Z9 4
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2004
VL 20
IS 4
BP 229
EP 242
DI 10.1007/s00371-003-0229-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 834XR
UT WOS:000222444700003
DA 2024-07-18
ER

PT J
AU Hua, J
   Qin, H
AF Hua, J
   Qin, H
TI Scalar-field-guided adaptive shape deformation and animation
SO VISUAL COMPUTER
LA English
DT Article
DE shape deformations; scalar fields; interaction techniques
ID LEVEL-SET APPROACH
AB In this paper, we propose a novel scalar-field-guided adaptive shape deformation (SFD) technique founded on PDE-based flow constraints and scalar fields of implicit functions. Scalar fields are used as embedding spaces. Upon deformation of the scalar field, a corresponding displacement/velocity field will be generated accordingly, which results in a shape deformation of the embedded object. In our system, the scalar field creation, sketching, and manipulation are both natural and intuitive. The embedded model is further enhanced with self-optimization capability. During the deformation we can also enforce various constraints on embedded models. In addition, this technique can be used to ease the animation design. Our experiments demonstrate that the new SFD technique is powerful, efficient, versatile, and intuitive for shape modeling and animation.
C1 SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM jinghua@cs.sunysb.edu; qin@cs.sunysb.edu
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [0830183] Funding Source: National Science Foundation
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   BARR AH, 1984, P SIGGRAPH 84 MINN U, P47
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   BLOOMENTHAL J, 1990, COMPUT GRAPH, V25, P109
   Breen DE, 2001, IEEE T VIS COMPUT GR, V7, P173, DOI 10.1109/2945.928169
   Bærentzen JA, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P175, DOI 10.1109/SMI.2002.1003543
   CHANG YK, 1994, P SIGGRAPH 94, P257
   COQUILLART S, 1991, COMP GRAPH, V25, P23, DOI 10.1145/127719.122720
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   CRESPIN B, 1999, P IMPL SURF 99 BORD, P17
   Desbrun M, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P143
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hua J, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P55, DOI 10.1109/SWG.2002.1226510
   Hua J, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P254, DOI 10.1109/PCCGA.2001.962881
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Jin XG, 2000, COMPUT GRAPH-UK, V24, P219, DOI 10.1016/S0097-8493(99)00156-9
   Karpenko O, 2002, COMPUT GRAPH FORUM, V21, P585, DOI 10.1111/1467-8659.t01-1-00709
   KIRSHNAMURTHY V, 1996, P SIGGRAPH 96 NEW OR, P313
   LAZARUS F, 1994, COMPUT AIDED DESIGN, V26, P607, DOI 10.1016/0010-4485(94)90103-1
   MacCracken Ron., 1996, SIGGRAPH, P181, DOI DOI 10.1145/237170.237247
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   MUSETH K, 2002, P SIGGRAPH, P330
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   PASKO A, 2001, VISUAL COMPUT, V11, P429
   Raviv A., 1999, Proceedings of Fifth ACM Symposium on Solid Modeling and Applications, P246
   Schmitt B., 2001, Proceedings of Sixth ACM Symposium on. Solid Modeling and Applications, P321
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   WELCH W, 1995, THESIS CARNEGIE MELL
   Whitaker RT, 1998, INT J COMPUT VISION, V29, P203, DOI 10.1023/A:1008036829907
   Witkin A.P., 1994, P 21 ANN C COMPUTER, P269, DOI [10.1145/1198555.1198656, DOI 10.1145/1198555.1198656, DOI 10.1145/192161.192227]
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
   Zeleznik R. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P163, DOI 10.1145/237170.237238
NR 34
TC 8
Z9 10
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2004
VL 20
IS 1
BP 47
EP 66
DI 10.1007/s00371-003-0225-z
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 809SA
UT WOS:000220655200004
DA 2024-07-18
ER

PT J
AU Wang, LY
   Wang, XW
   Li, B
   Xia, R
AF Wang, Luyao
   Wang, Xuewen
   Li, Bo
   Xia, Rui
TI A fast-training GAN for coal-gangue image augmentation based on a few
   samples
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Data enhancement; Coal-gangue detection; Machine vision; Deep learning;
   GAN
ID NETWORK
AB Data enhancement methods need to be carefully considered and studied for the widespread application of machine vision and deep learning in the mining field. Generative adversarial networks (GANs) prove successful at generating data. However, training a high-resolution image generation network depends on a large-scale dataset and takes a long time. For coal gangue detection, this paper proposes a stride-and-transpose-based progressive generative adversarial network (STP-GAN), which can achieve fast training on a few samples and generate high-resolution images in size of 10242. We employ stride convolutions, up-sampling, and average pooling to construct the model progressively and introduce noise and style optimization. We propose a hidden-layer-frozen progressive training scheme according to the model construction. Compared with other test GANs, STP-GAN generates more authentic and diverse images. The test results of advanced object detection models show that after the auxiliary training of STP-GAN, the mean average precision and average recall of coal-gangue detection are increased by up to 6.92% and 20.39%, respectively. The proposed method can effectively improve the accuracy of coal-gangue detection through data optimization.
C1 [Wang, Luyao; Wang, Xuewen; Li, Bo; Xia, Rui] Taiyuan Univ Technol, Sch Coll Mech & Vehicle Engn, Taiyuan 030024, Peoples R China.
C3 Taiyuan University of Technology
RP Wang, XW (corresponding author), Taiyuan Univ Technol, Sch Coll Mech & Vehicle Engn, Taiyuan 030024, Peoples R China.
EM 1198649735@qq.com; wangxuewen@tyut.edu.cn; libo@tyut.edu.cn
FU National Natural Science Foundation of China [51875386, 52204149];
   National Natural Science Foundation of China; Fund for Shanxi "1331"
   Project [202203021221051, 202103021223080]; Fundamental Research Program
   of Shanxi Province
FX This work was supported by the National Natural Science Foundation of
   China (51875386, 52204149); the Fund for Shanxi "1331" Project; and the
   Fundamental Research Program of Shanxi Province (202203021221051,
   202103021223080). We thank the anonymous reviewer for pointing out the
   issues in the manuscript.
CR Aggarwal A., 2021, International Journal of Information Management Data Insights, V1, DOI [10.1016/j.jjimei.2020.100004, DOI 10.1016/J.JJIMEI.2020.100004]
   Bochkovskiy A., 2020, PREPRINT
   Chen JQ, 2023, VISUAL COMPUT, V39, P5135, DOI 10.1007/s00371-022-02650-8
   Fan B, 2023, IEEE T MULTIMEDIA, V25, P1713, DOI 10.1109/TMM.2022.3154165
   Filippo MP, 2021, MINER ENG, V170, DOI 10.1016/j.mineng.2021.107007
   Ge Z, 2021, PREPRINT
   Gulrajani I, 2017, ADV NEUR IN, V30
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Li C., 2022, PREPRINT
   Li HF, 2022, IEEE T CIRC SYST VID, V32, P2814, DOI 10.1109/TCSVT.2021.3099943
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lv ZQ, 2021, APPL SOFT COMPUT, V113, DOI 10.1016/j.asoc.2021.107891
   Lv ZQ, 2021, POWDER TECHNOL, V377, P361, DOI 10.1016/j.powtec.2020.08.088
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mokhayeri F, 2020, IEEE WINT CONF APPL, P241, DOI 10.1109/WACV45572.2020.9093275
   Mumuni A, 2022, ARRAY-NY, V16, DOI 10.1016/j.array.2022.100258
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Obukhov Artem, 2020, Software Engineering Perspectives in Intelligent Systems. Proceedings of 4th Computational Methods in Systems and Software 2020. Advances in Intelligent Systems and Computing (AISC 1294), P102, DOI 10.1007/978-3-030-63322-6_8
   Phaphuangwittayakul A, 2023, VISUAL COMPUT, V39, P4015, DOI 10.1007/s00371-022-02566-3
   Qiao YX, 2021, IEEE T IMAGE PROCESS, V30, P3154, DOI 10.1109/TIP.2021.3058566
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Rao J, 2023, VISUAL COMPUT, V39, P2111, DOI 10.1007/s00371-022-02468-4
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shopon M, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR)
   Tang PZ, 2024, VISUAL COMPUT, V40, P2015, DOI 10.1007/s00371-023-02899-7
   Wang LY, 2023, MEASUREMENT, V219, DOI 10.1016/j.measurement.2023.113244
   Wang LY, 2023, INT J COAL PREP UTIL, V43, P1119, DOI 10.1080/19392699.2022.2096016
   Wang ZX, 2021, IEEE ACCESS, V9, P90816, DOI 10.1109/ACCESS.2021.3090780
   Xu DW, 2020, TRANSPORT RES C-EMER, V117, DOI 10.1016/j.trc.2020.102635
   Yan L, 2020, IEEE T GEOSCI REMOTE, V58, P3558, DOI 10.1109/TGRS.2019.2958123
   Yan PC, 2022, MEASUREMENT, V188, DOI 10.1016/j.measurement.2021.110530
   Yi X, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101552
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang W, 2021, IEEE T IND INFORM, V17, P7957, DOI 10.1109/TII.2021.3064377
   Zhang YC, 2022, MEASUREMENT, V198, DOI 10.1016/j.measurement.2022.111415
   Zhou H, 2020, OPT LETT, V45, P1695, DOI 10.1364/OL.387486
NR 39
TC 1
Z9 1
U1 13
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 22
PY 2023
DI 10.1007/s00371-023-03192-3
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DC1N7
UT WOS:001129743300001
DA 2024-07-18
ER

PT J
AU Li, W
   Gong, WJ
   Qian, YR
   Tian, HC
AF Li, Wei
   Gong, Weijun
   Qian, Yurong
   Tian, Haichen
TI STAM: a spatio-temporal adaptive module for improving static
   convolutions in action recognition
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Dynamic convolution; Adaptive learning; Self-attention; Multi-scale
   spatial attention
AB Temporal adaptive convolution has demonstrated superior performance over static convolution techniques in video understanding. However, it needs to be improved in long-time series modeling and multi-scale feature-map adaptation. To address these challenges, we introduce spatio-temporal hybrid adaptive convolution (STHAC), designed to enhance the spatio-temporal modeling capabilities of convolution. This is achieved by learning a set of spatio-temporal calibration filters to mitigate the spatial invariance intrinsic to static convolution methods. Specifically, STHAC learns a linear combination of N adaptive filters by parallelizing two lightweight attention branches. The resulting linearly mixed filters incorporate spatial multi-scale prior knowledge and long-range temporal dependencies. These spatio-temporal calibration filters modulate each frame's static convolutional weight parameters, thereby endowing static convolution with spatial multi-scale adaptability and long-range temporal modeling capabilities. Compared to other dynamic convolution methods, our proposed calibration filters require fewer parameters and incur lower computational complexity. Moreover, we introduce an Omni-dimensional aggregation module to augment the spatio-temporal modeling capacity of STHAC. When combined with STHAC, this aggregation module forms the spatio-temporal adaptive module (STAM) that can replace static convolution. We implement a spatio-temporal dynamic network based on STAM to validate our approach. Experimental results indicate that our model is competitive with state-of-the-art convolutional neural network architectures on action recognition benchmarks such as Kinetics-400(K400) and Something-Something V2(SSV2).
C1 [Li, Wei; Qian, Yurong] Xinjiang Univ, Sch Software, 499,Northwest Rd, Urumqi 830091, Xinjiang, Peoples R China.
   [Gong, Weijun; Qian, Yurong; Tian, Haichen] Xinjiang Univ, Sch Comp Sci & Technol, 777,Huarui St, Urumqi 830046, Xinjiang, Peoples R China.
   [Qian, Yurong] Xinjiang Univ, Key Lab Signal Detect & Proc, 666 Shengli Rd, Urumqi 830000, Xinjiang, Peoples R China.
C3 Xinjiang University; Xinjiang University; Xinjiang University
RP Qian, YR (corresponding author), Xinjiang Univ, Sch Software, 499,Northwest Rd, Urumqi 830091, Xinjiang, Peoples R China.; Qian, YR (corresponding author), Xinjiang Univ, Sch Comp Sci & Technol, 777,Huarui St, Urumqi 830046, Xinjiang, Peoples R China.; Qian, YR (corresponding author), Xinjiang Univ, Key Lab Signal Detect & Proc, 666 Shengli Rd, Urumqi 830000, Xinjiang, Peoples R China.
EM 107552101632@stu.xju.edu.cn; gongwj214@stu.xju.edu.cn; qyr@xju.edu.cn;
   107552101327@stu.xju.edu.cn
FU National Science Foundation of China; National Science and Technology
   Major Project [95-Y50G34-9001-22/23]; Autonomous Region Science and
   Technology Department International Cooperation Project [2020E01023]; 
   [62266043];  [U1803261]
FX This work was supported in part by the National Science Foundation of
   China under Grant 62266043 and U1803261, in part by National Science and
   Technology Major Project under Grant 95-Y50G34-9001-22/23, and in part
   by the Autonomous Region Science and Technology Department International
   Cooperation Project under Grant 2020E01023.
CR Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Elsayed G., 2020, P INT C MACH LEARN, P2868
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Geng TT, 2022, IEEE T IMAGE PROCESS, V31, P5484, DOI 10.1109/TIP.2022.3196175
   Gong WJ, 2023, NEURAL COMPUT APPL, V35, P6529, DOI 10.1007/s00521-022-08040-4
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Guo MH, 2023, COMPUT VIS MEDIA, V9, P733, DOI 10.1007/s41095-023-0364-2
   Han J, 1995, LECT NOTES COMPUT SC, V930, P195
   Hao Y., 2022, P IEEE CVF C COMP VI
   Hao YB, 2022, IEEE T IMAGE PROCESS, V31, P7279, DOI 10.1109/TIP.2022.3221292
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hinton G. E., 2012, arXiv
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZY, 2022, Arxiv, DOI arXiv:2110.06178
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lei Ba J., 2016, arXiv
   Li C, 2022, Arxiv, DOI arXiv:2209.07947
   Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214
   Li KC, 2021, Arxiv, DOI arXiv:2106.01603
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XH, 2020, PROC CVPR IEEE, P1089, DOI 10.1109/CVPR42600.2020.00117
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Nair Vinod, 2010, ICML, DOI DOI 10.5555/3104322.3104425
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ningning Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P776, DOI 10.1007/978-3-030-58555-6_46
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514, DOI 10.48550/ARXIV.1807.06514]
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2014, ADV NEUR IN, V27
   Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142
   Sudhakaran S., 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Tang CX, 2022, AAAI CONF ARTIF INTE, P2344
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xie Z, 2023, IEEE T MULTIMEDIA, V25, P7594, DOI 10.1109/TMM.2022.3224327
   Xudong Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P701, DOI 10.1007/978-3-030-58523-5_41
   Yang B, 2019, ADV NEUR IN, V32
   Yang ZQL, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10183290
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zhou JK, 2021, PROC CVPR IEEE, P6643, DOI 10.1109/CVPR46437.2021.00658
NR 55
TC 0
Z9 0
U1 3
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 7
PY 2023
DI 10.1007/s00371-023-03165-6
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2V2
UT WOS:001115674200002
DA 2024-07-18
ER

PT J
AU Jia, R
   Yang, HH
   Zhao, L
   Wu, XJ
   Zhang, YM
AF Jia, Ru
   Yang, Honghong
   Zhao, Li
   Wu, Xiaojun
   Zhang, Yumei
TI MPA-GNet: multi-scale parallel adaptive graph network for 3D human pose
   estimation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 3D human pose estimation; Multi-scale parallel network; Adaptive
   attention adjacency graph convolutional; Cluster graph pooling
ID SPARSE REPRESENTATION
AB Graph convolutional networks (GCNs) have achieved remarkable performance in the 2D-to-3D human pose estimation (HPE) task. The adjacency matrix in GCNs is crucial for feature aggregation in 3D HPE. However, existing GCN-based methods excessively rely on the fixed adjacency matrix to aggregate joint features from one-hop neighbor at a single scale, which limits the feature representation of skeleton data. To better improve the performance of 3D HPE, we have designed a multi-scale parallel adaptive graph network (MPA-GNet) for 3D HPE. The proposed network consists of three parallel multi-scale subgraph networks (PMS-Net) to efficiently capture human joint features at different scales. Specially, a multi-scale feature fusion module is devised to process multi-scale graph structural features and exchange information to generate rich hierarchical representations for skeleton data. To flexible construct graph topology in different scales, a special designed adaptive attention adjacency graph convolution network and a cluster graph pooling module are designed to construct the MPA-GNet in a parallel manner and capture the local subgraphs information in each PMS-Net. Finally, we conduct experiments on two 3D human pose challenging benchmark datasets Human3.6M and HumanEva-I for evaluating the effectiveness of the proposed model. The experimental results demonstrate that our model achieves competitive performance compared with some state-of-the-art 3D HPE methods.
C1 [Jia, Ru; Yang, Honghong; Zhao, Li; Wu, Xiaojun; Zhang, Yumei] Shaanxi Normal Univ, Sch Comp Sci, Xian 710062, Peoples R China.
   [Yang, Honghong; Wu, Xiaojun; Zhang, Yumei] Shaanxi Normal Univ, Key Lab Modern Teaching Technol, Minist Educ, Xian 710062, Peoples R China.
C3 Shaanxi Normal University; Shaanxi Normal University
RP Yang, HH; Wu, XJ (corresponding author), Shaanxi Normal Univ, Sch Comp Sci, Xian 710062, Peoples R China.; Yang, HH; Wu, XJ (corresponding author), Shaanxi Normal Univ, Key Lab Modern Teaching Technol, Minist Educ, Xian 710062, Peoples R China.
EM yanghonghong@snnu.edu.cn; xjwu@snnu.edu.cn
FU This work was supported in part by the National Natural Science
   Foundation of China (No.61907028, No.62007022, No.62107027,
   No.62377034), the Young science and technology stars in Shaanxi Province
   (2021KJXX-91), the Young Talent fund of University Associat [61907028,
   62007022, 62107027, 62377034]; National Natural Science Foundation of
   China [2021KJXX-91]; Young science and technology stars in Shaanxi
   Province [20200105]; Young Talent fund of University Association for
   Science and Technology in Shaanxi [2023YBGY158]; Fundamental Research
   Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China (No.61907028, No.62007022, No.62107027,
   No.62377034), the Young science and technology stars in Shaanxi Province
   (2021KJXX-91), the Young Talent fund of University Association for
   Science and Technology in Shaanxi (No.20200105) and the Fundamental
   Research Funds for the Central Universities (No.2023YBGY158,
   No.GK2021011004, No.2022TD-26, No.GK202205020, No.GK202205035).
CR Abu-El-Haifa S, 2019, PR MACH LEARN RES, V97
   Bai GH, 2023, IMAGE VISION COMPUT, V132, DOI 10.1016/j.imavis.2023.104649
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen ZH, 2020, LECT NOTES COMPUT SC, V12221, P276, DOI 10.1007/978-3-030-61864-3_24
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235
   Defferrard M, 2016, ADV NEUR IN, V29
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han CC, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108934
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Huang K., 2022, Multimed. Syst., P1
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang MX, 2019, NEUROCOMPUTING, V358, P332, DOI 10.1016/j.neucom.2019.05.034
   Kenkun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P318, DOI 10.1007/978-3-030-58607-2_19
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li H, 2023, Arxiv, DOI arXiv:2111.11927
   Li H, 2023, Arxiv, DOI [arXiv:2302.07408, 10.1609/aaai.v37i1.25213, DOI 10.1609/AAAI.V37I1.25213]
   Li SC, 2020, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR42600.2020.00621
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Liang S., 2022, Vis. Comput., P1
   Lin J., 2019, arXiv
   Lin MD, 2017, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2017.588
   Liu JF, 2021, IEEE INT CONF ROBOT, P3374, DOI 10.1109/ICRA48506.2021.9561605
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Shi BW, 2020, IEEE IMAGE PROC, P1491, DOI [10.1109/ICIP40778.2020.9191056, 10.1109/icip40778.2020.9191056]
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Verma P, 2022, VISUAL COMPUT, V38, P2417, DOI 10.1007/s00371-021-02120-7
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang KK, 2023, VISUAL COMPUT, V39, P429, DOI 10.1007/s00371-021-02339-4
   Wang L., 2019, P IEEECVF INT C COMP, P0
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Wu YQ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134987
   Wu YP, 2022, NEUROCOMPUTING, V487, P243, DOI 10.1016/j.neucom.2021.11.007
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yang HH, 2022, CHINESE J ELECTRON, V31, P266, DOI 10.1049/cje.2020.00.007
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Zeng AL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11416, DOI 10.1109/ICCV48922.2021.01124
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng CM, 2021, IEEE T MULTIMEDIA, V23, P2520, DOI 10.1109/TMM.2020.3013398
   Zhou K, 2019, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2019.00243
   Zou Z, 2020, BMVC
   Zou ZM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11457, DOI 10.1109/ICCV48922.2021.01128
NR 63
TC 0
Z9 0
U1 7
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 10
PY 2023
DI 10.1007/s00371-023-03142-z
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6QQ0
UT WOS:001099678500001
DA 2024-07-18
ER

PT J
AU Deng, ZM
   Yu, L
   Wang, LJ
   Ke, W
AF Deng, Zhongmin
   Yu, Ling
   Wang, Lijing
   Ke, Wei
TI An algorithm for cross-fiber separation in yarn hairiness image
   processing
SO VISUAL COMPUTER
LA English
DT Article
DE Yarn hairiness measurement; Cross-fiber separation; Image processing
   algorithm; Hairiness information table
AB Yarn hairiness affects yarn and fabric quality. The existing hairiness detection methods cannot discriminate crossover fibers or hairs. In order to accurately separate and detect crossover fibers, an algorithm of separating crossover hairs is proposed. By obtaining refined hair skeletons after image pretreatment, the positions of fiber intersection point were determined, and a hair information table according to the characteristics of the hair cross-point was constructed. After classifying each hair branch skeleton and screening out the hair common skeleton, the branch hair matching table by using the two end points of the true common hair skeleton adjacent to the hair branch was constructed. Through pairing the same cross-hair branch with the principle of the closest slope at the adjacent cross-end, each complete cross-hair skeleton was defined for hair count in a field of view. The detection results show that compared with the existing photoelectric hairiness detection instrument, the algorithm can realize the crossover hair separation, and calculate the length of complete crossover hairs and curved hairs with high accuracy. On average, the developed algorithm measures hair length about 11.1% longer than the manually measured results, while commercial apparatus would report hair length 62.5% shorter than the actual hair length.
C1 [Deng, Zhongmin; Yu, Ling; Ke, Wei] Wuhan Text Univ, State Key Lab New Text Mat & Adv Proc Technol, Wuhan 430200, Hubei, Peoples R China.
   [Wang, Lijing] RMIT Univ, Sch Fash & Text, 25 Dawson St, Brunswick, Vic 3056, Australia.
C3 Wuhan Textile University; Royal Melbourne Institute of Technology (RMIT)
RP Ke, W (corresponding author), Wuhan Text Univ, State Key Lab New Text Mat & Adv Proc Technol, Wuhan 430200, Hubei, Peoples R China.; Wang, LJ (corresponding author), RMIT Univ, Sch Fash & Text, 25 Dawson St, Brunswick, Vic 3056, Australia.
EM lijing.wang@rmit.edu.au; 328890925@qq.com
RI Ke, Wei/JXM-8153-2024
FU Royal Melbourne Institute of Technology
FX No Statement Available
CR Bai X, 2007, IEEE T PATTERN ANAL, V29, P449, DOI 10.1109/TPAMI.2007.59
   Fabijanska A, 2012, MACH VISION APPL, V23, P527, DOI 10.1007/s00138-012-0411-y
   Guha A, 2010, J TEXT I, V101, P214, DOI 10.1080/00405000802346412
   Huang XX, 2022, TEXT RES J, V92, P356, DOI 10.1177/00405175211035136
   Jing Jun-Feng, 2016, Journal of Donghua University (English Edition), V33, P587
   Khan K.R., 2020, J. Text. Sci. Technol., V06, P19, DOI [10.4236/jtst.2020.61003, DOI 10.4236/JTST.2020.61003]
   Li ZS, 2020, IEEE ACCESS, V8, P30928, DOI 10.1109/ACCESS.2020.2972967
   Ozkaya YA, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.1902743
   Rizvandi NB, 2008, Image Analysis and Recognition
   Srisang W., 2011, Walailak J. Sci. Tech, V3, P181
   Sun YY, 2017, J TEXT I, V108, P1271, DOI 10.1080/00405000.2016.1240144
   Tang Y., 2022, P SPIE 12331 INT C M
   Telli A, 2021, TEKST KONFEKSIYON, V31, P91, DOI 10.32710/tekstilvekonfeksiyon.770366
   Wang L, 2021, TEXT RES J, V91, P2263, DOI 10.1177/00405175211002863
   Wang WD, 2018, MEASUREMENT, V128, P220, DOI 10.1016/j.measurement.2018.06.029
   Xia ZG, 2019, TEXT RES J, V89, P4710, DOI 10.1177/0040517519841368
   Yuvaraj D, 2012, INDIAN J FIBRE TEXT, V37, P331
NR 17
TC 2
Z9 2
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3591
EP 3599
DI 10.1007/s00371-023-03053-z
EA AUG 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001062054700001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, ZK
   Guo, BL
   Meng, FJ
   Jiang, BT
AF Li, Zekun
   Guo, Baolong
   Meng, Fanjie
   Jiang, Bingting
TI Fast shape recognition via a bi-level restraint reduction of contour
   coding
SO VISUAL COMPUTER
LA English
DT Article
DE Restraint; Reduce restraint; Contour coding; Shape recognition
ID ENCODING SCHEME; CONTEXT; REPRESENTATION; MULTISCALE; TEXTURE
AB Shape recognition is an active research topic in the field of computer vision and graphic computing. Nevertheless, existing methods are still poor in accuracy and efficiency in some extent, which greatly limits their application in computer vision system. This paper investigates the restraint of feature structure that intrinsically deteriorates recognition performance. Furthermore, we propose a fast shape recognition method based on a bi-level restraint reduction of contour coding (CC2RR), which provides more effective theoretical support for the practical application of the visual algorithm. CC2RR reduces restraint performed from contour feature extraction and expression, respectively. First, for shape contour, the restraint of contour feature extraction is reduced by transforming the direction of contour points to contour segments; second, for the encoded contour segment, the restraint of the contour feature expression is reduced; in other words, the current direction is reduced to the previous and the next direction. Guided by these insights, Hamming code distance is used to match the coding features after the twofold restraint reduction, and the results are obtained. Experimental results verify that the method significantly improves the performance, which runs up to 500 times faster than the existing description methods based on shape contours while increasing robustness. This makes the method useful in practical software system.
C1 [Li, Zekun; Guo, Baolong; Meng, Fanjie] Xidian Univ, Inst Intelligent Control & Image Engn, TaiBai South Rd, Xian 710071, Peoples R China.
   [Jiang, Bingting] Hunan Univ, Coll Comp Sci & Elect Engn, LuShan South Rd, Changsha 410082, Peoples R China.
C3 Xidian University; Hunan University
RP Guo, BL (corresponding author), Xidian Univ, Inst Intelligent Control & Image Engn, TaiBai South Rd, Xian 710071, Peoples R China.
EM zkli@stu.xidian.edu.cn; blguo@xidian.edu.cn; fjmeng@xidian.edu.cn;
   jbingting@hnu.edu.cn
OI , Zekun/0000-0002-8406-0689
FU National Natural Science Foundation of China [62171341]; Natural Science
   Basic Research Program of Shaanxi Province of China [2020JM-196]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62171341 (Corresponding author: Baolong Guo) and
   Natural Science Basic Research Program of Shaanxi Province of China
   under Grant 2020JM-196 (Corresponding author: Fanjie Meng).
CR Aghito SM, 2006, IEEE T IMAGE PROCESS, V15, P2120, DOI 10.1109/TIP.2006.875168
   AN FP, 2022, VISUAL COMPUT, P1
   [Anonymous], T82 JBIG
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Bai X, 2012, IEEE T IMAGE PROCESS, V21, P2747, DOI 10.1109/TIP.2011.2170082
   Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85
   Bandyopadhyay SK, 2008, IEEE T CIRC SYST VID, V18, P840, DOI 10.1109/TCSVT.2008.918784
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bicego M, 2016, COMPUT VIS IMAGE UND, V145, P59, DOI 10.1016/j.cviu.2015.11.011
   BISWAS S, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383292
   CHANG CC, 1991, PATTERN RECOGN, V24, P1053, DOI 10.1016/0031-3203(91)90121-K
   Daliri MR, 2008, PATTERN RECOGN, V41, P1782, DOI 10.1016/j.patcog.2007.10.020
   Deutsch JA, 1955, BRIT J PSYCHOL, V46, P30, DOI 10.1111/j.2044-8295.1955.tb00521.x
   EDEN M, 1985, SIGNAL PROCESS, V8, P381, DOI 10.1016/0165-1684(85)90001-5
   Felzenszwalb PF, 2007, PROC CVPR IEEE, P367
   GRAHAM DN, 1967, PR INST ELECTR ELECT, V55, P336, DOI 10.1109/PROC.1967.5490
   GRATTONI P, 1990, PATTERN RECOGN LETT, V11, P95, DOI 10.1016/0167-8655(90)90119-M
   Grigorescu C, 2003, IEEE T IMAGE PROCESS, V12, P1274, DOI 10.1109/TIP.2003.816010
   Hoyer PO, 2002, VISION RES, V42, P1593, DOI 10.1016/S0042-6989(02)00017-2
   Hu RX, 2012, IEEE T IMAGE PROCESS, V21, P4667, DOI 10.1109/TIP.2012.2207391
   Jing Zhang, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P386, DOI 10.1109/ICDAR.2009.175
   Kaothanthong N, 2016, PATTERN RECOGN LETT, V78, P14, DOI 10.1016/j.patrec.2016.03.029
   Katsaggelos AK, 1998, P IEEE, V86, P1126, DOI 10.1109/5.687833
   Kera SB., 2022, VISUAL COMPUT, V2022, P1
   Kim KJ, 2000, IEEE T IMAGE PROCESS, V9, P1667, DOI 10.1109/83.869178
   Kumar AK, 2023, VISUAL COMPUT, V39, P2847, DOI 10.1007/s00371-022-02497-z
   Lai ZY, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.4.043009
   Lai ZY, 2011, IEEE IMAGE PROC, P225, DOI 10.1109/ICIP.2011.6116088
   Li ZK, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14225845
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Ling HB, 2010, LECT NOTES COMPUT SC, V6313, P411
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Luo HT, 2005, IEEE T CIRC SYST VID, V15, P345, DOI 10.1109/TCSVT.2004.842596
   Luo S, 2015, IEEE SENS J, V15, P5001, DOI 10.1109/JSEN.2015.2432127
   Martin K, 2006, IEEE T CIRC SYST VID, V16, P1196, DOI 10.1109/TCSVT.2006.882388
   Matousek Jiri, 2002, Lectures on discrete geometry, V212
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591
   Nunes P, 2000, SIGNAL PROCESS-IMAGE, V15, P585, DOI 10.1016/S0923-5965(99)00041-7
   OrganisationInternationale deNormalisation, 1999, INF TECHN GEN COD 2
   Peng HL, 1997, PATTERN RECOGN LETT, V18, P791, DOI 10.1016/S0167-8655(97)00050-0
   Qiang L, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P343, DOI 10.1109/ISPACS.2009.5383832
   Ross M, 2004, EUR EAT DISORD REV, V12, P129, DOI 10.1002/erv.531
   Schuster GM, 1998, IEEE T IMAGE PROCESS, V7, P13, DOI 10.1109/83.650847
   Shu X, 2011, IMAGE VISION COMPUT, V29, P286, DOI 10.1016/j.imavis.2010.11.001
   TARR MJ, 1989, COGNITIVE PSYCHOL, V21, P233, DOI 10.1016/0010-0285(89)90009-1
   TITU T., 1993, INF TECHN COD REPR P
   Wang B, 2019, IEEE T IMAGE PROCESS, V28, P5963, DOI 10.1109/TIP.2019.2921526
   Wang B, 2017, PROC CVPR IEEE, P2047, DOI 10.1109/CVPR.2017.221
   Wu Y, 2023, VISUAL COMPUT, V39, P393, DOI 10.1007/s00371-021-02337-6
   Wulandhari LA, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND ENGINEERING, PROCEEDINGS, P648, DOI 10.1109/ICIME.2009.32
   Yang CZ, 2018, NEUROCOMPUTING, V275, P1160, DOI 10.1016/j.neucom.2017.09.067
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zheng AM, 2017, IEEE T IMAGE PROCESS, V26, P574, DOI 10.1109/TIP.2016.2627813
   Zheng Y, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030486
NR 56
TC 0
Z9 0
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2599
EP 2614
DI 10.1007/s00371-023-02940-9
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001060100700003
DA 2024-07-18
ER

PT J
AU Dai, JC
   Fan, RB
   Song, YP
   Guo, Q
   He, FZ
AF Dai, Jicheng
   Fan, Rubin
   Song, Yupeng
   Guo, Qing
   He, Fazhi
TI MEAN: An attention-based approach for 3D mesh shape classification
SO VISUAL COMPUTER
LA English
DT Article
DE 3D mesh; 3D shape processing; Self-attention mechanism; Non-local
   features
ID SEGMENTATION
AB 3D shape processing is a fundamental computer application. Specifically, 3D mesh could provide a natural and detailed way for object representation. However, due to its non-uniform and irregular data structure, applying deep learning technologies to 3D mesh is difficult. Furthermore, previous deep learning approaches for 3D mesh mainly focus on local structural features and there is a loss of information. In this paper, to make better mesh shape awareness, a novel deep learning approach is proposed, which aims to full-use the information of mesh data and exploit comprehensive features for more accurate classification. To utilize self-attention mechanism and learn global features of mesh edges, we propose a novel attention-based structure with the edge attention module. Then, for local feature learning, our model aggregates edge features from adjacent edges. We refine the network by discarding pooling layers for efficiency. Thus, it captures comprehensive features from both local and global fields for better shape awareness. Moreover, we adopt spatial position encoding module based on spatial information of edges to enhance the model to better recognize edges and make full use of mesh data. We demonstrate effectiveness of our model in classification tasks with numerous experiments which show outperforming results on popular datasets.
C1 [Dai, Jicheng; Fan, Rubin; Song, Yupeng; Guo, Qing; He, Fazhi] Wuhan Univ, Sch Comp Sci, Bayi Rd, Wuhan 430072, Peoples R China.
   [He, Fazhi] Natl Engn Res Ctr Multimedia Software, Bayi Rd, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Bayi Rd, Wuhan 430072, Peoples R China.; He, FZ (corresponding author), Natl Engn Res Ctr Multimedia Software, Bayi Rd, Wuhan 430072, Peoples R China.
EM fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018
FU National Natural Science Foundation of China [62072348]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No.62072348). The numerical calculations in this paper have
   been done on the supercomputing system in the Supercomputing Center of
   Wuhan University.
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Ben Izhak R, 2022, IEEE WINT CONF APPL, P2937, DOI 10.1109/WACV51458.2022.00299
   Bukenberger DR, 2021, VISUAL COMPUT, V37, P2725, DOI 10.1007/s00371-021-02183-6
   Buonamici F, 2020, VISUAL COMPUT, V36, P375, DOI 10.1007/s00371-018-01624-z
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Chen Y, 2021, IEEE T MULTIMEDIA, V23, P3098, DOI 10.1109/TMM.2020.3020693
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Devlin J., 2018, BERT PRE TRAINING DE
   Díaz J, 2017, VISUAL COMPUT, V33, P47, DOI 10.1007/s00371-015-1151-6
   Dong Q, 2022, ARXIV
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Haouchine N, 2020, VISUAL COMPUT, V36, P211, DOI 10.1007/s00371-018-1600-0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu SM, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3506694
   Hua H, 2018, VISUAL COMPUT, V34, P985, DOI 10.1007/s00371-018-1548-0
   Huang J., 2018, ARXIV
   Jain N, 2018, VISUAL COMPUT, V34, P887, DOI 10.1007/s00371-018-1539-1
   Kim Y, 2021, J MECH SCI TECHNOL, V35, P3131
   Lahav A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417806
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Li S., VISUAL COMPUT
   Li XJ, 2022, LECT NOTES COMPUT SC, V13689, P541, DOI 10.1007/978-3-031-19818-2_31
   Lian CF, 2019, LECT NOTES COMPUT SC, V11769, P837, DOI 10.1007/978-3-030-32226-7_93
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Lin Kevin, 2021, P IEEE CVF INT C COM, P12939
   Mancini S, 2018, COMPUT IND ENG, V122, P140, DOI 10.1016/j.cie.2018.05.045
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Milano F., 2020, Advances in Neural Information Processing Systems, P952
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qiao YL, 2022, IEEE T VIS COMPUT GR, V28, P1317, DOI 10.1109/TVCG.2020.3014449
   Regli W, 2016, COMPUT AIDED DESIGN, V77, P73, DOI 10.1016/j.cad.2016.03.002
   Schneider L, 2021, COMPUT METH PROG BIO, V210, DOI 10.1016/j.cmpb.2021.106372
   Sharp N, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3507905
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Smirnov D, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459797
   Song YP, 2022, COMPUT AIDED DESIGN, V146, DOI 10.1016/j.cad.2022.103196
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P, 2017, ARXIV
   Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275
   Wang KK, 2021, VISUAL COMPUT, V37, P603, DOI 10.1007/s00371-020-01826-4
   Wang PY, 2018, COMPUT GRAPH-UK, V70, P128, DOI 10.1016/j.cag.2017.07.030
   Wang Y., 2010, COMPUT AIDED DES APP, V7, P759, DOI [10.3722/cadaps.2010.759-776, DOI 10.3722/CADAPS.2010.759-776]
   Xiao D, 2011, COMPUT GRAPH-UK, V35, P685, DOI 10.1016/j.cag.2011.03.020
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu HT, 2017, IEEE I CONF COMP VIS, P2717, DOI 10.1109/ICCV.2017.294
   Yeo C, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-01313-3
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zhang J, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-1523-9
   Zhang YW, 2021, IEEE T SYST MAN CY-S, V51, P3796, DOI 10.1109/TSMC.2019.2931723
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 62
TC 0
Z9 0
U1 7
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2987
EP 3000
DI 10.1007/s00371-023-03003-9
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001029353500001
DA 2024-07-18
ER

PT J
AU Hou, YX
   Ren, Z
   Hou, QM
   Tao, YB
   Jiang, YK
   Chen, W
AF Hou, Yuxuan
   Ren, Zhong
   Hou, Qiming
   Tao, Yubo
   Jiang, Yankai
   Chen, Wei
TI InstantTrace: fast parallel neuron tracing on GPUs
SO VISUAL COMPUTER
LA English
DT Article
DE Neuron tracing; Neuron visualization; Image processing; GPU acceleration
ID OPTICAL MICROSCOPY IMAGES; RECONSTRUCTION; VISUALIZATION
AB Neuron tracing, also known as neuron reconstruction, is an essential step in investigating the morphology of neuronal circuits and mechanisms of the brain. Since the ultra-high throughput of optical microscopy (OM) imaging leads to images of multiple gigabytes or even terabytes, it takes tens of hours for the state-of-the-art methods to generate a neuron reconstruction from a whole mouse brain OM image. We introduce InstantTrace, a novel framework that utilizes parallel neuron tracing on GPUs, achieving a significant speed boost of more than 20x compared to state-of-the-art methods with comparable reconstruction quality on the BigNeuron dataset. Our framework utilizes two methods to achieve this performance advance. Firstly, it takes advantage of the sparse feature and tree structure of the neuron image, which serial tracing methods cannot fully exploit. Secondly, all stages of the neuron tracing pipeline, including the initial reconstruction stage that have not been parallelized in the past, are executed on GPU using carefully designed parallel algorithms. Furthermore, to investigate the applicability and robustness of the InstantTrace framework, a test on a whole mouse brain OM Image is conducted, and a preliminary neuron reconstruction of the whole brain is finished within 1 h on a singleGPU, an order of magnitude faster than the existing methods. Our framework has the potential to significantly improve the efficiency of the neuron tracing process, allowing neuron image experts to obtain a preliminary reconstruction result instantly before engaging in manual verification and refinement.
C1 [Hou, Yuxuan; Ren, Zhong; Hou, Qiming; Tao, Yubo; Jiang, Yankai; Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Ren, Z (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM houyuxuan@zju.edu.cn; renzhong@zju.edu.cn; hqm03ster@gmail.com;
   taoyubo@cad.zju.edu.cn; jyk1996ver@zju.edu.cn; chenvis@zju.edu.cn
RI Chen, Wei/AAR-9817-2020; Jiang, Yankai/JXX-0818-2024
OI Jiang, Yankai/0000-0003-1903-2232
CR Al-Awami AK, 2016, IEEE T VIS COMPUT GR, V22, P738, DOI 10.1109/TVCG.2015.2467441
   Bakunas-Milanowski D., 2017, Int. J. Netw. Comput., V7, P208
   Boorboor S, 2019, IEEE T VIS COMPUT GR, V25, P1018, DOI 10.1109/TVCG.2018.2864852
   Brown KM, 2011, NEUROINFORMATICS, V9, P143, DOI 10.1007/s12021-010-9095-5
   Chen Hanbo, 2015, Brain Inform, V2, P135
   Chen WX, 2022, IEEE T MED IMAGING, V41, P1031, DOI 10.1109/TMI.2021.3130934
   Chen XJ, 2021, IEEE T MED IMAGING, V40, P3205, DOI 10.1109/TMI.2021.3080695
   Ghahremani P, 2022, IEEE T VIS COMPUT GR, V28, P4951, DOI 10.1109/TVCG.2021.3109460
   Haehn D, 2014, IEEE T VIS COMPUT GR, V20, P2466, DOI 10.1109/TVCG.2014.2346371
   Halavi M, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00049
   Jones S., 2012, GPU TECHNOLOGY C PRE, V338, P2012
   Li QF, 2020, IEEE T MED IMAGING, V39, P425, DOI 10.1109/TMI.2019.2926568
   Liu M, 2019, IEEE T MED IMAGING, V38, P1923, DOI 10.1109/TMI.2019.2893117
   Liu SQ, 2018, IEEE T MED IMAGING, V37, P2441, DOI 10.1109/TMI.2018.2833420
   Liu SQ, 2016, NEUROINFORMATICS, V14, P387, DOI 10.1007/s12021-016-9302-0
   McDonald T, 2021, IEEE T VIS COMPUT GR, V27, P744, DOI 10.1109/TVCG.2020.3030363
   Peng HC, 2017, NAT METHODS, V14, P332, DOI 10.1038/nmeth.4233
   Peng HC, 2015, NEURON, V87, P252, DOI 10.1016/j.neuron.2015.06.036
   Peng HC, 2011, BIOINFORMATICS, V27, pI239, DOI 10.1093/bioinformatics/btr237
   Peng HC, 2010, NAT BIOTECHNOL, V28, P348, DOI 10.1038/nbt.1612
   Quan TW, 2016, NAT METHODS, V13, P51, DOI [10.1038/nmeth.3662, 10.1038/NMETH.3662]
   Svoboda K, 2011, NEUROINFORMATICS, V9, P97, DOI 10.1007/s12021-011-9097-y
   Tan YH, 2020, IEEE T MED IMAGING, V39, P1195, DOI 10.1109/TMI.2019.2945980
   Usher W, 2018, IEEE T VIS COMPUT GR, V24, P994, DOI 10.1109/TVCG.2017.2744079
   Wang S, 2012, GRAPH MODELS, V74, P109, DOI 10.1016/j.gmod.2012.03.008
   Wang W, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-018-07979-0
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360619
   Xiao H, 2013, BIOINFORMATICS, V29, P1448, DOI 10.1093/bioinformatics/btt170
   Yang B, 2022, IEEE T MED IMAGING, V41, P903, DOI 10.1109/TMI.2021.3125777
   Yang J, 2019, NEUROINFORMATICS, V17, P185, DOI 10.1007/s12021-018-9392-y
   Zhao J, 2020, IEEE T MED IMAGING, V39, P4034, DOI 10.1109/TMI.2020.3009148
   Zhou Z, 2016, NEUROINFORMATICS, V14, P41, DOI 10.1007/s12021-015-9278-1
NR 32
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3783
EP 3796
DI 10.1007/s00371-023-02969-w
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001028675100001
DA 2024-07-18
ER

PT J
AU Zang, Y
   Cao, RL
   Li, H
   Hu, WJ
   Liu, QS
AF Zang, Ying
   Cao, Runlong
   Li, Hui
   Hu, Wenjun
   Liu, Qingshan
TI MAPD: multi-receptive field and attention mechanism for multispectral
   pedestrian detection
SO VISUAL COMPUTER
LA English
DT Article
DE Pedestrian detection; Multi-receptive field; Attention mechanism
ID NEURAL-NETWORKS; FUSION
AB For pedestrian detection in all weather conditions, multispectral imagery is the preferred solution for multimodal data acquisition. Due to the complementarity of multispectral data, the performance of pedestrian detection has been continuously improved. However, it is precise because of the diversity of data that the complexity of the model is increased. How to obtain a simple and lightweight high-performance network is the primary problem that the industry needs to solve. To solve this problem, this paper firstly proposes a lightweight high-performance network multi-receptive field and attention mechanism for multispectral pedestrian detection (MAPD). MAPD is a new scalable network based on multi-receptive field and attention mechanism. It cleverly combines multi-receptive field module and convolutional block attention module (CBAM) attention module to obtain multi-receptive field and attention module (MA). The module can be easily embedded into other network structures. After that, we analyze the proportion of pedestrian objects in the image to determine the receptive field range of the target, and use this range to design the multi-receptive field module of the network to obtain a network model suitable for detecting different object tasks. The MAPD network proposed in this paper has a very low parameter amount. Ablation studies on KAIST and CVC-14 datasets show that our method is effective and achieves state-of-the-art detection performance.
C1 [Zang, Ying; Cao, Runlong; Li, Hui; Hu, Wenjun; Liu, Qingshan] Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
C3 Huzhou University
RP Hu, WJ (corresponding author), Huzhou Univ, Sch Informat Engn, Huzhou 313000, Peoples R China.
EM 02750@zjhu.edu.cn; crl1567@163.com; 2096766348@qq.com;
   hoowenjun@foxmail.com; 02726@zjhu.edu.cn
RI Liu, Qingshan/A-2837-2011
OI Liu, Qing-Shan/0000-0002-0949-8066
CR Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen YT, 2022, LECT NOTES COMPUT SC, V13669, P139, DOI 10.1007/978-3-031-20077-9_9
   Choi H, 2016, INT C PATT RECOG, P621, DOI 10.1109/ICPR.2016.7899703
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kailai Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P787, DOI 10.1007/978-3-030-58523-5_46
   Kim J, 2021, IEEE ROBOT AUTOM LET, V6, P7846, DOI 10.1109/LRA.2021.3099870
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Li C.T., 2018, ARXIV
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu J., 2016, ARXIV
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mnih V, 2014, ADV NEUR IN, V27
   Park K, 2018, PATTERN RECOGN, V80, P143, DOI 10.1016/j.patcog.2018.03.007
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Su W., 2019, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q., 2020, P IEEE CVF C COMP VI, P8326, DOI [DOI 10.1109/CVPR42600.2020.00835, 10.1109/CVPR42600.2020.00835]
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang W., 2022, ARXIV
   Wenpeng Li, 2021, Journal of Physics: Conference Series, V1848, DOI 10.1088/1742-6596/1848/1/012074
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Xinjiang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13356, DOI 10.1109/CVPR42600.2020.01337
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yuan Y, 2018, OCNET OBJECT CONTEXT
   Zhang H, 2021, IEEE WINT CONF APPL, P72, DOI 10.1109/WACV48630.2021.00012
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang L, 2019, INFORM FUSION, V50, P20, DOI 10.1016/j.inffus.2018.09.015
   Zheng Y, 2019, ARXIV
NR 47
TC 1
Z9 1
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2819
EP 2831
DI 10.1007/s00371-023-02988-7
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001025599200001
DA 2024-07-18
ER

PT J
AU Zhao, YY
   Zhou, H
   Cheng, H
   Huang, CG
AF Zhao, Yuyao
   Zhou, Hang
   Cheng, Hai
   Huang, Chunguang
TI Cross-modal pedestrian re-recognition based on attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Cross-modality; Attention mechanisms; Data
   integration
AB Person re-identification, as an essential research direction in intelligent security, has gained the focus of researchers and scholars. In practical scenarios, visible light cameras depend highly on lighting conditions and have limited detection capability in poor light. Therefore, many scholars have gradually shifted their research goals to cross-modality person re-identification. However, there are few relevant studies, and challenges remain in resolving the differences in the images of different modalities. In order to solve these problems, this paper will use the research method based on the attention mechanism to narrow the difference between the two modes and guide the network in a more appropriate direction to improve the recognition performance of the network. Aiming at the problem of using the attention mechanism method can improve training efficiency. However, it is easy to cause the model training instability. This paper proposes a cross-modal pedestrian re-recognition method based on the attention mechanism. A new attention mechanism module is designed to allow the network to use less time to focus on more critical features of a person. In addition, a cross-modality hard center triplet loss is designed to supervise the model training better. The paper has conducted extensive experiments on the above two methods on two publicly available datasets, which obtained better performance than similar current methods and verified the effectiveness and feasibility of the proposed methods in this paper.
C1 [Zhao, Yuyao; Zhou, Hang; Cheng, Hai; Huang, Chunguang] Heilongjiang Univ, Coll Elect & Engn, Harbin, Heilongjiang, Peoples R China.
C3 Heilongjiang University
RP Cheng, H (corresponding author), Heilongjiang Univ, Coll Elect & Engn, Harbin, Heilongjiang, Peoples R China.
EM 2910753360@qq.com; 524563281@qq.com; chengh@hlju.edu.cn;
   2002041@hlju.edu.cn
FU National Natural Science Foundation of China [51607059]; Heilongjiang
   Postdoctoral Financial Assistance, China [LBH-Z20188]; Basic Science
   Research Project of Heilongjiang Univesity, China [KJCX201904,
   2020-KYYWF-1001]
FX This work is supported by the National Natural Science Foundation of
   China (51607059),Heilongjiang Postdoctoral Financial Assistance, China
   (LBH-Z20188) and the Basic Science Research Project of Heilongjiang
   Univesity, China (KJCX201904,2020-KYYWF-1001).
CR Bellen E., TENCON 2022 2022 IEE
   Cui Y., 2021, TF BLENDER TEMPORAL
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Frng Xia, 2020, COMPUTER APPL RES, V37, P3220
   Gong SG, 2011, VISUAL ANALYSIS OF BEHAVIOUR: FROM PIXELS TO SEMANTICS, P301, DOI 10.1007/978-0-85729-670-2_14
   Hao Y, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107533
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   Hermans Alexander, 2017, ARXIV170307737
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   [罗浩 Luo Hao], 2019, [自动化学报, Acta Automatica Sinica], V45, P2032
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Qi MB, 2021, MULTIMED TOOLS APPL, V80, P17645, DOI 10.1007/s11042-020-10431-5
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
NR 28
TC 0
Z9 0
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2405
EP 2418
DI 10.1007/s00371-023-02926-7
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001023990600002
DA 2024-07-18
ER

PT J
AU Zhang, CF
   Xu, YF
   Sheng, ZW
   He, J
   Yin, L
AF Zhang, Changfan
   Xu, Yifu
   Sheng, Zhenwen
   He, Jing
   Yin, Ling
TI Deformable residual attention network for defect detection of train
   wheelset tread
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Defect detection; Wheelset tread; Convolution network;
   Deformable convolution network
AB The wheelset tread has large noise interference and various forms of defects, resulting in the low detection accuracy and poor localization effect. We proposed a new detection model for wheel tread defect detection. First, we proposed a deformable residual attention network that combines deformable residual network modules and Channel-Fusion-Correct-Attention modules. The deformable residual network modules are to strengthen the capability of feature extraction for various forms of defects. While the Channel-Fusion-Correct-Attention modules can weaken noise interference. Second, adopt the path aggregation network feature fusion method combining top-down and bottom-up to reduce the loss of defect feature. Third, a deformable double-branch detection head is constructed to further optimize the overall performance of the model. A dataset of defects in the wheelset treads of trains was formulated for experiments to verify the performance of the model. The experiment results showed that the model could achieve accuracy of 78.2% and Recall of up to 91% when the IOU threshold was 0.5. It could detect 24.1 images per second on a single GPU, with 35.2 M parameters. The performance of the model was higher than classical models, such as Faster-RCNN and YOLO. The proposed method can be applied to the detection of defects on the tread surface of the wheelset.
C1 [Zhang, Changfan; Xu, Yifu; He, Jing; Yin, Ling] Hunan Univ Technol, Zhuzhou 412007, Hunan, Peoples R China.
   [Sheng, Zhenwen] Shandong Xiehe Univ, Jinan 250109, Shandong, Peoples R China.
C3 Hunan University of Technology
RP Sheng, ZW (corresponding author), Shandong Xiehe Univ, Jinan 250109, Shandong, Peoples R China.
EM shengzhenwen@sdxiehe.edu.cn
FU Natural Science Foundation of China [52172403, 62173137]; Hunan
   Provincial Natural Science Foundation of China [2021JJ50001]
FX AcknowledgementsThis work was supported by the Natural Science
   Foundation of China (52172403, 62173137), Hunan Provincial Natural
   Science Foundation of China (2021JJ50001).
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen MY, 2019, OPT LASER ENG, V122, P170, DOI 10.1016/j.optlaseng.2019.06.011
   Chen XJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075380
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Chen ZY, 2023, VISUAL COMPUT, V39, P3995, DOI 10.1007/s00371-022-02554-7
   Cui LS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3056744
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Guan ZY, 2023, VISUAL COMPUT, V39, P1271, DOI 10.1007/s00371-022-02403-7
   Guanglu Song, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11560, DOI 10.1109/CVPR42600.2020.01158
   Hao S., 2022, P CSEE, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hou WQ, 2024, VISUAL COMPUT, V40, P459, DOI 10.1007/s00371-023-02793-2
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   [李少波 Li Shaobo], 2020, [自动化学报, Acta Automatica Sinica], V46, P2319
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mao SA, 2013, COMPUT ELECTR ENG, V39, P863, DOI 10.1016/j.compeleceng.2013.03.004
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Su BY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3054415
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tang YC, 2022, STRUCTURES, V37, P426, DOI 10.1016/j.istruc.2021.12.055
   [陶显 Tao Xian], 2021, [自动化学报, Acta Automatica Sinica], V47, P1017
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang C., 2022, ELECT DRIVE LOCOMOT, V289, P1, DOI [10.13890/j.issn.1000-128X.2022.06.001, DOI 10.13890/J.ISSN.1000-128X.2022.06.001]
   Zhang CF, 2022, J ADV TRANSPORT, V2022, DOI 10.1155/2022/1172654
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 33
TC 1
Z9 1
U1 9
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1775
EP 1785
DI 10.1007/s00371-023-02885-z
EA MAY 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000993953600002
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Ma, H
   Liu, L
   Qiu, TR
   Lu, Y
   Suen, CY
AF Zhou, Yong
   Ma, Hui
   Liu, Li
   Qiu, Taorong
   Lu, Yue
   Suen, Ching Y.
TI Feature fusion and decomposition: exploring a new way for Chinese
   calligraphy style classification
SO VISUAL COMPUTER
LA English
DT Article
DE Chinese calligraphy style classification; Feature fusion; Feature
   decomposition; Joint supervision; Cross-entropy loss; Correlation loss
ID RECOGNITION; NETWORK
AB Chinese calligraphy is an invaluable legacy of Chinese culture, since it bears great artistic and aesthetic value. In this paper, we aim at the problem of Chinese calligraphy style classification, which is an important branch of Chinese calligraphy study. Chinese calligraphy style classification remains a challenging task due to its dramatic intra-class difference and tiny inter-class difference. Therefore, we propose a novel CNN embedded with feature fusion and feature decomposition modules to solve this problem. We first fuse the features of several images from the same category to augment their potential style-related features. Then we feed the fused feature to an attention module to decompose it to two components, viz. style-related feature and style-unrelated feature. We further apply two types of loss function to jointly supervise our network. On the one hand, we feed the style-related feature to a style classifier which is supervised by cross-entropy loss. On the other hand, we construct a correlation loss based on the Pearson correlation coefficient to make the two decomposed features as orthogonal as possible. By optimizing these two types of loss simultaneously, our proposed network has obtained the accuracies of 98.63% and 94.35% respectively on two datasets. Besides, substantial experiments demonstrate the effectiveness of the feature fusion and decomposition modules. The proposed approach compares favorably with state-of-the-art methods.
C1 [Zhou, Yong; Liu, Li; Qiu, Taorong] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Ma, Hui] Xian Microelect Technol Inst, Xian 710065, Peoples R China.
   [Lu, Yue] East China Normal Univ, Sch Commun & Elect Engn, Shanghai 200241, Peoples R China.
   [Suen, Ching Y.] Concordia Univ, Ctr Pattern Recognit & Machine Intelligence, Montreal, PQ H3G 1M8, Canada.
C3 Nanchang University; East China Normal University; Concordia University
   - Canada
RP Liu, L (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
EM liuli_033@163.com
FU National Natural Science Foundation of China [61603256]; Natural
   Sciences and Engineering Research Council of Canada
FX AcknowledgementsThis work is supported by National Natural Science
   Foundation of China under Grant 61603256 and the Natural Sciences and
   Engineering Research Council of Canada.
CR Bi FK, 2022, VISUAL COMPUT, V38, P2581, DOI 10.1007/s00371-021-02133-2
   Chen J, 2021, I C FIELD PROG LOGIC, P1, DOI [10.1109/ICME51207.2021.9428404, 10.1109/FPL53798.2021.00009]
   Dai FR, 2018, LECT NOTES COMPUT SC, V11304, P359, DOI 10.1007/978-3-030-04212-7_31
   Gao PC, 2015, MULTIMED TOOLS APPL, V74, P7221, DOI 10.1007/s11042-014-1969-3
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C., 2020, DIGITAL LIB I REPOSI, P67
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang SP, 2018, PATTERN RECOGN, V77, P395, DOI 10.1016/j.patcog.2017.10.018
   Huang ZZ, 2021, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR46437.2021.00720
   Jiang HC, 2018, LECT NOTES COMPUT SC, V11305, P483, DOI 10.1007/978-3-030-04221-9_43
   Kera SB, 2023, VISUAL COMPUT, V39, P2347, DOI 10.1007/s00371-022-02445-x
   Leilei Xiang, 2020, 2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP), P883, DOI 10.1109/ICSIP49896.2020.9339418
   Li M, 2019, 2019 COMPANION OF THE 19TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY (QRS-C 2019), P291, DOI 10.1109/QRS-C.2019.00062
   Liu L., 2021, P 28 INT C NEUR INF, P425
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Lv G., 2022, VISUAL COMPUT, P1
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Meng LX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3146, DOI 10.1145/3394171.3413499
   Mu YS, 2018, INFORM SCIENCES, V435, P40, DOI 10.1016/j.ins.2017.12.059
   Omid-Zohoor A, 2018, IEEE T CIRC SYST VID, V28, P1102, DOI 10.1109/TCSVT.2017.2653187
   Park J., 2018, BRIT MACHINE VISION
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Phaphuangwittayakul A., 2022, VISUAL COMPUT, V38, P1
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ramachandran P, 2019, ADV NEUR IN, V32
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Slimane F, 2013, PATTERN RECOGN LETT, V34, P209, DOI 10.1016/j.patrec.2012.09.012
   Song WK, 2015, PROC INT CONF DOC, P376, DOI 10.1109/ICDAR.2015.7333787
   Tan M, 2019, IEEE ACCESS, V7, P117944, DOI 10.1109/ACCESS.2019.2936118
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tao DP, 2016, IEEE T CYBERNETICS, V46, P756, DOI 10.1109/TCYB.2015.2414920
   Tao DP, 2014, NEUROCOMPUTING, V129, P159, DOI 10.1016/j.neucom.2013.09.044
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Wang YZ, 2018, LECT NOTES COMPUT SC, V10704, P229, DOI 10.1007/978-3-319-73603-7_19
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Yang H, 2023, VISUAL COMPUT, V39, P3887, DOI 10.1007/s00371-022-02533-y
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang H., 2021, arXiv
   Zhang JL, 2021, 2021 IEEE 7TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2021), P352, DOI 10.1109/ICVR51878.2021.9483820
   Zhang JL, 2019, INT J DOC ANAL RECOG, V22, P177, DOI 10.1007/s10032-019-00324-1
   Zhang SY, 2013, IEEE SYS MAN CYBERN, P4271, DOI 10.1109/SMC.2013.728
   Zhang XF, 2012, PROC SPIE, V8297, DOI 10.1117/12.908872
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y., 2022, VISUAL COMPUT, P1
   Zhang Yanyan, 2013, IEEE INT C MULTIMEDI
   Zhao H., 2021, VISUAL COMPUT, P1
   Zhao T, 2022, VISUAL COMPUT, V38, P1619, DOI 10.1007/s00371-021-02092-8
   Zhou PC, 2021, MULTIMED TOOLS APPL, V80, P6737, DOI 10.1007/s11042-020-09709-5
NR 52
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1631
EP 1642
DI 10.1007/s00371-023-02875-1
EA MAY 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000991071000003
DA 2024-07-18
ER

PT J
AU Guo, XX
   Tu, ZC
   Li, GY
   Shen, ZR
   Wu, WJ
AF Guo, Xiaoxin
   Tu, Zhenchuan
   Li, Guangyu
   Shen, Zhengran
   Wu, Weijia
TI A novel lightweight multi-dimension feature fusion network for
   single-image super-resolution reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-dimension feature fusion; Single-image super-resolution
   reconstruction; Channel and spatial attention module
AB In recent years, due to the powerful feature extraction capabilities of convolutional neural networks (CNNs), many single-image super-resolution (SISR) methods based on CNN have achieved remarkable results. However, more model parameters and higher computational cost make these methods unsuitable for devices with limited computing power. In this paper, a lightweight multi-dimension feature fusion network (LMDFFN) is proposed to reduce model parameters. In nonlinear mapping module of the LMDFFN, the lightweight feature extraction block (LFEB) and the multi-dimensional feature extraction block (MDFEB) embedded in each LFEB are designed to significantly improve the reconstruction performance. The channel split operation can group the features. Combined with square-kernel convolution kernel and asymmetric convolution kernel, different types of convolution kernels can focus on different types of features, and the grouping mechanism can effectively reduce the complexity of the model. In addition, a channel and spatial attention module (CSAM) dedicated to SISR is proposed to focus on image details and different semantic level in feature maps. Moreover, a lightweight reconstruction module is designed with depth-wise separable convolution, and the designed spatial attention module is integrated into the reconstruction module to focus on more important features in the reconstruction stage. Compared with recent lightweight SISR methods, LMDFFN has better reconstruction accuracy with lower model parameters and computational complexity, and the reconstructed images have higher visual quality.
C1 [Guo, Xiaoxin; Tu, Zhenchuan; Li, Guangyu; Shen, Zhengran] Jilin Univ, Coll Comp Sci & Technol, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
   [Wu, Weijia] Jilin Univ, Coll Software, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Guo, XX (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
EM guoxx@jlu.edu.cn; tuzc20@mails.jlu.edu.cn; gyli21@jlu.edu.cn;
   shenzr21@mails.jlu.edu.cn; wuwj20@mails.jlu.edu.cn
RI zhengran, shen/JHS-5421-2023
FU National Natural Science Foundation of China [82071995]; Key Research
   and Development Program of Jilin Province, China [20220201141GX];
   Natural Science Foundation of Jilin Province, China [20200201292JC]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China under Grant 82071995, the Key Research and
   Development Program of Jilin Province, China, under Grant 20220201141GX
   and the Natural Science Foundation of Jilin Province, China, under Grant
   20200201292JC.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Choi JS, 2017, IEEE COMPUT SOC CONF, P1150, DOI 10.1109/CVPRW.2017.153
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2018, TENCON IEEE REGION, P0090, DOI 10.1109/TENCON.2018.8650166
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu ZS, 2019, IEEE COMPUT SOC CONF, P2041, DOI 10.1109/CVPRW.2019.00256
   Liu ZS, 2019, IEEE INT CONF COMP V, P3517, DOI 10.1109/ICCVW.2019.00436
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Muqeet Abdul, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P103, DOI 10.1007/978-3-030-67070-2_6
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Wang C, 2019, ARXIV
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu J., 2018, ARXIV
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhao W., 2019, arXiv
NR 40
TC 2
Z9 2
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1685
EP 1696
DI 10.1007/s00371-023-02879-x
EA MAY 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000985157100001
DA 2024-07-18
ER

PT J
AU Zhang, J
   Qin, QG
   Liu, XY
   Ye, Q
   Du, W
AF Zhang, Jing
   Qin, Qiuge
   Liu, Xinyu
   Ye, Qi
   Du, Wen
TI Emotion-wise feature interaction analysis-based visual emotion
   distribution learning
SO VISUAL COMPUTER
LA English
DT Article
DE Visual emotion distribution learning; Feature interaction analysis;
   Interclass relationships
AB Image emotion is not exclusive, which makes emotion distribution learning more meaningful than emotion classification for visual emotion recognition. Consider the emotion correlations implicit in complex images do not strictly follow the universal psychological laws, which is essential for image sentiment analysis. We propose a novel emotion-wise feature interaction analysis (EFIA) method to study the emotion correlations for emotion distribution learning. It facilitates the interaction of specific features categories to learn complicated and specific inter-class relationships from the emotion feature perspective. In addition, we propose a distribution-oriented multi-task learning method to obtain a specialized distribution learning model. Experiments on public emotion datasets illustrate that our proposed method can achieve excellent performance on image emotion distribution learning and outperform most state-of-the-art methods.
C1 [Zhang, Jing; Qin, Qiuge; Liu, Xinyu; Ye, Qi] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Du, Wen] DS Informat Technol Co Ltd, Shanghai, Peoples R China.
C3 East China University of Science & Technology
RP Zhang, J; Ye, Q (corresponding author), East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM jingzhang@ecust.edu.cn; yeh_qi1125@ecust.edu.cn
FU Nature Science Foundation of Shanghai [22ZR1418400]
FX This study was funded by the Nature Science Foundation of Shanghai
   (Grant Number 22ZR1418400).
CR Ali AR, 2017, IEEE WINT CONF APPL, P679, DOI 10.1109/WACV.2017.81
   [Anonymous], 2016, 2016 IEEE International Conference on Multimedia and Expo
   [Anonymous], 1997, Information theory and statistics
   [Anonymous], 2020, IEEE INT CON MULTI
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Cordel MO, 2019, PROC CVPR IEEE, P4021, DOI 10.1109/CVPR.2019.00415
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Fan Y., 2018, 2018 IEEE International Conference on Communications Workshops (ICC Workshops), P1
   Fan YY, 2019, IEEE ACCESS, V7, P129997, DOI 10.1109/ACCESS.2019.2939681
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He T, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P382, DOI 10.1145/3323873.3326593
   Kumar RJR, 2021, VISUAL COMPUT, V37, P2315, DOI 10.1007/s00371-020-01988-1
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Liu TT, 2021, INFRARED PHYS TECHN, V112, DOI 10.1016/j.infrared.2020.103594
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Miller A, 2016, ARXIV
   Niu L., 2017, ARXIV
   Panda MR, 2022, VISUAL COMPUT, V38, P1975, DOI 10.1007/s00371-021-02260-w
   PENG KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Toisoul A, 2021, NAT MACH INTELL, V3, P42, DOI 10.1038/s42256-020-00280-0
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiong HT, 2019, AAAI CONF ARTIF INTE, P363
   Xu Z., 2021, IEEE T AFFECT COMPUT
   Yang H, 2023, VISUAL COMPUT, V39, P3887, DOI 10.1007/s00371-022-02533-y
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Zhang J, 2019, IEEE INT CON MULTI, P1126, DOI 10.1109/ICME.2019.00197
   Zhou Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2733373.2806328
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 35
TC 1
Z9 1
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1359
EP 1368
DI 10.1007/s00371-023-02854-6
EA APR 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000979483600004
DA 2024-07-18
ER

PT J
AU Ye, B
   Fujimoto, Y
   Sawabe, T
   Kanbara, M
   Kato, H
AF Ye, Bi
   Fujimoto, Yuichiro
   Sawabe, Taishi
   Kanbara, Masayuki
   Kato, Hirokazu
TI Artifact reduction in lenslet array near-eye displays
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Ray tracking; Image rendering; Microdisplay image
   processing
ID WIDE VIEWING ANGLE; FIELD; SIZE
AB Lenslet array near-eye displays are a revolutionary technology that generates a virtual image in the field of view of the observer. Although this technology is advantageous in creating compact near-eye displays, undesirable artifacts occur when the user pupil moves outside of the pupil practical movable region (PPMR). Even with dynamic image updating based on eye-tracking techniques, artifacts can still be perceived when human eyes turn rapidly. To enlarge PPMR, we proposed a new rendering method in previous work. To improve the rendering speed in the eye tracking system, look-up tables are used. The disadvantage of the onboard system is the large memory consumption. In this study, we analyzed the system parameters of the incident pupil and pupil margin light columns, the feasibility of the optimized system, and evaluated the optimized system that can adapt to themaximum velocity of the saccadic pupil movement. We optimized the rendering method to reduce memory consumption in the process of generating microdisplay images. In addition, we provide GPU rendering method to improve system speed and reduce system latency to meet the maximum human eye rotation speed. We conducted user studies to evaluate the effect of the method using the optimized rendering method combined with eye tracking to reduce artifacts for fast eye rotation on different images and videos. Results showed that our method effectively reduced artifacts via the optimized rendering method with eye tracking, which adapted to faster human eye movements.
C1 [Ye, Bi; Fujimoto, Yuichiro; Sawabe, Taishi; Kanbara, Masayuki; Kato, Hirokazu] Nara Inst Sci & Technol, Grad Sch Sci & Technol, 8916-5 Takayama, Ikoma, Nara 6300192, Japan.
C3 Nara Institute of Science & Technology
RP Ye, B (corresponding author), Nara Inst Sci & Technol, Grad Sch Sci & Technol, 8916-5 Takayama, Ikoma, Nara 6300192, Japan.
EM bi.ye.bu1@is.naist.jp
RI Kanbara, Masayuki/ABD-7780-2021; Fujimoto, Yuichiro/HTS-3801-2023
OI Ye, Bi/0000-0002-1714-8373
CR ABRAMS RA, 1989, J EXP PSYCHOL HUMAN, V15, P529, DOI 10.1037/0096-1523.15.3.529
   Adelson S. J., 1993, Visual Computer, V10, P127, DOI 10.1007/BF01900903
   Andersen D, 2016, VISUAL COMPUT, V32, P1481, DOI 10.1007/s00371-015-1135-6
   [Anonymous], US
   Bang K, 2021, IEEE T VIS COMPUT GR, V27, P2545, DOI 10.1109/TVCG.2021.3067758
   Bi Y., 2022, ICAT EGVE2022, P131
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Brooks C. W., 1991, Understanding lens surfacing
   Cakmakci Ozan, 2019, SID Symposium Digest of Technical Papers, V50, P438, DOI 10.1002/sdtp.12950
   Chang CL, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-55346-w
   Cheng DW, 2022, OPT EXPRESS, V30, P6584, DOI 10.1364/OE.452747
   Chervyakov N, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041223
   Cholewiak SA, 2020, OPT EXPRESS, V28, P38008, DOI 10.1364/OE.408404
   Chung S, 2023, VISUAL COMPUT, V39, P3145, DOI 10.1007/s00371-022-02676-y
   Dunn D, 2017, IEEE T VIS COMPUT GR, V23, P1275, DOI 10.1109/TVCG.2017.2657058
   Geng Ying., 2018, Digital Optics for Immersive Displays, V10676, P19
   Hahn J, 2008, OPT EXPRESS, V16, P12372, DOI 10.1364/OE.16.012372
   Huang DJ, 2022, VISUAL COMPUT, V38, P1195, DOI 10.1007/s00371-021-02140-3
   Jang C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275069
   Jang C, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130889
   Jo Y, 2021, APPL OPTICS, V60, pA268, DOI 10.1364/AO.408707
   Kim J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322987
   Kim SB, 2018, OPT LETT, V43, P767, DOI 10.1364/OL.43.000767
   Kloiber S, 2020, VISUAL COMPUT, V36, P1937, DOI 10.1007/s00371-020-01942-1
   Kress B., 2013, Proc. SPIE, V8720, P62, DOI [DOI 10.1117/12.20156541, 10.1117/12.2015654.full]
   Kuo G., 2020, ACM Transactions on Graphics (TOG), V39, P1
   Lanman D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508366
   Lee S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10451-2
   Lee S, 2018, IEEE ACCESS, V6, DOI 10.1109/ACCESS.2017.2782219
   Lee S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925971
   Li X, 2018, OPT EXPRESS, V26, P2349, DOI 10.1364/OE.26.002349
   Lin SF, 2020, OPT LASER ENG, V126, DOI 10.1016/j.optlaseng.2019.105895
   Lin SF, 2019, OPT EXPRESS, V27, P15926, DOI 10.1364/OE.27.015926
   Lin TG, 2020, OPT EXPRESS, V28, P38616, DOI 10.1364/OE.413471
   Lippmann G, 1908, CR HEBD ACAD SCI, V146, P446
   Maimone A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392416
   Maimone A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073624
   Maimone A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2503144
   Mathôt S, 2018, BEHAV RES METHODS, V50, P94, DOI 10.3758/s13428-017-1007-2
   Nakano K, 2021, IEEE T VIS COMPUT GR, V27, P4204, DOI 10.1109/TVCG.2021.3106513
   O'Mahony M., 2004, ACM Trans. Internet Technol, V4, DOI DOI 10.1145/1031114.1031116
   Park J. H., 2022, Light: Adv. Manuf., V3, P9
   Rakkolainen I, 2017, PROCEEDINGS OF THE 21ST INTERNATIONAL ACADEMIC MINDTREK CONFERENCE (ACADEMIC MINDTREK), P227, DOI 10.1145/3131085.3131088
   Ratcliff J, 2020, IEEE T VIS COMPUT GR, V26, P1981, DOI 10.1109/TVCG.2020.2973064
   Rathinavel K, 2018, IEEE T VIS COMPUT GR, V24, P2857, DOI 10.1109/TVCG.2018.2868570
   Reif R, 2008, VISUAL COMPUT, V24, P987, DOI 10.1007/s00371-008-0271-7
   Salomon D., 2008, IEEE Signal Process. Mag., V25, P147, DOI 10.1109/MSP.2007.914710
   Skola F, 2016, VISUAL COMPUT, V32, P761, DOI 10.1007/s00371-016-1246-8
   Sutherland IE., 1968, Assoc. Comput. Machinery, V68, P757, DOI [DOI 10.1145/1476589.1476686, 10.1145/1476589.1476686, 10.1145/1476589.1476686.2.2.1]
   Tschoerner B, 2021, VISUAL COMPUT, V37, P3063, DOI 10.1007/s00371-021-02251-x
   Vivado, about us
   Wang JG, 2015, J DISP TECHNOL, V11, P889, DOI 10.1109/JDT.2014.2361147
   Weisberg M, 2006, PHILOS SCI, V73, P730, DOI 10.1086/518628
   Welstead S.T., 1999, Fractal and Wavelet Image Compression Techniques, V40
   Wilson D., 2015, Vertex distance and pantoscopic angle-a review
   Witting MD, 2003, ANN EMERG MED, V41, P247, DOI 10.1067/mem.2003.8
   Wu F, 2018, APPL OPTICS, V57, P1447, DOI 10.1364/AO.57.001447
   Xia XX, 2019, IEEE T VIS COMPUT GR, V25, P3114, DOI 10.1109/TVCG.2019.2932238
   Yang X, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030819
   Yang X, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194164
   Yao C, 2018, OPT EXPRESS, V26, P18292, DOI 10.1364/OE.26.018292
   Ye B, 2022, OPT EXPRESS, V30, P16196, DOI 10.1364/OE.455482
   Yoo C, 2020, OPT EXPRESS, V28, P3116, DOI 10.1364/OE.383386
   Zhao J, 2019, IEEE PHOTONICS J, V11, DOI 10.1109/JPHOT.2019.2893934
NR 64
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 601
EP 618
DI 10.1007/s00371-023-02804-2
EA MAR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000958560100002
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, CZ
   Hua, Z
   Li, JJ
AF Liu, Cunzhe
   Hua, Zhen
   Li, Jinjiang
TI Reference-based dual-task framework for motion deblurring
SO VISUAL COMPUTER
LA English
DT Article
DE Motion deblurring; Reference image; Deep neural network; Structure and
   texture details
AB Deep learning algorithms have made significant progress for deblurring in dynamic scenes. However, most of the existing image deblurring methods use a single blurry image as the input of the algorithm, which limits the acquisition of information and fails to preserve satisfactory structural texture. In contrast, we present a reference-based dual-task framework to recover a high-quality image by deblurring and enhancing a blurry image under the guidance of a reference image. Specifically, the framework includes two tasks: single image deblurring and reference feature transfer. The single image deblurring task deblurs the blurry image leveraging only the blurry image itself. The reference feature transfer task extracts and transfers abundant textures from the reference image to the coarsely result of the single image deblurring task. Benefiting from the reference image, our proposed method achieves more realistic visual effects with sharper texture details. Experimental results on GoPro, HIDE and RealBlur datasets demonstrate that our method outperforms state-of-the-art methods both quantitatively and qualitatively.
C1 [Liu, Cunzhe; Hua, Zhen] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
   [Li, Jinjiang] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University
RP Hua, Z (corresponding author), Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
EM huazhen66@foxmail.com
RI Hua, Zhen/AGN-6068-2022
FU National Natural Science Foundation of China [61772319, 62002200,
   62272281, 61972235]; Shandong Natural Science Foundation of China
   [ZR2021MF107, ZR2022MA076]; Shandong Provincial Science and Technology
   Support Program of Youth Innovation Team in Colleges [2021KJ069,
   2019KJN042]; Yantai science and technology innovation development plan
   [2022JCYJ031]
FX AcknowledgementsThe authors acknowledge the National Natural Science
   Foundation of China (61772319, 62002200, 62272281 and 61972235), and
   Shandong Natural Science Foundation of China (ZR2021MF107, ZR2022MA076),
   Shandong Provincial Science and Technology Support Program of Youth
   Innovation Team in Colleges (2021KJ069, 2019KJN042), Yantai science and
   technology innovation development plan(2022JCYJ031).
CR Abuolaim A., 2022, IEEE CVF WINT C APPL, P1231
   Cao J., 2022, EUROPEAN C COMPUTER, P325
   Chan KCK, 2021, AAAI CONF ARTIF INTE, V35, P973
   Chan Kelvin CK, 2022, P IEEE CVF C COMP VI, P5972
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Dongwon Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P327, DOI 10.1007/978-3-030-58539-6_20
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Gyumin Shim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8422, DOI 10.1109/CVPR42600.2020.00845
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y., 2022, CVPR, P5931
   Jaesung Rim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P184, DOI 10.1007/978-3-030-58595-2_12
   Kingma D. P., 2014, arXiv
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4096, DOI 10.1109/ICCV48922.2021.00408
   Lu LY, 2021, PROC CVPR IEEE, P6364, DOI 10.1109/CVPR46437.2021.00630
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Mechrez R, 2019, LECT NOTES COMPUT SC, V11363, P427, DOI 10.1007/978-3-030-20893-6_27
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niu WJ, 2021, IEEE T IMAGE PROCESS, V30, P7101, DOI 10.1109/TIP.2021.3101402
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Paszke A, 2019, ADV NEUR IN, V32
   Purohit K, 2020, AAAI CONF ARTIF INTE, V34, P11882
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567
   Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Sun L., 2013, IEEE INT C COMPUTATI, P1, DOI [10.1109/ICCPhot.2013.6528301, DOI 10.1109/ICCPHOT.2013.6528301]
   Sun L, 2022, LECT NOTES COMPUT SC, V13678, P412, DOI 10.1007/978-3-031-19797-0_24
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tsai F, 2021, ARXIV
   Tu ZG, 2017, PATTERN RECOGN, V65, P11, DOI 10.1016/j.patcog.2016.10.027
   Tu ZG, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.5.053018
   Wang TF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1981, DOI 10.1109/ICCV48922.2021.00201
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang DJ, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107312
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang KH, 2022, INT J COMPUT VISION, V130, P2103, DOI 10.1007/s11263-022-01633-5
   Zhang KH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P384, DOI 10.1145/3394171.3413929
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zheng H., 2017, BMVC, DOI [10.5244/C.31.138, DOI 10.5244/C.31.138]
   Zheng H., 2018, P IEEE EUR C COMP VI, P88
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 58
TC 0
Z9 0
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 137
EP 151
DI 10.1007/s00371-023-02771-8
EA FEB 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000925971100001
DA 2024-07-18
ER

PT J
AU Tahiri, MA
   Karmouni, H
   Bencherqui, A
   Daoui, A
   Sayyouri, M
   Qjidaa, H
   Hosny, KM
AF Tahiri, Mohamed Amine
   Karmouni, Hicham
   Bencherqui, Ahmed
   Daoui, Achraf
   Sayyouri, Mhamed
   Qjidaa, Hassan
   Hosny, Khalid M.
TI New color image encryption using hybrid optimization algorithm and
   Krawtchouk fractional transformations
SO VISUAL COMPUTER
LA English
DT Article
DE RGB image encryption; Discrete fractional Krawtchouk moments;
   Metaheuristic algorithms; Optimization; Slap swarm algorithm; Arithmetic
   optimization algorithm; Modified 3D fractional Henon map
ID DNA ENCRYPTION; SCHEME; MAP; NETWORK; LORENZ
AB This paper proposes a new method for encryption of RGB color images by combining two encryption approaches: the spatial approach and the transformation approach. The proposed method uses the 3D fractional modified Henon map (3D FrMHM) and the discrete fractional Krawtchouk moments (FrDKM). We have also proposed a new hybrid optimization algorithm (H-SSAOA) to optimize the parameters of the proposed Henon map and the parameters of the Krawtchouk fractional moments. This algorithm is based on the hybridization of two metaheuristic algorithms: the "Salp Swarm Algorithm " (SSA) and the "Arithmetic Optimization Algorithm " (AOA). The simulation results reveal the optimization efficiency of the proposed hybrid algorithm H-SSAOA compared to other meta-heuristic algorithms and the efficiency of the suggested encryption method for encrypting RGB color images in terms of sensitivity to the security key and resistance to different attacks.
C1 [Tahiri, Mohamed Amine; Karmouni, Hicham; Qjidaa, Hassan] Sidi Mohamed Ben Abdellah Fez Univ, Dhar El Mahrez Fac Sci, Lab Elect Signals & Syst Informat LESSI, CED ST,STIC, Fes, Morocco.
   [Karmouni, Hicham; Bencherqui, Ahmed; Daoui, Achraf; Sayyouri, Mhamed] Sidi Mohamed Ben Abdellah Univ, Natl Sch Appl Sci, Engn Syst & Applicat Lab, Fes, Morocco.
   [Hosny, Khalid M.] Zagazig Univ, Fac Comp & Informat, Dept Informat Technol, Zagazig 44519, Egypt.
   [Karmouni, Hicham] Cadi Ayyad Univ, Natl Sch Appl Sci, Marrakech, Morocco.
C3 Sidi Mohamed Ben Abdellah University of Fez; Sidi Mohamed Ben Abdellah
   University of Fez; Egyptian Knowledge Bank (EKB); Zagazig University;
   Cadi Ayyad University of Marrakech
RP Karmouni, H (corresponding author), Sidi Mohamed Ben Abdellah Fez Univ, Dhar El Mahrez Fac Sci, Lab Elect Signals & Syst Informat LESSI, CED ST,STIC, Fes, Morocco.; Karmouni, H (corresponding author), Sidi Mohamed Ben Abdellah Univ, Natl Sch Appl Sci, Engn Syst & Applicat Lab, Fes, Morocco.; Karmouni, H (corresponding author), Cadi Ayyad Univ, Natl Sch Appl Sci, Marrakech, Morocco.
EM mohamedamine.tahiri@usmba.ac.ma; hicham.karmouni@usmba.ac.ma;
   ahmed.bencherqui@usmba.ac.ma; achraf.daoui@usmba.ac.ma;
   mhamed.sayyouri@usmba.ac.ma; qjidah@yahoo.fr; k_hosny@yahoo.com
RI MOHAMED AMINE, TAHIRI/GMN-4987-2022; Sayyouri, Mhamed/AAB-5496-2020;
   Hosny, Khalid M./B-1404-2008
OI MOHAMED AMINE, TAHIRI/0000-0003-4799-6629; Sayyouri,
   Mhamed/0000-0002-1615-419X; Hosny, Khalid M./0000-0001-8065-8977
CR Abualigah L, 2021, COMPUT METHOD APPL M, V376, DOI 10.1016/j.cma.2020.113609
   Alipour M.C., 2019, ACM INT CONFER PROC, DOI [10.1145/3352411.3352436, DOI 10.1145/3352411.3352436]
   Annaby MH, 2016, SIGNAL PROCESS-IMAGE, V49, P25, DOI 10.1016/j.image.2016.09.006
   [Anonymous], 1998, TABU SEARCH, V3, P621
   Ansari A, 2020, IEEE ACCESS, V8, P176640, DOI 10.1109/ACCESS.2020.3026529
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Belazi A, 2016, SIGNAL PROCESS, V128, P155, DOI 10.1016/j.sigpro.2016.03.021
   Ben Slimane N, 2018, MULTIMED TOOLS APPL, V77, P30993, DOI 10.1007/s11042-018-6145-8
   Bigdeli N, 2012, ENG APPL ARTIF INTEL, V25, P753, DOI 10.1016/j.engappai.2012.01.007
   Castelli M, 2022, EXPERT SYST APPL, V189, DOI 10.1016/j.eswa.2021.116029
   Chai XL, 2019, SIGNAL PROCESS, V155, P44, DOI 10.1016/j.sigpro.2018.09.029
   Chen LF, 2015, OPT LASER TECHNOL, V69, P80, DOI 10.1016/j.optlastec.2014.12.007
   Chen LP, 2021, SIGNAL PROCESS-IMAGE, V97, DOI 10.1016/j.image.2021.116363
   Chen W, 2016, IEEE PHOTONICS J, V8, DOI 10.1109/JPHOT.2016.2550322
   Cheng X, 2022, IEEE SENS J, V22, P5889, DOI 10.1109/JSEN.2022.3149337
   Dawood ZM, 2019, INTERNATIONAL CONFERENCE OF INFORMATION AND COMMUNICATION TECHNOLOGY (ICICT 2019), P127, DOI 10.1145/3321289.3321322
   Neto JRD, 2020, IEEE T CIRC SYST VID, V30, P2489, DOI 10.1109/TCSVT.2019.2925522
   El Ogri O, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106346
   Girdhar A, 2018, MULTIMED TOOLS APPL, V77, P27017, DOI 10.1007/s11042-018-5902-z
   Gonchenko SV, 2005, INT J BIFURCAT CHAOS, V15, P3493, DOI 10.1142/S0218127405014180
   Hosny KM, 2022, J AMB INTEL HUM COMP, V13, P973, DOI 10.1007/s12652-021-03675-y
   Hua ZY, 2021, INFORM SCIENCES, V546, P1063, DOI 10.1016/j.ins.2020.09.032
   Huang WB, 2023, IEEE T MOBILE COMPUT, V22, P5064, DOI 10.1109/TMC.2022.3174816
   Ibrahim A, 2020, IEEE ACCESS, V8, P122121, DOI 10.1109/ACCESS.2020.3007336
   Institute of Electrical and Electronics Engineers, 2017, IDAACS 2017 P 2017 I
   Jourdan L, 2002, KNOWL-BASED SYST, V15, P235, DOI 10.1016/S0950-7051(01)00145-9
   Kang XJ, 2019, IEEE ACCESS, V7, P114459, DOI 10.1109/ACCESS.2019.2930183
   Kang XJ, 2019, IEEE T CIRC SYST VID, V29, P1595, DOI 10.1109/TCSVT.2018.2851983
   Kaur M, 2020, APPL PHYS B-LASERS O, V126, DOI 10.1007/s00340-020-07480-x
   Kaur M, 2020, ARCH COMPUT METHOD E, V27, P15, DOI 10.1007/s11831-018-9298-8
   Khennaoui AA, 2020, INT J BIFURCAT CHAOS, V30, DOI 10.1142/S021812742050217X
   Kumar V, 2021, MULTIMED TOOLS APPL, V80, P3749, DOI 10.1007/s11042-020-09854-x
   Liu DR, 2022, VEHICLE SYST DYN, V60, P433, DOI 10.1080/00423114.2020.1817508
   Liu Q, 2020, IEEE ACCESS, V8, P83596, DOI 10.1109/ACCESS.2020.2991420
   Liu XL, 2017, IEEE T SIGNAL PROCES, V65, P1894, DOI 10.1109/TSP.2017.2652383
   Liu Y, 2015, MULTIMED TOOLS APPL, V74, P3171, DOI 10.1007/s11042-013-1778-0
   Luan GY, 2020, IEEE PHOTONICS J, V12, DOI 10.1109/JPHOT.2020.2963921
   Luo X., 2012, APPL MECH MAT, V182, P1839, DOI [10.4028/www.scientific.net/AMM.182-183.1839, DOI 10.4028/WWW.SCIENTIFIC.NET/AMM.182-183.1839]
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Muñoz-Guillermo M, 2021, INFORM SCIENCES, V552, P352, DOI 10.1016/j.ins.2020.11.045
   Pei SC, 2015, IEEE T SIGNAL PROCES, V63, P4207, DOI 10.1109/TSP.2015.2437845
   Pham TX, 2020, IEEE T IMAGE PROCESS, V29, P6507, DOI 10.1109/TIP.2020.2990346
   Kari AP, 2021, MULTIMED TOOLS APPL, V80, P2753, DOI 10.1007/s11042-020-09648-1
   Premkumar M, 2021, IEEE ACCESS, V9, P84263, DOI 10.1109/ACCESS.2021.3085529
   Qais MH, 2019, ENG APPL ARTIF INTEL, V80, P82, DOI 10.1016/j.engappai.2019.01.011
   Rathore V, 2021, MULTIMED TOOLS APPL, V80, P22275, DOI 10.1007/s11042-021-10719-0
   Raza SF, 2019, NONLINEAR DYNAM, V95, P859, DOI 10.1007/s11071-018-4600-8
   Sabzalian MH, 2021, ENG APPL ARTIF INTEL, V100, DOI 10.1016/j.engappai.2021.104163
   Senel FA, 2019, ENG COMPUT-GERMANY, V35, P1359, DOI 10.1007/s00366-018-0668-5
   Singh N, 2017, ENG SCI TECHNOL, V20, P1586, DOI 10.1016/j.jestch.2017.11.001
   Sivakumar T, 2015, KSII T INTERNET INF, V9, P2317, DOI 10.3837/tiis.2015.06.020
   Sui LS, 2016, OPT EXPRESS, V24, P499, DOI 10.1364/OE.24.000499
   Suri S, 2019, J AMB INTEL HUM COMP, V10, P2277, DOI 10.1007/s12652-018-0825-0
   Tahiri M.A., 2022, 2D 3D IMAGE LOCALIZA
   Tahiri MA, 2020, 2020 INT C INTELL SY, P0, DOI [10.1109/ISCV49265.2020.9204118, DOI 10.1109/ISCV49265.2020.9204118]
   Tahiri MA, 2022, INT C INT SYST COMP, P1, DOI [10.1109/ISCV54655.2022.9806106, DOI 10.1109/ISCV54655.2022.9806106]
   Tahiri MA, 2020, 2020 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES (ICDS), DOI [10.1109/icds50568.2020.9268685, 10.1109/TPEC48276.2020.9042513]
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Tang Y, 2023, IEEE T IND ELECTRON, V70, P2106, DOI 10.1109/TIE.2022.3161812
   Tao R, 2010, IEEE T INF FOREN SEC, V5, P734, DOI 10.1109/TIFS.2010.2068289
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang XG, 2017, IEEE PHOTONICS J, V9, DOI 10.1109/JPHOT.2017.2684179
   Wang XG, 2014, OPT COMMUN, V328, P67, DOI 10.1016/j.optcom.2014.04.059
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   Wu XJ, 2018, MULTIMED TOOLS APPL, V77, P12349, DOI 10.1007/s11042-017-4885-5
   Wu XJ, 2018, SIGNAL PROCESS, V148, P272, DOI 10.1016/j.sigpro.2018.02.028
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Xing ZK, 2019, IEEE ACCESS, V7, P37672, DOI 10.1109/ACCESS.2019.2904511
   Xu L, 2016, OPT LASER ENG, V78, P17, DOI 10.1016/j.optlaseng.2015.09.007
   Yang TF, 2018, IEEE ACCESS, V6, P47521, DOI 10.1109/ACCESS.2018.2866861
   Yousri D, 2020, ENG APPL ARTIF INTEL, V92, DOI 10.1016/j.engappai.2020.103662
NR 72
TC 20
Z9 20
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6395
EP 6420
DI 10.1007/s00371-022-02736-3
EA DEC 2022
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000897942700001
DA 2024-07-18
ER

PT J
AU Yang, S
   Qiao, K
   Shi, SH
   Wang, LY
   Hu, G
   Yan, B
   Chen, J
AF Yang, Shuai
   Qiao, Kai
   Shi, Shuhao
   Wang, Linyuan
   Hu, Guoen
   Yan, Bin
   Chen, Jian
TI EnNeRFACE: improving the generalization of face reenactment with
   adaptive ensemble neural radiance fields
SO VISUAL COMPUTER
LA English
DT Article
DE Face reenactment; Dynamic neural radiance fields; 3D reconstruction;
   Ensemble learning
AB Face reenactment is a critical technology of digital face editing. Lately, the NeRFACE, a face reenactment method based on neural radiance fields, has been proposed, making the reconstruction accuracy of the training dataset much higher than the previous methods. However, face reenactment in realistic scenes often encounters poses and expressions that have not been seen before, which requires further improvement of the model's generalization capability. Based on the idea of ensemble learning, we present EnNeRFACE as using the adaptive ensemble neural radiance fields architecture, which is mainly composed of a set of subgenerators and a controller. We divide the short video of human portraits into non-intersecting sub-datasets based on time correlation, thus enabling the trained subgenerators to have differentiated modeling capabilities. In response to different expression vectors, the generator dynamically adjusts the weights assigned to each generator so that the capabilities of the subgenerators are adequately exploited. Extensive experiments show that EnNeRFACE has more stable and superior performance in generalization (i.e., identity preservation, manipulation of expression and pose) than the state-of-the-art methods, demonstrating the effectiveness of our proposed adaptive ensemble structure.
C1 [Yang, Shuai; Qiao, Kai; Shi, Shuhao; Wang, Linyuan; Hu, Guoen; Yan, Bin; Chen, Jian] PLA Strategy Support Force Informat Engn Univ, Henan Key Lab Imaging & Intelligence Proc, Sci Ave, Zhengzhou 450001, Henan, Peoples R China.
C3 PLA Information Engineering University
RP Chen, J (corresponding author), PLA Strategy Support Force Informat Engn Univ, Henan Key Lab Imaging & Intelligence Proc, Sci Ave, Zhengzhou 450001, Henan, Peoples R China.
EM ysstation@foxmail.com; chenjian198042@163.com
RI Shi, Shuhao/ABS-9684-2022
OI Shi, Shuhao/0000-0003-2926-8230
CR Agrawal SC, 2022, VISUAL COMPUT, V38, P781, DOI 10.1007/s00371-020-02049-3
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Baque P, 2018, PR MACH LEARN RES, V80
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Deng Kangle, 2021, arXiv
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Guo H, 2022, VISUAL COMPUT, V38, P3897, DOI 10.1007/s00371-021-02230-2
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jeong Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5826, DOI 10.1109/ICCV48922.2021.00579
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li P, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8508702
   Lindell DB, 2021, PROC CVPR IEEE, P14551, DOI 10.1109/CVPR46437.2021.01432
   Liu Lingjie, 2020, NEURIPS, V33, P15651
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459863, 10.1145/3476576.3476608]
   Luo Y, 2022, VISUAL COMPUT, V38, P3109, DOI 10.1007/s00371-022-02567-2
   Mallick S., 2016, Head Pose Estimation using OpenCV and Dlib
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Mildenhall Ben, 2020, EUR C COMP VIS ECCV
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, ADV NEUR IN, V30
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Reiser C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14315, DOI 10.1109/ICCV48922.2021.01407
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Sun Chuxiong, 2021, arXiv
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Thies J, 2019, COMMUN ACM, V62, P96, DOI 10.1145/3292039
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   Wang YH, 2020, IEEE WINT CONF APPL, P1149, DOI [10.1109/WACV45572.2020.9093492, 10.1109/wacv45572.2020.9093492]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2021, PROC CVPR IEEE, P5700, DOI 10.1109/CVPR46437.2021.00565
   Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980233
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P878, DOI 10.1109/TNNLS.2013.2246578
   Yang Y., 2018, IEEE C COMPUTER VISI, V22, P1
   Zhang JM, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108485
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 51
TC 3
Z9 3
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6015
EP 6028
DI 10.1007/s00371-022-02709-6
EA OCT 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000876238600001
DA 2024-07-18
ER

PT J
AU Boughanem, H
   Ghazouani, H
   Barhoumi, W
AF Boughanem, Hadjer
   Ghazouani, Haythem
   Barhoumi, Walid
TI Multichannel convolutional neural network for human emotion recognition
   from in-the-wild facial expressions
SO VISUAL COMPUTER
LA English
DT Article
DE Deep features; Multichannel CNN; In-The-Wild emotion recognition; Human
   emotion recognition; Feature concatenation
ID DEEP
AB Facial emotions reflect the person's moods and show the human affective state that is correlative with non-verbal intentions and behaviors. Despite the advances on computer vision techniques, capturing automatically the facial expressions in-the-wild remains a very difficult task. In this context, we propose a multichannel convolutional neural network based on the quality and the strengths of three well-known pre-trained models, namely VGG19, GoogleNet, and ResNet101. Indeed, the complementarity of the features extracted from the three models is exploited in order to form a more robust feature vector. During the training phase, a freezing weight is applied for each architecture. Then, the layers containing the most relevant information are marked, and the final feature descriptor for emotion prediction is thereafter defined by concatenating the obtained vectors. In fact, the three architectures have showed their efficiency severally in term of emotion recognition, and notably they do not err in the same images. The final vector, obtained by concatenating the features extracted from the different models, is fed to a support vector machine classifier in order to predict the final emotions. Extensive experiments have been conducted on four challenging datasets (JAFFE, CK+, FER2013 and SFEW_2.0) covering in-the-wild as well as in-the-laboratory facial expressions. The obtained results show that the suggested method is not only more accurate compared to each pre-trained CNN model but it also outperforms relevant state-of-the-art methods.
C1 [Boughanem, Hadjer; Ghazouani, Haythem; Barhoumi, Walid] Univ Tunis El Manar, Inst Super Informat El Manar, LR16ES06 Lab Rech Informat Modelisat & Traitement, Res Team Intelligent Syst Imaging & Artificial Vi, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
   [Ghazouani, Haythem; Barhoumi, Walid] Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Tunis 2035, Tunisia.
C3 Universite de Tunis-El-Manar; Universite de Carthage
RP Barhoumi, W (corresponding author), Univ Tunis El Manar, Inst Super Informat El Manar, LR16ES06 Lab Rech Informat Modelisat & Traitement, Res Team Intelligent Syst Imaging & Artificial Vi, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.; Barhoumi, W (corresponding author), Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Tunis 2035, Tunisia.
EM walid.barhoumi@enicarthage.rnu.tn
RI Barhoumi, Walid/C-6576-2014; Ghazouani, Haythem/N-1013-2014
OI Barhoumi, Walid/0000-0003-2123-4992; Ghazouani,
   Haythem/0000-0002-6521-5024
CR Abdulrahman M, 2015, SIG PROCESS COMMUN, P276, DOI 10.1109/SIU.2015.7129813
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Ahmed TU, 2019, 2019 JOINT 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2019 3RD INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR) WITH INTERNATIONAL CONFERENCE ON ACTIVITY AND BEHAVIOR COMPUTING (ABC), P336, DOI [10.1109/ICIEV.2019.8858529, 10.1109/iciev.2019.8858529]
   Alreshidi A, 2020, INFORMATICS-BASEL, V7, DOI 10.3390/informatics7010006
   Alshamsi H, 2017, 2017 IEEE 8TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (UEMCON), P577, DOI 10.1109/UEMCON.2017.8249000
   Vo A, 2018, PROCEEDINGS OF 2018 4TH INTERNATIONAL CONFERENCE ON GREEN TECHNOLOGY AND SUSTAINABLE DEVELOPMENT (GTSD), P739, DOI 10.1109/GTSD.2018.8595551
   [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bericat E, 2016, CURR SOCIOL, V64, P491, DOI 10.1177/0011392115588355
   Bodapati Jyostna Devi, 2022, Journal of the Institution of Engineers (India): Series B (Electrical, Electronics & Telecommunication and Computer Engineering), V103, P439, DOI 10.1007/s40031-021-00681-8
   Boughanem H., 2021, IEEEACS INT C COMPUT, P1
   CAMRAS L, 1980, AM J PSYCHOL, V93, P751, DOI 10.2307/1422394
   Chen L, 2021, EMOTION RECOGNITION, P25
   Damasio A., 2006, ERREUR DESCARTES RAI
   Darwin C., 1872, P374
   Dhall A., 2014, PROC ICMI, P461, DOI DOI 10.1145/2663204.2666275
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Dou S., 2020, J PHYS C SER, V1453
   Fan XJ, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102659
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fasel B, 2002, INT C PATT RECOG, P40, DOI 10.1109/ICPR.2002.1048231
   Gite B, 2017, PROCEEDINGS OF THE 2017 INTELLIGENT SYSTEMS CONFERENCE (INTELLISYS), P849, DOI 10.1109/IntelliSys.2017.8324228
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Hazarika D, 2021, INFORM FUSION, V65, P1, DOI 10.1016/j.inffus.2020.06.005
   Hua WT, 2019, IEEE ACCESS, V7, P24321, DOI 10.1109/ACCESS.2019.2900231
   Huang YX, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11101189
   Hung JC, 2019, APPL SOFT COMPUT, V84, DOI 10.1016/j.asoc.2019.105724
   Jain DK, 2019, PATTERN RECOGN LETT, V120, P69, DOI 10.1016/j.patrec.2019.01.008
   Kanade T., 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]
   Karnati M, 2022, IEEE T COGN DEV SYST, V14, P971, DOI 10.1109/TCDS.2021.3086011
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Khorsheed JA, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P2085, DOI 10.1109/SIU.2016.7496182
   Kim JH, 2019, IEEE ACCESS, V7, P41273, DOI 10.1109/ACCESS.2019.2907327
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2019, ARTIF INTELL REV, V52, P927, DOI 10.1007/s10462-018-9650-2
   Kumar P, 2016, 2016 INTERNATIONAL CONFERENCE ON COMPUTING, ANALYTICS AND SECURITY TRENDS (CAST), P289, DOI 10.1109/CAST.2016.7914982
   Lai YH, 2018, IEEE INT CONF AUTOMA, P263, DOI 10.1109/FG.2018.00046
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liu X, 2021, IEEE SENS J, V21, P11532, DOI 10.1109/JSEN.2020.3028075
   Liu Y, 2020, IEEE T COGN DEV SYST, V12, P311, DOI 10.1109/TCDS.2019.2917711
   Lopes AT, 2015, SIBGRAPI, P273, DOI 10.1109/SIBGRAPI.2015.14
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons MJ., 1998, The Japanese female facial expression (JAFFE) database
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Mohan K, 2021, NEURAL COMPUT APPL, V33, P9125, DOI 10.1007/s00521-020-05676-y
   Mohan K, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3031835
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Naeem MR, 2020, P 2020 INT C COMPUTI, P1, DOI DOI 10.1109/ICCIT-144147971.2020.9213757
   Ramos ALA, 2020, LECT NOTES ELECTR EN, V603, P469, DOI 10.1007/978-981-15-0058-9_45
   Ravi R, 2020, 2020 4 INT C COMP ME
   Saurav S, 2022, VISUAL COMPUT, V38, P1083, DOI 10.1007/s00371-021-02069-7
   Saurav S, 2021, APPL INTELL, V51, P5543, DOI 10.1007/s10489-020-02125-0
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   Shiqing Zhang, 2012, WSEAS Transactions on Signal Processing, V8, P21
   Shouse Eric., 2005, MC J, V8, DOI [https://doi.org/10.5204/mcj.2443, DOI 10.5204/MCJ.2443, 10.5204/mcj.2443]
   Siam AI, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/8032673
   Simard PY, 2003, PROC INT CONF DOC, P958
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siqueira H, 2020, AAAI CONF ARTIF INTE, V34, P5800
   Slimani K, 2018, PROCEEDINGS OF THE 2ND MEDITERRANEAN CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (MEDPRAI-2018), P88, DOI 10.1145/3177148.3180092
   Sun X, 2019, COGN COMPUT, V11, P587, DOI 10.1007/s12559-019-09654-y
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12
   Umer S, 2022, J AMB INTEL HUM COMP, V13, P721, DOI 10.1007/s12652-020-02845-8
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wirdiani NA., 2019, INT J IMAGE GRAPH SI, V11, P1
   Wu M, 2021, IEEE T SYST MAN CY-S, V51, P1473, DOI 10.1109/TSMC.2019.2897330
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Yan KY, 2019, IEEE ACCESS, V7, P108906, DOI 10.1109/ACCESS.2019.2930359
   Salmam FZ, 2019, SIGNAL IMAGE VIDEO P, V13, P609, DOI 10.1007/s11760-018-1388-4
   Zhang HF, 2020, IEEE ACCESS, V8, P37976, DOI 10.1109/ACCESS.2020.2975913
   Zhang HP, 2020, PATTERN RECOGN LETT, V131, P128, DOI 10.1016/j.patrec.2019.12.013
   Zhang LG, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3158369
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhou LY, 2022, NEURAL COMPUT APPL, V34, P925, DOI 10.1007/s00521-021-06045-z
NR 84
TC 4
Z9 4
U1 4
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5693
EP 5718
DI 10.1007/s00371-022-02690-0
EA OCT 2022
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000870979400001
DA 2024-07-18
ER

PT J
AU Sun, ST
   Park, U
   Sun, SQ
   Liu, RK
AF Sun, Shantong
   Park, Unsang
   Sun, Shuqiao
   Liu, Rongke
TI Fusion representation learning for keypoint detection and description
SO VISUAL COMPUTER
LA English
DT Article
DE Keypoint detection; Joint learning; Fusion representation; Adaptive
   feature fusion
ID SCALE
AB Keypoint detection and description are the basis of many computer vision applications such as object recognition and image analysis. Current deep learning-based methods have made great progress in joint learning of keypoint detection and description construction. Low-level features have been proved to be helpful for keypoint detection and description. However, current detector and descriptor focus more on high-level feature and ignore the importance of low-level feature. They simply concatenate features and are lack of sufficient feature fusion. In this work, we propose a fusion representation learning network, which fuses different levels of features for both detectors and descriptors. Furthermore, we design and propose an adaptive feature fusion structure for the descriptor. Extensive experiments on HPatches, FM-Bench and Day-Night datasets demonstrate the superiority of our approach.
C1 [Sun, Shantong] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Sun, Shantong; Sun, Shuqiao; Liu, Rongke] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Park, Unsang] Sogang Univ, Sch Comp Sci & Engn, Seoul 04107, South Korea.
   [Liu, Rongke] Beihang Univ, Shenzhen Inst, Shenzhen 518063, Peoples R China.
C3 Jiangsu University; Beihang University; Sogang University; Beihang
   University
RP Sun, ST (corresponding author), Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.; Sun, ST (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM sunshantong@gmail.com
RI Sun, Shuqiao/ABB-4381-2021
OI Sun, Shuqiao/0000-0002-5420-9745; Sun, Shantong/0000-0001-8621-7072
FU National Research Foundation of Korea (NRF) - Ministry of Education
   [NRF-2020R1F1A1072332]; National Natural Science Foundation of China
   [61231010]; China Scholarship Council (CSC) [202006020119]
FX This work was supported in part by Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education under the Grant NRF-2020R1F1A1072332, in part by
   National Natural Science Foundation of China under the Grant 61231010,
   in part by the scholarship from China Scholarship Council (CSC) under
   the Grant CSC No. 202006020119.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bian J.W., 2019, ARXIV
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Geiger A., 2012, CVPR
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Joshi K, 2020, INT J MULTIMED INF R, V9, P231, DOI 10.1007/s13735-020-00200-3
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Kong H, 2013, IEEE T CYBERNETICS, V43, P1719, DOI 10.1109/TSMCB.2012.2228639
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mishchuk A, 2017, ADV NEUR IN, V30
   Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18
   Nai K, 2018, IEEE T IMAGE PROCESS, V27, P4958, DOI 10.1109/TIP.2018.2848465
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Ono Y., 2018, ARXIV
   Qin ZY, 2020, IEEE INT CONF ROBOT, P7278, DOI [10.1109/ICRA40945.2020.9196971, 10.1109/icra40945.2020.9196971]
   Revaud J, 2019, ADV NEUR IN, V32
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76
   Shen XL, 2019, PROC CVPR IEEE, P8124, DOI 10.1109/CVPR.2019.00832
   Sipiran I, 2013, VISUAL COMPUT, V29, P1319, DOI 10.1007/s00371-013-0870-9
   Song Y, 2020, ARXIV
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   visiblelearningmetax, ABOUT US
   Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5
   Yang YM, 2021, IEEE INTL CONF IND I, DOI 10.1109/INDIN45523.2021.9557392
   Yao QL, 2019, INT GEOSCI REMOTE SE, P1450, DOI [10.1109/IGARSS.2019.8897851, 10.1109/igarss.2019.8897851]
   Yi KM, 2016, PROC CVPR IEEE, P107, DOI 10.1109/CVPR.2016.19
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhang WJ, 2016, VISUAL COMPUT, V32, P275, DOI 10.1007/s00371-015-1065-3
   Zhang X, 2017, PROC CVPR IEEE, P4923, DOI 10.1109/CVPR.2017.523
   Zhou L, 2018, LECT NOTES COMPUT SC, V11219, P527, DOI 10.1007/978-3-030-01267-0_31
NR 41
TC 0
Z9 0
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5683
EP 5692
DI 10.1007/s00371-022-02689-7
EA OCT 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000867518700001
DA 2024-07-18
ER

PT J
AU Jiang, Q
   Zhang, ZY
   Da, FP
   Gai, SY
AF Jiang, Qian
   Zhang, Ziyu
   Da, Feipeng
   Gai, Shaoyan
TI Two-stream inter-class variation enhancement network for facial
   expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Deep convolutional neural network; Inter-class variation; Difficult
   expressions
ID CONVOLUTIONAL NEURAL-NETWORK; FEATURES; DEEP
AB Automatic facial expression recognition plays a crucial role in computer vision, and pattern recognition. However, most existing deep learning-based facial expression classifiers usually obtain high average accuracy but have poor recognition accuracy for difficult expressions, like fear and disgust. In this paper, we propose a novel end-to-end architecture termed two-stream inter-class variation enhancement network, which learns the high-level semantic features and subtle inter-class variations in a joint fashion. More precisely, the global feature extraction network is used to extract spatial-channel semantic features, and the variations between different expressions are modeled by a distinction-reinforced network. The outputs of these two streams are weighted integrated in the expression classification network. In addition, a class balanced-weighted cross-entropy loss is designed to further improve feature discrimination. Experiment results indicate that the proposed network can significantly improve the recognition of difficult expressions and achieve a satisfactory average recognition accuracy of 73.67% on FER2013, 86.17% on RAFDB, 98.19% on CK+, and 98.85% on Oulu-CASIA, which outperforms the other state-of-the-art methods.
C1 [Jiang, Qian; Zhang, Ziyu; Da, Feipeng; Gai, Shaoyan] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Jiang, Qian; Zhang, Ziyu; Da, Feipeng; Gai, Shaoyan] Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.
   [Da, Feipeng] Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Da, FP (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.; Da, FP (corresponding author), Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing 210096, Jiangsu, Peoples R China.; Da, FP (corresponding author), Southeast Univ, Shenzhen Res Inst, Shenzhen 518063, Guangdong, Peoples R China.
EM seu18805107250@163.com; zzy1993@seu.edu.cn; dafp@seu.edu.cn;
   qxxymm@163.com
FU Special Project on Basic Research of Frontier Leading Technology of
   Jiangsu Province of China [BK20192004C]; Natural Science Foundation of
   Jiangsu Province of China [BK20181269]
FX This work was supported by the Special Project on Basic Research of
   Frontier Leading Technology of Jiangsu Province of China (Grant No.
   BK20192004C) and the Natural Science Foundation of Jiangsu Province of
   China (Grant No. BK20181269).
CR Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   Ali K, 2021, INT C PATT RECOG, P9460, DOI 10.1109/ICPR48806.2021.9412172
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen WC, 2023, IEEE T AFFECT COMPUT, V14, P800, DOI 10.1109/TAFFC.2020.3027340
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Danqing Qian, 2021, 2021 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA), P66, DOI 10.1109/CogSIMA51574.2021.9475948
   Farzaneh AH, 2020, IEEE COMPUT SOC CONF, P1631, DOI 10.1109/CVPRW50498.2020.00211
   Florea C., 2019, BMVC
   Gan YL, 2020, IEEE ACCESS, V8, P7383, DOI 10.1109/ACCESS.2020.2963913
   Georgescu MI, 2019, IEEE ACCESS, V7, P64827, DOI 10.1109/ACCESS.2019.2917266
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang MY, 2022, IEEE T CIRC SYST VID, V32, P1431, DOI 10.1109/TCSVT.2021.3073558
   Jeong M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124270
   Ji YL, 2023, IEEE T KNOWL DATA EN, V35, P4190, DOI 10.1109/TKDE.2021.3136606
   Ji YL, 2020, IEEE T CIRC SYST VID, V30, P2114, DOI 10.1109/TCSVT.2019.2912988
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Kong FZ, 2019, PERS UBIQUIT COMPUT, V23, P531, DOI 10.1007/s00779-019-01238-9
   Li J, 2020, NEUROCOMPUTING, V411, P340, DOI 10.1016/j.neucom.2020.06.014
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3178, DOI 10.1109/TCSVT.2021.3103760
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3190, DOI 10.1109/TCSVT.2021.3103782
   Li Y, 2018, INT C PATT RECOG, P2209, DOI 10.1109/ICPR.2018.8545853
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Lin F, 2018, IEEE IMAGE PROC, P1957, DOI 10.1109/ICIP.2018.8451039
   Liu CJ, 2023, VISUAL COMPUT, V39, P2637, DOI 10.1007/s00371-022-02483-5
   Liu XF, 2021, INT C PATT RECOG, P7508, DOI 10.1109/ICPR48806.2021.9412820
   Liu XQ, 2020, VISUAL COMPUT, V36, P1635, DOI 10.1007/s00371-019-01759-7
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Ming ZH, 2018, INT C PATT RECOG, P3507, DOI 10.1109/ICPR.2018.8545274
   Ni TG, 2021, IEEE T COMPUT SOC SY, V8, P1213, DOI 10.1109/TCSS.2020.3013938
   Riaz MN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041087
   Saurav S, 2022, VISUAL COMPUT, V38, P1083, DOI 10.1007/s00371-021-02069-7
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao J, 2021, APPL INTELL, V51, P549, DOI 10.1007/s10489-020-01855-5
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   Sun X, 2021, NEUROCOMPUTING, V444, P378, DOI 10.1016/j.neucom.2019.11.127
   Sun Y, 2014, ADV NEUR IN, V27
   Taini M, 2008, INT C PATT RECOG, P1438
   Tian Y, 2019, IEEE IMAGE PROC, P46, DOI [10.1109/icip.2019.8802918, 10.1109/ICIP.2019.8802918]
   Tsai KY, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116321
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang H, 2020, J SUPERCOMPUT, V76, P3211, DOI 10.1007/s11227-018-2554-8
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang W., 2019, A fine-grained facial expression database for end-to-end multi-pose facial expression recognition
   Wang ZZ, 2020, EXPERT SYST APPL, V145, DOI 10.1016/j.eswa.2019.113102
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu M, 2021, IEEE T SYST MAN CY-S, V51, P1473, DOI 10.1109/TSMC.2019.2897330
   Wu WQ, 2018, INT C PATT RECOG, P1524, DOI 10.1109/ICPR.2018.8545725
   Xia YF, 2022, IEEE T CIRC SYST VID, V32, P1443, DOI 10.1109/TCSVT.2021.3074032
   Xie SY, 2021, IEEE T CIRC SYST VID, V31, P2359, DOI 10.1109/TCSVT.2020.3024201
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xie WC, 2021, IEEE T CYBERNETICS, V51, P2787, DOI 10.1109/TCYB.2019.2925095
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yu ZB, 2018, NEUROCOMPUTING, V317, P50, DOI 10.1016/j.neucom.2018.07.028
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang HF, 2021, IEEE T COGN DEV SYST, V13, P898, DOI 10.1109/TCDS.2020.3034807
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao GZ, 2020, IEEE ACCESS, V8, P38528, DOI 10.1109/ACCESS.2020.2964752
   Zhao S., 2018, BMVC
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zhong L, 2019, IEEE INT CONF AUTOMA, P270
   Zhu DD, 2021, SIGNAL IMAGE VIDEO P, V15, P263, DOI 10.1007/s11760-020-01753-w
   Zou W, 2022, APPL INTELL, V52, P2918, DOI 10.1007/s10489-021-02575-0
NR 76
TC 0
Z9 0
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5209
EP 5227
DI 10.1007/s00371-022-02655-3
EA SEP 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000854417000001
DA 2024-07-18
ER

PT J
AU Sun, JD
   Hui, ZK
   Tang, CS
   Wu, XS
AF Sun, Junding
   Hui, Zhenkun
   Tang, Chaosheng
   Wu, Xiaosheng
TI Liver segmentation based on complementary features U-Net
SO VISUAL COMPUTER
LA English
DT Article
DE U-Net; Liver segmentation; Attention; Complementary features
AB Automatic segmentation of the liver in abdominal CT images is critical for guiding liver cancer biopsies and treatment planning. Yet, automatic segmentation of CT liver images remains challenging due to the poor contrast between the liver and surrounding organs in abdominal CT images. In this paper, we propose a novel network for liver segmentation, and the network is essentially a U-shaped network with an encoder-decoder structure. Firstly, the complementary feature enhancement unit is designed in the network to mitigate the semantic gap between encoder and decoder. The complementary feature enhancement unit is based on subtraction, which enhances the complementary features between encoder and decoder. Secondly, this paper proposes a new cross attention model that no longer generates value by convolution, which reduces redundant information and enhances the contextual information of single sparse attention by encoding contextual information by 3 x 3 convolution. The dice score, accuracy, and precision of our network on the LiTS dataset were 95.85%, 97.19%, and 97.11%, and the dice score, accuracy, and precision on the dataset consisted of 3Dircadb and CHAOS were 93.65%, 94.38%, and 97.53%.
C1 [Sun, Junding; Hui, Zhenkun; Tang, Chaosheng; Wu, Xiaosheng] Henan Polytech Univ, Coll Comp Sci & Technol, Shiji Rd, Jiaozuo 454003, Henan, Peoples R China.
C3 Henan Polytechnic University
RP Wu, XS (corresponding author), Henan Polytech Univ, Coll Comp Sci & Technol, Shiji Rd, Jiaozuo 454003, Henan, Peoples R China.
EM sunjd@hpu.edu.cn; hui_zhenkun@163.com; tcs@hpu.edu.cn; wuxs@hpu.edu.cn
RI Tang, Chaosheng/AGX-8816-2022
OI Tang, Chaosheng/0000-0001-6923-855X
FU Key Science and Technology Program of Henan Province [212102310084]
FX This work was supported by the Key Science and Technology Program of
   Henan Province(212102310084)
CR Bilic P., ARXIV
   Cai K, 2017, NEUROCOMPUTING, V220, P138, DOI 10.1016/j.neucom.2016.03.106
   Chen L.-C., PREPRINT
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen SY, 2021, COMPUT BIOL MED, V135, DOI 10.1016/j.compbiomed.2021.104551
   Christ Patrick Ferdinand, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P415, DOI 10.1007/978-3-319-46723-8_48
   Fan TL, 2021, SIGNAL IMAGE VIDEO P, V15, P1089, DOI 10.1007/s11760-020-01835-9
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gu F., PREPRINT
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Han X., PREPRINT
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hyunseok S., PREPRINT
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.21492, 10.3322/caac.20107, 10.3322/caac.20115]
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Kavur AE, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101950
   Li H, PREPRINT, DOI 10.48550/arXiv.1303.3997
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Liu Z, 2021, MULTIMEDIA SYST, V27, P111, DOI 10.1007/s00530-020-00709-x
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Oktay O., PREPRINT
   Pan G, 2020, AUTOMAT CONSTR, V119, DOI 10.1016/j.autcon.2020.103383
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tokunaga H, 2019, PROC CVPR IEEE, P12589, DOI 10.1109/CVPR.2019.01288
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang J, 2022, MULTIMED TOOLS APPL, V81, P35983, DOI 10.1007/s11042-021-10841-z
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao XY, 2021, COMPUT BIOL MED, V135, DOI 10.1016/j.compbiomed.2021.104526
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 38
TC 2
Z9 2
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4685
EP 4696
DI 10.1007/s00371-022-02617-9
EA AUG 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000836989900002
DA 2024-07-18
ER

PT J
AU Phaphuangwittayakul, A
   Ying, FL
   Guo, Y
   Zhou, LT
   Chakpitak, N
AF Phaphuangwittayakul, Aniwat
   Ying, Fangli
   Guo, Yi
   Zhou, Liting
   Chakpitak, Nopasit
TI Few-shot image generation based on contrastive meta-learning generative
   adversarial network
SO VISUAL COMPUTER
LA English
DT Article
DE Few-shot image generation; Contrastive learning; Meta-learning;
   Generative adversarial network
AB Traditional deep generative models rely on enormous training data for generating images from a given class. However, they face the challenges associated with expensive and time-consuming in data acquisition as well as the requirements for fast learning from limited data of new categories. In this study, a contrastive meta-learning generative adversarial network (CML-GAN) is proposed to generate novel images of unseen classes from a few images by applying a self-supervised contrastive learning strategy to a fast adaptive meta-learning framework. By introducing a meta-learning framework into a GAN-based model, our model can efficiently learn the feature representations and quickly adapt to new generation tasks with only a few samples. The proposed model takes original input and generated images from the GAN-based model as inputs and evaluates both contrastive loss and distance loss based on the feature representations of the inputs extracted from the encoder. The original input image and its generated version from the generator are considered a positive pair, while the rest of the generated images in the same batch are considered negative samples. Then, the model converges to differentiate positive samples from negative ones and learns to generate distinct representations of the same samples, which prevents model overfitting. Thus, our model can generalize to generate diverse images from only a few samples of unseen categories, while fast adapting to new image generation tasks. Furthermore, the effectiveness of our model is demonstrated through extensive experiments on three datasets.
C1 [Phaphuangwittayakul, Aniwat; Guo, Yi] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Ying, Fangli] East China Univ Sci & Technol, Dept Comp Sci & Engn, State Key Lab Bioreactor Engn, Shanghai, Peoples R China.
   [Guo, Yi] Natl Engn Lab Big Data Distribut & Exchange Techn, Shanghai, Peoples R China.
   [Guo, Yi] Shanghai Engn Res Ctr Big Data & Internet Audienc, Shanghai, Peoples R China.
   [Zhou, Liting] Dublin City Univ, ADAPT Ctr, Sch Comp, Dublin 9, Ireland.
   [Chakpitak, Nopasit] Chiang Mai Univ, Int Coll Digital Innovat, Chiang Mai, Thailand.
C3 East China University of Science & Technology; East China University of
   Science & Technology; Dublin City University; Chiang Mai University
RP Ying, FL (corresponding author), East China Univ Sci & Technol, Dept Comp Sci & Engn, State Key Lab Bioreactor Engn, Shanghai, Peoples R China.
EM yfangli@ecust.edu.cn
RI Zhou, Liting/AAI-3991-2021; Phaphuangwittayakul, Aniwat/ABV-7858-2022
OI Phaphuangwittayakul, Aniwat/0000-0002-2289-3116
FU National Key Research and Development Program of China [2018YFC0807105,
   2020YFA0907800]; open funding project of the State Key Laboratory of
   Bioreactor Engineering, East China University of Science and Technology,
   Shanghai, China; Science and Technology Committee of Shanghai
   Municipality (STCSM) [17DZ1101003, 18511106602, 18DZ2252300];
   International College of Digital Innovation (ICDI), Chiang Mai
   University, Thailand
FX This research is financially supported by National Key Research and
   Development Program of China (No. 2020YFA0907800); is partially
   supported by open funding project of the State Key Laboratory of
   Bioreactor Engineering, East China University of Science and Technology,
   Shanghai, China; is also supported by The National Key Research and
   Development Program of China (Grant Number 2018YFC0807105) and Science
   and Technology Committee of Shanghai Municipality (STCSM) (under Grant
   Numbers 17DZ1101003, 18511106602 and 18DZ2252300); and International
   College of Digital Innovation (ICDI), Chiang Mai University, Thailand.
CR Andrychowicz, 2016, CORR ARXIV160604474
   Antoniou A., 2017, CORR ARXIV171104340
   ^atre Louis Clou, 2019, ARXIV190102199
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bartunov Sergey, 2018, PMLR, P670
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   CHEN T, 2020, INT C MACH LEARN, P1597, DOI DOI 10.48550/ARXIV.2002.05709
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hensel M, 2017, ADV NEUR IN, V30
   Hong Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2535, DOI 10.1145/3394171.3413561
   Hong Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102917
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaiswal A, 2021, TECHNOLOGIES, V9, DOI 10.3390/technologies9010002
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Kingma D. P., 2014, arXiv
   Kingma D.P., 2014, CORR ARXIV13126114
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li H., 2021, VISUAL COMPUT, V37, P1
   Li J., 2020, ICLR
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Liang W., 2020, CORR ARXIV200100576
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Miyato T, 2018, INT C LEARN REPR
   Munkhdalai Tsendsuren, 2017, Proc Mach Learn Res, V70, P2554
   Nichol A., 2018, CORR ARXIV180302999
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Ravi S., 2016, INT C LEARNING REPRE
   Rezende DJ, 2016, PR MACH LEARN RES, V48
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   van den Oord A, 2018, CORR ARXIV180703748
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Varghese D., 2020, P 14 INT RUL CHALL, V2644, P10
   Wang J., 2021, CORR ARXIV210806647
   Wang Y., 2018, CORR ARXIV180702872
   Xiao C., 2021, P IEEECVF WINTER C A, P2252
   Xu Q., 2018, CORR ARXIV180607755
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
NR 45
TC 6
Z9 6
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4015
EP 4028
DI 10.1007/s00371-022-02566-3
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000828445600001
DA 2024-07-18
ER

PT J
AU Pan, SH
   Wang, RX
   Lin, C
AF Pan, Shenghui
   Wang, Ruixing
   Lin, Chuan
TI Bio-inspired feature cascade network for edge detection
SO VISUAL COMPUTER
LA English
DT Article
DE Edge detection; Biological vision; Decoding network; Feature cascade;
   Multi-level information fusion
ID CONVOLUTIONAL FEATURES; COLOR; CONTOUR
AB Edge detection is a key step in various image processing tasks. Edge detection based on deep learning is usually composed of encoding and decoding networks. Encoding networks are usually built based on classifiers (e.g., VGG16) while focusing on the construction of decoding networks. In this paper, an encoding-decoding network is proposed by simulating the visual pathway of the retina-lateral geniculate nucleus (LGN)- the primary visual cortex (V1)-V2-V4- the inferior temporal cortex. Bio-inspired Feature Cascade Network (BFCN) was designed to simulate the transmission modes of feedforward propagation, horizontal connection, and feedback propagation among neurons in the IT, which is conducive to enhancing the characteristic analysis ability of the decoding network. Firstly, to simulate the information processing model of feedforward propagation, a Feedforward Propagation Network is designed to fully fuse the underlying information. Secondly, to simulate the information processing model of the horizontal connection between neurons, the Inter-Layer Information module (ILI) is designed to process the interlayer information of FPNet, which is beneficial to enhancing the feature extraction ability. Finally, to simulate the feedback propagation, the Proximity Combination Network (PCNet) is designed to integrate the feature prediction of each stage and strengthen the generalization ability of the network. Experimental results show that the proposed contour detection model outperforms current similar models.
C1 [Pan, Shenghui; Wang, Ruixing; Lin, Chuan] Guangxi Univ Sci & Technol, Sch Elect Elect & Comp Sci, Liuzhou 545006, Peoples R China.
C3 Guangxi University of Science & Technology
RP Lin, C (corresponding author), Guangxi Univ Sci & Technol, Sch Elect Elect & Comp Sci, Liuzhou 545006, Peoples R China.
EM chuanlin@gxust.edu.cn
RI lin, chuan/HIK-1290-2022; lin, chuan/JBJ-7047-2023; lin,
   chuan/HHD-2571-2022; luo, chuan/IVH-5370-2023
OI lin, chuan/0000-0003-1779-1753; 
FU National Natural Science Foundation of China [61866002]; Guangxi Natural
   Science Foundation [2020GXNSFDA297006, 2018GXNSFAA138122,
   2015GXNSFAA139293]; Innovation Project of Guangxi University of Science
   and Technology Graduate Education [GKYC202005]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 61866002), Guangxi Natural Science Foundation (Grant
   No.2020GXNSFDA297006, Grant No. 2018GXNSFAA138122, Grant No.
   2015GXNSFAA139293), and Innovation Project of Guangxi University of
   Science and Technology Graduate Education (Grant No. GKYC202005).
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao YJ, 2021, IEEE T MULTIMEDIA, V23, P761, DOI 10.1109/TMM.2020.2987685
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng RX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4435, DOI 10.1145/3474085.3475593
   Deng RX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P304, DOI 10.1145/3394171.3413750
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   DESIMONE R, 1985, VISION RES, V25, P441, DOI 10.1016/0042-6989(85)90069-0
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Duda R.O., 2003, IEEE T AUTOMAT CONTR, V19, P462
   ENGEL SA, 1994, NATURE, V369, P525, DOI 10.1038/369525a0
   Gao LL, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P594
   GROBSTEIN P, 1983, ANIM BEHAV, V31, P621, DOI 10.1016/S0003-3472(83)80093-1
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Hu XW, 2018, NEUROCOMPUTING, V313, P377, DOI 10.1016/j.neucom.2018.05.088
   Huan LX, 2022, IEEE T PATTERN ANAL, V44, P6602, DOI 10.1109/TPAMI.2021.3084197
   Kandel E., 2013, PRINCIPLES NEURAL SC
   Kokkinos I., 2016, INT C REPRESENTATION
   Kokkinos I., 2015, arXiv preprint arXiv:151107386
   Lin C, 2020, NEUROCOMPUTING, V409, P361, DOI 10.1016/j.neucom.2020.06.069
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu Y, 2017, COMMUN MATH BIOL NEU, DOI 10.1080/10408398.2017.1329704
   Liu Y, 2022, INT J COMPUT VISION, V130, P179, DOI 10.1007/s11263-021-01539-8
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Poma X. S., 2019, DENSE EXTREME INCEPT
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang Q., 2019, IEEE INTERNET THINGS, P1
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu D, 2017, ADV NEUR IN, V30
   Yang KF, 2015, PROC CVPR IEEE, P2254, DOI 10.1109/CVPR.2015.7298838
   Yang KF, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425538
   Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210
   Yang KF, 2013, PROC CVPR IEEE, P2810, DOI 10.1109/CVPR.2013.362
   Zeng C, 2011, NEUROIMAGE, V55, P49, DOI 10.1016/j.neuroimage.2010.11.067
   Zhang RF, 2021, J REAL-TIME IMAGE PR, V18, P647, DOI 10.1007/s11554-020-00980-1
NR 49
TC 2
Z9 2
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4149
EP 4164
DI 10.1007/s00371-022-02581-4
EA JUL 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000820093800001
DA 2024-07-18
ER

PT J
AU Firoze, A
   Benes, B
   Aliaga, D
AF Firoze, Adnan
   Benes, Bedrich
   Aliaga, Daniel
TI Urban tree generator: spatio-temporal and generative deep learning for
   urban tree localization and modeling
SO VISUAL COMPUTER
LA English
DT Article
DE Tree location; Procedural generation; Shape and surface modeling; Shape
   analysis and image retrieval; Urban tree
AB We present a vision-based algorithm that uses spatio-temporal satellite imagery, pattern recognition, procedural modeling, and deep learning to perform tree localization in urban settings. Our method resolves two primary challenges. First, automated city-scale tree localization at high accuracy typically requires significant acquisition/user intervention. Second, vegetation-index segmentation methods from satellites require manual thresholding, which varies across geographic areas, and are not robust across cities with varying terrain, geometry, altitude, and canopy. In our work, we compensate for the lack of visual detail by using satellite snapshots across twelve months and segment cities into various vegetation clusters. Then, we use multiple GAN-based networks to plant trees by recognizing placement patterns inside segmented regions procedurally. We present comprehensive experiments over four cities (Chicago, Austin, Indianapolis, Lagos), achieving tree count accuracies of 87-97%. Finally, we show that the knowledge accumulated from each model (trained on a particular city) can be transferred to a different city.
C1 [Firoze, Adnan; Benes, Bedrich; Aliaga, Daniel] Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Firoze, A (corresponding author), Purdue Univ, Dept Comp Sci, 305 N Univ St, W Lafayette, IN 47907 USA.
EM afiroze@purdue.edu
RI Benes, Bedrich/A-8150-2016; Firoze, Adnan/GOK-0795-2022
OI Benes, Bedrich/0000-0002-5293-2112; Firoze, Adnan/0000-0002-2751-009X
FU National Science Foundation [10001387]; Foundation for Food and
   Agriculture Research Grant [602757]; Integrated Digital Forestry
   Initiative (iDIF) at Purdue University; NSF [2106717, 1835739]; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [2106717] Funding Source: National Science Foundation; Office of
   Advanced Cyberinfrastructure (OAC); Direct For Computer & Info Scie &
   Enginr [1835739] Funding Source: National Science Foundation
FX This research was funded in part by National Science Foundation grant
   #10001387, Functional Proceduralization of 3D Geometric Models and by
   the Foundation for Food and Agriculture Research Grant ID: 602757. We
   thank the Integrated Digital Forestry Initiative (iDIF) at Purdue
   University for their partial support. We also thank NSF grant #1835739
   "U-Cube: A Cyberinfrastructure for Unified and Ubiquitous Urban Canopy
   Parameterization" and NSF grant #2106717 "Deep Generative Modeling for
   Urban and Archaeological Recovery."
CR Adebayo J, 2018, ADV NEUR IN, V31
   Aliaga DG, 2013, ENVIRON PLANN B, V40, P271, DOI 10.1068/b38084
   [Anonymous], 2013, Google Earth
   [Anonymous], 2021, OPEN STREET MAP CONT
   [Anonymous], 2008, City of Pittsburgh, Pennsylvania, Municipal Forest Resource Analysis
   [Anonymous], 2021, JAPAN AEROSPACE EXPL
   [Anonymous], 2021, PLANET PLANET EXPLOR
   Arief HA, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060973
   Benes B., 2011, S INTERACTIVE 3D GRA, P167
   Bovik A.C, 2005, A visual information fidelity approach to video quality assessment
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chicago Department of Transportation, 2007, STREET SIT PLAN DES
   City of Austin Texas, 2013, DOWNT TREE INV
   City of Indianapolis/Marion Indiana., 2021, COD ORD CHAPT 701 TR
   City of New York, 2021, NYC OP DAT MAP TIL
   Earth Observing System (EOS) NASA., 2021, IC CLOUD LAND EL SAT
   Food and Agriculture Organization of the United Nations, 2010, GLOB FOR RES ASS COU
   Guo JW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3394105
   Hahsler M, 2019, J STAT SOFTW, V91, P1, DOI 10.18637/jss.v091.i01
   Hojas-Gascon L., 2014, FIELD GUIDE FOREST M, DOI [10.2788/657954, DOI 10.2788/657954]
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu Q., 2020, SEMANTIC SEGMENTATIO, DOI [10.48550/ARXIV.2009.03137, DOI 10.48550/ARXIV.2009.03137]
   Huang S, 2021, J FORESTRY RES, V32, P1, DOI 10.1007/s11676-020-01155-1
   Jiang ZY, 2006, REMOTE SENS ENVIRON, V101, P366, DOI 10.1016/j.rse.2006.01.003
   Joon-Seok Kim, 2018, SIGSPATIAL Special, V10, P34, DOI 10.1145/3292390.3292397
   Keren S, 2020, FORESTS, V11, DOI 10.3390/f11050531
   Krizhevsky A., 2012, NEURIPS, V25, p1097
   Lee S., 2019, EARTH RESOURCES ENV, V11156, P313
   Li B., ACM T GRAPHIC, V40, P2021
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480486
   Lu D, 2007, INT J REMOTE SENS, V28, P823, DOI 10.1080/01431160600746456
   Maggiori E., 2017, 2017 IEEE INT GEOSCI
   Martinovic A, 2013, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2013.33
   McBride J., 2011, THESIS LUND U LUND S
   Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077
   Niese T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3502220
   Nowak D.J., 2010, Air quality effects of urban trees and parks
   Nowak D.J., 2016, Austin's Urban Forest
   Potapov P, 2021, REMOTE SENS ENVIRON, V253, DOI 10.1016/j.rse.2020.112165
   Pretzsch H, 2015, URBAN FOR URBAN GREE, V14, P466, DOI 10.1016/j.ufug.2015.04.006
   Prusinkiewicz P., 1990, The algorithmic beauty of plants, DOI [10.1007/978-1-4613-8476-2, DOI 10.1007/978-1-4613-8476-2]
   Huang ZR, 2020, TRANSPORT RES REC, V2674, P836, DOI 10.1177/0361198120925069
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russwurm M, 2017, IEEE COMPUT SOC CONF, P1496, DOI 10.1109/CVPRW.2017.193
   Shen B, 2014, CAN J REMOTE SENS, V40, P67, DOI 10.1080/07038992.2014.917580
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tinchev G., 2018, SEEING WOOD TREES RE, DOI [10.48550/ARXIV.1809.02846, DOI 10.48550/ARXIV.1809.02846]
   Tree Care Industry Association (TCIA), 2017, A300 ANSI
   Urban Design Division/Planning & Development Review Department, 2012, CIT AUST GREAT STREE
   USDA Forest Service, 2020, ITREE
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weier J., 2000, Measuring Vegetation (NDVI & EVI), V20
   Yang SD, 2021, IEEE WINT CONF APPL, P869, DOI 10.1109/WACV48630.2021.00091
   Yao L, 2021, ECOL INDIC, V125, DOI 10.1016/j.ecolind.2021.107591
   Zabelskyte G, 2022, LAND-BASEL, V11, DOI 10.3390/land11020238
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 60
TC 4
Z9 4
U1 2
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3327
EP 3339
DI 10.1007/s00371-022-02526-x
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000814039500001
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, Y
   Zhang, QH
   Guan, ZL
   Zhao, Y
   Chen, W
AF Chen, Yi
   Zhang, Qinghui
   Guan, Zeli
   Zhao, Ying
   Chen, Wei
TI GEMvis: a visual analysis method for the comparison and refinement of
   graph embedding models
SO VISUAL COMPUTER
LA English
DT Article
DE Visual analytics; Graph embedding model; Node metric; Evaluation;
   Comparison; Refinement
AB Graph embedding, which constructs vector representation of nodes in a network, has shown effectiveness in many graph analysis tasks, such as node classification, node clustering, and link prediction. However, due to the complexity of graph embedding models (GEMs) and their nontransparency of hyperparameters, evaluation and comparison of embedding results in retaining the original graph features, and consequently, the selection of suitable GEMs according to graph analysis tasks are challenging for people. In this paper, we present a visual analysis method, GEMvis, to support the evaluation and comparison of GEMs from the original graph, node metric, and embedding result spaces. The method also supports the online refining of GEM by tuning the parameters in its three components (graph sampling method, neural network structure, and loss function). A series of metrics, R_node metrics, for measuring GEMs' ability to preserve specific node metrics, such as R_degree and R_closeness, is also proposed to support quantitative evaluation and comparison of GEMs' ability to preserve original graph features. Finally, three case studies and expert feedback illustrate the effectiveness of GEMvis.
C1 [Chen, Yi; Zhang, Qinghui; Guan, Zeli] Beijing Technol & Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing, Peoples R China.
   [Zhao, Ying] Cent South Univ, Sch Comp Sci & Engn, Changsha, Hunan, Peoples R China.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
C3 Beijing Technology & Business University; Central South University;
   Zhejiang University
RP Chen, Y (corresponding author), Beijing Technol & Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing, Peoples R China.
EM chenyi@th.btbu.edu.cn
OI Zhang, Qinghui/0000-0002-2580-6241; Chen, Yi/0000-0002-4141-0554
FU National Natural Science Foundation of China [61972010, 62132017,
   61972122]; National Key R &D program of China [2018YFC1603602]
FX This work is supported by the National Natural Science Foundation of
   China (61972010, 62132017, 61972122) and National Key R &D program of
   China (2018YFC1603602).
CR Ayala D, 2019, LECT NOTES COMPUT SC, V11503, P397, DOI 10.1007/978-3-030-21348-0_26
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Chen W, 2019, IEEE T VIS COMPUT GR, V25, P555, DOI 10.1109/TVCG.2018.2865139
   Chen Y, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2647-3
   Chen Y, 2019, J VISUAL-JAPAN, V22, P625, DOI 10.1007/s12650-019-00551-y
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Hamilton W.L., 2020, Graph Representation Learning, DOI 10.2200/s01045ed1v01y202009aim046
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Heimerl F, 2022, IEEE T VIS COMPUT GR, V28, P2953, DOI 10.1109/TVCG.2020.3045918
   Inselberg A., 1987, P COMPUTER GRAPHICS, P25
   Li Q, 2018, IEEE CONF VIS ANAL, P48, DOI 10.1109/VAST.2018.8802454
   Liu SS, 2018, IEEE T VIS COMPUT GR, V24, P553, DOI 10.1109/TVCG.2017.2745141
   Liu Y, 2019, COMPUT GRAPH FORUM, V38, P67, DOI 10.1111/cgf.13672
   [刘知远 Liu Zhiyuan], 2016, [计算机研究与发展, Journal of Computer Research and Development], V53, P247
   Martin C., 2020, ARXIV PREPRINT ARXIV
   Misawa H, 2012, IEICE T INF SYST, VE95D, P804, DOI 10.1587/transinf.E95.D.804
   Molino P., 2019, Ludwig: a type-based declarative deep learning toolbox
   Parberry I., 1995, SIGACT THEORETICAL C, P197
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Do P, 2022, APPL INTELL, V52, P636, DOI 10.1007/s10489-021-02460-w
   Pienta R, 2015, INT CONF BIG DATA, P271, DOI 10.1109/35021BIGCOMP.2015.7072812
   Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061
   Rizi FS, 2017, ALGORITHMS, V10, DOI 10.3390/a10040109
   Smilkov Daniel, 2016, ARXIV PREPRINT ARXIV
   Spielman Daniel A., 2004, P 36 ANN ACM S THEOR, P81, DOI DOI 10.1145/1007352.1007372
   Surendran S., 2013, INT J COMPUT SCI EME, V4, P29
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Landesberger T, 2011, COMPUT GRAPH FORUM, V30, P1719, DOI 10.1111/j.1467-8659.2011.01898.x
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang YH, 2019, IEEE T VIS COMPUT GR, V25, P820, DOI 10.1109/TVCG.2018.2864912
NR 34
TC 9
Z9 9
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3449
EP 3462
DI 10.1007/s00371-022-02548-5
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000812611200003
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, B
   Zhang, Y
   Xu, HH
   Yin, BC
AF Li, Bo
   Zhang, Yong
   Xu, Haihui
   Yin, Baocai
TI CCST: crowd counting with swin transformer
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd counting; Transformer; Uneven distribution of crowd density; Large
   span of head size; Feature adaptive fusion
ID IMAGE
AB Accurately estimating the number of individuals contained in an image is the purpose of the crowd counting. It has always faced two major difficulties: uneven distribution of crowd density and large span of head size. Focusing on the former, most CNN-based methods divide the image into multiple patches for processing, ignoring the connection between the patches. For the latter, the multi-scale feature fusion method using feature pyramid ignores the matching relationship between the head size and the hierarchical features. In response to the above issues, we propose a crowd counting network named CCST based on swin transformer, and tailor a feature adaptive fusion regression head called FAFHead. Swin transformer can fully exchange information within and between patches, and effectively alleviate the problem of uneven distribution of crowd density. FAFHead can adaptively fuse multi-level features, improve the matching relationship between head size and feature pyramid hierarchy, and relief the problem of large span of head size available. Experimental results on common datasets show that CCST has better counting performance than all weakly supervised counting works and great majority of popular density map-based fully supervised works.
C1 [Li, Bo; Zhang, Yong; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Dept Informat Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Xu, Haihui] Beijing Municipal Transportat Operat Coordinat Ct, Beijing 100161, Peoples R China.
C3 Beijing University of Technology
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Dept Informat Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM zhangyong2010@bjut.edu.cn
RI Zhang, Yong/AAW-8880-2021; Li, Bo/JLN-1186-2023
OI Zhang, Yong/0000-0001-6650-6790; Li, Bo/0000-0003-0608-1502
FU National Key R &D Program of China [2021ZD0111902]; National Natural
   Science Foundation of China [62072015, U1811463, U19B2039]; Beijing
   Natural Science Foundation [4222021]
FX The research project is partially supported by National Key R &D Program
   of China (No.2021ZD0111902), National Natural Science Foundation of
   China (No.62072015, No.U21B2038, U1811463, U19B2039), Beijing Natural
   Science Foundation (4222021).
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   AmirgholipourKasmani S., 2018, ARXIV1804
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Cenggoro TW, 2019, PROCEDIA COMPUT SCI, V157, P175, DOI 10.1016/j.procs.2019.08.155
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong L, 2020, INFORM SCIENCES, V528, P79, DOI 10.1016/j.ins.2020.04.001
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gao J., 2021, ARXIV PREPRINT ARXIV, P2021
   Gao J, 2020, ARXIV200209545
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Han K, 2017, J ADV COMPUT INTELL, V21, P632, DOI 10.20965/jaciii.2017.p0632
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Kalyani B. LV, 2021, EFFICIENT CROWD COUN
   Kang Di, 2018, BMVC
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Kingma D. P., 2014, arXiv
   Lei T., 2021, IET IMAGE PROCESS
   Lei YJ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107616
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li BZ, 2022, INT J ENVIRON HEAL R, V32, P2260, DOI 10.1080/09603123.2021.1952166
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li Z., 2022, VISUAL COMPUT, P1
   Liang D., 2021, ARXIV PREPRINT ARXIV
   Liang D., 2022, ARXIV PREPRINT ARXIV
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu S., 2019, CVPR
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Sabzmeydani P., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383134
   Sajid U, 2021, IEEE INT CONF COMP V, P2249, DOI 10.1109/ICCVW54120.2021.00254
   Sam DB, 2019, AAAI CONF ARTIF INTE, P8868
   Sam DB, 2018, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2018.00381
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shi ZL, 2019, IEEE I CONF COMP VIS, P4199, DOI 10.1109/ICCV.2019.00430
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Song QY, 2021, AAAI CONF ARTIF INTE, V35, P2576
   Sun G., 2021, ARXIV PREPRINT ARXIV
   Tian Y., 2021, ARXIV PREPRINT ARXIV
   Tian YK, 2020, IEEE T IMAGE PROCESS, V29, P2714, DOI 10.1109/TIP.2019.2952083
   Varior R. R, 2019, ARXIV190106026
   von Borstel M, 2016, LECT NOTES COMPUT SC, V9905, P365, DOI 10.1007/978-3-319-46448-0_22
   Walach E, 2016, LECT NOTES COMPUT SC, V9906, P660, DOI 10.1007/978-3-319-46475-6_41
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang WH, 2022, IEEE T PATTERN ANAL, V44, P5349, DOI 10.1109/TPAMI.2021.3077555
   Wang Y, 2016, IEEE IMAGE PROC, P3653, DOI 10.1109/ICIP.2016.7533041
   Wen XL, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207236
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Xu CF, 2022, INT J COMPUT VISION, V130, P405, DOI 10.1007/s11263-021-01542-z
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689
   Zhang Hanwang, 2017, P IEEE C COMP VIS PA, P5532, DOI DOI 10.48550/ARXIV.1702.08319
   Zhang L, 2018, IEEE WINT CONF APPL, P1113, DOI 10.1109/WACV.2018.00127
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zheng Sixiao, 2020, ARXIV201215840, DOI [DOI 10.1109/CVPR46437.2021.00681, 10.1109/CVPR46437.2021.00681]
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu L., 2019, ARXIV190201115
NR 78
TC 16
Z9 17
U1 2
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2671
EP 2682
DI 10.1007/s00371-022-02485-3
EA APR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000785909400001
DA 2024-07-18
ER

PT J
AU Fang, PF
   Liu, H
   Wu, CM
   Liu, M
AF Fang, Pengfei
   Liu, Han
   Wu, Chengmao
   Liu, Min
TI A survey of image encryption algorithms based on chaotic system
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Chaotic system; Information security; Image processing
ID SEMI-TENSOR PRODUCT; MATRIX; NETWORKS; REPRESENTATION
AB Many researchers have devoted themselves to studying image encryption based on chaotic system and have made significant strides in research in recent decades. This paper first combs and summarizes the development of the current image encryption algorithm. Then, the classification of chaotic system is described, and representative work and the latest achievements regarding image encryption algorithms based on chaotic system are analysed, expounding their advantages and disadvantages. Finally, we discuss future research directions and development trends in image encryption algorithms based on chaotic system.
C1 [Fang, Pengfei; Liu, Han] Xian Univ Technol, Sch Automat & Informat Engn, Xian 710048, Peoples R China.
   [Wu, Chengmao] Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710061, Peoples R China.
   [Liu, Min] Australian Natl Univ, Sch Engn & Comp Sci, Canberra, ACT 0200, Australia.
C3 Xi'an University of Technology; Xi'an University of Posts &
   Telecommunications; Australian National University
RP Liu, H (corresponding author), Xian Univ Technol, Sch Automat & Informat Engn, Xian 710048, Peoples R China.
EM liuhan@xaut.edu.cn
RI Lu, Xiaomei/IUQ-2139-2023; Liu, Jinyu/JYQ-6274-2024; li,
   yang/IQV-3559-2023; Liu, Han/A-8156-2008; LI, XIAO/IQV-9318-2023; liu,
   junyang/IXD-1252-2023; yang, zhuo/JPK-3133-2023; wei,
   xiao/ISB-6027-2023; cheng, shu/IZE-4788-2023; liu,
   junyang/IXD-1201-2023; ZHAO, S/IWV-4219-2023; zhang,
   yuyang/IVV-5089-2023; gupta, shailender/Y-8231-2019; zhang,
   lin/IZQ-4870-2023; liang, YU/IYT-4334-2023
OI Liu, Han/0000-0002-6618-1380; gupta, shailender/0000-0003-1383-7152;
   Fang, Pengfei/0000-0003-2739-0996; liang, YU/0009-0007-3922-3454; Liu,
   Min/0000-0003-3112-0999; Wu, Chengmao/0000-0002-5881-4723
FU National Natural Science Foundation of China [61973248]; Key Project of
   Shaanxi Key Research and Development Program [2018ZDXM-GY-089]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61973248 and the Key Project of Shaanxi Key Research
   and Development Program under Grant 2018ZDXM-GY-089.
CR Ahmad I, 2021, APPL MATH COMPUT, V395, DOI 10.1016/j.amc.2020.125858
   Alshammari BM, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13010129
   Anwar MS, 2020, EUR PHYS J-SPEC TOP, V229, P1343, DOI 10.1140/epjst/e2020-900250-6
   Aparna H, 2021, J INF SECUR APPL, V63, DOI 10.1016/j.jisa.2021.102972
   Arpaci B, 2020, J ELECTR ENG TECHNOL, V15, P1413, DOI 10.1007/s42835-020-00393-x
   Bento PHS, 2021, J PHYS A-MATH THEOR, V54, DOI 10.1088/1751-8121/abcf58
   Biratu ES, 2021, J IMAGING, V7, DOI 10.3390/jimaging7090179
   Cai T, 2021, J STAT PLAN INFER, V213, P50, DOI 10.1016/j.jspi.2020.11.002
   Cai XS, 2019, INT J MOD PHYS B, V33, DOI 10.1142/S0217979219503831
   Chen H, 2021, OPT LASER ENG, V138, DOI 10.1016/j.optlaseng.2020.106448
   Chen LJ, 2018, MATH COMPUT SIMULAT, V146, P44, DOI 10.1016/j.matcom.2017.10.002
   Chowdhury SN, 2020, EUR PHYS J-SPEC TOP, V229, P1299, DOI 10.1140/epjst/e2020-900166-7
   Conejero JA, 2021, RACSAM REV R ACAD A, V115, DOI 10.1007/s13398-020-00996-z
   Daoui A, 2021, SIGNAL PROCESS, V180, DOI 10.1016/j.sigpro.2020.107854
   Debelee TG, 2020, J IMAGING, V6, DOI 10.3390/jimaging6110121
   Debelee TG, 2020, EVOL SYST-GER, V11, P143, DOI 10.1007/s12530-019-09297-2
   Ding LN, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22030310
   Ding Y, 2022, IEEE T NEUR NET LEAR, V33, P4915, DOI [10.1109/TNNLS.2021.3062754, 10.1080/00206814.2021.1969525]
   Dong EZ, 2018, INT J BIFURCAT CHAOS, V28, DOI 10.1142/S0218127418500815
   Dong WL, 2021, CHAOS SOLITON FRACT, V153, DOI 10.1016/j.chaos.2021.111539
   Duan XY, 2021, COMPUT SYST SCI ENG, V37, P135, DOI 10.32604/csse.2021.014030
   Elmanfaloty RA, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/7647421
   Fang PF, 2021, IEEE ACCESS, V9, P18497, DOI 10.1109/ACCESS.2020.3040573
   Fang PF, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/6691547
   Faragallah OS, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106333
   Faragallah OS, 2018, OPT QUANT ELECTRON, V50, DOI 10.1007/s11082-018-1363-x
   Feng CF, 2021, PHYS SCRIPTA, V96, DOI 10.1088/1402-4896/abe4f1
   Feng W, 2021, IEEE ACCESS, V9, P145459, DOI 10.1109/ACCESS.2021.3123571
   Feng Y, 2020, EUR PHYS J-SPEC TOP, V229, P1279, DOI 10.1140/epjst/e2020-900097-0
   Gan JY, 2020, CHINESE J ELECTRON, V29, P312, DOI 10.1049/cje.2020.01.009
   Ghosh D, 2021, INFORM SOFTWARE TECH, V133, DOI 10.1016/j.infsof.2021.106512
   Girdhar A, 2021, APPL PHYS B-LASERS O, V127, DOI 10.1007/s00340-021-07585-x
   Gong LH, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/6627005
   Hagras EAA, 2020, MULTIMED TOOLS APPL, V79, P23203, DOI 10.1007/s11042-019-08517-w
   He Y, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-85377-1
   Herbadji D, 2019, TRAIT SIGNAL, V36, P407, DOI 10.18280/ts.360505
   Hernández-Díaz E, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10040413
   Hosny KM, 2023, VISUAL COMPUT, V39, P1027, DOI 10.1007/s00371-021-02382-1
   Hosny KM, 2022, J AMB INTEL HUM COMP, V13, P973, DOI 10.1007/s12652-021-03675-y
   Huang HQ, 2020, IET IMAGE PROCESS, V14, P1157, DOI 10.1049/iet-ipr.2019.0551
   Huang LL, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/3965281
   Hui YY, 2023, MULTIMED TOOLS APPL, V82, P21983, DOI 10.1007/s11042-021-10526-7
   Janani T, 2021, J INF SECUR APPL, V59, DOI 10.1016/j.jisa.2021.102832
   Javeed A, 2020, CHINESE J PHYS, V66, P645, DOI 10.1016/j.cjph.2020.04.008
   Jiang DH, 2021, SIGNAL PROCESS, V188, DOI 10.1016/j.sigpro.2021.108220
   Kanso A, 2012, COMMUN NONLINEAR SCI, V17, P2943, DOI 10.1016/j.cnsns.2011.11.030
   Khan M, 2021, INTEGRATION, V81, P108, DOI 10.1016/j.vlsi.2021.05.007
   Kong YN, 2019, ARTIF INTELL REV, V52, P607, DOI 10.1007/s10462-018-9640-4
   Kumari M, 2020, MULTIMED TOOLS APPL, V79, P33161, DOI 10.1007/s11042-020-09627-6
   Le PQ, 2011, QUANTUM INF PROCESS, V10, P63, DOI 10.1007/s11128-010-0177-y
   Li T, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.5902
   Li Z, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10217469
   Liang XK, 2019, INT J PATTERN RECOGN, V33, DOI 10.1142/S0218001419540223
   Lima VS, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103772
   Liu BY, 2016, IEEE INT SYMP CIRC S, P1326, DOI 10.1109/ISCAS.2016.7527493
   Liu H, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21040343
   Lu Q, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13122317
   Luo C, 2021, APPL MATH COMPUT, V393, DOI 10.1016/j.amc.2020.125759
   Ma CG, 2021, NONLINEAR DYNAM, V103, P2867, DOI 10.1007/s11071-021-06276-8
   Man ZL, 2021, CHAOS SOLITON FRACT, V152, DOI 10.1016/j.chaos.2021.111318
   Mansouri A, 2021, MULTIMED TOOLS APPL, V80, P21955, DOI 10.1007/s11042-021-10757-8
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Midoun MA, 2021, OPT LASER ENG, V139, DOI 10.1016/j.optlaseng.2020.106485
   Mondal B, 2022, MULTIMED TOOLS APPL, V81, P34547, DOI 10.1007/s11042-021-11657-7
   Mousavi M, 2021, MULTIMED TOOLS APPL, V80, P13157, DOI 10.1007/s11042-020-10440-4
   Muthu JS, 2022, OPTIK, V251, DOI 10.1016/j.ijleo.2021.168416
   Naik K, 2018, INT ARAB J INF TECHN, V15, P739
   Nawaz SA, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232902
   Nezhad SYD, 2020, OPTIK, V224, DOI 10.1016/j.ijleo.2020.165661
   Paknejad P, 2021, FUTURE GENER COMP SY, V117, P12, DOI 10.1016/j.future.2020.11.002
   Ponte B, 2013, 2013 INTERNATIONAL CONFERENCE ON NEW CONCEPTS IN SMART CITIES: FOSTERING PUBLIC AND PRIVATE ALLIANCES (SMARTMILE)
   Preishuber M, 2018, IEEE T INF FOREN SEC, V13, P2137, DOI 10.1109/TIFS.2018.2812080
   Qi GY, 2021, APPL MATH MODEL, V92, P333, DOI 10.1016/j.apm.2020.11.015
   Rajasekaran S, 2021, SIGNAL PROCESS, V182, DOI 10.1016/j.sigpro.2020.107947
   Shafique A, 2021, IEEE ACCESS, V9, P9383, DOI 10.1109/ACCESS.2020.3046528
   Shafique A, 2020, WIRELESS PERS COMMUN, V115, P2243, DOI 10.1007/s11277-020-07680-w
   Shahna KU, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116495
   Shahna K, 2020, APPL SOFT COMPUT, V90, DOI 10.1016/j.asoc.2020.106162
   Shao ZH, 2021, MULTIMED TOOLS APPL, V80, P8973, DOI 10.1007/s11042-020-09961-9
   Sivaraman R, 2020, IET IMAGE PROCESS, V14, P2987, DOI 10.1049/iet-ipr.2019.0168
   Sneha PS, 2020, J AMB INTEL HUM COMP, V11, P1289, DOI 10.1007/s12652-019-01385-0
   Sun JW, 2020, IJST-T ELECTR ENG, V44, P449, DOI 10.1007/s40998-019-00239-x
   Sun LM, 2021, MULTIMED TOOLS APPL, V80, P10285, DOI 10.1007/s11042-020-10075-5
   Talhaoui MZ, 2021, INFORM SCIENCES, V550, P13, DOI 10.1016/j.ins.2020.10.048
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Tang YL, 2020, SECUR COMMUN NETW, V2020, DOI 10.1155/2020/6665702
   Tong FH, 2021, SIGNAL PROCESS, V182, DOI 10.1016/j.sigpro.2020.107951
   Tong XJ, 2012, IMAGING SCI J, V60, P294, DOI 10.1179/1743131X11Y.0000000042
   Veena G, 2021, INT J ADV COMPUT SC, V12, P379
   Venegas-Andraca SE, 2010, QUANTUM INF PROCESS, V9, P1, DOI 10.1007/s11128-009-0123-z
   Vidhya R, 2020, APPL INTELL, V50, P3101, DOI 10.1007/s10489-020-01697-1
   Wang B, 2018, OPTIK, V154, P538, DOI 10.1016/j.ijleo.2017.10.080
   Wang KS, 2021, MULTIMED TOOLS APPL, V80, P18875, DOI 10.1007/s11042-021-10511-0
   Wang SC, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2019.105995
   Wang XY, 2010, NONLINEAR DYNAM, V62, P615, DOI 10.1007/s11071-010-9749-8
   Wang XY, 2023, VISUAL COMPUT, V39, P43, DOI 10.1007/s00371-021-02311-2
   Wang XY, 2021, INFORM SCIENCES, V574, P505, DOI 10.1016/j.ins.2021.06.032
   Wang XY, 2021, INT J BIFURCAT CHAOS, V31, DOI 10.1142/S0218127421500036
   Wang XY, 2021, MULTIMED TOOLS APPL, V80, P11655, DOI 10.1007/s11042-020-10202-2
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2020, OPTIK, V217, DOI 10.1016/j.ijleo.2020.164884
   Wang XY, 2019, J FRANKLIN I, V356, P11638, DOI 10.1016/j.jfranklin.2019.10.006
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Watt SD, 2021, APPL MATH LETT, V115, DOI 10.1016/j.aml.2020.106960
   Wirthmüller F, 2021, IEEE ROBOT AUTOM LET, V6, P2357, DOI 10.1109/LRA.2021.3058930
   Wu JJ, 2021, MULTIMED TOOLS APPL, V80, P2647, DOI 10.1007/s11042-020-09828-z
   Wu YH, 2020, J INTELL FUZZY SYST, V39, P5085, DOI 10.3233/JIFS-179994
   Wu ZM, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23091159
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Xiao Y, 2020, PHYS SCRIPTA, V95, DOI 10.1088/1402-4896/ab842e
   Xie J, 2021, EXPERT SYST APPL, V169, DOI 10.1016/j.eswa.2020.114442
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Yang CH, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12020189
   Yao W, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0165937
   Yao W, 2015, NONLINEAR DYNAM, V81, P151, DOI 10.1007/s11071-015-1979-3
   Ye GD, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78127-2
   Yildirim M, 2020, MICROELECTRON J, V104, DOI 10.1016/j.mejo.2020.104878
   Yu JY, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23030278
   Yuan SZ, 2014, QUANTUM INF PROCESS, V13, P1353, DOI 10.1007/s11128-014-0733-y
   Zang HY, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9040365
   Zhang JX, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-019-1520-6
   Zhang Y, 2020, INFORM SCIENCES, V526, P180, DOI 10.1016/j.ins.2020.03.054
   Zhao CF, 2020, NONLINEAR DYNAM, V100, P679, DOI 10.1007/s11071-020-05526-5
   Zhao D, 2021, KNOWL-BASED SYST, V216, DOI 10.1016/j.knosys.2020.106510
   Zhao HX, 2021, OPTIK, V230, DOI 10.1016/j.ijleo.2021.166307
   Zheng J, 2021, MULTIMED TOOLS APPL, V80, P20883, DOI 10.1007/s11042-021-10751-0
   Zhou RG, 2020, INT J QUANTUM INF, V18, DOI 10.1142/S0219749920500227
NR 128
TC 24
Z9 24
U1 19
U2 212
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1975
EP 2003
DI 10.1007/s00371-022-02459-5
EA APR 2022
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000779611700002
DA 2024-07-18
ER

PT J
AU Üzen, H
   Turkoglu, M
   Aslan, M
   Hanbay, D
AF Uzen, Huseyin
   Turkoglu, Muammer
   Aslan, Muzaffer
   Hanbay, Davut
TI Depth-wise Squeeze and Excitation Block-based Efficient-Unet model for
   surface defect detection
SO VISUAL COMPUTER
LA English
DT Article
DE Pixel-level surface defects' detection; Convolutional neural network;
   Depth-wise Squeeze and Excitation Block; Unet
ID CONVOLUTIONAL NEURAL-NETWORK; VISION SYSTEM; CLASSIFICATION; ALGORITHM;
   IMAGES
AB Detection of surface defects in manufacturing systems is crucial for product quality. Detection of surface defects with high accuracy can prevent financial and time losses. Recently, efforts to develop high-performance automatic surface defect detection systems using computer vision and machine-learning methods have become prominent. In line with this purpose, this paper proposed a novel approach based on Depth-wise Squeeze and Excitation Block-based Efficient-Unet (DSEB-EUNet) for automatic surface defect detection. The proposed model consists of an encoder-decoder, the basic structure of the Unet architecture, and a Depth-wise Squeeze and Excitation Block added to the skip-connection of Unet. First, in the encoder part of the proposed model, low-level and high-level features were obtained by the EfficientNet network. Then, these features were transferred to the Depth-wise Squeeze and Excitation Block. The proposed DSEB based on the combination of Squeeze-Excitation and Depth-wise Separable Convolution enabled to reveal of critical information by weighting the features with a lightweight gating mechanism for surface defect detection. Besides, in the decoder part of the proposed model, the structure called Multi-level Feature Concatenated Block (MFCB) transferred the weighted features to the last layers without losing spatial detail. Finally, pixel-level defect detection was performed using the sigmoid function. The proposed model was tested using three general datasets for surface defect detection. In experimental works, the best F1-scores for MT, DAGM, and AITEX datasets using the proposed DSEB-EUNet architecture were 89.20%, 85.97%, and 90.39%, respectively. These results showed the proposed model outperforms higher performance compared to state-of-the-art approaches.
C1 [Uzen, Huseyin] Bingol Univ, Fac Engn, Dept Comp Engn, Bingol, Turkey.
   [Turkoglu, Muammer] Samsun Univ, Fac Engn, Dept Software Engn, Samsun, Turkey.
   [Aslan, Muzaffer] Bingol Univ, Fac Engn, Dept Elect Elect Engn, Bingol, Turkey.
   [Hanbay, Davut] Inonu Univ, Engn Fac, Dept Comp Engn, Malatya, Turkey.
C3 Bingol University; Samsun University; Bingol University; Inonu
   University
RP Üzen, H (corresponding author), Bingol Univ, Fac Engn, Dept Comp Engn, Bingol, Turkey.
EM huzen@bingol.edu.tr; muammer.turkoglu@samsun.edu.tr;
   muzafferaslan@bingol.edu.tr; davut.hanbay@inonu.edu.tr
RI Aslan, Muzaffer/U-5355-2018; UZEN, Huseyin/CZK-0841-2022; Hanbay,
   Davut/AAG-8511-2019
OI Aslan, Muzaffer/0000-0002-2418-9472; UZEN, Huseyin/0000-0002-0998-2130;
   Hanbay, Davut/0000-0003-2271-7865
FU Inonu University Scientific Research Projects Coordination
   [FDK-2021-2725]
FX This work was supported by the Inonu University Scientific Research
   Projects Coordination [Grant Number FDK-2021-2725].
CR Ai YH, 2013, J IRON STEEL RES INT, V20, P80, DOI 10.1016/S1006-706X(13)60102-8
   Ari A, 2018, TURK J ELECTR ENG CO, V26, P2275, DOI 10.3906/elk-1801-8
   Aslam Y, 2021, J AMB INTEL HUM COMP, V12, P4205, DOI 10.1007/s12652-020-01803-8
   Augustauskas R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092557
   Baheti B, 2020, IEEE COMPUT SOC CONF, P1473, DOI 10.1109/CVPRW50498.2020.00187
   Bi Mingde, 2012, Information Technology Journal, V11, P673, DOI 10.3923/itj.2012.673.685
   Bissi L, 2013, J VIS COMMUN IMAGE R, V24, P838, DOI 10.1016/j.jvcir.2013.05.011
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cao X., 2020, ISPCE CN 2020 IEEE I, DOI 10.1109/ISPCE-CN51288.2020.9321855
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen H, 2021, NEURAL COMPUT APPL, V33, P3713, DOI 10.1007/s00521-020-05227-5
   Chen J., 2021, Transunet: transformers make strong encoders for medical image segmentation
   Chen PH, 2016, IEEE IMAGE PROC, P749, DOI 10.1109/ICIP.2016.7532457
   Cheon S, 2019, IEEE T SEMICONDUCT M, V32, P163, DOI 10.1109/TSM.2019.2902657
   Choi DC, 2011, APPL OPTICS, V50, P5122, DOI 10.1364/AO.50.005122
   Dai WT, 2021, VISUAL COMPUT, V37, P3093, DOI 10.1007/s00371-021-02257-5
   Dai WT, 2022, VISUAL COMPUT, V38, P1181, DOI 10.1007/s00371-021-02137-y
   Damacharla P., 2021, ARXIV PREPRINT ARXIV
   Deitsch S, 2019, SOL ENERGY, V185, P455, DOI 10.1016/j.solener.2019.02.067
   Djukic D., 2007, P IMAGE VISION COMPU, P158
   Dong HW, 2020, IEEE T IND INFORM, V16, P7448, DOI 10.1109/TII.2019.2958826
   Fu X, 2020, NEUROCOMPUTING, V380, P212, DOI 10.1016/j.neucom.2019.11.002
   Gayubo F, 2006, INT C PATT RECOG, P723
   Ghorai S, 2013, IEEE T INSTRUM MEAS, V62, P612, DOI 10.1109/TIM.2012.2218677
   Hanbay K., 2017, INT ART INT DAT PROC, P1, DOI DOI 10.1109/IDAP.2017.8090180
   Hanbay K, 2016, OPTIK, V127, P11960, DOI 10.1016/j.ijleo.2016.09.110
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   HU J, 2017, IEEE T PATTERN ANAL, V42
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Jing JF, 2022, TEXT RES J, V92, P30, DOI 10.1177/0040517520928604
   Kaddah W, 2020, VISUAL COMPUT, V36, P1369, DOI 10.1007/s00371-019-01742-2
   Katsamenis Iason, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12509), P160, DOI 10.1007/978-3-030-64556-4_13
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Duy Huynh N.B., 2020, CEUR WORKSHOP P, V2595, P13
   Li JY, 2018, IFAC PAPERSONLINE, V51, P76, DOI 10.1016/j.ifacol.2018.09.412
   Li YT, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091678
   Lin H, 2019, J INTELL MANUF, V30, P2525, DOI 10.1007/s10845-018-1415-x
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu J, 2021, OPT LASER ENG, V136, DOI 10.1016/j.optlaseng.2020.106324
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo Q, 2019, NDT&E INT, V108, DOI 10.1016/j.ndteint.2019.102164
   Luo QW, 2020, IEEE T INSTRUM MEAS, V69, P626, DOI 10.1109/TIM.2019.2963555
   Lv XM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061562
   MAK KL, 2005, IEEE INT C IND TECHN, P799, DOI DOI 10.1109/ICIT.2005.1600745
   Masci J, 2013, IEEE IJCNN
   Masci J, 2012, IEEE IJCNN
   Mujeeb A, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P391, DOI 10.1109/CW.2018.00076
   Natarajan V, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P986, DOI 10.1109/ICIT.2017.7915495
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Park JK, 2016, INT J PR ENG MAN-GT, V3, P303, DOI 10.1007/s40684-016-0039-x
   Pastor-López I, 2021, NEUROCOMPUTING, V456, P622, DOI 10.1016/j.neucom.2020.08.094
   Qian SY, 2022, IEEE T INTELL TRANSP, V23, P6883, DOI 10.1109/TITS.2021.3063199
   Qiu LT, 2019, IEEE ACCESS, V7, P15884, DOI 10.1109/ACCESS.2019.2894420
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Ruan LF, 2020, NEUROCOMPUTING, V417, P441, DOI 10.1016/j.neucom.2020.07.093
   Sari-Sarraf H, 1999, IEEE T IND APPL, V35, P1252, DOI 10.1109/28.806035
   Seçkin AC, 2022, ALEX ENG J, V61, P2887, DOI 10.1016/j.aej.2021.08.017
   Sermanet P., 2014, INT C LEARN REPR
   Shanmugamani R, 2015, MEASUREMENT, V60, P222, DOI 10.1016/j.measurement.2014.10.009
   Shi JY, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020518
   Silvestre-Blanes J, 2019, AUTEX RES J, V19, P363, DOI 10.2478/aut-2019-0035
   Simonyan K., 2014, CORR
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tsai DM, 2012, MACH VISION APPL, V23, P869, DOI 10.1007/s00138-011-0403-3
   Uzen H., 2019, 2019 INT C ART INT D
   UZEN H, 2019, 2019 INT ARTIFICIAL, P1
   Uzen H, 2021, EXPERT SYST APPL, V175, DOI 10.1016/j.eswa.2021.114838
   Wang YL, 2018, MULTIMED TOOLS APPL, V77, P16741, DOI 10.1007/s11042-017-5238-0
   Weimer D, 2016, CIRP ANN-MANUF TECHN, V65, P417, DOI 10.1016/j.cirp.2016.04.072
   Wieler M., Weakly Supervised Learning for Industrial Optical Inspection
   Xu YY, 2021, MEASUREMENT, V178, DOI 10.1016/j.measurement.2021.109316
   Yakubovskiy, SEGMENTATION MODELS
   Yan H, 2017, TECHNOMETRICS, V59, P102, DOI 10.1080/00401706.2015.1102764
   Yang J, 2018, APPL OPTICS, V57, P2490, DOI 10.1364/AO.57.002490
   Yang TJ, 2021, VISUAL COMPUT, V37, P1559, DOI 10.1007/s00371-020-01901-w
   Yuxiang W., 2021, J PHYS C SER, V1914, DOI 10.1088/1742-6596/1914/1/012037
   Yuxin Li V., 2021, J PHYS C SER, V1948, DOI 10.1088/1742-6596/1948/1/012160
   Zhang DF, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3040890
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng XQ, 2021, INT J ADV MANUF TECH, V113, P35, DOI 10.1007/s00170-021-06592-8
   Zhou QH, 2021, TEXT RES J, V91, P962, DOI 10.1177/0040517520966733
   Zijian Zhang, 2020, 2020 7th International Conference on Dependable Systems and Their Applications (DSA), P407, DOI 10.1109/DSA51864.2020.00071
NR 89
TC 28
Z9 28
U1 10
U2 67
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1745
EP 1764
DI 10.1007/s00371-022-02442-0
EA MAR 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000776062200002
DA 2024-07-18
ER

PT J
AU Hosny, KM
   Kamal, ST
   Darwish, MM
AF Hosny, Khalid M.
   Kamal, Sara T.
   Darwish, Mohamed M.
TI A novel color image encryption based on fractional shifted Gegenbauer
   moments and 2D logistic-sine map
SO VISUAL COMPUTER
LA English
DT Article
DE Color image encryption; Fractional-order orthogonal moments; Image
   encryption; Image decryption
ID ORTHOGONAL MOMENTS; FOURIER MOMENTS; CHAOTIC SYSTEM; ALGORITHM; DNA;
   REPRESENTATION; SET; CRYPTANALYSIS
AB A novel cryptosystem of color images is proposed. We defined a multi-channel orthogonal Gegenbauer moments with fractional order (FrMGMs) in Cartesian coordinates and then combined the FrMGMs with the 2D logistic-sine map. The proposed color image encryption method has three main steps in this process. The first step is confusion, where image pixels' values are changed based on a 2D logistic-sine map to reconstruct the scrambled image. In the second step, the key is generated, which is used in encryption. The key is produced by combining the logistic map and the multi-channel Gegenbauer moments with fractional order. We used the original image to set the logistic map's initial condition. In the third step, the scrambled image is diffused using the key to obtain an encrypted image. The experiments' results and the security analysis show that the proposed technique has an extensive keyspace, high key sensitivity, and good encryption effect and can be judged robust against common security attacks.
C1 [Hosny, Khalid M.] Zagazig Univ, Dept Informat Technol, Zagazig, Egypt.
   [Kamal, Sara T.; Darwish, Mohamed M.] Assiut Univ, Dept Comp Sci, Assiut, Egypt.
C3 Egyptian Knowledge Bank (EKB); Zagazig University; Egyptian Knowledge
   Bank (EKB); Assiut University
RP Hosny, KM (corresponding author), Zagazig Univ, Dept Informat Technol, Zagazig, Egypt.
EM k_hosny@yahoo.com
RI Hosny, Khalid M./B-1404-2008; Darwish, Mohamed/U-2874-2017; Darwish,
   Mohamed/Q-7906-2019; Tarik, Sara/GXN-0388-2022
OI Hosny, Khalid M./0000-0001-8065-8977; Darwish,
   Mohamed/0000-0002-1372-6922; 
CR Abd Elaziz M, 2020, APPL SOFT COMPUT, V95, DOI 10.1016/j.asoc.2020.106504
   Abdel-Aziz MM, 2021, MULTIMED TOOLS APPL, V80, P12641, DOI 10.1007/s11042-020-10217-9
   Alawida M, 2019, SIGNAL PROCESS, V160, P45, DOI 10.1016/j.sigpro.2019.02.016
   Ali TS, 2020, MULTIMED TOOLS APPL, V79, P19853, DOI 10.1007/s11042-020-08850-5
   [Anonymous], 2019, MULTIMEDIA TOOLS APP
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Batool SI, 2019, MULTIMED TOOLS APPL, V78, P27611, DOI 10.1007/s11042-019-07881-x
   Benouini R, 2019, PATTERN RECOGN, V86, P332, DOI 10.1016/j.patcog.2018.10.001
   Broumandnia A, 2019, FUTURE GENER COMP SY, V99, P489, DOI 10.1016/j.future.2019.04.005
   Chai XL, 2020, NEURAL COMPUT APPL, V32, P8065, DOI 10.1007/s00521-019-04312-8
   Chen BJ, 2018, IEEE ACCESS, V6, P56637, DOI 10.1109/ACCESS.2018.2871952
   Chen H, 2019, OPT LASER ENG, V112, P7, DOI 10.1016/j.optlaseng.2018.08.020
   Chen JX, 2020, INFORM SCIENCES, V520, P130, DOI 10.1016/j.ins.2020.02.024
   Daoui A, 2021, SIGNAL PROCESS, V180, DOI 10.1016/j.sigpro.2020.107854
   El Ogri O, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106346
   Faires J. D., 2002, Numerical Methods, V3th
   GURPREET K, VISUAL COMPUT, DOI DOI 10.1007/S00371-021-02066-W
   Hosny KM, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103007
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P505, DOI 10.1007/s11042-021-11384-z
   Hosny KM, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091066
   Hosny KM, 2021, IEEE ACCESS, V9, P47425, DOI 10.1109/ACCESS.2021.3068211
   Hosny KM, 2021, NEURAL COMPUT APPL, V33, P5419, DOI 10.1007/s00521-020-05280-0
   Hosny KM, 2020, J ADV RES, V25, P57, DOI 10.1016/j.jare.2020.05.024
   Hosny KM, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107545
   Hosny KM, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107324
   Hosny KM, 2020, IEEE ACCESS, V8, P40732, DOI 10.1109/ACCESS.2020.2976759
   Hosny KM, 2020, J FRANKLIN I, V357, P2533, DOI 10.1016/j.jfranklin.2020.01.025
   Hosny KM, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3325193
   Hosny KM, 2019, PATTERN RECOGN, V88, P153, DOI 10.1016/j.patcog.2018.11.014
   Hosny KM, 2018, J MATH IMAGING VIS, V60, P717, DOI 10.1007/s10851-018-0786-0
   Hosny KM, 2011, PATTERN RECOGN LETT, V32, P795, DOI 10.1016/j.patrec.2011.01.006
   Hua ZY, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.107998
   Hua ZY, 2021, NONLINEAR DYNAM, V104, P4505, DOI 10.1007/s11071-021-06472-6
   Hua ZY, 2021, NONLINEAR DYNAM, V104, P807, DOI 10.1007/s11071-021-06308-3
   Hua ZY, 2021, INFORM SCIENCES, V546, P1063, DOI 10.1016/j.ins.2020.09.032
   Hua ZY, 2020, IEEE T SIGNAL PROCES, V68, P1937, DOI 10.1109/TSP.2020.2979596
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Huang HQ, 2020, IET IMAGE PROCESS, V14, P1157, DOI 10.1049/iet-ipr.2019.0551
   Huang LQ, 2019, OPT LASER ENG, V115, P7, DOI 10.1016/j.optlaseng.2018.11.015
   Irani BY, 2019, NONLINEAR DYNAM, V97, P2693, DOI 10.1007/s11071-019-05157-5
   Kamal ST, 2021, IEEE ACCESS, V9, P37855, DOI 10.1109/ACCESS.2021.3063237
   Kamrani A, 2020, MULTIMED TOOLS APPL, V79, P20263, DOI 10.1007/s11042-020-08879-6
   Kang XJ, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115670
   Kaur M, 2020, APPL PHYS B-LASERS O, V126, DOI 10.1007/s00340-020-07480-x
   Kaur M, 2020, FUTURE GENER COMP SY, V107, P333, DOI 10.1016/j.future.2020.02.029
   Kaur M, 2019, NEURAL COMPUT APPL, V31, P7975, DOI 10.1007/s00521-018-3642-7
   Khan JS, 2021, J INF SECUR APPL, V58, DOI 10.1016/j.jisa.2020.102711
   Khedmati Y, 2020, INFORM SCIENCES, V512, P855, DOI 10.1016/j.ins.2019.10.028
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Liu HJ, 2019, MULTIMED TOOLS APPL, V78, P15997, DOI 10.1007/s11042-018-6996-z
   Liu ZT, 2019, IEEE ACCESS, V7, P78367, DOI 10.1109/ACCESS.2019.2922376
   Luo YL, 2020, OPT LASER ENG, V124, DOI 10.1016/j.optlaseng.2019.105836
   Mani P, 2019, INFORM SCIENCES, V491, P74, DOI 10.1016/j.ins.2019.04.007
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Ghadirli HM, 2021, MULTIMED TOOLS APPL, V80, P8445, DOI 10.1007/s11042-020-10014-4
   Pak C, 2019, MULTIMED TOOLS APPL, V78, P12027, DOI 10.1007/s11042-018-6739-1
   Patro KAK, 2019, J INF SECUR APPL, V46, P23, DOI 10.1016/j.jisa.2019.02.006
   Raza SF, 2019, NONLINEAR DYNAM, V95, P859, DOI 10.1007/s11071-018-4600-8
   Rehman AU, 2019, MULTIMED TOOLS APPL, V78, P2105, DOI 10.1007/s11042-018-6346-1
   Singh C, 2018, DIGIT SIGNAL PROCESS, V78, P376, DOI 10.1016/j.dsp.2018.04.001
   Sun YJ, 2021, MULTIMED TOOLS APPL, V80, P12155, DOI 10.1007/s11042-020-10373-y
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Toughi S, 2017, SIGNAL PROCESS, V141, P217, DOI 10.1016/j.sigpro.2017.06.010
   Wang XY, 2019, OPT LASER ENG, V115, P107, DOI 10.1016/j.optlaseng.2018.11.010
   Wang XY, 2021, CHAOS SOLITON FRACT, V147, DOI 10.1016/j.chaos.2021.110970
   Wang XY, 2022, VISUAL COMPUT, V38, P763, DOI 10.1007/s00371-020-02048-4
   Wu JH, 2018, SIGNAL PROCESS, V153, P11, DOI 10.1016/j.sigpro.2018.06.008
   Wu JH, 2018, SIGNAL PROCESS, V142, P292, DOI 10.1016/j.sigpro.2017.06.014
   Xiao B, 2020, INFORM SCIENCES, V516, P545, DOI 10.1016/j.ins.2019.12.044
   Xiong ZG, 2019, MULTIMED TOOLS APPL, V78, P31035, DOI 10.1007/s11042-018-7081-3
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Yang B, 2018, MULTIMED TOOLS APPL, V77, P21803, DOI 10.1007/s11042-017-5590-0
   Ye GD, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78127-2
   Zhang QY, 2021, MULTIMED TOOLS APPL, V80, P13841, DOI 10.1007/s11042-020-10437-z
   Zhang YQ, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2020.106040
   Zhao FX, 2021, OPT LASER TECHNOL, V135, DOI 10.1016/j.optlastec.2020.106610
   Zhou J, 2020, OPT LASER TECHNOL, V131, DOI 10.1016/j.optlastec.2020.106437
   Zhou S, 2020, CHAOS SOLITON FRACT, V141, DOI 10.1016/j.chaos.2020.110225
NR 80
TC 30
Z9 30
U1 10
U2 74
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1027
EP 1044
DI 10.1007/s00371-021-02382-1
EA JAN 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741630200005
DA 2024-07-18
ER

PT J
AU Gilles, M
   Ibrahimpasic, S
AF Gilles, Maximilian
   Ibrahimpasic, Sascha
TI Unsupervised deep learning based ego motion estimation with a downward
   facing camera
SO VISUAL COMPUTER
LA English
DT Article
DE Visual odometry; Spatial transformer layer; Unsupervised learning; Ego
   motion estimation; Downward facing camera
ID VISUAL ODOMETRY; HOMOGRAPHY
AB Knowing the robot's pose is a crucial prerequisite for mobile robot tasks such as collision avoidance or autonomous navigation. Using powerful predictive models to estimate transformations for visual odometry via downward facing cameras is an understudied area of research. This work proposes a novel approach based on deep learning for estimating ego motion with a downward looking camera. The network can be trained completely unsupervised and is not restricted to a specific motion model. We propose two neural network architectures based on the Early Fusion and Slow Fusion design principle: "EarlyBird" and "SlowBird". Both networks share a Spatial Transformer layer for image warping and are trained with a modified structural similarity index (SSIM) loss function. Experiments carried out in simulation and for a real world differential drive robot show similar and partially better results of our proposed deep learning based approaches compared to a state-of-the-art method based on fast Fourier transformation.
C1 [Gilles, Maximilian] Karlsruher Inst Technol, Inst Mat Handling & Logist, KIT, Karlsruhe, Germany.
   [Ibrahimpasic, Sascha] Things Alive Robot GmbH, Karlsruhe, Germany.
RP Gilles, M (corresponding author), Karlsruher Inst Technol, Inst Mat Handling & Logist, KIT, Karlsruhe, Germany.
EM maximilian.gilles@kit.edu
OI Gilles, Maximilian/0000-0002-0528-5709
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Birem M, 2018, J REAL-TIME IMAGE PR, V14, P637, DOI 10.1007/s11554-017-0706-3
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fu B, 2019, IEEE INT CONF ROBOT, P1841, DOI [10.1109/ICRA.2019.8793741, 10.1109/icra.2019.8793741]
   Gao L, 2020, IEEE INT CONF ROBOT, P2696, DOI [10.1109/icra40945.2020.9196595, 10.1109/ICRA40945.2020.9196595]
   Goecke Roland, 2007, Proceedings of the 2007 IEEE Intelligent Vehicles Symposium, P450
   Hartley R., 2015, MULTIPLE VIEW GEOMET, V2
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Jaderberg M, 2015, ADV NEUR IN, V28
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Ke QF, 2003, PROC CVPR IEEE, P390
   Kingma D. P., 2014, arXiv
   Kitt B., 2011, P 5 EUR C MOB ROB EC
   More V, 2015, 2015 INTERNATIONAL CONFERENCE ON COGNITIVE COMPUTING AND INFORMATION PROCESSING (CCIP)
   Nourani-Vatani N, 2009, IEEE INT CONF ROBOT, P1411
   Ri Y, 2018, INT WORK ADV MOT, P295, DOI 10.1109/AMC.2019.8371106
   Saurer O, 2017, IEEE T PATTERN ANAL, V39, P327, DOI 10.1109/TPAMI.2016.2545663
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Schmid JF, 2020, IEEE INT CONF ROBOT, P1315, DOI [10.1109/icra40945.2020.9197221, 10.1109/ICRA40945.2020.9197221]
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Nguyen T, 2018, IEEE ROBOT AUTOM LET, V3, P2346, DOI 10.1109/LRA.2018.2809549
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber M, 2017, IEEE INT C INTELL TR
   Xin Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P51, DOI 10.1007/978-3-030-58574-7_4
   Yu Y, 2011, IEEE ASME INT C ADV, P862, DOI 10.1109/AIM.2011.6027050
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 25
TC 5
Z9 5
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 785
EP 798
DI 10.1007/s00371-021-02345-6
EA NOV 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000722954800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Chen, H
   Han, Q
   Li, Q
   Tong, XJ
AF Chen, Hao
   Han, Qi
   Li, Qiong
   Tong, Xiaojun
TI A novel general blind detection model for image forensics based on DNN
SO VISUAL COMPUTER
LA English
DT Article
DE Image forensics; Steganalysis; Object detection; Deep residual network
ID STEGANALYSIS
AB Image steganography and image tampering usually produce weak characteristic signals that are different from the natural information of the image. Aiming at the unnatural features in images, this paper proposes a blind detection model for image forensics based on weak feature extraction. The model extracts weak image features from three aspects: the spatial domain, the JPEG domain, and the natural characteristics. It has the characteristics of a wide detection range and a good detection accuracy. The model can perform general blind detection for image spatial domain steganography, image JPEG domain steganography, image copy-move, image splicing, and image removal. It is mainly composed of four types of artificial neural network modules, two feature classification networks, and a target regression network. The model combines the multi-layer convolution RoI feature extraction method and uses three deep residual networks and RPN to extract weak features of the image. The model integrates the features extracted by the four networks and makes the final judgment on the image detection through two feature classification networks and a target regression network. We tested the image steganography algorithms in the typical spatial domain and JPEG domain, and image content tampering operations such as image copy-move, splicing, and removal. At the same time, we made a double image dataset containing tampering and steganography and tested the model's ability to detect the double image dataset. The experimental results show that the model has a relatively ideal detection effect on the typical algorithms tested. The model can also detect the mixed steganography and tampering information in the double image dataset.
C1 [Chen, Hao; Han, Qi; Li, Qiong; Tong, Xiaojun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Han, Q (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM chenhaoxv@126.com; qi.han@hit.edu.cn; qiong.li@hit.edu.cn;
   tong_xiaojun@163.com
RI Rakha, Hesham Ahmed/C-6231-2015
OI Rakha, Hesham Ahmed/0000-0002-5845-2929
FU National Natural Science Foundation of China [61771168]
FX This work was supported by the National Natural Science Foundation of
   China [Grant Numbers 61771168]. The authors would like to thank the
   Institute of Information Countermeasures Technology providing deep
   learning servers.
CR [Anonymous], 2001, INF HID 4 INT WORKSH, DOI 10.1007/3-540-
   Avcibas I, 2001, PROC SPIE, V4314, P523, DOI 10.1117/12.435436
   Avcibas Ismail, 2005, EURASIP J ADV SIG PR, V2005, P1
   Bappy JH, 2019, IEEE T IMAGE PROCESS, V28, P3286, DOI 10.1109/TIP.2019.2895466
   Bappy JH, 2017, IEEE I CONF COMP VIS, P4980, DOI 10.1109/ICCV.2017.532
   Bas P., 2011, 13 INT C IH 2011 PRA
   Bi Y, 2019, IEEE GLOB COMM CONF
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Bunk J, 2017, IEEE COMPUT SOC CONF, P1881, DOI 10.1109/CVPRW.2017.235
   Chen X., 2021, IMAGE MANIPULATION D
   Dong J., 2010, Casia image tampering detection evaluation database
   Dong J., 2013, CASIA image tampering detection evaluation database
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Furon T, 2008, EURASIP J INF SECUR, DOI 10.1155/2008/597040
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   Guo LJ, 2014, IEEE T INF FOREN SEC, V9, P814, DOI 10.1109/TIFS.2014.2312817
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hu Xuefeng, 2020, SPAN SPATIAL PYRAMID
   Joseph A, 2020, VISUAL COMPUT, V36, P529, DOI 10.1007/s00371-019-01628-3
   Li B., 2015, 2014 IEEE INT C IM P
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu X., 2021, PSCC NET PROGRESSIVE
   Lyu S, 2003, LECT NOTES COMPUT SC, V2578, P340
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Park J, 2018, LECT NOTES COMPUT SC, V11209, P656, DOI 10.1007/978-3-030-01228-1_39
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Provos N, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 10TH USENIX SECURITY SYMPOSIUM, P323
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sallee P, 2004, LECT NOTES COMPUT SC, V2939, P154
   Sedighi V., 2015, IEEE T INF FOREN SEC, V11, P1
   Wei XY, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11101223
   Xu G., 2016, P 4 ACM WORKSH INF H, P103, DOI DOI 10.1145/2909827.2930798
   Xu GS, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P67, DOI 10.1145/3082031.3083236
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhong Z., 2016, IEEE INT C AC
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
NR 43
TC 2
Z9 2
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 27
EP 42
DI 10.1007/s00371-021-02310-3
EA OCT 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000706056900001
DA 2024-07-18
ER

PT J
AU Sarkar, D
   Gunturi, SK
AF Sarkar, Dipu
   Gunturi, Sravan Kumar
TI Online health status monitoring of high voltage insulators using deep
   learning model
SO VISUAL COMPUTER
LA English
DT Article
DE Health monitoring; YOLOv3; SRCNN
AB The inspection of electrical components has long been an important issue in the power distribution system. Unmanned drones are impressive surveillance systems with a powerful spatial and remote sensing capability. This paper proposes a system to monitor the health of the ceramic insulators that uses aerial images as a source of information and deep structured learning model for the data interpretation. The key drawbacks of existing monitoring systems are poor detection accuracy and lack of real-time execution, making it more complicated to obtain attributes from aerial photographs. The focus of this paper is to increase accuracy of detection while operating in real-time using You Only Look Once version 3 (YOLOv3). The novelty of the proposed system is that it combines deep learning and the Internet of Things using a single embedded device called Raspberry Pi. For the scientific investigation, we equipped Raspberry Pi with a test image as an input to detect an insulator's health status using YOLOv3. Many aerial images are not clear due to motion blur. Excluding such low-resolution training images will affect accuracy. So we used a super-resolution CNN to reconstruct a blurred image as high-resolution image. The efficiency of the proposed system has been tested using a private data set consisting of a variety of scenes containing high-voltage power line insulators. The results show that the suggested system is quick and accurate in the identification and classification of insulators.
C1 [Sarkar, Dipu] Natl Inst Technol Nagaland, Dept Elect & Elect Engn, Dimapur 797103, India.
   [Gunturi, Sravan Kumar] Natl Inst Technol Nagaland, Dept Elect & Instrumentat Engn, Dimapur 797103, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Nagaland; National Institute of Technology (NIT System);
   National Institute of Technology Nagaland
RP Sarkar, D (corresponding author), Natl Inst Technol Nagaland, Dept Elect & Elect Engn, Dimapur 797103, India.
EM dipusarkar5@rediffmail.com
RI gunturi, sravan Kumar/B-8443-2019
CR Anjum S, 2017, IEEE T DIELECT EL IN, V24, P183, DOI 10.1109/TDEI.2016.005867
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Cruz-Roa A, 2014, PROC SPIE, V9041, DOI 10.1117/12.2043872
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gencoglu MT, 2009, EXPERT SYST APPL, V36, P7338, DOI 10.1016/j.eswa.2008.11.008
   Gencoglu MT, 2009, EXPERT SYST APPL, V36, P10789, DOI 10.1016/j.eswa.2009.02.021
   Ghenescu Veta., 2018, 2018 International Symposium on Electronics and Telecommunications (ISETC), P1, DOI DOI 10.1109/ROLCG.2018.8572026
   Iwai K, 1998, IEEE T POWER DELIVER, V13, P1412, DOI 10.1109/61.714516
   Khan S, 2019, PATTERN RECOGN LETT, V125, P1, DOI 10.1016/j.patrec.2019.03.022
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li XF, 2020, IEEE ACCESS, V8, P59934, DOI 10.1109/ACCESS.2020.2982288
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YC, 2021, VISUAL COMPUT, V37, P1769, DOI 10.1007/s00371-020-01937-y
   Lopez AR, 2017, 2017 13TH IASTED INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING (BIOMED), P49, DOI 10.2316/P.2017.852-053
   Malon Christopher D, 2013, J Pathol Inform, V4, P9, DOI 10.4103/2153-3539.112694
   Miao XR, 2019, IEEE ACCESS, V7, P9945, DOI 10.1109/ACCESS.2019.2891123
   Montoya G, 2004, IEE P-GENER TRANSM D, V151, P334, DOI 10.1049/ip-gtd:20040225
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Nasr-Esfahani E, 2016, IEEE ENG MED BIO, P1373, DOI 10.1109/EMBC.2016.7590963
   Ouyang XG, 2018, IEEE T DIELECT EL IN, V25, P263, DOI 10.1109/TDEI.2018.006662
   Rao C, 2019, LECT NOTES COMPUT SC, V11414, P219, DOI 10.1007/978-3-030-14085-4_18
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ruder S., ARXIV160904747
   Sadykova D, 2020, IEEE T POWER DELIVER, V35, P1599, DOI 10.1109/TPWRD.2019.2944741
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   VAILLANCOURT GH, 1994, IEEE T POWER DELIVER, V9, P208, DOI 10.1109/61.277692
   Nguyen VN, 2018, INT J ELEC POWER, V99, P107, DOI 10.1016/j.ijepes.2017.12.016
   Vasudev N, 2019, INT C HIGH VOLT ENG, P1
   Wong KL, 2004, IEEE T DIELECT EL IN, V11, P1057, DOI 10.1109/TDEI.2004.1387829
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yang L, 2019, IET SCI MEAS TECHNOL, V13, P131, DOI 10.1049/iet-smt.2018.5069
   Zhao J., 2018, INT C HUM CTR COMP, P347
   Zhong D., 2019, 2019 INT C COMP NETW
NR 37
TC 6
Z9 6
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4457
EP 4468
DI 10.1007/s00371-021-02308-x
EA SEP 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000700932100001
DA 2024-07-18
ER

PT J
AU Markovic, V
   Jakovljevic, Z
   Budak, I
AF Markovic, Veljko
   Jakovljevic, Zivana
   Budak, Igor
TI Automatic recognition of cylinders and planes from unstructured point
   clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Reverse engineering; 3D point cloud processing; Cylinders recognition;
   Planes recognition
ID MESH SEGMENTATION; RECONSTRUCTION; EXTRACTION; QUADRICS; MODEL
AB 3D scanning devices are traditionally employed in reverse engineering tasks that can be carried out semi-automatically, with user assistance. However, their application in manufacturing process control requires automatic point cloud segmentation and extraction of geometric primitives. In this paper, we propose a method for automatic recognition of planes and cylinders (most frequently encountered geometric primitives in mechanical engineering) from unstructured point clouds. The method is based on the scatter of data during least squares fitting of second order surfaces. It consists of three phases and the first phase represents automatic point cloud segmentation. The second phase deals with merging of over-segmented regions and surfaces parameters estimation, whereas the final phase provides extraction of recognized geometric primitives. The method is experimentally verified using three real-world case studies, and its performances are compared with two state of the art recognition algorithms. The results have shown that the proposed method outperforms alternative approaches in terms of appropriately recognized planes and cylinders without surface type confusion, as well as when the recognition of non-existent primitives is considered. In addition, the method determines surfaces parameters with high accuracy.
C1 [Markovic, Veljko; Jakovljevic, Zivana] Univ Belgrade, Fac Mech Engn, Belgrade, Serbia.
   [Budak, Igor] Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.
C3 University of Belgrade; University of Novi Sad
RP Markovic, V (corresponding author), Univ Belgrade, Fac Mech Engn, Belgrade, Serbia.
EM markovicveljko@yahoo.com; zjakovljevic@mas.bg.ac.rs; budaki@uns.ac.rs
RI Jakovljevic, Zivana/R-6611-2019
OI Jakovljevic, Zivana/0000-0002-7878-2909; Markovic,
   Veljko/0000-0003-2873-2150; Budak, Igor/0000-0001-9548-181X
FU Science Fund of the Republic of Serbia [6523109]; Ministry of Education,
   Science and Technological Development of the Serbian Government
   [451-03-68/2020-14/200105]; AI-MISSION4.0, 2020-2022
FX This research was supported by the Science Fund of the Republic of
   Serbia, Grant No. 6523109, AI-MISSION4.0, 2020-2022, as well as by the
   Ministry of Education, Science and Technological Development of the
   Serbian Government, Grant No. 451-03-68/2020-14/200105.
CR Al-Sharadqah A, 2009, ELECTRON J STAT, V3, P886, DOI 10.1214/09-EJS419
   [Anonymous], 2020, CLOUDCOMPARE 3D POIN
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Benko P, 2001, COMPUT AIDED DESIGN, V33, P839, DOI 10.1016/S0010-4485(01)00100-2
   Chaperon T., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P35
   Chen MY, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105508
   Chen MY, 2019, OPT LASER ENG, V122, P170, DOI 10.1016/j.optlaseng.2019.06.011
   Di Angelo L, 2019, COMPUT AIDED DESIGN, V110, P78, DOI 10.1016/j.cad.2019.01.003
   Di Angelo L, 2015, COMPUT AIDED DESIGN, V62, P44, DOI 10.1016/j.cad.2014.09.006
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Hubert M, 2005, TECHNOMETRICS, V47, P64, DOI 10.1198/004017004000000563
   Hwang S, 2020, INT J PR ENG MAN-GT, V7, P699, DOI 10.1007/s40684-020-00192-9
   Jakovljevic, 2015, P 8 INT WORK C TOT Q, P353
   Jakovljevic Z, 2016, PROC CIRP, V57, P292
   Jakovljevic Z, 2015, IEEE T IND INFORM, V11, P342, DOI 10.1109/TII.2015.2389195
   Khalid MU, 2019, IEEE INT CON AUTO SC, P1138, DOI [10.1109/COASE.2019.8843050, 10.1109/coase.2019.8843050]
   Koch S, 2019, PROC CVPR IEEE, P9593, DOI 10.1109/CVPR.2019.00983
   Kwon SW, 2004, AUTOMAT CONSTR, V13, P67, DOI 10.1016/j.autcon.2003.08.007
   Lai HC, 2009, ADV ENG SOFTW, V40, P1011, DOI 10.1016/j.advengsoft.2009.03.017
   Lalonde J., 2006, CMURI-TR-06-21
   Li C, 2021, IEEE SENS J, V21, P8501, DOI 10.1109/JSEN.2019.2910826
   Li Y, 2020, IEEE T GEOSCI REMOTE, V58, P3588, DOI 10.1109/TGRS.2019.2958517
   Liu Y, 2019, INT J ADV MANUF TECH, V105, P1295, DOI 10.1007/s00170-019-04344-3
   Liu YJ, 2013, IEEE T VIS COMPUT GR, V19, P1700, DOI 10.1109/TVCG.2013.74
   Markovic, 2016, J PROD ENG, V19, P65
   Markovic V, 2019, TEH VJESN, V26, P985, DOI 10.17559/TV-20180328175336
   Morel J, 2020, VISUAL COMPUT, V36, P2419, DOI 10.1007/s00371-020-01966-7
   Mukhopadhyay P, 2015, PATTERN RECOGN, V48, P993, DOI 10.1016/j.patcog.2014.08.027
   Nagy B, 2019, IEEE SENS J, V19, P10034, DOI 10.1109/JSEN.2019.2927269
   Nurunnabi A, 2017, INT ARCH PHOTOGRAMM, V42-1, P63, DOI 10.5194/isprs-archives-XLII-1-W1-63-2017
   OGUNDANA O, 2011, OPT ENG, V50, P1
   Ogundana OO, 2007, OPT ENG, V46, DOI 10.1117/1.2739011
   Oh I, 2021, VISUAL COMPUT, V37, P1385, DOI 10.1007/s00371-020-01872-y
   Patil AK, 2017, AUTOMAT CONSTR, V75, P65, DOI 10.1016/j.autcon.2016.12.002
   Petitjean S, 2002, ACM COMPUT SURV, V34, P211, DOI 10.1145/508352.508354
   Qi CR, 2017, ADV NEUR IN, V30
   RABBANI T., 2006, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V36, P248
   Rabbani T., 2005, ISPRS WG 3 3 3 4 5 3
   Reza A, 2017, APPL MATH COMPUT, V314, P349, DOI 10.1016/j.amc.2017.07.025
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Sharma G., 2021, PARSENET PARAMETRIC
   Sharma Gopal, 2020, COMPUTER VISION ECCV, P261, DOI DOI 10.1007/978-3-030-58571-6_16
   Tang YC, 2020, ADV CIV ENG, V2020, DOI 10.1155/2020/1236021
   Tang YC, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00510
   Tang YC, 2019, ROBOT CIM-INT MANUF, V59, P36, DOI 10.1016/j.rcim.2019.03.001
   Tran TT, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0076-1
   Tran TT, 2016, VISUAL COMPUT, V32, P1205, DOI 10.1007/s00371-015-1157-0
   Tran TT, 2015, COMPUT GRAPH-UK, V46, P345, DOI 10.1016/j.cag.2014.09.027
   Tsuchie S, 2022, VISUAL COMPUT, V38, P493, DOI 10.1007/s00371-020-02030-0
   Vieira M, 2005, COMPUT AIDED GEOM D, V22, P771, DOI 10.1016/j.cagd.2005.03.006
   Wang J, 2012, COMPUT IND ENG, V63, P1189, DOI 10.1016/j.cie.2012.07.009
   Wang JL, 2021, IEEE T IND INFORM, V17, P7913, DOI 10.1109/TII.2020.3044106
   Wang WC, 2020, INT J ADV MANUF TECH, V106, P1553, DOI 10.1007/s00170-019-04779-8
   Winiwarter L, 2019, PFG-J PHOTOGRAMM REM, V87, P75, DOI 10.1007/s41064-019-00073-0
   Xia SB, 2020, IEEE J-STARS, V13, P685, DOI 10.1109/JSTARS.2020.2969119
   Xu YJ, 2020, IEEE ACCESS, V8, P70262, DOI 10.1109/ACCESS.2020.2978506
   Yan MZ, 2020, J MANUF PROCESS, V53, P396, DOI 10.1016/j.jmapro.2020.03.034
   Ying XH, 2012, INT C PATT RECOG, P3228
NR 59
TC 2
Z9 2
U1 5
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4329
EP 4352
DI 10.1007/s00371-021-02299-9
EA SEP 2021
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000697092400001
DA 2024-07-18
ER

PT J
AU Cha, IH
   Ko, HS
AF Cha, Ick-Hoon
   Ko, Hyeong-Seok
TI BLI-resolver: resolving the boundary-loop-interior type intersections
   for clothing simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Clothing simulation; Discrete collision handling; Intersection;
   Untangling
AB In triangular mesh-based clothing simulation, collision failure manifests itself in terms of mesh-to-mesh intersections, which can be classified into seven types. Since convergent techniques are available for the other six types of intersections, this paper focuses on the BLI (tokened from boundary, loop vertex, interior) intersection type. The paper analyzes how BLIs occur, and notes that the desired form of resolution (i.e., resolution style) can vary depending on the type or particular region of the garment. We identify the need for three resolution algorithms for BLI, namely Mesh-Tearing, Regional-Flip, Crease-Flip, in order to cover the resolution styles. This is the first work that (1) identified the need for the resolution styles for the case of BLI, (2) proposed the actual algorithms to cover each resolution style, and (3) demonstrated the proposed resolution styles and algorithms work stably for BLIs.
C1 [Cha, Ick-Hoon; Ko, Hyeong-Seok] Seoul Natl Univ, Seoul, South Korea.
C3 Seoul National University (SNU)
RP Cha, IH (corresponding author), Seoul Natl Univ, Seoul, South Korea.
EM ckdlrgns@gmail.com; hsko@graphics.snu.ac.kr
OI Cha, Ick-Hoon/0000-0002-0497-2486
FU National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT [2018M3E3A1057288]; Brain of Korea 21 Project in 2020; ASRI
   (Automation and Systems Research Institute at Seoul National University)
FX This research was supported by R&D program for Advanced
   Integrated-intelligence for Identification (AIID) through the National
   Research Foundation ofKorea (NRF) funded by Ministry of Science and ICT
   (2018M3E3A1057288), the Brain of Korea 21 Project in 2020, and ASRI
   (Automation and Systems Research Institute at Seoul National
   University).
CR Baraff D, 2003, ACM T GRAPHIC, V22, P862, DOI 10.1145/882262.882357
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Buffet T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323010
   Cha IH, 2022, IEEE T VIS COMPUT GR, V28, P2764, DOI 10.1109/TVCG.2020.3039566
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Han DH, 2019, PROCEEDINGS OF THE 32ND INTERNATIONAL CONFERENCE ON COMPUTER ANIMATION AND SOCIAL AGENTS (CASA 2019), P53, DOI 10.1145/3328756.3328759
   Hu SX, 2017, INT J CLOTH SCI TECH, V29, P25, DOI 10.1108/IJCST-06-2015-0068
   Volino P, 2006, ACM T GRAPHIC, V25, P1154, DOI 10.1145/1141911.1142007
   Wicke Martin., 2006, Proc. of Vision, Modeling, P349
   Ye J., 2012, P ACM SIG GRAPH EUR, P311
   Ye JT, 2017, COMPUT GRAPH FORUM, V36, P217, DOI 10.1111/cgf.13287
   Zhong YQ, 2009, TEXT RES J, V79, P815, DOI [10.1177/004051750809561, 10.1177/0040517508095611]
NR 12
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1359
EP 1368
DI 10.1007/s00371-021-02279-z
EA SEP 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000692452300001
DA 2024-07-18
ER

PT J
AU Wang, XY
   Su, YN
   Zhang, H
   Zou, CY
AF Wang, Xingyuan
   Su, Yining
   Zhang, Hao
   Zou, Chengye
TI A new hybrid image encryption algorithm based on Gray code
   transformation and snake-like diffusion
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Lorenz system; Gray code; Snake-like diffusion
ID SEMI-TENSOR PRODUCT; MATRIX; SCHEME; BREAKING; CIPHER; SYSTEM
AB Because Gray code has good spatial traversal and easy to generate binary conversion, this paper proposes an image encryption algorithm based on Gray code scrambling. Firstly, the Lorenz system is used to generate chaotic sequences for binary and Gray code transformation to achieve global scrambling. Different from the traditional row (column) XOR method, this paper uses the snake shape to diffuse the row (column) XOR. The lines of the scrambled image are first XORed from left to right in a serpentine order, and the row elements are cyclically shifted. Then, the columns of the image are XORed from right to left in a serpentine order, and the column elements are cyclically shifted. Finally, the scrambled image is further diffused. The comparative experiment shows that the algorithm has good encryption effect. Not only is it well resistant to differential cryptanalysis and exhaustive attack, but it also improves key sensitivity.
C1 [Wang, Xingyuan; Su, Yining] Dalian Maritime Univ, Fac Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Zhang, Hao] Taiyuan Univ Technol, Coll Informat & Comp, Jinzhong 030600, Peoples R China.
   [Zou, Chengye] Anyang Normal Univ, Sch Math & Stat, Anyang 455000, Peoples R China.
C3 Dalian Maritime University; Taiyuan University of Technology; Anyang
   Normal University
RP Wang, XY (corresponding author), Dalian Maritime Univ, Fac Informat Sci & Technol, Dalian 116026, Peoples R China.
EM xywang@dlmu.edu.cn
RI Wang, Xing-yuan/I-6353-2015
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key RAMP;D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 universities'
   Funding Projects Introducing Innovation Team Program [2019GXRC031];
   Research Fund of Guangxi Key Lab of Multi-source Information Mining
   Security [MIMS20-M-02]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103), Jinan City '20 universities' Funding Projects
   Introducing Innovation Team Program (No: 2019GXRC031), Research Fund of
   Guangxi Key Lab of Multi-source Information Mining & Security (No:
   MIMS20-M-02).
CR Al-Hazaimeh OM, 2019, NEURAL COMPUT APPL, V31, P2395, DOI 10.1007/s00521-017-3195-1
   Aqeel-ur-Rehman, 2018, OPTIK, V153, P117, DOI 10.1016/j.ijleo.2017.09.099
   Artiles JAP, 2019, SIGNAL PROCESS-IMAGE, V79, P24, DOI 10.1016/j.image.2019.08.014
   Ben Farah MA, 2020, NONLINEAR DYNAM, V99, P3041, DOI 10.1007/s11071-019-05413-8
   Ben Slimane N, 2018, MULTIMED TOOLS APPL, V77, P30993, DOI 10.1007/s11042-018-6145-8
   Chen JX, 2015, OPT LASER ENG, V67, P191, DOI 10.1016/j.optlaseng.2014.11.017
   Ding Wen Xia, 2007, Journal of China Institute of Communications, V28, P34
   Faragallah OS, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106333
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Guesmi R, 2021, MULTIMED TOOLS APPL, V80, P1925, DOI 10.1007/s11042-020-09672-1
   Hanis S, 2019, NONLINEAR DYNAM, V95, P421, DOI 10.1007/s11071-018-4573-7
   Lang J, 2010, OPT COMMUN, V283, P2092, DOI 10.1016/j.optcom.2010.01.060
   Li CQ, 2011, COMMUN NONLINEAR SCI, V16, P837, DOI 10.1016/j.cnsns.2010.05.008
   Li JF, 2018, OPT LASER ENG, V102, P170, DOI 10.1016/j.optlaseng.2017.11.001
   Li S., 2004, GEN CRYPTANALYSIS PE, V374, P2004
   Li Z, 2018, NONLINEAR DYNAM, V94, P1319, DOI 10.1007/s11071-018-4426-4
   Liu XB, 2021, QUANTUM INF PROCESS, V20, DOI 10.1007/s11128-020-02952-7
   Liu Y, 2016, MULTIMED TOOLS APPL, V75, P4363, DOI 10.1007/s11042-015-2479-7
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Midoun MA, 2021, OPT LASER ENG, V139, DOI 10.1016/j.optlaseng.2020.106485
   Pak C, 2019, MULTIMED TOOLS APPL, V78, P12027, DOI 10.1007/s11042-018-6739-1
   Patro KAK, 2020, J INF SECUR APPL, V52, DOI 10.1016/j.jisa.2020.102470
   Raza SF, 2019, NONLINEAR DYNAM, V95, P859, DOI 10.1007/s11071-018-4600-8
   Rehman AU, 2015, MULTIMED TOOLS APPL, V74, P4655, DOI 10.1007/s11042-013-1828-7
   Rhouma R, 2010, COMMUN NONLINEAR SCI, V15, P1887, DOI 10.1016/j.cnsns.2009.07.007
   Sahasrabuddhe A, 2021, INFORM SCIENCES, V550, P252, DOI 10.1016/j.ins.2020.10.031
   Wang K, 2005, PHYS LETT A, V343, P432, DOI 10.1016/j.physleta.2005.05.040
   Wang T, 2020, OPT LASER TECHNOL, V132, DOI 10.1016/j.optlastec.2020.106355
   Wang XY, 2015, OPT LASER ENG, V68, P126, DOI 10.1016/j.optlaseng.2014.12.025
   Wang XY, 2021, INFORM SCIENCES, V569, P217, DOI 10.1016/j.ins.2021.04.013
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2019, INFORM SCIENCES, V486, P340, DOI 10.1016/j.ins.2019.02.049
   Wang XY, 2015, NONLINEAR DYNAM, V79, P1141, DOI 10.1007/s11071-014-1729-y
   Wang XY, 2011, OPT COMMUN, V284, P5804, DOI 10.1016/j.optcom.2011.08.053
   Wu XJ, 2018, MULTIMED TOOLS APPL, V77, P12349, DOI 10.1007/s11042-017-4885-5
   Wu XJ, 2018, SIGNAL PROCESS, V148, P272, DOI 10.1016/j.sigpro.2018.02.028
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Xu L, 2016, OPT LASER ENG, V78, P17, DOI 10.1016/j.optlaseng.2015.09.007
   Xu M, 2019, INFORM SCIENCES, V478, P1, DOI 10.1016/j.ins.2018.11.010
   Ye GD, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78127-2
   Zhang YS, 2014, NONLINEAR DYNAM, V76, P1645, DOI 10.1007/s11071-014-1235-2
   Zhao FZ, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.4.043011
   Zhou NR, 2018, QUANTUM INF PROCESS, V17, DOI 10.1007/s11128-018-1902-1
   Zhou YC, 2013, IEEE T CYBERNETICS, V43, P515, DOI 10.1109/TSMCB.2012.2210706
NR 46
TC 10
Z9 10
U1 9
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3831
EP 3852
DI 10.1007/s00371-021-02224-0
EA JUL 2021
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000679625200003
DA 2024-07-18
ER

PT J
AU Hossein-Nejad, Z
   Nasri, M
AF Hossein-Nejad, Zahra
   Nasri, Mehdi
TI Clustered redundant keypoint elimination method for image mosaicing
   using a new Gaussian-weighted blending algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Mosaicing process; Image stitching; Image registration; Blending method;
   SIFT; RKEM
ID VIDEO; SIFT; SURF
AB In this paper, a new method for image mosaicing (image stitching) is introduced based on Scale Invariant Feature transform (SIFT). One of the main drawbacks of SIFT is the redundancy of the extracted keypoints, which leads to lower image mosaicing quality. Recently, a new method called Redundant Keypoint Elimination (RKEM) was presented to remove these redundant features, and enhance image registration performance. Despite the applicability of RKEM, its threshold value is considered the same in all parts of the image. This characteristic leads to inappropriate removal of keypoints due to the fact that distribution of keypoints in the high-detailed region is denser than the low-detailed ones. This paper proposes a new method to improve RKEM called Clustered RKEM (CRKEM) which is based on keypoints distribution. Moreover, in this paper a new blending algorithm is proposed based on a Gaussian-weighted function. In the proposed blending method, the Gaussian function is proposed based on the mean and variance of the pixels in the overlapped region of images to be mosiaced. In comparison with the classical methods, the experimental results confirm the superiority of the proposed method in image mosaicing as well as to image registration and matching.
C1 [Hossein-Nejad, Zahra] Islamic Azad Univ, Dept Elect Engn, Sirjan Branch, Sirjan, Iran.
   [Nasri, Mehdi] Islamic Azad Univ, Dept Elect Engn, Khomeinishahr Branch, Khomeinishahr, Iran.
C3 Islamic Azad University; Islamic Azad University
RP Nasri, M (corresponding author), Islamic Azad Univ, Dept Elect Engn, Khomeinishahr Branch, Khomeinishahr, Iran.
EM hoseinnejad.zahra@yahoo.com; nasri_me@iaukhsh.ac.ir
RI Nasri, Mehdi/C-2071-2016
OI Nasri, Mehdi/0000-0002-9254-3584
CR Adel E., 2014, International Journal of Computer Applications, V99, P1, DOI DOI 10.5120/17374-7818
   Ai YT, 2020, IEEE ACCESS, V8, P181526, DOI 10.1109/ACCESS.2020.2995153
   [Anonymous], 2014, INT J INNOVATIVE RES
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhosle U, 2002, IETE J RES, V48, P317, DOI 10.1080/03772063.2002.11416292
   Bhowmik A, 2020, PROC CVPR IEEE, P4947, DOI 10.1109/CVPR42600.2020.00500
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Cheung W, 2007, I S BIOMED IMAGING, P720, DOI 10.1109/ISBI.2007.356953
   Chipman LJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC248
   Choi YH, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P74, DOI 10.1109/ICCE.2002.1013933
   Derpanis KG, 2004, HARRIS CORNER DETECT, V2
   Deshmukh P., 2019, 2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS), P1
   Ghosh D, 2016, J VIS COMMUN IMAGE R, V34, P1, DOI 10.1016/j.jvcir.2015.10.014
   Gracias N, 2004, 5 IFAC S INT AUT VEH, P78
   Hemanth D.J., 2017, Deep Learning for Image Processing Applications
   Hossein-Nejad Z., 2019, CRYPTOGRAPHIC INFORM, P773
   Hossein-Nejad Z., 2020, J Mach Vision Image Process, V7, P165
   Hossein-Nejad Z, 2021, PATTERN ANAL APPL, V24, P669, DOI 10.1007/s10044-020-00938-w
   Hossein-Nejad Z, 2019, IRAN CONF ELECTR ENG, P1294, DOI [10.1109/iraniancee.2019.8786443, 10.1109/IranianCEE.2019.8786443]
   Hossein-Nejad Z, 2018, BIOMED SIGNAL PROCES, V45, P325, DOI 10.1016/j.bspc.2018.06.002
   Hossein-Nejad Z, 2017, COMPUT ELECTR ENG, V62, P524, DOI 10.1016/j.compeleceng.2016.11.034
   Hossein-Nejad Z, 2017, IET IMAGE PROCESS, V11, P273, DOI 10.1049/iet-ipr.2016.0440
   Hossein-Nejad Z, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P1087, DOI 10.1109/ICCSP.2016.7754318
   Hu R, 2007, IEEE INT CONF INF VI, P871
   IRANI M, 1995, SIGNAL PROCESS-IMAGE, V7, P529, DOI 10.1016/0923-5965(95)00022-1
   Jain M.P. M., 2013, International Journal of Computational Engineering Research, V3, P106
   Jingxin Hong, 2009, 2009 1st International Conference on Information Science and Engineering (ICISE 2009), P1287, DOI 10.1109/ICISE.2009.650
   Jinwei C., 2016, DESTECH T ENG TECHNO, DOI 10.12783/dtetr/ICMITE20162016/4576
   Joshi K, 2020, INT J MULTIMED INF R, V9, P231, DOI 10.1007/s13735-020-00200-3
   Kamboj A, 2022, VISUAL COMPUT, V38, P2383, DOI 10.1007/s00371-021-02119-0
   KAUR J, 2016, INDIAN J SCI TECHNOL
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kekec T, 2014, ROBOT AUTON SYST, V62, P1755, DOI 10.1016/j.robot.2014.07.010
   Khan HA, 2021, OR SURG OR MED OR PA, V131, P711, DOI 10.1016/j.oooo.2020.08.024
   Krishnakumar K, 2020, VISUAL COMPUT, V36, P1837, DOI 10.1007/s00371-019-01780-w
   Laraqui A, 2018, MULTIMED TOOLS APPL, V77, P7517, DOI 10.1007/s11042-017-4659-0
   Laraqui A, 2017, MULTIMED TOOLS APPL, V76, P8803, DOI 10.1007/s11042-016-3478-z
   Li AG, 2017, INT C INTEL HUM MACH, P415, DOI 10.1109/IHMSC.2017.205
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Lingua A, 2009, SENSORS-BASEL, V9, P3745, DOI 10.3390/s90503745
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lyu W, 2019, VIRTUAL REALITY INTE, V1, P55, DOI [10.3724/SP.J.2096-5796.2018.0008, DOI 10.3724/SP.J.2096-5796.2018.0008]
   Ma WP, 2017, IEEE GEOSCI REMOTE S, V14, P3, DOI 10.1109/LGRS.2016.2600858
   Mahesh V., 2012, IMAGE, V1, P40
   Mao LC, 2019, IEEE ACCESS, V7, P172231, DOI 10.1109/ACCESS.2019.2956508
   Mistry S., 2016, Int. Res. J. Eng. Technol. (IRJET), V3, P2220
   Monali R, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS IN ELECTRONICS, INFORMATION & COMMUNICATION TECHNOLOGY (RTEICT), P773, DOI 10.1109/RTEICT.2016.7807931
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Niu C, 2013, VISUAL COMPUT, V29, P253, DOI 10.1007/s00371-012-0763-3
   Okade M, 2014, IETE J RES, V60, P373, DOI 10.1080/03772063.2014.962627
   Pandey A., 2018, SOFT COMPUTING THEOR
   Peng Kang, 2011, 2011 International Conference on Multimedia Technology, P155
   Prathap KSV, 2016, INT CONF COMP COMMUN
   PRATHAP KSV, 2016, INDIAN J SCI TECHNOL, V9, pNI215
   Saha M., 2016, INT J-TORONTO, V6, P51
   Scheidegger F, 2021, VISUAL COMPUT, V37, P1593, DOI 10.1007/s00371-020-01922-5
   Sedaghat A, 2011, IEEE T GEOSCI REMOTE, V49, P4516, DOI 10.1109/TGRS.2011.2144607
   Sharma SK, 2020, J INDIAN SOC REMOTE, V48, P1389, DOI 10.1007/s12524-020-01163-y
   Shum HY, 2001, MG COMP SCI, P227
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tamimi H, 2006, ROBOT AUTON SYST, V54, P758, DOI 10.1016/j.robot.2006.04.018
   Tang HL, 2018, J MED IMAG HEALTH IN, V8, P240, DOI 10.1166/jmihi.2018.2283
   Tian F, 2014, 2014 7TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP 2014), P693, DOI 10.1109/CISP.2014.7003867
   Vaghela D, 2014, ARXIV PREPRINT ARXIV
   Vishwakarma A, 2020, MULTIMED TOOLS APPL, V79, P23599, DOI 10.1007/s11042-020-09124-w
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2020, MULTIMEDIA SYST, V26, P1, DOI [10.1007/s00530-019-00643-7, DOI 10.1007/S00530-019-00643-7]
   Yan WQ, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107541
   Yang AM, 2019, IEEE ACCESS, V7, P24204, DOI 10.1109/ACCESS.2019.2897131
   Yi Z, 2008, ELECTRON LETT, V44, P107, DOI 10.1049/el:20082477
   Yonghong J., 1998, Remote Sensing Technology and Application, V13, P46
   Zagrouba E, 2009, MACH VISION APPL, V20, P139, DOI 10.1007/s00138-007-0114-y
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Q, 2015, IMAGE VISION COMPUT, V36, P23, DOI 10.1016/j.imavis.2015.01.008
   Zhang T, 2020, IEEE ACCESS, V8, P163637, DOI 10.1109/ACCESS.2020.3020808
   Zhang WP, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0323-5
   Zhang XX, 2020, PROC SPIE, V11373, DOI 10.1117/12.2557189
   Zhang Y, 2019, VISUAL COMPUT, V35, P823, DOI 10.1007/s00371-019-01694-7
   Zhen YM, 2016, INT CONF INSTR MEAS, P202, DOI 10.1109/IMCCC.2016.145
NR 82
TC 7
Z9 7
U1 6
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1991
EP 2007
DI 10.1007/s00371-021-02261-9
EA JUL 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000674546600004
DA 2024-07-18
ER

PT J
AU Kim, H
   Dischler, JM
   Rushmeier, H
   Benes, B
AF Kim, Hansoo
   Dischler, Jean-Michel
   Rushmeier, Holly
   Benes, Bedrich
TI Edge-based procedural textures
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Procedural modeling; Image analysis
ID IMAGE SYNTHESIS
AB We introduce an edge-based procedural texture (EBPT), a procedural model for semi-stochastic texture generation. EBPT quickly generates large textures from a small input image. EBPT focuses on edges as the visually salient features extracted from the input image and organizes into groups with clearly established spatial properties. EBPT allows the users to interactively or automatically design new textures by utilizing the edge groups. The output texture can be significantly larger than the input, and EBPT does not need multiple textures to mimic the input. EBPT-based texture synthesis consists of two major steps, input analysis and texture synthesis. The input analysis stage extracts edges, builds the edge groups, and stores procedural properties. The texture synthesis stage distributes edge groups with affine transformation. This step can be done interactively or automatically using the procedural model. Then, it generates the output using edge group-based seamless image cloning. We demonstrate our method on various semi-stochastic inputs. With just a few input parameters defining the final structure, our method can analyze the input size of 512 x 512 in 0.7 s and synthesize the output texture of 2048 x 2048 pixels in 0.5 s.
C1 [Kim, Hansoo] Purdue Univ, W Lafayette, IN 47907 USA.
   [Benes, Bedrich] Purdue Univ, Technol, W Lafayette, IN 47907 USA.
   [Benes, Bedrich] Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
   [Dischler, Jean-Michel] Univ Strasbourg, Comp Sci, Strasbourg, France.
   [Rushmeier, Holly] Yale Univ, New Haven, CT USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; Purdue University System; Purdue University;
   Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Yale University
RP Kim, H (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM hansookim@google.com; dischler@unistra.fr; holly.rushmeier@yale.edu;
   bbenes@purdue.edu
RI Benes, Bedrich/A-8150-2016
OI Benes, Bedrich/0000-0002-5293-2112; Rushmeier, Holly/0000-0001-5241-0886
FU National Science Foundation [1608762, 10001387]; Functional
   Proceduralization of 3D Geometric Models; Inverse Procedural Material
   Modeling for Battery Design; Div Of Civil, Mechanical, & Manufact Inn;
   Directorate For Engineering [1608762] Funding Source: National Science
   Foundation
FX This research was funded by National Science Foundation GrantNo.
   10001387, Functional Proceduralization of 3D Geometric Models, and
   National Science Foundation Grant No. 1608762, Inverse Procedural
   Material Modeling for Battery Design.
CR Aliaga D.G., 2018, P ACM SIGGRAPH 2016, P1, DOI [DOI 10.1145/2897826.2927323, 10.1145/2897826.2927323]
   [Anonymous], 2017, ARXIV170602823
   [Anonymous], 2009, Proceedings of the 7th International Symposium on Non-Photorealistic Animation and Rendering (NPAR), DOI DOI 10.1145/1572614.1572623
   [Anonymous], 2016, ABS161200835 CORR
   [Anonymous], 2017, COMPUTER GRAPHICS FO
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   Barnes Connelly, 2017, [Computational Visual Media, 计算可视媒体], V3, P3
   BIEDERMAN I, 1988, COGNITIVE PSYCHOL, V20, P38, DOI 10.1016/0010-0285(88)90024-2
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Choi Y., 2017, arXiv
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Deng G, 2017, IEEE T IMAGE PROCESS, V26, P900, DOI 10.1109/TIP.2016.2633941
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Emilien A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766975
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI [DOI 10.1109/TEC.1961.5219197, 10.1109/TEC.1961.5219197]
   Galerne B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185569
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gilet G, 2010, COMPUT GRAPH FORUM, V29, P1411, DOI 10.1111/j.1467-8659.2010.01738.x
   Gilet G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661249
   Glondu L, 2012, COMPUT GRAPH FORUM, V31, P1547, DOI 10.1111/j.1467-8659.2012.03151.x
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guehl P, 2020, COMPUT GRAPH FORUM, V39, P159, DOI 10.1111/cgf.14061
   Guérin E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130804
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Hu SM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508381
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kendall W. S., 2013, Stochastic geometry and its applications
   Kingma D. P., 2013, ARXIV13126114
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lockerman YD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925964
   Lukác M, 2015, COMPUT GRAPH FORUM, V34, P257, DOI 10.1111/cgf.12764
   Lukác M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461956
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Mould David., 2005, P GRAPHICS INTERFACE, P219
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Radford A., 2015, ARXIV
   Sibbing D, 2010, COMPUT GRAPH FORUM, V29, P2135, DOI 10.1111/j.1467-8659.2010.01801.x
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   St'ava O, 2010, COMPUT GRAPH FORUM, V29, P665, DOI 10.1111/j.1467-8659.2009.01636.x
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Wei L. Y., 2009, EUROGRAPHICS 2009 ST, P93, DOI [DOI 10.2312/EGST.20091063, 10.2312/egst.20091063]
   Wu FZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601162
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Wu RB, 2014, IEEE T VIS COMPUT GR, V20, P436, DOI 10.1109/TVCG.2013.113
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zhu Jun-Yan, 2017, ARXIV171111586
NR 54
TC 2
Z9 3
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2595
EP 2606
DI 10.1007/s00371-021-02212-4
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000673536600002
DA 2024-07-18
ER

PT J
AU Kumawat, A
   Panda, S
AF Kumawat, Anchal
   Panda, Sucheta
TI A robust edge detection algorithm based on feature-based image
   registration (FBIR) using improved canny with fuzzy logic (ICWFL)
SO VISUAL COMPUTER
LA English
DT Article
DE Image registration; Image restoration; Image enhancement; Edge
   detection; Fuzzy logic
AB The problem of edge detection plays a crucial role in almost all research areas of image processing. If edges are detected accurately, one can detect the location of objects and the parameters such as shape and area can be measured more precisely. In order to overcome the above problem, a feature-based image registration (FBIR) method in combination with an improved version of canny with fuzzy logic is proposed for accurate detection of edges. The major contributions of the present work are summarized in three steps. In the first step, a restoration-based enhancement algorithm is proposed to get a fine image from a distorted noisy image. In the second step, two versions of input images are registered using a modified FBIR approach. In the third step, to overcome the drawback of canny edge detection algorithm, each step of the algorithm is modified. The output is then fed to a "fuzzy inference system". The "fuzzy rule-based technique", when applied to the problem of "edge detection", is very "efficient" because the thickness of the edges can be controlled by simply changing "rules and output parameters". The domain of the images under consideration is various well-known image databases such as Berkeley and USC-SIPI databases, whereas the proposed method is also suitable for other types of both indoor and outdoor images. The robustness of the proposed method is analysed, compared and evaluated with seven image assessment quality (IAQ) parameters. The performance of the proposed method is compared with some of the state-of-the-art edge detection methods in terms of the seven IAQ parameters.
C1 [Kumawat, Anchal; Panda, Sucheta] Veer Surendra Sai Univ Technol VSSUT, Dept Comp Applicat, Sambalpur 768018, Odisha, India.
C3 Veer Surendra Sai University of Technology
RP Kumawat, A (corresponding author), Veer Surendra Sai Univ Technol VSSUT, Dept Comp Applicat, Sambalpur 768018, Odisha, India.
EM akumawat_phdca@vssut.ac.in; suchetapanda_mca@vssut.ac.in
CR Abdullah-Al-Nahid, 2016, UKSIM EURO SYMP COMP, P64, DOI 10.1109/EMS.2016.20
   Aborisade DO., 2010, J COMPUT SCI TECHNOL, V10, P78
   Ahmad S, 2018, VISUAL COMPUT, V34, P21, DOI 10.1007/s00371-016-1307-z
   Alshennawy A.A., 2009, WORLD ACAD SCI ENG T, V5, P264
   Anas E, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P169, DOI 10.1109/SPIN.2016.7566682
   ARGYLE E, 1971, PR INST ELECTR ELECT, V59, P285, DOI 10.1109/PROC.1971.8136
   Banerjee A, 2022, VISUAL COMPUT, V38, P321, DOI 10.1007/s00371-020-02017-x
   Becerikli Y, 2005, LECT NOTES COMPUT SC, V3512, P943
   Bhardwaj S, 2012, PROC TECH, V4, P220, DOI 10.1016/j.protcy.2012.05.033
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chung IF, 2018, IEEE T FUZZY SYST, V26, P734, DOI 10.1109/TFUZZ.2017.2688358
   Dhivya R, 2019, CLUSTER COMPUT, V22, P11891, DOI 10.1007/s10586-017-1508-x
   ELKHAMY SE, 2000, P IEEE 17 NAT RAD SC, P1
   Gonzalez CI, 2019, J MULT-VALUED LOG S, V33, P431
   Gonzalez CI, 2016, APPL SOFT COMPUT, V47, P631, DOI 10.1016/j.asoc.2014.12.010
   Gonzalez CI, 2016, SOFT COMPUT, V20, P773, DOI 10.1007/s00500-014-1541-0
   Gonzalez R. C., 2002, DIGITAL IMAGE PROCES
   Heath M, 1998, COMPUT VIS IMAGE UND, V69, P38, DOI 10.1006/cviu.1997.0587
   Katiyar S K, 2014, IEEE Transactions on Geoscience & Remote Sensing, V50, P68
   Kaur, 2014, INT J COMPUT TRENDS, V3, P148
   Kumawat Anchal, 2018, Procedia Computer Science, V132, P277, DOI 10.1016/j.procs.2018.05.176
   Kumawat A., 2018, INT C COMP INT PATT
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Martínez GE, 2019, J IMAGING, V5, DOI 10.3390/jimaging5080071
   Mathur N, 2016, PROCEDIA COMPUT SCI, V93, P431, DOI 10.1016/j.procs.2016.07.230
   Meer P, 2001, IEEE T PATTERN ANAL, V23, P1351, DOI 10.1109/34.977560
   Melin P, 2014, IEEE T FUZZY SYST, V22, P1515, DOI 10.1109/TFUZZ.2013.2297159
   Mittal M, 2019, IEEE ACCESS, V7, P33240, DOI 10.1109/ACCESS.2019.2902579
   Moya-Albor E, 2017, ACTA POLYTECH HUNG, V14, P149, DOI 10.12700/APH.14.3.2017.3.9
   Ontiveros-Robles E, 2019, J MULT-VALUED LOG S, V33, P295
   PELI T, 1982, COMPUT VISION GRAPH, V20, P1, DOI 10.1016/0146-664X(82)90070-3
   Pugina E.V., 2017, 3 INT C INF TECHN NA
   Renjie Song, 2017, Pattern Recognition and Image Analysis, V27, P740
   Samant S., 2014, INT J SCI ENG RES, V5, P266
   Sharifi M, 2002, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P117, DOI 10.1109/ITCC.2002.1000371
   TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Xuan L, 2017, INT CONF SOFTW ENG, P275, DOI 10.1109/ICSESS.2017.8342913
   Zitova J.F., 2005, IMAGE REGISTRATION S
NR 39
TC 21
Z9 21
U1 5
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3681
EP 3702
DI 10.1007/s00371-021-02196-1
EA JUL 2021
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000673536600001
DA 2024-07-18
ER

PT J
AU Hu, QQ
   Wang, JD
   Liang, RY
AF Hu, Qianqian
   Wang, Jiadong
   Liang, Ruyi
TI Weighted local progressive-iterative approximation property for
   triangular Bezier surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Progressive-iterative approximation; Local format; Weight; Triangular
   Bezier surface; Bernstein basis function; Convergence rate
ID B-SPLINE CURVE; BASES
AB Progressive-iterative approximation (abbr. PIA) is an important and intuitive method for fitting and interpolating scattered data points. The triangular Bernstein basis with uniformly distributed parameters has the PIA property. For the sake of more flexibility, this paper presents a local progressive-iterative approximation (abbr. LPIA) format, which allows only a chosen subset of the initial control points to adjust and shows that the LPIA format is convergent for triangular Bezier surface of degree n <= 17 with uniform parameters. Furthermore, in order to accelerate the convergence rate, we develop a weighted LPIA format for triangular Bezier surfaces and prove that the weighted LPIA format has a faster convergence rate than the LPIA format when an optimal value of the weight is chosen. Finally, some numerical examples are presented to show the effectiveness of the LPIA method and the fast convergence of the weighted LPIA method.
C1 [Hu, Qianqian; Wang, Jiadong; Liang, Ruyi] Zhejiang Gongshang Univ, Sch Math & Stat, Hangzhou 310018, Peoples R China.
C3 Zhejiang Gongshang University
RP Hu, QQ (corresponding author), Zhejiang Gongshang Univ, Sch Math & Stat, Hangzhou 310018, Peoples R China.
EM qianqian_hu@163.com
RI Liang, Ruyi/GQH-7241-2022
OI Hu, Qianqian/0000-0002-0880-8173
FU National Natural Science Foundation of China [61772025, 61872316];
   Natural Science Foundation of Zhejiang Province, China [LY19F020004]
FX The authors thank the anonymous referees for their valuable suggestions
   and comments. This work is supported by the National Natural Science
   Foundation of China (Grant Nos 61772025, 61872316), the Natural Science
   Foundation of Zhejiang Province, China (No. LY19F020004).
CR Chen J, 2011, COMPUT AIDED DESIGN, V43, P889, DOI 10.1016/j.cad.2011.03.012
   de Boor C, 1979, ARO report 79-3, P299
   Delgado J, 2007, COMPUT AIDED GEOM D, V24, P10, DOI 10.1016/j.cagd.2006.10.001
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Ebrahimi A, 2019, J COMPUT APPL MATH, V359, P1, DOI 10.1016/j.cam.2019.03.025
   Farin G., 1987, Computer-Aided Geometric Design, V4, P329, DOI 10.1016/0167-8396(87)90007-0
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Hahmann S, 2000, COMPUT AIDED GEOM D, V17, P731, DOI 10.1016/S0167-8396(00)00021-2
   [胡倩倩 Hu Qianqian], 2020, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V32, P360
   Hu QQ, 2013, APPL MATH COMPUT, V219, P9308, DOI 10.1016/j.amc.2013.03.053
   Jaklic G, 2014, APPL MATH COMPUT, V227, P320, DOI 10.1016/j.amc.2013.11.034
   Lai MJ., 2007, SPLINE FUNCTIONS TRI, DOI [10.1017/CBO9780511721588, DOI 10.1017/CBO9780511721588]
   Lin HW, 2010, COMPUT AIDED GEOM D, V27, P322, DOI 10.1016/j.cagd.2010.01.003
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Liu CZ, 2020, J COMPUT APPL MATH, V366, DOI 10.1016/j.cam.2019.112389
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Qi D, 1975, Acta Math Sin, V18, P173
   [史利民 SHI Limin], 2006, [数学研究与评论, Journal of Mathematical Research and Exposition], V26, P735
   Varga R. S., 2000, Matrix Iterative Analysis, DOI [10.1007/978-3-642-05156-2, DOI 10.1007/978-3-642-05156-2]
   Xu, 2001, ORTHOGONAL POLYNOMIA
   Yu Zhao, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P239, DOI 10.1109/CAD/Graphics.2011.73
   Zhang L, 2018, J COMPUT APPL MATH, V329, P331, DOI 10.1016/j.cam.2017.05.034
   Zhang L, 2016, VISUAL COMPUT, V32, P1109, DOI 10.1007/s00371-015-1170-3
NR 24
TC 4
Z9 5
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3819
EP 3830
DI 10.1007/s00371-021-02223-1
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000672455800001
DA 2024-07-18
ER

PT J
AU Li, LY
   Tang, JS
   Shao, ZW
   Tan, X
   Ma, LZ
AF Li, Luying
   Tang, Junshu
   Shao, Zhiwen
   Tan, Xin
   Ma, Lizhuang
TI Sketch-to-photo face generation based on semantic consistency preserving
   and similar connected component refinement
SO VISUAL COMPUTER
LA English
DT Article
DE Sketch-to-photo face synthesis; Generative adversarial networks (GAN);
   Image generation; Image-to-image translation
AB Sketch-to-photo face generation has recently gained remarkable attention in computer vision and signal processing communities, because the sketches that employ concise lines are easily available and can describe significant facial attributes conveniently. Most existing sketch-to-photo works fail to maintain geometric structures and improve local details simultaneously, which limits their performance. In this work, we propose a two-stage sketch-to-photo generative adversarial network for face generation. In the first stage, we propose a semantic loss to maintain semantic consistency. In the second stage, we define the similar connected component and propose a color refinement loss to generate fine-grained details. Moreover, we introduce a multi-scale discriminator and design a patch-level local discriminator. We also propose a texture loss to enhance the local fidelity of synthesized images. Experiments show that our proposed method can significantly generate better results while preserving facial attributes than the state-of-the-art methods.
C1 [Li, Luying; Tang, Junshu; Tan, Xin; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Shao, Zhiwen] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.
   [Shao, Zhiwen] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.
   [Ma, Lizhuang] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
C3 Shanghai Jiao Tong University; China University of Mining & Technology;
   East China Normal University
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Shao, ZW (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.; Shao, ZW (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.; Ma, LZ (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM liluying@sjtu.edu.cn; tangjs@sjtu.edu.cn; zhiwen_shao@cumt.edu.cn;
   tanxin2017@sjtu.edu.cn; ma-lz@cs.sjtu.edu.cn
RI Tan, Xin/GRJ-0367-2022; Sun, Peng/KDO-4243-2024; Shao,
   Zhiwen/N-8985-2018
OI Tan, Xin/0000-0001-9346-1196; Shao, Zhiwen/0000-0002-9383-8384; Li,
   Luying/0000-0002-9188-1768
FU National Key Research and Development Program of China [2019YFC1521104];
   National Natural Science Foundation of China [61972157]; Fundamental
   Research Funds for the Central Universities [2021QN1072]
FX This work is supported by the National Key Research and Development
   Program of China (No. 2019YFC1521104), National Natural Science
   Foundation of China (No. 61972157), and Fundamental Research Funds for
   the Central Universities (No. 2021QN1072).
CR Baba T, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P504, DOI 10.1109/ACPR.2015.7486554
   Chang L, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2890-8
   Chao WT, 2019, IEEE IMAGE PROC, P4699, DOI [10.1109/ICIP.2019.8803549, 10.1109/icip.2019.8803549]
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Gao XN, 2008, IEEE T CIRC SYST VID, V18, P487, DOI 10.1109/TCSVT.2008.918770
   Gao XB, 2012, IEEE T CIRC SYST VID, V22, P1213, DOI 10.1109/TCSVT.2012.2198090
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I. J., 2014, ARXIV
   Gucluturk Yagmur, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P810, DOI 10.1007/978-3-319-46604-0_56
   Guo Q, 2017, IEEE IMAGE PROC, P2946, DOI 10.1109/ICIP.2017.8296822
   Hensel M, 2017, ADV NEUR IN, V30
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kazemi Hadi, 2018, 2018 International Conference of the Biometrics Special Interest Group (BIOSIG), P1
   Khan Mansoor, 2020, 2020 IEEE 8th International Conference on Photonics (ICP), P1, DOI 10.1109/ICP46580.2020.9206421
   Khan Muhammad Jaleed, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1097, DOI 10.1109/ICDAR.2019.00178
   Khan MJ, 2022, VISUAL COMPUT, V38, P509, DOI 10.1007/s00371-020-02031-z
   King DB, 2015, ACS SYM SER, V1214, P1
   Lee, 2019, ARXIV PREPRINT ARXIV
   Li HR, 2020, SOFT COMPUT, V24, P6851, DOI 10.1007/s00500-019-04324-5
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liang Chang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2146, DOI 10.1109/ICPR.2010.526
   Liang Y., 2012, P 2012 ASIA PACIFIC, P1
   Lin CY, 2016, 2016 IEEE SECOND INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P229, DOI 10.1109/BigMM.2016.23
   Lin Y, 2020, IEEE SIGNAL PROC LET, V27, P1095, DOI 10.1109/LSP.2020.3005039
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Nannan Wang, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P82, DOI 10.1109/ICIG.2011.112
   Osahor U, 2020, IEEE COMPUT SOC CONF, P3575, DOI 10.1109/CVPRW50498.2020.00418
   Peng CL, 2016, IEEE T NEUR NET LEAR, V27, P2201, DOI 10.1109/TNNLS.2015.2464681
   Quan Q, 2021, VISUAL COMPUT, V37, P245, DOI 10.1007/s00371-020-01796-7
   Salimans T, 2016, ADV NEUR IN, V29
   Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang XO, 2004, IEEE T CIRC SYST VID, V14, P50, DOI 10.1109/TCSVT.2003.818353
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia, 2019, ARXIV PREPRINT ARXIV
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xiao B, 2009, SIGNAL PROCESS, V89, P1576, DOI 10.1016/j.sigpro.2009.02.008
   Yang BX, 2020, LECT NOTES COMPUT SC, V11961, P790, DOI 10.1007/978-3-030-37731-1_64
   Yang Y, 2017, MULTIMED TOOLS APPL, V76, P523, DOI 10.1007/s11042-015-3063-x
   Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P6251, DOI 10.1109/TIP.2020.2990354
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yousaf A, 2020, EXPERT SYST, V37, DOI 10.1111/exsy.12503
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu J, 2021, IEEE T CYBERNETICS, V51, P4350, DOI 10.1109/TCYB.2020.2972944
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
   Zhang MJ, 2019, IEEE T IMAGE PROCESS, V28, P642, DOI 10.1109/TIP.2018.2869688
   Zhang SD, 2020, NEUROCOMPUTING, V410, P363, DOI 10.1016/j.neucom.2020.06.041
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang ZP, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON MATHEMATICS AND ARTIFICIAL INTELLIGENCE (ICMAI 2020), P187, DOI 10.1145/3395260.3395269
   Zhao TY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102926
   Zulfiqar M, 2019, 2019 International Conference on Electrical, Communication, and Computer Engineering (ICECCE), P1
NR 60
TC 9
Z9 8
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3577
EP 3594
DI 10.1007/s00371-021-02188-1
EA JUN 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000661817000001
DA 2024-07-18
ER

PT J
AU Azizi, V
   Usman, M
   Zhou, HL
   Faloutsos, P
   Kapadia, M
AF Azizi, Vahid
   Usman, Muhammad
   Zhou, Honglu
   Faloutsos, Petros
   Kapadia, Mubbasir
TI Graph-based generative representation learning of semantically and
   behaviorally augmented floorplans
SO VISUAL COMPUTER
LA English
DT Article
DE Floorplan representation; Floorplan generation; LSTM Variational
   autoencoder; Attributed graph; Design semantic features; Human
   behavioral features
ID SPACE ALLOCATION PROBLEM; LOCAL SEARCH TECHNIQUE; ARCHITECTURE
AB Floorplans are commonly used to represent the layout of buildings. Research works toward computational techniques that facilitate the design process, such as automated analysis and optimization, often using simple floorplan representations that ignore the space's semantics and do not consider usage-related analytics. We present a floorplan embedding technique that uses an attributed graph to model the floorplans' geometric information, design semantics, and behavioral features as the node and edge attributes. A long short-term memory (LSTM) variational autoencoder (VAE) architecture is proposed and trained to embed attributed graphs as vectors in a continuous space. A user study is conducted to evaluate the coupling of similar floorplans retrieved from the embedding space for a given input (e.g., design layout). The qualitative, quantitative, and user study evaluations show that our embedding framework produces meaningful and accurate vector representations for floorplans. Besides, our proposed model is generative. We studied and showcased its effectiveness for generating new floorplans. We also release the dataset that we have constructed. We include the design semantic attributes and simulation-generated human behavioral features for each floorplan in the dataset for further study in the community.
C1 [Azizi, Vahid; Zhou, Honglu; Kapadia, Mubbasir] Rutgers State Univ, Comp Sci Dept, Piscataway, NJ 08854 USA.
   [Usman, Muhammad; Faloutsos, Petros] York Univ, Elect Engn & Comp Sci Dept, Toronto, ON, Canada.
   [Faloutsos, Petros] UHN Toronto Rehabil Inst, Toronto, ON, Canada.
C3 Rutgers University System; Rutgers University New Brunswick; York
   University - Canada; University of Toronto; University Health Network
   Toronto
RP Azizi, V (corresponding author), Rutgers State Univ, Comp Sci Dept, Piscataway, NJ 08854 USA.
EM vahid.azizi@cs.rutgers.edu; usman@cse.yorku.ca; hz289@cs.rutgers.edu;
   pfal@cse.yorku.ca; mk1353@cs.rutgers.edu
OI Usman, Muhammad/0000-0003-2059-7206
FU Ontario Graduate Scholarship; NSF [IIS-1703883, IIS-1955404,
   IIS-1955365]; ISSUM
FX This research has been partially funded by grants from ISSUM, Ontario
   Graduate Scholarship, and in part by NSF awards: IIS-1703883,
   IIS-1955404, and IIS-1955365. The authors would also like to thank
   Mathew Schwartz for helping in editing and proofreading the manuscript.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2018, KDD DEEP LEARNING DA
   [Anonymous], P 18 INT C MACH LEAR
   Azizi V., 2020, P 11 ANN S SIMULATIO
   B HAWORTH., 2015, P 8 ACM SIGGRAPH C M, P91
   Bai Y., 2019, ARXIV PREPRINT ARXIV
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Berseth G, 2015, COMPUT ANIMAT VIRT W, V26, P377, DOI 10.1002/cav.1652
   Bowman S. R., 2015, GENERATING SENTENCES
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Chaillou SJHU., 2019, AI+ architecture: Towards a new approach
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   de las Heras L.-P., 2013, WORKSHOP GRAPHICS RE, P135
   Donath, 2007, LOOKING BACK FUTURE, P285
   Dutta A, 2011, PROC INT CONF DOC, P982, DOI 10.1109/ICDAR.2011.199
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Feng T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925894
   Gonzalez J.E., 2014, 11 USENIX S OP SYST, P599
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Haworth B, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1783
   Heylighen A, 2001, COMPUT AIDED DESIGN, V33, P1111, DOI 10.1016/S0010-4485(01)00055-0
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu R., 2020, ARXIV PREPRINT ARXIV
   Kapadia, 2020, HOUSEEXPO DATASET AU
   Kapadia, 2020, P EUR C COMP VIS ECC
   Kingma D. P., 2014, arXiv
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   Kumar P, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P830, DOI 10.1109/SC.2016.70
   Lambert G., 1995, Image Analysis and Processing. 8th International Conference, ICIAP '95. Proceedings, P347
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Meng, 2019, HOUSEEXPO LARGE SCAL
   Merrell P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866203
   Mousavi SF, 2017, PATTERN RECOGN, V61, P245, DOI 10.1016/j.patcog.2016.07.043
   Narayanan A., 2017, ARXIV PREPRINT ARXIV
   Nauata N., 2020, HOUSE GAN RELATIONAL
   Niepert M, 2016, PR MACH LEARN RES, V48
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Rodrigues E, 2013, COMPUT AIDED DESIGN, V45, P887, DOI 10.1016/j.cad.2013.01.001
   Sabri QU, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P50, DOI 10.5220/0006112800500060
   Sharma D, 2018, IET COMPUT VIS, V12, P702, DOI 10.1049/iet-cvi.2017.0581
   Sharma D, 2017, PROC INT CONF DOC, P420, DOI 10.1109/ICDAR.2017.76
   Sharma D, 2016, INT C PATT RECOG, P2422, DOI 10.1109/ICPR.2016.7899999
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41
   Singh S, 2009, LECT NOTES COMPUT SC, V5884, P158, DOI 10.1007/978-3-642-10347-6_15
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Weber M., 2010, Proceedings 2010 12th International Conference on Frontiers in Handwriting Recognition (ICFHR 2010), P289, DOI 10.1109/ICFHR.2010.122
   Wu WM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356556
NR 50
TC 10
Z9 11
U1 2
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2785
EP 2800
DI 10.1007/s00371-021-02155-w
EA MAY 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000653632500001
DA 2024-07-18
ER

PT J
AU Arora, S
   Bhatia, MPS
   Mittal, V
AF Arora, Shefali
   Bhatia, M. P. S.
   Mittal, Vipul
TI A robust framework for spoofing detection in faces using deep learning
SO VISUAL COMPUTER
LA English
DT Article
DE Spoofing; Autoencoders; Deep learning; CNN; Security; Face; Biometric
   systems
ID ATTENTION
AB Face recognition is used in biometric systems to verify and authenticate an individual. However, most face authentication systems are prone to spoofing attacks such as replay attacks, attacks using 3D masks etc. Thus, the importance of face anti-spoofing algorithms is becoming essential in these systems. Recently, deep learning has emerged and achieved excellent results in challenging tasks related to computer vision. The proposed framework relies on the extraction of features from the faces of individuals. The approach relies on dimensionality reduction and feature extraction of input frames using pre-trained weights of convolutional autoencoders, followed by classification using softmax classifier. Experimental analysis on three benchmarks, Idiap Replay Attack, CASIA- FASD and 3DMAD, shows that the proposed framework can attain results comparable to state-of-the-art methods in both cross-database and intra-database testing.
C1 [Arora, Shefali; Bhatia, M. P. S.; Mittal, Vipul] Netaji Subhas Inst Technol, Div Comp Engn, Delhi, India.
C3 Netaji Subhas University of Technology
RP Arora, S (corresponding author), Netaji Subhas Inst Technol, Div Comp Engn, Delhi, India.
EM arorashef@gmail.com; bhatia.mps@gmail.com; vipulmittal@gmail.com
RI Arora, Shefali/HMD-8033-2023
OI arora, shefali/0000-0002-8839-749X
CR Alotaibi A, 2017, SIGNAL IMAGE VIDEO P, V11, P713, DOI 10.1007/s11760-016-1014-2
   Alrikabi, 2020, SOLID STATE TECHNOL, V63, P14646
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Galbally J, 2014, IEEE ACCESS, V2, P1530, DOI 10.1109/ACCESS.2014.2381273
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Gan JY, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP), P1, DOI 10.1109/ICMIP.2017.9
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Hadid A., 2017, FACE ANTISPOOFING BI, P1
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Khammari M, 2019, IET IMAGE PROCESS, V13, P1880, DOI 10.1049/iet-ipr.2018.5560
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Li L., 2019, ADV CIV ENG, P1, DOI 10.5194/acp-2018-860
   Li L, 2018, IET BIOMETRICS, V7, P3, DOI 10.1049/iet-bmt.2017.0089
   Li XB, 2016, INT C PATT RECOG, P4244, DOI 10.1109/ICPR.2016.7900300
   Ling Li, 2016, 2016 IEEE International Workshop on Electromagnetics (iWEM): Applications and Student Innovation Competition, P1, DOI 10.1109/iWEM.2016.7504905
   Liu SQ, 2016, LECT NOTES COMPUT SC, V9911, P85, DOI 10.1007/978-3-319-46478-7_6
   Maatta J, 2011, INT JOINT C BIOM IJC, P1, DOI DOI 10.1109/IJCB.2011.6117510
   Mahitha, 2018, INT RES J ENG TECHNO, V9, P1
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Nesli Erdogmus, 2013, IEEE 6 INT C BIOM TH, P1, DOI 10.1109/BTAS.2013.6712688
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Rehman YAU, 2018, EXPERT SYST APPL, V108, P159, DOI 10.1016/j.eswa.2018.05.004
   Sun L, 2007, LECT NOTES COMPUT SC, V4642, P252
   Sun WY, 2020, IEEE ACCESS, V8, P66553, DOI 10.1109/ACCESS.2020.2985453
   Sun YJ, 2021, VISUAL COMPUT, V37, P1015, DOI 10.1007/s00371-020-01849-x
   Tammina S, 2019, Int. J. Sci. Res. Publ, V9, P143, DOI DOI 10.29322/IJSRP.9.10.2019.P9420
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Tu XK, 2017, LECT NOTES COMPUT SC, V10635, P686, DOI 10.1007/978-3-319-70096-0_70
   Vatsa, 2015, IIITDTR2014002002 IN, P1
   Wang GQ, 2019, INT CONF BIOMETR
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Yang JS, 2013, IEEE GLOB COMM CONF, P1, DOI 10.1109/GLOCOM.2013.6831038
   Zhang Z, 2012, IEEE INT C BIO BIO W
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
NR 39
TC 13
Z9 13
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2461
EP 2472
DI 10.1007/s00371-021-02123-4
EA APR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000639803700002
DA 2024-07-18
ER

PT J
AU Mao, WD
   Wang, MJ
   Huang, H
   Gong, ML
AF Mao, Wendong
   Wang, Mingjie
   Huang, Hui
   Gong, Minglun
TI A robust framework for multi-view stereopsis
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; CNN; MVS; Point cloud consolidation
AB Various approaches using neural networks have been proposed to address multi-view stereopsis, but most of them lack capabilities to handle large textureless regions. Hence, a compelling matching network learning comprehensive information from stereo images is constructed to enforce smoothness constraints globally. Trained over binocular stereo datasets only, we show that the network can directly handle the DTU multi-view stereo dataset. When merging together multiple depth maps obtained using either stereo matching, an additional point consolidation procedure is often needed for removing outliers and better aligning individual patches. A second network that consolidates 3D point clouds through directly projecting individual 3D points based on point distributions in their neighborhoods is proposed. Unlike the matching network, this network is trained on local information and is scalable for handling point clouds of any sizes and is capable of processing selected areas of interest as well. Quantitative evaluation on the DTU dataset demonstrates our two networks together can generate point clouds comparable to existing state-of-the-art approaches.
C1 [Mao, Wendong; Wang, Mingjie] Mem Univ Newfoundland, St John, NF, Canada.
   [Huang, Hui] Shenzhen Univ, Shenzhen, Peoples R China.
   [Gong, Minglun] Univ Guelph, Guelph, ON, Canada.
C3 Memorial University Newfoundland; Shenzhen University; University of
   Guelph
RP Mao, WD (corresponding author), Mem Univ Newfoundland, St John, NF, Canada.
EM wm0330@mun.ca; mingjiew@mun.ca; huihuang@szu.edu.cn; minglun@uoguelph.ca
RI Huang, Hui/JGB-1049-2023; Gong, Minglun/AAU-3103-2020
OI Huang, Hui/0000-0003-3212-0544; Gong, Minglun/0000-0001-5820-5381
CR Aanæs H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9
   [Anonymous], 2017, P NIPS
   [Anonymous], 2018, ARXIV181111286
   [Anonymous], 2016, J MACH LEARN RES
   [Anonymous], 2018, CORR
   Arvanitis G, 2018, IEEE IMAGE PROC, P3888, DOI 10.1109/ICIP.2018.8451099
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Choi S, 2018, IEEE COMPUT SOC CONF, P389, DOI 10.1109/CVPRW.2018.00065
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052
   Galliani S, 2016, PROC CVPR IEEE, P5479, DOI 10.1109/CVPR.2016.591
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Im S., 2019, INT C LEARN REPR
   Ji M., 2017, ARXIV PREPRINT ARXIV
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kim P, 2018, AUTOMAT CONSTR, V89, P38, DOI 10.1016/j.autcon.2018.01.009
   Kim SH, 2015, MULTIMED TOOLS APPL, V74, P8939, DOI 10.1007/s11042-013-1584-8
   Luo KY, 2019, IEEE I CONF COMP VIS, P10451, DOI 10.1109/ICCV.2019.01055
   Mao WD, 2019, LECT NOTES COMPUT SC, V11678, P635, DOI 10.1007/978-3-030-29888-3_52
   Menze M, 2015, ISPRS ANN PHOTO REM, VII-3, P427, DOI 10.5194/isprsannals-II-3-W5-427-2015
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Park H, 2017, IEEE SIGNAL PROC LET, V24, P1788, DOI 10.1109/LSP.2016.2637355
   Poms A, 2018, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2018.00321
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Qi C.R., 2017, ARXIV PREPRINT ARXIV
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Romanoni A, 2019, IEEE I CONF COMP VIS, P10412, DOI 10.1109/ICCV.2019.01051
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8
   Wu SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818073
   Yan TM, 2019, IEEE T IMAGE PROCESS, V28, P3885, DOI 10.1109/TIP.2019.2903318
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Ye XQ, 2017, IEEE ACCESS, V5, P18745, DOI 10.1109/ACCESS.2017.2754318
   Yu L., 2018, P EUR C COMP VIS ECC
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu ZH, 2020, PROC CVPR IEEE, P1946, DOI 10.1109/CVPR42600.2020.00202
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zollhofer M, 2016, ACM J COMPUT CULT HE, V9, DOI 10.1145/2770877
NR 55
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1539
EP 1551
DI 10.1007/s00371-021-02087-5
EA MAR 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000630256700003
DA 2024-07-18
ER

PT J
AU Febin, IP
   Jidesh, P
AF Febin, I. P.
   Jidesh, P.
TI Despeckling and enhancement of ultrasound images using non-local
   variational framework
SO VISUAL COMPUTER
LA English
DT Article
DE Despeckling; Non-local Retinex model; Perceptually inspired framework;
   Variational formulation; Bregman scheme
ID ALGORITHM; NOISE; REMOVAL; FILTER; RATIO; MODEL
AB Speckles are introduced in the ultrasound data due to constructive and destructive interference of the probing signals that are used for capturing the characteristics of the tissue being imaged. There are a plethora of models discussed in the literature to improve the contrast and resolution of the ultrasound images by despeckling them. There is a class of models that assumes that the noise is multiplicative in its original form, and transforming the model to a log domain makes it an additive one. Nevertheless, such a transformation duly oversimplifies the scenario and does not capture the inherent properties of the data-correlated nature of speckles. Therefore, it results in poor reconstruction. This problem is addressed to a considerable extent in the subsequent works by adopting various models to address the data-correlated nature of the noise and its distributions. This work introduces a weberized non-local total bounded variational model based on the noise distribution built on the Retinex theory. This perceptually inspired model apparently restores and improves the contrast of the images without compromising much on the details inherently present in the data. The numerical implementation of the model is carried out using the Bregman formulation to improve the convergence rate and reduce the parameter sensitivity. The experimental results are highlighted and compared to demonstrate the efficiency of the model.
C1 [Febin, I. P.; Jidesh, P.] Natl Inst Technol Karnataka, Dept Math & Computat Sci, Mangalore 575025, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Karnataka
RP Jidesh, P (corresponding author), Natl Inst Technol Karnataka, Dept Math & Computat Sci, Mangalore 575025, India.
EM jidesh@nitk.edu.in
RI P, J/KCK-9262-2024; P, Jidesh/C-6030-2017
OI P, Jidesh/0000-0001-9448-1906
FU Science and Engineering Research Board, India [ECR/2017/000230]
FX The authors I.P Febin and P. Jidesh would like to thank Science and
   Engineering Research Board, India, for providing financial support under
   the Project Grant No. ECR/2017/000230
CR Achim A, 2001, IEEE T MED IMAGING, V20, P772, DOI 10.1109/42.938245
   Alberola-Lopez C., 2002, IEEE T IMAGE PROCESS, V11, P1260, DOI [10.1109/TIP.2002.804276, DOI 10.1109/TIP.2002.804276]
   Ambrosanio M, 2020, IEEE ACCESS, V8, P150773, DOI 10.1109/ACCESS.2020.3014909
   Aubert G, 2008, SIAM J APPL MATH, V68, P925, DOI 10.1137/060671814
   Bini AA, 2014, MULTIDIM SYST SIGN P, V25, P41, DOI 10.1007/s11045-012-0184-5
   Born J., 2020, ARXIV200412084V3EESI
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chan SH, 2011, IEEE T IMAGE PROCESS, V20, P3097, DOI 10.1109/TIP.2011.2158229
   Choi H, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12060938
   Coupé P, 2009, IEEE T IMAGE PROCESS, V18, P2221, DOI 10.1109/TIP.2009.2024064
   EKSTROM MP, 1982, IEEE T ACOUST SPEECH, V30, P31, DOI 10.1109/TASSP.1982.1163844
   Febin, 2020, IEEE GEOSCI REMOTE S, P1
   Febin IP, 2020, IEEE J-STARS, V13, P941, DOI 10.1109/JSTARS.2020.2975044
   FROST VS, 1982, IEEE T PATTERN ANAL, V4, P157, DOI 10.1109/TPAMI.1982.4767223
   Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592
   Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z
   Gomez L, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9040389
   Huang LL, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/250768
   Jidesh P, 2018, COMPUT ELECTR ENG, V70, P631, DOI 10.1016/j.compeleceng.2017.09.013
   Jidesh P, 2017, SIGNAL IMAGE VIDEO P, V11, P977, DOI 10.1007/s11760-016-1047-6
   Karathanassi V, 2007, INT J REMOTE SENS, V28, P2309, DOI 10.1080/01431160600606890
   Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998
   Krissian K, 2007, IEEE T IMAGE PROCESS, V16, P1412, DOI 10.1109/TIP.2007.891803
   KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LEE JS, 1980, IEEE T PATTERN ANAL, V2, P165, DOI 10.1109/TPAMI.1980.4766994
   Li HF, 2012, IEEE T GEOSCI REMOTE, V50, P3053, DOI 10.1109/TGRS.2011.2178075
   Li Y, 2011, IEEE T GEOSCI REMOTE, V49, P3105, DOI 10.1109/TGRS.2011.2121072
   Liu, 2018, P 2018 3 INT C CONTR, P114
   Liu XW, 2014, MATH COMPUT SIMULAT, V97, P224, DOI 10.1016/j.matcom.2013.10.001
   Liu XW, 2010, J MATH ANAL APPL, V372, P486, DOI 10.1016/j.jmaa.2010.07.013
   Matkovic K., 2005, Computational Aesthetics, P159
   Mei KQ, 2020, IEEE T IMAGE PROCESS, V29, P2845, DOI 10.1109/TIP.2019.2953361
   Michailovich OV, 2006, IEEE T ULTRASON FERR, V53, P64, DOI 10.1109/TUFFC.2006.1588392
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Nagare MB, 2017, J MED SYST, V41, DOI 10.1007/s10916-016-0675-2
   Ng MK, 2011, SIAM J IMAGING SCI, V4, P345, DOI 10.1137/100806588
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Ramos-Llordén G, 2015, IEEE T IMAGE PROCESS, V24, P345, DOI 10.1109/TIP.2014.2371244
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shen JH, 2003, PHYSICA D, V175, P241, DOI 10.1016/S0167-2789(02)00734-0
   Simoncelli EP, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P379
   Timischl F, 2015, SCANNING, V37, P54, DOI 10.1002/sca.21179
   Xiao L, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/490384
   Yu YJ, 2002, IEEE T IMAGE PROCESS, V11, P1260, DOI 10.1109/TIP.2002.804279
   Zhu XZ, 2008, 2008 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND INFORMATION TECHNOLOGY, PROCEEDINGS, P326, DOI 10.1109/MMIT.2008.134
   Zosso D, 2015, SIAM J IMAGING SCI, V8, P787, DOI 10.1137/140972664
NR 48
TC 9
Z9 9
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1413
EP 1426
DI 10.1007/s00371-021-02076-8
EA FEB 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000622654300001
PM 33678932
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Jiang, AL
   Liu, J
   Zhou, JL
   Zhang, M
AF Jiang, Anling
   Liu, Ji
   Zhou, Jianling
   Zhang, Min
TI Skeleton extraction from point clouds of trees with complex branches via
   graph contraction
SO VISUAL COMPUTER
LA English
DT Article
DE Geodesic neighbor; 3D tree-shaped point clouds; Complex branches;
   Skeleton extraction
ID RECONSTRUCTION; OPTIMIZATION; MODELS
AB Extracting a skeleton from a 3D tree-shaped point cloud with complex branches is a challenging issue due to the diversity of branches and their natural topological complexity. In this paper, we first introduce a novel contraction method called "graph contraction" to contract 3D tree-shaped point clouds. The computation of the graph contraction is formulated as the minimization of an energy function that consists of a contraction term that minimizes the sum of the graph geodesic distances of thek-nearest geodesic neighbors and a topology-preserving term that prevents point clouds from shrinking in the local principal direction. After graph contraction, we downsample the initial skeleton and obtain skeleton nodes. Finally, we introduce an algorithm that can be used to extract a topologically correct skeleton by connecting skeleton nodes. The proposed method has been validated on various 3D tree-shaped point clouds, including reconstructed tree-shaped point clouds, artificially generated tree-shaped point clouds and raw scan data. We use the Chamfer distance to measure the correctness of the skeleton. The experimental results show that the Chamfer distance obtained using our method is smaller than that obtained using current state-of-the-art methods for skeleton extraction of 3D tree-shaped point clouds with complex branches.
C1 [Jiang, Anling; Liu, Ji; Zhou, Jianling; Zhang, Min] Chongqing Univ, Coll Comp Sci, 174 Shazheng St, Chongqing 400044, Peoples R China.
C3 Chongqing University
RP Liu, J (corresponding author), Chongqing Univ, Coll Comp Sci, 174 Shazheng St, Chongqing 400044, Peoples R China.
EM liujiboy@cqu.edu.cn
OI Liu, Ji/0000-0002-0059-4588
FU Chongqing Research Program of Basic Research and Frontier Technology
   [cstc2019jcyj-msxmX0033]; National Natural Science Foundation of China
   [61701051]; Fundamental Research Funds for the Central Universities
   [2019CDYGYB012]
FX This work was supported by the Chongqing Research Program of Basic
   Research and Frontier Technology (Grant No. cstc2019jcyj-msxmX0033), the
   National Natural Science Foundation of China (Grant No. 61701051) and
   Fundamental Research Funds for the Central Universities (Grant No.
   2019CDYGYB012).
CR Aiteanu F, 2014, VISUAL COMPUT, V30, P763, DOI 10.1007/s00371-014-0977-7
   Atienza R, 2019, IEEE COMPUT SOC CONF, P1177, DOI 10.1109/CVPRW.2019.00155
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Bucksch A, 2008, ISPRS J PHOTOGRAMM, V63, P115, DOI 10.1016/j.isprsjprs.2007.10.004
   Cao J., 2010, SHAP MOD INT C SMI 2, P187, DOI [DOI 10.1109/SMI.2010.25, 10.1109/SMI.2010.25]
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Demir I, 2019, IEEE COMPUT SOC CONF, P1143, DOI 10.1109/CVPRW.2019.00149
   Gagvani N, 1999, GRAPH MODEL IM PROC, V61, P149, DOI 10.1006/gmip.1999.0495
   Hackenberg J, 2015, FORESTS, V6, P4245, DOI 10.3390/f6114245
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Liu C., 2019, PARAMETRIC SKELETON
   Livny Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866177
   Mei J, 2017, INT J GEOGR INF SCI, V31, P999, DOI 10.1080/13658816.2016.1264075
   Mémoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y
   Morsdorf F, 2004, REMOTE SENS ENVIRON, V92, P353, DOI 10.1016/j.rse.2004.05.013
   Mullen P, 2010, COMPUT GRAPH FORUM, V29, P1733, DOI 10.1111/j.1467-8659.2010.01782.x
   Panichev O, 2019, IEEE COMPUT SOC CONF, P1186, DOI 10.1109/CVPRW.2019.00157
   Qin HX, 2020, IEEE T VIS COMPUT GR, V26, P2805, DOI 10.1109/TVCG.2019.2903805
   Seitz S.M., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI https://doi.org/10.1109/CVPR.2006.19
   SMALL CG, 1990, INT STAT REV, V58, P263, DOI 10.2307/1403809
   Song CF, 2018, VISUAL COMPUT, V34, P243, DOI 10.1007/s00371-016-1331-z
   Tagliasacchi A, 2016, COMPUT GRAPH FORUM, V35, P573, DOI 10.1111/cgf.12865
   Tagliasacchi A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531377
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Verroust A, 2000, VISUAL COMPUT, V16, P15, DOI 10.1007/PL00007210
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Z, 2016, IEEE T GEOSCI REMOTE, V54, P4749, DOI 10.1109/TGRS.2016.2551286
   Wang Z, 2014, IEEE T GEOSCI REMOTE, V52, P5653, DOI 10.1109/TGRS.2013.2291815
   Xu H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289610
   Yang C, 2016, PATTERN RECOGN, V55, P183, DOI 10.1016/j.patcog.2016.01.022
   Yang LP, 2019, IEEE COMPUT SOC CONF, P1162, DOI 10.1109/CVPRW.2019.00152
   Zhang XP, 2014, IEEE T VIS COMPUT GR, V20, P1214, DOI 10.1109/TVCG.2014.2316001
   Zhu QS, 2016, PATTERN RECOGN LETT, V80, P30, DOI 10.1016/j.patrec.2016.05.007
NR 35
TC 10
Z9 12
U1 2
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2235
EP 2251
DI 10.1007/s00371-020-01983-6
EA OCT 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000580750200002
DA 2024-07-18
ER

PT J
AU Rohith, G
   Kumar, LS
AF Rohith, G.
   Kumar, Lakshmi Sutha
TI Paradigm shifts in super-resolution techniques for remote sensing
   applications
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Frequency; and spatial-domain methods; Deep learning;
   Remote sensing applications
ID SINGLE IMAGE SUPERRESOLUTION; HIGH-RESOLUTION IMAGE; SUPER RESOLUTION;
   PARAMETER-ESTIMATION; SATELLITE IMAGERY; RECONSTRUCTION; NETWORK;
   REPRESENTATIONS; ENHANCEMENT; ALGORITHM
AB Super-resolution (SR) algorithms have now become a bottleneck for several remote sensing applications. SR is a technique that enhances minute details of the image by increasing spatial resolution of imaging systems. SR overcomes the problems of conventional resolution enhancement techniques such as introduction of noise, spectral distortion, and lack of clarity in the details of the image. In this paper, a survey has been conducted since the inception of SR algorithm till the latest state-of-the-art SR techniques to elucidate the importance of the SR algorithms that lead to paradigm shifts in the last two decades revolutionizing toward visually pleasing high-resolution image. Inspired from the natural images, the algorithms addressing the SR problems such as ill-posed, prior and regularization problem, inverse problem, multi-frame problem and illumination and shadow problem in remote sensing applications are analyzed. For an intuitive understanding of the paradigm shifts, publicly available images are tested with representative paradigm shift SR algorithms. The result of this paradigm shift analysis is done both qualitatively and quantitatively in terms of blurs in the image, pattern clarity, edge strength, and super-resolving capability. The convergence of the natural image to the remote sensed image is critically analyzed. The challenges with possible solutions for super-resolving the remote sensed image are recommended. On experimentation, it is found that deep learning-based SR algorithms produces visually pleasing images retaining sharp edges, enhanced spatial data, and clarity in feature representation while zooming at a certain level beyond interest.
C1 [Rohith, G.; Kumar, Lakshmi Sutha] Natl Inst Technol, Dept Elect & Commun Engn, Pondicherry, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Puducherry
RP Rohith, G (corresponding author), Natl Inst Technol, Dept Elect & Commun Engn, Pondicherry, India.
EM rohith.giridharan@gmail.com
RI Kumar, Lakshmi Sutha/P-9990-2019; Giridharan, Rohith/ABB-4427-2020
OI , ROHITH G/0000-0001-6120-4056; Kumar, Lakshmi Sutha/0000-0002-9069-3132
CR Abd El-Samie FE, 2012, IMAGE SUPER RESOLUTI
   Agustsson E., 2017, P IEEE CVF C COMP VI, P126
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2002, Super-Resolution Imaging
   [Anonymous], 2005, P WORKSH THES DISS C
   [Anonymous], 1999, ADV NEURAL INFORM PR
   Aymaz S, 2019, INFORM FUSION, V45, P113, DOI 10.1016/j.inffus.2018.01.015
   Azarang A, 2019, IEEE ACCESS, V7, P35673, DOI 10.1109/ACCESS.2019.2905511
   Azarang A, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND IMAGE ANALYSIS (IPRIA), P1, DOI 10.1109/PRIA.2017.7983017
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bi ZQ, 1999, IEEE T AERO ELEC SYS, V35, P267, DOI 10.1109/7.745697
   Bose N. K., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P269, DOI 10.1109/ICASSP.1993.319799
   Cai WT, 2018, INT GEOSCI REMOTE SE, P7046, DOI 10.1109/IGARSS.2018.8518121
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   CHEESEMAN P, 1996, MAXIMUM ENTROPY BAYE
   Chen CB, 2015, VISUAL COMPUT, V31, P1217, DOI 10.1007/s00371-014-1007-5
   Cireundefinedan D.C., 2011, IJCAI INT JOINT C AR, VTwo, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Dare PM, 2005, PHOTOGRAMM ENG REM S, V71, P169, DOI 10.14358/PERS.71.2.169
   Demirel H, 2011, IEEE T IMAGE PROCESS, V20, P1458, DOI 10.1109/TIP.2010.2087767
   DESANTIS P, 1975, OPT ACTA, V22, P691, DOI 10.1080/713819094
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Efrat N, 2013, IEEE I CONF COMP VIS, P2832, DOI 10.1109/ICCV.2013.352
   Enríquez-Cervantes CJ, 2015, COMPUT SIST, V19, P211, DOI 10.13053/CyS-19-2-2068
   Fan D.P., 2018, COMPUTER VISION PATT
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Freeman W.T., 1999, TR99 MITS EL RES LAB
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gajjar P, 2010, LECT NOTES COMPUT SC, V6134, P63, DOI 10.1007/978-3-642-13681-8_8
   GERCHBERG RW, 1974, OPT ACTA, V21, P709, DOI 10.1080/713818946
   Gil ML, 2014, GISCI REMOTE SENS, V51, P630, DOI 10.1080/15481603.2014.988433
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gotoh T., 2004, Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pII
   Gunturk BK, 2002, IEEE SIGNAL PROC LET, V9, P170, DOI 10.1109/LSP.2002.800503
   Haefner B, 2018, PROC CVPR IEEE, P164, DOI 10.1109/CVPR.2018.00025
   Hayat K, 2018, DIGIT SIGNAL PROCESS, V81, P198, DOI 10.1016/j.dsp.2018.07.005
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang D., 2015, Journal of Computer Science Technology Updates, V2, P19, DOI DOI 10.15379/2410-2938.2015.02.02.03
   Huang NB, 2017, LECT NOTES COMPUT SC, V10635, P622, DOI 10.1007/978-3-319-70096-0_64
   Huang T.S., 1984, ADV COMPUTER VISION, V1, P317
   Huang W, 2015, IEEE GEOSCI REMOTE S, V12, P1037, DOI 10.1109/LGRS.2014.2376034
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Jiji CV, 2007, MULTIDIM SYST SIGN P, V18, P123, DOI 10.1007/s11045-007-0024-1
   Keren D., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P742, DOI 10.1109/CVPR.1988.196317
   Keshk HM, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS AND COMPUTING (ICSPCC)
   Nguyen K, 2018, PATTERN RECOGN, V78, P23, DOI 10.1016/j.patcog.2018.01.002
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   KIM SP, 1990, IEEE T ACOUST SPEECH, V38, P1013, DOI 10.1109/29.56062
   Kumar N, 2017, PATTERN RECOGN LETT, V90, P65, DOI 10.1016/j.patrec.2017.03.014
   Lanaras C, 2018, ISPRS J PHOTOGRAMM, V146, P305, DOI 10.1016/j.isprsjprs.2018.09.018
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lei S, 2017, IEEE GEOSCI REMOTE S, V14, P1243, DOI 10.1109/LGRS.2017.2704122
   Li BC, 2020, PATTERN RECOGN LETT, V130, P21, DOI 10.1016/j.patrec.2018.07.023
   Li F, 2008, IEEE IMAGE PROC, P333, DOI 10.1109/ICIP.2008.4711759
   Li HL, 2019, SIGNAL PROCESS-IMAGE, V72, P25, DOI 10.1016/j.image.2018.12.001
   Li KQY, 2019, SIGNAL PROCESS-IMAGE, V72, P58, DOI 10.1016/j.image.2018.12.006
   Li L, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17051062
   Liebel L, 2016, INT ARCH PHOTOGRAMM, V41, P883, DOI 10.5194/isprsarchives-XLI-B3-883-2016
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu N, 2019, OPT LASER TECHNOL, V110, P135, DOI 10.1016/j.optlastec.2018.01.043
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu J, 2019, SIGNAL PROCESS-IMAGE, V70, P210, DOI 10.1016/j.image.2018.10.003
   Lu T, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131588
   Luo YM, 2017, IEEE GEOSCI REMOTE S, V14, P2398, DOI 10.1109/LGRS.2017.2766204
   Ma W, 2019, IEEE T GEOSCI REMOTE, V57, P3512, DOI 10.1109/TGRS.2018.2885506
   Ma W, 2018, INT GEOSCI REMOTE SE, P1148, DOI 10.1109/IGARSS.2018.8517442
   Haut JM, 2018, IEEE T GEOSCI REMOTE, V56, P6792, DOI 10.1109/TGRS.2018.2843525
   Märtens M, 2019, ASTRODYNAMICS-CHINA, V3, P387, DOI 10.1007/s42064-019-0059-8
   Meer P, 2001, FDN IMAGE UNDERSTAND
   Miravet C, 2005, LECT NOTES COMPUT SC, V3696, P499, DOI 10.1007/11550822_78
   Miravet C, 2007, IMAGE VISION COMPUT, V25, P1449, DOI 10.1016/j.imavis.2006.12.016
   Mirotznik M., 2009, PRACTICAL ENHANCED R, DOI [10.1117/12.819484, DOI 10.1117/12.819484]
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Molina R, 2006, IEEE IMAGE PROC, P1749, DOI 10.1109/ICIP.2006.312720
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Nguyen N, 2001, IEEE T IMAGE PROCESS, V10, P573, DOI 10.1109/83.913592
   Noor DF, 2019, IEEE IMAGE PROC, P2164, DOI [10.1109/icip.2019.8803156, 10.1109/ICIP.2019.8803156]
   Pan ZX, 2019, IEEE T GEOSCI REMOTE, V57, P7918, DOI 10.1109/TGRS.2019.2917427
   Panda Sudam, 2011, IJCSI INT J COMPUT S, V8
   Pandey G, 2020, VISUAL COMPUT, V36, P1291, DOI 10.1007/s00371-019-01729-z
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   PELEG S, 1987, PATTERN RECOGN LETT, V5, P223, DOI 10.1016/0167-8655(87)90067-5
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Pendurkar S, 2019, IMAGE ANAL PROCESSIN
   Pineda F, 2020, INFORM MANAGEMENT BI
   Pohl C, 1998, INT J REMOTE SENS, V19, P823, DOI 10.1080/014311698215748
   Pouliot D, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10030394
   Purnamasayangsukasih PR, 2016, IOP C SER EARTH ENV, V37, DOI 10.1088/1755-1315/37/1/012034
   QIFANG X, 2017, PROCEDIA COMPUTER SC, V107, P454
   Rahiman A, 2016, 2016 3rd International Conference on Recent Advances in Information Technology (RAIT), P473, DOI 10.1109/RAIT.2016.7507947
   Rahiman VA, 2018, COMPUT ELECTR ENG, V70, P674, DOI 10.1016/j.compeleceng.2017.09.020
   Rahiman VA, 2017, COMPUT ELECTR ENG, V62, P281, DOI 10.1016/j.compeleceng.2016.12.018
   Rajaram S, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/51306
   Ran Q, 2020, MULTIMED TOOLS APPL, V79, P8985, DOI 10.1007/s11042-018-7091-1
   Robinson M. D., 2010, Super-Resolution Imaging, P384
   Rohith G, 2015, 2015 IEEE SEVENTH NATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND INFORMATION SYSTEMS (NCCCIS), P5, DOI 10.1109/NCCCIS.2015.7295905
   Rohith G., 2019, IEEE BOMB SECT SIGN, P1, DOI [10.1109/ibssc47189.2019.8973105, DOI 10.1109/IBSSC47189.2019.8973105]
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sadaka NG, 2011, IEEE IMAGE PROC, P1197, DOI 10.1109/ICIP.2011.6115645
   Sakurai M, 2013, IEEE IMAGE PROC, P854, DOI 10.1109/ICIP.2013.6738176
   Salakhutdinov R., 2010, JMLR WORKSHOP C P, P693
   Schultz RR, 1998, J VIS COMMUN IMAGE R, V9, P38, DOI 10.1006/jvci.1997.0370
   SCHULTZ RR, 1994, IEEE T IMAGE PROCESS, V3, P233, DOI 10.1109/83.287017
   Shermeyer J, 2019, IEEE COMPUT SOC CONF, P1432, DOI 10.1109/CVPRW.2019.00184
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shuai Y, 2018, 2018 IEEE INTERNATIONAL MAGNETIC CONFERENCE (INTERMAG)
   Shukla KK, 2014, EFFICIENT ALGORITHMS
   Singh A, 2014, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2014.364
   STARK H, 1989, J OPT SOC AM A, V6, P1715, DOI 10.1364/JOSAA.6.001715
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tatem AJ, 2001, IEEE T GEOSCI REMOTE, V39, P781, DOI 10.1109/36.917895
   Tian JY, 2014, INTERNATIONAL SYMPOSIUM ON FUZZY SYSTEMS, KNOWLEDGE DISCOVERY AND NATURAL COMPUTATION (FSKDNC 2014), P114
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tsagkatakis G, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19183929
   Tuna C, 2018, INT J REMOTE SENS, V39, P2463, DOI 10.1080/01431161.2018.1425561
   Venkatanath N., P 21 NAT C COMM NCC
   Ha VK, 2019, INT J AUTOM COMPUT, V16, P413, DOI 10.1007/s11633-019-1183-x
   WALSH DO, 1994, J OPT SOC AM A, V11, P572, DOI 10.1364/JOSAA.11.000572
   Wang HB, 2016, NEUROCOMPUTING, V216, P286, DOI 10.1016/j.neucom.2016.07.044
   Wang J, 2008, ENCY MULTIMEDIA
   Wang TW, 2018, IEEE GEOSCI REMOTE S, V15, P769, DOI 10.1109/LGRS.2018.2810893
   Wang Y, 2008, IEEE IMAGE PROC, P345, DOI 10.1109/ICIP.2008.4711762
   Wei YC, 2017, IEEE GEOSCI REMOTE S, V14, P1795, DOI 10.1109/LGRS.2017.2736020
   Witwit W, 2018, MULTIMED TOOLS APPL, V77, P27641, DOI 10.1007/s11042-018-5941-5
   Wu J., 2018, IEEE GLOB C SIGN INF
   Xie H, 2013, J SIGNAL INFORM PROC, V4, P222
   Xie N, 2009, 2009 INTERNATIONAL FORUM ON COMPUTER SCIENCE-TECHNOLOGY AND APPLICATIONS, VOL 3, PROCEEDINGS, P315, DOI 10.1109/IFCSTA.2009.317
   Xiong YM, 2019, IEEE ACCESS, V7, P106295, DOI 10.1109/ACCESS.2019.2932015
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Xu WJ, 2018, INT GEOSCI REMOTE SE, P8889, DOI 10.1109/IGARSS.2018.8518855
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang S, 2001, IEEE T CIRC SYST VID, V11, P963, DOI 10.1109/76.937440
   Yuan QQ, 2012, IEEE T CIRC SYST VID, V22, P379, DOI 10.1109/TCSVT.2011.2163447
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zareapoor M, 2018, COGN SYST RES, V52, P49, DOI 10.1016/j.cogsys.2018.06.007
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zeng K, 2019, APPL INTELL, V49, P292, DOI 10.1007/s10489-018-1270-7
   Zhang D, 2012, IMAGE VISION COMPUT, V30, P100, DOI 10.1016/j.imavis.2012.01.005
   Zhang HY, 2012, SIGNAL PROCESS, V92, P2082, DOI 10.1016/j.sigpro.2012.01.020
   Zhang J, 2017, MULTIEXAMPLE FEATURE, P73
   Zhang Junlei, 2017, [Computational Visual Media, 计算可视媒体], V3, P73
   Zhao JW, 2019, J VIS COMMUN IMAGE R, V58, P651, DOI 10.1016/j.jvcir.2018.12.036
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 148
TC 16
Z9 16
U1 1
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1965
EP 2008
DI 10.1007/s00371-020-01957-8
EA SEP 2020
PG 44
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000565484100001
DA 2024-07-18
ER

PT J
AU Rasmuson, S
   Sintorn, E
   Assarsson, U
AF Rasmuson, Sverker
   Sintorn, Erik
   Assarsson, Ulf
TI A low-cost, practical acquisition and rendering pipeline for real-time
   free-viewpoint video communication
SO VISUAL COMPUTER
LA English
DT Article
DE Free-viewpoint video; 3D acquisition; Stereo reconstruction
AB We present a semiautomatic real-time pipeline for capturing and rendering free-viewpoint video using passive stereo matching. The pipeline is simple and achieves agreeable quality in real time on a system of commodity web cameras and a single desktop computer. We suggest an automatic algorithm to compute a constrained search space for an efficient and robust hierarchical stereo reconstruction algorithm. Due to our fast reconstruction times, we can eliminate the need for an expensive global surface reconstruction with a combination of high coverage and aggressive filtering. Finally, we employ a novel color weighting scheme that generates credible new viewpoints without noticeable seams, while keeping the computational complexity low. The simplicity and low cost of the system make it an accessible and more practical alternative for many applications compared to previous methods.
C1 [Rasmuson, Sverker; Sintorn, Erik; Assarsson, Ulf] Chalmers Univ Technol, Gothenburg, Sweden.
C3 Chalmers University of Technology
RP Rasmuson, S (corresponding author), Chalmers Univ Technol, Gothenburg, Sweden.
EM sverker.rasmuson@chalmers.se; erik.sintorn@chalmers.se; uffe@chalmers.se
OI Rasmuson, Sverker/0000-0003-4623-5115
FU Chalmers University of Technology; Swedish Research Council [2014-4559]
FX Open access funding provided by Chalmers University of Technology. This
   work was supported by the Swedish Research Council under Grant
   2014-4559.
CR [Anonymous], 2014, ACM T GRAPHIC, DOI DOI 10.1145/2601097.2601165
   Beeler T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778777
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14
   Bradski G., 2000, Opencv. Dr. Dobb's journal of software tools
   Cannon E., GREENSCREEN CODE HIN
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Feng Y., 2018, ARXIV180307835 CORR
   Hansard M., 2012, Time-of-Flight Cameras: Principles, Methods and Applications
   Kazemi Vahid, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P369, DOI 10.1109/3DV.2014.93
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kowalczuk J, 2013, IEEE T CIRC SYST VID, V23, P94, DOI 10.1109/TCSVT.2012.2203200
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Maimone A., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P137, DOI 10.1109/ISMAR.2011.6092379
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Petit B, 2010, INT J DIGIT MULTIMED, V2010, DOI 10.1155/2010/247108
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Richardson E., 2016, ARXIV161105053 CORR
   Rong G., 2006, P 2006 S INTERACTIVE, P109, DOI DOI 10.1145/1111411.1111431
   Tan FW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P64, DOI 10.1145/3123266.3123281
   Tewari A., 2017, ARXIV171202859 CORR
   Tewari A., 2017, ARXIV170310580 CORR
NR 25
TC 6
Z9 6
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 553
EP 565
DI 10.1007/s00371-020-01823-7
EA MAR 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000518343000001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, XJ
   Huang, H
   Zhao, HL
   Wang, YD
   Hu, MX
AF Li, Xujie
   Huang, Hui
   Zhao, Hanli
   Wang, Yandan
   Hu, Mingxiao
TI Learning a convolutional neural network for propagation-based stereo
   image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo image segmentation; Convolutional neural network; Coherent
   disparities; Energy minimization framework
AB Stereo image segmentation is the key technology in stereo image editing with the population of stereoscopic 3D media. Most previous methods perform stereo image segmentation on both views relying primarily on per-pixel disparities, which results in the segmentation quality closely connected to the accuracy of the disparities. Therefore, a mechanism to remove the errors of the disparities are highly demanded. To date, there's no such a method yet that can produce accurate disparity maps. In this paper, we propose a novel convolutional neural network (CNN)-based framework, which will automatically propagate the segmentation result from one view to the other. The key problem of accurate stereo image segmentation is the missing of occluded regions. To solve this problem, the CNN architecture is proposed to improve the stereo segmentation performance. In order to address the inevitable inaccuracies problem of the disparities computed from a stereo pair of images, we utilize the coherent disparity propagation that propagates segment result via those pixels with coherent disparities. The pixels by coherent disparity propagation and the high confidence pixels of the object probability map produced by the CNN architecture are then used to generate the initial reliable pixels to perform an energy minimization framework-based segmentation. A comprehensive evaluations and comparisons on Middlebury and Adobe benchmark datasets show the effectiveness of our proposed method in terms of high-quality results, and the robustness against various types of inputs.
C1 [Li, Xujie; Huang, Hui; Zhao, Hanli; Wang, Yandan; Hu, Mingxiao] Wenzhou Univ, Intelligent Informat Syst Inst, Wenzhou 325035, Peoples R China.
C3 Wenzhou University
RP Li, XJ (corresponding author), Wenzhou Univ, Intelligent Informat Syst Inst, Wenzhou 325035, Peoples R China.
EM lixujie101@aliyun.com
CR [Anonymous], 2017, IEEE T DEPENDABLE SE
   [Anonymous], 2017, ARXIV170700652
   [Anonymous], 2018, MAR GEORESOUR GEOTEC
   BERTASIUS G, 2016, ARXIV160507681
   Casaca W, 2014, PROC CVPR IEEE, P384, DOI 10.1109/CVPR.2014.56
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Endo Y, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.12822
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Huang H, 2015, MULTIMED TOOLS APPL, V74, P7555, DOI 10.1007/s11042-014-1991-5
   Ju R, 2015, IEEE I CONF COMP VIS, P1724, DOI 10.1109/ICCV.2015.201
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li XJ, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.5.053031
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Lo WY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866173
   Ma W, 2016, IEEE SIGNAL PROC LET, V23, P1533, DOI 10.1109/LSP.2016.2605133
   Ma W, 2016, MULTIMED TOOLS APPL, V75, P10935, DOI 10.1007/s11042-015-2817-9
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Peng JT, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P22, DOI 10.1109/ICCVW.2013.10
   Price BL, 2011, IEEE I CONF COMP VIS, P1148, DOI 10.1109/ICCV.2011.6126363
   Ran Ju, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P418, DOI 10.1007/978-3-319-03731-8_39
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Shen XY, 2016, COMPUT GRAPH FORUM, V35, P93, DOI 10.1111/cgf.12814
   Tasli HE, 2013, SIGNAL PROCESS-IMAGE, V28, P1374, DOI 10.1016/j.image.2013.08.003
   Wang LN, 2017, NEURAL NETWORKS, V93, P219, DOI 10.1016/j.neunet.2017.06.003
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
NR 39
TC 25
Z9 25
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 39
EP 52
DI 10.1007/s00371-018-1582-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800005
DA 2024-07-18
ER

PT J
AU Pradhan, J
   Ajad, A
   Pal, AK
   Banka, H
AF Pradhan, Jitesh
   Ajad, Ashok
   Pal, Arup Kumar
   Banka, Haider
TI Multi-level colored directional motif histograms for content-based image
   retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Content-based image retrieval (CBIR); Directional motifs; Histogram;
   Local binary patterns (LBP)
ID WAVELET; CLASSIFICATION; FEATURES
AB Color features and local geometrical structures are the two basic image features which are sufficient to convey the image semantics. Both of these features show diverse nature on the different regions of a natural image. Traditional local motif patterns are standard tools to emphasize these local visual image features. Thesemotif-based schemes consider either structural orientations or limited directional patterns which are not sufficient to realize the detailed local geometrical properties of an image. To address these issues, we have proposed a new multi-level colored directional motif histogram (MLCDMH) for devising a content-based image retrieval scheme. The proposed scheme extracts local structural features at three different levels. Initially, MLCDMH scheme extracts directional structural patterns from a 3 x 3 pixel grids of an image. This reflects the 99 different structural arrangements using 28 directional patterns. Next, we have used a weighted neighboring similarity (WNS) scheme to exploit the uniqueness of each motif pixel in its local surrounding. The WNS scheme will compute the importance of each directional motif pattern in its 3 x 3 local neighborhood. In the last level, we have fused all directional motif images into a single directional difference matrix which reflects the local structural and directional motif features in detail and also reduces the computation overhead. The MLCDMH considers all possible permutations and rotations of the motif patterns to generate rotational invariant structural features. The image retrieval performance of this proposed scheme has been evaluated using different Corel/natural, object, texture and heterogeneous image datasets. The results of the retrieval experiments have shown satisfactory improvement over other motif- and non-motif-based CBIR approaches.
C1 [Pradhan, Jitesh; Ajad, Ashok; Pal, Arup Kumar; Banka, Haider] Indian Inst Technol ISM Dhanbad, Dept Comp Sci & Engn, Dhanbad, Bihar, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (Indian School of Mines) Dhanbad
RP Pradhan, J (corresponding author), Indian Inst Technol ISM Dhanbad, Dept Comp Sci & Engn, Dhanbad, Bihar, India.
EM jitpradhan02@gmail.com; cse.aa9@gmail.com; arupkrpal@gmail.com;
   haider.banka@gmail.com
OI Pradhan, Jitesh/0000-0002-6264-4093
CR An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   Anuar FM, 2013, EXPERT SYST APPL, V40, P105, DOI 10.1016/j.eswa.2012.07.031
   Bhunia A. K., 2018, ARXIV180100879
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Cheng SL, 2019, VISUAL COMPUT, V35, P1255, DOI 10.1007/s00371-018-1583-x
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng QH, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18061943
   Galshetwar GM, 2019, J VIS COMMUN IMAGE R, V64, DOI 10.1016/j.jvcir.2019.102615
   GUDIVADA VN, 1995, COMPUTER, V28, P18, DOI 10.1109/2.410145
   Han J, 2007, IMAGE VISION COMPUT, V25, P1474, DOI 10.1016/j.imavis.2006.12.015
   He ZY, 2009, SIGNAL PROCESS, V89, P1501, DOI 10.1016/j.sigpro.2009.01.021
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3
   Jhanwar N, 2004, IMAGE VISION COMPUT, V22, P1211, DOI 10.1016/j.imavis.2004.03.026
   Khatami A, 2018, EXPERT SYST APPL, V100, P224, DOI 10.1016/j.eswa.2018.01.056
   Kokare M, 2007, PATTERN RECOGN LETT, V28, P1240, DOI 10.1016/j.patrec.2007.02.006
   Krommweh J, 2010, J VIS COMMUN IMAGE R, V21, P364, DOI 10.1016/j.jvcir.2010.02.011
   Li CR, 2017, PATTERN RECOGN, V64, P118, DOI 10.1016/j.patcog.2016.10.030
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   Lin CH, 2014, EXPERT SYST APPL, V41, P3276, DOI 10.1016/j.eswa.2013.11.017
   Liu GH, 2015, PATTERN RECOGN, V48, P2554, DOI 10.1016/j.patcog.2015.02.005
   Liu GH, 2013, PATTERN RECOGN, V46, P188, DOI 10.1016/j.patcog.2012.06.001
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003
   Liu GH, 2010, PATTERN RECOGN, V43, P2380, DOI 10.1016/j.patcog.2010.02.012
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lu TC, 2007, INFORM PROCESS MANAG, V43, P461, DOI 10.1016/j.ipm.2006.07.014
   Lu ZM, 2005, ELECTRON LETT, V41, P956, DOI 10.1049/el:20052176
   Mohammed MM, 2015, EXPERT SYST APPL, V42, P4927, DOI 10.1016/j.eswa.2015.02.019
   Obulesu A., 2018, International Journal of Image, Graphics and Signal Processing, V10, P59, DOI 10.5815/ijigsp.2018.04.07
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Penatti OAB, 2012, J VIS COMMUN IMAGE R, V23, P359, DOI 10.1016/j.jvcir.2011.11.002
   Pradhan J, 2019, MULTIMED TOOLS APPL, V78, P1685, DOI 10.1007/s11042-018-6246-4
   Pradhan J, 2016, 2016 FOURTH INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND GRID COMPUTING (PDGC), P447, DOI 10.1109/PDGC.2016.7913237
   Thuy QDT, 2017, EXPERT SYST APPL, V72, P30, DOI 10.1016/j.eswa.2016.12.004
   Sadeghi B, 2019, VISUAL COMPUT, V35, P1373, DOI 10.1007/s00371-018-01616-z
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Subrahmanyam M, 2012, EXPERT SYST APPL, V39, P5104, DOI 10.1016/j.eswa.2011.11.029
   SWAIN MJ, 1993, P SOC PHOTO-OPT INS, V1908, P95, DOI 10.1117/12.143659
   Takala Valtteri., 2005, Block-Based Methods for Image Retrieval Using Local Binary Patterns, SCIA 2005, LNCS 3540, P882
   Varish N, 2017, MULTIMED TOOLS APPL, V76, P15885, DOI 10.1007/s11042-016-3882-4
   Vipparthi SK, 2014, COMPUT ELECTR ENG, V40, P163, DOI 10.1016/j.compeleceng.2014.04.018
   Vipparthi SK, 2014, EXPERT SYST APPL, V41, P8016, DOI 10.1016/j.eswa.2014.07.001
   Wang XY, 2014, MULTIMED TOOLS APPL, V68, P545, DOI 10.1007/s11042-012-1055-7
   Won CS, 2002, ETRI J, V24, P23, DOI 10.4218/etrij.02.0102.0103
   Yao CH, 2003, PATTERN RECOGN, V36, P913, DOI 10.1016/S0031-3203(02)00124-3
   Yue J, 2011, MATH COMPUT MODEL, V54, P1121, DOI 10.1016/j.mcm.2010.11.044
   Zeng S, 2016, NEUROCOMPUTING, V171, P673, DOI 10.1016/j.neucom.2015.07.008
NR 50
TC 13
Z9 13
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1847
EP 1868
DI 10.1007/s00371-019-01773-9
EA DEC 2019
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000541644200002
DA 2024-07-18
ER

PT J
AU Hu, SY
   Chen, XJ
   Tong, X
AF Hu, Siyu
   Chen, Xuejin
   Tong, Xin
TI Point sets joint registration and co-segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Registration; Co-segmentation
AB We present a novel approach of joint registration and co-segmentation for point sets where objects move in different ways. We consider joint registration and co-segmentation as two problems that are heavily entangled with each other; thus, we represent the input point sets as samples from a generative model and bring up with a novel formulation based on Gaussian mixture model. By maximizing the posterior probability of the samples, we gradually recover the latent object models as well as an object-level segmentation and simultaneously align the segmented points to the latent object models. Along with the formulation, we design an interactive tool that helps users intuitively intervene the process to optimize the registration and segmentation results. The experiment results on a group of synthetic and scanned point clouds demonstrate that our method is powerful and effective for joint registration and co-segmentation on point sets of multiple objects.
C1 [Hu, Siyu; Chen, Xuejin] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei, Peoples R China.
   [Tong, Xin] Microsoft Res Asia, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Chen, XJ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei, Peoples R China.
EM sy891228@mail.ustc.edu.cn; xjchen99@ustc.edu.cn; xtong@microsoft.com
OI Tong, Xin/0000-0001-8788-2453
FU National Natural Science Foundation - National Natural Science
   Foundation of China [61472377, 61632006, 6133101]
FX We would like to thank YantingLin and Jian Wu. They helped with data
   preparation for our experiments. We would also like to thank the
   National Natural Science Foundation for their funding. This study was
   funded by the National Natural Science Foundation of China under Nos.
   61472377, 61632006, and 6133101.
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Campbell D, 2016, PROC CVPR IEEE, P5685, DOI 10.1109/CVPR.2016.613
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Dema MA, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P58, DOI 10.1109/ISM.2012.19
   Evangelidis GD, 2014, LECT NOTES COMPUT SC, V8695, P109, DOI 10.1007/978-3-319-10584-0_8
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Guo H, 2016, VISUAL COMPUT, V32, P1511, DOI 10.1007/s00371-015-1136-5
   Jia ZY, 2015, IEEE T PATTERN ANAL, V37, P905, DOI 10.1109/TPAMI.2014.2359435
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Li Y, 2016, PROC CVPR IEEE, P1619, DOI 10.1109/CVPR.2016.179
   Liu ZZ, 2015, P INST NAVIG PAC PNT, P25
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178
   Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460
   Tombari Federico, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P349, DOI 10.1109/PSIVT.2010.65
   Wan LL, 2017, VISUAL COMPUT, V33, P1497, DOI 10.1007/s00371-016-1293-1
   Xu K, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818075
   Xu K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461968
NR 29
TC 1
Z9 1
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1841
EP 1853
DI 10.1007/s00371-018-1578-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300012
DA 2024-07-18
ER

PT J
AU Yang, XM
   Zhang, JW
   Liu, YN
   Zheng, XJ
   Liu, K
AF Yang, Xiaomei
   Zhang, Jiawei
   Liu, Yanan
   Zheng, Xiujuan
   Liu, Kai
TI Super-resolution image reconstruction using fractional-order total
   variation and adaptive regularization parameters
SO VISUAL COMPUTER
LA English
DT Article
DE Fractional-order total variation; Adaptive regularization parameter;
   Alternating direction multiplier method; Discrepancy principle;
   Super-resolution image reconstruction
ID ALGORITHM; DECONVOLUTION; RESTORATION; SELECTION
AB Single-image super-resolution (SR) reconstruction aims to obtain a high-resolution (HR) image from a low-resolution (LR) image. In this paper, a hybrid single-image SR model integrated total variation (TV) and fractional-order TV (FOTV) is proposed to pursuit the adaptive reconstruction of the HR image. Specifically, fractional order in the proposed SR model is adaptively set according to the textural feature of the LR image firstly; then, the SR model is separated into two sub-models with each of them containing exactly one regularization parameter. These two sub-models are solved by using alternating direction multiplier method, and two regularization parameters are concurrently updated by using discrepancy principle. Finally, the solutions of two sub-models are interactively averaged to reconstruct HR image. The results of experiments indicated that the proposed hybrid SR model with adaptive regularization parameters has a comparative performance compared with state-of-the-art methods. Moreover, it would be potentially more adaptive for the condition of varied blurred kernels.
C1 [Yang, Xiaomei; Zhang, Jiawei; Liu, Yanan; Zheng, Xiujuan; Liu, Kai] Sichuan Univ, Coll Elect Engn & Informat Technol, Chengdu, Peoples R China.
C3 Sichuan University
RP Zheng, XJ (corresponding author), Sichuan Univ, Coll Elect Engn & Informat Technol, Chengdu, Peoples R China.
EM yangxiaomei@scu.edu.cn; xiujuanzheng@scu.edu.cn
RI Zhang, Jiawei/ABE-8432-2020; yang, xiao/HJI-7815-2023; Liu,
   Kai/IST-6808-2023
OI Zhang, Jiawei/0000-0002-9202-0070; Zheng, Xiujuan/0000-0002-4703-9530
CR Babacan SD, 2009, IEEE T IMAGE PROCESS, V18, P12, DOI 10.1109/TIP.2008.2007354
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Chen CB, 2015, VISUAL COMPUT, V31, P1217, DOI 10.1007/s00371-014-1007-5
   Chen G, 2016, J VIS COMMUN IMAGE R, V38, P407, DOI 10.1016/j.jvcir.2016.03.018
   Chen K, 2014, NUMER ALGORITHMS, V67, P73, DOI 10.1007/s11075-013-9775-y
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   He C, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2360133
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Huang JZ, 2011, MED IMAGE ANAL, V15, P670, DOI 10.1016/j.media.2011.06.001
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jun Z, 2011, APPL MATH MODEL, V35, P2516, DOI 10.1016/j.apm.2010.11.049
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Li F, 2009, J VIS COMMUN IMAGE R, V20, P293, DOI 10.1016/j.jvcir.2009.01.003
   Lin YZ, 2010, SIGNAL PROCESS, V90, P2546, DOI 10.1016/j.sigpro.2010.02.025
   MAELAND E, 1988, IEEE T MED IMAGING, V7, P213, DOI 10.1109/42.7784
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mathieu B, 2003, SIGNAL PROCESS, V83, P2421, DOI 10.1016/S0165-1684(03)00194-4
   Ng MK, 2010, SIAM J SCI COMPUT, V32, P2710, DOI 10.1137/090774823
   Oliveira JP, 2009, SIGNAL PROCESS, V89, P1683, DOI 10.1016/j.sigpro.2009.03.018
   Pu YF, 2010, IEEE T IMAGE PROCESS, V19, P491, DOI 10.1109/TIP.2009.2035980
   Ren ZM, 2013, SIGNAL PROCESS, V93, P2408, DOI 10.1016/j.sigpro.2013.02.015
   Santos ETF, 2007, COMPUT GEOSCI-UK, V33, P618, DOI 10.1016/j.cageo.2006.08.013
   Srivastava R, 2013, PATTERN RECOGN LETT, V34, P1175, DOI 10.1016/j.patrec.2013.03.026
   Tian D, 2015, INFORM SCIENCES, V296, P147, DOI 10.1016/j.ins.2014.10.050
   VAINIKKO GM, 1982, USSR COMP MATH MATH+, V22, P1, DOI 10.1016/0041-5553(82)90120-3
   Wahlberg B., 2010, P 16 IFAC S SYST ID, P83
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen YW, 2012, IEEE T IMAGE PROCESS, V21, P1770, DOI 10.1109/TIP.2011.2181401
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang J, 2014, SIGNAL PROCESS, V98, P381, DOI 10.1016/j.sigpro.2013.12.009
   Zhang J, 2012, J MATH IMAGING VIS, V43, P39, DOI 10.1007/s10851-011-0285-z
   Zhang K., 2017, ARXIV171206116
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
NR 41
TC 8
Z9 8
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1755
EP 1768
DI 10.1007/s00371-018-1570-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300006
DA 2024-07-18
ER

PT J
AU Park, JK
   Kang, DJ
AF Park, Je-Kang
   Kang, Dong-Joong
TI Unified convolutional neural network for direct facial keypoints
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Facial keypoints detection; Face alignment; Unified convolutional neural
   networks; Shared network
ID FACE ALIGNMENT
AB We propose a novel approach to directly estimate the position of the facial keypoints via convolutional neural networks (CNN). Our method estimates the global position and the local positions from a unified CNN and combines them through a simplified optimization process. There are twofolds of advantages for our approach. First, the global geometrical position and the local detailed position of the facial keypoints are combined complementarily to avoid local minimums caused by occlusions and pose variations. Second, unlike the traditional method such as a cascade of multiple CNN, we propose a unified deep and large architecture network consisted by global position network and local position network. Our design shares most of computations for facial features between networks, and this efficient high-level features improves largely to the precise estimate of facial keypoints. We conduct comparative experiments with the state-of-the-art researches and commercial services. In experiments, our approach shows a remarkable performance.
C1 [Park, Je-Kang] Pusan Natl Univ, Grad Sch, Busan, South Korea.
   [Kang, Dong-Joong] Pusan Natl Univ, Sch Mech Engn, Busan, South Korea.
C3 Pusan National University; Pusan National University
RP Kang, DJ (corresponding author), Pusan Natl Univ, Sch Mech Engn, Busan, South Korea.
EM jkp@pusan.ac.kr; djkang@pusan.ac.kr
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2016R1A2B4007608]; National IT Industry Promotion Agency (NIPA) - Korea
   government (MSIT) [S0602-17-1001]; Korea government (MSIT) [C0507460]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (No.
   2016R1A2B4007608), National IT Industry Promotion Agency (NIPA) grant
   funded by the Korea government (MSIT) (No. S0602-17-1001) and Technology
   & Information Promotion Agency for SMEs (TIPA) grant funded by the Korea
   government (MSIT) (No. C0507460).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], LOCATING FACIAL FEAT
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015
   CAO ZM, 2010, PROC CVPR IEEE, P2707, DOI DOI 10.1109/CVPR.2010.5539992
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Ding L, 2014, VISUAL COMPUT, V30, P189, DOI 10.1007/s00371-013-0795-3
   Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu JX, 2013, VISUAL COMPUT, V29, P949, DOI 10.1007/s00371-013-0850-0
   JESORSKY O, 2001, ROBUST FACE DETECTIO, P90, DOI DOI 10.1007/3-540-45344-X_14
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang L, 2008, FACE ALIGNMENT VIA C, P72, DOI DOI 10.1007/978-3-540-88688-4_6
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Saatci Y, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P393
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zhang C, 2014, IEEE WINT CONF APPL, P1036, DOI 10.1109/WACV.2014.6835990
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 30
TC 12
Z9 12
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1615
EP 1626
DI 10.1007/s00371-018-1561-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000009
DA 2024-07-18
ER

PT J
AU Kabbai, L
   Abdellaoui, M
   Douik, A
AF Kabbai, Leila
   Abdellaoui, Mehrez
   Douik, Ali
TI Image classification by combining local and global features
SO VISUAL COMPUTER
LA English
DT Article
DE SURF; BoW; LBP; LTP; Wavelet transform; Image classification
ID TEXTURE CLASSIFICATION; COLOR; DESCRIPTORS; REPRESENTATION; EXTRACTION;
   OBJECT; SIFT
AB Several techniques have recently been proposed to extract the features of an image. Feature extraction is one of the most important steps in various image processing and computer vision applications such as image retrieval, image classification, matching, object recognition. Relevant feature (global or local) contains discriminating information and is able to distinguish one object from others. Global features describe the entire image, whereas local features describe the image patches (small group of pixels). In this paper, we present a novel descriptor to extract the color-texture features via two information types. Our descriptor named concatenation of local and global color features is based on the fusion of global features using wavelet transform and a modified version of local ternary pattern, whereas, for the local features, speeded-up robust feature descriptor and bag of words model were used. All the features are extracted from the three color planes. To evaluate the effectiveness of our descriptor for image classification, we carried out experiments using the challenging datasets: New-BarkTex, Outex-TC13, Outex-TC14, MIT scene, UIUC sports event, Caltech 101 and MIT indoor scene. Experimental results showed that our descriptor outperforms the existing state-of-the-art methods.
C1 [Kabbai, Leila; Douik, Ali] Univ Sousse, Natl Engn Sch Sousse, ENISo, Sousse, Tunisia.
   [Abdellaoui, Mehrez] Univ Kairouan, High Inst Appl Technol Kairouan, Kairouan, Tunisia.
C3 Universite de Sousse; Universite de Kairouan
RP Kabbai, L (corresponding author), Univ Sousse, Natl Engn Sch Sousse, ENISo, Sousse, Tunisia.
EM kabbai.leila@gmail.com; mehrez.abdellaoui@enim.rnu.tn;
   ali.douik@enim.rnu.tn
RI Abdellaoui, Mehrez/AAR-5016-2020
OI DOUIK, Ali/0000-0002-0178-501X
CR Ai DN, 2010, IEICE T INF SYST, VE93D, P2577, DOI 10.1587/transinf.E93.D.2577
   Alvarez S, 2012, PATTERN RECOGN, V45, P4312, DOI 10.1016/j.patcog.2012.04.032
   [Anonymous], IEEE TRANS IMAGE PRO
   [Anonymous], 2017, IEEE SYST J
   [Anonymous], ARXIV14053531 CORR
   [Anonymous], 2010, P NIPS
   [Anonymous], VIS COMPUT
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2016, J THEORETICAL APPL I
   [Anonymous], 2014, IEEE PES T D C EXP C
   [Anonymous], 2010, KU LEUVEN LEUVEN, DOI DOI 10.1007/S10329-011-0272-4
   [Anonymous], 2012, LBP and Color Descriptors for Image Classification BT-Cross Disciplinary Biometric Systems
   Bai S, 2017, MULTIMED TOOLS APPL, V76, P16145, DOI 10.1007/s11042-016-3900-6
   Banerji S, 2013, NEUROCOMPUTING, V117, P173, DOI 10.1016/j.neucom.2013.02.014
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Berbar MA, 2014, VISUAL COMPUT, V30, P19, DOI 10.1007/s00371-013-0774-8
   Bian XY, 2017, IEEE J-STARS, V10, P2889, DOI 10.1109/JSTARS.2017.2683799
   Bo L., 2011, Neural Information Processing Systems, P2115
   Cusano C, 2014, J OPT SOC AM A, V31, P1453, DOI 10.1364/JOSAA.31.001453
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Harris C., 1988, ALVEY VISION C, P147151
   Hu XM, 2017, PROC SPIE, V10322, DOI 10.1117/12.2265595
   Kabbai L, 2017, IET IMAGE PROCESS, V11, P109, DOI 10.1049/iet-ipr.2016.0349
   Kabbai L, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL AND IMAGE PROCESSING (ATSIP), P151, DOI 10.1109/ATSIP.2016.7523086
   Ke Y, 2004, PROC CVPR IEEE, P506
   Khan Rahat, 2013, International Journal of Computer Theory and Engineering, V5, P65, DOI 10.7763/IJCTE.2013.V5.648
   Khan R, 2015, COMPUT VIS IMAGE UND, V132, P102, DOI 10.1016/j.cviu.2014.09.005
   Ledoux A, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.6.061404
   Lee S, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P609, DOI 10.1109/ICNIDC.2009.5360944
   Li L, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.2.023022
   Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Muralidharan R., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P1, DOI 10.1109/ICPRIME.2012.6208278
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Papari G, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/71828
   Porebski A, 2014, MULTIMED TOOLS APPL, V70, P543, DOI 10.1007/s11042-013-1418-8
   Rahman M., 2017, SoutheastCon 2017, P1
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Sandid F, 2016, PATTERN RECOGN LETT, V80, P15, DOI 10.1016/j.patrec.2016.05.010
   Sandid F, 2015, IET IMAGE PROCESS, V9, P634, DOI 10.1049/iet-ipr.2014.0895
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Sinha A, 2014, MACH VISION APPL, V25, P361, DOI 10.1007/s00138-013-0561-6
   Smith JR, 1996, INT CONF ACOUST SPEE, P2239, DOI 10.1109/ICASSP.1996.545867
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Yang J, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010053
   Yu J, 2013, NEUROCOMPUTING, V120, P355, DOI 10.1016/j.neucom.2012.08.061
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu C, 2013, PATTERN RECOGN, V46, P1949, DOI 10.1016/j.patcog.2013.01.003
   Zou JY, 2016, INFORM SCIENCES, V348, P209, DOI 10.1016/j.ins.2016.02.021
NR 56
TC 50
Z9 52
U1 0
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 679
EP 693
DI 10.1007/s00371-018-1503-0
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900006
DA 2024-07-18
ER

PT J
AU Zhou, W
   Ma, CW
   Yao, T
   Chang, P
   Zhang, Q
   Kuijper, A
AF Zhou, Wei
   Ma, Caiwen
   Yao, Tong
   Chang, Peng
   Zhang, Qi
   Kuijper, Arjan
TI Histograms of Gaussian normal distribution for 3D feature matching in
   cluttered scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Local surface patch; Local reference frame; Local feature descriptor;
   Point cloud
ID OBJECT RECOGNITION; SURFACE-FEATURE; IMAGES; REPRESENTATION; SIGNATURES
AB 3D feature descriptors provide essential information to find given models in captured scenes. In practical applications, these scenes often contain clutter. This imposes severe challenges on the 3D object recognition leading to feature mismatches between scenes and models. As such errors are not fully addressed by the existing methods, 3D feature matching still remains a largely unsolved problem. We therefore propose our Histograms of Gaussian Normal Distribution (HGND) for capturing salient feature information on a local reference frame (LRF) that enables us to solve this problem. We define a LRF on each local surface patch by using the eigenvectors of the scatter matrix. Different from the traditional local LRF-based methods, our HGND descriptor is based on the combination of geometrical and spatial information without calculating the distribution of every point and its geometrical information in a local domain. This makes it both simple and efficient. We encode the HGND descriptors in a histogram by the geometrical projected distribution of the normal vectors. These vectors are based on the spatial distribution of the points. We use three public benchmarks, the Bologna, the UWA and the Ca' Foscari Venezia dataset, to evaluate the speed, robustness, and descriptiveness of our approach. Our experiments demonstrate that the HGND is fast and obtains a more reliable matching rate than state-of-the-art approaches in cluttered situations.
C1 [Zhou, Wei; Yao, Tong] Xian Inst Opt & Precis Mech CAS, Xian 710119, Shaanxi, Peoples R China.
   [Ma, Caiwen] Xian Inst Opt & Precis Mech CAS, Signal & Informat Proc, Xian 710119, Shaanxi, Peoples R China.
   [Zhou, Wei; Yao, Tong; Zhang, Qi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Zhou, Wei; Kuijper, Arjan] Tech Univ Darmstadt, Fraunhofer IGD, D-64283 Darmstadt, Germany.
   [Chang, Peng] Northeastern Univ, Elect & Comp Engn, Boston, MA 02115 USA.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Chinese Academy of Sciences; Xi'an Institute of Optics &
   Precision Mechanics, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Technical University of Darmstadt;
   Northeastern University
RP Zhou, W (corresponding author), Xian Inst Opt & Precis Mech CAS, Xian 710119, Shaanxi, Peoples R China.; Zhou, W (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.; Zhou, W (corresponding author), Tech Univ Darmstadt, Fraunhofer IGD, D-64283 Darmstadt, Germany.
EM zhouwei1@opt.cn
RI li, liu/JXN-7328-2024; Zhou, Wei/ABA-9056-2020
OI Zhou, Wei/0000-0001-8328-3736
FU University of Chinese Academy of Sciences (UCAS) Joint PhD Training
   Program [UCAS[2015]37]
FX This work was supported by the University of Chinese Academy of Sciences
   (UCAS) Joint PhD Training Program (UCAS[2015]37).
CR Aldoma A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P585, DOI 10.1109/ICCVW.2011.6130296
   Aldoma A, 2012, IEEE ROBOT AUTOM MAG, V19, P80, DOI 10.1109/MRA.2012.2206675
   [Anonymous], 2008, INTRO PROBABILITY TH
   Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7
   Bariya P, 2010, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2010.5539774
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Berretti S, 2014, VISUAL COMPUT, V30, P1275, DOI 10.1007/s00371-014-0932-7
   Buch AG, 2016, SPRINGERPLUS, V5, DOI 10.1186/s40064-016-1906-1
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   DAROM T, 1973, TIP, V21, P2758, DOI DOI 10.1109/TIP.2012.2183142
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Fan YL, 2018, VISUAL COMPUT, V34, P659, DOI 10.1007/s00371-017-1405-6
   Flint A., 2007, 9 BIENN C AUSTR PATT, P182, DOI [DOI 10.1109/DICTA.2007.4426794, 10.1109/DICTA.2007.4426794]
   Frome A., 2004, EUR C COMP VIS, P224
   Gomes RB, 2013, COMPUT GRAPH-UK, V37, P496, DOI 10.1016/j.cag.2013.03.005
   GUO Y, 2014, TPAMI, V36, P2270, DOI DOI 10.1109/TPAMI.2014.2316828
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2015, INFORM SCIENCES, V293, P196, DOI 10.1016/j.ins.2014.09.015
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Han PF, 2015, COMPUT GRAPH-UK, V50, P36, DOI 10.1016/j.cag.2015.05.021
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Levoy M, 2005, The Stanford 3D scanning repository
   Li ZM, 2016, COMPUT GRAPH-UK, V54, P8, DOI 10.1016/j.cag.2015.07.002
   Lim J, 2019, VISUAL COMPUT, V35, P71, DOI 10.1007/s00371-017-1453-y
   Lo TWR, 2009, COMPUT VIS IMAGE UND, V113, P1235, DOI 10.1016/j.cviu.2009.06.005
   López-Sastre RJ, 2013, COMPUT GRAPH-UK, V37, P473, DOI 10.1016/j.cag.2013.04.003
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Mian A. S, 2009, UWA DATASET 3D MODEL
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Michael B., 2011, ROBUST FEATURE DETEC, P71
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Petrelli A, 2011, IEEE I CONF COMP VIS, P2244, DOI 10.1109/ICCV.2011.6126503
   Redondo-Cabrera C, 2012, PROC CVPR IEEE, P3458, DOI 10.1109/CVPR.2012.6248087
   Rodolà E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280
   Shah SAA, 2016, NEUROCOMPUTING, V205, P1, DOI 10.1016/j.neucom.2015.11.019
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Song R, 2013, VISUAL COMPUT, V29, P695, DOI 10.1007/s00371-013-0806-4
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785
   Taati B, 2011, COMPUT VIS IMAGE UND, V115, P681, DOI 10.1016/j.cviu.2010.11.021
   Tang KK, 2017, IEEE ACCESS, V5, P1833, DOI 10.1109/ACCESS.2017.2658681
   Tombari F., 2010, BOLOGNA DATASET
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI DOI 10.1145/1877808.1877821
   Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Wohlkinger W., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2987, DOI 10.1109/ROBIO.2011.6181760
   Yang JQ, 2017, COMPUT VIS IMAGE UND, V160, P133, DOI 10.1016/j.cviu.2017.02.004
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zabulis X, 2016, VISUAL COMPUT, P1
   Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zhang Z, 2015, KNOWL-BASED SYST, V84, P78, DOI 10.1016/j.knosys.2015.04.003
NR 61
TC 9
Z9 9
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 489
EP 505
DI 10.1007/s00371-018-1478-x
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800003
DA 2024-07-18
ER

PT J
AU Guan, H
   Cheng, BZ
AF Guan, Hao
   Cheng, Baozhong
TI How do deep convolutional features affect tracking performance: an
   experimental study
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Appearance model; Convolutional networks; Deep learning
ID VISUAL TRACKING; OBJECT TRACKING
AB Visual tracking is an import topic in computer vision with many practical applications. Recently, deep learning methods have been introduced into the tracking community to improve tracking performance. How deep features affect tracking performance, however, has not been studied thoroughly. In this paper, we carry out an experimental study to deeply investigate the impact of convolutional features on tracking performance. We adopt the most influential and representative Convolutional Neural Network (CNN) models that are widely used in computer vision to equip our baseline tracking framework. Firstly, we carry out experiments on each CNN to reveal the relationship between tracking performance and different CNN layers. Secondly, we have a vertical comparison of tracking performance between different CNN models. In addition, we explore the effect of ensemble strategies of CNN features on tracking performance. Our work has got several valuable findings on the relationship between tracking performance and convolutional features. Based on our findings, we have derived a few useful guidelines for designing trackers with better performance. We have also developed a simple baseline tracker with the guidelines and it outperforms several state-of-the-art trackers very easily on challenging benchmark video sequences.
C1 [Guan, Hao; Cheng, Baozhong] Beijing Univ Posts & Telecommun, Sch Software Engn, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Guan, H (corresponding author), Beijing Univ Posts & Telecommun, Sch Software Engn, Beijing, Peoples R China.
EM guanhao@bupt.edu.cn; bzcheng@bupt.edu.cn
FU Fundamental Research Funds for the Central Universities [2017RC54]
FX This work was supported by The Fundamental Research Funds for the
   Central Universities (Grant No. 2017RC54).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   [Anonymous], 2014, P IEEE C COMP VIS PA
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar B.V. K. Vijaya., 2005, Correlation Pattern Recognition
   LeCun Y., 2012, INT S CIRC SYST, P1097
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Valmadre J., 2017, P IEEE C COMP VIS PA, P2217
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N, 2013, P ADV NEURAL INFORM
   Wang YW, 2017, VISUAL COMPUT, V33, P235, DOI 10.1007/s00371-015-1189-5
   Wang Z, 2016, VISUAL COMPUT, V32, P307, DOI 10.1007/s00371-015-1067-1
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
NR 46
TC 8
Z9 8
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1701
EP 1711
DI 10.1007/s00371-017-1445-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400007
DA 2024-07-18
ER

PT J
AU Abdurrahim, SH
   Samad, SA
   Huddin, AB
AF Abdurrahim, Salem Hamed
   Samad, Salina Abdul
   Huddin, Aqilah Baseri
TI Review on the effects of age, gender, and race demographics on automatic
   face recognition
SO VISUAL COMPUTER
LA English
DT Review
DE Face recognition; Demographic covariates; Race; Gender; Age
ID OWN; ALGORITHMS; SIMULATION; FEATURES; BIAS
AB The performance of face recognition algorithms is affected by external factors and internal subject characteristics. Identifying these aspects and understanding their behaviors on performance can aid in predicting the performance of algorithms and in designing suitable acquisition settings at prospective locations to enhance performance. Factors that affect the performance of face recognition systems, such as pose, illumination, expression, and image resolution, are recognized as face recognition problems. These are substantially studied, and many algorithms have been developed to tackle these problems. However, the influence of population demographics (i.e., race, age, and gender) on face recognition performance has not received considerable attention. Early findings that deal with demographic influence give conflicting results. The studies conducted in the last decade resolve some of the contentions. Nonetheless, some findings have not reached consensus. Existing reviews on the influence of covariates are either outdated or do not cover the influence of demographic covariates on the performance of face recognition algorithms. This paper gives an intensive and focused review that covers recent research on demographic covariates. The effects of age, gender, and race covariates on face recognition are summarized based on these findings, and suggestions on the future direction of the field are given to have a significant understanding of these effects individually and their interactions with one another.
C1 [Abdurrahim, Salem Hamed; Samad, Salina Abdul; Huddin, Aqilah Baseri] Univ Kebangsaan Malaysia, Dept Elect Elect & Syst Engn, Bangi, Selangor, Malaysia.
C3 Universiti Kebangsaan Malaysia
RP Abdurrahim, SH (corresponding author), Univ Kebangsaan Malaysia, Dept Elect Elect & Syst Engn, Bangi, Selangor, Malaysia.
EM salemeez@siswa.ukm.edu.my; salinasamad@ukm.edu.my; aqilah@ukm.edu.my
CR Akhtar Z, 2013, LECT NOTES COMPUT SC, V8157, P309, DOI 10.1007/978-3-642-41184-7_32
   [Anonymous], IEEE COMP SOC C 2005
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], TECHNICAL REPORT
   [Anonymous], 2009, THESIS
   [Anonymous], EVALUATION METHODOLO
   [Anonymous], ETHNICITY VERSUS RAC
   [Anonymous], FACIAL IMAGE PROCESS
   [Anonymous], IEEE WORKSH 2012 BIO
   [Anonymous], IEEE 3 INT C 2009 BT
   [Anonymous], 2015, COVARIATES FACE RECO
   [Anonymous], 1 IEEE INT C BIOM TH
   Beveridge J. R., 2008, Automatic Face Gesture Recognition, P1
   Beveridge JR, 2010, IMAGE VISION COMPUT, V28, P732, DOI 10.1016/j.imavis.2009.09.005
   Beveridge JR, 2009, COMPUT VIS IMAGE UND, V113, P750, DOI 10.1016/j.cviu.2008.12.007
   CHELLAPPA R, 1995, P IEEE, V83, P705, DOI 10.1109/5.381842
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   CHIRORO P, 1995, Q J EXP PSYCHOL-A, V48, P879, DOI 10.1080/14640749508401421
   Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005
   Farinella G, 2012, 2012 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV), P383, DOI 10.1109/ICIEV.2012.6317383
   Fu SY, 2014, IEEE T PATTERN ANAL, V36, P2483, DOI 10.1109/TPAMI.2014.2321570
   Furl N, 2002, COGNITIVE SCI, V26, P797, DOI 10.1016/S0364-0213(02)00084-8
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Givens G, 2004, PROC CVPR IEEE, P381
   Givens G., 2003, C COMP VIS PATT REC, P96, DOI [DOI 10.1109/CVPRW, 10.1109/CVPRW.2003.10088, DOI 10.1109/CVPRW.2003.10088]
   Gorodnichy D.O., 2009, Encyclopedia of Biometrics, P295
   Gross R, 2005, HANDBOOK OF FACE RECOGNITION, P301, DOI 10.1007/0-387-27257-7_14
   Gross R, 2001, CITESEER
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Grother P. J., 2010, 7709106 NIST, V7709, P106
   Guo G., 2010, IEEE COMP SOC C COMP, P71, DOI DOI 10.1109/CVPRW.2010.5543609
   Guo G., 2010, CVPR Workshop, P79
   Guo GD, 2009, IEEE I CONF COMP VIS, P1986, DOI 10.1109/ICCV.2009.5459438
   Guodong Guo, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3392, DOI 10.1109/ICPR.2010.828
   Han H, 2015, IEEE T PATTERN ANAL, V37, P1148, DOI 10.1109/TPAMI.2014.2362759
   Han H, 2013, INT CONF BIOMETR
   Ho WH, 2007, LECT NOTES COMPUT SC, V4673, P351
   Jia H., 2008, Proc. FG, P1
   Kelly DJ, 2007, PSYCHOL SCI, V18, P1084, DOI 10.1111/j.1467-9280.2007.02029.x
   Lanitis A., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P391, DOI 10.1109/AFGR.2000.840664
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li S.Z., 2005, Handbook of Face Recognition
   Li Y, 2016, VISUAL COMPUT, V32, P1525, DOI 10.1007/s00371-015-1137-4
   LUCKMAN AJ, 1995, NEUROCOMPUTING, V7, P3, DOI 10.1016/0925-2312(93)E0052-F
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mahalingam G, 2012, IMAGE VISION COMPUT, V30, P1052, DOI 10.1016/j.imavis.2012.10.003
   Makwana RM, 2010, PROCEDIA COMPUT SCI, V2, P101, DOI 10.1016/j.procs.2010.11.013
   MALPASS RS, 1969, J PERS SOC PSYCHOL, V13, P330, DOI 10.1037/h0028434
   Martinez A., 1998, AR FACE DATABASE
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   Ngan M., 2015, Face recognition vendor test (frvt)-performance of automated gender classification algorithms, DOI [DOI 10.6028/NIST.IR.8052, https://doi.org/10.6028/NIST.IR.8052]
   O'Toole AJ, 2007, IEEE T PATTERN ANAL, V29, P1642, DOI 10.1109/TPAMI.2007.1107
   O'Toole AJ, 2013, VIS COGN, V21, P1121, DOI 10.1080/13506285.2013.803505
   O'Toole AJ, 2012, IMAGE VISION COMPUT, V30, P169, DOI 10.1016/j.imavis.2011.12.007
   O'Toole AliceJ., 2008, 2008 8th IEEE International Conference on Automatic Face Gesture Recognition, P1
   Otto C, 2012, LECT NOTES COMPUT SC, V7584, P189, DOI 10.1007/978-3-642-33868-7_19
   Panis G, 2016, IET BIOMETRICS, V5, P37, DOI 10.1049/iet-bmt.2014.0053
   Panis G, 2015, LECT NOTES COMPUT SC, V8926, P737, DOI 10.1007/978-3-319-16181-5_56
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Phillips P J., 2003, IEEE Interational Workshop on Analysis and Modeling of Faces and Gestures, page, P44
   Phillips PJ, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870082
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sezer OG, 2006, PROC SPIE, V6077, DOI 10.1117/12.645868
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Wang JY, 2006, INT C PATT RECOG, P913
   Zhang X, 2009, PATTERN RECOGN, V42, P2876, DOI 10.1016/j.patcog.2009.04.017
NR 74
TC 33
Z9 39
U1 2
U2 55
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1617
EP 1630
DI 10.1007/s00371-017-1428-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GV9ZR
UT WOS:000446521700011
DA 2024-07-18
ER

PT J
AU Archer, J
   Leach, G
   Knowles, P
   van Schyndel, R
AF Archer, Jesse
   Leach, Geoff
   Knowles, Pyarelal
   van Schyndel, Ron
TI Hybrid Lighting for faster rendering of scenes with many lights
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Lighting; Deferred; Performance; Real time
AB There is growing interest in rendering scenes with many lights, where scenes typically contain hundreds to thousands of lights. Each light illuminates geometry within a finite extent called a light volume. A key aspect of performance is determining which lights apply to what geometry, and then applying those lights efficiently. We present a GPU-based approach using spatial data structures, binning lights by depth analytically while also taking advantage of hardware rasterization. This improves light binning performance by 3-6. We also present a GPU memory and cache friendly data structure that takes two passes to build, giving 4-10 improved performance when applying lighting and an overall improvement of 1.3-4 for total frametime.
C1 [Archer, Jesse; Knowles, Pyarelal] RMIT Univ, Melbourne, Vic, Australia.
   [Leach, Geoff; van Schyndel, Ron] RMIT Univ, Sch Sci, Melbourne, Vic, Australia.
C3 Royal Melbourne Institute of Technology (RMIT); Royal Melbourne
   Institute of Technology (RMIT)
RP Archer, J (corresponding author), RMIT Univ, Melbourne, Vic, Australia.
EM jesse.archer@rmit.edu.au
CR Bezrati A., 2015, GPU PRO 6 ADV RENDER, P183
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Deering M., 1988, Computer Graphics, V22, P21, DOI 10.1145/378456.378468
   Harada T., 2012, SIGGRAPH ASIA 2012 T, P18
   Harada T., 2012, EUROGRAPHICS 2012 SH, DOI [10.2312/conf/EG2012/short/005-008, DOI 10.2312/CONF/EG2012/SHORT/005-008]
   Kämpe V, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462024
   Knowles P, 2012, OPENGL INSIGHTS, P279
   Maule M., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P134, DOI 10.1109/SIBGRAPI.2012.27
   McGuire M., 2017, Computer Graphics Archive
   ODonnell Y., 2017, P 21 ACM SIGGRAPH S, P1
   Olsson O., 2012, Proceedings of the Fourth ACM SIGGRAPH/Eurographics conference on HighPerformance Graphics, P87
   Olsson O., 2011, Journal of Graphics, GPU, and Game Tools, V15, P235, DOI DOI 10.1080/2151237X.2011.621761
   Olsson O, 2012, SIGGRAPH '12: SPECIAL INTEREST GROUP ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES CONFERENCE, DOI 10.1145/2343045.2343095
   Ortegren K., 2016, GPU PRO 7 ADV RENDER, P43
   Persson E., 2013, SIGGRAPH, V13, P1
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
NR 16
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 853
EP 862
DI 10.1007/s00371-018-1535-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400010
DA 2024-07-18
ER

PT J
AU Ma, CY
   Wang, AN
   Chen, G
   Xu, C
AF Ma, Chunyong
   Wang, Anni
   Chen, Ge
   Xu, Chi
TI Hand joints-based gesture recognition for noisy dataset using nested
   interval unscented Kalman filter with LSTM network
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Gesture recognition; Noisy dataset; NIUKF-LSTM; Hand joints
AB Hand joints-based gesture recognition using a neural network provides excellent performance in hand gesture recognition. However, during the collection of sequential skeletal datasets, joints identification through hand pose estimations usually includes noise and even errors, which often diminish the accuracy of gesture recognition. To promote the availability of hand gesture recognition for such noisy datasets, this paper presents a nested interval unscented Kalman filter (UKF) with long short-term memory (NIUKF-LSTM) network to improve the accuracy of hand gesture recognition from noisy datasets. This nested interval method with the UKF changes the distribution of the sigma points based on two sampling intervals. By considering the information of previous frames, the nested interval method helps the NIUKF-LSTM network revise the noise in the sequential hand skeletal data and improve the recognition accuracy. The experimental results showing the removal of noisy skeletal data from the dynamic hand gesture dataset demonstrate the effectiveness of our NIUKF-LSTM network, which achieves better performance than do other state-of-the-art methods.
C1 [Ma, Chunyong; Wang, Anni; Chen, Ge] Ocean Univ China, Marine Informat Technol Lab, Minist Educ, Qingdao 266100, Peoples R China.
   [Ma, Chunyong; Chen, Ge] Qingdao Natl Lab Marine Sci & Technol, Lab Reg Oceanog & Numer Modeling, Qingdao 266100, Peoples R China.
   [Xu, Chi] China Univ Geosci, Sch Automat, Wuhan 430074, Hubei, Peoples R China.
   [Xu, Chi] Hubei Key Lab Adv Control & Intelligent Automat C, Wuhan 430074, Hubei, Peoples R China.
C3 Ocean University of China; Laoshan Laboratory; China University of
   Geosciences
RP Wang, AN (corresponding author), Ocean Univ China, Marine Informat Technol Lab, Minist Educ, Qingdao 266100, Peoples R China.
EM chunyongma@ouc.edu.cn; wanganni@stu.ouc.edu.cn; gechen@ouc.edu.cn;
   xuchi@cug.edu.cn
FU Fundamental Research Funds for the Central Universities [201762005];
   Qingdao major projects of independent innovation [16-7-1-1-zdzx-xx];
   National Key Scientific Instrument and Equipment Development Projects of
   National Natural Science Foundation of China [41527901]; Qingdao Source
   Innovation Program [17-1-1-6-jch]
FX The authors acknowledge the Fundamental Research Funds for the Central
   Universities (No. 201762005), Qingdao major projects of independent
   innovation (No. 16-7-1-1-zdzx-xx), the National Key Scientific
   Instrument and Equipment Development Projects of National Natural
   Science Foundation of China (No. 41527901) and the Qingdao Source
   Innovation Program (No. 17-1-1-6-jch).
CR [Anonymous], P IEEE S AD SYST SIG
   [Anonymous], CIRCUITS SYST SIGNAL
   [Anonymous], ONLINE HUMAN ACTION
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE APPL COMPUTER V
   [Anonymous], INT C ART NEUR NETW
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 3D HAND GESTURE RECO
   [Anonymous], 2015, DISCRETE DYNAMICS NA
   [Anonymous], IEEE INT C IM PROC
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], J ROBOT
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2012, SUPERVISED SEQUENCE
   Dardas NH, 2011, IEEE T INSTRUM MEAS, V60, P3592, DOI 10.1109/TIM.2011.2161140
   De Smedt Q., 2016, IEEE COMPUTER VISION, P1
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ge LH, 2017, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR.2017.602
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kandepu R, 2008, J PROCESS CONTR, V18, P753, DOI 10.1016/j.jprocont.2007.11.004
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869
   Li R, 2018, MULTIMED TOOLS APPL, V77, P663, DOI 10.1007/s11042-016-4268-3
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Lin J, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P121, DOI 10.1109/HUMO.2000.897381
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Marin G, 2014, IEEE IMAGE PROC, P1565, DOI 10.1109/ICIP.2014.7025313
   Molchanov Pavlo, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163132
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Neverova N, 2015, LECT NOTES COMPUT SC, V8925, P474, DOI 10.1007/978-3-319-16178-5_33
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Sinha A, 2016, PROC CVPR IEEE, P4150, DOI 10.1109/CVPR.2016.450
   Tan DJ, 2016, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR.2016.605
   Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965
   Turner R, 2012, NEUROCOMPUTING, V80, P47, DOI 10.1016/j.neucom.2011.07.029
   Wan CD, 2017, PROC CVPR IEEE, P1196, DOI 10.1109/CVPR.2017.132
   Wetzler A., 2015, P BRIT MACHINE VISIO
   Xu C, 2017, INT J COMPUT VISION, V123, P454, DOI 10.1007/s11263-017-0998-6
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yang H., 2016, P AS C COMP VIS TAIP, P452
   Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 49
TC 40
Z9 44
U1 1
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1053
EP 1063
DI 10.1007/s00371-018-1556-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400027
DA 2024-07-18
ER

PT J
AU Khmag, A
   Al-Haddad, S
   Ramli, AR
   Kalantar, B
AF Khmag, Asem
   Al-Haddad, S. A. R.
   Ramli, Abd Rahman
   Kalantar, Bahareh
TI Single image dehazing using second-generation wavelet transforms and the
   mean vector L2-norm
SO VISUAL COMPUTER
LA English
DT Article
DE Scene segmentation; Mean vector L2-norm; Second-generation wavelet;
   Image recovery
ID DARK CHANNEL; COLOR
AB Single image dehazing remains a seminal area of study in computer vision. Despite the huge number of studies that have addressed haze in a single image, the restoration images have not yet reached a satisfactory level in terms of visual appearance and time complexity burden. In this paper, a novel single image haze removal technique based on edge and fine texture preserving is introduced. To achieve better visual quality from the hazy image, the proposed technique uses mean vector L2-norm that is core of window sampling to estimate the transmission map. Then, second-generation wavelet transform filter is utilized in order to enhance the estimated transmission map of the resulted image. The usage of second-generation wavelet filter in this paper is due to its effectiveness while achieving fast speed. Experimental outcomes present that the proposed technique achieves competitive achievements in comparison with up-to-date state-of-the-art image dehazing methods in both quantitative and qualitative assessments, i.e., visual effects, universality, and computational processing speed.
C1 [Khmag, Asem] Univ Zawia, Fac Engn, Zawiya, Libya.
   [Al-Haddad, S. A. R.; Ramli, Abd Rahman; Kalantar, Bahareh] Univ Putra Malaysia, Fac Engn, Serdang, Malaysia.
C3 Universiti Putra Malaysia
RP Khmag, A (corresponding author), Univ Zawia, Fac Engn, Zawiya, Libya.
EM khmaj2002@gmail.com; sar@upm.my; arr@upm.edu.my; bahare_kgh@yahoo.com
RI Khmag, Asem/AAH-1051-2019; Al-Haddad, S. A. R./AAM-6449-2020
OI Khmag, Asem/0000-0002-1360-5346; Kalantar, Bahareh/0000-0002-2822-3463
CR Ancuti Codruta O., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P501, DOI 10.1007/978-3-642-19309-5_39
   [Anonymous], INT REV COMPUT SOFTW
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], SPIE DEFENSE SECURIT
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Choi LK, 2014, IEEE SW SYMP IMAG, P165, DOI 10.1109/SSIAI.2014.6806055
   Cozman F, 1997, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.1997.609419
   Dong XM, 2010, IEEE IMAGE PROC, P3593, DOI 10.1109/ICIP.2010.5651965
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gibson KB, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-37
   Gibson KB, 2011, IEEE IMAGE PROC, P1861, DOI 10.1109/ICIP.2011.6115830
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Hautiere N., 2007, 2007 IEEE C COMP VIS, P1
   Khmag A, 2016, IEEJ T ELECTR ELECTR, V11, P339, DOI 10.1002/tee.22223
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Li B, 2014, IET COMPUT VIS, V8, P131, DOI 10.1049/iet-cvi.2013.0011
   Meng D, 2015, OPTIK, V126, P3522, DOI 10.1016/j.ijleo.2015.08.220
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Omer I, 2004, PROC CVPR IEEE, P946
   Qi M, 2015, OPTIK, V126, P3400, DOI 10.1016/j.ijleo.2015.07.114
   Schaul L, 2009, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2009.5413700
   Shi ZW, 2014, OPTIK, V125, P3868, DOI 10.1016/j.ijleo.2014.01.170
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Tan R. T., IEEE C COMPUTER VISI, P1
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang JB, 2015, NEUROCOMPUTING, V149, P718, DOI 10.1016/j.neucom.2014.08.005
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   XU YS, 1994, IEEE T IMAGE PROCESS, V3, P747, DOI 10.1109/83.336245
   Yoon I, 2012, IEEE T CONSUM ELECTR, V58, P111, DOI 10.1109/TCE.2012.6170062
   Zhang Y, 2012, ELIFE, V1, DOI 10.7554/eLife.00065
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 36
TC 30
Z9 30
U1 4
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 675
EP 688
DI 10.1007/s00371-017-1406-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100007
DA 2024-07-18
ER

PT J
AU Song, R
   Liu, YH
   Martin, RR
   Echavarria, KR
AF Song, Ran
   Liu, Yonghuai
   Martin, Ralph R.
   Echavarria, Karina Rodriguez
TI Local-to-global mesh saliency
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh saliency; Laplacian; Global distinctness
ID VISUAL-ATTENTION; SPARSE; MODEL; SHAPE
AB As a measure of regional importance in agreement with human perception of 3D shape, mesh saliency should be based on local geometric information within a mesh but more than that. Recent research has shown that global consideration has a significant role in mesh saliency. This paper proposes a local-to-global framework for computing mesh saliency where we offer novel solutions to solve three inherent problems: (1) an algorithm based on statistic Laplacian which does not only compute local saliency, but also facilitates the later computation of global saliency; (2) a local-to-global method based on pooling and global distinctness to compute global saliency; (3) a framework to integrate local and global saliency. Experiments demonstrate that our approach can effectively detect salient features consistent with human perceptual interest. We also provide comparisons to existing state-of-the-art methods for mesh saliency and show improved results produced by our method.
C1 [Song, Ran; Echavarria, Karina Rodriguez] Univ Brighton, Sch Comp Engn & Math, Brighton, E Sussex, England.
   [Liu, Yonghuai] Aberystwyth Univ, Dept Comp Sci, Aberystwyth, Dyfed, Wales.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 University of Brighton; Aberystwyth University; Cardiff University
RP Song, R (corresponding author), Univ Brighton, Sch Comp Engn & Math, Brighton, E Sussex, England.
EM r.song@brighton.ac.uk
RI Martin, Ralph R/D-2366-2010; Liu, Yonghuai/ABF-3794-2020; Echavarria,
   Karina Rodriguez/AAF-5093-2020
OI Echavarria, Karina Rodriguez/0000-0002-8679-1602; Martin,
   Ralph/0000-0002-8495-8536; Song, Ran/0000-0002-1344-4415
FU EPSRC [EP/L006685/1]; EPSRC [EP/L006685/1] Funding Source: UKRI
FX This work is partly funded by EPSRC via the 'Automatic Semantic Analysis
   of 3D Content in Digital Repositories' project (EP/L006685/1). This
   support is gratefully acknowledged.
CR [Anonymous], P SIGGRAPH
   [Anonymous], P SIGGRAPH
   [Anonymous], MIT Saliency Benchmark
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dey TK, 2010, PROC APPL MATH, V135, P650
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Guy G, 1997, IEEE T PATTERN ANAL, V19, P1265, DOI 10.1109/34.632985
   Hou TB, 2013, IEEE T VIS COMPUT GR, V19, P3, DOI 10.1109/TVCG.2012.111
   Howard IP., 2002, SEEING DEPTH
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Koch C, 1999, NAT NEUROSCI, V2, P9, DOI 10.1038/4511
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lee SM, 2005, COLOR RES APPL, V30, P265, DOI 10.1002/col.20122
   Leifman G., 2012, P CVPR ORAL
   Mantiuk R., 2003, PROCS 19 SPRING C CO, P239
   Matlin M. W., 2013, COGNITION TXB
   Newman M. E. J., 2008, NEW PALGRAVE ENCY EC, V2nd
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pelphrey KA, 2002, J AUTISM DEV DISORD, V32, P249, DOI 10.1023/A:1016374617369
   Perron O, 1907, MATH ANN, V64, P248, DOI 10.1007/BF01449896
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 36
TC 15
Z9 20
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 323
EP 336
DI 10.1007/s00371-016-1334-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, HL
   Jiang, L
   Jin, XG
   Du, H
   Li, XJ
AF Zhao, Hanli
   Jiang, Lei
   Jin, Xiaogang
   Du, Hui
   Li, Xujie
TI Constant time texture filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Edge-preserving smoothing; Texture smoothing; Bilateral filter; Median
   filter
AB We present a novel texture-filtering method which can effectively separate the main image structures from textures even with high variations. Our nonlinear image decomposition is based on a variant of the weighted-median filter which incorporates structure and texture information into the guidance image. To guarantee effective texture filtering, the guidance image not only contains prominent structure edges of the input, but also reduces the contrast in texture regions. We develop a constant time algorithm for the generation of guidance image by formulating the calculation of local extrema as a histogram volume aggregation problem. The local nature of the algorithm enables an efficient parallel GPU implementation. In addition, we demonstrate the effectiveness of our texture-filtering method in the context of detail enhancement, JPEG artifact removal, inverse halftoning, image segmentation, edge detection, and image stylization.
C1 [Zhao, Hanli; Jiang, Lei; Li, Xujie] Wenzhou Univ, Intelligent Informat Syst Inst, Wenzhou, Peoples R China.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Du, Hui] Zhejiang Univ Media & Commun, New Media Coll, Hangzhou, Zhejiang, Peoples R China.
C3 Wenzhou University; Zhejiang University; Communication University of
   Zhejiang
RP Zhao, HL (corresponding author), Wenzhou Univ, Intelligent Informat Syst Inst, Wenzhou, Peoples R China.
EM hanlizhao@wzu.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [LY15F020019,
   LQ14F020006]; Science and Technology Planning Project of Wenzhou, China
   [G20150019]; Open Project of the State Key Lab of CAD&CG, Zhejiang
   University [A1610, A1510]; National Natural Science Foundation of China
   [61472351]; Scientific Research Fund of Zhejiang Provincial Education
   Department, China [Y201534269]; Higher Education Class Teaching Reform
   Project of Zhejiang Province [kg2015283]; NVIDIA Corporation
FX We would like to thank our anonymous reviewers for their constructive
   suggestions and comments which definitely improve the paper. This paper
   was supported by the Zhejiang Provincial Natural Science Foundation of
   China (Grant Nos. LY15F020019 and LQ14F020006), the Science and
   Technology Planning Project of Wenzhou, China (Grant No. G20150019), and
   the Open Project of the State Key Lab of CAD&CG, Zhejiang University
   (Grant Nos. A1610 and A1510). X. Jin was supported by the National
   Natural Science Foundation of China (Grant No. 61472351). H. Du was
   partially supported by the Scientific Research Fund of Zhejiang
   Provincial Education Department, China (Grant No. Y201534269) and the
   Higher Education Class Teaching Reform Project of Zhejiang Province
   (Grant No. kg2015283). We gratefully acknowledge the support of NVIDIA
   Corporation with the donation of the GeForce GTX Titan X GPU used for
   this research.
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Du H, 2016, VISUAL COMPUT, V32, P1537, DOI 10.1007/s00371-015-1138-3
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   He K., ARXIV50500996
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kopf J., 2012, ACM T GRAPHIC, V31
   Kopf J., 2007, ACM T GRAPHIC, V26
   Kyprianidis J. E., 2008, P EG UK THEOR PRACT, P51
   Liu CX, 2016, VISUAL COMPUT, V32, P911, DOI 10.1007/s00371-016-1259-3
   Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Subr K., 2009, ACM T GRAPHIC, V28
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xu L., ACM T GRAPH, V31
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yin WT, 2005, LECT NOTES COMPUT SC, V3752, P73
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhao HL, 2008, VISUAL COMPUT, V24, P727, DOI 10.1007/s00371-008-0254-8
NR 26
TC 7
Z9 7
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 83
EP 92
DI 10.1007/s00371-016-1315-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200009
DA 2024-07-18
ER

PT J
AU Xie, NW
   Wang, LL
   Popescu, V
AF Xie, Naiwen
   Wang, Lili
   Popescu, Voicu
TI Non-redundant rendering for efficient multi-view scene discretization
SO VISUAL COMPUTER
LA English
DT Article
DE Scene sampling; Sampling redundancy; Non-redundant sampling
AB A powerful approach for managing scene complexity is to sample the scene with a set of images. However, conventional images from nearby viewpoints have a high level of redundancy, which reduces scene sampling efficiency. We present non-redundant rendering, which detects and avoids redundant samples as the image is computed. We show that non-redundant rendering leads to improved scene sampling quality according to several view-independent and view-dependent metrics, compared to conventional scene discretization using redundant images and compared to depth peeling. Non-redundant images have a higher degree of fragmentation and, therefore, conventional approaches for scene reconstruction from samples are ineffective. We present a novel reconstruction approach that is well suited to scene discretization by non-redundant rendering. Finally, we apply non-redundant rendering and scene reconstruction techniques to soft shadow rendering where we show that our approach has an accuracy advantage over conventional images and over depth peeling.
C1 [Xie, Naiwen; Wang, Lili] Beihang Univ, Beijing, Peoples R China.
   [Popescu, Voicu] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Beihang University; Purdue University System; Purdue University
RP Xie, NW (corresponding author), Beihang Univ, Beijing, Peoples R China.
EM sienaiwun@gmail.com
RI wang, lili/HJP-8047-2023
FU National Natural Science Foundation of China [61272349, 61190121,
   61190125]; National High Technology Research and Development Program of
   China through 863 Program [2013AA01A604]; China Scholarship Council
   (CSC) [201506020037]
FX This work was supported in part by the National Natural Science
   Foundation of China through Projects 61272349, 61190121 and 61190125, by
   the National High Technology Research and Development Program of China
   through 863 Program No. 2013AA01A604. Naiwen Xie gratefully acknowledges
   financial support from China Scholarship Council (CSC) through No.
   201506020037.
CR Agarwala A, 2006, ACM T GRAPHIC, V25, P853, DOI 10.1145/1141911.1141966
   Bavoil L., 2008, Order Independent Transparency with Dual Depth Peeling, P1
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571
   Eisemann E, 2007, COMPUT GRAPH FORUM, V26, P202, DOI 10.1111/j.1467-8659.2007.01013.x
   Everitt C., 2001, WHITE PAPER NVIDIA, V2, P7
   Liu BQ, 2009, INT C COMP AID DES C, P452, DOI 10.1109/CADCG.2009.5246861
   Maciel P. W. C., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P95, DOI 10.1145/199404.199420
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Max N, 1995, SPRING COMP SCI, P74
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Mei CH, 2005, COMPUT GRAPH FORUM, V24, P335, DOI 10.1111/j.1467-8659.2005.00858.x
   Nguyen KT, 2010, VISUAL COMPUT, V26, P1497, DOI 10.1007/s00371-010-0507-1
   Niessner M, 2010, VISUAL COMPUT, V26, P679, DOI 10.1007/s00371-010-0486-2
   NVIDIA, OPT TM RAY TRAC ENG
   Rademacher P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P199, DOI 10.1145/280814.280871
   Rom'an A., 2006, RENDERING TECHNIQUES, V2, P161
   Román A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Wilson A, 2003, ACM T GRAPHIC, V22, P678, DOI 10.1145/882262.882325
   Wonka P, 2006, ACM T GRAPHIC, V25, P494, DOI 10.1145/1141911.1141914
   Yu JY, 2004, LECT NOTES COMPUT SC, V3022, P14
NR 22
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1555
EP 1569
DI 10.1007/s00371-016-1300-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600006
DA 2024-07-18
ER

PT J
AU Xia, SH
   Su, L
   Fei, XY
   Wang, H
AF Xia, Shihong
   Su, Le
   Fei, Xinyu
   Wang, Han
TI Toward accurate real-time marker labeling for live optical motion
   capture
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Motion capture; Marker labeling; Graph matching; Point correspondence
ID REGISTRATION
AB Marker labeling plays an important role in optical motion capture pipeline especially in real-time applications; however, the accuracy of online marker labeling is still unclear. This paper presents a novel accurate real-time online marker labeling algorithm for simultaneously dealing with missing and ghost markers. We first introduce a soft graph matching model that automatically labels the markers by using Hungarian algorithm for finding the global optimal matching. The key idea is to formulate the problem in a combinatorial optimization framework. The objective function minimizes the matching cost, which simultaneously measures the difference of markers in the model and data graphs as well as their local geometrical structures consisting of edge constraints. To achieve high subsequent marker labeling accuracy, which may be influenced by limb occlusions or self-occlusions, we also propose an online high-quality full-body pose reconstruction process to estimate the positions of missing markers. We demonstrate the power of our approach by capturing a wide range of human movements and achieve the state-of-the-art accuracy by comparing against alternative methods and commercial system like VICON.
C1 [Xia, Shihong; Su, Le; Fei, Xinyu; Wang, Han] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Su, Le; Fei, Xinyu; Wang, Han] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xia, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM xsh@ict.ac.cn
CR Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   [Anonymous], 1981, Practical Optimization
   [Anonymous], J BONE JOINT SURG
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2007, Computer Vision and Pattern Recognition (CVPR)
   [Anonymous], 10 EUR C COMP VIS EC
   Aristidou A, 2013, VISUAL COMPUT, V29, P7, DOI 10.1007/s00371-011-0671-y
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Burke M, 2016, J BIOMECH, V49, P1854, DOI 10.1016/j.jbiomech.2016.04.016
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228
   Cour T., 2006, Advances in Neural Information Processing Systems, V19, P313, DOI [DOI 10.7551/MITPRESS/7503.003.0044, 10.7551/mitpress/7503.003.0044]
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lee JH, 2011, IEEE T PATTERN ANAL, V33, P427, DOI 10.1109/TPAMI.2010.179
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li B, 2008, PATTERN RECOGN, V41, P418, DOI 10.1016/j.patcog.2007.06.002
   Li BH, 2004, IEEE T SYST MAN CY B, V34, P1412, DOI 10.1109/TSMCB.2004.825914
   Lia BH, 2005, PATTERN RECOGN, V38, P2391, DOI 10.1016/j.patcog.2005.03.004
   Lou H, 2010, IEEE T VIS COMPUT GR, V16, P870, DOI 10.1109/TVCG.2010.23
   Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8
   Nguyen Nam., 2010, Proceedings of the SCA 2010, P189
   RANGARAJAN K, 1991, CVGIP-IMAG UNDERSTAN, V54, P56, DOI 10.1016/1049-9660(91)90075-Z
   Sahillioglu Y, 2010, PROC CVPR IEEE, P453, DOI 10.1109/CVPR.2010.5540178
   SETHI IK, 1987, IEEE T PATTERN ANAL, V9, P56, DOI 10.1109/TPAMI.1987.4767872
   Veenman CJ, 2001, IEEE T PATTERN ANAL, V23, P54, DOI 10.1109/34.899946
   Xia SH, 2017, J COMPUT SCI TECH-CH, V32, P536, DOI 10.1007/s11390-017-1742-y
   Yu Q, 2007, COMPUT GRAPH FORUM, V26, P477, DOI 10.1111/j.1467-8659.2007.01070.x
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   ZHAO JM, 1994, ACM T GRAPHIC, V13, P313, DOI 10.1145/195826.195827
   Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81
NR 31
TC 5
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 993
EP 1003
DI 10.1007/s00371-017-1400-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800029
DA 2024-07-18
ER

PT J
AU Wang, B
   Guo, JC
   Zhang, Y
   Li, CY
AF Wang, Bo
   Guo, Jichang
   Zhang, Yan
   Li, Chongyi
TI Hierarchical feature concatenation-based kernel sparse representations
   for image categorization
SO VISUAL COMPUTER
LA English
DT Article
DE Image categorization; Kernel sparse representation; Feature
   concatenation; Hierarchical learning; Linear support vector machine
ID VISUAL OBJECT RECOGNITION; CLASSIFICATION; RETRIEVAL; LEVEL; SCALE;
   SCENE
AB In order to obtain improved performance in complicated visual categorization tasks, considerable research has adopted multiple kernel learning based on dozens of different features. However, it is a complex process that needs to extract a multitude of features and seeks the optimal combination of multiple kernels. Inspired by the key idea of hierarchical learning, in this paper, we propose to find sparse representation based on feature concatenation using hierarchical kernel orthogonal matching pursuit (HKOMP). In addition to commonly used spatial pyramid feature for kernel representation, our method only employs one type of generic image feature, i.e., p.d.f gradient-based orientation histogram for concatenation of sparse codes. Next, the resulting concatenated features kernelized with widely used Gaussian radial basis kernel function form compact sparse representations in the second layer for linear support vector machine. HKOMP algorithm combines the advantages of building image representations layer-by-layer and kernel learning. Several publicly available image datasets are used to evaluate the presented approach and empirical results for various datasets show that the proposed scheme outperforms many kernel learning based and other competitive image categorization algorithms.
C1 [Wang, Bo; Guo, Jichang; Zhang, Yan; Li, Chongyi] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Wang, B (corresponding author), Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
EM neuwb@tju.edu.cn
RI Guo, Jichang/GQY-5798-2022
OI Guo, Jichang/0000-0003-3130-1685
FU National Program on Key Basic Research Project [2014CB340403]; Natural
   Science Foundation of Tianjin [15JCYBJC15500]
FX This project is supported by the National Program on Key Basic Research
   Project (No. 2014CB340403) and Natural Science Foundation of Tianjin
   (No. 15JCYBJC15500). The authors would like to be grateful to the
   editors' and reviewers' valuable comments which improved the quality of
   this paper.
CR Aiolli F, 2015, NEUROCOMPUTING, V169, P215, DOI 10.1016/j.neucom.2014.11.078
   [Anonymous], 2007, Computer Vision
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383197
   [Anonymous], VISUAL COMPUTER
   [Anonymous], 2007, P 24 INT C MACH LEAR, DOI DOI 10.1145/1273496.1273594
   Berg AC, 2001, PROC CVPR IEEE, P607
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Bo Liefeng, ADV NEURAL INFORM PR, P2115
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   Chiang CK, 2013, IEEE T IMAGE PROCESS, V22, P4775, DOI 10.1109/TIP.2013.2277825
   Gao SH, 2013, IEEE T IMAGE PROCESS, V22, P423, DOI 10.1109/TIP.2012.2215620
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Guan T, 2015, ACM T INTEL SYST TEC, V7, DOI 10.1145/2795234
   Guan T, 2013, IEEE T MULTIMEDIA, V15, P1688, DOI 10.1109/TMM.2013.2265674
   Han YN, 2014, IEEE T CYBERNETICS, V44, P137, DOI 10.1109/TCYB.2013.2248710
   Han YN, 2012, IEEE T SYST MAN CY B, V42, P827, DOI 10.1109/TSMCB.2011.2179291
   Hanxi Li, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P72, DOI 10.1109/DICTA.2011.20
   Nguyen HV, 2013, IEEE T IMAGE PROCESS, V22, P5123, DOI 10.1109/TIP.2013.2282078
   Nguyen HV, 2012, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2012.6288305
   Jain Ashesh., 2012, KDD, P750
   Ji RR, 2014, IEEE T GEOSCI REMOTE, V52, P1811, DOI 10.1109/TGRS.2013.2255297
   Jian M, 2013, IEEE T SIGNAL PROCES, V61, P4416, DOI 10.1109/TSP.2013.2271479
   Kobayashi T, 2013, PROC CVPR IEEE, P747, DOI 10.1109/CVPR.2013.102
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Liu BY, 2014, INT C PATT RECOG, P4293, DOI 10.1109/ICPR.2014.736
   Liu X, 2014, VISUAL COMPUT, V30, P5, DOI 10.1007/s00371-012-0772-2
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Shrivastava A, 2014, IEEE T IMAGE PROCESS, V23, P3013, DOI 10.1109/TIP.2014.2324290
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Thiagarajan JJ, 2014, IEEE T IMAGE PROCESS, V23, P2905, DOI 10.1109/TIP.2014.2322938
   Tuytelaars T, 2010, PROC CVPR IEEE, P2281, DOI 10.1109/CVPR.2010.5539911
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Wang LF, 2014, IEEE T CIRC SYST VID, V24, P1132, DOI 10.1109/TCSVT.2014.2302496
   Wang P, 2013, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2013.368
   Wu JX, 2009, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2009.5459178
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yan SY, 2015, IEEE T CYBERNETICS, V45, P395, DOI 10.1109/TCYB.2014.2326596
   Yang JJ, 2012, IEEE T IMAGE PROCESS, V21, P2838, DOI 10.1109/TIP.2012.2183139
   Yin J, 2012, NEUROCOMPUTING, V77, P120, DOI 10.1016/j.neucom.2011.08.018
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Zhang L, 2014, VISUAL COMPUT, V30, P1359, DOI 10.1007/s00371-013-0891-4
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3241, DOI 10.1109/TIP.2014.2328894
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhao YQ, 2015, IEEE T GEOSCI REMOTE, V53, P296, DOI 10.1109/TGRS.2014.2321557
   Zhao ZQ, 2012, IEEE T IMAGE PROCESS, V21, P4218, DOI 10.1109/TIP.2012.2197631
   Zheng JW, 2015, VISUAL COMPUT, V31, P643, DOI 10.1007/s00371-014-0991-9
NR 61
TC 5
Z9 5
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 647
EP 663
DI 10.1007/s00371-016-1215-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300009
DA 2024-07-18
ER

PT J
AU Zuo, Q
   Qi, Y
AF Zuo, Qing
   Qi, Yue
TI A novel spatial-temporal optical flow method for estimating the velocity
   fields of a fluid sequence
SO VISUAL COMPUTER
LA English
DT Article
DE Optical flow; Velocity estimation; Spatial-temporal; Fluid sequence;
   Navier-Stokes equations
ID OPTIMIZATION
AB In this paper, we propose a novel optical flow method to estimate the velocity fields of fluid sequences based on spatial-temporal physical principles. Our novel optical flow model mainly takes scalar field correspondence, velocity field temporal coherence and incompressibility into consideration. And the velocity field smoothness assumption is also used for better convergence. Compared with existing methods which only estimate the velocity field between a single pair of frames, our novel optical flow model can deal with the fields of all frames of the sequence simultaneously. For some frames of the sequence, the image quality is higher and more accurate velocity fields can be obtained, but for other frames, the estimated fields may be rather far from the groundtruth due to the lack of information. And our model can propagate the correspondence information from the accurate frames to neighbor frames and help the optimization converge to a better result. Also, several sophisticated optimization techniques are employed to solve our novel model efficiently and robustly.
C1 [Zuo, Qing; Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Zuo, Q (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM zuoqing1988@aliyun.com
RI qi, yue/KLE-0386-2024
FU National Key Technology Research and Development Program of China
   [2014BAK18B01]; National Natural Science Foundation of China [61272348,
   61202235]; Ph.D. Program Foundation of Ministry of Education of China
   [20111102110018]
FX This paper was supported by the National Key Technology Research and
   Development Program of China (No. 2014BAK18B01), National Natural
   Science Foundation of China (Nos. 61272348, 61202235), Ph.D. Program
   Foundation of Ministry of Education of China (No. 20111102110018).
CR Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Corpetti T, 2006, EXP FLUIDS, V40, P80, DOI 10.1007/s00348-005-0048-y
   Corpetti T, 2002, IEEE T PATTERN ANAL, V24, P365, DOI 10.1109/34.990137
   Corpetti T, 2000, INT C PATT RECOG, P1033, DOI 10.1109/ICPR.2000.903722
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Gregson J., ACM T GR, V31, P7
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   Gupta SN, 1996, IEEE SIGNAL PROC LET, V3, P32, DOI 10.1109/97.484208
   Hawkins T, 2005, ACM T GRAPHIC, V24, P812, DOI 10.1145/1073204.1073266
   Heitz D, 2008, EXP FLUIDS, V45, P595, DOI 10.1007/s00348-008-0567-4
   Herlin I, 2012, LECT NOTES COMPUT SC, V7575, P15, DOI 10.1007/978-3-642-33765-9_2
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu Y, 2010, SCI CHINA INFORM SCI, V53, P1141, DOI 10.1007/s11432-010-0104-y
   Ihrke Ivo, 2004, P ACM SIGGRAPHEUROGR, P365, DOI DOI 10.1145/1028523.1028572
   Kadri-Harouna S, 2013, INT J COMPUT VISION, V103, P80, DOI 10.1007/s11263-012-0595-7
   Ke Y, 2004, PROC CVPR IEEE, P506
   KEANE RD, 1992, APPL SCI RES, V49, P191, DOI 10.1007/BF00384623
   KEANE RD, 1990, MEAS SCI TECHNOL, V1, P1202, DOI 10.1088/0957-0233/1/11/013
   Konstantinidis E, 2012, LECT NOTES COMPUT SC, V7203, P589, DOI 10.1007/978-3-642-31464-3_60
   Memin E, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P933, DOI 10.1109/ICCV.1998.710828
   Okabe M, 2011, COMPUT GRAPH FORUM, V30, P1973, DOI 10.1111/j.1467-8659.2011.02062.x
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Pighin Frederic., 2004, SCA'04: Proceedings ofthe 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P223, DOI 10.1145/1028523.1028552
   Ruhnau P, 2007, EXP FLUIDS, V42, P61, DOI 10.1007/s00348-006-0220-z
   Ruhnau P, 2007, MEAS SCI TECHNOL, V18, P755, DOI 10.1088/0957-0233/18/3/027
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Volz S, 2011, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2011.6126359
   Wang H, 2009, J OPTOELECTRON BIOME, V1, P1
   Yuan J, 2007, J MATH IMAGING VIS, V28, P67, DOI 10.1007/s10851-007-0014-9
NR 32
TC 4
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 293
EP 302
DI 10.1007/s00371-015-1195-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300004
DA 2024-07-18
ER

PT J
AU Wang, YW
   Chen, HC
   Li, SM
   Zhang, JP
   Gao, C
AF Wang, Yawen
   Chen, Hongchang
   Li, Shaomei
   Zhang, Jianpeng
   Gao, Chao
TI Object tracking by color distribution fields with adaptive hierarchical
   structure
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Distribution field; Appearance modeling; Simulated
   annealing
ID GRADIENTS; MODELS
AB The essence of visual tracking is to distinguish the target from background, so how to describe the difference between target and background is a key problem. In this paper, tracking algorithm by color distribution fields with adaptive hierarchical structure is presented to solve this problem. First, multichannel color distribution fields are presented for appearance modeling, which represents color distinction between the target and background. Second, in order to adapt to the individuality of each target, the hierarchical structure of its color distribution fields are generated via k-means cluster. Third, weighted multichannel distance is used to measure the similarity between the candidate region and the template; the weight of each channel is adjusted online according to its discrimination. Finally, a search strategy based on simulated annealing is proposed to improve the search efficiency and reduce the probability of falling into the local optimum. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art tracking algorithms.
C1 [Wang, Yawen] Natl Digital Switching Syst Engn & Technol R& Ctr, Commun & Informat Syst, Zhengzhou, Peoples R China.
   [Chen, Hongchang; Li, Shaomei; Zhang, Jianpeng; Gao, Chao] Natl Digital Switching Syst Engn & Technol R& Ctr, Zhengzhou, Peoples R China.
C3 PLA Information Engineering University; PLA Information Engineering
   University
RP Wang, YW (corresponding author), Natl Digital Switching Syst Engn & Technol R& Ctr, Commun & Informat Syst, Zhengzhou, Peoples R China.
EM 15738321455@163.com
RI chen, yijia/KGM-4378-2024; SUN, Ye/KBC-8159-2024
FU National Nature Science Foundation of China [61379151, 61521003]; Henan
   Province Science Found for Distinguished Young Scholars of China
   [14410051-0001]; Ministry of Science and Technology of China
   [2014B-AH30B01]
FX This work was supported by the National Nature Science Foundation of
   China (Nos. 61379151, 61521003), the Henan Province Science Found for
   Distinguished Young Scholars of China (No. 14410051-0001), the National
   Key Technology Research and Development Program of the Ministry of
   Science and Technology of China (No. 2014B-AH30B01).
CR [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], ARXIV150104587
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Birchfield S, 1998, PROC CVPR IEEE, P232, DOI 10.1109/CVPR.1998.698614
   Burghouts GJ, 2009, COMPUT VIS IMAGE UND, V113, P48, DOI 10.1016/j.cviu.2008.07.003
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Elgammal A.M., 2000, P EUR C COMP VIS
   Fan ZM, 2005, PROC CVPR IEEE, P502
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Hager GD, 2004, PROC CVPR IEEE, P790
   Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He S., 2013, P IEEE INT C COMP VI
   Jia X., 2012, P IEEE INT C COMP VI
   Kalal Z., 2010, PROCEEDINGS OF THE I
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kwon J., 2010, P IEEE INT C COMP VI
   Kwon J, 2014, IEEE T PATTERN ANAL, V36, P1428, DOI 10.1109/TPAMI.2013.213
   Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ning JF, 2013, IMAGE VISION COMPUT, V31, P853, DOI 10.1016/j.imavis.2013.09.003
   Pernici F, 2014, IEEE T PATTERN ANAL, V36, P2538, DOI 10.1109/TPAMI.2013.250
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Wang ZL, 2015, NANO ENERGY, V14, P1, DOI 10.1016/j.nanoen.2015.01.011
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie CJ, 2014, J VIS COMMUN IMAGE R, V25, P423, DOI 10.1016/j.jvcir.2013.12.012
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zheng-Biao LI, 2014, Matéria (Rio J.), V19, P1
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 37
TC 6
Z9 6
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 235
EP 247
DI 10.1007/s00371-015-1189-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400010
DA 2024-07-18
ER

PT J
AU Wu, H
   Zou, BJ
   Zhao, YQ
   Guo, JJ
AF Wu, Hui
   Zou, Beiji
   Zhao, Yu-qian
   Guo, Jianjing
TI Scene text detection using adaptive color reduction, adjacent character
   model and hybrid verification strategy
SO VISUAL COMPUTER
LA English
DT Article
DE Scene text detection; Adaptive color reduction; Adjacent character
   model; Hybrid text verification strategy
ID SEGMENTATION; TRANSFORM; IMAGES; LOCALIZATION; RECOGNITION; DESCRIPTOR
AB Text detection is a primary task for text recognition and understanding, which can be used in many image analysis techniques. In this paper, we propose an effective scene text detection method including three major steps: connected components (CCs) extraction, character-linking and text/non-text classification. First, for CCs extraction, we design an adaptive color reduction scheme by analyzing image color histogram, which reasonably selects color centers and generates unfixed number of color layers for images in different color complexities. Then, for character-linking, an adjacent character model is built by training an extreme learning machine (ELM), instead of setting various thresholds in previous approaches. Finally, a hybrid text verification strategy is adopted, combining convolutional neural network with ELM for text/non-text classification and performing better than just using one of them. Experimental results on some publicly available datasets illustrate the effectiveness of our method and comparative results with some state-of-the-art algorithms demonstrate our competitiveness.
C1 [Wu, Hui; Zou, Beiji; Zhao, Yu-qian; Guo, Jianjing] Cent South Univ, Sch Informat Sci & Engn, Changsha 410083, Hunan, Peoples R China.
   [Wu, Hui; Zou, Beiji; Zhao, Yu-qian; Guo, Jianjing] Mobile Hlth Minist Educ, China Mobile Joint Lab, Changsha 410012, Hunan, Peoples R China.
   [Zhao, Yu-qian] Cent South Univ, Sch Geosci & Infophys, Changsha 410083, Peoples R China.
C3 Central South University; Central South University
RP Zou, BJ; Zhao, YQ (corresponding author), Cent South Univ, Sch Informat Sci & Engn, Changsha 410083, Hunan, Peoples R China.; Zou, BJ; Zhao, YQ (corresponding author), Mobile Hlth Minist Educ, China Mobile Joint Lab, Changsha 410012, Hunan, Peoples R China.
EM wuhuikx@csu.edu.cn; bjzou@csu.edu.cn; zyq@csu.edu.cn
FU National Natural Science Foundation of China [61172184, 61379107,
   61402539, 61573380]; Program for New Century Excellent Talents in
   University of Education Ministry in China [NCET-13-0603]; Specialized
   Research Fund for the Doctoral Program of Higher Education
   [20130162110016]; Fundamental Research Funds for the Central
   Universities of Central South University [2015zzts052]
FX This work is partly supported by the National Natural Science Foundation
   of China (Grant Nos. 61172184, 61379107, 61402539, and 61573380),
   Program for New Century Excellent Talents in University of Education
   Ministry in China (Grant No. NCET-13-0603), Specialized Research Fund
   for the Doctoral Program of Higher Education (Grant No. 20130162110016)
   and the Fundamental Research Funds for the Central Universities of
   Central South University (Grant No. 2015zzts052).
CR [Anonymous], 2007, WORKSH CAM BAS DOC A
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chen H., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2609, DOI 10.1109/ICIP.2011.6116200
   Chen XR, 2004, PROC CVPR IEEE, P366
   Chucai Yi, 2012, Camera-Based Document Analysis and Recognition. 4th International Workshop, CBDAR 2011. Revised Selected Papers, P15, DOI 10.1007/978-3-642-29364-1_2
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fabrizio J, 2009, IEEE IMAGE PROC, P2373, DOI 10.1109/ICIP.2009.5413435
   Fabrizio J, 2013, PATTERN ANAL APPL, V16, P519, DOI 10.1007/s10044-013-0329-7
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Koo HI, 2013, IEEE T IMAGE PROCESS, V22, P2296, DOI 10.1109/TIP.2013.2249082
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liu XT, 2016, VISUAL COMPUT, V32, P501, DOI 10.1007/s00371-015-1084-0
   Lucas SM, 2005, PROC INT CONF DOC, P80, DOI 10.1109/ICDAR.2005.231
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Minetto R, 2013, PATTERN RECOGN, V46, P1078, DOI 10.1016/j.patcog.2012.10.009
   Neumann L, 2011, PROC INT CONF DOC, P687, DOI 10.1109/ICDAR.2011.144
   Nikolaou N, 2009, INT J IMAG SYST TECH, V19, P14, DOI 10.1002/ima.20174
   Shi CZ, 2014, IEEE T CIRC SYST VID, V24, P1235, DOI 10.1109/TCSVT.2014.2302522
   Shi CZ, 2013, PATTERN RECOGN LETT, V34, P107, DOI 10.1016/j.patrec.2012.09.019
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang Kai., 2010, Word spotting in the wild
   Wang T, 2012, INT C PATT RECOG, P3304
   Wang XB, 2013, PROC INT CONF DOC, P1375, DOI 10.1109/ICDAR.2013.278
   Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yi Chucai, 2011, IEEE Trans Image Process, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yi CC, 2014, IEEE T IMAGE PROCESS, V23, P2972, DOI 10.1109/TIP.2014.2317980
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Yin XC, 2011, PROC INT CONF DOC, P136, DOI 10.1109/ICDAR.2011.36
   Zhang X, 2014, VISUAL COMPUT, V30, P401, DOI 10.1007/s00371-013-0864-7
   Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x
NR 39
TC 19
Z9 20
U1 4
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 113
EP 126
DI 10.1007/s00371-015-1156-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200011
DA 2024-07-18
ER

PT J
AU Cibulski, L
   Gracanin, D
   Diehl, A
   Splechtna, R
   Elshehaly, M
   Delrieux, C
   Matkovic, K
AF Cibulski, Lena
   Gracanin, Denis
   Diehl, Alexandra
   Splechtna, Rainer
   Elshehaly, Mai
   Delrieux, Claudio
   Matkovic, Kresimir
TI ITEA-interactive trajectories and events analysis: exploring sequences
   of spatio-temporal events in movement data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Interactive visual analysis; Movement data; Spatio-temporal data;
   Coordinated multiple views
ID ANALYTICS
AB Widespread use of GPS and similar technologies makes it possible to collect extensive amounts of trajectory data. These data sets are essential for reasonable decision making in various application domains. Additional information, such as events taking place along a trajectory, makes data analysis challenging, due to data size and complexity. We present an integrated solution for interactive visual analysis and exploration of events along trajectories data. Our approach supports analysis of event sequences at three different levels of abstraction, namely spatial, temporal, and events themselves. Customized views as well as standard views are combined to form a coordinated multiple views system. In addition to trajectories and events, we include on-the-fly derived data in the analysis. We evaluate our integrated solution using the IEEE VAST 2015 Challenge data set. A successful detection and characterization of malicious activity indicate the usefulness and efficiency of the presented approach.
C1 [Cibulski, Lena; Splechtna, Rainer] VRVis Res Ctr, Vienna, Austria.
   [Matkovic, Kresimir] VRVis Res Ctr, Interact Visualizat Grp, Vienna, Austria.
   [Cibulski, Lena] Univ Magdeburg, Magdeburg, Germany.
   [Gracanin, Denis] Virginia Tech, Dept Comp Sci, Blacksburg, VA USA.
   [Diehl, Alexandra] Univ Buenos Aires, Course Algorithms & Data Struct 1, Fac Exact & Nat Sci, Buenos Aires, DF, Argentina.
   [Diehl, Alexandra] Univ Buenos Aires, Course Informat Visualizat, Buenos Aires, DF, Argentina.
   [Elshehaly, Mai] Univ Maryland, Baltimore, MD 21201 USA.
   [Delrieux, Claudio] Univ South, Bahia Blanca, Buenos Aires, Argentina.
C3 Otto von Guericke University; Virginia Polytechnic Institute & State
   University; University of Buenos Aires; University of Buenos Aires;
   University System of Maryland; University of Maryland Baltimore
RP Cibulski, L (corresponding author), VRVis Res Ctr, Vienna, Austria.; Cibulski, L (corresponding author), Univ Magdeburg, Magdeburg, Germany.
EM lena.cibulski@st.ovgu.de
RI Delrieux, Claudio/M-1688-2019; Delrieux, Claudio/O-8917-2019; Diehl,
   Alexandra/Y-7176-2019; Matkovic, Kresimir/AAT-8896-2021
OI Delrieux, Claudio/0000-0002-2727-8374; Diehl,
   Alexandra/0000-0002-2943-4051; Elshehaly, Mai/0000-0002-5867-6121;
   Matkovic, Kresimir/0000-0001-9406-8943; Cibulski,
   Lena/0000-0002-8246-5746; Gracanin, Denis/0000-0001-6831-2818
CR Albers D, 2011, IEEE T VIS COMPUT GR, V17, P2392, DOI 10.1109/TVCG.2011.232
   Andrews C., 2015, VAST CHALLENGE 2015
   [Anonymous], 2013, Visual Analytics of Movement
   Buchmüller J, 2015, IEEE CONF VIS ANAL, P121
   Ferreira N, 2013, IEEE T VIS COMPUT GR, V19, P2149, DOI 10.1109/TVCG.2013.226
   Giannotti F, 2011, VLDB J, V20, P695, DOI 10.1007/s00778-011-0244-8
   Gotz D, 2014, IEEE T VIS COMPUT GR, V20, P1783, DOI 10.1109/TVCG.2014.2346682
   Guo HQ, 2011, IEEE PAC VIS SYMP, P163, DOI 10.1109/PACIFICVIS.2011.5742386
   Konyha Z, 2006, IEEE T VIS COMPUT GR, V12, P1373, DOI 10.1109/TVCG.2006.99
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Malik S., 2015, Proceedings of the 20th International Conference on Intelligent User Interfaces, P38, DOI [DOI 10.1145/2678025.27014072, DOI 10.1145/2678025.2701407, 10. 1145/2678025.2701407]
   Mannila H, 1997, DATA MIN KNOWL DISC, V1, P259, DOI 10.1023/A:1009748302351
   Matkovic K, 2007, IEEE INT CONF INF VI, P59
   Monroe M, 2013, IEEE T VIS COMPUT GR, V19, P2227, DOI 10.1109/TVCG.2013.200
   Orellana D, 2012, TOURISM MANAGE, V33, P672, DOI 10.1016/j.tourman.2011.07.010
   Patnaik D., 2011, Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, P360, DOI DOI 10.1145/2020408.2020468
   Steptoe M, 2015, IEEE CONF VIS ANAL, P119
   Whiting M, 2015, IEEE CONF VIS ANAL, P113
   Wongsuphasawat K, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1747
NR 19
TC 2
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 847
EP 857
DI 10.1007/s00371-016-1255-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600017
DA 2024-07-18
ER

PT J
AU Pan, RJ
   Taubin, G
AF Pan, Rongjiang
   Taubin, Gabriel
TI Automatic segmentation of point clouds from multi-view reconstruction
   using graph-cut
SO VISUAL COMPUTER
LA English
DT Article
DE Graph-cut; Point clouds; Segmentation; Multi-view reconstruction;
   Fixation constraints
AB In multi-view reconstruction systems, the recovered point cloud often consists of numerous unwanted background points. We propose a graph-cut based method for automatically segmenting point clouds from multi-view reconstruction. Based on the observation that the object of interest is likely to be central to the intended multi-view images, our method requires no user interaction except two roughly estimated parameters of objects covering in the central area of images. The proposed segmentation process is carried out in two steps: first, we build a weighted graph whose nodes represent points and edges that connect each point to its k-nearest neighbors. The potentials of each point being object and background are estimated according to distances between its projections in images and the corresponding image centers. The pairwise potentials between each point and its neighbors are computed according to their positions, colors and normals. Graph-cut optimization is then used to find the initial binary segmentation of object and background points. Second, to refine the initial segmentation, Gaussian mixture models (GMMs) are created from the color and density features of points in object and background classes, respectively. The potentials of each point being object and background are re-calculated based on the learned GMMs. The graph is updated and the segmentation of point clouds is improved by graph-cut optimization. The second step is iterated until convergence. Our method requires no manual labeling points and employs available information of point clouds from multi-view systems. We test the approach on real-world data generated by multi-view reconstruction systems.
C1 [Pan, Rongjiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Pan, Rongjiang] Brown Univ, Sch Engn, Providence, RI 02912 USA.
   [Taubin, Gabriel] Minist Educ PRC, Engn Res Ctr Digital Media Technol, Jinan, Peoples R China.
C3 Shandong University; Brown University
RP Pan, RJ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.; Pan, RJ (corresponding author), Brown Univ, Sch Engn, Providence, RI 02912 USA.
EM panrj@sdu.edu.cn; taubin@brown.edu
OI Taubin, Gabriel/0000-0002-1983-7607
FU State Scholarship Fund of China; Program of Science and Technology
   Development of Shandong Province [2014GGX101016]; Fundamental Research
   Funds of Shandong University [2014JC003]; Directorate For Engineering;
   Div Of Industrial Innovation & Partnersh [1500249] Funding Source:
   National Science Foundation
FX This work was supported by State Scholarship Fund of China, Program of
   Science and Technology Development of Shandong Province (2014GGX101016),
   the Fundamental Research Funds of Shandong University (2014JC003).
CR [Anonymous], 2009, IEEE WORKSH SEARCH 3
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Calakli F., 2012, STATE OF THE ART VOL
   Campbell N., 2011, VISUAL MEDIA PRODUCT
   Campbell NDF, 2010, IMAGE VISION COMPUT, V28, P14, DOI 10.1016/j.imavis.2008.09.005
   Cignoni P, 2008, ERCIM NEWS, P45
   Djelouah A, 2013, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2013.328
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gay E., 2010, P CVPR WORKSH APPL C
   Johnson-Roberson M, 2010, IEEE INT C INT ROBOT, P1165, DOI 10.1109/IROS.2010.5649872
   KOWDLE A., 2012, P 12 EUR C COMP VIS
   Mishra AK, 2012, IEEE T PATTERN ANAL, V34, P639, DOI 10.1109/TPAMI.2011.171
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sedlacek D, 2009, LECT NOTES COMPUT SC, V5876, P218, DOI 10.1007/978-3-642-10520-3_20
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Wu Changchang., VisualSFM: A Visual Structure from Motion System
NR 19
TC 18
Z9 19
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 601
EP 609
DI 10.1007/s00371-015-1076-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800006
DA 2024-07-18
ER

PT J
AU Yang, W
   Toyoura, M
   Xu, JY
   Ohnuma, F
   Mao, XY
AF Yang, Wei
   Toyoura, Masahiro
   Xu, Jiayi
   Ohnuma, Fumio
   Mao, Xiaoyang
TI Example-based caricature generation with exaggeration control
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Caricature; Example based; Exaggeration; Visual appearance facial
   feature
AB Caricature is a popular artistic media widely used for effective communications. The fascination of caricature lies in its expressive depiction of a person's prominent features, which is usually realized through the so-called exaggeration technique. This paper proposes a new example-based automatic caricature generation system supporting the exaggeration of both the shape of facial components and the spatial relationships among the components. Given the photograph of a face, the system automatically computes the feature vectors representing the shape of facial components as well as the spatial relationship among the components. Those features are exaggerated and then used to search the learning database for the corresponding caricature components and for arranging the retrieved components to create the caricature. Experimental results show that our system can generate the caricatures of the example style capturing the prominent features of the subjects.
C1 [Yang, Wei; Toyoura, Masahiro; Mao, Xiaoyang] Yamanashi Univ, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
   [Xu, Jiayi] Hangzhou Dianzi Univ, 2 St, Hangzhou, Zhejiang, Peoples R China.
   [Ohnuma, Fumio] Aanatano Brand Lab Inc, 1-5-20-502 Nagamachi, Sendai, Miyagi, Japan.
C3 University of Yamanashi; Hangzhou Dianzi University
RP Mao, XY (corresponding author), Yamanashi Univ, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
EM mao@yamanashi.ac.jp
OI Toyoura, Masahiro/0000-0002-5897-7573; mao, xiaoyang/0000-0001-9531-3197
FU Grants-in-Aid for Scientific Research [25280037] Funding Source: KAKEN
CR Akleman E., 2007, SIGGRAPH VIS P ART I, P145
   [Anonymous], 2000, P INT C VIS COMP
   Beucher S., 1993, MATH MORPHOLOGY IMAG
   Bookstein F. L., 1979, SHAPE MODELS
   Brennan S., 1985, LEONARDO, V40, P392
   Chen H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P433, DOI 10.1109/ICCV.2001.937657
   Chen Hong., 2004, P 3 INT S NONPHOTORE, P95, DOI DOI 10.1145/987
   Chen W., 2009, P C COMP AN SOC AG
   Chiang P., 2004, P AS C COMP VIS
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Klare B. F., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P139, DOI 10.1109/ICB.2012.6199771
   Koshimizu H., 1999, P INT C SYST MAN CYB
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Liu J., 2006, ACM MULTIMEDIA, P683
   Liu JF, 2009, LECT NOTES COMPUT SC, V5371, P413
   Min F, 2007, LECT NOTES COMPUT SC, V4679, P184
   Mo Z., 2004, SIGGRAPH SKETCHES
   Nakasu T., 2009, APPL VARIOUS ARTISIT, V63, P1241
   Redman L., 1984, DRAW CARICATURES
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Shet R. N., 2005, P INT C VIS INF ENG
   Tseng CC, 2007, LECT NOTES COMPUT SC, V4843, P314
   Tseng CC, 2012, IMAGE VISION COMPUT, V30, P15, DOI 10.1016/j.imavis.2011.11.006
   Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50
   Yang W, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P237, DOI 10.1109/CW.2014.40
NR 26
TC 8
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 383
EP 392
DI 10.1007/s00371-015-1177-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200011
DA 2024-07-18
ER

PT J
AU Tan, CH
   Hou, JH
   Chau, LP
AF Tan, Cheen-Hau
   Hou, JunHui
   Chau, Lap-Pui
TI Motion capture data recovery using skeleton constrained singular value
   thresholding
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Mocap recovery; Matrix completion
ID MISSING MARKERS; TRACKING; MODELS
AB Motion capture data could be missing due to imperfections during the acquisition process. Singular value thresholding (SVT) is an effective method to recover missing motion capture data. However, its effectiveness decreases significantly when markers are missing for longer periods of time. To alleviate this problem, we utilize the fact that human bones are rigid to constrain inter-marker distances of specific sets of markers. We extend the SVT method for mocap recovery to include skeleton constraints. On average, our proposed method improves on the SVT method by 40 %, and performs 4 % better than a recent state-of-the-art method at up to 11 times faster computation time.
C1 [Tan, Cheen-Hau; Hou, JunHui; Chau, Lap-Pui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Chau, LP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM tanc0158@e.ntu.edu.sg; houj0001@ntu.edu.sg; elpchau@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593; Hou, Junhui/0000-0003-3431-2021
CR [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], CMU GRAPH LAB MOT CA
   [Anonymous], 2003, ACM S VIRT REAL SOFT
   Aristidou Andreas, 2008, 2008 2nd International Conference on Bioinformatics and Biomedical Engineering (ICBBE '08), P1343, DOI 10.1109/ICBBE.2008.665
   Aristidou A, 2013, VISUAL COMPUT, V29, P7, DOI 10.1007/s00371-011-0671-y
   Baumann J., 2011, P WORKSH VIRT REAL I
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Herda L, 2000, COMP ANIM CONF PROC, P77, DOI 10.1109/CA.2000.889046
   Herda L, 2001, HUM MOVEMENT SCI, V20, P313, DOI 10.1016/S0167-9457(01)00050-1
   Hornung A, 2005, P IEEE VIRT REAL ANN, P75
   Hou J., 2013, P IEEE INT C IM PROC
   Hou J., 2014, P IEEE INT C MULT EX
   Lai R. Y. Q., 2011, EUROGRAPHICS, P45, DOI 10.2312/EG2011/short/045-048
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Li L., 2010, P 2010 ACM SIGGRAPH, P179, DOI [10.2312/SCA/SCA10/179-188, DOI 10.2312/SCA/SCA10/179-188]
   Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507
   Liu GD, 2006, VISUAL COMPUT, V22, P721, DOI 10.1007/s00371-006-0080-9
   Lou H, 2010, IEEE T VIS COMPUT GR, V16, P870, DOI 10.1109/TVCG.2010.23
   Papadimitriou Spiros, 2003, VLDB, P560
   Piazza T, 2009, LECT NOTES COMPUT SC, V5903, P125, DOI 10.1007/978-3-642-10470-1_11
   Tan CH, 2013, ELECTRON LETT, V49, P752, DOI 10.1049/el.2013.0442
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wiley DJ, 1997, P IEEE VIRT REAL ANN, P156, DOI 10.1109/VRAIS.1997.583065
   Xiao J, 2011, COMPUT ANIMAT VIRT W, V22, P221, DOI 10.1002/cav.413
   Yi B.-K., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P13, DOI 10.1109/ICDE.2000.839383
   Yi P, 2012, IET IMAGE PROCESS, V6, P1331, DOI 10.1049/iet-ipr.2012.0186
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 31
TC 17
Z9 17
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1521
EP 1532
DI 10.1007/s00371-014-1031-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Mukherjee, S
   Gil, S
   Ray, N
AF Mukherjee, Satarupa
   Gil, Stephani
   Ray, Nilanjan
TI Unique people count from monocular videos
SO VISUAL COMPUTER
LA English
DT Article
DE Unique people count; Influx; Outflux; Occlusion; Boundary tracking
ID SCALE
AB Counting unique number of people in a video (i.e., counting a person only once while the person passes through the field of view) is required in many video analytic applications, such as transit passenger and pedestrian volume count in railway stations, malls, and road intersections. The principal roadblock here is occlusion. To avoid this bottleneck, we adopt a combination of (a) a radical new approach of unique influx and outflux count (UIOC) of people within a region of interest (ROI), which is adopted from computational fluidics, (b) a nonlinear regressor to estimate the number of people within a ROI, and (c) ROI boundary tracking (as opposed to object or feature tracking) for a short period. In UIOC, we compute influx/outflux rate, i.e., number of people entering or exiting the ROI per unit time. Then, we sum the influx/outflux rate between any two time points to estimate the number of people that entered and/or left the ROI within that time interval. Our framework is validated on 19 publicly available datasets, with abundant occlusion, obtaining more than 95 % accuracy for each video. Our framework is online and real time. Our framework is comparatively inexpensive to install and operate as only one camera is used. These features make the proposed framework suitable for low-cost, small-business/residential and/or commercial applications. We also extend our framework beyond monocular videos and apply it on multiple views of a publicly available dataset with about 99 % accuracy.
C1 [Mukherjee, Satarupa; Gil, Stephani; Ray, Nilanjan] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
C3 University of Alberta
RP Mukherjee, S (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM satarupa@ualberta.ca
FU NSERC; AQL Management Consulting Inc.; Computing Science, University of
   Alberta
FX The authors would like to thank Dr. Yang Cong for the LHI dataset. The
   authors also acknowledge the following sources of funding for this work:
   NSERC, AQL Management Consulting Inc., and Computing Science, University
   of Alberta.
CR [Anonymous], 2005, 1530 U WISC
   [Anonymous], 2008, CVPR
   [Anonymous], 2006, COMPUTER VISION PATT, DOI 10.1109/CVPR.2006.92
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Box P. C., 2010, MANUAL TRAFFIC ENG S, P17
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Chan A.B., 2009, ICCV, p1 7
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Chateau T., 2006, WORKSHOP DYNAMICAL V, P218
   Chengbin Zeng, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2069, DOI 10.1109/ICPR.2010.509
   Cong Y, 2009, PROC CVPR IEEE, P1093, DOI 10.1109/CVPRW.2009.5206648
   Conte D., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1743, DOI 10.1109/ICPR.2010.431
   Conte D, 2013, MACH VISION APPL, V24, P1029, DOI 10.1007/s00138-013-0491-3
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Harasse S, 2005, PROC WRLD ACAD SCI E, V4, P221
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hou CP, 2010, PATTERN RECOGN, V43, P720, DOI 10.1016/j.patcog.2009.07.015
   Kharbat M., HORN SCHUNCK OPTICAL
   Kim BS, 2008, LECT NOTES COMPUT SC, V5227, P1117
   Kim J.W., 2002, ITC-CSCC, P1418
   Krahnstoever N., 2008, PETS-Winter Workshop, V1, P1
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Ma Z, 2013, PROC CVPR IEEE, P2539, DOI 10.1109/CVPR.2013.328
   MCFARLANE NJB, 1995, MACH VISION APPL, V8, P187, DOI 10.1007/BF01215814
   Mukherjee S., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2969, DOI 10.1109/ICIP.2011.6116284
   Mukherjee S., 2014, IEEE ISBI IN PRESS
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Tan B, 2011, PATTERN RECOGN, V44, P2297, DOI 10.1016/j.patcog.2010.10.002
   Yao B, 2007, LECT NOTES COMPUT SC, V4679, P169
NR 31
TC 5
Z9 5
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1405
EP 1417
DI 10.1007/s00371-014-1022-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800009
DA 2024-07-18
ER

PT J
AU Reynolds, DT
   Laycock, SD
   Day, AM
AF Reynolds, D. T.
   Laycock, S. D.
   Day, A. M.
TI Real-time accumulation of occlusion-based snow
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time simulation; Real-time rendering; Computer graphics; Snow
   simulation; Snow accumulation
AB This paper describes a technique to allow the real-time simulation of snowfall accumulation in a dynamic 3D environment. The implementation maps surface-bound accumulation buffers to each object in the scene, forming height-maps of accumulated snow cover. The environment is rendered from above similar to the way shadow mapping is typically produced. However, unique buffer IDs and texture co-ordinates are output within each pixel of the render. A new technique for performing the mapping between buffers is proposed where a series of quads are rendered, one per pixel of the occlusion projection, to map directly visible surfaces to their corresponding accumulation height-map. Blurring is performed on the maps and per-pixel detail is given by procedurally generated normal maps of each surface updated frame by frame. Additional detail and shaping is performed using view-dependent tessellation and the snow surface is created by re-colouring and offsetting the existing scene geometry. This is the first implementation of snow simulation which allows persistent accumulation on a dynamic, moving scene in real-time.
C1 [Reynolds, D. T.; Laycock, S. D.; Day, A. M.] Univ E Anglia, Norwich NR4 7TJ, Norfolk, England.
C3 University of East Anglia
RP Reynolds, DT (corresponding author), Univ E Anglia, Norwich Res Pk, Norwich NR4 7TJ, Norfolk, England.
EM Daniel.Reynolds@uea.ac.uk; S.Laycock@uea.ac.uk; Andy.Day@uea.ac.uk
CR Chen YY, 2003, J VISUAL COMP ANIMAT, V14, P21, DOI 10.1002/vis.301
   Doumani G.A., 1967, Physics of Snow and Ice: Proceedings, International Conference on Low Temperature Science, V1, P1119
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   Feldman B.E., 2002, ACM SIGGRAPH 2002 C, P218
   Festenberg N. v., 2009, Eurographics workshop on natural phenomena, P15
   Foldes D., 2007, P 4 WORKSH VRIPHYS, P35
   Haglund H., 2002, P SIGRAD, P11
   Kim T., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P86
   KIM T, 2004, P 2004 ACM SIGGRAPH, P305, DOI DOI 10.1145/1028523.1028564
   Langer M.S., 2004, P EGSR, P217
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Maréchal N, 2010, COMPUT GRAPH FORUM, V29, P449, DOI 10.1111/j.1467-8659.2009.01614.x
   Muraoka K, 2000, SEVENTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS: WORKSHOPS, PROCEEDINGS, P187, DOI 10.1109/PADSW.2000.884537
   Nishita T, 1997, COMPUT GRAPH FORUM, V16, pC357, DOI 10.1111/1467-8659.00173
   Ohlsson P., 2004, Proceedings of SIGRAD, P25
   Reynolds D.H., 2011, Technology-enhanced assessment of talent, P1
   Sumner R., 2001, COMPUT GRAPH FORUM, V18, P17
   Tokoi K., 2006, International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06), P310
   von Festenberg N, 2011, COMPUT GRAPH FORUM, V30, P1837, DOI 10.1111/j.1467-8659.2011.01904.x
   Wang CB, 2006, VISUAL COMPUT, V22, P315, DOI 10.1007/s00371-006-0012-8
NR 20
TC 7
Z9 7
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 689
EP 700
DI 10.1007/s00371-014-0995-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400014
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Jin, LQ
   Nie, YW
   Wang, RF
   Sun, HQ
   Ma, KL
AF Xiao, Chunxia
   Jin, Liqiang
   Nie, Yongwei
   Wang, Renfang
   Sun, Hanqiu
   Ma, Kwan-Liu
TI Content-aware model resizing with symmetry-preservation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Model resizing; Symmetry-preservation; Geometry saliency; Protective
   mesh; Space deformation
ID DEFORMATION
AB This paper presents a new model resizing approach intended for preservation of important geometric content and geometric symmetries of the model to be resized. We first extract high-level symmetric regions and other low-level salient regions from the input model. Then, we map the extracted low-level and high-level geometry information to a protective tetrahedral mesh around the model, and we define a symmetry-preserving and content-aware resizing function on the tetrahedral mesh using 3D mean value coordinates space deformation. By interpolating within the resized tetrahedral mesh, we obtain the final resizing results for the embedded model. Interactively defined user constraints are also incorporated into our framework to produce more desirable results. Our results show that the resized models preserve the geometric features of important regions in addition to the symmetric aspects of the original model, and eliminate the undesirable distortion in other less important regions.
C1 [Xiao, Chunxia; Jin, Liqiang; Nie, Yongwei] Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
   [Wang, Renfang] Zhejiang Wanli Univ, Coll Comp Sci & Informat Technol, Ningbo, Zhejiang, Peoples R China.
   [Sun, Hanqiu] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Ma, Kwan-Liu] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 Wuhan University; Zhejiang Wanli University; Chinese University of Hong
   Kong; University of California System; University of California Davis
RP Xiao, CX (corresponding author), Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
EM cxxiao@whu.edu.cn; jlq5202008@gmail.com; nieyongwei@gmail.com;
   renfangwang@gmail.com; hanqiu@cse.cuhk.edu.hk; ma@cs.ucdavis.edu
FU National Basic Research Program of China [2012CB725303]; NSFC [61070081,
   41271431]; State Key Lab of CADCG [A1208]; Project of the Science and
   Technology Plan for Zhejiang Province [2012C21004]; Fundamental Research
   Funds for the Central Universities; NCET [NCET-13-0441]; Key grant
   project of Hubei province [2013AAA020]; Academic Award for Excellent
   Ph.D. Candidates - Ministry of Education of China [5052012211001]
FX This work was partly supported by the National Basic Research Program of
   China (No. 2012CB725303), the NSFC (No. 61070081, and No. 41271431), the
   Open Project Program of the State Key Lab of CAD&CG(Grant No. A1208),
   the Project of the Science and Technology Plan for Zhejiang Province
   (Grant No. 2012C21004), the Fundamental Research Funds for the Central
   Universities, NCET (NCET-13-0441), Key grant project of Hubei province
   (2013AAA020) and the Academic Award for Excellent Ph.D. Candidates
   funded by Ministry of Education of China (No. 5052012211001)
CR Bokeloh M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778841
   Bokeloh M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185574
   Bokeloh M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024157
   Botsch M., 2010, Polygon Mesh Processing
   Chen L., 2009, P 2009 SIAM ACM JOIN, P289
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Gain J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409629
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Kraevoy V, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409064
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Li X., 2005, S GEOMETRY PROCESSIN, V255, P217
   Li XY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461917
   Liao B, 2013, COMPUT AIDED DESIGN, V45, P1394, DOI 10.1016/j.cad.2013.06.015
   Liao B, 2013, COMPUT AIDED DESIGN, V45, P861, DOI 10.1016/j.cad.2013.02.003
   Liao B, 2012, VISUAL COMPUT, V28, P387, DOI 10.1007/s00371-011-0625-4
   Lin JJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024217
   LIPMAN Y, 2010, ACM T GRAPHIC, V29, DOI DOI 10.1145/1805964.1805971
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Mitra NJ, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239514
   Popa T., 2006, P SMI 2006
   Saad Y., 1996, Iterative Methods for Sparse Linear Systems
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Si Hang., 2006, A quality tetrahedral mesh generator and three-dimensional delaunay triangulator
   Wang KP, 2009, COMPUT GRAPH-UK, V33, P433, DOI 10.1016/j.cag.2009.03.004
   Wang YS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964983
   Xiao CX, 2010, COMPUT GRAPH FORUM, V29, P2065, DOI 10.1111/j.1467-8659.2010.01793.x
   Xu K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366200
   XU W., 2009, SIGGRAPH 09, V28, p[1, 3]
   Yu HC, 2012, VISUAL COMPUT, V28, P849, DOI 10.1007/s00371-012-0708-x
NR 33
TC 6
Z9 6
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 155
EP 167
DI 10.1007/s00371-014-0919-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800006
DA 2024-07-18
ER

PT J
AU Choi, HF
   Chincisan, A
   Becker, M
   Magnenat-Thalmann, N
AF Choi, Hon Fai
   Chincisan, Andra
   Becker, Matthias
   Magnenat-Thalmann, Nadia
TI Multimodal composition of the digital patient: a strategy for the knee
   articulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Virtual human; Medical imaging; Segmentation; Multiscale modeling
ID KINEMATICS; MUSCLE; MODEL
AB Creating virtual bodies of real patients and using them for diagnosis and treatment planning offer the potential to further empower clinical decision making by medical experts. Virtual patient modeling allows to examine the mechanical and physiological conditions under which articulations are operating in a variety of activities without putting the patient in hazard. The continuous scientific progress has led to an increased range of musculoskeletal data and knowledge being available, covering multiple scales of the musculoskeletal system. A fuller integration of these modalities can broaden the scientific basis of virtual articulation modeling in patients, but poses challenges for data fusion and coupling of simulations. Here, we present a multimodal strategy to compose virtual models of the knee articulation based on a complementary spectrum of data that enables simulations on different scales.
C1 [Choi, Hon Fai; Chincisan, Andra; Becker, Matthias; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, Geneva, Switzerland.
C3 University of Geneva
RP Choi, HF (corresponding author), Univ Geneva, MIRALab, Geneva, Switzerland.
EM choi@miralab.ch; chincisan@miralab.ch; becker@miralab.ch;
   thalmann@miralab.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
FU EU [289897]
FX This work has been funded by the EU FP7 Marie-Curie ITN project
   MultiScaleHuman under Grant number 289897. We thank the University
   Hospital of Geneva in Switzerland, for providing the medical images, and
   the biomechanics laboratory LBB-MHH of the medical school in Hanover,
   Germany, for the experimental data of knee displacement. One of the
   authors, Nadia Magnenat Thalmann, is grateful to Humboldt Foundation to
   have allowed her to spend some time in Germany for collaboration with
   LBB-MHH and the Leibniz University in Hanover.
CR Angelini ED, 2002, PROC SPIE, V4684, P401, DOI 10.1117/12.467182
   Baudin PY, 2012, LECT NOTES COMPUT SC, V7510, P569, DOI 10.1007/978-3-642-33415-3_70
   Becker M., 2014, 3D MULTISCALE PHYSL, P81, DOI DOI 10.1007/978-1-4471-6275-9_4
   Blemker SS, 2005, ANN BIOMED ENG, V33, P661, DOI 10.1007/s10439-005-1433-7
   Bredno J, 2003, IEEE T PATTERN ANAL, V25, P550, DOI 10.1109/TPAMI.2003.1195990
   Chan TE, 2000, J VIS COMMUN IMAGE R, V11, P130, DOI 10.1006/jvci.1999.0442
   Choi HF, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077576
   Erdemir A., 2013, P ASME FDA 2013 1 AN
   Fernandez JW, 2005, BIOMECH MODEL MECHAN, V4, P20, DOI 10.1007/s10237-005-0072-0
   Geremia E, 2010, LECT NOTES COMPUT SC, V6361, P111
   Gilles B, 2010, MED IMAGE ANAL, V14, P291, DOI 10.1016/j.media.2010.01.006
   Heimann T, 2010, COMPUTATIONAL BIOMECHANICS FOR MEDICINE, P107, DOI 10.1007/978-1-4419-5874-7_12
   John D., 2013, LECT NOTES COMPUTATI, V4, P55
   KADABA MP, 1990, J ORTHOP RES, V8, P383, DOI 10.1002/jor.1100080310
   Kalliokoski KK, 2011, FRONT PHYSIOL, V2, DOI 10.3389/fphys.2011.00075
   Kazemi M, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/718423
   Küpper JC, 2007, CLIN BIOMECH, V22, P1, DOI 10.1016/j.clinbiomech.2006.08.003
   Lee D, 2011, FOUND TRENDS COMPUT, V7, P229, DOI 10.1561/0600000036
   Maas S.A., 2013, J BIOMECH ENG, V134
   Maas S.A., 2013, J BIOMECH ENG, V134
   Magnenat-Thalmann N, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P1, DOI 10.1109/CW.2010.41
   Maurice X, 2009, VISUAL COMPUT, V25, P835, DOI 10.1007/s00371-009-0313-9
   McKee CT, 2011, TISSUE ENG PART B-RE, V17, P155, DOI 10.1089/ten.teb.2010.0520
   Pandy MG, 2010, ANNU REV BIOMED ENG, V12, P401, DOI 10.1146/annurev-bioeng-070909-105259
   Pelechano N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P99
   Peña E, 2006, J BIOMECH, V39, P1686, DOI 10.1016/j.jbiomech.2005.04.030
   Rossi R, 2011, BMC SPORTS SCI MED R, V3, DOI 10.1186/1758-2555-3-25
   Scheys L, 2011, GAIT POSTURE, V33, P158, DOI 10.1016/j.gaitpost.2010.11.003
   Shim VB, 2010, IFMBE PROC, V25, P2315
   Sibole SC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037538
   Viceconti M, 2006, P IEEE, V94, P725, DOI 10.1109/JPROC.2006.871769
   Yang NH, 2010, J ORTHOP RES, V28, P1539, DOI 10.1002/jor.21174
NR 32
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 739
EP 749
DI 10.1007/s00371-014-0983-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700017
DA 2024-07-18
ER

PT J
AU Chen, N
   Prasanna, VK
AF Chen, Na
   Prasanna, Viktor K.
TI A bag-of-semantics model for image clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Image clustering; Image representation; Image semantics; Object relation
   network
AB This paper presents a novel method to organize a collection of images into a hierarchy of clusters based on image semantics. Given a group of raw images with no metadata as input, our method describes the semantics of each image with a bag-of-semantics model (i.e., a set of meaningful descriptors), which is derived from the image's Object Relation Network (Chen et al. in Proceedings of the 21st International Conference on World Wide Web, 2012)-an expressive graph model representing rich semantics for image objects and their relations. We adopt the class hierarchies in a guide ontology as different levels of lenses to view the bag-of-semantics models. Image clusters are automatically extracted by grouping images with the same bag-of-semantics viewed through a certain lens. With a series of coarse-to-fine lenses, images are clustered in a top-down hierarchical manner. In addition, given that users can have different perspectives regarding how images should be clustered, our method allows each user to control the clustering process while browsing, and thus dynamically adjusts the clustering result according to the user's preferences.
C1 [Chen, Na; Prasanna, Viktor K.] Univ So Calif, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Chen, N (corresponding author), Univ So Calif, Los Angeles, CA 90089 USA.
EM nchen@usc.edu; prasanna@usc.edu
FU Chevron Corp. under Center for Interactive Smart Oilfield Technologies
   (CiSoft), at the University of Southern California
FX This work is supported by Chevron Corp. under the joint project, Center
   for Interactive Smart Oilfield Technologies (CiSoft), at the University
   of Southern California.
CR [Anonymous], 2010, ECCV
   [Anonymous], 2004, CVPR
   [Anonymous], 2009, CVPR
   Bi J., 2005, CVPR
   Biswas A., 2012, CVPR
   Bosch A., 2006, ECCV
   Cai D., 2004, ACM Multimedia
   Chen N, 2012, P 1 INT C COMP VIS M
   Chen N., 2012, P 21 INT C WORLD WID
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen Y., 2003, IEEE T IMAGE PROCESS
   Chen Y., 2004, J MACH LEARN RES
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao Bin., 2005, ACM MULTIMEDIA
   Gemert J.C., 2008, ECCV
   Gordon S., 2003, ICCV
   Jing F., 2006, ACM MULTIMEDIA
   Li F.-F., 2005, CVPR
   Liu Y, 2009, J VIS COMUN IMAGE RE
   Nguyen B.P., 2012, VIS COMPUT
   Nwogu I., 2010, CVPR
   RODDEN K, 2001, P SIGCHI C HUM FACT
   Torralba A., 2010, COMMUN
   Wang X.J., 2005, ACM MULTIMEDIA
   Williams C.K.I., PASCAL VISUAL OBJECT
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Zheng Xin., 2004, ACM MULTIMEDIA
NR 28
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1221
EP 1229
DI 10.1007/s00371-013-0785-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300010
DA 2024-07-18
ER

PT J
AU Gois, JP
   Trevisan, DF
   Batagelo, HC
   Macêdo, I
AF Gois, Joao Paulo
   Trevisan, Diogo Fernando
   Batagelo, Harlen Costa
   Macedo, Ives
TI Generalized Hermitian Radial Basis Functions Implicits from polygonal
   mesh constraints
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Radial basis functions; Generalized Hermitian data; Surface
   reconstruction; Mesh interpolation
ID REPRESENTATION; SURFACES; SHAPE
AB In this work we investigate a generalized interpolation approach using radial basis functions to reconstruct implicit surfaces from polygonal meshes. With this method, the user can define with great flexibility three sets of constraint interpolants: points, normals, and tangents; allowing to balance computational complexity, precision, and feature modeling. Furthermore, this flexibility makes possible to avoid untrustworthy information, such as normals estimated on triangles with bad aspect ratio. We present results of the method for applications related to the problem of modeling 2D curves from polygons and 3D surfaces from polygonal meshes. We also apply the method to problems involving subdivision surfaces and front-tracking of moving boundaries. Finally, as our technique generalizes the recently proposed HRBF Implicits technique, comparisons with this approach are also conducted.
C1 [Gois, Joao Paulo; Trevisan, Diogo Fernando; Batagelo, Harlen Costa] Univ Fed ABC, Santo Andre, Brazil.
   [Macedo, Ives] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
C3 Universidade Federal do ABC (UFABC); University of British Columbia
RP Gois, JP (corresponding author), Univ Fed ABC, Santo Andre, Brazil.
EM joao.gois@ufabc.edu.br
RI Batagelo, Harlen/G-1318-2014; Gois, Joao Paulo/D-1182-2010
OI Batagelo, Harlen/0000-0002-2325-2070; Gois, Joao
   Paulo/0000-0002-9437-6943; Macedo, Ives/0000-0002-7993-7298
CR Alexa M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516531
   [Anonymous], RR5526 INRIA
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   Boyé S, 2010, COMPUT GRAPH FORUM, V29, P2021, DOI 10.1111/j.1467-8659.2010.01788.x
   Brazil E.V., 2010, Proceedings of the Seventh Sketch- Based Interfaces and Modeling Symposium, P1, DOI 10.2312/SBM/SBM10/001-008
   Brazil EV, 2011, COMPUT GRAPH-UK, V35, P43, DOI 10.1016/j.cag.2010.09.017
   Brochu T, 2009, SIAM J SCI COMPUT, V31, P2472, DOI 10.1137/080737617
   Buhmann M.D., 2003, C MO AP C M, V12, DOI 10.1017/CBO9780511543241
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chen YQ, 2008, ACM T MATH SOFTWARE, V35, DOI 10.1145/1391989.1391995
   Du J, 2006, J COMPUT PHYS, V213, P613, DOI 10.1016/j.jcp.2005.08.034
   Enright D, 2002, J COMPUT PHYS, V183, P83, DOI 10.1006/jcph.2002.7166
   Fasshauer G.E., 2007, Meshfree approximation methods with MATLAB
   Fasshauer GE, 2007, COMPUT METH APPL SCI, V5, P221
   Fedkiw R., 2003, LEVEL SET METHODS DY, V153
   Gelas A, 2007, IEEE T IMAGE PROCESS, V16, P1873, DOI 10.1109/TIP.2007.898969
   Gois JP, 2010, COMPUT GRAPH FORUM, V29, P1969, DOI 10.1111/j.1467-8659.2010.01663.x
   Gomes AbelJ P., 2009, Implicit curves and surfaces: Mathematics, data structures and algorithms
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Jin XG, 2003, COMPUT GRAPH-UK, V27, P763, DOI 10.1016/S0097-8493(03)00149-3
   Kanai T., 2006, P 4 EUR S GEOM PROC, P21
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lucas BC, 2013, IEEE T VIS COMPUT GR, V19, P852, DOI 10.1109/TVCG.2012.162
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Muller M., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P237, DOI DOI 10.1145/1599470.1599501
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pan RJ, 2009, SCI CHINA SER F, V52, P308, DOI 10.1007/s11432-009-0032-x
   Park T, 2012, IEEE T VIS COMPUT GR, V18, P1638, DOI 10.1109/TVCG.2011.286
   Gois JP, 2008, J COMPUT PHYS, V227, P9643, DOI 10.1016/j.jcp.2008.07.013
   Pébay PP, 2003, MATH COMPUT, V72, P1817, DOI 10.1090/S0025-5718-03-01485-6
   Pereira T, 2011, GRAPH MODELS, V73, P97, DOI 10.1016/j.gmod.2010.11.001
   SAVCHENKO VV, 1995, COMPUT GRAPH FORUM, V14, P181, DOI 10.1111/1467-8659.1440181
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Vrankar L, 2010, COMPUT FLUIDS, V39, P1480, DOI 10.1016/j.compfluid.2010.04.015
   Walder C, 2006, COMPUT GRAPH FORUM, V25, P635, DOI 10.1111/j.1467-8659.2006.00983.x
   Wendland H., 2004, Scattered data approximation, DOI [10.1017/CBO9780511617539, DOI 10.1017/CBO9780511617539]
   Xie XH, 2011, IMAGE VISION COMPUT, V29, P167, DOI 10.1016/j.imavis.2010.08.011
   Yngve G, 2002, IEEE T VIS COMPUT GR, V8, P346, DOI 10.1109/TVCG.2002.1044520
NR 40
TC 10
Z9 12
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 651
EP 661
DI 10.1007/s00371-013-0802-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400019
DA 2024-07-18
ER

PT J
AU Tao, YB
   Wang, C
   Lin, H
   Dong, F
   Clapworthy, G
AF Tao, Yubo
   Wang, Chao
   Lin, Hai
   Dong, Feng
   Clapworthy, Gordon
TI Opacity volume based halo generation and depth-dependent halos
SO VISUAL COMPUTER
LA English
DT Article
DE Volume rendering; Illustrative visualization; Halos; Depth-dependent
   halos; Depth perception
ID FEATURE ENHANCEMENT
AB Halos are generally used to enhance depth perception and display spatial relationships in illustrative visualization. In this paper, we present a simple and effective method to create volumetric halo illustration. At the preprocessing stage, we generate, on graphics hardware, a view-independent halo intensity volume, which contains all of the potential halos around the boundaries of features, based on the opacity volume. During halo rendering, the halo intensity volume is used to extract halos only around the contours of structures for the current viewpoint. The performance of our approach is significantly faster than previous halo illustration methods, which perform both halo generation and rendering during direct volume rendering. We further propose depth-dependent halo effects, including depth color fading and depth width fading. These halo effects adaptively modulate the visual properties of halos to provide more perceptual cues for depth interpretation. Experimental results demonstrate the efficiency of our proposed approach and the effectiveness of depth-dependent halos.
C1 [Tao, Yubo; Lin, Hai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Wang, Chao] Univ Wollongong, Wollongong, NSW, Australia.
   [Dong, Feng; Clapworthy, Gordon] Univ Bedfordshire, Ctr Comp Graph & Visualizat, Luton, Beds, England.
C3 Zhejiang University; University of Wollongong; University of
   Bedfordshire
RP Tao, YB (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM taoyubo@cad.zju.edu.cn; w-c05@mails.tsinghua.edu.cn; lin@cad.zju.edu.cn;
   Feng.Dong@beds.ac.uk; Gordon.Clapworthy@beds.ac.uk
FU 863 Program [2012AA12A404]; NFS of China [60873122, 60903133]; State Key
   Lab of CAD&CG, Zhejiang University [A1012]
FX This work was partially supported by 863 Program Project 2012AA12A404,
   NFS of China (No. 60873122 and No. 60903133), and the Open Project
   Program of the State Key Lab of CAD&CG (Grant No. A1012), Zhejiang
   University. The data sets are courtesy of Olaf Ronneberger, SFB 382,
   General Electric, Philips Research, VoreenPub.
CR [Anonymous], ACM SIGGRAPH 2007 CO
   Appel A., 1979, Proc. SIGGRAPH, P151, DOI [10.1145/965103.807437, DOI 10.1145/965103.807437]
   Bruckner S, 2007, IEEE T VIS COMPUT GR, V13, P1344, DOI 10.1109/TVCG.2007.70555
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1077, DOI 10.1109/TVCG.2006.140
   Chiu K., 1993, Proceedings Graphics Interface '93, P245
   Everts MH, 2009, IEEE T VIS COMPUT GR, V15, P1299, DOI 10.1109/TVCG.2009.138
   Interrante V, 1997, VISUALIZATION '97 - PROCEEDINGS, P421, DOI 10.1109/VISUAL.1997.663912
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Lu AD, 2003, IEEE T VIS COMPUT GR, V9, P127, DOI 10.1109/TVCG.2003.1196001
   Nagy Z, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P497
   Rautek P., 2008, COMPUT GRAPH Q, V42
   Rheingans P, 2001, IEEE T VIS COMPUT GR, V7, P253, DOI 10.1109/2945.942693
   Ruiz M., 2010, P COMP AESTH GRAPH V, P51
   Solteszova Veronika., 2011, Proc. of NPAR, P105, DOI DOI 10.1145/2024676.2024694
   Svakhine NA, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P347, DOI 10.1109/PCCGA.2003.1238276
   Svakhine NA, 2009, IEEE T VIS COMPUT GR, V15, P77, DOI 10.1109/TVCG.2008.56
   Tao YB, 2009, VISUAL COMPUT, V25, P581, DOI 10.1007/s00371-009-0328-2
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   Viola I., 2005, COMPUTATIONAL AESTHE, P209, DOI [DOI 10.2312/COMPAESTH/COMPAESTH05/209-216, 10.2312/COMPAESTH/COMPAESTH05/209216]
   Wang LJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P367
   Wenger A, 2004, IEEE T VIS COMPUT GR, V10, P664, DOI 10.1109/TVCG.2004.46
   Yuan Xiaoru., 2004, Proceedings of Joint IEEE/EG Symposium on Visualization, P9
   Yubo Tao, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P418, DOI 10.1109/CAD/Graphics.2011.81
NR 25
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2013
VL 29
IS 4
SI SI
BP 287
EP 296
DI 10.1007/s00371-012-0764-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AM
UT WOS:000316784200006
DA 2024-07-18
ER

PT J
AU Byelozyorov, S
   Jochem, R
   Pegoraro, V
   Slusallek, P
AF Byelozyorov, Sergiy
   Jochem, Rainer
   Pegoraro, Vincent
   Slusallek, Philipp
TI From real cities to virtual worlds using an open modular architecture
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual worlds; The Web; Open architecture; Geographic information
   system
AB The technologies for the Web and virtual worlds are currently converging, but although there are some efforts made to integrate them with each other, they typically rely on technologies foreign to most Web developers. In this paper, we present a new open architecture that combines several emerging and established technologies to provide convenient tools for developing virtual worlds directly in the Web. These technologies are easy to learn and understand by the Web community and allow for quick prototyping. Overall the modular architecture allows virtual worlds to be developed more quickly and more widely deployed. Additionally, we demonstrate that creating an adequate virtual environment can be an easy task when applying the principles of crowd-sourcing. We present an application that uses one of the largest available open data sources of geospatial information to bring 3D cities from the real world into the virtual environment.
C1 [Byelozyorov, Sergiy; Pegoraro, Vincent] Univ Saarland, Comp Graph Lab CGUdS, Dept Comp Sci, D-66123 Saarbrucken, Germany.
   [Jochem, Rainer] DFKI, Agents & Simulated Real Lab, Saarbrucken, Germany.
   [Pegoraro, Vincent] MMCI, Saarbrucken, Germany.
C3 Saarland University; German Research Center for Artificial Intelligence
   (DFKI)
RP Byelozyorov, S (corresponding author), Univ Saarland, Comp Graph Lab CGUdS, Dept Comp Sci, D-66123 Saarbrucken, Germany.
EM byelozyorov@cs.uni-saarland.de; rainer.jochem@dfki.de;
   pegoraro@cs.uni-saarland.de; slusallek@cg.uni-saarland.de
OI Slusallek, Philipp/0000-0002-2189-2429
CR Behr J., 2009, WEB3D 09 P 14 INT C, P127, DOI [DOI 10.1145/1559764.1559784, 10.1145/1559764.1559784]
   Byelozyorov S., 2011, CYBERWORLDS, P46
   Group K., 2010, OPENGL ES COMM PROF
   Horn D., 2010, To infinity and not beyond: Scaling communication in virtual worlds with Meru
   Intel, 2011, SCAL VIRT ENV
   Khronos Group, 2011, WEBGL SPEC
   Klein F., 2012, XFLOW DECLARATIVE DA
   Neis P, 2012, FUTURE INTERNET, V4, P1, DOI 10.3390/fi4010001
   Sons K, 2010, P 15 INT C WEB 3D TE, P175
   Sons K., 2011, WORKSH VIRT REAL INT
   W3C, 2011, DOC OBJ MOD
   W3C, 2011, HYP TRANSF PROT
   W3C, 2009, CSS 3D TRANSF MOD LE
   W3C, 2011, HTML5 DRAFT SPEC
   W3C, 2011, XMLHTTPREQUEST
   W3C, 1997, VRML97 REL SPEC
   Web3DConsortium, 2008, ISO IEC 19775 200X E
   WHATWG, 2011, WEB WORK
   World Wide Web Consortium (W3C), 2011, WEBSOCKET API
   World Wide Web Consortium [w3G], 2011, SCAL VECT GRAPH
NR 20
TC 0
Z9 0
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2013
VL 29
IS 2
SI SI
BP 141
EP 153
DI 10.1007/s00371-012-0717-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 072FM
UT WOS:000313654700006
DA 2024-07-18
ER

PT J
AU Chen, YJ
   Thalmann, NM
   Allen, BF
AF Chen, Yujun
   Thalmann, Nadia Magnenat
   Allen, Brian Foster
TI Physical simulation of wet clothing for virtual humans
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Computer animation; Wet garment simulation; Nonlinear friction;
   Imperfection sensitivity model
AB We present a technique that simulates wet garments for virtual humans with realistic folds and wrinkles. Our approach combines three new models to allow realistic simulation of wet garments: (1) a simplified saturation model that modifies the masses, (2) a nonlinear friction model derived from previously reported, real-world measurements, and (3) a wrinkle model based on imperfection sensitivity theory. In contrast to previous approaches to wet cloth, the proposed models are supported by the experimental results reported in the textile literature with parameters varying over the course of the simulation. As a result, the wet garment motions simulated by our method are comparable to that of real wet garments. Our approach recognizes the special, practical importance of contact models with human skin and provides a specific skin-cloth friction solution. We evaluate our approach by draping a rotating sphere and simulating a typical garment on a virtual human in the rain. Both of these examples are typical scenarios within computer graphics research.
C1 [Chen, Yujun] Peking Univ, Beijing 100871, Peoples R China.
   [Chen, Yujun; Thalmann, Nadia Magnenat; Allen, Brian Foster] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
   [Thalmann, Nadia Magnenat] Univ Geneva, Interdisciplinary MIRALab, Geneva, Switzerland.
C3 Peking University; Nanyang Technological University; University of
   Geneva
RP Chen, YJ (corresponding author), Peking Univ, Beijing 100871, Peoples R China.
EM yujunc@gmail.com; thalmann@miralab.ch; vector@acm.org
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR Abdelmoula R., 2007, EUR J MECH, V1, P1
   [Anonymous], 2008, WT KOITERS ELASTIC S
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Breen D. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P365, DOI 10.1145/192161.192259
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Bridson R., 2005, ACM SIGGRAPH 2005 CO, P3
   Charfi H, 2006, TEXT RES J, V76, P787, DOI 10.1177/0040517507068526
   Choi K.-J., 2002, SIGGRAPH 02, P1
   Das B, 2009, J ENG FIBER FABR, V4, P20
   Decaudin P, 2006, COMPUT GRAPH FORUM, V25, P625, DOI 10.1111/j.1467-8659.2006.00982.x
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239500
   Huang C., 2011, COMPUT ANIMAT VIRTUA, V22, P99106
   Huber M., 2011, ACM SIGGRAPH 2011 PO, P10
   Kawabata S., 1980, STANDARDIZATION ANAL, V2nd
   KENINS P, 1994, TEXT RES J, V64, P722, DOI 10.1177/004051759406401204
   Lenaerts T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360648
   LI Y, 1995, TEXT RES J, V65, P316, DOI 10.1177/004051759506500602
   Luo W.-d., 2004, J WUHAN U SCI ENG, V17, P5
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Ozgen O., ACM T GRAPH, V29, P1
   Pabst Simon., 2009, P 2009 ACM SIGGRAPHE, P149, DOI DOI 10.1145/1599470.1599490
   Provot X., 1997, GRAPHICS INTERFACE, P177
   PROVOT X, 1995, DEFORMATION CONSTRAI
   Robinson-Mosher A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360645
   Taibi EH, 2001, TEXT RES J, V71, P582, DOI 10.1177/004051750107100703
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Volino P, 2007, VISUAL COMPUT, V23, P133, DOI 10.1007/s00371-006-0034-2
   Volino P, 2006, ACM T GRAPHIC, V25, P1154, DOI 10.1145/1141911.1142007
   Weil J., 1986, Computer Graphics, V20, P49, DOI 10.1145/15886.15891
NR 29
TC 18
Z9 24
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 765
EP 774
DI 10.1007/s00371-012-0687-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500024
DA 2024-07-18
ER

PT J
AU Liu, H
   Zhang, L
   Huang, H
AF Liu, Hong
   Zhang, Lei
   Huang, Hua
TI Web-image driven best views of 3D shapes
SO VISUAL COMPUTER
LA English
DT Article
DE Best view; Web images; Silhouette; Curvature; Saliency
ID RECOGNITION
AB The rapid advance of the Internet provides available huge database of web images. In this paper, we introduce a novel approach for automatically computing the best views of 3D shapes based on their web images. Best view selection is generally an intuitive task of getting the most information of a 3D shape. The novelty of our approach is to directly explore human perception on observing 3D shapes from the relevant web images. Those images are captured from biased views of different people, thus sufficiently reflecting view choice when observing the 3D shapes. By collecting web images possibly captured from the similar views, the best view is selected as the one possessing the most web images. We experiment our method with the shapes in Princeton Shape Benchmark (PSB), as well make comparisons with traditional geometric descriptor based approaches. The results demonstrate that our method is not only robust but also able to produce more canonical views in accordance with human perception.
C1 [Liu, Hong; Zhang, Lei; Huang, Hua] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Zhang, L (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM cgzhanglei@gmail.com
RI Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702
FU Program for New Century Excellent Talents in University [NCET-09-0635]
FX We thank the anonymous reviewers for their helpful comments. This work
   was partly supported by the Program for New Century Excellent Talents in
   University (No. NCET-09-0635).
CR [Anonymous], EUR WORKSH 3D OBJ RE
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041
   Denton T, 2004, INT C PATT RECOG, P273, DOI 10.1109/ICPR.2004.1334159
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Gooch B., 2003, ACM SIGGRAPH COURSE
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   MEYER M., 2002, P VISMATH C, P1
   Mortara M, 2009, COMPUT GRAPH-UK, V33, P280, DOI 10.1016/j.cag.2009.03.003
   Page DL, 2003, IEEE IMAGE PROC, P229
   Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y
   Raimondo S., 2001, SURVEY METHODS COLOU, P183
   Schroff F, 2007, IEEE I CONF COMP VIS, P2120
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Takahashi S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P495
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Vazquez P.-P., 2002, P VISSYM, V22, P183, DOI DOI 10.2312/VISSYM/VISSYM02/183-188
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 26
TC 30
Z9 36
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2012
VL 28
IS 3
BP 279
EP 287
DI 10.1007/s00371-011-0638-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 896QS
UT WOS:000300585400004
DA 2024-07-18
ER

PT J
AU Csurka, G
   Skaff, S
   Marchesotti, L
   Saunders, C
AF Csurka, Gabriela
   Skaff, Sandra
   Marchesotti, Luca
   Saunders, Craig
TI Building look & feel concept models from color combinations With
   applications in image classification, retrieval, and color transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Pattern recognition; Color vision; Aesthetics
ID PREFERENCE; EMOTION
AB In this paper, we tackle the problem of associating combinations of colors to abstract concepts (e. g. capricious, classic, cool, delicate, etc.). Since such concepts are difficult to represent using single colors, we consider combinations of colors or color palettes. We leverage two novel databases for color palettes, and learn categorization models using both low and high level descriptors. It is shown that the Bag of Colors and Fisher Vectors are the most rewarding descriptors for palettes categorization and retrieval.
   A simple but novel and efficient method for cleaning weakly annotated data, whilst preserving the visual coherence of categories is also given.
   Finally, we demonstrate that abstract category models learned on color palettes can be used in different applications such as image personalization, concept-based palette, and image retrieval and color transfer.
C1 [Csurka, Gabriela; Saunders, Craig] XRCE Xerox Res Ctr Europe, Textual & Visual Pattern Anal Grp, Meylan, France.
RP Csurka, G (corresponding author), XRCE Xerox Res Ctr Europe, Textual & Visual Pattern Anal Grp, 6 Chemin Maupertuis, Meylan, France.
EM Gabriela.Csurka@xrce.xerox.com; Sandra.Skaff@xerox.com;
   Luca.Marchesotti@xrce.xerox.com; Craig.Saunders@xrce.xerox.com
FU Agence Nationale de la Recherche through the OMNIA [ANR-07-MDCO-009-02]
FX This work was supported by the Agence Nationale de la Recherche through
   the OMNIA project (ANR-07-MDCO-009-02).
CR [Anonymous], THESIS U BUFFALO
   [Anonymous], 2007, CVPR
   [Anonymous], 1998, ICCV
   [Anonymous], ECCV
   [Anonymous], IND C COMP VIS GRAPH
   [Anonymous], 2004, ECCV WORKSHOPS
   Benavente R, 2008, J OPT SOC AM A, V25, P2582, DOI 10.1364/JOSAA.25.002582
   Berlin Brent, 1969, Basic Color Terms: Their Universality and Evolution
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Conway D., 1992, Proceedings of the East-West International Conference on Human-Computer Interaction, EWHCI '92, P328
   DATTA R, 2008, IEEE INT C IM PROC S
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Davis BC, 2008, IEEE IMAGE PROC, P109, DOI 10.1109/ICIP.2008.4711703
   Eiseman L., 2000, Pantone guide to communicating with color
   Farquhar J.D. H., 2005, Improving "bag-of-keypoints" image categorisation: Generative models and pdf-kernels
   Fedorovskaya E, 2008, IEEE IMAGE PROC, P121, DOI 10.1109/ICIP.2008.4711706
   GREENFIELD GR, 2005, EUR WORKSH COMP AEST, P91
   HOU X, 2007, INT MULT C AUGSB GER, P265
   Jacobsen T, 2006, NEUROIMAGE, V29, P276, DOI 10.1016/j.neuroimage.2005.07.010
   KRISHNAPURAM B, 2005, PAMI, V27
   Ou LC, 2004, COLOR RES APPL, V29, P381, DOI 10.1002/col.20047
   Ou LC, 2004, COLOR RES APPL, V29, P232, DOI 10.1002/col.20010
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   SKAFF S, 2011, COL IM C
   Solli Martin, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1885, DOI 10.1109/ICCVW.2009.5457512
   SOLLI M, 2008, CGIV
   Solli M, 2009, LECT NOTES COMPUT SC, V5702, P573, DOI 10.1007/978-3-642-03767-2_70
   Tai YW, 2005, PROC CVPR IEEE, P747
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Yang CK, 2008, IEEE COMPUT GRAPH, V28, P52, DOI 10.1109/MCG.2008.24
   ZHANG L, 2006, MM
NR 32
TC 12
Z9 16
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1039
EP 1053
DI 10.1007/s00371-011-0657-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500002
DA 2024-07-18
ER

PT J
AU Goradia, R
   Kashyap, MSS
   Chaudhuri, P
   Chandran, S
AF Goradia, Rhushabh
   Kashyap, M. S. Sriram
   Chaudhuri, Parag
   Chandran, Sharat
TI Tracing specular light paths in point-based scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Ray tracing; Caustics; Point models; GPU; Implicit surface; Octree
AB Massive point data sets representing meticulous details of various heritage sites and statues are now becoming available due to recent advances in multi-view stereo techniques. Photorealistic rendering of such point sets has not yet, however, matched their polygonal counterparts with respect to the interactivity of applications as well as the quality of light simulations.
   In this paper, we present a framework for tracing specular light paths in massive point model environments at interactive frame rates on Graphics Processing Units (GPUs). We introduce the Sample Octree (S-Octree), a lightweight data structure for efficient, sampled representation of point set information. The Implicit Surface Octree (ISO), an instance of the S-Octree, provides a compact representation of point set surfaces. The ISO defines a local manifold approximation of the input point data. The Caustic Sample Map (CSM), another instance of the S-Octree, represents contributions of caustic paths. These data structures enable us to further the state of the art by demonstrating reflections, refractions, shadows and caustic effects on massive, complex point models at interactive frame rates.
C1 [Goradia, Rhushabh; Chaudhuri, Parag] Indian Inst Technol, ViGIL, Dept Comp Sci & Engn, Bombay, Maharashtra, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay
RP Goradia, R (corresponding author), Indian Inst Technol, ViGIL, Dept Comp Sci & Engn, Bombay, Maharashtra, India.
EM rhushabh@cse.iitb.ac.in; kashyap@cse.iitb.ac.in; paragc@cse.iitb.ac.in;
   sharat@cse.iitb.ac.in
CR Adams B, 2005, COMPUT GRAPH FORUM, V24, P677, DOI 10.1111/j.1467-8659.2005.00892.x
   Adamson A, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P272
   [Anonymous], 2005, GPU GEMS
   Carr NA, 2006, PROC GRAPH INTERF, P203
   Carr NathanA., 2002, P ACM SIGGRAPHEUROGR, P37
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Dobashi Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P152, DOI 10.1109/PCCGA.2004.1348345
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   GARANZHA K, 2010, COMPUT GRAPHICS FORU, V29
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   GORADIA R, 2008, ACM SIGGRAPH 2008 S
   GORADIA R, 2008, P IND C COMP VIS GRA, P560
   GORADIA R, 2010, 18 PAC C COMP GRAPH
   GOSWAMI P, 2010, 18 PAC C COMP GRAPH
   Gross M., 2007, POINT BASED GRAPHICS
   GUNTHER J, 2004, RENDERING TECHNIQUES, P111
   HECKBERT PS, 1992, THESIS
   Horn DR, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P167
   HUANG Y, 2006, EUR S POINT BAS GRAP, P103, DOI DOI 10.2312/SPBG/SPBG06/103-110
   KASHYAP S, 2010, ACM SIGGRAPH 2010 S
   Kashyap S., 2010, P 7 IND C COMP VIS G, P227, DOI [10.1145/1924559.1924590, DOI 10.1145/1924559.1924590]
   Knoll A, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P115
   Laine S., 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P55, DOI DOI 10.1145/1730804.1730814
   Linsen L, 2007, JOURNAL WSCG, V15, P51
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   Ren L, 2002, COMPUT GRAPH FORUM, V21, P461, DOI 10.1111/1467-8659.00606
   Schaufler G, 2000, SPRING COMP SCI, P319
   Wald I., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P9, DOI 10.1109/PBG.2005.194058
   Wand M, 2003, PROC GRAPH INTERF, P139
NR 31
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1083
EP 1097
DI 10.1007/s00371-011-0654-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500005
DA 2024-07-18
ER

PT J
AU Attene, M
   Marini, S
   Spagnuolo, M
   Falcidieno, B
AF Attene, M.
   Marini, S.
   Spagnuolo, M.
   Falcidieno, B.
TI Part-in-whole 3D shape matching and docking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE Object recognition; Partial matching; Shape complementarity
ID OBJECTS; SIMILARITY; RETRIEVAL; SEARCH
AB A new algorithmic framework is proposed to efficiently recognize instances of template shapes within target 3D models or scenes. The new framework provides an efficient solution of the part-in-whole matching problem and, with simple adaptations, it can also be exploited to quickly select sites in the target which properly fit with the template. Therefore, the method proposed potentially offers a new approach to all applications where complementarity has to be analysed quickly such as, for instance, docking. By assuming that the template is small when compared to the target, the proposed approach distinguishes from the previous literature because the part-in-whole matching is obtained by extracting offline only the shape descriptor of the template, while the description of the target is dynamically and adaptively extracted during the matching process. This novel framework, called the Fast Reject schema, exploits the incremental nature of a class of local shape descriptors to significantly reduce the part-in-whole matching time, without any expensive processing of the models for the extraction of the shape descriptors. The schema has been tested on three different descriptors and results are discussed in detail. Experiments show that the gain in computational performances does not compromise the accuracy of the matching results. An additional descriptor is introduced to compute parts of the target having a complementary shape with respect to the template. Results of such a shape complementarity detection are shown in domains such as cultural heritage and drug design.
C1 [Attene, M.; Marini, S.; Spagnuolo, M.; Falcidieno, B.] IMATI GE CNR, I-16149 Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Attene, M (corresponding author), IMATI GE CNR, Via Marini 6, I-16149 Genoa, Italy.
EM marco.attene@cnr.it; marini@ge.imati.cnr.it; spagnuolo@ge.imati.cnr.it;
   falcidieno@ge.imati.cnr.it
RI Spagnuolo, Michela/ABA-1927-2021; Marini, Simone/C-3872-2012; Spagnuolo,
   Michela/F-5068-2013
OI Spagnuolo, Michela/0000-0002-5682-6990; Marini,
   Simone/0000-0003-0665-7815; Spagnuolo, Michela/0000-0002-5682-6990
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], IPSJ T COMPUTER VISI
   [Anonymous], EUR WORKSH 3D OBJ RE
   Atilgan E, 2010, P 10 INT C HYBR INT, P113
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P271
   Attene M, 2009, COMPUT AIDED DESIGN, V41, P756, DOI 10.1016/j.cad.2009.01.003
   Bajaj C, 2011, IEEE ACM T COMPUT BI, V8, P45, DOI 10.1109/TCBB.2009.57
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bespalov D., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P208, DOI DOI 10.1145/781606.781638
   BIAN X, 2009, INT C MECH AUT, P2107
   Biasotti S, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391731
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Bronstein AM, 2009, INT J COMPUT VISION, V84, P163, DOI 10.1007/s11263-008-0147-3
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   CHRISTIANSEN S, 2008, AER C, P1
   Cohen F., 2010, IEEE COMP SOC C COMP, P55
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   Corney J, 2002, IEEE COMPUT GRAPH, V22, P65, DOI 10.1109/MCG.2002.999789
   Dong Z, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P43
   Elad M, 2002, SPRING EUROGRAP, P107
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Ferreira A, 2010, INT J COMPUT VISION, V89, P327, DOI 10.1007/s11263-009-0257-6
   Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Golovinskiy A., 2009, INT C COMP VIS ICCV
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Igwe P.C., 2006, Proceedings of the IEEE Conference on Geometric Modeling and Imaging: New Trends, P9, DOI DOI 10.1109/GMAI.2006.1
   Johnson AE, 1997, PROC CVPR IEEE, P684, DOI 10.1109/CVPR.1997.609400
   KAZHDAN M, 2004, THESIS PRINCETON U
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Mortara M, 2004, ALGORITHMICA, V38, P227, DOI 10.1007/s00453-003-1051-4
   Papaioannou G, 2003, IMAGE VISION COMPUT, V21, P401, DOI 10.1016/S0262-8856(03)00008-8
   Papaioannou G, 2002, IEEE T PATTERN ANAL, V24, P114, DOI 10.1109/34.982888
   Ruiz-Correa S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1126
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   SUZUKI MT, 2005, P 9 IASTED INT C SOF, P389
   SUZUKI MT, 2005, ACM SIGGRAPH 2005 PO, P128
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Viola P., 2001, P 2001 IEEE COMP SOC, DOI [10.1109/CVPR.2001.990517, DOI 10.1109/CVPR.2001.990517]
   Yang JZ, 2009, INT CONF BIOMED, P75
NR 42
TC 15
Z9 15
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 991
EP 1004
DI 10.1007/s00371-011-0622-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400005
DA 2024-07-18
ER

PT J
AU Ribardière, M
   Carré, S
   Bouatouch, K
AF Ribardiere, Mickael
   Carre, Samuel
   Bouatouch, Kadi
TI Adaptive records for volume irradiance caching
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Rendering; Participating media; Global illumination; Irradiance cache
ID PARTICIPATING MEDIA
AB In this paper, we present a new irradiance caching scheme using Monte Carlo ray tracing for efficiently rendering participating media. The irradiance cache algorithm is extended to participating media. Our method allows to adjust the density of cached records depending on illumination changes. Direct and indirect contributions can be stored in the records but also multiple scattering. An adaptive shape of the influence zone of records, depending on geometrical features and irradiance variations, is introduced. To avoid a high density of cached records in low interest areas, a new method controls the density of the cache when adding new records. This record density control depends on the interpolation quality and on the photometric characteristics of the medium. Reducing the number of records accelerates both the computation pass and the rendering pass by decreasing the number of queries to the cache data structure (Kd-tree). Finally, instead of using an expensive ray marching to find records that cover the ray, we gather all the contributive records along the ray. With our method, pre-computing and rendering passes are significantly speeded-up.
C1 [Ribardiere, Mickael; Bouatouch, Kadi] IRISA, Rennes, France.
   [Carre, Samuel] CSTB, Lighting Simulat Team, Nantes, France.
C3 Universite de Rennes
RP Ribardière, M (corresponding author), IRISA, Rennes, France.
EM mickael.ribardiere@irisa.fr; samuel.carre@cstb.fr;
   kadi.bouatouch@irisa.fr
OI Ribardiere, Mickael/0000-0003-2964-2608
CR Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chandrasekhar S., 1950, RAD TRANSFER
   *CIE, 1996, S003E1996 CIE
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   IESNA, 2002, LM6302 IESNA
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P557, DOI 10.1111/j.1467-8659.2008.01153.x
   Jarosz W, 2008, COMPUT GRAPH FORUM, V27, P1087, DOI 10.1111/j.1467-8659.2008.01246.x
   JENSEN H.W., 1998, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, P311
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   KRIVANEK J, 2006, RENDERING TECHNIQUES
   KRIVANEK J, 2008, ACM SIGGRAPH 08 CLAS
   Krivanek Jaroslav, 2009, Practical Global Illumination With Irradiance Caching
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   PATTANAIK SN, 1993, J VISUAL COMP ANIMAT, V4, P133, DOI 10.1002/vis.4340040303
   Pauly M, 2000, SPRING COMP SCI, P11
   RIBARDIERE M, 2010, THESIS U RENNES 1
   Ribardière M, 2011, COMPUT GRAPH FORUM, V30, P1603, DOI 10.1111/j.1467-8659.2010.01846.x
   TABELLION E, 2004, SIGGRAPH 04 ACM SIGG, P469
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
   [No title captured]
NR 20
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 655
EP 664
DI 10.1007/s00371-011-0573-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600024
DA 2024-07-18
ER

PT J
AU Nescher, T
   Kunz, A
AF Nescher, Thomas
   Kunz, Andreas
TI An interactive whiteboard for immersive telecollaboration
SO VISUAL COMPUTER
LA English
DT Article
DE Remote collaboration; People on content; Shared workspaces; Mixed
   presence groupware; Digital whiteboard
AB In this paper, we present CollaBoard, a collaboration system that gives a higher feeling of presence to the local auditory and to the persons on the remote site. By overlaying the remote life-sized video image atop the shared artifacts on the common whiteboard and by keeping the whiteboard's content editable at both sites, it creates a higher involvement of the remote partners into a collaborative teamwork. All deictic gestures of the remote user are shown in the right context with the shared artifacts on the common whiteboard and thus preserve their meaning. The paper describes the hardware setup, as well as the software implementation and the performed user studies with two identical interconnected systems.
C1 [Nescher, Thomas; Kunz, Andreas] ETH, Inst Machine Tools & Mfg, CLA G9, CH-8092 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Kunz, A (corresponding author), ETH, Inst Machine Tools & Mfg, CLA G9, Tannenstr 3, CH-8092 Zurich, Switzerland.
EM nescher@iwf.mavt.ethz.ch; kunz@iwf.mavt.ethz.ch
RI Kunz, Andreas/B-9241-2008
OI Kunz, Andreas/0000-0002-6495-4327
FU Eureka project [Sigma! 4066]
FX This work was done within the Eureka project Sigma! 4066. We want to
   thank all people who contributed to this work.
CR Coldefy F, 2007, PROCEEDINGS OF THE SECOND IASTED INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION, P37
   DAFT RL, 1984, RES ORGAN BEHAV, V6, P191
   Everitt K.M., 2003, PROC CHI 2003, P553
   FRIEDLAND G, 2006, LULU COM
   GAVER W, 1993, P INTERCHI 93, P335, DOI DOI 10.1145/169059.169268
   Hauber J., 2006, P 2006 20 ANN C COMP, P413, DOI [10.1145/1180875.1180937, DOI 10.1145/1180875.1180937]
   ISHII H, 1991, PROCEEDINGS OF THE SECOND EUROPEAN CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK : ECSCW 91, P163
   ISHII H, 1993, NTT REVIEW, V5, P24
   Ishii H., 1992, P SIGCHI C HUM FACT, P3525, DOI [10.1145/142750.142977, DOI 10.1145/142750.142977]
   Kirk D, 2006, P SIGCHI C HUM FACT, P1191, DOI DOI 10.1145/1124772.1124951
   Kirk D, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1039
   Kunz A, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P430, DOI 10.1109/CW.2010.17
   Louwerse M, 2005, P 27 ANN M COGN SCI
   MESTER R, 2001, LNCS, V2191, P170
   Migge B, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P32, DOI 10.1109/CW.2010.18
   *SKYP, SKYP VID C SOFTW
   Smith Jason, 2003, P UIST 2003, P57
   Tang A, 2004, VID P CSCW, V4
   Tang A, 2007, PEOPLE AND COMPUTERS XX - ENGAGE, P85, DOI 10.1007/978-1-84628-664-3_8
   Tang J.C., 1991, CHI 91, P315, DOI DOI 10.1145/108844.108932
   TANG JC, 1991, INT J MAN MACH STUD, V34, P143, DOI 10.1016/0020-7373(91)90039-A
   TANG JC, 1991, ACM T INFORM SYST, V9, P170, DOI 10.1145/123078.128729
NR 22
TC 8
Z9 10
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 311
EP 320
DI 10.1007/s00371-011-0546-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 739IC
UT WOS:000288707700007
DA 2024-07-18
ER

PT J
AU Airieau, B
   Meneveaux, D
   Bridault, F
   Blasi, P
AF Airieau, Boris
   Meneveaux, Daniel
   Bridault, Flavien
   Blasi, Philippe
TI Photon streaming for interactive global illumination in dynamic scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Lighting simulation; Interactive rendering; Photon streaming
AB While many methods exist for simulating diffuse light inter-reflections, relatively few of them are adapted to dynamic scenes. Despite approximations made on the formal rendering equation, managing dynamic environments at interactive or real-time frame rates still remains one of the most challenging problems. This paper presents a lighting simulation system based on photon streaming, performed continuously on the central processor unit. The power corresponding to each photon impact is accumulated onto predefined points, called virtual light accumulators (or VLA). VLA are used during the rendering phase as virtual light sources. We also introduce a priority management system that automatically adapts to brutal changes during lighting simulation (for instance due to visibility changes or fast object motion). Our system naturally benefits from multi-core architecture. The rendering process is performed in real time using a graphics processor unit, independently from the lighting simulation process. As shown in the results, our method provides high framerates for dynamic scenes, with moving viewpoint, objects and light sources.
C1 [Airieau, Boris; Meneveaux, Daniel; Blasi, Philippe] Univ Poitiers, CNRS, UMR 6172, XLIM SIC Lab, F-86962 Futuroscope, France.
   Etranges Libellules, F-69100 Villeurbanne, France.
C3 Universite de Poitiers; Centre National de la Recherche Scientifique
   (CNRS)
RP Meneveaux, D (corresponding author), Univ Poitiers, CNRS, UMR 6172, XLIM SIC Lab, Bat SP2MI,Teleport 2,Bvd Marie & Pierre Curie,BP3, F-86962 Futuroscope, France.
EM airieau@sic.univ-poitiers.fr; daniel@sic.univ-poitiers.fr;
   blasi@sic.univ-poitiers.fr
RI Blasi, Pasquale/O-9345-2015
OI Meneveaux, Daniel/0000-0001-7160-3026
FU Poitou-Charentes region; NavII FEDER
FX This project as been funded by Poitou-Charentes region, thanks to PPF
   GIC and the NavII FEDER project. We would also like to thank the
   anonymous reviewers for their valuable feedback.
CR [Anonymous], 1994, Radiosity and global illumination
   Brouillat J, 2008, COMPUT GRAPH FORUM, V27, P1971, DOI 10.1111/j.1467-8659.2008.01346.x
   Cohen Michael F., 1993, Radiosity and realistic image synthesis
   DACHBACHER C, 2006, S INT 3D GRAPH, P93
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   DMITRIEV K, 2002, EGRW 02, P25
   GAUTRON P, 2005, EUR S REND, P55
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kelemen C, 2002, COMPUT GRAPH FORUM, V21, P531, DOI 10.1111/1467-8659.t01-1-00703
   KELLER A, 1997, SIGGRAPH 97, P49
   KONTKANEN J., 2006, EUR S REND
   Kristensen AW, 2005, ACM T GRAPHIC, V24, P1208, DOI 10.1145/1073204.1073334
   KRIVANEK J, 2006, EUR S REND, P127
   Laine S., 2007, P EUR S REND, P277, DOI DOI 10.2312/EGWR/EGSR07/277-286
   LARSEN BD, 2004, EUR S REND, P123
   Lehtinen J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360636
   Nichols G, 2009, COMPUT GRAPH FORUM, V28, P1141, DOI 10.1111/j.1467-8659.2009.01491.x
   Nijasure M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P450, DOI 10.1109/PCCGA.2003.1238293
   Ritschel T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409082
   Segovia B, 2007, COMPUT GRAPH FORUM, V26, P425, DOI 10.1111/j.1467-8659.2007.01065.x
   Segovia B., 2006, P 21 ACM SIGGRAPHEUR, P53, DOI DOI 10.1145/1283900.1283909
   Segovia B., 2006, Eurographics Symposium on Rendering, P389
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Wald I., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P74
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Zaninetti J, 1998, COMPUT GRAPH FORUM, V17, pC149, DOI 10.1111/1467-8659.00262
NR 28
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 229
EP 240
DI 10.1007/s00371-010-0528-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000005
DA 2024-07-18
ER

PT J
AU Pang, WJ
   Hui, KC
AF Pang, Wenjun
   Hui, K. C.
TI Interactive evolutionary 3D fractal modeling
SO VISUAL COMPUTER
LA English
DT Article
DE 3D fractal; Iterated function system; Genetic algorithm; Interactive
   evolutionary design
AB This paper presents a technique for creating 3D fractal art forms automatically. Using this approach, designers can get access to a large number of 3D art shapes that can be modified interactively. This is based on a modified evolutionary algorithm using Fractal Transform (FT) and Iterated Function System (IFS), which provides tunable geometric parameters. Fitness function for measuring the aesthetics of a fractal shape is formulated based on characteristic parameters in fractal theory, including capacity dimension, correlation dimension, and largest Lyapunov exponent. The productivity of visually appealing fractal can be enhanced by using the proposed technique. Experiments demonstrated the effectiveness of the proposed method, which can be applied to the design of jewelry, light fixture, and decorative patterns.
C1 [Pang, Wenjun; Hui, K. C.] Chinese Univ Hong Kong, Dept Mech & Automat Engn, CAD Lab, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Hui, KC (corresponding author), Chinese Univ Hong Kong, Dept Mech & Automat Engn, CAD Lab, Hong Kong, Hong Kong, Peoples R China.
EM kchui@mae.cuhk.edu.hk
FU Research Grants Council of the Hong Kong Special Administration Region
   [412508]
FX This work is partially supported by the Research Grants Council of the
   Hong Kong Special Administration Region (Project No. 412508).
CR Aks DeborahJ., 1996, Empirical Studies of the Arts, V14, P1, DOI DOI 10.2190/6V31-7M9R-T9L5-CDG9
   [Anonymous], 1998, Fractals Everywhere
   [Anonymous], 1999, EVOLUTIONARY DESIGN
   BARNSLEY MF, 1985, P ROY SOC LOND A MAT, V399, P243, DOI 10.1098/rspa.1985.0057
   BERKOWITZ J, 1998, FRACTAL COSMOS ART M
   ECKMANN JP, 1985, REV MOD PHYS, V57, P617, DOI 10.1103/RevModPhys.57.617
   GRASSBERGER P, 1983, PHYSICA D, V9, P189, DOI 10.1016/0167-2789(83)90298-1
   JACOB C, 1992, PSYCHOL BULL, V112, P55
   Joye Y, 2005, INT J ART DES EDUC, V24, P175, DOI 10.1111/j.1476-8070.2005.00438.x
   Kruger A, 1996, COMPUT PHYS COMMUN, V98, P224, DOI 10.1016/0010-4655(96)00080-X
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MERKWIRTH C, 2002, TSTOOL MATLAB TOOLBO
   Mitina OV, 2003, INT J MOD PHYS C, V14, P1047, DOI 10.1142/S0129183103005182
   SCOTT D, 2004, FRACTAL FLAME ALGORI
   SCOTT D, 2006, ACM SIGEVOLUTION, V1, P10
   SIMS K, 1991, COMP GRAPH, V25, P319, DOI 10.1145/127719.122752
   Spehar B, 2003, COMPUT GRAPH-UK, V27, P813, DOI 10.1016/S0097-8493(03)00154-7
   SPROTT JC, 1994, COMPUT GRAPH, V18, P417, DOI 10.1016/0097-8493(94)90042-6
   Todd S., 1992, Evolutionary art and computers
   Wannarumon S., 2006, Computer-Aided Design and Applications, V3, P385
   Wannarumon S, 2008, AI EDAM, V22, P19, DOI 10.1017/S0890060408000024
NR 21
TC 4
Z9 5
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2010
VL 26
IS 12
BP 1467
EP 1483
DI 10.1007/s00371-010-0500-8
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 678WA
UT WOS:000284112400004
DA 2024-07-18
ER

PT J
AU Huang, H
   Zang, Y
   Li, CF
AF Huang, Hua
   Zang, Yu
   Li, Chen-Feng
TI Example-based painting guided by color features
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Example-based painting; Color features learning
AB In this paper, by analyzing and learning the color features of the reference painting with a novel set of measures, an example-based approach is developed to transfer some key color features from the template to the source image. First, color features of a given template painting is analyzed in terms of hue distribution and the overall color tone. These features are then extracted and learned by the algorithm through an optimization scheme. Next, to ensure the spatial coherence of the final result, a segmentation based post processing is performed. Finally, a new color blending model, which avoids the dependence of edge detection and adjustment of inconvenient tune parameters, is developed to provide a flexible control for the accuracy of painting. Experimental results show that the new example-based painting system can produce paintings with specific color features of the template, and it can also be applied to changing color themes of art pieces, designing color styles of paintings/real images, and specific color harmonization.
C1 [Huang, Hua; Zang, Yu] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian, Peoples R China.
   [Li, Chen-Feng] Swansea Univ, Sch Engn, Swansea, W Glam, Wales.
C3 Xi'an Jiaotong University; Swansea University
RP Huang, H (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, 28 Xianning W Rd, Xian, Peoples R China.
EM huanghua@xjtu.edu.cn; zangyu7@126.com; c.f.li@swansea.ac.uk
RI Li, Chenfeng/AFQ-6554-2022; Huang, Hua/M-9684-2013; Li,
   Chenfeng/D-4034-2014; Li, Mengqi/AAG-6804-2021
OI Li, Chenfeng/0000-0003-0441-211X; Huang, Hua/0000-0003-2587-1702; 
FU National Natural Science Foundation of China [60970068]; Chinese
   Ministry of Education [109142]; MOE-Intel Joint Research Fund
   [MOE-INTEL-09-07]
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant No. 60970068), the Key Project of Chinese
   Ministry of Education (Grant No. 109142) and the MOE-Intel Joint
   Research Fund (Grant No. MOE-INTEL-09-07).
CR Chang YH, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P176, DOI 10.1109/CGI.2003.1214463
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Drori I, 2003, PROC CVPR IEEE, P143
   Greenfield GR, 2003, WSCG'2003, VOL 11, NO 1, CONFERENCE PROCEEDINGS, P189
   GRUNDL M, 2005, WORKSH COMP AESTH, P101
   Hays J., 2004, PROC NPAR 01, P113
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Hertzmann A., 2002, NPAR, P91, DOI 10.1145/508530.508546
   Litwinowicz P., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, SIGGRAPH '97, P407
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sawant N, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P576, DOI 10.1109/ICVGIP.2008.17
   Tai YW, 2005, PROC CVPR IEEE, P747
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhang SH, 2009, SCI CHINA SER F, V52, P162, DOI 10.1007/s11432-009-0035-7
NR 19
TC 13
Z9 16
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 933
EP 942
DI 10.1007/s00371-010-0498-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 602JQ
UT WOS:000278135800053
DA 2024-07-18
ER

PT J
AU Silcowitz-Hansen, M
   Niebe, S
   Erleben, K
AF Silcowitz-Hansen, Morten
   Niebe, Sarah
   Erleben, Kenny
TI A nonsmooth nonlinear conjugate gradient method for interactive contact
   force problems
SO VISUAL COMPUTER
LA English
DT Article
DE Contact force computation; Rigid body simulation; Nonsmooth conjugate
   gradients
AB Interactive rigid body simulation is important for robot simulation and virtual design. A vital part of the simulation is the computation of contact forces. This paper addresses the contact force problem, as used in interactive simulation. The contact force problem can be formulated in the form of a nonlinear complementarity problem (NCP), which can be solved using an iterative splitting method, such as the projected Gauss-Seidel (PGS) method. We present a novel method for solving the NCP problem by applying a Fletcher-Reeves type nonlinear nonsmooth conjugate gradient (NNCG) type method. We analyze and present experimental convergence behavior and properties of the new method. Our results show that the NNCG method has at least the same convergence rate as PGS, and in many cases better.
C1 [Silcowitz-Hansen, Morten; Niebe, Sarah; Erleben, Kenny] Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark.
C3 University of Copenhagen
RP Niebe, S (corresponding author), Univ Copenhagen, Dept Comp Sci, Univ Sparken 1, DK-2100 Copenhagen, Denmark.
EM mosh@diku.dk; niebe@diku.dk; kenny@diku.dk
RI Erleben, Kenny/AAZ-6556-2020; Niebe, Sarah/E-6760-2015
OI Erleben, Kenny/0000-0001-6808-4747; Niebe, Sarah/0000-0001-8914-4542
CR ANITESCU M, 1997, NONLINEAR DYN INT J
   [Anonymous], THESIS U WISCONSIN M
   Baraff D., 1994, SIGGRAPH 94
   Clarke F, 1990, CLASSICS APPL MATH, V5
   Coumans Erwin., 2005, Bullet physics library
   Cyberbotics, 2009, WEB 6
   Erleben K, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243986
   Guendelman E., 2003, ACM T GRAPH
   Hahn J. K., 1988, SIGGRAPH 88
   Kaufman DM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409117
   KOENG N, 2009, GAZEBO 3D MULTIPLE R
   Mirtich BV., 1996, Impulse-based dynamic simulation of rigid body systems
   Nocedal J., 1999, NUMERICAL OPTIMIZATI, DOI [DOI 10.1007/B98874, 10.1007/b98874]
   O'Sullivan C., 2003, ACM T GRAPH, V22
   POULSEN M, 2010, P WSCG
   Redon S., 2003, P IEEE INT C ROB AUT
   Schmidl H, 2004, IEEE T VIS COMPUT GR, V10, P189, DOI 10.1109/TVCG.2004.1260770
   Scholtes S., 1994, Introduction to Piecewise Differentiable Equations
   SILCOWITZ M, 2009, VRIPHYS 09, P105
   Smith R., 2000, OPEN DYNAMICS ENGINE
   Stewart D. E., 2000, SIAM REV
   STEWART DE, 1996, INT J NUMER METH ENG
   Stone, 1992, LINEAR COMPLEMENTARI
NR 23
TC 13
Z9 15
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 893
EP 901
DI 10.1007/s00371-010-0502-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800049
DA 2024-07-18
ER

PT J
AU Zhang, L
   He, Y
   Seah, HS
AF Zhang, Long
   He, Ying
   Seah, Hock-Soon
TI Real-time computation of photic extremum lines (PELs)
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time line drawing; Object-space algorithm; Gradient operator;
   Non-photorealistic shading
AB Photic extremum lines (PELs) are view-dependent, object-space feature lines that characterize the significant changes of surface illumination. Though very effective for conveying 3D shapes, PELs are computationally expensive due to the heavy involvement of the third- and fourth-order derivatives. Also, they require the user to manually place a few auxiliary lights to depict the model details, which is usually tedious work. To overcome these challenges, we present a novel computational framework that improves both the speed and quality of PELs. First, we derive a simple, closed-form formula of gradient operator such that various orders of derivatives can be computed efficiently and in parallel using graphics processing units (GPUs). The GPU-based PEL extraction algorithm is one order of magnitude faster than the original one. Second, we propose to extract PELs from various non-photorealistic shadings that not only depict the overall shape but also bring out the details at different frequencies simultaneously. As a result, the user can easily control the relative emphasis to different scales and obtain the desired line drawing results. We demonstrate the improved PELs on a wide range of real-world objects.
C1 [Zhang, Long; He, Ying; Seah, Hock-Soon] Nanyang Technol Univ, Singapore, Singapore.
   [Zhang, Long] Hangzhou Dianzi Univ, Hangzhou, Zhejiang, Peoples R China.
C3 Nanyang Technological University; Hangzhou Dianzi University
RP He, Y (corresponding author), Nanyang Technol Univ, Singapore, Singapore.
EM zhanglong@ntu.edu.sg; yhe@ntu.edu.sg; ashsseah@ntu.edu.sg
RI Seah, Hock Soon/AAK-9900-2020; He, Ying/A-3708-2011
OI Seah, Hock Soon/0000-0003-2699-7147; He, Ying/0000-0002-6749-4485
FU National Research Foundation; AcRF [69/07]; NSFC [60903085]; State Key
   Lab of CADCG [A0905]
FX This work has been supported by the National Research Foundation grant,
   which is administered by the Media Development Programme Office, MDA
   (IDMPO). Ying He is partially supported by AcRF 69/07. Long Zhang is
   partially supported by NSFC 60903085 and an Open Project Program of the
   State Key Lab of CAD&CG (A0905).
CR [Anonymous], CVPR
   BUCHANAN JW, 2000, P 1 INT S NONPH AN R, P39
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DECARLO D, 2007, P NPAR, P63
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   JUDD T, 2007, P ACM SIGGRAPH
   Kalogerakis E, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477937
   Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110
   Lee CH, 2006, IEEE T VIS COMPUT GR, V12, P197, DOI 10.1109/TVCG.2006.30
   Lee Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239469
   Maciejewski R, 2008, IEEE COMPUT GRAPH, V28, P62, DOI 10.1109/MCG.2008.35
   Ni A., 2006, P S INTERACTIVE 3D G, P133, DOI DOI 10.1145/1111411.1111435
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   RASKAR R, 2005, SIGGRAPH COURS NOT
   RUSINKIEWICZ S, 2008, SIGGRAPH COURS NOT
   Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Vergne R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531331
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xie X, 2007, IEEE T VIS COMPUT GR, V13, P1328, DOI 10.1109/TVCG.2007.70538
   Zhang Long., 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games, I3D '09, P129, DOI DOI 10.1145/1507149.1507170]
NR 24
TC 7
Z9 9
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 399
EP 407
DI 10.1007/s00371-010-0454-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800002
DA 2024-07-18
ER

PT J
AU Ahmed, OW
   Qahwaji, R
   Colak, T
   De Wit, TD
   Ipson, S
AF Ahmed, O. W.
   Qahwaji, R.
   Colak, T.
   De Wit, T. Dudok
   Ipson, S.
TI A new technique for the calculation and 3D visualisation of magnetic
   complexities on solar satellite images
SO VISUAL COMPUTER
LA English
DT Article
DE Active regions; Solar disk; Solar flares; Magnetic complexity; Energy;
   Satellite images; 3D Sun
AB In this paper, we introduce two novel models for processing real-life satellite images to quantify and then visualise their magnetic structures in 3D. We believe this multidisciplinary work is a real convergence between image processing, 3D visualisation and solar physics. The first model aims to calculate the value of the magnetic complexity in active regions and the solar disk. A series of experiments are carried out using this model and a relationship has been indentified between the calculated magnetic complexity values and solar flare events. The second model aims to visualise the calculated magnetic complexities in 3D colour maps in order to identify the locations of eruptive regions on the Sun. Both models demonstrate promising results and they can be potentially used in the fields of solar imaging, space weather and solar flare prediction and forecasting.
C1 [Ahmed, O. W.; Qahwaji, R.; Colak, T.; Ipson, S.] Univ Bradford, Sch Comp Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
   [De Wit, T. Dudok] Univ Orleans, LPC2E, F-45071 Orleans 2, France.
C3 University of Bradford; Centre National de la Recherche Scientifique
   (CNRS); Universite de Orleans
RP Ahmed, OW (corresponding author), Univ Bradford, Sch Comp Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
EM O.W.Ahmed@bradford.ac.uk; r.s.r.qahwaji@bradford.ac.uk;
   t.colak@bradford.ac.uk; ddwit@cnrs-orleans.fr; S.S.Ipson@Bradford.ac.uk
RI Dudok de Wit, Thierry/O-6478-2014
OI Dudok de Wit, Thierry/0000-0002-4401-0943
FU EPSRC [EP/F022948/1] Funding Source: UKRI
CR AHMED O, 2008, MOSH INT C COMM COMP, P43
   AHMED O, 2008, 5 INT MULT SYST SIGN, P1
   Ahsan A, 1998, BIOPHYS J, V74, P132, DOI 10.1016/S0006-3495(98)77774-4
   [Anonymous], 2008, INTRO SPACE WEATHER
   COLAK RQT, 2008, SPACE WEATHER
   Colak T, 2008, SOL PHYS, V248, P277, DOI 10.1007/s11207-007-9094-3
   DISPATCH ST, 1996, SOLAR TERRESTRIAL DI
   Feynman J, 2000, J GEOPHYS RES-SPACE, V105, P10543, DOI 10.1029/1999JA000141
   GONZALEZ RC, 1982, DIGITAL IMAGE PROCES
   Irback A, 1996, P NATL ACAD SCI USA, V93, P9533, DOI 10.1073/pnas.93.18.9533
   ISING E, 1925, Z PHYS, V31, P3
   KNOLL MJ, 2008, STAT PREDICTION SOLA
   Koskinen H., 2001, ESA SPACE WEATHER ST
   Lang KennethR., 1995, Sun, Earth and Sky
   LENZ D, 2004, IND PHYS, P18
   *NRC COMM SOC EC I, 2008, SEV SPAC WEATH EV UN
   *OOTFCF, 1995, MET NAT SPAC WEATH P
   Pick M., 2001, ESA SPACE WEATHER PR
   Qahwaji R, 2005, INT J IMAG SYST TECH, V15, P199, DOI 10.1002/ima.20053
   QAHWAJI R, 2007, IEEE C REC ADV SPAC
   ROWE GW, 1983, J THEOR BIOL, V101, P171, DOI 10.1016/0022-5193(83)90333-8
   Scherrer PH, 1995, SOL PHYS, V162, P129, DOI 10.1007/BF00733429
   *SCI NETLINKS, TRACK MOV SUNSP
   Sethna JP., 2007, STAT MECH ENTROPY OR
   WIERZCHON ST, 1996, ISING MODEL
   2004, SPACE TODAY, V2008
NR 26
TC 15
Z9 15
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 385
EP 395
DI 10.1007/s00371-010-0418-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800008
OA Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Charbonnier, C
   Assassi, L
   Volino, P
   Magnenat-Thalmann, N
AF Charbonnier, Caecilia
   Assassi, Lazhari
   Volino, Pascal
   Magnenat-Thalmann, Nadia
TI Motion study of the hip joint in extreme postures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE Motion capture; Physically-based simulation; Extreme motion; Hip
   osteoarthritis
ID SKIN MOVEMENT ARTIFACT; SOFT-TISSUE ARTIFACT; ORIENTATION IN-SPACE;
   ACETABULAR LABRUM; KNEE KINEMATICS; POSITION; COMPENSATION; ERRORS;
   BONES; SHANK
AB Many causes can be at the origin of hip osteoarthritis (e.g., cam/pincer impingements), but the exact pathogenesis for idiopathic osteoarthritis has not yet been clearly delineated. The aim of the present work is to analyze the consequences of repetitive extreme hip motion on the labrum cartilage. Our hypothesis is that extreme movements can induce excessive labral deformations and lead to early arthritis. To verify this hypothesis, an optical motion capture system is used to estimate the kinematics of patient-specific hip joint, while soft tissue artifacts are reduced with an effective correction method. Subsequently, a physical simulation system is used during motion to compute accurate labral deformations and to assess the global pressure of the labrum, as well as any local pressure excess that may be physiologically damageable. Results show that peak contact pressures occur at extreme hip flexion/abduction and that the pressure distribution corresponds with radiologically observed damage zones in the labrum.
C1 [Charbonnier, Caecilia; Assassi, Lazhari; Volino, Pascal] Univ Geneva, MIRALab, CH-1227 Carouge, Switzerland.
   [Magnenat-Thalmann, Nadia] Univ Geneva, MIRALAB CUI, CH-1211 Geneva 4, Switzerland.
C3 University of Geneva; University of Geneva
RP Charbonnier, C (corresponding author), Univ Geneva, MIRALab, 7 Rte Drize, CH-1227 Carouge, Switzerland.
EM charbonnier@miralab.unige.ch; thalmann@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021; Charbonnier, Caecilia/HLV-7680-2023
OI Thalmann, Nadia/0000-0002-1459-5960; Charbonnier,
   Caecilia/0000-0002-7018-885X; assassi, Lazhari/0000-0002-2551-7431
CR AHMET CC, 2007, TRENDS BIOMATERIALS, V21, P63
   Alexander EJ, 2001, J BIOMECH, V34, P355, DOI 10.1016/S0021-9290(00)00192-5
   [Anonymous], EUROGRAPHICS
   [Anonymous], 1995, FINITE ELEMENT PROCE
   Assassi L., 2007, P COMP ASS ORTH SURG, P259
   Benoit DL, 2006, GAIT POSTURE, V24, P152, DOI 10.1016/j.gaitpost.2005.04.012
   Binningsley D, 2003, BRIT J SPORT MED, V37, P84, DOI 10.1136/bjsm.37.1.84
   Bonet J., 1997, Nonlinear continuum mechanics for finite element analysis
   BROWN TD, 1983, J BIOMECH, V16, P373, DOI 10.1016/0021-9290(83)90071-4
   Cappello A, 2005, IEEE T BIO-MED ENG, V52, P992, DOI 10.1109/TBME.2005.846728
   CAPPOZZO A, 1995, CLIN BIOMECH, V10, P171, DOI 10.1016/0268-0033(95)91394-T
   Cappozzo A, 1996, CLIN BIOMECH, V11, P90, DOI 10.1016/0268-0033(95)00046-1
   Cereatti A, 2006, J NEUROENG REHABIL, V3, DOI 10.1186/1743-0003-3-7
   CHEGINI S, 2006, 7 INT S COMP METH BI, P705
   Chegini S, 2009, J ORTHOP RES, V27, P195, DOI 10.1002/jor.20747
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Della Croce U, 2005, GAIT POSTURE, V21, P226, DOI 10.1016/j.gaitpost.2004.05.003
   Dy CJ, 2008, J BONE JOINT SURG AM, V90A, P1464, DOI 10.2106/JBJS.G.00467
   Ferguson SJ, 2001, J ORTHOPAED RES, V19, P887, DOI 10.1016/S0736-0266(01)00007-9
   Ganz R, 2003, CLIN ORTHOP RELAT R, P112, DOI 10.1097/01.blo.0000096804.78689.c2
   Garling EH, 2007, J BIOMECH, V40, pS18, DOI 10.1016/j.jbiomech.2007.03.003
   Gilles B, 2005, ACAD RADIOL, V12, P1285, DOI 10.1016/j.acra.2005.08.006
   GILLES B, 2007, THESIS U GENEVE
   Gilles B, 2006, LECT NOTES COMPUT SC, V4190, P289
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   HAUTH M, 2003, EUR S COMP AN, P17
   Hauth M., 2004, PROC WSCG, V12, P137
   HAUTH M, 2001, EUROGRAPHICS 01, P137
   Hirota G, 2001, COMP ANIM CONF PROC, P136, DOI 10.1109/CA.2001.982387
   HODGE WA, 1989, J BONE JOINT SURG AM, V71A, P1378, DOI 10.2106/00004623-198971090-00015
   Holden JP, 1997, GAIT POSTURE, V5, P217, DOI 10.1016/S0966-6362(96)01088-0
   KEPPLE TM, 1994, J BIOMECH, V27, P365, DOI 10.1016/0021-9290(94)90012-4
   KUMAGAI M, 2003, SUMM BIOENG C, P53
   LAFORTUNE MA, 1992, J BIOMECH, V25, P347, DOI 10.1016/0021-9290(92)90254-X
   Lawrence CT, 2001, SIAM J OPTIMIZ, V11, P1092, DOI 10.1137/S1052623498344562
   Lewis CL, 2006, PHYS THER, V86, P110, DOI 10.1093/ptj/86.1.110
   Lu TW, 1999, J BIOMECH, V32, P129, DOI 10.1016/S0021-9290(98)00158-4
   Lucchetti L, 1998, J BIOMECH, V31, P977, DOI 10.1016/S0021-9290(98)00083-9
   Maciel A., 2005, P COMP ASS ORTH SURG, P298
   Manal K, 2002, GAIT POSTURE, V15, P10, DOI 10.1016/S0966-6362(01)00174-6
   Mason JB, 2001, CLIN SPORT MED, V20, P779
   Mavcic B, 2002, J ORTHOP RES, V20, P1025, DOI 10.1016/S0736-0266(02)00014-1
   Pfirrmann CWA, 2006, RADIOLOGY, V240, P778, DOI 10.1148/radiol.2403050767
   Picinbono G, 2003, GRAPH MODELS, V65, P305, DOI 10.1016/S1524-0703(03)00045-6
   Reynolds D, 1999, J BONE JOINT SURG BR, V81B, P281, DOI 10.1302/0301-620X.81B2.8291
   RUSSELL ME, 2006, J ORTHOPTERA RES, V1, P169
   SCHNEIDER P, 2003, M KAUFMANN SERIES CO
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   SODERKVIST I, 1993, J BIOMECH, V26, P1473, DOI 10.1016/0021-9290(93)90098-Y
   Stagni R, 2005, CLIN BIOMECH, V20, P320, DOI 10.1016/j.clinbiomech.2004.11.012
   Wu G, 2002, J BIOMECH, V35, P543, DOI 10.1016/S0021-9290(01)00222-6
NR 51
TC 18
Z9 22
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 873
EP 882
DI 10.1007/s00371-009-0317-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100007
DA 2024-07-18
ER

PT J
AU Hu, JX
   Hua, J
AF Hu, Jiaxi
   Hua, Jing
TI Salient spectral geometric features for shape matching and retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Shape matching; Spectral geometry; Geometric analysis
AB This paper introduces a new method for extracting salient features from surfaces that are represented by triangle meshes. Our method extracts salient geometric feature points in the Laplace-Beltrami spectral domain instead of usual spatial domains. Simultaneously, a spatial region is determined as a local support of each feature point, which is correspondent to the "frequency" where the feature point is identified. The local shape descriptor of a feature point is the Laplace-Beltrami spectrum of the spatial region associated to the points which are stable and distinctive. Our method leads to the salient spectral geometric features invariant to spatial transforms such as translation, rotation, and scaling. The properties of the discrete Laplace-Beltrami operator make them invariant to isometric deformations and mesh triangulations as well. With the scale information transformed from the "frequency", the local supporting region always maintains the same ratio to the original model no matter how it is scaled. This means that the spatial region is scale-invariant as well. Therefore, both global and partial matching can be achieved with these salient feature points. We demonstrate the effectiveness of our method with many experiments and applications.
C1 [Hu, Jiaxi; Hua, Jing] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
C3 Wayne State University
RP Hua, J (corresponding author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
EM jiaxihu@wayne.edu; jinghua@wayne.edu
CR [Anonymous], 2010, Vision
   Bemporad A., 1999, P EUR CONTR C
   Chavel I., 1984, EIGENVALUES RIEMANNI
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   Hua J, 2008, IEEE T VIS COMPUT GR, V14, P1643, DOI 10.1109/TVCG.2008.134
   Ip CY, 2002, P 7 ACM S SOL MOD AP, P273, DOI 10.1145/566282.566322
   IYER N, 2003, P ASME COMP INF ENG, P1
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Kazhdan M., 2003, S GEOMETRY PROCESSIN, P167
   KENDALL DG, 1977, ADV APPL PROBAB, V9, P428, DOI 10.2307/1426091
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   MEYER M., 2002, P VISMATH C, P1
   Novoselov VV, 2003, GENE EXPR PATTERNS, V3, P225, DOI 10.1016/S1567-133X(02)00077-7
   Ohbuchi R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P293, DOI 10.1109/PCCGA.2003.1238271
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   SADJADI FA, 1980, IEEE T PATTERN ANAL, V2, P127, DOI 10.1109/TPAMI.1980.4766990
   Shum HY, 1996, PROC CVPR IEEE, P526, DOI 10.1109/CVPR.1996.517122
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tung T, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P229, DOI 10.1109/SMI.2008.4547981
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
NR 26
TC 58
Z9 71
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 667
EP 675
DI 10.1007/s00371-009-0340-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300032
DA 2024-07-18
ER

PT J
AU Imagire, T
   Johan, H
   Nishita, T
AF Imagire, Takashi
   Johan, Henry
   Nishita, Tomoyuki
TI A fast method for simulating destruction and the generated dust and
   debris
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Destruction simulation; Fine debris; Dust generation; Voronoi diagram
AB Simulating the destruction of objects due to collisions has many applications in computer graphics. Previous methods on the destruction of objects perform physically-based simulation of the fracture of objects into relatively large size fragments. However, if we observe the process of the destruction of objects, we can see that dust and fine debris are also generated. In particular, previous methods do not take into account the dust generation. In this paper, we present a unified framework for simulating destruction and the generated dust and various sizes of debris. Our method simulates destruction on three different scales: coarse fracture, fine debris and dust. We compute the distribution and the amount of fine debris and dust based on the fracture energy which is the energy that causes the object to be fractured. We demonstrate the effectiveness of the proposed method, in terms of the generated effects and the simulation speed, by showing the simulation results of destruction caused by the collision between objects.
C1 [Imagire, Takashi] Univ Tokyo, Dept Complex Sci & Engn, Tokyo, Japan.
   [Imagire, Takashi] Namco Bandai Games Inc, Tokyo, Japan.
   [Johan, Henry] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Nishita, Tomoyuki] Univ Tokyo, Dept Informat Sci, Tokyo 113, Japan.
C3 University of Tokyo; Nanyang Technological University; University of
   Tokyo
RP Imagire, T (corresponding author), Univ Tokyo, Dept Complex Sci & Engn, Tokyo, Japan.
EM imagire@nis-lab.is.s.u-tokyo.ac.jp; henryjohan@ntu.edu.sg;
   nis@nis-lab.is.s.u-tokyo.ac.jp
RI Johan, Henry/A-3707-2011
CR BELL N, 2005, P 2005 ACM SIGGRAPH, P7
   BOND FC, 1952, T AM I MIN MET ENG, V193, P484
   Chen YZ, 1999, PROG NAT SCI-MATER, V9, P81
   Crane K., 2007, GPU GEMS, P633
   CUNDALL PA, 1979, GEOTECHNIQUE, V29, P47, DOI 10.1680/geot.1979.29.1.47
   Gaudin AM, 1926, T AM I MIN MET ENG, V73, P253
   KICH F, 1985, GESETZ PROPORIONALEM
   Meguro K., 1989, Structural Engineering/Earthquake Engineering, V6, P283
   MULLER M, 2001, EUROGRAPHICS 2001 CO, P27
   Neyret F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P147
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Norton A., 1991, Visual Computer, V7, P210, DOI 10.1007/BF01900837
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Rosin P., 1933, J I FUEL, V7, P29, DOI DOI 10.2514/3.62687
   Schuhmann R., 1940, AIMETP, P1189
   Smith J, 2001, COMPUT GRAPH FORUM, V20, P81, DOI 10.1111/1467-8659.t01-1-00202
   Stam Jos., 1995, Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, SIGGRAPH '95, P129
   Terzopoulos D., 1988, Proceedings of the 15th annual conference on Computer graphics and interactive techniques, P269
   von Rittinger P., 1867, LEHRBUCH AUFBEREITUN
   Zhang N., 2006, S POINT BAS GRAPH, P145
NR 21
TC 5
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 719
EP 727
DI 10.1007/s00371-009-0319-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300038
DA 2024-07-18
ER

PT J
AU Liu, YL
   Qin, XY
   Xu, SH
   Nakamae, E
   Peng, QS
AF Liu, Yanli
   Qin, Xueying
   Xu, Songhua
   Nakamae, Eihachiro
   Peng, Qunsheng
TI Light source estimation of outdoor scenes for mixed reality
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Light source recovery; Outdoor scenes; Augmented reality
AB Illumination consistency is important for photorealistic rendering of mixed reality. However, it is usually difficult to acquire illumination conditions of natural environments. In this paper, we propose a novel method for evaluating the light conditions of a static outdoor scene without knowing its geometry, material, or texture. In our method, we separate respectively the shading effects of the scene due to sunlight and skylight through learning a set of sample images which are captured with the same sun position. A fixed illumination map of the scene under sunlight or skylight is then derived reflecting the scene geometry, surface material properties and shadowing effects. These maps, one for sunlight and the other for skylight, are therefore referred to as basis images of the scene related to the specified sun position. We show that the illumination of the same scene under different weather conditions can be approximated as a linear combination of the two basis images. We further extend this model to estimate the lighting condition of scene images under deviated sun positions, enabling virtual objects to be seamlessly integrated into images of the scene at any time. Our approach can be applied for online video process and deal with both cloudy and sun shine situations. Experiment results successfully verify the effectiveness of our approach.
C1 [Liu, Yanli; Qin, Xueying; Xu, Songhua; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Nakamae, Eihachiro] Sanei Co, Hiroshima, Japan.
C3 Zhejiang University; Shandong University
RP Qin, XY (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM xyqin@cad.zju.edu.cn; peng@cad.zju.edu.cn
RI Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
CR Agusanto K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P208, DOI 10.1109/ISMAR.2003.1240704
   ANDERSEN MS, 2006, ICPR, V4, P91
   Choudhury B, 2006, GRAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P176
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Debevec P., 1998, SIGGRAPH98, P189, DOI DOI 10.1145/280814.280864
   Gibson S., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P219
   Hara K, 2005, IEEE T PATTERN ANAL, V27, P493, DOI 10.1109/TPAMI.2005.82
   Jacobs K, 2006, COMPUT GRAPH FORUM, V25, P29, DOI 10.1111/j.1467-8659.2006.00816.x
   Jacobs N, 2007, IEEE I CONF COMP VIS, P1305
   KIM FJP, 2008, CVPR
   KOPPAL SJ, 2006, CVPR, V2, P1323
   LALONDE SGN, 2008, ECCV
   Loscos C, 2000, IEEE T VIS COMPUT GR, V6, P289, DOI 10.1109/2945.895874
   Matsushita Y, 2004, LECT NOTES COMPUT SC, V3022, P274
   NAKAMAE E, 1986, P SIGGRAPH 86 COMPUT, V20, P207
   Narasimhan SG, 2002, LECT NOTES COMPUT SC, V2352, P148
   Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977
   Rees W.G., 1990, PHYS PRINCIPLES REMO
   Sato I, 1999, IEEE T VIS COMPUT GR, V5, P1, DOI 10.1109/2945.764865
   Sato I., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P306, DOI 10.1109/CVPR.1999.786956
   Seitz SM, 2005, IEEE I CONF COMP VIS, P1440
   Sharma S, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/785364
   Sunkavalli K., 2008, CVPR
   Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276504, 10.1145/1239451.1239552]
   Tadamura K., 1993, Comput. Graph. Forum, V12, P189, DOI [10.1111/1467-8659.1230189, DOI 10.1111/1467-8659.1230189]
   Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185
   Wang Y, 2003, GRAPH MODELS, V65, P185, DOI 10.1016/S1524-0703(03)00043-2
   Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606
   YU Y, 1999, SIGGRAPH, V99, P215
   Zhang YF, 2001, IEEE T PATTERN ANAL, V23, P915, DOI 10.1109/34.946995
NR 30
TC 33
Z9 46
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 637
EP 646
DI 10.1007/s00371-009-0342-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300029
DA 2024-07-18
ER

PT J
AU Mac Manus, L
   Iwasaki, M
   Kanamori, K
   Sato, S
   Dodgson, NA
AF Mac Manus, Lorcan
   Iwasaki, Masahiro
   Kanamori, Katsuhiro
   Sato, Satoshi
   Dodgson, Neil Anthony
TI Inherent limitations on specular highlight analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Microfacet; Texture synthesis; Specular reflection
ID APPEARANCE
AB We analyse specular highlight modelling using microfacet-based physics illumination models. The ability to perform effective modelling is shown to depend on the ratio of the quantisation noise, epsilon, in the normal data to the object's surface roughness parameter, m. We characterise how the accuracy degrades with increasing normal vector noise, when fitting is done in the Least Mean Squares sense. We show that it is not possible to accurately characterise sharp specular highlights, unless epsilon is very much less than m, and give examples of the practical implications of this theoretical result. We observe that the recently reported frequency-domain approaches can obscure this problem. We also present a novel characterisation of the importance of the geometric attenuation term in the microfacet models.
C1 [Iwasaki, Masahiro; Kanamori, Katsuhiro; Sato, Satoshi] Panasonic Corp, Adv Technol Res Labs, Seika, Kyoto 6190237, Japan.
   [Mac Manus, Lorcan; Dodgson, Neil Anthony] Univ Cambridge, Comp Lab, Cambridge CB3 0FD, England.
C3 Panasonic; University of Cambridge
RP Dodgson, NA (corresponding author), Univ Cambridge, Comp Lab, 15 JJ Thomson Ave, Cambridge CB3 0FD, England.
EM nad@cl.cam.ac.uk
RI Dodgson, Neil/A-4506-2009
OI Dodgson, Neil/0000-0001-7649-8528
CR ASHIKHMIN M, 2006, DISTRIBUTION BASED B
   Blinn JF, 1977, P 4 ANN C COMP GRAPH, P192, DOI [10.1145/563858.563893, DOI 10.1145/563858.563893]
   COOK RL, 1981, P SIGGRAPH, P307
   GELLERT W., 1977, VNR CONCISE ENCY MAT
   Glassner A.S., 1995, PRINCIPLES DIGITAL I, VTwo
   Gunther J., 2005, VISION, MODELING, AND VISUALIZATION 2005 (VMV'05), P487
   IKEUCHI K, 1991, IEEE T PATTERN ANAL, V13, P1139, DOI 10.1109/34.103274
   Lafortune E. P., 1994, Using the modified Phong reflectance model for physically based rendering
   LAFORTUNE EPF, 1997, P SIGGRAPH, P117
   Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Matusik W., 2003, PhD thesis
   Müller G, 2006, COMPUT GRAPH FORUM, V25, P369, DOI 10.1111/j.1467-8659.2006.00956.x
   Nayar S. K., 1989, CMURITR897
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271
   Rusinkiewicz S, 2002, ACM T GRAPHIC, V21, P438, DOI 10.1145/566570.566600
   Sato I, 2007, INT J COMPUT VISION, V75, P29, DOI 10.1007/s11263-007-0036-1
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Schruder P., 1995, Proc. 22nd Ann. Conf. Comput. Graphics Interactive Techniques (SIGGRAPH'95), P161
   Tanaka N, 2000, INT C PATT RECOG, P596, DOI 10.1109/ICPR.2000.903616
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Weston S., 2004, Proceedings of the Society of Feed Technologists, 2003, P1
NR 24
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 647
EP 656
DI 10.1007/s00371-009-0331-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300030
DA 2024-07-18
ER

PT J
AU Ha, D
   Han, J
AF Ha, Dongwook
   Han, JungHyun
TI Motion synthesis with decoupled parameterization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE character animation; motion capture; motion blending; parametric motion
   synthesis
AB In real-time animation systems, motion interpolation techniques are widely used for their controllability and efficiency. The techniques sample the parameter space using example motions, and interpolate them to compute the blend weights corresponding to the given parameters. A main problem of the techniques is that, as the dimension n of the parameter space increases, the number of required example motions increases exponentially, i.e. O(c(n)). To resolve the problem, this paper proposes to use two decoupled parameter spaces for controlling the upper body and the lower body separately. At each frame time, a parameterized motion space produces a source frame, and the target frame is synthesized by splicing the upper body of one source frame with the lower body of the other. In order to have the two source frames correlated with each other, a time-warping scheme has been developed. Furthermore, in order to handle the dynamic properties of the parameter samples of the upper body, we have developed an approximation technique for quickly determining the sample positions in its parameter space. This decoupled parameterization method alleviates the complexity problem, e.g. from O(c(6)) to O(c(3)), while providing the users with the capability of convenient control over the character.
C1 [Ha, Dongwook; Han, JungHyun] Korea Univ, Game Res Ctr, Seoul 136713, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Game Res Ctr, Seoul 136713, South Korea.
EM zzuree@korea.ac.kr; jhan@korea.ac.kr
CR [Anonymous], SCA 06 P 2006 ACM SI
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   ARIKAN O, 2002, SIGGRAPH 02, P483
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   Gleicher M, 2003, ACM T GRAPHIC, V22, P702, DOI 10.1145/882262.882333
   Heck R, 2006, COMPUT GRAPH FORUM, V25, P459, DOI 10.1111/j.1467-8659.2006.00965.x
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   IKEMOTO L, 2004, SCA 04, P99
   Jang WS, 2008, VISUAL COMPUT, V24, P271, DOI 10.1007/s00371-007-0200-1
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KOVAR L, 2004, SIGGRAPH 04, P559
   KOVAR L, 2003, SCA 03, P214
   Lee J, 2006, GRAPH MODELS, V68, P158, DOI 10.1016/j.gmod.2005.03.004
   Lee J.J., 2002, P491
   MAJKOWSKA A, 2006, SCA 06, P309
   MUKAI T, 2005, SIGGRAPH 05, P1062
   Park SI, 2004, COMPUT ANIMAT VIRT W, V15, P125, DOI 10.1002/cav.15
   Park SI., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. SCA2, P105, DOI DOI 10.1145/545261.545279
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Rose CF, 2001, COMPUT GRAPH FORUM, V20, pC239
   Wiley DJ, 1997, IEEE COMPUT GRAPH, V17, P39, DOI 10.1109/38.626968
NR 23
TC 7
Z9 10
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 587
EP 594
DI 10.1007/s00371-008-0239-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800014
DA 2024-07-18
ER

PT J
AU Oztireli, AC
   Basdogan, C
AF Oztireli, A. Cengiz
   Basdogan, Cagatay
TI A new feature-based method for robust and efficient rigid-body
   registration of overlapping point clouds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE 3D registration; feature extraction; distance invariants; geometric
   descriptors; nearest neighbor search
ID ALGORITHM
AB We propose a new feature-based registration method for rigid-body alignment of overlapping point clouds (PCs) efficiently under the influence of noise and outliers. The proposed registration method is independent of the initial position and orientation of PCs, and no assumption is necessary about their underlying geometry. In the process, we define a simple and efficient geometric descriptor, a novel k-NN search algorithm that outperforms most of the existing nearest neighbor search algorithms used for the same task, and a new algorithm to find corresponding points between PCs based on the invariance of Euclidian distance under rigid-body transformation.
C1 [Basdogan, Cagatay] Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
   [Oztireli, A. Cengiz] ETH, Dept Comp Sci, CH-8092 Zurich, Switzerland.
C3 Koc University; Swiss Federal Institutes of Technology Domain; ETH
   Zurich
RP Basdogan, C (corresponding author), Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
EM cengizo@student.ethz.ch; cbasdogan@ku.edu.tr
RI Basdogan, Cagatay/O-9184-2019
OI Basdogan, Cagatay/0000-0002-6382-7334
CR AKCA D., 2003, Optical 3-D Measurement Techniques, VVI I, P330, DOI DOI 10.3929/ETHZ-A-004656666
   [Anonymous], QSLIM SIMPLIFICATION
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BORNAZ L, 2002, CIPA WG 6 INT WORKSH, P52
   Chen CS, 1999, IEEE T PATTERN ANAL, V21, P1229, DOI 10.1109/34.809117
   Dorai C, 1998, IEEE T PATTERN ANAL, V20, P83, DOI 10.1109/34.655652
   Frome A., 2004, EUROPEAN C COMPUTER, P224
   Funkhouser T, 2005, COMMUN ACM, V48, P58, DOI 10.1145/1064830.1064859
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Liu Yonghuai, 2002, J Digit Imaging, V15 Suppl 1, P267, DOI 10.1007/s10278-002-5006-0
   MITRA NJ, 2003, SCG 03, P322
   Nene SA, 1997, IEEE T PATTERN ANAL, V19, P989, DOI 10.1109/34.615448
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pottmann, 2005, ACM INT C P SERIES, V255, P197
   Turk Greg., Large geometric models archive
   [No title captured]
NR 18
TC 20
Z9 36
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 679
EP 688
DI 10.1007/s00371-008-0248-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800023
DA 2024-07-18
ER

PT J
AU Zhao, Y
AF Zhao, Ye
TI Lattice boltzmann based PDE solver on the GPU
SO VISUAL COMPUTER
LA English
DT Article
DE lattice Boltzmann model; diffusion; Laplace and Poisson equation; volume
   smoothing; surface fairing; image editing
ID DIFFUSION
AB In this paper, we propose a hardware-accelerated PDE (partial differential equation) solver based on the lattice Boltzmann model (LBM). The LBM is initially designed to solve fluid dynamics by constructing simplified microscopic kinetic models. As an explicit numerical scheme with only local operations, it has the advantage of being easy to implement and especially suitable for graphics hardware (GPU) acceleration. Beyond the Navier-Stokes equation of fluid mechanics, a typical LBM can be modified to solve the parabolic diffusion equation, which is further used to solve the elliptic Laplace and Poisson equations with a diffusion process. These PDEs are widely used in modeling and manipulating images, surfaces and volumetric data sets. Therefore, the LBM scheme can be used as an GPU-based numerical solver to provide a fast and convenient alternative to traditional implicit iterative solvers. We apply this method to several examples in volume smoothing, surface fairing and image editing, achieving outstanding performance on contemporary graphics hardware. It has the great potential to be used as a general GPU computing framework for efficiently solving PDEs in image processing, computer graphics and visualization.
C1 Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
C3 University System of Ohio; Kent State University; Kent State University
   Salem; Kent State University Kent
RP Zhao, Y (corresponding author), Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
EM zhao@cs.kent.edu
CR [Anonymous], P C GRAPH HARDW
   [Anonymous], P ACM SIGGRAPH EUROG
   BELLA G, 2002, P 4 EUR WORKSH OPENM
   BHATNAGAR PL, 1954, PHYS REV, V94, P511, DOI 10.1103/PhysRev.94.511
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   BORIS J, 1987, ANNU REV FLUID MECH, V21, P695
   Buick JM, 2000, PHYS REV E, V61, P5307, DOI 10.1103/PhysRevE.61.5307
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Desplat JC, 2001, COMPUT PHYS COMMUN, V134, P273, DOI 10.1016/S0010-4655(00)00205-8
   Fan Z., 2004, SC 04, P47, DOI [DOI 10.1109/SC.2004.26, 10.1109/SC.2004.26]
   Gibson SFF, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P23, DOI 10.1109/SVV.1998.729581
   He XY, 1997, J STAT PHYS, V88, P927, DOI 10.1023/B:JOSS.0000015179.12689.e4
   Jawerth B, 1999, J MATH IMAGING VIS, V11, P231, DOI 10.1023/A:1008304519705
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Krüger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   LEFOHN A, 2002, UUSCS02017
   Li W., 2005, Flow Simulation with Complex Boundaries, GPU Gems 2, P747
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MASSAIOLI F, 2002, SCICOMP06 TALK
   Museth K, 2002, ACM T GRAPHIC, V21, P330, DOI 10.1145/566570.566585
   Neumann Laszlo;., 2002, PROC VISSYM, P105
   Owens J.D., 2005, P EUROGRAPHICS 2005, P21
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   RODGMAN D, 2006, SCI VISUALIZATION VI, P163
   Rumpf M, 2001, SPRING EUROGRAP, P75
   Sramek M, 1999, IEEE T VIS COMPUT GR, V5, P251, DOI 10.1109/2945.795216
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   SUCCI S, 2001, L BOLTZMANN EQUATION
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Thuerey N., 2004, VISION MODELING VISU, P199
   van der Sman RGM, 1999, J STAT PHYS, V94, P203, DOI 10.1023/A:1004515413793
   WANG SW, 1994, IEEE COMPUT GRAPH, V14, P26, DOI 10.1109/38.310721
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P719, DOI 10.1109/TVCG.2004.48
   WHITAKER R, 2000, P S VOL VIS OCT, P23
   WOLFGLADROW D, 1995, J STAT PHYS, V79, P1023, DOI 10.1007/BF02181215
   Zhao Y, 2007, IEEE T VIS COMPUT GR, V13, P179, DOI 10.1109/TVCG.2007.24
   Zhao Y, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P181
NR 39
TC 83
Z9 108
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 323
EP 333
DI 10.1007/s00371-007-0191-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200002
DA 2024-07-18
ER

PT J
AU Debard, JB
   Balp, R
   Chaine, R
AF Debard, Jean-Baptiste
   Balp, Romain
   Chaine, Raphaelle
TI Dynamic delaunay tetrahedralisation of a deforming surface
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE delaunay tetrahedralisation; surface reconstruction; particle sampling
ID TRIANGULATION
AB Reconstruction algorithms make it possible to retrieve a surface from the Delaunay tetrahedralisation (DT) of a point sampling, whose density reflects the surface local geometry and thickness. Most of these algorithms are static and some work remains to be done to handle deforming surfaces. In such case, we defend the idea that each point of the sampling should move with the surface using the information given by the motion to allow fast reconstruction. In this article, we tackle the problem of producing a good evolving sampling of a deforming surface S, and maintaining its DT along the motion. The surface is known only through a projection operator (O-1):R-3 -> S, and a normal operator (O-2) that returns the oriented normal at a point on the surface. On that basis, we offer some perspectives on how reconstruction algorithms can be extended to the tracking of deforming surfaces.
C1 Tsinghua Univ, Dept Comp Sci, Beijing, Peoples R China.
   Univ Lyon 1, LIRIS, Lyon, France.
C3 Tsinghua University; Institut National des Sciences Appliquees de Lyon -
   INSA Lyon; Universite Claude Bernard Lyon 1
RP Debard, JB (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing, Peoples R China.
EM jb.debard@gmail.com; romain.balp@gmail.com;
   raphaelle.chaine@liris.cnrs.fr
CR Allègre R, 2007, COMPUT GRAPH-UK, V31, P190, DOI 10.1016/j.cag.2006.11.013
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   BORODACHOV SV, 2006, ASYMPTOTICS BEST PAC
   Chaine R., 2003, Symposium on Geometry Processing, P218
   Cheng HL, 2001, SIAM PROC S, P47
   Crossno P, 1997, VISUALIZATION '97 - PROCEEDINGS, P495, DOI 10.1109/VISUAL.1997.663930
   de Figueiredo L. H., 1992, Proceedings. Graphics Interface '92, P250
   Dey T.K., 2006, CURVE SURFACE RECONS
   Edelsbrunner H., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P285, DOI 10.1145/177424.178010
   Giesen J., 2006, DELAUNAY TRIANGULATI
   GUIBAS L, 2004, SCG 04 P 20 ANN S CO, P170
   GUIBAS L, 2004, KINETIC DATA STRUCTU
   Hardin D., 2004, AMS, V51, P1186
   Karkanis T, 2001, IEEE COMPUT GRAPH, V21, P60, DOI 10.1109/38.909016
   Levet F, 2006, P IEEE INT C HAP MOD, P39
   LEVET F, 2005, J WSCG, V13
   Meyer MD, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/SMI.2005.41
   Mousa M., 2006, Journal of Graphics Tools, V11, P17
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   ROCH A, 1997, EUROGRAPH COMPUT GRA, V16, P295
   Rodrian H.-C., 1996, P IMPL SURF 96, P37
   SANKARANARAYANA.HS, 2006, P 3 IEEE EUR S POINT
   SZELISKI R, 1992, COMP GRAPH, V26, P185, DOI 10.1145/142920.134037
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   Witkin A. P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P269, DOI 10.1145/192161.192227
   ZONENSCHEIN R, 1998, P 3 EUR WORKSH IMPL, P131
NR 28
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 975
EP 986
DI 10.1007/s00371-007-0172-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400003
DA 2024-07-18
ER

PT J
AU Unger, J
   Gustavson, S
   Ynnerman, A
AF Unger, Jonas
   Gustavson, Stefan
   Ynnerman, Anders
TI Spatially varying image based lighting by light probe sequences -
   Capture, processing and rendering
SO VISUAL COMPUTER
LA English
DT Article
DE high dynamic range imaging; image based lighting
AB We present a novel technique for capturing spatially or temporally resolved light probe sequences, and using them for image based lighting. For this purpose we have designed and built a real-time light probe, a catadioptric imaging system that can capture the full dynamic range of the lighting incident at each point in space at video frame rates, while being moved through a scene. The real-time light probe uses a digital imaging system which we have programmed to capture high quality, photometrically accurate color images of 512x512 pixels with a dynamic range of 10000000:1 at 25 frames per second.
   By tracking the position and orientation of the light probe, it is possible to transform each light probe into a common frame of reference in world coordinates, and map each point and direction in space along the path of motion to a particular frame and pixel in the light probe sequence. We demonstrate our technique by rendering synthetic objects illuminated by complex real world lighting, first by using traditional image based lighting methods and temporally varying light probe illumination, and second an extension to handle spatially varying lighting conditions across large objects and object motion along an extended path.
C1 Linkoping Univ, VITA, Dept Sci & Technol, S-60174 Norrkoping, Sweden.
C3 Linkoping University
RP Unger, J (corresponding author), Linkoping Univ, VITA, Dept Sci & Technol, S-60174 Norrkoping, Sweden.
EM jonas.unger@itn.liu.se
CR Adelson EH, 1991, COMPUTATIONAL MODELS
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   [Anonymous], 2005, MAX PLANK INSTITUT I
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   DEBEVEC P, 1998, P SIGGRAPH 98
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270
   Larson GregW., 1992, GRAPHICS GEMS 2, P80, DOI [10.1016/B978-0-08-050754-5.50025-6, DOI 10.1016/B978-0-08-050754-5.50025-6]
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Madden BrianC., 1993, Extended intensity range imaging
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   Masselus V, 2003, ACM T GRAPHIC, V22, P613, DOI 10.1145/882262.882315
   MILLER GS, 1984, SIGGRAPH 84 COURS NO
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Reinhard E., 2006, HIGH DYNAMIC RANGE I, DOI 10.1016/B978-012585263-0/50005-1
   Robertson M. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P159, DOI 10.1109/ICIP.1999.817091
   Sato I, 1999, IEEE T VIS COMPUT GR, V5, P1, DOI 10.1109/2945.764865
   Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1
   Unger J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P141
   UNGER J, 2006, P 4 INT C COMP GRAPH
   Unger J, 2007, PROC SPIE, V6501, DOI 10.1117/12.703050
   Unger Jonas., 2004, P 25 EUROGRAPHICS AN, P17
   Waese J., 2002, P 27 ANN C COMP GRAP, P247
NR 25
TC 10
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 453
EP 465
DI 10.1007/s00371-007-0127-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Crespo, JL
   Zorrilla, M
   Bernardos, P
   Mora, E
AF Crespo, Jose Luis
   Zorrilla, Marta
   Bernardos, Pilar
   Mora, Eduardo
TI A new image prediction model based on spatio-temporal techniques
SO VISUAL COMPUTER
LA English
DT Article
DE causal images; spatio-temporal autoregressive model; tracking and
   prediction; image sequence; compression
ID ACTIVE SHAPE MODELS; ALGORITHM
AB This paper addresses an image prediction problem focused on images with no identifiable objects. In it, we present several approaches to predict the next image of a given sequence, when the image lacks the well-defined objects, such as meteorological maps or satellite imagery. In these images no clear borders are present, and any object candidate moves, changes, appears and disappears in any image. Nevertheless, this evolution, though unrestricted, is gradual and, hence, prediction looks feasible. One of the approaches presented here, based on a spatio-temporal autoregressive (STAR) model, offers good results for these kinds of images. The main contribution of this paper is to adapt spatio-temporal models to an image prediction problem. As a byproduct of this research, we have achieved a new image compression method, suitable for images without defined shapes.
C1 Univ Cantabria, Dept Appl Math & Comp Sci, E-39005 Santander, Spain.
C3 Universidad de Cantabria
RP Crespo, JL (corresponding author), Univ Cantabria, Dept Appl Math & Comp Sci, E-39005 Santander, Spain.
EM crespoj@unican.es
RI zorrilla, marta/I-7617-2012
OI zorrilla, marta/0000-0002-0475-8834
CR [Anonymous], 1987, ALAMOS NATL LAB REPO, DOI DOI 10.1109/NNSP.1991.239502
   Bakker R, 2000, NEURAL COMPUT, V12, P2355, DOI 10.1162/089976600300014971
   Beichel R, 2005, IEEE T MED IMAGING, V24, P1151, DOI 10.1109/TMI.2005.853237
   Bors AG, 2000, IEEE T IMAGE PROCESS, V9, P1441, DOI 10.1109/83.855440
   Box GEP, 1976, TIME SERIES ANAL FOR
   Butcher D, 1999, IEE CONF PUBL, P392, DOI 10.1049/cp:19990350
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, PROC SPIE, V4322, P236, DOI 10.1117/12.431093
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   COOTES TF, 1999, P BRIT MACH VIS C, V1, P173
   Crespo JL, 2005, LECT NOTES COMPUT SC, V3643, P101, DOI 10.1007/11556985_15
   CRESPO JL, 2004, LECT NOTES COMPUTER, V2809, P65
   CRESPO JL, 1992, THESIS U CANTABRIA
   Danyali H, 2004, IEE P-VIS IMAGE SIGN, V151, P498, DOI 10.1049/ip-vis:20040734
   DEUTSCH SJ, 1986, WATER RESOUR BULL, V22, P967
   Egmont-Petersen M, 2002, PATTERN RECOGN, V35, P2279, DOI 10.1016/S0031-3203(01)00178-9
   ERKELENS JS, 2005, AUTOREGRESSIVE MODEL
   FAHLMAN S, 1991, CMUCS9100
   Hill A, 1996, IMAGE VISION COMPUT, V14, P601, DOI 10.1016/0262-8856(96)01097-9
   Jordan M. I., 1986, P 8 ANN C COGN SCI S
   Marshall J A, 2000, Int J Neural Syst, V10, P59, DOI 10.1016/S0129-0657(00)00006-5
   *MI STAT U DEP FOR, 2006, INT ENV AN STARMA MO
   Mincer J., 1969, EVALUATION EC FORECA
   Minsky M., 1967, Computation: finite and infinite machines
   Mitiche L, 2004, SIGNAL PROCESS, V84, P1805, DOI 10.1016/j.sigpro.2004.05.029
   Pace RK, 1998, J REAL ESTATE FINANC, V17, P15, DOI 10.1023/A:1007799028599
   Pearlmutter BA, 1989, NEURAL COMPUT, V1, P263, DOI 10.1162/neco.1989.1.2.263
   PFEIFER PE, 1980, TECHNOMETRICS, V22, P35, DOI 10.2307/1268381
   Pineda FJ, 1989, NEURAL COMPUT, V1, P161, DOI 10.1162/neco.1989.1.2.161
   RAO TS, 2004, IMA VOL MATH APPL, V139, P123
   SZUMMER M, 1996, P 1996 IEEE INT C IM
   Weigend A. S., 1993, Time Series Prediction
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
NR 33
TC 14
Z9 14
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 419
EP 431
DI 10.1007/s00371-007-0114-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800004
DA 2024-07-18
ER

PT J
AU Ji, JF
   Wu, EH
   Li, S
   Liu, XH
AF Ji, Junfeng
   Wu, Enhua
   Li, Sheng
   Liu, Xuehui
TI View-dependent refinement of multiresolution meshes using programmable
   graphics hardware
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE view-dependent multiresolution rendering; level of detail (LOD);
   quadtree; texture atlas; graphics processing unit (GPU)
AB View-dependent multiresolution rendering places a heavy load on CPU. This paper presents a new method on view-dependent refinement of multiresolution meshes by using the computation power of modern programmable graphics hardware (GPU). Two rendering passes using this method are included. During the first pass, the level of detail selection is performed in the fragment shaders. The resultant buffer from the first pass is taken as the input texture to the second rendering pass by vertex texturing, and then the node culling and triangulation can be performed in the vertex shaders. Our approach can generate adaptive meshes in real-time, and can be fully implemented on GPU. The method improves the efficiency of mesh simplification, and significantly alleviates the computing load on CPU.
C1 Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macao, Peoples R China.
   Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
   State Informat Ctr, Beijing, Peoples R China.
   Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; University of
   Macau; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Peking University
RP Ji, JF (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
EM jijfn@cei.gov.cn; ehwu@umac.mo; lisheng@graphics.pku.edu.cn;
   lxh@ios.ac.cn
RI Lin, Fan/JZT-1441-2024
OI Lin, Fan/0000-0002-7330-3833
NR 0
TC 11
Z9 14
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 424
EP 433
DI 10.1007/s00371-006-0020-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500006
DA 2024-07-18
ER

PT J
AU Ben Azouz, Z
   Rioux, M
   Shu, C
   Lepage, R
AF Ben Azouz, Z
   Rioux, M
   Shu, C
   Lepage, R
TI Characterizing human shape variation using 3D anthropometric data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Animation and Social Agent Conference (CASA 2004)
CY JUL, 2004
CL Univ Geneva, Geneva, SWITZERLAND
HO Univ Geneva
DE 3D anthropometry; volumetric description; human body modeling
AB Characterizing the variations of the human body shape is fundamentally important in many applications ranging from animation to product design. 3D scanning technology makes it possible to digitize the complete surfaces of a large number of human bodies, providing much richer information about the body shape than traditional anthropometric measurements. This technology opens up opportunities to extract new measurements for quantifying the body shape. In this paper, we present a new method for extracting the main modes of variations of the human shape from a 3D anthropometric database. Previous approaches rely on anatomical landmarks. Using a volumetric representation, we show that human shape analysis can be performed despite the lack of such information. We first introduce a technique for repairing the 3D models from the original scans. Principal component analysis analysis is then applied to the volumetric description of a set of human models to extract dominant components of shape variability for a target population. We demonstrate a good reconstruction of the original models from a reduced number of components. Finally, we provide tools for visualizing the main modes of human shape variation.
C1 Natl Res Council Canada, Inst Informat Technol, Ottawa, ON K1A 0R6, Canada.
   Ecole Technol Super Montreal, Montreal, PQ H3C 1K3, Canada.
C3 National Research Council Canada; University of Quebec; Ecole de
   Technologie Superieure - Canada
RP Natl Res Council Canada, Inst Informat Technol, 1200 Montreal Rd,Bldg M-50, Ottawa, ON K1A 0R6, Canada.
EM Zouhour.benazouz@nrc-cnrc.gc.ca; Marc.Rioux@nrc-cnrc.gc.ca;
   Chang.Shu@nrc-cnrc.gc.ca; Richard.Lepage@etsmtl.ca
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   AZOUZ ZB, 2003, P 15 TRIENN C INT ER
   Burnsides D, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P393, DOI 10.1109/IM.2001.924485
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4
   Davis J., 2002, P 1 INT S 3D DAT PRO
   Jones MW, 1996, COMPUT GRAPH FORUM, V15, P311, DOI 10.1111/1467-8659.1550311
   Kaufman A., 1991, VOLUME VISUALIZATION
   KAUFMAN A, 1987, P EUROGRAPHICS, P197
   KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Magnenat-Thalmann N, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P2, DOI 10.1109/IM.2003.1240226
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   PAQUET E, 2000, J ELECT IMAGING, V9
   ROBINETTE K, 1997, NEUILLY SUR SEINE N
   Robinette K. M., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P380, DOI 10.1109/IM.1999.805368
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Sheldon William., 1940, VARIETIES HUMAN PHYS
   STRALEN M, 2003, P 15 TRIENN C INT ER
   Taubin G., 2000, EUROGRAPHICS
   Wang S. W., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P78, DOI 10.1109/VISUAL.1993.398854
NR 21
TC 38
Z9 49
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 302
EP 314
DI 10.1007/s00371-006-0006-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500003
OA Green Published
DA 2024-07-18
ER

PT J
AU Jia, YT
   Hu, SM
   Martin, RR
AF Jia, YT
   Hu, SM
   Martin, RR
TI Video completion using tracking and fragment merging
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE video completion; texture synthesis; mean shift; graph cut; tracking
ID IMAGE
AB Video completion is the problem of automatically filling space-time holes in video sequences left by the removal of unwanted objects in a scene. We solve it using texture synthesis, filling a hole inwards using three steps iteratively: we select the most promising target pixel at the edge of the hole, we find the source fragment most similar to the known part of the target's neighborhood, and we merge source and target fragments to complete the target neighborhood, reducing the size of the hole.
   Earlier methods were slow, due to searching the whole video data for source fragments or completing holes pixel by pixel; they also produced blurred results due to sampling and smoothing. For speed, we track moving objects, allowing us to use a much smaller search space when seeking source fragments; we also complete holes fragment by fragment instead of pixelwise. Fine details are maintained by use of a graph cut algorithm when merging source and target fragments. Further techniques ensure temporal consistency of hole filling over successive frames.
   Examples demonstrate the effectiveness of our method.
C1 Tsinghua Univ, Beijing 100084, Peoples R China.
   Cardiff Univ, Cardiff, Wales.
C3 Tsinghua University; Cardiff University
RP Tsinghua Univ, Beijing 100084, Peoples R China.
EM jiayt@cg.cs.tsinghua.edu.cn; ralph@cs.cf.ac.uk
RI Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Arya Sunil., 1993, P 4 ANN ACM SIAM S D, P271
   Bertalmio M, 2003, PROC CVPR IEEE, P707
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bertalmio M., 2001, PROC CVPR IEEE, V1, P1, DOI DOI 10.1109/CVPR.2001.990497
   Boykov Y, 2001, LECT NOTES COMPUT SC, V2134, P359
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   DeMenthon D., 2002, STAT METHODS VIDEO P
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Heuer J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P261, DOI 10.1145/319463.319620
   JIA J, 2004, CVPR, V1, P364
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wexler Y, 2004, PROC CVPR IEEE, P120
   ZHANG Y, 2004, EUROGRAPHICS 2004 SH
   ZHANG Y, 2005, IN PRESS IEEE WORKSH
NR 23
TC 44
Z9 60
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 601
EP 610
DI 10.1007/s00371-005-0313-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400012
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Bar-Lev, A
   Bruckstein, AM
   Elber, G
AF Bar-Lev, A
   Bruckstein, AM
   Elber, G
TI Virtual marionettes: a system and paradigm for real-time 3D animation
SO VISUAL COMPUTER
LA English
DT Article
DE computer graphics system; virtual marionettes; computer game; animation
   system; realtime physics-based animation
ID COLLISION DETECTION
AB This paper describes a computer graphics system that enables users to define virtual marionette puppets, operate them using relatively simple hardware input devices, and display the scene from a given viewpoint on the computer screen. This computerized marionette theater has the potential to become a computer game for children, an interaction tool over the Internet, enabling the creation of simultaneously viewed and operated marionette show by users on the World Wide Web, and, most importantly, a versatile and efficient professional animation system.
C1 Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
EM freddy@cs.technion.ac.il
OI Bar-Lev, Adi/0009-0006-7913-3915; Bruckstein, Alfred/0000-0001-5669-0037
CR [Anonymous], CLASSICAL MECH
   Baraff D., 1990, Computer Graphics, V24, P19, DOI 10.1145/97880.97881
   Baraff D., 1989, Computer Graphics, V23, P223, DOI 10.1145/74334.74356
   BARLEV A, 2003, THESIS TECHNION ISRA
   Barzel R., 1988, Computer Graphics, V22, P179, DOI 10.1145/378456.378509
   Dahlquist G., 1974, NUMERICAL METHODS
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   Garcia A.L., 1994, Numerical Methods for Physics
   Greenwood DT., 1997, CLASSICAL DYNAMICS
   HEMAMI H, 1993, IEEE T SYST MAN CYB, V23, P502, DOI 10.1109/21.229462
   Hubbard PM, 1996, ACM T GRAPHIC, V15, P179, DOI 10.1145/231731.231732
   HUBBARD PM, 1995, IEEE T VIS COMPUT GR, V1, P218, DOI 10.1109/2945.466717
   ISAACS PM, 1987, P ACM SIGGRAPH, P215
   Lee D, 2001, J VISUAL COMP ANIMAT, V12, P81, DOI 10.1002/vis.247
   MERIAM JL, 1993, DYNAMICS
   MOUREY A, 1993, MARIONETTES ATELIER
   Rose C., 1996, Dans Proc. SIGGRAPH '96, P147
   VANOVERVELD CWAM, 1994, J VISUAL COMP ANIMAT, V5, P17, DOI 10.1002/vis.4340050103
   Watt A., 1992, ADV ANIMATION RENDER
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   Witkin A., 1990, Computer Graphics, V24, P243
   Witkin A., 1990, Computer Graphics, V24, P11, DOI 10.1145/91394.91400
   WITKINS A, 1997, SIGGRAPH SHORT COURS
NR 23
TC 6
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2005
VL 21
IS 7
BP 488
EP 501
DI 10.1007/s00371-005-0297-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GE
UT WOS:000230991200005
DA 2024-07-18
ER

PT J
AU Shi, L
   Yu, YZ
   Wojtan, C
   Chenney, S
AF Shi, L
   Yu, YZ
   Wojtan, C
   Chenney, S
TI Controllable motion synthesis in a gaseous medium
SO VISUAL COMPUTER
LA English
DT Article
DE spacetime constraints/optimization; stylistic motion planning; smoothed
   particle hydrodynamics; vortices; trajectory library
AB The generation of realistic motion satisfying user-defined requirements is one of the most important goals of computer animation. Our aim in this paper is the synthesis of realistic, controllable motion for lightweight natural objects in a gaseous medium. We formulate this problem as a large-scale spacetime optimization with user controls and fluid motion equations as constraints. We have devised novel and effective methods to make this large optimization tractable. Initial trajectories are generated with data-driven synthesis based on stylistic motion planning. Smoothed particle hydrodynamics (SPH) is used during optimization to produce fluid simulations at a reasonable computational cost, while interesting vortex-based fluid motion is generated by recording the presence of vortices in the initial trajectories and maintaining them through optimization. Object rotations are refined as a postprocess to enhance the visual quality of the results. We demonstrate our techniques on a number of animations involving single or multiple objects.
C1 Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Wisconsin System; University of Wisconsin Madison
RP Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.
EM linshi@uiuc.edu; yyz@cs.uiuc.edu; wojtan@cc.gatech.edu;
   schenney@cs.wisc.edu
RI YU, YIZHOU/D-1603-2013; /F-3345-2010
OI /0000-0002-0470-5548; Wojtan, Chris/0000-0001-6646-5546
CR Anderson M., 2003, em SCA'03: Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer animation, P286
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Barzel R., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P183
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Cheng P., 2001, Archives ofcontrol sciences, V11, P167
   Chenney S, 2000, COMP GRAPH, P219, DOI 10.1145/344779.344882
   Chenney Stephen., 2004, Proceedings of the 2004 ACM SIGGRAPH/Euro- graphics symposium on Computer animation, P233, DOI [10.1145/1028523.1028553, DOI 10.1145/1028523.1028553.]
   Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   Desbrun M., 1999, Space-Time Adaptive Simulation of Highly Deformable Substances
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Fattal R, 2004, ACM T GRAPHIC, V23, P441, DOI 10.1145/1015706.1015743
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   FOSTER N, 1997, SIGGRAPH 97 C P ANN, P181
   Gersho A., 2003, Vector Quantization and Signal Compression
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   James DL, 2003, ACM T GRAPHIC, V22, P879, DOI 10.1145/882262.882359
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kuffner J. J.  Jr., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P995, DOI 10.1109/ROBOT.2000.844730
   Lee J.J., 2002, P491
   LI Y, 2002, SIGGRAPH, P465
   Ling L, 1996, VISUAL COMPUT, V12, P84, DOI 10.1007/BF01782107
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   MIHALEF V, 2004, SCA 04, P315
   MILLER G, 1989, COMPUT GRAPH-UK, V13, P305, DOI 10.1016/0097-8493(89)90078-2
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Muller M., 2003, SCA, P154
   PIGHIN F, 2004, SCA 04, P223
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   Pullen Katherine., 2002, SIGGRAPH 02, P501
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tanco LM, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P137, DOI 10.1109/HUMO.2000.897383
   TANG D, 1995, J VISUAL COMP ANIMAT, V6, P143, DOI 10.1002/vis.4340060304
   Terzopoulos D., 1989, Proceeding of Graphics Interface, P219
   Tonnesen D., 1991, Proceedings. Graphics Interface '91, P255
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Unuma Munetoshi, 1995, SIGGRAPH, P91
   WEI X, 2003, SCA 03, P75
   WEJCHERT J, 1991, COMP GRAPH, V25, P19, DOI 10.1145/127719.122719
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   WITKIN A, 1995, SIGGRAPH 95, P105
   Yamane K, 2004, ACM T GRAPHIC, V23, P532, DOI 10.1145/1015706.1015756
   Yngve GD, 2000, COMP GRAPH, P29, DOI 10.1145/344779.344801
   YOON HY, 1996, INT J NUMER METHODS, V30, P407
NR 52
TC 6
Z9 6
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2005
VL 21
IS 7
BP 474
EP 487
DI 10.1007/s00371-005-0296-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GE
UT WOS:000230991200004
DA 2024-07-18
ER

PT J
AU Zheng, JM
   Cai, YY
AF Zheng, JM
   Cai, YY
TI Making Doo-Sabin surface interpolation always work over irregular meshes
SO VISUAL COMPUTER
LA English
DT Article
DE arbitrary topology; subdivision surfaces; interpolation; normal
   condition; shape control
AB This paper presents a reliable method for constructing a control mesh whose Doo-Sabin subdivision surface interpolates the vertices of a given mesh with arbitrary topology. The method improves on existing techniques in two respects: (1) it is guaranteed to always work for meshes of arbitrary topological type; (2) there is no need to solve a system of linear equations to obtain the control points. Extensions to include normal vector interpolation and/or shape adjustment are also discussed.
C1 Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zheng, JM (corresponding author), Nanyang Technol Univ, Sch Comp Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
EM asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Cai, Yiyu/A-3816-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Cai, Yiyu/0000-0002-8406-9536
CR [Anonymous], MATRIX COMPUTATIONS
   Brunet P., 1988, Computer-Aided Geometric Design, V5, P41, DOI 10.1016/0167-8396(88)90019-2
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Doo D. W. H., 1978, Proceeding of the International Conference Interactive Techniques in Computer Aided Design, P157
   Foley JamesD., 1997, COMPUTER GRAPHICS PR
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Nasri A. H., 1991, Computer-Aided Geometric Design, V8, P89, DOI 10.1016/0167-8396(91)90051-C
   NASRI AH, 1987, ACM T GRAPHIC, V6, P29, DOI 10.1145/27625.27628
   Sabin M.A., 1991, CURVES SURFACES, P411, DOI DOI 10.1016/B978-0-12-438660-0.50065-0
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   ZORIN D, 2000, COURS NOT SIGGRAPH 2
   ZORIN D, 1996, COMPUTER GRAPHICS, V30, P189
NR 14
TC 9
Z9 11
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2005
VL 21
IS 4
BP 242
EP 251
DI 10.1007/s00371-005-0285-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 928MD
UT WOS:000229274900003
DA 2024-07-18
ER

PT J
AU Rocchini, C
   Cignoni, P
   Ganovelli, F
   Montani, C
   Pingi, P
   Scopigno, R
AF Rocchini, C
   Cignoni, P
   Ganovelli, F
   Montani, C
   Pingi, P
   Scopigno, R
TI The <i>Marching Intersections</i> algorithm for merging range images
SO VISUAL COMPUTER
LA English
DT Article
DE 3D scanning; range map; range image merging; volumetric approach to
   fusion; Marching Intersections
ID INTEGRATION; MESHES
AB A new algorithm for the integration of partially overlapping range images into a triangular mesh is presented. The algorithm consists of three main steps: it locates the intersections between the range surfaces and a reference grid chosen by the user, then merges all nearly coincident and redundant intersections according to a proximity criterion, and, finally, reconstructs the merged surface(s) from the filtered intersection set. Compared with previous methods, which adopt a volumetric approach, our algorithm shows lower computational costs and improves the accuracy of the surfaces produced. It takes into account the quality of the input measurements and is able to patch small holes corresponding to the parts of the 3D scanned object that were not observed by the acquisition device. The algorithm has been tested on several datasets of range maps; graphical and numeric results are reported.
C1 CNR, Ist Sci & Tecnol Informaz A Faedo, I-56124 Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP CNR, Ist Sci & Tecnol Informaz A Faedo, Via G Moruzzi 1, I-56124 Pisa, Italy.
EM Claudio.Montani@isti.cnr.it
RI Cignoni, Paolo/B-7192-2012; scopigno, roberto/AAH-7645-2020; Pan,
   Chong/B-2996-2012
OI Cignoni, Paolo/0000-0002-2686-8567; PINGI, PAOLO/0000-0003-2347-2011
CR ALGORRI M, 1995, LECT NOTES COMPUTER, V905, P420
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   CALLIERI M, 2003, P VAST 2003 EG S GRA
   CALLIERI M, 2002, P 7 INT FALL WORKSH
   CHEN Y, 1995, COMPUT VIS IMAGE UND, V61, P325, DOI 10.1006/cviu.1995.1026
   Cignoni P, 2003, IEEE T VIS COMPUT GR, V9, P525, DOI 10.1109/TVCG.2003.1260746
   Cignoni P, 1999, VISUAL COMPUT, V15, P519, DOI 10.1007/s003710050197
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   DASILVA R, 1998, P 11 SIBGRAPI C RIO, P123
   Davis J., 2002, P 1 INT S 3D DAT PRO
   Goesele M, 2003, ACM T GRAPHIC, V22, P621, DOI 10.1145/882262.882316
   GROSSKOPF S, 1998, LECT NOTES COMPUTER, V1506, P266
   HAUSLER G, 1997, P 3D IM AN SYNTH 97, P191
   Hilton A, 1998, COMPUT VIS IMAGE UND, V69, P273, DOI 10.1006/cviu.1998.0664
   Hilton A, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P181, DOI 10.1109/IM.1997.603864
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   HOWIE CT, 1994, COMPUT GRAPH FORUM, V13, pC65
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   KOBBELT LP, 2001, COMPUT GRAPH, V35, P57
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MENCL R, 1998, P EUR 98 STAR STAT A, P51
   Montani C., 1994, Visual Computer, V10, P353, DOI 10.1007/BF01900830
   Narkhede A., 1995, GRAPHICS GEMS, V5, P394, DOI 10.1016/B978-0-12-543457-7.50059-0
   Pito R, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P397, DOI 10.1109/ICIP.1996.560846
   Pulli K, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P205, DOI 10.1109/IM.1997.603867
   Pulli K., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P160, DOI 10.1109/IM.1999.805346
   RATISHAUSER M, 1994, P IEEE C COMP VIS PA, P573
   Rocchini C, 2002, VISUAL COMPUT, V18, P186, DOI 10.1007/s003710100146
   ROCCHINI C, 2001, P INT C SHAP MOD APP
   Roth G, 1997, PROC GRAPH INTERF, P173
   SOUCY M, 1995, MACH VISION APPL, V8, P53, DOI 10.1007/BF01213638
   SOUCY M, 1995, IEEE T PATTERN ANAL, V17, P344, DOI 10.1109/34.385982
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Varadhan G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P99, DOI 10.1109/VISUAL.2003.1250360
   WHEELER MD, 1998, P IEEE INT C COMP VI
NR 36
TC 18
Z9 24
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 149
EP 164
DI 10.1007/s00371-003-0237-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900005
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Yuen, MMF
   Tang, K
AF Liu, YJ
   Yuen, MMF
   Tang, K
TI Manifold-guaranteed out-of-core simplification of large meshes with
   controlled topological type
SO VISUAL COMPUTER
LA English
DT Article
DE out-of-core mesh simplification; large data; two-manifold meshes;
   controlled topological type
AB In this paper, a simple and efficient algorithm is proposed for manifold-guaranteed out-of-core simplification of large meshes with controlled topological type. By dual-sampling the input large mesh model, the proposed algorithm utilizes a set of Hermite data (sample points with normals) as an intermediate model representation, which allows the topological structure of the mesh model to be encoded implicitly and thus makes it particularly suitable for out-of-core mesh simplification. Benefiting from the construction of an in-core signed distance field, the proposed algorithm possesses a set of features including manifoldness of the simplified meshes, toleration of nonmanifold mesh data input, topological noise removal, topological type control and, sharp features and boundary preservation. A novel, detailed implementation of the proposed algorithm is presented, and experimental results demonstrate that very large meshes can be simplified quickly on a low-cost off-the-shelf PC with tightly bounded approximation errors and with time and space efficiency.
C1 Hong Kong Univ Sci & Technol, Dept Mech Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Hong Kong Univ Sci & Technol, Dept Mech Engn, Clear Water Bay, Kowloon, Hong Kong, Peoples R China.
EM liuyj@ust.hk; meymf@ust.hk; mektang@ust.hk
RI Tang, Kai/ABA-9642-2021; Yuen, Matthew M.F./E-5621-2011
OI Tang, Kai/0000-0002-5184-2086; 
CR BERNARDINI F, 1999, P ACM SIGGRAPH 99 LO
   Bernardini N, 2002, J NEUROSCI, V22, DOI 10.1523/JNEUROSCI.22-12-j0002.2002
   Campbell T, 1998, EXOT PET PRACT, V3, P1
   Chiang YJ, 1998, VISUALIZATION '98, PROCEEDINGS, P167, DOI 10.1109/VISUAL.1998.745299
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P, 1997, IEEE T VIS COMPUT GR, V3, P158, DOI 10.1109/2945.597798
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cignoni P, 2003, IEEE T VIS COMPUT GR, V9, P525, DOI 10.1109/TVCG.2003.1260746
   El-Sana J, 2000, COMPUT GRAPH FORUM, V19, pC139, DOI 10.1111/1467-8659.00406
   Fei GZ, 2002, COMPUT GRAPH FORUM, V21, P111, DOI 10.1111/1467-8659.00571
   Fomenko A.T., 1997, TOPOLOGICAL MODELING
   Garland M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P117, DOI 10.1109/VISUAL.2002.1183765
   GARLAND M, 1999, P EUR 99 IT SEP 1999, P111
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Garland M., 1999, THESIS CARNEGIE MELL
   Guéziec A, 2001, IEEE T VIS COMPUT GR, V7, P136, DOI 10.1109/2945.928166
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Hubeli A, 2001, IEEE T VIS COMPUT GR, V7, P207, DOI 10.1109/2945.942689
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Lindstrom P, 2001, IEEE VISUAL, P121, DOI 10.1109/VISUAL.2001.964502
   Lindstrom P, 2000, COMP GRAPH, P259, DOI 10.1145/344779.344912
   Liu YJ, 2003, VISUAL COMPUT, V19, P23, DOI 10.1007/s00371-002-0162-2
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Montani C., 1994, Visual Computer, V10, P353, DOI 10.1007/BF01900830
   POPOVIC J, 1997, P SIGGRAPH 97, P217
   Rocchini C, 2001, COMPUT GRAPH FORUM, V20, pC299
   Rossignac J., 1993, Geometric Modeling in Computer Graphics, P455
   Shaffer E, 2001, IEEE VISUAL, P127, DOI 10.1109/VISUAL.2001.964503
   Taubin G, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P180, DOI 10.1109/PCCGA.2001.962871
   VANGELDER A, 1994, ACM T GRAPHIC, V13, P337, DOI 10.1145/195826.195828
   Wood Zoe, 2002, MSRTR200228
   Ying L, 2001, IEEE VISUAL, P325, DOI 10.1109/VISUAL.2001.964528
   Zheng JY, 1999, IEEE COMPUT GRAPH, V19, P6, DOI 10.1109/38.761541
NR 36
TC 7
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 565
EP 580
DI 10.1007/s00371-003-0222-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600012
DA 2024-07-18
ER

PT J
AU Sun, LH
   Wang, J
   Zhu, Q
   Liu, JY
   Yu, JW
AF Sun, Longhua
   Wang, Jin
   Zhu, Qing
   Liu, Jiaying
   Yu, Jiawen
TI Cluster-based two-branch framework for point cloud attribute compression
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Point cloud compression; Attribute compression; Graph Laplacian
   transform; Intra-prediction
ID RGB-D SLAM; MOTION REMOVAL; SCHEME
AB Owing to the irregular distribution of point clouds in 3D space, effectively compressing the point cloud is still challenging. Recently, numerous compression methods have been developed with outstanding performance for the compression of geometry information of point clouds. On the contrary, limited explorations have been devoted to point cloud attribute compression (PCAC). Thus, this paper focuses on the study of point cloud attribute compression by applying geometric information as decoded prior information. In this paper, a novel cluster-based two-branch framework for PCAC is proposed. Specifically, the point cloud is first divided into adaptive un-overlapped blocks via K-means according to geometric information, which enables an efficient local representation of the point cloud. Then, we split the point cloud attributes into two components(Block-Mean component and Block-Residual component) and compressed them separately through the designed block-based dual branches. For the Block-Mean component, we design a prediction scheme to remove the inter-block attribute redundancy. The Block-Residual component is further compacted by applying a designed graph Fourier transform generated by geometric prior information. Then, the transform coefficients are encoded using an arithmetic coder. Extensive experimental results demonstrate that the proposed point cloud attribute compression scheme outperforms existing attribute compression schemes in terms of both objective quality and subjective quality.
C1 [Sun, Longhua; Wang, Jin; Zhu, Qing; Liu, Jiaying; Yu, Jiawen] Beijing Univ Technol, Fac Informat Technol, 100 Pingleyuan, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Wang, J (corresponding author), Beijing Univ Technol, Fac Informat Technol, 100 Pingleyuan, Beijing 100124, Peoples R China.
EM islhua@emails.bjut.edu.cn; ijinwang@bjut.edu.cn; ccgszq@bjut.eud.cn;
   liujiaying001225@163.com; yujiawen115601@163.com
RI Liu, Jiaying/HIR-7522-2022
OI Wang, Jin/0000-0001-5437-3150
FU National Natural Science Foundation of China [62272016, 61976011];
   National Natural Science Foundation of China [4232017]; Beijing Natural
   Science Foundation [KM202010005013]; Common Program of Beijing Municipal
   Commission of Education; Opening Project of Beijing Key Laboratory of
   Internet Culture and Digital Dissemination Research [NTUT-BJUT-111-02];
   Special Academic Collaborative Research Projects between BJUT
FX This work was supported by the National Natural Science Foundation of
   China (62272016, 61976011), Beijing Natural Science Foundation
   (4232017), the Common Program of Beijing Municipal Commission of
   Education under Grant KM202010005013, the Opening Project of Beijing Key
   Laboratory of Internet Culture and Digital Dissemination Research, and
   the Special Academic Collaborative Research Projects between BJUT and
   NTUT (NTUT-BJUT-111-02).
CR Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   [Anonymous], 2003, 1 ITUT ISOIEC JTC
   Chang WC, 2020, VISUAL COMPUT, V36, P593, DOI 10.1007/s00371-019-01642-5
   Charles L., 2016, ISO/IEC MPEG m38673
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Cohen RA, 2016, IEEE IMAGE PROC, P1374, DOI 10.1109/ICIP.2016.7532583
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   dEon E., 2016, ISO/IEC MPEG/JPEG m38673/M72012
   Dolonius D, 2019, IEEE T VIS COMPUT GR, V25, P1270, DOI 10.1109/TVCG.2017.2741480
   github, Mpeg-Pcc-Tmc13
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Gumhold S., 2005, ACM SIGGRAPH 2005 SK, P137
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Houshiar H, 2015, 2015 XXV INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATION AND AUTOMATION TECHNOLOGIES (ICAT)
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   ITU-T and ISO/IEC, 1992, ITU-T Rec. T.81 and ISO/IEC 10918-1 (JPEG)
   JACKINS CL, 1980, COMPUT VISION GRAPH, V14, P249, DOI 10.1016/0146-664X(80)90055-6
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Krüsi P, 2017, J FIELD ROBOT, V34, P940, DOI 10.1002/rob.21700
   Li HT, 2022, VISUAL COMPUT, V38, P1759, DOI 10.1007/s00371-021-02103-8
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P6237, DOI 10.1109/TIP.2020.2989576
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621
   Ma C, 2020, IEEE I C VI COM I PR, P50, DOI 10.1109/vcip49819.2020.9301768
   Mammou K., 2017, document ISO/IEC JTC1/SC29/WG11 m41649
   MAMMOU K, 2018, JTC1SC29WG11M42640 I
   Mammou K., 2017, document ISO/IEC JTC, V1
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Milani S, 2020, IEEE T IMAGE PROCESS, V29, P8213, DOI 10.1109/TIP.2020.3011811
   Munagala V, 2021, VISUAL COMPUT, V37, P2173, DOI 10.1007/s00371-020-01978-3
   Ochotta T, 2008, COMPUT GRAPH FORUM, V27, P1647, DOI 10.1111/j.1467-8659.2008.01178.x
   Pavez E, 2018, APSIPA TRANS SIGNAL, V7, DOI 10.1017/ATSIP.2018.15
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/ICIP.2019.8803413, 10.1109/icip.2019.8803413]
   Rizvi S. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P440, DOI 10.1109/ICIP.1999.821647
   Said A., 2004, DATA COMPRESSION C 2, DOI [10.1109/DCC.2004.1281538, DOI 10.1109/DCC.2004.1281538]
   Sandri G, 2018, IEEE IMAGE PROC, P1153, DOI 10.1109/ICIP.2018.8451367
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shao YT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1199, DOI 10.1145/3240508.3240696
   Su GD, 2023, VISUAL COMPUT, V39, P4623, DOI 10.1007/s00371-022-02613-z
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Tariq J, 2020, VISUAL COMPUT, V36, P1603, DOI 10.1007/s00371-019-01764-w
   Teng CH, 2018, VISUAL COMPUT, V34, P1507, DOI 10.1007/s00371-017-1425-2
   Tulvan C., 2016, ISO/IECJTC1/SC29/WG11 (MPEG) output document N16331
   Wei H., 2019, 2019 IEEE VISUAL COM, P1
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Xu YQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1753, DOI 10.1109/ICASSP.2018.8462684
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang X, 2020, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC47342.2020.00015
   Zhang XM, 2017, J SIGNAL PROCESS SYS, V86, P41, DOI 10.1007/s11265-015-1095-0
   Zhao BQ, 2021, IEEE T CIRC SYST VID, V31, P4590, DOI 10.1109/TCSVT.2021.3101126
NR 55
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 25
PY 2023
DI 10.1007/s00371-023-03146-9
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6UT1
UT WOS:001106597500002
DA 2024-07-18
ER

PT J
AU Liu, SB
   Han, YH
AF Liu, Shibin
   Han, Yahong
TI ATRA: Efficient adversarial training with high-robust area
SO VISUAL COMPUTER
LA English
DT Article
DE Adversarial training; Adversarial examples; Defense mechanisms; Robust
   optimization
AB Recent research has shown the vulnerability of deep networks to adversarial perturbations. Adversarial training and its variants have been shown to be effective defense algorithms against adversarial attacks, enhancing the defense abilities of deep neural networks by training them to fit adversarial examples. However, the significant computational burden of generating strong adversarial examples has rendered the process time-consuming, presenting a challenge for efficient training. In this paper, we propose adversarial training with robust area (ATRA), a highly efficient variant of adversarial training. We experimentally find that certain pixels in the image play a crucial role in improving robust accuracy, which we refer to the collection of discrete pixels as the high-robust area. Based on the robust area of the input instance, ATRA generates adversarial examples by applying an adaptive perturbation. Furthermore, we investigate the transferability of the high-robust area during the attack iteration process and experimentally demonstrate its effectiveness. Therefore, ATRA has the advantage of reducing the additional cost of generating strong adversarial examples while maintaining model robustness. Our experimental results on MNIST, CIFAR10, and TinyImageNet show that our method outperforms current state-of-the-art baselines with significantly less additional training time required, especially on MNIST where our method requires 18x\documentclass[12pt]{minimal}\usepackage{amsmath}\usepackage{wasysym}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{mathrsfs}\usepackage{upgreek}\setlength{\oddsidemargin}{-69pt}\begin{document}$$\times $$\end{document} less training time. Furthermore, our method also achieves good performance under different adversarial attacks such as FGSM, CW, and AutoAttack.
C1 [Liu, Shibin; Han, Yahong] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
C3 Tianjin University
RP Han, YH (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
EM yahong@tju.edu.cn
OI liu, shibin/0000-0002-9164-0905
CR Addepalli S., 2022, NeurIPS, V35, P1488
   Andriushchenko M., 2020, Advances in Neural Information Processing Systems, P16048
   [Anonymous], 2017, ACM WORKSHOP ARTIFIC
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Croce F., 2020, INT C MACHINE LEARNI
   de Jorge Aranda P., 2022, Advances in Neural Information Processing Systems, P12881
   Erhan D, 2009, Univ Montr, V1341, P1
   Fan L., 2023, SECUR COMMUN NETW, V2023
   Goodfellow I. J., 2014, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Z., 2023, P IEEECVF C COMPUTER
   Hu S., 2022, arXiv
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Huang Z., 2022, arXiv
   Jia XK, 2022, VISUAL COMPUT, V38, P963, DOI 10.1007/s00371-021-02061-1
   Krishna N.H., 2020, 2020 19 IEEE INT C M
   Li T, 2022, PROC CVPR IEEE, P13399, DOI 10.1109/CVPR52688.2022.01305
   Long YY, 2022, LECT NOTES COMPUT SC, V13664, P549, DOI 10.1007/978-3-031-19772-7_32
   Madry A., 2018, ARXIV
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Mustafa A, 2020, IEEE T IMAGE PROCESS, V29, P1711, DOI 10.1109/TIP.2019.2940533
   Naseer M, 2019, IEEE WINT CONF APPL, P1300, DOI 10.1109/WACV.2019.00143
   Nikfam F, 2022, IEEE ACCESS, V10, P108997, DOI 10.1109/ACCESS.2022.3213734
   Rasheed B, 2023, INT J COMPUT INT SYS, V16, DOI 10.1007/s44196-023-00266-x
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shafahi A, 2019, ADV NEUR IN, V32
   Simonyan K, 2013, ARXIV
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tobia J., 2022, ADV INTELLIGENT DATA
   Wang HX, 2023, VISUAL COMPUT, V39, P639, DOI 10.1007/s00371-021-02363-4
   Wang Y, 2020, INT C LEARN REPR
   Wong Eric, 2020, ARXIV
   Wu BX, 2022, LECT NOTES COMPUT SC, V13673, P307, DOI 10.1007/978-3-031-19778-9_18
   Xiong YF, 2022, PROC CVPR IEEE, P14963, DOI 10.1109/CVPR52688.2022.01456
   Xu CK, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103143
   Xu HXX, 2022, COMPUT AIDED GEOM D, V97, DOI 10.1016/j.cagd.2022.102122
   Zagoruyko S., 2016, ARXIV
   Zhang DH, 2019, ADV NEUR IN, V32
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang J., 2020, ARXIV
   Zhang JM, 2022, J AMB INTEL SMART EN, V14, P317, DOI 10.3233/AIS-220038
   Zhang JM, 2022, HUM-CENT COMPUT INFO, V12, DOI 10.22967/HCIS.2022.12.023
   Zhang Y., 2022, INT C MACHINE LEARNI
   Zhang ZY, 2023, NEUROCOMPUTING, V522, P11, DOI 10.1016/j.neucom.2022.12.013
   Zheng HZ, 2020, PROC CVPR IEEE, P1178, DOI 10.1109/CVPR42600.2020.00126
NR 47
TC 0
Z9 0
U1 6
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3649
EP 3661
DI 10.1007/s00371-023-03057-9
EA AUG 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001060113400001
DA 2024-07-18
ER

PT J
AU Feng, Q
   Li, F
   Li, H
   Liu, XD
   Fei, JY
   Xu, S
   Lu, C
   Yang, Q
AF Feng, Qiang
   Li, Fang
   Li, Hua
   Liu, Xiaodong
   Fei, Jiyou
   Xu, Shuai
   Lu, Chang
   Yang, Qi
TI Feature reused network: a fast segmentation network model for strip
   steel surfaces defects based on feature reused
SO VISUAL COMPUTER
LA English
DT Article
DE Strip steel; Defect detection; Image segmentation; Feature reused
ID CONVOLUTIONAL NEURAL-NETWORK; SALIENT OBJECT DETECTION
AB The strip steel is a common metallic material with a wide range of applications in various industries. However, the issue of surface defects that possess high concealment and low discrimination, which arises during the process of inspecting the quality of strip steel, imposes limitations on the overall quality of strip steel products. The challenging task of industrial quality inspection stems from the difficulty and inefficiency of detecting these defects. This paper addresses the aforementioned challenges by introducing a fast segmentation network model called the feature reused network (FR-Net), which aims to improve the detection of small defects and enhance real-time detection performance in the strip steel surface quality inspection process. In FR-Net, a feature fusion module is used to construct a feature reused fusion bypass to improve the segmentation accuracy of small defects. In addition, a lightweight feature refinement module is proposed to enhance the expression capability of the feature extraction network without increasing the computational effort. Finally, an atrous spatial pyramid pooling module with residual connectivity is proposed to fuse deep features of different scales and enhance the perception of objects of different scales. Experiments on the publicly available datasets showed that the proposed FR-Net achieved the mean intersection over union (mIoU) for NEU-Seg and SD-Saliency-900 datasets were 84.53% and 87.24%, respectively. Meanwhile, the detection speed on a single GPU was 58 FPS. Finally, the size of the proposed FR-Net model is only 45 MB, achieving a good trade-off between accuracy, speed, and model size, thus, providing a new solution for network model deployment on industrial equipment.
C1 [Feng, Qiang; Li, Fang; Li, Hua; Liu, Xiaodong; Fei, Jiyou; Xu, Shuai; Lu, Chang; Yang, Qi] Dalian Jiaotong Univ, Coll Locomot & Rolling Stock Engn, Dalian 116028, Peoples R China.
   [Lu, Chang] PLA Army Acad Artillery & Air Def, Shenyang 110000, Peoples R China.
C3 Dalian Jiaotong University
RP Fei, JY (corresponding author), Dalian Jiaotong Univ, Coll Locomot & Rolling Stock Engn, Dalian 116028, Peoples R China.
EM fjy@djtu.edu.cn
RI Li, Fang/B-4861-2018; xu, shuai/KFS-8484-2024
OI Li, Fang/0000-0003-0589-9644; Feng, Qiang/0000-0002-6842-8404
CR Antwi-Bekoe E, 2022, NEURAL COMPUT APPL, V34, P7253, DOI 10.1007/s00521-021-06792-z
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bao YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3083561
   Cao JG, 2022, J SIGNAL PROCESS SYS, V94, P1531, DOI 10.1007/s11265-022-01801-3
   Cao JG, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3033726
   Chen L.-C., 2018, Pertanika J. Trop. Agric. Sci., P801, DOI [10.1007/978-3-030-01234-2_49, DOI 10.1007/978-3-030-01234-249, DOI 10.1007/978-3-030-01234-2_49]
   Chen XJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075380
   Choi W, 2020, IEEE T IND ELECTRON, V67, P8016, DOI 10.1109/TIE.2019.2945265
   Cui LS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3056744
   Ding T, 2022, MEASUREMENT, V199, DOI 10.1016/j.measurement.2022.111429
   Dong HW, 2020, IEEE T IND INFORM, V16, P7448, DOI 10.1109/TII.2019.2958826
   Elhassan M. A. M., 2022, ARXIV
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guan SQ, 2020, IEEE ACCESS, V8, P49885, DOI 10.1109/ACCESS.2020.2979755
   Hao RY, 2021, J INTELL MANUF, V32, P1833, DOI 10.1007/s10845-020-01670-2
   He D, 2019, COMPUT IND ENG, V128, P290, DOI 10.1016/j.cie.2018.12.043
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Z, 2021, MATER LETT, V301, DOI 10.1016/j.matlet.2021.130271
   Kaddah W, 2019, VISUAL COMPUT, V35, P1293, DOI 10.1007/s00371-018-1515-9
   Kang DQ, 2022, NEURAL COMPUT APPL, V34, P13697, DOI 10.1007/s00521-022-07192-7
   Konovalenko I, 2020, METALS-BASEL, V10, DOI 10.3390/met10060846
   Lee SY, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9245449
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu P, 2021, MATERIALS, V14, DOI 10.3390/ma14247504
   Liu Y, 2020, MATERIALS, V13, DOI 10.3390/ma13204629
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Y, 2019, LECT NOTES COMPUT SC, V11554, P97, DOI 10.1007/978-3-030-22796-8_11
   Luo QW, 2016, ROBOT CIM-INT MANUF, V38, P16, DOI 10.1016/j.rcim.2015.09.008
   Lv XM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061562
   Ma ZX, 2023, J INTELL MANUF, V34, P2431, DOI 10.1007/s10845-022-01930-3
   Ma ZX, 2022, COMPUT IND, V136, DOI 10.1016/j.compind.2021.103585
   Mordia R, 2022, ENG FAIL ANAL, V134, DOI 10.1016/j.engfailanal.2022.106047
   Roth K, 2022, PROC CVPR IEEE, P14298, DOI 10.1109/CVPR52688.2022.01392
   Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh SA, 2023, J INTELL MANUF, V34, P1995, DOI 10.1007/s10845-021-01878-w
   Song GR, 2020, IEEE T INSTRUM MEAS, V69, P9709, DOI 10.1109/TIM.2020.3002277
   Song GR, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2019.106000
   Song LM, 2019, IEEE ACCESS, V7, P27547, DOI 10.1109/ACCESS.2019.2894863
   Sun J, 2023, VISUAL COMPUT, V39, P4391, DOI 10.1007/s00371-022-02597-w
   Tian RS, 2022, MEASUREMENT, V187, DOI 10.1016/j.measurement.2021.110211
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Wan C, 2022, COATINGS, V12, DOI 10.3390/coatings12111730
   Wang YY, 2021, MEASUREMENT, V170, DOI 10.1016/j.measurement.2020.108698
   Wei C, 2023, VISUAL COMPUT, V39, P6655, DOI 10.1007/s00371-022-02754-1
   Wu WH, 2020, IEEE ACCESS, V8, P166184, DOI 10.1109/ACCESS.2020.3022405
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu F., ARXIV
   Zhang JW, 2020, COMPUT IND, V122, DOI 10.1016/j.compind.2020.103231
   Zhang SY, 2021, MECH SYST SIGNAL PR, V153, DOI 10.1016/j.ymssp.2020.107541
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3026760
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou XF, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3132082
   Zhou XF, 2021, IEEE ACCESS, V9, P149465, DOI 10.1109/ACCESS.2021.3124814
NR 57
TC 2
Z9 2
U1 8
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3633
EP 3648
DI 10.1007/s00371-023-03056-w
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001052016900001
DA 2024-07-18
ER

PT J
AU Wang, L
   Yi, LA
   Zhang, YT
   Wang, XF
   Wang, W
   Wang, XJ
AF Wang, Lin
   Yi, Lina
   Zhang, Yuetong
   Wang, Xiaofang
   Wang, Wei
   Wang, Xiangjun
TI 3D reconstruction method based on <i>N</i>-step phase unwrapping
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; Structured light; Absolute phase retrieval; Phase
   unwrapping
ID FRINGE PROJECTION PROFILOMETRY; CODING METHOD; ERROR COMPENSATION;
   SHIFTING METHOD; HIGH-SPEED; RELIABILITY; RETRIEVAL
AB Reducing the number of images in fringe projection profilometry has emerged as a significant research focus. Traditional temporal phase unwrapping algorithms typically require an additional set of coding fringe or phase shift fringe images to determine the fringe order and facilitate phase unwrapping, in addition to the essential sinusoidal phase shift fringe for calculating the wrapped phase. In order to reduce the required number of fringe images and increase reconstruction speed, this paper proposes a three-dimensional (3D) reconstruction method inspired by spatial phase unwrapping. The proposed method is based on the N-step temporal phase unwrapping algorithm and can solve the wrapped phase and fringe order using only a set of sinusoidal phase shift fringe images. Our method achieves a further reduction in the required number of images without compromising reconstruction accuracy. In the calculation of the absolute phase, our proposed method only requires an N-step standard phase shift sinusoidal fringe image, eliminating the need for additional fringe images to determine the fringe order. Firstly, we employ the standard N-step phase shift algorithm to compute the wrapped phase and apply a mask for background removal. Next, we directly calculate the fringe order using the wrapped phase and mask and solve for the absolute phase based on the connected region labeling theorem. Our method achieves 3D reconstruction using a minimum of three fringe images, while maintaining reconstruction precision comparable to that of the traditional temporal phase unwrapping technique. As no additional fringe image is required to solve the fringe order, our method has the potential to achieve significantly faster reconstruction speed.
C1 [Wang, Lin; Yi, Lina; Zhang, Yuetong; Wang, Wei; Wang, Xiangjun] Tianjin Univ, State Key Lab Precis Measuring Technol & Instrumen, Tianjin, Peoples R China.
   [Wang, Lin; Yi, Lina; Zhang, Yuetong; Wang, Wei; Wang, Xiangjun] Tianjin Univ, Key Lab MOEMS, Minist Educ, Tianjin, Peoples R China.
   [Wang, Lin; Yi, Lina; Zhang, Yuetong; Wang, Wei; Wang, Xiangjun] Tianjin Univ, Sch Precis Instrument & Optoelect Engn, Tianjin, Peoples R China.
   [Wang, Xiaofang] Unit 32382 PLA, Wuhan, Peoples R China.
C3 Tianjin University; Tianjin University; Tianjin University
RP Wang, L (corresponding author), Tianjin Univ, State Key Lab Precis Measuring Technol & Instrumen, Tianjin, Peoples R China.; Wang, L (corresponding author), Tianjin Univ, Key Lab MOEMS, Minist Educ, Tianjin, Peoples R China.; Wang, L (corresponding author), Tianjin Univ, Sch Precis Instrument & Optoelect Engn, Tianjin, Peoples R China.
EM Nchuwl@163.com
RI Wang, Fei/KEH-6292-2024; Yang, Ning/KHD-1133-2024; Liu,
   yuqing/KEI-3260-2024; Yu, ZH/KBC-6889-2024; Wang, Yifan/KDO-8319-2024;
   Liu, Zhe/KEJ-5299-2024; zhang, yingying/KGM-8162-2024; lin,
   lin/KFB-9548-2024; yang, zhou/KBB-6972-2024
OI zhang, yingying/0000-0001-7479-3398; 
FU National Natural Science Foundation of China (NSFC) [51575388]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (NSFC) (Grant no.51575388).
CR Chen F, 2000, OPT ENG, V39, P10, DOI 10.1117/1.602438
   Cheng NJ, 2021, PHOTONICS-BASEL, V8, DOI 10.3390/photonics8090362
   CHENG YY, 1984, APPL OPTICS, V23, P4539, DOI 10.1364/AO.23.004539
   CREATH K, 1987, APPL OPTICS, V26, P2810, DOI 10.1364/AO.26.002810
   Ford KR, 2007, MED SCI SPORT EXER, V39, P2021, DOI 10.1249/mss.0b013e318149332d
   Ghiglia D.C., 1998, 2 DIMENSIONAL PHASE
   GHIGLIA DC, 1994, J OPT SOC AM A, V11, P107, DOI 10.1364/JOSAA.11.000107
   Gorthi SS, 2010, OPT LASER ENG, V48, P133, DOI 10.1016/j.optlaseng.2009.09.001
   He XY, 2019, OPT LASER ENG, V121, P358, DOI 10.1016/j.optlaseng.2019.04.009
   Huang PS, 2006, APPL OPTICS, V45, P5086, DOI 10.1364/AO.45.005086
   Huang PS, 2005, OPT ENG, V44, DOI 10.1117/1.2147311
   HUNTLEY JM, 1989, APPL OPTICS, V28, P3268, DOI 10.1364/AO.28.003268
   Jia P., 2005, 2 STEP TRIANGULAR PH
   Jia P, 2008, OPT LASER ENG, V46, P311, DOI 10.1016/j.optlaseng.2007.11.004
   Jia PR, 2007, J OPT SOC AM A, V24, P3150, DOI 10.1364/JOSAA.24.003150
   Lin C, 2020, OPT LETT, V45, P3115, DOI 10.1364/OL.392102
   Marrugo AG, 2020, J OPT SOC AM A, V37, pB60, DOI 10.1364/JOSAA.398644
   Porras-Aguilar R, 2017, OPT LASER ENG, V91, P242, DOI 10.1016/j.optlaseng.2016.12.009
   Rajshekhar G, 2012, OPT LASER ENG, V50, pIII, DOI 10.1016/j.optlaseng.2012.04.006
   REID GT, 1986, OPT LASER ENG, V7, P37, DOI 10.1016/0143-8166(86)90034-5
   Salvi J, 2010, PATTERN RECOGN, V43, P2666, DOI 10.1016/j.patcog.2010.03.004
   Sayed M, 2022, LECT NOTES COMPUT SC, V13693, P1, DOI 10.1007/978-3-031-19827-4_1
   Smit AJ, 2004, J APPL PHYCOL, V16, P245, DOI 10.1023/B:JAPH.0000047783.36600.ef
   Sun JF, 2022, ADV COMPOS HYBRID MA, V5, P627, DOI 10.1007/s42114-022-00435-0
   Tabata S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020377
   Tu ZG, 2023, IEEE T PATTERN ANAL, V45, P9469, DOI 10.1109/TPAMI.2023.3247907
   Wang FL, 2018, J MOD OPTIC, V65, P136, DOI 10.1080/09500340.2017.1380853
   Wang L, 2022, APPL OPTICS, V61, P5320, DOI 10.1364/AO.458442
   Wang L, 2019, OPT LASER ENG, V113, P62, DOI 10.1016/j.optlaseng.2018.10.004
   Wang YJ, 2012, OPT LETT, V37, P2067, DOI 10.1364/OL.37.002067
   Wang YW, 2020, APPL OPTICS, V59, P4279, DOI 10.1364/AO.391387
   Wang ZY, 2010, OPT LASER ENG, V48, P218, DOI 10.1016/j.optlaseng.2009.06.005
   Wen YF, 2010, APPL OPTICS, V49, P6563, DOI 10.1364/AO.49.006563
   Wu ZJ, 2020, PHOTONICS RES, V8, P819, DOI 10.1364/PRJ.389076
   Xing Y, 2016, OPT LASER ENG, V87, P97, DOI 10.1016/j.optlaseng.2016.03.018
   Xu J, 2012, OPT LASER ENG, V50, P1274, DOI 10.1016/j.optlaseng.2012.03.009
   Zhang QC, 2012, OPT LASER ENG, V50, P574, DOI 10.1016/j.optlaseng.2011.06.024
   Zhang S, 2018, OPT LASER ENG, V107, P28, DOI 10.1016/j.optlaseng.2018.03.003
   Zhang S, 2010, OPT EXPRESS, V18, P9684, DOI 10.1364/OE.18.009684
   Zheng DL, 2017, APPL OPTICS, V56, P3660, DOI 10.1364/AO.56.003660
   Zheng DL, 2012, OPT EXPRESS, V20, P24139, DOI 10.1364/OE.20.024139
   Zhou CL, 2015, OPT LASER ENG, V66, P269, DOI 10.1016/j.optlaseng.2014.09.011
   Zuo C, 2022, LIGHT-SCI APPL, V11, DOI 10.1038/s41377-022-00714-x
   Zuo C, 2016, OPT LASER ENG, V85, P84, DOI 10.1016/j.optlaseng.2016.04.022
   Zuo C, 2012, OPT EXPRESS, V20, P19493, DOI 10.1364/OE.20.019493
NR 45
TC 2
Z9 2
U1 8
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3601
EP 3613
DI 10.1007/s00371-023-03054-y
EA AUG 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001051056900001
DA 2024-07-18
ER

PT J
AU Guo, HR
   Wang, JG
   Liu, YL
AF Guo, Hairu
   Wang, Jin'ge
   Liu, Yongli
TI Multi-threshold image segmentation algorithm based on Aquila
   optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Aquila optimization; Hybrid chaotic mapping; Simulated annealing; Lens
   imaging learning; Symmetric cross-entropy; Muti-threshold segment
ID CUCKOO SEARCH ALGORITHM; ENTROPY
AB Aquila Optimization (AO) is a recently proposed meta-heuristic algorithm, which has been proved to be more competitive than other meta-heuristic algorithms in function optimization and practical applications. However, when solving more complex optimization problems, AO still has the shortcomings of local optimal stagnation and low solving accuracy. To overcome these shortcomings, an improved Aquila Optimization algorithm (IAO) is proposed in this paper. During the initialization of IAO population, a hybrid chaotic mapping mechanism was introduced to initialize the population, improving both the population diversity and the uniformity of the population distribution. The elite dimensional lens imaging learning strategy is introduced for elite individual to improve the optimization quality of the algorithm as elite individual has more useful information than ordinary individuals. Then the probabilistic jump mechanism of simulated annealing algorithm is used to select the position update mode to balance local development and global search. The experimental results on the CEC2005 test function verify the viability and effectiveness of IAO. IAO is used to the multi-threshold segmentation problem based on symmetric cross entropy to demonstrate its capacity to resolve practical optimization problems. The segmentation performance on different reference images shows that IAO has good segmentation performance in most cases.
C1 [Guo, Hairu; Wang, Jin'ge; Liu, Yongli] Henan Polytech Univ, Coll Comp Sci & Technol, Jiaozuo 454000, Peoples R China.
C3 Henan Polytechnic University
RP Guo, HR (corresponding author), Henan Polytech Univ, Coll Comp Sci & Technol, Jiaozuo 454000, Peoples R China.
EM guohr@hpu.edu.cn
OI Guo, Hairu/0009-0003-6226-794X
CR Abualigah L, 2022, EXPERT SYST APPL, V191, DOI 10.1016/j.eswa.2021.116158
   Abualigah L, 2021, COMPUT IND ENG, V157, DOI 10.1016/j.cie.2021.107250
   Agushaka JO, 2023, NEURAL COMPUT APPL, V35, P4099, DOI 10.1007/s00521-022-07854-6
   Agushaka JO, 2022, COMPUT METHOD APPL M, V391, DOI 10.1016/j.cma.2022.114570
   Ait-Saadi A, 2022, COMPUT ELECTR ENG, V104, DOI 10.1016/j.compeleceng.2022.108461
   Balavand A, 2022, VISUAL COMPUT, V38, P149, DOI 10.1007/s00371-020-02009-x
   Bas E, 2023, ENG APPL ARTIF INTEL, V118, DOI 10.1016/j.engappai.2022.105592
   Bhandari AK, 2014, EXPERT SYST APPL, V41, P3538, DOI 10.1016/j.eswa.2013.10.059
   Brink AD, 1996, PATTERN RECOGN, V29, P179, DOI 10.1016/0031-3203(95)00066-6
   Bürger A, 2023, AUTOMATICA, V152, DOI 10.1016/j.automatica.2023.110967
   Chen Y, 2022, EXPERT SYST APPL, V194, DOI 10.1016/j.eswa.2022.116511
   Dehghani M, 2021, IEEE ACCESS, V9, P162059, DOI 10.1109/ACCESS.2021.3133286
   Dixit A, 2022, VISUAL COMPUT, V38, P3525, DOI 10.1007/s00371-021-02176-5
   Ezugwu AE, 2022, NEURAL COMPUT APPL, V34, P20017, DOI 10.1007/s00521-022-07530-9
   Houssein EH, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.106075
   Houssein EH, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107348
   Jati GK, 2023, APPL SOFT COMPUT, V139, DOI 10.1016/j.asoc.2023.110219
   Kaveh M, 2023, MATH COMPUT SIMULAT, V208, P95, DOI 10.1016/j.matcom.2022.12.027
   Kaya Y, 2020, MULTIMED TOOLS APPL, V79, P23387, DOI 10.1007/s11042-020-09080-5
   LI CH, 1993, PATTERN RECOGN, V26, P617, DOI 10.1016/0031-3203(93)90115-D
   Lin T, 2021, STAT MED, V40, P1705, DOI 10.1002/sim.8865
   Liu JX, 2022, ENG COMPUT-GERMANY, V38, P4479, DOI 10.1007/s00366-021-01477-6
   Long W, 2022, EXPERT SYST APPL, V201, DOI 10.1016/j.eswa.2022.117217
   Ma GY, 2022, ENG APPL ARTIF INTEL, V113, DOI 10.1016/j.engappai.2022.104960
   Morales-Castañeda B, 2020, SWARM EVOL COMPUT, V54, DOI 10.1016/j.swevo.2020.100671
   Nadimi-Shahraki MH, 2021, EXPERT SYST APPL, V166, DOI 10.1016/j.eswa.2020.113917
   Oliva D, 2014, NEUROCOMPUTING, V139, P357, DOI 10.1016/j.neucom.2014.02.020
   Rajabioun R, 2011, APPL SOFT COMPUT, V11, P5508, DOI 10.1016/j.asoc.2011.05.008
   Sheng Mengmeng, 2023, Journal of Ambient Intelligence and Humanized Computing, P9329, DOI 10.1007/s12652-022-04432-5
   Shubham S, 2019, MULTIMED TOOLS APPL, V78, P17197, DOI 10.1007/s11042-018-7034-x
   Suresh S, 2016, EXPERT SYST APPL, V58, P184, DOI 10.1016/j.eswa.2016.03.032
   Tahiri MA, 2023, VISUAL COMPUT, V39, P6395, DOI 10.1007/s00371-022-02736-3
   Tao XM, 2021, INFORM SCIENCES, V578, P457, DOI 10.1016/j.ins.2021.07.008
   Turgut OE, 2023, MATH COMPUT SIMULAT, V206, P302, DOI 10.1016/j.matcom.2022.11.020
   Utama DM, 2022, RESULTS CONTROL OPTI, V9, DOI 10.1016/j.rico.2022.100177
   Vasile A, 2022, PROCEDIA STRUCT INTE, V37, P857, DOI 10.1016/j.prostr.2022.02.019
   Wang JQ, 2023, APPL SOFT COMPUT, V137, DOI 10.1016/j.asoc.2023.110130
   Wu B, 2020, INFORM SCIENCES, V533, P72, DOI 10.1016/j.ins.2020.05.033
   Zhang PL, 2023, EXPERT SYST APPL, V224, DOI 10.1016/j.eswa.2023.120058
   Zhang QK, 2023, KNOWL-BASED SYST, V261, DOI 10.1016/j.knosys.2022.110206
   Zhao D, 2021, EXPERT SYST APPL, V167, DOI 10.1016/j.eswa.2020.114122
   Zhao SW, 2021, COMPUT BIOL MED, V139, DOI 10.1016/j.compbiomed.2021.105015
NR 42
TC 0
Z9 0
U1 17
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2905
EP 2932
DI 10.1007/s00371-023-02993-w
EA JUL 2023
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001035510700001
DA 2024-07-18
ER

PT J
AU Chen, ZY
   Yu, F
   Jiang, MH
   Wang, H
   Hua, AL
   Peng, T
   Hu, XR
   Zhu, P
AF Chen, Ziyi
   Yu, Feng
   Jiang, Minghua
   Wang, Hua
   Hua, Ailing
   Peng, Tao
   Hu, Xinrong
   Zhu, Ping
TI Three stages of 3D virtual try-on network with appearance flow and shape
   field
SO VISUAL COMPUTER
LA English
DT Article
DE Appearance flow; Shape field; Virtual try-on; 3D reconstruction
ID RECONSTRUCTION
AB The virtual try-on technology can satisfy the demands for online shopping and help consumers experience online clothes through image generation technology. Compared with image-based try-on, the 3D virtual try-on methods can realize the multi-perspective of try-on simulation and get the attention of many researchers. The current 3D virtual try-on methods mainly base on the thin-plate spline method and depth information-based 3D reconstruction, which increases the costs of implementing 3D virtual try-on and the results lack of clothing details such as folds and patterns. To solve those problems, we propose a novel 3D virtual try-on network based on appearance flow and shape field called AFSF-3DVTON. Specifically, this network consists of three modules. First, the appearance flow warping module generates the desired warped clothes according to the appearance flow of the original clothes. Then, the flat try-on module facilitates geometric matching between the warped clothes and reference person images and synthesizes 2D try-on results. Third, to increase the image's details to 3D try-on synthesis, the shape field-based reconstruction is adopted, which extracts shape features of 2D try-on results to improve the quality of 3D try-on reconstruction. We evaluate the proposed method on the VITON and MPV3D datasets, and several state-of-the-art virtual try-on algorithms are used as comparisons. The qualitative analyses verify the superiority of the proposed method, and the evaluation indexes, including Abs., Sq., and RMSE, demonstrate the outperformance of the proposed network.
C1 [Chen, Ziyi; Yu, Feng; Jiang, Minghua; Wang, Hua; Hua, Ailing; Peng, Tao; Hu, Xinrong; Zhu, Ping] Wuhan Text Univ, Dept Comp & Artificial Intelligent, Yangguang St, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University
RP Yu, F (corresponding author), Wuhan Text Univ, Dept Comp & Artificial Intelligent, Yangguang St, Wuhan 430200, Hubei, Peoples R China.
EM ZiyiChenCindy@163.com; yufeng@wtu.edu.cn; minghuajiang@wtu.edu.cn;
   wangfua@yeah.ne; hal_wtu@163.com; pt@wtu.edu.cn; hxr@wtu.edu.cn;
   zhuping@wtu.edu.cn
RI Yu, Feng/U-9998-2019
OI Yu, Feng/0000-0003-1913-2882
FU National natural science foundation of China [62202346]; Hubei key
   research and development program [2021BAA042]; open project of
   engineering research center of Hubei province for clothing information
   [2022HBCI01]; Wuhan applied basic frontier research project
   [2022013988065212]; MIIT's AI Industry Innovation Task unveils flagship
   projects (Key technologies, equipment, and systems for flexible
   customized and intelligent manufacturing in the clothing industry);
   Hubei science and technology project of safe production special fund
FX This work was supported by the National natural science foundation of
   China (No. 62202346), Hubei key research and development program (No.
   2021BAA042), open project of engineeringresearch center of Hubei
   province for clothing information (No.2022HBCI01), Wuhan applied basic
   frontier research project (No.2022013988065212), MIIT's AI Industry
   Innovation Task unveils flagship projects (Key technologies, equipment,
   and systems for flexible customized and intelligent manufacturing in the
   clothing industry), and Hubei science and technology project of safe
   production special fund(Scene control platform based on proprioception
   information comput-ing of artificial intelligence)
CR BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chaudhuri B, 2021, PROC CVPR IEEE, P7987, DOI 10.1109/CVPR46437.2021.00790
   Chen CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13789, DOI 10.1109/ICCV48922.2021.01355
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Du CH, 2023, IEEE T MULTIMEDIA, V25, P777, DOI 10.1109/TMM.2022.3152367
   Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Grant E, 2016, LECT NOTES COMPUT SC, V9915, P266, DOI 10.1007/978-3-319-49409-8_22
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Heusel M., Advances in Neural Information Processing Systems, V30
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jiang HY, 2019, IEEE I CONF COMP VIS, P5430, DOI 10.1109/ICCV.2019.00553
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445
   Lizhen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P430, DOI 10.1007/978-3-030-58565-5_26
   Minar M. R., 2020, CVPR WORKSH
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Mustafa A, 2021, PROC CVPR IEEE, P14469, DOI 10.1109/CVPR46437.2021.01424
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Saito S, 2020, INT CONF GLOBAL SOFT, P81, DOI 10.1145/3372787.3389301
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sobel I., 1968, STANF ART PROJ, P271
   Su ZQ, 2022, IEEE T VIS COMPUT GR, V28, P1862, DOI 10.1109/TVCG.2020.3027763
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784
   Tewari A, 2020, IEEE T PATTERN ANAL, V42, P357, DOI 10.1109/TPAMI.2018.2876842
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang KK, 2020, PROC CVPR IEEE, P7273, DOI 10.1109/CVPR42600.2020.00730
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Xiu Y., 2022, P IEEE C COMP VIS PA, V2
   Yang Han, 2020, P IEEE CVF C COMP VI
   Yang Z, 2021, PROC CVPR IEEE, P13279, DOI 10.1109/CVPR46437.2021.01308
   Zhang HR, 2023, IEEE T MULTIMEDIA, V25, P3868, DOI 10.1109/TMM.2022.3167887
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13219, DOI 10.1109/ICCV48922.2021.01299
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 51
TC 1
Z9 1
U1 5
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3545
EP 3559
DI 10.1007/s00371-023-02946-3
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035720700003
DA 2024-07-18
ER

PT J
AU Wang, TY
   Zheng, H
   Guo, ZY
   You, CH
   Ju, JP
AF Wang, Tianyu
   Zheng, Hong
   Guo, Zhongyuan
   You, Changhui
   Ju, Jianping
TI Anti-counterfeiting textured pattern
SO VISUAL COMPUTER
LA English
DT Article
DE Textured pattern; Arnold; DWT domain; Spot matching
ID LOCAL BINARY PATTERN; QR CODE; ROBUST; WATERMARKING
AB Due to the proliferation of high-quality copying devices and the significant profits of counterfeit products, it is critical to establish an effective scheme for detecting and preventing the counterfeiting of goods. At present, most anti-faking schemes leave much to be desired in terms of cost, convenience, and ability to facilitate pre-sale authentication. The paper designs a unique textured pattern and proposes a triple anti-counterfeiting authentication (TACA). First, the textured pattern consists of triple encryptions (the first is that the key area of the QR code is covered, the second includes scale and Arnold transformation, and the third involves replacing the black areas of the pattern with random multi-level grayscales), the abundant details in the texture not only effectively conceal information, but also that their distortion will increase. Second, TACA comprises interpretability analysis (IA), spectral feature analysis (SFA), and spot matching analysis (SMA) in a cascaded way. In further detail, IA mainly exploits the positional transformation of individual pixels and the block features of local regions to restore interpretability. SFA uses the low-frequency subgraph of discrete wavelet transform (DWT) at a specified scale to capture macroscopic structural information. SMA is able to capture the detailed information of the pattern by utilizing SURF to detect the peak region rate positions and employing BRISK to accurately describe them before. Finally, this paper investigates the robustness of the proposed anti-counterfeiting scheme under a variety of copying methods (replicating, scanning-printing), capturing devices (smartphones), and attack scenarios (no attack, cropping, noise, blur).
C1 [Wang, Tianyu; Zheng, Hong] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
   [Guo, Zhongyuan] Southwest Univ, Coll Elect Informat Engn, Chongqing 400715, Peoples R China.
   [You, Changhui] Wuhan Univ, Sch Cyber Sci & Engn, Wuhan 430000, Peoples R China.
   [Ju, Jianping] Hubei Business Coll, Sch Artificial Intelligence, Wuhan 430070, Peoples R China.
C3 Wuhan University; Southwest University - China; Wuhan University
RP Zheng, H (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM zh@whu.edu.cn; gjdxjjp@whu.edu.cn
RI Guo, Zhongyuan/JLN-1557-2023
FU National Key Research and Development Program of China [2020YFF0304902];
   Science and Technology Research Project of Jiangxi Provincial Department
   of Education [GJJ212507]
FX AcknowledgementsThis research is supported by the National Key Research
   and Development Program of China (Grant No. 2020YFF0304902) and the
   Science and Technology Research Project of Jiangxi Provincial Department
   of Education (Grant No. GJJ212507).
CR Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Alkhatib M, 2019, IEEE T IMAGE PROCESS, V28, P5407, DOI 10.1109/TIP.2019.2916742
   [Anonymous], 2019, INT J INNOV TECHNOL
   Bai WM, 2021, IEEE T IMAGE PROCESS, V30, P8439, DOI 10.1109/TIP.2021.3114989
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bi XL, 2021, IEEE T IMAGE PROCESS, V30, P7228, DOI 10.1109/TIP.2021.3104163
   Borwankar R, 2018, IEEE T INSTRUM MEAS, V67, P690, DOI 10.1109/TIM.2017.2783098
   Cao Z, 2021, IEEE ELECTR DEVICE L, V42, P597, DOI 10.1109/LED.2021.3057638
   Chaban R, 2022, IEEE INT WORKS INFOR, DOI 10.1109/WIFS55849.2022.9975380
   Chen CS, 2020, IEEE T INF FOREN SEC, V15, P1056, DOI 10.1109/TIFS.2019.2934861
   Chen L, 2020, ADV SCI, V7, DOI 10.1002/advs.202000803
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Cui ZP, 2020, 2020 4TH INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY (ICCSP 2020), P68, DOI 10.1145/3377644.3377651
   El Khadiri I, 2018, INFORM SCIENCES, V467, P634, DOI 10.1016/j.ins.2018.02.009
   Focardi R, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102369
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Hazgui M, 2022, VISUAL COMPUT, V38, P457, DOI 10.1007/s00371-020-02028-8
   Hong XP, 2014, IEEE T IMAGE PROCESS, V23, P2557, DOI 10.1109/TIP.2014.2316640
   Hu RW, 2021, IEEE T IMAGE PROCESS, V30, P318, DOI 10.1109/TIP.2020.3036727
   Hu YW, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202102108
   Joshi S, 2018, IEEE T INF FOREN SEC, V13, P1603, DOI 10.1109/TIFS.2017.2779441
   Kumar P, 2016, NANOSCALE, V8, P14297, DOI 10.1039/c5nr06965c
   Lee IH, 2019, OPT EXPRESS, V27, P24512, DOI 10.1364/OE.27.024512
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lin YH, 2021, SMALL, V17, DOI 10.1002/smll.202100244
   Liu H, 2021, MATER CHEM FRONT, V5, P2294, DOI 10.1039/d0qm00903b
   Liu JP, 2019, IEEE T IND INFORM, V15, P6139, DOI 10.1109/TII.2019.2916140
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Nguyen HP, 2019, IEEE ACCESS, V7, P131839, DOI 10.1109/ACCESS.2019.2937465
   Okazaki S, 2012, J ADVERTISING RES, V52, P102, DOI 10.2501/JAR-52-1-102-117
   Pan JS, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104049
   Patil VC, 2022, IEEE T CONSUM ELECTR, V68, P5, DOI 10.1109/TCE.2021.3139356
   Peng F, 2021, IEEE T CIRC SYST VID, V31, P411, DOI 10.1109/TCSVT.2020.2969464
   Nguyen HP, 2017, IEEE GLOB CONF SIG, P288, DOI 10.1109/GlobalSIP.2017.8308650
   Pilania E., 2016, INT J ADV TRENDS COM, DOI [10.18535/ijecs/v5i7.04, DOI 10.18535/IJECS/V5I7.04]
   Saranya K, 2016, PROCEEDINGS OF 2ND IEEE INTERNATIONAL CONFERENCE ON ENGINEERING & TECHNOLOGY ICETECH-2016, P173, DOI 10.1109/ICETECH.2016.7569235
   Song TC, 2021, IEEE T CIRC SYST VID, V31, P189, DOI 10.1109/TCSVT.2020.2972155
   Szucs J, 2022, VISUAL COMPUT, V38, P4221, DOI 10.1007/s00371-021-02290-4
   Tiwari S., 2016, 2016 INT C INF TECHN, P39, DOI DOI 10.1109/ICIT.2016.021
   Tkachenko I, 2016, INT CONF ACOUST SPEE, P2149, DOI 10.1109/ICASSP.2016.7472057
   Tkachenko I, 2016, IEEE T INF FOREN SEC, V11, P571, DOI 10.1109/TIFS.2015.2506546
   Tu B, 2021, IEEE J-STARS, V14, P8326, DOI 10.1109/JSTARS.2021.3104153
   Tutt J, 2022, IEEE INT WORKS INFOR, DOI 10.1109/WIFS55849.2022.9975447
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Wang TY, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23020795
   Wang ZY, 2019, VISUAL COMPUT, V35, P885, DOI 10.1007/s00371-019-01681-y
   Wong CW, 2017, IEEE T INF FOREN SEC, V12, P1885, DOI 10.1109/TIFS.2017.2694404
   Xi JN, 2016, IEEE ACM T COMPUT BI, V13, P656, DOI 10.1109/TCBB.2015.2474404
   Xiao B, 2019, IEEE T CIRC SYST VID, V29, P2796, DOI 10.1109/TCSVT.2018.2869841
   Xie N, 2022, IEEE T CIRC SYST VID, V32, P437, DOI 10.1109/TCSVT.2021.3059092
   Xie RS, 2015, NEUROCOMPUTING, V167, P625, DOI 10.1016/j.neucom.2015.04.026
   Xu J, 2019, ACS APPL MATER INTER, V11, P35294, DOI 10.1021/acsami.9b10989
   Yan YL, 2021, IEEE INTERNET THINGS, V8, P6789, DOI 10.1109/JIOT.2020.3035697
   Zhang YP, 2019, IEEE T IND INFORM, V15, P6146, DOI 10.1109/TII.2019.2938806
   Zheng H, 2024, VISUAL COMPUT, V40, P441, DOI 10.1007/s00371-023-02792-3
   Zheng ZH, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115410
   Zuo MZ, 2018, ACS APPL MATER INTER, V10, P39214, DOI 10.1021/acsami.8b14110
NR 58
TC 0
Z9 0
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2139
EP 2160
DI 10.1007/s00371-023-02909-8
EA JUL 2023
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001017717100001
DA 2024-07-18
ER

PT J
AU Li, XB
   Yu, HF
   Chen, HY
AF Li, Xinbin
   Yu, Haifeng
   Chen, Haiyang
TI Multi-scale aggregation feature pyramid with cornerness for underwater
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Underwater object detection; Feature pyramid network; Generalized
   intersection over union; Centerness strategy
AB Underwater object detection is a fascinating but challengeable subject in computer vision. Features are difficult to extract due to the color cast and blur of underwater images. Moreover, given the small scale of the underwater object, some details will be lost after several layers of convolution. Therefore, a multi-scale aggregation feature pyramid network is proposed to integrate multi-scale features and improve underwater object detection performance. Specifically, a lightweight and efficient network is used to extract the basic features. A special subnet is designed to improve the feature extraction capability of the backbone network to enrich the detailed features of small underwater objects. In addition, a multi-scale feature pyramid is proposed to enrich feature map. Each feature map enhances contextual information through a combination of up-sampling and down-sampling. The centerness strategy of the fully convolutional one-stage object detection head is improved by adding corner point regression to enhance the recall rate of small objects. Generalized intersection over union (GIoU) instead of IoU can better reflect the degree of coincidence between the actual box and the predicted box. Therefore, the regression loss is changed to GIoU loss. This paper evaluates the network on the underwater image dataset and obtains 78.90% mAP. Meanwhile, the experiment on the PASCAL VOC datasets is conducted and gets 84.3% mAP.
C1 [Li, Xinbin; Chen, Haiyang] Yanshan Univ, Inst Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
   [Yu, Haifeng] North China Univ Sci & Technol, Coll Elect Engn, Tangshan 063210, Peoples R China.
C3 Yanshan University; North China University of Science & Technology
RP Yu, HF (corresponding author), North China Univ Sci & Technol, Coll Elect Engn, Tangshan 063210, Peoples R China.
EM yhf5170@163.com
RI Chen, Haiyang/ABI-7889-2020
OI Yu, Haifeng/0000-0003-4171-5566
FU Hebei Natural Science Foundation, China [F2020203037, F2022203025];
   National Natural Science Foundation of China [61873224, 62271437,
   62003295]; Science and Technology Research Project of Universities in
   Hebei, China [QN2020301]
FX This work was supported in part by the Hebei Natural Science Foundation,
   China under Grant F2020203037, and F2022203025, in part by the National
   Natural Science Foundation of China under Grant 61873224, Grant
   62271437, and Grant 62003295, in part by the Science and Technology
   Research Project of Universities in Hebei, China under Grant QN2020301.
CR Ammari H, 2018, SIAM J IMAGING SCI, V11, P1, DOI 10.1137/17M1126540
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen L, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207506
   Chen Z., 2020, EUR C COMP VIS
   Dhillon A, 2020, PROG ARTIF INTELL, V9, P85, DOI 10.1007/s13748-019-00203-0
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Han M, 2020, IEEE T SYST MAN CY-S, V50, P1820, DOI 10.1109/TSMC.2017.2788902
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Lee YW, 2019, IEEE COMPUT SOC CONF, P752, DOI 10.1109/CVPRW.2019.00103
   Li H., 2018, ARXIV
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rodner E., 2016, PROC BRIT MACH VIS C
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang J, 2020, IEEE ACCESS, V8, P130719, DOI 10.1109/ACCESS.2020.3003351
   Wang XH, 2019, J OCEAN U CHINA, V18, P376, DOI 10.1007/s11802-019-3858-x
   Wei J, 2020, IEEE T INTELL TRANSP, V21, P1572, DOI 10.1109/TITS.2019.2910643
   Xu FQ, 2021, NEURAL COMPUT APPL, V33, P3637, DOI 10.1007/s00521-020-05217-7
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
NR 31
TC 4
Z9 4
U1 15
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1299
EP 1310
DI 10.1007/s00371-023-02849-3
EA APR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000966510900001
DA 2024-07-18
ER

PT J
AU Wu, HY
   Li, B
   Tian, LF
   Feng, JJ
   Dong, C
AF Wu, Huangyuan
   Li, Bin
   Tian, Lianfang
   Feng, Junjian
   Dong, Chao
TI An adaptive loss weighting multi-task network with attention-guide
   proposal generation for small size defect inspection
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-task learning; Proposal generation; Adaptive loss weighting
   algorithm; Surface defect detection; Small object detection; Computer
   vision
AB Computer vision-based detection approaches have been widely used in defect inspection tasks. However, identifying small-sized defects is still a challenge for most existing methods. It is mainly because: (1) the existing methods fail to extract sufficient information from the small-sized defects; (2) the existing detectors cannot generate effective region proposals for small-sized defects, which results in a low recall rate. To address the above issues, an adaptive loss weighting multi-task model with attention-guide proposal generation is proposed. First, the proposed multi-task model can excavate contextual information to enrich the feature information of small-sized defect areas, enhancing the model's representation capability. Additionally, to improve the recall rate of small-sized defects, an object attention-guide proposal generation module is proposed by leveraging object attention to guide the confidence enhancement of small-sized defects, which can generate more high-quality region proposals for small-sized defects. Finally, to speed up the joint optimization of the proposed multi-task framework, an adaptive loss weighting algorithm is proposed to learn the optimal combination of multi-task loss functions by maintaining the gradient direction consistency and tuning each task's loss magnitude. The experimental results on the two public defect datasets demonstrate that the proposed method outperforms other state-of-the-art methods.
C1 [Wu, Huangyuan; Li, Bin; Tian, Lianfang; Feng, Junjian] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510640, Peoples R China.
   [Tian, Lianfang] Southern Marine Sci & Engn Guangdong Lab, Zhuhai 519000, Peoples R China.
   [Dong, Chao] South China Sea Marine Survey & Technol Ctr, Guangzhou 510300, Peoples R China.
C3 South China University of Technology; Southern Marine Science &
   Engineering Guangdong Laboratory
RP Li, B; Tian, LF (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510640, Peoples R China.; Tian, LF (corresponding author), Southern Marine Sci & Engn Guangdong Lab, Zhuhai 519000, Peoples R China.
EM binlee@scut.edu.cn; chlftian@scut.edu.cn
FU Key-Area Research and Development Program of Guangdong Province
   [2020B1111010002, 2018B010109001]; 2021 Guangdong Provincial Science and
   Technology Special Fund ("Big Project + Task List") [210719145863737];
   Guangdong Marine Economic Development Project [GDNRC[2020]018];
   Laboratory of Autonomous Systems and Network Control of Ministry of
   Education
FX This work was supported by the Key-Area Research and Development Program
   of Guangdong Province under Grant (2020B1111010002, 2018B010109001),
   2021 Guangdong Provincial Science and Technology Special Fund ("Big
   Project + Task List") under Grant 210719145863737, the Guangdong Marine
   Economic Development Project under Grant GDNRC[2020]018, and Laboratory
   of Autonomous Systems and Network Control of Ministry of Education.
CR Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen G, 2022, IEEE T SYST MAN CY-S, V52, P936, DOI 10.1109/TSMC.2020.3005231
   Chen Z, 2018, PR MACH LEARN RES, V80
   Dong RC, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13173362
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Duan KW, 2020, IEEE T CIRC SYST VID, V30, P1639, DOI 10.1109/TCSVT.2019.2906246
   Farhadi A., 2018, JAE P YOLOV3 INCREME
   Fu JM, 2021, IEEE T GEOSCI REMOTE, V59, P1331, DOI 10.1109/TGRS.2020.3005151
   Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332
   Ge Z., 2021, YOLOX: exceeding YOLO series in 2021, V2021
   Guo M, 2018, LECT NOTES COMPUT SC, V11220, P282, DOI 10.1007/978-3-030-01270-0_17
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hong MB, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3103069
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Jha A., 2020, P 2020 IEEECVF C COM, P3027
   Jha A, 2020, IEEE COMPUT SOC CONF, P3027, DOI 10.1109/CVPRW50498.2020.00361
   Jia J, 2020, IEEE J-STSP, V14, P688, DOI 10.1109/JSTSP.2020.2976566
   Jia J, 2019, IEEE INTERNET THINGS, V6, P9919, DOI 10.1109/JIOT.2019.2933254
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Krishna H, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P340, DOI 10.1109/ACPR.2017.149
   Lian J, 2020, IEEE T IND INFORM, V16, P1343, DOI 10.1109/TII.2019.2945403
   Liang X, 2020, IEEE T CIRC SYST VID, V30, P1758, DOI 10.1109/TCSVT.2019.2905881
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2019, ADV NEUR IN, V32
   Liu GH, 2022, VISUAL COMPUT, V38, P639, DOI 10.1007/s00371-020-02040-y
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YQ, 2022, IEEE T IMAGE PROCESS, V31, P541, DOI 10.1109/TIP.2021.3132828
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Parashar D, 2022, J DIGIT IMAGING, V35, P1283, DOI 10.1007/s10278-022-00648-1
   Parashar D, 2021, IEEE SIGNAL PROC LET, V28, P66, DOI 10.1109/LSP.2020.3045638
   Quan Y, 2021, INT C PATT RECOG, P954, DOI 10.1109/ICPR48806.2021.9412726
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shihavuddin ASM, 2019, ENERGIES, V12, DOI 10.3390/en12040676
   Vandenhende S, 2022, IEEE T PATTERN ANAL, V44, P3614, DOI 10.1109/TPAMI.2021.3054719
   Wang L, 2019, IEEE-ASME T MECH, V24, P1271, DOI 10.1109/TMECH.2019.2908233
   Wang L, 2018, IEEE T SMART GRID, V9, P2824, DOI 10.1109/TSG.2016.2621135
   Wang L, 2017, IEEE T IND ELECTRON, V64, P7293, DOI 10.1109/TIE.2017.2682037
   Wilms C, 2019, LECT NOTES COMPUT SC, V11362, P678, DOI 10.1007/978-3-030-20890-5_43
   Yang HS, 2023, VISUAL COMPUT, V39, P2177, DOI 10.1007/s00371-022-02472-8
   Yeung CC, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3176239
   Yu YJ, 2020, NEUROCOMPUTING, V376, P1, DOI 10.1016/j.neucom.2019.09.071
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang JH, 2021, MULTIMED TOOLS APPL, V80, P16153, DOI 10.1007/s11042-019-08578-x
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu YC, 2022, IEEE T CIRC SYST VID, V32, P4188, DOI 10.1109/TCSVT.2021.3126590
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
NR 58
TC 4
Z9 4
U1 6
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 681
EP 698
DI 10.1007/s00371-023-02809-x
EA MAR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000946043200001
DA 2024-07-18
ER

PT J
AU Yang, M
   Feng, YH
   Rao, AS
   Rajasegarar, S
   Tian, SC
   Zhou, ZC
AF Yang, Meng
   Feng, Yanghe
   Rao, Aravinda S.
   Rajasegarar, Sutharshan
   Tian, Shucong
   Zhou, Zhengchun
TI Evolving graph-based video crowd anomaly detection
SO VISUAL COMPUTER
LA English
DT Article
DE Abnormal detection; Signal processing; Video surveillance; Graph
   understanding
ID TRACKING
AB Detecting anomalous crowd behavioral patterns from videos is an important task in video surveillance and maintaining public safety. In this work, we propose a novel architecture to detect anomalous patterns of crowd movements via graph networks. We represent individuals as nodes and individual movements with respect to other people as the node-edge relationship of an evolving graph network. We then extract the motion information of individuals using optical flow between video frames and represent their motion patterns using graph edge weights. In particular, we detect the anomalies in crowded videos by modeling pedestrian movements as graphs and then by identifying the network bottlenecks through a max-flow/min-cut pedestrian flow optimization scheme (MFMCPOS). The experiment demonstrates that the proposed framework achieves superior detection performance compared to other recently published state-of-the-art methods. Considering that our proposed approach has relatively low computational complexity and can be used in real-time environments, which is crucial for present day video analytics for automated surveillance.
C1 [Yang, Meng; Tian, Shucong; Zhou, Zhengchun] Southwest Jiaotong Univ, Sch Math, Chengdu 610000, Peoples R China.
   [Feng, Yanghe] Natl Univ Def Technol, Sch Syst Engn, Changsha 410073, Peoples R China.
   [Rao, Aravinda S.] Univ Melbourne, Dept Elect & Elect Engn, Melbourne 3010, Australia.
   [Rajasegarar, Sutharshan] Deakin Univ, Sch Informat Technol, Geelong 3125, Australia.
C3 Southwest Jiaotong University; National University of Defense Technology
   - China; University of Melbourne; Deakin University
RP Feng, YH (corresponding author), Natl Univ Def Technol, Sch Syst Engn, Changsha 410073, Peoples R China.
EM yangmeng@swjtu.edu.cn; fengyanghe@nudt.edu.cn;
   aravinda.rao@unimelb.edu.au; sutharshan.rajasegarar@deakin.edu.au;
   sctian@my.swjtu.edu.cn; zzc@swjtu.edu.cn
RI Rao, Aravinda/I-6941-2016; Zhou, Zhengchun/B-9530-2014
OI Rao, Aravinda/0000-0003-2319-6539; feng, yanghe/0000-0003-1608-8695;
   Rajasegarar, Sutharshan/0000-0002-6559-6736
FU Natural Science Foundation of China [12201523]; Fundamental Research
   Funds for the Central Universities [2682021CX078]
FX AcknowledgementsThe authors are very grateful to Editor and the
   anonymous reviewers for their valuable comments and suggestions that
   improved the presentation and quality of this paper highly. This work
   was supported by the Natural Science Foundation of China under Grants
   12201523 and also supported by the Fundamental Research Funds for the
   Central Universities under Grants No. 2682021CX078.
CR Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bouguet, 2001, INTEL CORP, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Chaker R, 2017, PATTERN RECOGN, V61, P266, DOI 10.1016/j.patcog.2016.06.016
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dantzig G., 2003, LINEAR INEQUALITIES, V38, P225
   Dubey S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11031344
   Farooq MU, 2022, VISUAL COMPUT, V38, P1553, DOI 10.1007/s00371-021-02088-4
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Ionescu RT, 2019, IEEE WINT CONF APPL, P1951, DOI 10.1109/WACV.2019.00212
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LY, 2008, IEEE T SYST MAN CY B, V38, P1254, DOI 10.1109/TSMCB.2008.927265
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mo X, 2014, IEEE T CIRC SYST VID, V24, P631, DOI 10.1109/TCSVT.2013.2280061
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Moshtaghi M, 2011, PATTERN RECOGN, V44, P2197, DOI 10.1016/j.patcog.2011.03.007
   Patel AS, 2023, VISUAL COMPUT, V39, P2127, DOI 10.1007/s00371-022-02469-3
   Rao AS, 2015, VISUAL COMPUT, V31, P1533, DOI 10.1007/s00371-014-1032-4
   Ravanbakhsh M, 2018, IEEE WINT CONF APPL, P1689, DOI 10.1109/WACV.2018.00188
   Reddy V., 2011, Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, P55
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sabih M, 2022, VISUAL COMPUT, V38, P1719, DOI 10.1007/s00371-021-02100-x
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Schroeder J., 2004, RELATORIOTECNICO RT
   Smeureanu S, 2017, LECT NOTES COMPUT SC, V10485, P779, DOI 10.1007/978-3-319-68548-9_70
   Tomasi C., 1991, CMU TECHNICAL REPORT
   Xu D., 2015, P BRIT MACHINE VISIO, P801
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Yang M, 2018, IEEE COMPUT SOC CONF, P328, DOI 10.1109/CVPRW.2018.00059
   Yang M, 2018, LECT NOTES ARTIF INT, V11012, P891, DOI 10.1007/978-3-319-97304-3_68
   Zeppelzauer M, 2010, LECT NOTES COMPUT SC, V5916, P433, DOI 10.1007/978-3-642-11301-7_44
   Zhou B., 2011, P CVPR, P3441, DOI DOI 10.1109/CVPR.2011.5995459
NR 41
TC 5
Z9 5
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 303
EP 318
DI 10.1007/s00371-023-02783-4
EA JAN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000919380800001
DA 2024-07-18
ER

PT J
AU Reimann, M
   Buchheim, B
   Semmo, A
   Döllner, J
   Trapp, M
AF Reimann, Max
   Buchheim, Benito
   Semmo, Amir
   Doellner, Juergen
   Trapp, Matthias
TI Controlling strokes in fast neural style transfer using content
   transforms
SO VISUAL COMPUTER
LA English
DT Article
AB Fast style transfer methods have recently gained popularity in art-related applications as they make a generalized real-time stylization of images practicable. However, they are mostly limited to one-shot stylizations concerning the interactive adjustment of style elements. In particular, the expressive control over stroke sizes or stroke orientations remains an open challenge. To this end, we propose a novel stroke-adjustable fast style transfer network that enables simultaneous control over the stroke size and intensity, and allows a wider range of expressive editing than current approaches by utilizing the scale-variance of convolutional neural networks. Furthermore, we introduce a network-agnostic approach for style-element editing by applying reversible input transformations that can adjust strokes in the stylized output. At this, stroke orientations can be adjusted, and warping-based effects can be applied to stylistic elements, such as swirls or waves. To demonstrate the real-world applicability of our approach, we present StyleTune, a mobile app for interactive editing of neural style transfers at multiple levels of control. Our app allows stroke adjustments on a global and local level. It furthermore implements an on-device patch-based upsampling step that enables users to achieve results with high output fidelity and resolutions of more than 20 megapixels. Our approach allows users to art-direct their creations and achieve results that are not possible with current style transfer applications.
C1 [Reimann, Max; Buchheim, Benito; Doellner, Juergen; Trapp, Matthias] Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
   [Semmo, Amir] DigitalMasterpieces GmbH, R&D, Potsdam, Germany.
C3 University of Potsdam
RP Reimann, M (corresponding author), Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
EM max.reimann@hpi.uni-potsdam.de; amir.semmo@digitalmasterpieces.com
RI Trapp, Matthias/J-4456-2014; Reimann, Max/JDH-5860-2023; Semmo,
   Amir/KPA-5814-2024
OI Trapp, Matthias/0000-0003-3861-5759; Reimann, Max/0000-0003-2146-4229;
   Semmo, Amir/0000-0002-1553-4940
FU Projekt DEAL; German Federal Ministry of Education and Research (BMBF)
   [01IS18092, 01IS19006]
FX Open Access funding enabled and organized by Projekt DEAL. This work was
   partially funded by the German Federal Ministry of Education and
   Research (BMBF) through grants 01IS18092("mdViPro") and 01IS19006
   ("KI-LAB-ITSE").
CR Amato G, 2019, Arxiv, DOI arXiv:1905.04175
   Babaeizadeh M., 2020, 8 INT C LEARNING REP
   Barnes C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766934
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Dapkus D., How to transfer styles to images with Adobe Pho- toshop
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Fiser J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925948
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gobbi DG, 2003, COMPUT MED IMAG GRAP, V27, P255, DOI 10.1016/S0895-6111(02)00091-5
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isenberg T., 2016, PROC NPAR SER EXPRES, P8996
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Klingbeil M, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3132803
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Li YJ, 2017, ADV NEUR IN, V30
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Lin T.-Y., 2014, CoRR, P740
   Marques Oge, 2020, MACHINE LEARNING COR, P29, DOI 10.1007/978-3-030-54032-64
   Mohanty S, 2012, J PHYS CONF SER, V368, DOI 10.1088/1742-6596/368/1/012024
   Moiseenkov A., 2021, PRISMA
   Pasewaldt S., 2016, SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications, P1
   Paszke A, 2019, ADV NEUR IN, V32
   Reimann M, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P1, DOI 10.1109/CW52790.2021.00009
   Reimann M, 2019, VISUAL COMPUT, V35, P1531, DOI 10.1007/s00371-019-01654-1
   Reimann M, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P9, DOI 10.1109/CW.2018.00016
   Semmo Amir., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, V5, P1, DOI DOI 10.1145/3092919.3092920
   Simonyan K., 2015, P ICLR
   Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Texler Ondrej, 2019, P 8 ACM EG EXPR S, P43
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu H, 2019, NEUROCOMPUTING, V370, P39, DOI 10.1016/j.neucom.2019.08.075
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Yang LC, 2018, COMPUT GRAPH FORUM, V37, P97, DOI 10.1111/cgf.13551
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Youssef V, 2017, RANDOM NUMBER GENERA
   Zhang H, 2019, LECT NOTES COMPUT SC, V11132, P349, DOI 10.1007/978-3-030-11018-5_32
   Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1
NR 49
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4019
EP 4033
DI 10.1007/s00371-022-02518-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000899328400004
OA hybrid
DA 2024-07-18
ER

PT J
AU Roy, A
   Sharma, LD
   Shukla, AK
AF Roy, Amarjit
   Sharma, Lakhan Dev
   Shukla, Alok Kumar
TI Multiclass CNN-based adaptive optimized filter for removal of impulse
   noise from digital images
SO VISUAL COMPUTER
LA English
DT Article
DE Fixed valued impulse noise; Convolutional neural network; Vectored
   minimum mean value; Particle swarm optimization; Image quality metric
ID SWITCHING MEDIAN FILTER; FUZZY FILTER; LINEAR PREDICTION; BILATERAL
   FILTER; PEPPER NOISE; DETECTOR; REDUCTION
AB Multiclass CNN-based window adaptive optimized filter has been proposed in this research work to remove impulse noise from colored images. Instead of applying adaptive filter on the whole images, selective adaptive window has been applied on the test image so that the effective computational complexity of the system is reduced. In this article, multiclass CNN has been used for the choosing the range of selective window. After that, vectored minimum mean value-based detection is being applied on the pixel under operation on a particular kernel of image for detection of noise and defining the appropriate size of the window surrounding the pixel under operation. An adaptive vector median filter incorporated with particle swarm optimization (AVMPSO) is passed over the corrupted pixel if the pixel is determined as corrupted after detection of noisy and noise-free pixels. The performance comparison of the proposed AVMPSO has been carried out along with the available state-of-the-art filters in terms of peak signal-to-noise ratio (PSNR), mean square error (MSE), structural similarity index matrix (SSIM), and feature similarity index matrix (FSIMc). A large set of images has been considered to conclude how AVMPSO outperforms the existing methods at all levels of noise by similar to 1.5 to 3 dB (PSNR) at least.
C1 [Roy, Amarjit] Ghani Khan Inst Engn & Technol, Dept Elect Engn, Malda, India.
   [Sharma, Lakhan Dev] VIT AP Univ, Sch Elect, Vijayawada, India.
   [Shukla, Alok Kumar] Thapar Inst Engn & Technol Patiala, Dept Comp Sci Engn, Patiala, Punjab, India.
C3 VIT-AP University; Thapar Institute of Engineering & Technology
RP Roy, A (corresponding author), Ghani Khan Inst Engn & Technol, Dept Elect Engn, Malda, India.
EM royamarjit90@gmail.com; devsharmalakhan@gmail.com;
   alokjestshukla@gmail.com
RI SHARMA, LAKHAN DEV/D-4578-2019; Roy, Amarjit/ABD-1033-2020
OI SHARMA, LAKHAN DEV/0000-0002-5389-3928; 
CR Abdelmounaime Safia, 2013, ISRN Machine Vision, DOI 10.1155/2013/876386
   Ahmed F, 2014, IEEE T FUZZY SYST, V22, P1352, DOI 10.1109/TFUZZ.2013.2286634
   ALPARONE L, 1995, IEEE T CIRCUITS-II, V42, P130, DOI 10.1109/82.365355
   ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807
   Bhadouria VS, 2014, SIGNAL IMAGE VIDEO P, V8, P71, DOI 10.1007/s11760-013-0487-5
   Budak C, 2015, NEURAL COMPUT APPL, V26, P835, DOI 10.1007/s00521-014-1767-x
   Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196
   Dawood H, 2015, MULTIMED TOOLS APPL, V74, P11485, DOI 10.1007/s11042-014-2246-1
   Eng HL, 2001, IEEE T IMAGE PROCESS, V10, P242, DOI 10.1109/83.902289
   Esakkirajan S, 2011, IEEE SIGNAL PROC LET, V18, P287, DOI 10.1109/LSP.2011.2122333
   Geng X, 2012, SIGNAL PROCESS, V92, P150, DOI 10.1016/j.sigpro.2011.06.015
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Horng SJ, 2013, J VIS COMMUN IMAGE R, V24, P956, DOI 10.1016/j.jvcir.2013.06.012
   Jafar IF, 2013, IEEE T IMAGE PROCESS, V22, P1223, DOI 10.1109/TIP.2012.2228496
   KO SJ, 1991, IEEE T CIRCUITS SYST, V38, P984, DOI 10.1109/31.83870
   Laskar R.H., 2009, TENCON 2009 2009 IEE, P1
   Lin CH, 2010, IEEE T IMAGE PROCESS, V19, P2307, DOI 10.1109/TIP.2010.2047906
   Liu SY, 2022, BIOMED OPT EXPRESS, V13, P2247, DOI 10.1364/BOE.450259
   Masood S, 2014, APPL SOFT COMPUT, V21, P107, DOI 10.1016/j.asoc.2014.03.006
   Mohapatra S, 2012, NEURAL COMPUT APPL, V21, P281, DOI 10.1007/s00521-011-0583-9
   Mursal ASN, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122034
   PITAS I, 1992, P IEEE, V80, P1893, DOI 10.1109/5.192071
   Roy A., 2016, CINE 2016 C, P1
   Roy A, 2015, TENCON IEEE REGION
   Roy A, 2017, AEU-INT J ELECTRON C, V72, P114, DOI 10.1016/j.aeue.2016.12.006
   Roy A, 2016, SIGNAL PROCESS, V128, P262, DOI 10.1016/j.sigpro.2016.04.007
   Roy A, 2016, APPL SOFT COMPUT, V46, P816, DOI 10.1016/j.asoc.2015.09.032
   Schulte S, 2007, IMAGE VISION COMPUT, V25, P1377, DOI 10.1016/j.imavis.2006.10.002
   Singh KM, 2012, INT J COMPUT SCI ENG, V7, P345, DOI 10.1504/IJCSE.2012.049729
   Singh KM, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P396
   Smolka B, 2003, REAL-TIME IMAGING, V9, P261, DOI 10.1016/j.rti.2003.09.015
   SUN T, 1994, PATTERN RECOGN LETT, V15, P341, DOI 10.1016/0167-8655(94)90082-5
   Tukey J.W., 1977, EXPLORATORY DATA ANA
   Tukey J. W., 1974, P EASCOM, P673
   Veerakumar T, 2019, EXPERT SYST APPL, V121, P18, DOI 10.1016/j.eswa.2018.12.009
   Verma OP, 2013, MULTIDIM SYST SIGN P, V24, P181, DOI 10.1007/s11045-011-0164-1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 1999, IEEE T CIRCUITS-II, V46, P78, DOI 10.1109/82.749102
   Yuan SQ, 2006, SIGNAL PROCESS, V86, P2123, DOI 10.1016/j.sigpro.2006.01.009
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SQ, 2002, IEEE SIGNAL PROC LET, V9, P360, DOI 10.1109/LSP.2002.805310
NR 41
TC 4
Z9 4
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5809
EP 5822
DI 10.1007/s00371-022-02697-7
EA OCT 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000873352200001
DA 2024-07-18
ER

PT J
AU Furuya, T
   Liu, WJ
   Ohbuchi, R
   Kuang, ZZ
AF Furuya, Takahiko
   Liu, Wujie
   Ohbuchi, Ryutarou
   Kuang, Zhenzhong
TI Hyperplane patch mixing-and-folding decoder and weighted chamfer
   distance loss for 3D point set reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point set; 3D shape reconstruction; Computer vision; Deep learning
AB 3D point set reconstruction is an important and challenging 3D shape analysis task. Current state-of-the-art algorithms for 3D point set reconstruction employ a deep neural network (DNN) having an encoder-decoder architecture. Recently, the decoder DNNs that transform multiple 2D planar patches to reconstruct a 3D shape have seen some success. These "patch-folding" decoders are adept at approximating smooth surfaces in 3D objects. However, 3D point sets generated by these decoders often lack local geometrical details, as 2D planar patches tend to overly constrain the patch folding process. In this paper, we propose a novel decoder DNN for 3D point sets called Hyperplane Mixing and Folding Net (HMF-Net). HMF-Net uses less constrained hyperplane, not 2D plane, patches as its input to the folding process. HMF-Net has, as its core building block, a stack of token-mixing layers to effectively learn global consistency among the hyperplane patches. In addition to HMF-Net, we also propose a novel loss for 3D point set reconstruction called Weighted Chamfer Distance (WCD). WCD tries to weight, or amplify, loss from parts of shape that are highly variable across training samples by emphasizing higher point-pair distance values between a generated point set and a groundtruth point set. This helps the decoder DNN learn shape details better. We comprehensively evaluate our algorithm under three 3D point set reconstruction scenarios, that are, shape completion, shape upsampling, and shape reconstruction from 2D images. Experimental results demonstrate that our algorithm yields accuracies higher than the existing algorithms for 3D point set reconstruction.
C1 [Furuya, Takahiko; Ohbuchi, Ryutarou] Univ Yamanashi, Dept Comp Sci & Engn, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
   [Liu, Wujie; Kuang, Zhenzhong] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310000, Peoples R China.
C3 University of Yamanashi; Hangzhou Dianzi University
RP Furuya, T (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
EM takahikof@yamanashi.ac.jp
FU Japan Society for the Promotion of Science (JSPS) KAKENHI [21K17763];
   Zhejiang Provincial Natural Science Foundation of China [LY22F020028];
   Grants-in-Aid for Scientific Research [21K17763, 21K11903] Funding
   Source: KAKEN
FX Takahiko Furuya and Wujie Liu contributed equally. This work was
   supported by the Japan Society for the Promotion of Science (JSPS)
   KAKENHI (Grant No. 21K17763) and was supported in part by Zhejiang
   Provincial Natural Science Foundation of China (Grant No. LY22F020028).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Achlioptas P, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Chang A. X., 2015, ARXIV
   Choe J., ARXIV
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deprelle T, 2019, ADV NEUR IN, V32
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Furuya T., 2016, BMVC 2016
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   Gong Z., 2022, P IEEECVF C COMPUTER, P1726
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2016, ARXIV
   Hui L., 2020, P EUR C COMP VIS, P397
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li RH, 2021, PROC CVPR IEEE, P344, DOI 10.1109/CVPR46437.2021.00041
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li YY, 2018, ADV NEUR IN, V31
   Liu H., 2021, PROC NEURIPS, V34
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   MANDIKAL P, 2018, ARXIV180707796
   Melas-Kyriazi L., ARXIV
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Qi CR, 2017, ADV NEUR IN, V30
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047
   Tolstikhin Ilya O., 2021, P 35 C NEURAL INFORM
   Touvron H., ARXIV
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X., 2022, P IEEECVF C COMPUTER
   Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736
   Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201
   Xiang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5479, DOI 10.1109/ICCV48922.2021.00545
   Yang J., 2021, PROC IEEE INT C COMP, P6413
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279, DOI 10.1109/ICASSP.2018.8462291
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 59
TC 2
Z9 2
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5167
EP 5184
DI 10.1007/s00371-022-02652-6
EA SEP 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000850763100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Cai, RT
   Fang, M
AF Cai, Rongtai
   Fang, Ming
TI Blind image quality assessment by simulating the visual cortex
SO VISUAL COMPUTER
LA English
DT Article
DE Blind image quality assessment; Natural scene statistics; Feature
   extraction; Filter; Bioinspired computing
ID RECEPTIVE-FIELDS; FUNCTIONAL ARCHITECTURE; NATURAL IMAGES; STATISTICS;
   ORGANIZATION; BIPOLAR; RETINA; CELLS; MODEL
AB Objective assessment of image quality seeks to predict image quality without human perception. Given that the ultimate goal of a blind/no-reference image quality assessment (BIQA) algorithm is to provide a score consistent with the subject's prediction, it makes sense to design an algorithm that resembles human behavior. Recently, a large number of image features have been introduced to image quality assessment. However, only a few of these features are generated by using the computational mechanisms of the visual cortex. In this paper, we propose bioinspired algorithms to extract image features for BIQA by simulating the visual cortex. We extract spatial features like texture and energy from images by mimicking the retinal circuit. We extract spatial-frequency features from images by imitating the simple cell of the primary visual cortex. And we extract color features from images by employing the color opponent mechanism of the biological vision system. Then, by using the statistical features derived from these physiologically plausible features, we train a support vector regression model to predict image quality. The experimental results show that the proposed algorithm is more consistent with subjective evaluations than the comparison algorithms in predicting image quality.
C1 [Cai, Rongtai] Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou 350117, Fujian, Peoples R China.
   [Fang, Ming] Fujian Normal Univ, Fujian Prov Engn Technol Res Ctr Photoelect Sensi, Fuzhou 350117, Peoples R China.
   [Fang, Ming] Fujian Normal Univ, Key Lab OptoElect Sci & Technol Med, Minist Educ, Fuzhou 350117, Peoples R China.
C3 Fujian Normal University; Fujian Normal University; Fujian Normal
   University
RP Cai, RT (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou 350117, Fujian, Peoples R China.
EM gjrtcai@fjnu.edu.cn
OI Cai, Rongtai/0000-0003-4505-7927
CR Beaudot WHA, 1996, NETWORK-COMP NEURAL, V7, P317, DOI 10.1088/0954-898X/7/2/012
   Benoit A, 2010, COMPUT VIS IMAGE UND, V114, P758, DOI 10.1016/j.cviu.2010.01.011
   Berga D, 2019, VISION RES, V154, P60, DOI 10.1016/j.visres.2018.10.006
   Berman MG, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114572
   Dacey D, 2000, VISION RES, V40, P1801, DOI 10.1016/S0042-6989(00)00039-0
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gupta P, 2018, SIGNAL PROCESS-IMAGE, V66, P87, DOI 10.1016/j.image.2018.05.009
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji JY, 2023, VISUAL COMPUT, V39, P443, DOI 10.1007/s00371-021-02340-x
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   KANEKO A, 1973, J PHYSIOL-LONDON, V235, P133, DOI 10.1113/jphysiol.1973.sp010381
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kardan O, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00471
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kolb H, 2003, AM SCI, V91, P28, DOI 10.1511/2003.11.841
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lasmar NE, 2009, IEEE IMAGE PROC, P2281, DOI 10.1109/ICIP.2009.5414404
   LENNIE P, 1990, J NEUROSCI, V10, P649
   Li CF, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116064
   Li QH, 2019, NEUROCOMPUTING, V331, P189, DOI 10.1016/j.neucom.2018.11.015
   Liu LX, 2019, IEEE T MULTIMEDIA, V21, P2305, DOI 10.1109/TMM.2019.2900941
   Ma JP, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102895
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Nizami IF, 2020, MULTIMED TOOLS APPL, V79, P26285, DOI 10.1007/s11042-020-09229-2
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Pridmore RW, 2021, ATTEN PERCEPT PSYCHO, V83, P1797, DOI 10.3758/s13414-020-02216-7
   Rajalakshmi T, 2016, BIOL INSPIR COGN ARC, V18, P95, DOI 10.1016/j.bica.2016.09.005
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Thung KH, 2009, 2009 INTERNATIONAL CONFERENCE FOR TECHNICAL POSTGRADUATES (TECHPOS 2009), P153
   TSO DY, 1988, J NEUROSCI, V8, P1712
   VALETON JM, 1983, VISION RES, V23, P1549, DOI 10.1016/0042-6989(83)90168-2
   Wang Z., 2006, Modern Image Quality Assessment, DOI 10.2200/S00010ED1V01Y200508IVM003
   Wu JJ, 2019, J VIS COMMUN IMAGE R, V58, P353, DOI 10.1016/j.jvcir.2018.12.005
   Wu JJ, 2016, INFORM SCIENCES, V351, P18, DOI 10.1016/j.ins.2016.02.043
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Y, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043025
   Zhu H., 2020, P IEEE CVF C COMP VI, P14143
NR 57
TC 5
Z9 5
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4639
EP 4656
DI 10.1007/s00371-022-02614-y
EA AUG 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000840290600003
DA 2024-07-18
ER

PT J
AU Zhu, JG
   Wei, J
   Hao, BB
AF Zhu, Jianguang
   Wei, Juan
   Hao, Binbin
TI Ultrasound images speckle noise removal by nonconvex hybrid overlapping
   group sparsity model
SO VISUAL COMPUTER
LA English
DT Article
DE Speckle noise; Nonconvex high-order total variation; Overlapping group
   sparse total variation; Alternating direction method of multipliers;
   Iteratively re-weighted l(1) algorithm
ID REDUCTION
AB In this paper, a novel hybrid variational model is proposed for speckle noise removal. This model contains the regularization term combined the nonconvex high-order total variation (HOTV) and overlapping group sparse total variation (OGSTV) and the data fidelity term depicted by a generalized Kullback-Leibler divergence. The proposed model inherits the advantages of nonconvex HOTV regularization and overlapping group sparse regularization and can more effectively preserve the edges and simultaneously eliminate staircase artifacts. Under the framework of alternating direction method of multipliers, we develop an efficient alternating minimization algorithm by using iteratively re-weighted l(1) algorithm, majorization-minimization algorithm, and Newton iteration algorithm to solve the corresponding iterative scheme. Numerical experiments show that the proposed model performs better in comparison with some state-of-the-art models in visual quality and certain image quality measurement.
C1 [Zhu, Jianguang; Wei, Juan] Shandong Univ Sci & Technol, Coll Math & Syst Sci, Qingdao 266590, Peoples R China.
   [Hao, Binbin] China Univ Petr, Coll Sci, Qingdao 266580, Peoples R China.
C3 Shandong University of Science & Technology; China University of
   Petroleum
RP Hao, BB (corresponding author), China Univ Petr, Coll Sci, Qingdao 266580, Peoples R China.
EM bbhao981@163.com
FU Project of Shandong Province Higher Educational Science and Technology
   Program [J17KA166]; Major Program of the National Natural Science
   Foundation of China [11991024]; National Natural Science Foundation of
   China [61976126]; Science and Technology Support Plan for Youth
   Innovation of Colleges and Universities of Shandong Province of China
   [2019KJI005]
FX This work was supported by a Project of Shandong Province Higher
   Educational Science and Technology Program (J17KA166), by the Major
   Program of the National Natural Science Foundation of China (11991024),
   by the National Natural Science Foundation of China (61976126), by the
   Science and Technology Support Plan for Youth Innovation of Colleges and
   Universities of Shandong Province of China (2019KJI005).
CR Afonso M, 2015, NEUROCOMPUTING, V150, P200, DOI 10.1016/j.neucom.2014.08.073
   Arias P, 2019, IEEE COMPUT SOC CONF, P1917, DOI 10.1109/CVPRW.2019.00243
   Chen BH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3048488
   Chen BH, 2020, IEEE SIGNAL PROC LET, V27, P1670, DOI 10.1109/LSP.2020.3024990
   Choi H, 2019, J X-RAY SCI TECHNOL, V27, P885, DOI 10.3233/XST-190515
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   Han Y, 2013, PATTERN RECOGN, V46, P989, DOI 10.1016/j.patcog.2012.10.010
   Hou B, 2012, IEEE J-STARS, V5, P809, DOI 10.1109/JSTARS.2012.2196680
   Huang J, 2013, SIGNAL PROCESS, V93, P684, DOI 10.1016/j.sigpro.2012.09.005
   Huang YM, 2009, SIAM J IMAGING SCI, V2, P20, DOI 10.1137/080712593
   Ji TY, 2017, APPL MATH MODEL, V48, P410, DOI 10.1016/j.apm.2017.04.002
   Jin ZM, 2011, J MATH IMAGING VIS, V39, P62, DOI 10.1007/s10851-010-0225-3
   Krissian K, 2005, PROC CVPR IEEE, P547
   Kwak Y, 2019, IEEE GEOSCI REMOTE S, V16, P549, DOI 10.1109/LGRS.2018.2877599
   Li CY, 2020, J COMPUT APPL MATH, V370, DOI 10.1016/j.cam.2019.112684
   Li G, 2017, MULTIMED TOOLS APPL, V76, P17615, DOI 10.1007/s11042-015-2810-3
   Liu G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122562
   Liu J, 2016, NEUROCOMPUTING, V216, P502, DOI 10.1016/j.neucom.2016.07.049
   Liu XW, 2021, SIGNAL PROCESS, V188, DOI 10.1016/j.sigpro.2021.108247
   Liu XW, 2019, VISUAL COMPUT, V35, P1883, DOI 10.1007/s00371-018-1581-z
   Lv YH, 2020, J APPL MATH COMPUT, V62, P489, DOI 10.1007/s12190-019-01293-8
   Lyu Q, 2013, NEUROCOMPUTING, V119, P413, DOI 10.1016/j.neucom.2013.03.017
   Mei JJ, 2018, J FRANKLIN I, V355, P574, DOI 10.1016/j.jfranklin.2017.10.035
   Parrilli S, 2012, IEEE T GEOSCI REMOTE, V50, P606, DOI 10.1109/TGRS.2011.2161586
   Röhlig M, 2018, VISUAL COMPUT, V34, P1209, DOI 10.1007/s00371-018-1486-x
   Rudin L, 2003, GEOMETRIC LEVEL SET METHODS IN IMAGING, VISION AND GRAPHICS, P103, DOI 10.1007/0-387-21810-6_6
   Tang LM, 2019, NEUROCOMPUTING, V359, P15, DOI 10.1016/j.neucom.2019.05.073
   Wang S, 2018, NUMER ALGORITHMS, V78, P513, DOI 10.1007/s11075-017-0386-x
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu TT, 2021, APPL MATH COMPUT, V410, DOI 10.1016/j.amc.2021.126170
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 32
TC 0
Z9 0
U1 2
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4787
EP 4799
DI 10.1007/s00371-022-02627-7
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000840290600001
DA 2024-07-18
ER

PT J
AU Xin, LQ
   Wang, DW
   Shi, WX
AF Xin, Liqi
   Wang, Dingwen
   Shi, Wenxuan
TI FISTA-CSNet: a deep compressed sensing network by unrolling iterative
   optimization algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Compressed sensing; Deep learning; Image reconstruction; FISTA; Sampling
   matrix
ID THRESHOLDING ALGORITHM; SPARSE RECONSTRUCTION; IMAGE-RECONSTRUCTION;
   SIGNAL RECOVERY; REPRESENTATION
AB In order to fast sample an image and accurately reconstruct the image from a small amount of sampled data, we design a novel deep network for optimization-based algorithm mapping to efficiently tackle the problem of image compressed sensing (CS). The new deep network structure, dubbed FISTA-CSNet, unrolls the fast iterative shrinkage-thresholding algorithm (FISTA) into two modules: sampling matrix module and reconstruction network module. The two modules are optimized jointly and the parameters in the matrix and network are discriminatively learned by end-to-end training. The sampling matrix is adaptively learned from the training images, which can better utilize the image texture information for CS reconstruction. The reconstruction network module is subdivided into two parts. The first part casts the optimization-based algorithm into deep network form and the second part uses a set of convolutional filters and nonlinear activation function to reduce the blocking artifacts introduced by block CS. In view of the unavailability of the reconstruction network at different sampling ratios, the ratio-adaptive sampling matrix and the reconstruction network are proposed to realize the multi-sampling ratio reuse version of FISTA-CSNet, dubbed FISTA-CSNet*, so that the system can operate on a range of sampling ratios. Extensive experiments show that the proposed FISTA-CSNets outperform previous state-of-the-art CS methods in term of PSNR, SSIM, FSIM and visual quality.
C1 [Xin, Liqi; Wang, Dingwen] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Shi, Wenxuan] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Shi, WX (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
EM shiwx@whu.edu.cn
OI Shi, Wenxuan/0000-0002-9193-3185
FU National Natural Science Foundation of China [61501334]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61501334.
CR Lahaw ZB, 2022, VISUAL COMPUT, V38, P2431, DOI 10.1007/s00371-021-02121-6
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Dhengre N, 2022, VISUAL COMPUT, V38, P837, DOI 10.1007/s00371-020-02054-6
   Dong JX, 2022, IEEE T PATTERN ANAL, V44, P9960, DOI 10.1109/TPAMI.2021.3138787
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Gregor K, 2010, P 27 INT C INT C MAC, P399
   Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031
   He LH, 2009, IEEE T SIGNAL PROCES, V57, P3488, DOI 10.1109/TSP.2009.2022003
   Hernández-Bautista I, 2021, VISUAL COMPUT, V37, P957, DOI 10.1007/s00371-020-01846-0
   Iliadis M, 2018, DIGIT SIGNAL PROCESS, V72, P9, DOI 10.1016/j.dsp.2017.09.010
   Kim YK, 2010, IEEE IMAGE PROC, P3365, DOI 10.1109/ICIP.2010.5652744
   KULKARNI K, 2016, PROC CVPR IEEE, P449, DOI DOI 10.1109/CVPR.2016.55
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li ChunLong Li ChunLong, 2009, China Vegetables, P46
   Li LRH, 2019, INT J COMPUT VISION, V127, P1025, DOI 10.1007/s11263-018-01146-0
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Montazeri A, 2021, VISUAL COMPUT, V37, P707, DOI 10.1007/s00371-020-01970-x
   Mousavi A, 2017, INT CONF ACOUST SPEE, P2272, DOI 10.1109/ICASSP.2017.7952561
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Mun S, 2010, IEEE DATA COMPR CONF, P547, DOI 10.1109/DCC.2010.90
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Pfister L, 2015, PROC SPIE, V9597, DOI 10.1117/12.2188663
   Qu XB, 2014, MED IMAGE ANAL, V18, P843, DOI 10.1016/j.media.2013.09.007
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun Jian, 2016, ADV NEURAL INFORM PR, P10
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang S., 2016, BIOMED RES INT, V2016, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wright SJ, 2008, INT CONF ACOUST SPEE, P3373, DOI 10.1109/ICASSP.2008.4518374
   Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhan ZF, 2016, IEEE T BIO-MED ENG, V63, P1850, DOI 10.1109/TBME.2015.2503756
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2012, IEEE J EM SEL TOP C, V2, P380, DOI 10.1109/JETCAS.2012.2220391
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao C, 2016, IEEE DATA COMPR CONF, P161, DOI 10.1109/DCC.2016.104
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P1182, DOI 10.1109/TCSVT.2016.2527181
   Zhou SZ, 2021, IEEE T INTELL TRANSP, V22, P6784, DOI 10.1109/TITS.2020.2994779
NR 47
TC 3
Z9 3
U1 1
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4177
EP 4193
DI 10.1007/s00371-022-02583-2
EA JUL 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000830968500001
DA 2024-07-18
ER

PT J
AU Liu, J
   Tan, JQ
   He, L
AF Liu, Jing
   Tan, Jieqing
   He, Lei
TI A fast blind image deblurring method using salience map and gradient
   cepstrum
SO VISUAL COMPUTER
LA English
DT Article
DE Image deblurring; Deconvolution; Kernel estimation; Salience map;
   Gradient cepstrum
ID SHAKEN
AB The prior-based blind image deblurring methods have recently achieved good performance. However, many state-of-art algorithms are time-consuming since some nonlinear operators are involved. Presented in this paper is a fast blind image deblurring algorithm which uses the salience map and gradient cepstrum. The inspiration for this work comes from the fact that the extreme values of the salience map of the clear image are more sparse than those of the blurred one. By enforcing the L-0 norm constraint to the terms involving salience map and incorporating them into the traditional deblurring framework, an effective optimization scheme is explored. Furthermore, gradient cepstrum is used to adjust the number of iterations in each scale and determine the size of the initial kernel. Experimental results illustrate that our algorithm outperforms the state-of-art deblurring algorithms in both benchmark datasets and real blur scenes. Besides, this algorithm greatly shortens the running time since it restrains excessive iterations and does not involve any nonlinear operators.
C1 [Liu, Jing] Anhui Jianzhu Univ, Sch Elect & Informat Engn, Hefei 230601, Peoples R China.
   [Tan, Jieqing; He, Lei] Hefei Univ Technol, Sch Math, Hefei 230009, Peoples R China.
C3 Anhui Jianzhu University; Hefei University of Technology
RP Tan, JQ (corresponding author), Hefei Univ Technol, Sch Math, Hefei 230009, Peoples R China.
EM jieqingtan@126.com
RI Tan, Jie/IVV-5250-2023
FU National Natural Science Foundation of China [62172135]
FX We would like to thank the reviewers for their helpful comments and
   suggestions which greatly improve the quality of the paper. This work
   was supported by the National Natural Science Foundation of China under
   Grant 62172135.
CR Cai JR, 2020, IEEE T IMAGE PROCESS, V29, P6885, DOI 10.1109/TIP.2020.2995048
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P1302, DOI 10.1109/TIP.2015.2400217
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Lai WS, 2015, PROC CVPR IEEE, P64, DOI 10.1109/CVPR.2015.7298601
   Lee H, 2020, IEEE T IMAGE PROCESS, V29, P710, DOI 10.1109/TIP.2019.2933739
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li TH, 2002, IEEE T IMAGE PROCESS, V11, P847, DOI 10.1109/TIP.2002.801127
   Liu J, 2020, IEEE ACCESS, V8, P219295, DOI 10.1109/ACCESS.2020.3039281
   Liu SG, 2015, VISUAL COMPUT, V31, P733, DOI 10.1007/s00371-014-0998-2
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Mingzhu S., 2015, INT J DISTRIB SENSOR
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Parvaz R., 2021, POINT SPREAD FUNCTIO
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shao WZ, 2020, NEUROCOMPUTING, V413, P305, DOI 10.1016/j.neucom.2020.06.093
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Sun LB, 2013, IEEE INT CONF COMPUT
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Whyte O, 2014, INT J COMPUT VISION, V110, P185, DOI 10.1007/s11263-014-0727-3
   Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   YAN YY, 2017, PROC CVPR IEEE, P6978, DOI DOI 10.1109/CVPR.2017.738
   Yang H, 2019, VISUAL COMPUT, V35, P1627, DOI 10.1007/s00371-018-1562-2
   Yu HC, 2019, KSII T INTERNET INF, V13, P855, DOI 10.3837/tiis.2019.02.020
   Zhai Y., 2006, ACM INT C MULT SANT
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang Z., 2022, VISUAL COMPUT
NR 44
TC 3
Z9 3
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3091
EP 3107
DI 10.1007/s00371-022-02515-0
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000815765300001
DA 2024-07-18
ER

PT J
AU Huang, WK
   Zhu, ZY
   Chen, LG
   Go, K
   Chen, XD
   Mao, XY
AF Huang, Wangkang
   Zhu, Zhenyang
   Chen, Ligeng
   Go, Kentaro
   Chen, Xiaodiao
   Mao, Xiaoyang
TI Image recoloring for Red-Green dichromats with compensation range-based
   naturalness preservation and refined dichromacy gamut
SO VISUAL COMPUTER
LA English
DT Article
DE Image recoloring; Color vision deficiency; Contrast compensation;
   Naturalness preservation
ID COLOR CONTRAST ENHANCEMENT; SIMULATION; VISION; PEOPLE
AB People with color vision deficiency (CVD) have difficulty discriminating colors, which can cause loss of chromatic contrast in the perception of affected individuals. To compensate for contrast loss, image recoloring approaches have been proposed in the existing studies. In state-of-the-art studies, recoloring models were built based on an approximated gamut of CVD in the CIE L*a*b* (Lab) color space, which significantly deviates from the original gamut. In addition, luminance was not considered during recoloring. Moreover, existing methods also present problems, such as high computational costs and insufficient naturalness preservation . In this paper, we propose a novel recoloring method to compensate for CVD that enhances contrast through adopting a luminance channel-considered optimization model while preserving naturalness by imposing hard constraints on the amount of changes to the original colors. To obtain a better compensation effect, we fit a new curved surface for representing the gamut of dichromacy in the Lab color space more accurately. Furthermore, a discrete solver is implemented to solve the optimization problem efficiently. For effective assessment, qualitative, quantitative, and subjective experiments were conducted, and a new metric, called preference, is proposed to evaluate the contrast enhancement and naturalness preservation comprehensively.
C1 [Huang, Wangkang; Chen, Xiaodiao] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Peoples R China.
   [Zhu, Zhenyang; Go, Kentaro; Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi, Japan.
   [Huang, Wangkang; Chen, Ligeng] Univ Yamanashi, Grad Sch Engn, Kofu, Yamanashi, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi; University of
   Yamanashi
RP Chen, XD (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Peoples R China.; Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu, Yamanashi, Japan.
EM huang10669668281@gmail.com; zzhu@yamanashi.ac.jp;
   ligengchen422@gmail.com; go@yamanashi.ac.jp; xiaodiao@hdu.edu.cn;
   mao@yamanashi.ac.jp
RI Mao, Xiaoyang/AAG-1294-2020
OI Mao, Xiaoyang/0000-0001-5010-6952; Zhu, Zhenyang/0000-0003-1023-3193
FU JSPS [17H00738, 19H05472, 20J15406, 22H0054]; National Natural Science
   Foundation of China [61972120]; Grants-in-Aid for Scientific Research
   [20K20408, 22H00549, 22K21274, 20J15406] Funding Source: KAKEN
FX The work reported in this paper. This work is supported by JSPS
   Grants-in-Aid for Scientific Research (Grant Nos. 17H00738, 19H05472,
   20J15406, 22H0054) and the National Natural Science Foundation of China
   (Grant Nos. 61972120).
CR Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   Hassan MF, 2019, MULTIDIM SYST SIGN P, V30, P1975, DOI 10.1007/s11045-019-00638-7
   Hassan MF, 2017, SIGNAL PROCESS-IMAGE, V57, P126, DOI 10.1016/j.image.2017.05.011
   Hassan MF, 2015, ASIAPAC SIGN INFO PR, P1176, DOI 10.1109/APSIPA.2015.7415457
   Hu XH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356534
   Hunt R. W. G., 2005, The Reproduction of Colour
   Ichikawa A, 2004, IEEE SYS MAN CYBERN, P36
   Jefferson L, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1535
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   JUDD DB, 1966, P NATL ACAD SCI USA, V55, P1313, DOI 10.1073/pnas.55.6.1313
   Kuhn GR, 2008, IEEE T VIS COMPUT GR, V14, P1747, DOI 10.1109/TVCG.2008.112
   Laccarino G., 2006, WWW, P919
   Lau C, 2011, IEEE I CONF COMP VIS, P1172, DOI 10.1109/ICCV.2011.6126366
   Machado GM, 2010, COMPUT GRAPH FORUM, V29, P933, DOI 10.1111/j.1467-8659.2009.01701.x
   Machado GM, 2009, IEEE T VIS COMPUT GR, V15, P1291, DOI 10.1109/TVCG.2009.113
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   Rigden C, 1999, BRIT TELECOMMUN ENG, V17, P291
   Semary N.A., 2014, INT C ENG TECHN ICET, P1
   Sharpe L.T., 1999, COLOR VISION GENES P, P3
   Shen WY, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925878
   VIENOT F, 1995, NATURE, V376, P127, DOI 10.1038/376127a0
   Wakita Ken., 2005, Assets '05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, P158, DOI [10.1145/1090785.1090815, DOI 10.1145/1090785.1090815]
   Wang XY, 2021, COMPUT GRAPH-UK, V98, P19, DOI 10.1016/j.cag.2021.04.027
   Zhong F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356552
   Zhu ZY, 2022, IEEE T MULTIMEDIA, V24, P1721, DOI 10.1109/TMM.2021.3070108
   Zhu ZY, 2021, VISUAL COMPUT, V37, P2999, DOI 10.1007/s00371-021-02240-0
   Zhu ZY, 2019, SIGNAL PROCESS-IMAGE, V76, P68, DOI 10.1016/j.image.2019.04.004
   Zhu Z, 2019, VISUAL COMPUT, V35, P1053, DOI 10.1007/s00371-019-01689-4
NR 29
TC 2
Z9 2
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3405
EP 3418
DI 10.1007/s00371-022-02549-4
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000814039600001
OA Bronze
DA 2024-07-18
ER

PT J
AU Hawkins, B
   Ricks, B
AF Hawkins, Brian
   Ricks, Brian
TI Improving virtual pipes model of hydraulic and thermal erosion with
   vegetation considerations
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive simulation; Hydraulic erosion; Thermal erosion; GPU; Virtual
   pipes model
ID FLOOD MITIGATION; WATER
AB Current research in real-time water simulation can also calculate hydraulic erosion to a landscape; however, vegetation, which carries one of the biggest impacts on hydraulic erosion, is often not considered. We proposed an improvement upon the virtual pipes model for real-time hydraulic and thermal erosion simulation by incorporating considerations for vegetation cover. This method advances previous work by considering the limiting of soil erosion in vegetated areas due to soil binding, leaf cover, and transpiration from the plants. It also considers the impacts of vegetation death and the erosion and transportation of dead vegetation matter. Our algorithm creates a more realistic real-time erosion simulation, as seen by comparing erosion results with and without considering vegetation. Our results run in real time and clearly show the erosion effects expected by combining these methods.
C1 [Hawkins, Brian; Ricks, Brian] Peter Kiewit Inst, 1110 S 67th St, Omaha, NE 68182 USA.
RP Hawkins, B (corresponding author), Peter Kiewit Inst, 1110 S 67th St, Omaha, NE 68182 USA.
EM bjhawkins@unomaha.edu; bricks@unomaha.edu
RI Ricks, Brian/AAG-9889-2020
FU NSF [1718139]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1718139] Funding Source: National
   Science Foundation
FX This research was partially funded by the NSF 1718139.
CR Al-Hashemi HMB, 2018, POWDER TECHNOL, V330, P397, DOI 10.1016/j.powtec.2018.02.003
   Andersson J., 2007, SIGGRAPH 2007 COURSE, P3858
   Bene B., 2007, WORKSHOP VIRTUAL REA, DOI 10.2312/PE/vriphys/vriphys07/043-050
   Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Borgeat L, 2011, LECT NOTES COMPUT SC, V6891, P323, DOI 10.1007/978-3-642-23623-5_41
   Butt MJ, 2010, WATER RESOUR MANAG, V24, P3701, DOI 10.1007/s11269-010-9627-7
   CASULLI V, 1990, J COMPUT PHYS, V86, P56, DOI 10.1016/0021-9991(90)90091-E
   Chorin A.J., 1990, A Mathematical Introduction to Fluid Mechanics
   Cordonnier G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073667
   Crespin B, 2014, COMPUT GRAPH-UK, V45, P1, DOI 10.1016/j.cag.2014.07.001
   De Asis AM, 2007, ISPRS J PHOTOGRAMM, V62, P309, DOI 10.1016/j.isprsjprs.2007.05.013
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Jk B., 2011, EUROGRAPHICS SHORT P, P5760
   Kellomäki T, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/2700533
   Kellomäki T, 2014, INT J COMPUT GAMES T, V2014, DOI 10.1155/2014/580154
   Kellomki T., 2013, J WORLD SOC COMPUT G
   Mei X., 2007, 15 PACIFIC C COMPUTE, P4756
   Mould D, 1997, COMPUT GRAPH-UK, V21, P801, DOI 10.1016/S0097-8493(97)00059-9
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   OBrien J.F., 1995, P COMPUTER ANIMATION
   RENARD KG, 1991, J SOIL WATER CONSERV, V46, P30
   Stam J., 1999, P 26 ANN C COMPUTER
   Tasseff B, 2019, WATER RESOUR RES, V55, P1490, DOI 10.1029/2018WR024362
   Tasseff B, 2016, LECT NOTES COMPUT SC, V9676, P358, DOI 10.1007/978-3-319-33954-2_26
   tava O., 2008, P 2008 ACM SIGGRAPHE
   Wischmeier W.H., 1978, USDA AGR RES SERVICE, P57
NR 27
TC 0
Z9 0
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2835
EP 2846
DI 10.1007/s00371-022-02496-0
EA MAY 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000793634200003
DA 2024-07-18
ER

PT J
AU Kumar, AK
   Mai, NN
   Guo, SL
   Han, LN
AF Kumar, Amit Krishan
   Nguyen Ngoc Mai
   Guo, Shuli
   Han, Lina
TI Entanglement inspired approach for determining the preeminent
   arrangement of static cameras in a multi-view computer vision system
SO VISUAL COMPUTER
LA English
DT Article
DE Entanglement; Multi-view; Feature ranking; Hidden features;
   Classification; Static camera
ID RECOGNITION; NETWORK; IMAGE
AB This paper is on the concept of quantum steering and quantum entanglement of two observers. The concept is applied to a multi-view computer vision system that incorporates two cameras. Three separate multi-view static camera setups are used to compare the recognition accuracy and to improve the arrangement of cameras in gesture recognition and classroom organisation applications. Prominent features which are partially hidden from a viewpoint can increase performance if given attention. In view of that, this paper proposes an entanglement-based ranking technique that updates the weights of attentive features to improve the classification rate. Principles of entanglement are also used to optimise the position of cameras such that the authoritative hidden features are visible. The proposed technique has a high recognition rate with static cameras. It also shows a low error rate in the field of view when switching to other applications. The results are validated with derangement and Bland-Altman agreement test. The entanglement approach for determining the fine-tuned position of static cameras in a recognition task outperforms many active camera networks.
C1 [Kumar, Amit Krishan; Guo, Shuli] Sch Automat, Beijing Inst Technol, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China.
   [Nguyen Ngoc Mai] Nguyen Tat Thanh Univ, Dist 12, Ho Chi Minh City, Vietnam.
   [Han, Lina] Chinese Peoples Liberat Army Gen Hosp, Natl Clin Res Ctr Geriatr Dis, Dept Cardiovasc Internal Med, Nanlou Branch, Beijing, Peoples R China.
C3 Beijing Institute of Technology; Nguyen Tat Thanh University (NTTU);
   Chinese People's Liberation Army General Hospital
RP Kumar, AK (corresponding author), Sch Automat, Beijing Inst Technol, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China.; Mai, NN (corresponding author), Nguyen Tat Thanh Univ, Dist 12, Ho Chi Minh City, Vietnam.
EM fste_11@yahoo.com; nnmai@ntt.edu.vn; guoshuli@bit.edu.cn;
   2438381279@qq.com
RI Kumar, Amit Krishan/GRR-5642-2022; Han, Lina/ABG-9162-2020
OI Kumar, Amit Krishan/0000-0002-0173-2081; 
CR Abavisani M., 2019, IEEECVF C COMPUTER V
   Abdellali H., 2019, IEEECVF INT C COMPUT
   Abdulmunem A., 2019, HUMAN ACTION RECOGNI, DOI [10.1063/1.5123119, DOI 10.1063/1.5123119]
   Abura'ed N, 2017, ACM COMPUT SURV, V49, DOI 10.1145/3009965
   Ahn JW, 2016, SCI PROGRAMMING-NETH, V2016, DOI 10.1155/2016/4801784
   An L, 2012, 2012 SIXTH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS (ICDSC)
   [Anonymous], 2011, Res Lett Inf Math Sci
   Azad R, 2019, IEEE T CIRC SYST VID, V29, P1729, DOI 10.1109/TCSVT.2018.2855416
   Bae J, 2020, NPJ QUANTUM INFORM, V6, DOI 10.1038/s41534-020-0242-z
   Barbhuiya AA, 2021, MULTIMED TOOLS APPL, V80, P3051, DOI 10.1007/s11042-020-09829-y
   Bayoudh K, 2021, APPL INTELL, V51, P124, DOI 10.1007/s10489-020-01801-5
   Bilgin M., 2019, AM SIGN LANGUAGE CHA, P16
   Bouyagoub S, 2010, IEEE IMAGE PROC, P681, DOI 10.1109/ICIP.2010.5649559
   Cagnoni S., 2009, ENCY ARTIFICIAL INTE, P848, DOI [10.4018/978-1-59904-849-9.ch125, DOI 10.4018/978-1-59904-849-9.CH125]
   Chakraborty S, 2020, INT J THEOR PHYS, V59, P3348, DOI 10.1007/s10773-020-04590-2
   Chen CH, 2017, HUMAN RECOGNITION IN UNCONSTRAINED ENVIRONMENTS: USING COMPUTER VISION, PATTERN RECOGNITION AND MACHINE LEARNING METHODS FOR BIOMETRICS, P31, DOI 10.1016/B978-0-08-100705-1.00002-6
   Das S., 2019, IEEECVF INT C COMPUT
   Das Srijan, 2020, COMPUTER VISIONECCV, P72
   Deng DL, 2017, PHYS REV X, V7, DOI 10.1103/PhysRevX.7.021021
   Tran DS, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020722
   Donahue J., 2015, IEEE CVPR
   Eichhardt I., 2019, BMVC
   Fu Y., 2018, LEARNING REPRESENTAT
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Gao F, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0206038
   Hamdi A., 2020, ARXIV ABS201113244
   Ruiz AH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1087, DOI 10.1145/3123266.3123299
   Hussein, 2013, INT JOINT C ART INT
   Iliyasu AM, 2013, ENTROPY-SWITZ, V15, P2874, DOI 10.3390/e15082874
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Jana Biswapati, 2016, International Journal of Network Security, V18, P633
   Jang HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092743
   Jankowski N, 2011, LECT NOTES COMPUT SC, V7063, P238, DOI 10.1007/978-3-642-24958-7_28
   Kertsz G., 2015, IEEE 10 JUB INT S AP, P237241
   Kpkl O., 2019, 14 IEEE INT C AUT FA, P18
   Kumar A, 2016, LECT NOTES COMPUT SC, V10036, P128, DOI 10.1007/978-3-319-51969-2_11
   Kumar AK, 2020, IET IMAGE PROCESS, V14, P4606, DOI 10.1049/iet-ipr.2019.1458
   Li HS, 2014, PROCEEDINGS OF 2014 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN INFORMATICS AND COMPUTING (PIC), P237, DOI 10.1109/PIC.2014.6972332
   Lin WY, 2014, IEEE T CIRC SYST VID, V24, P826, DOI 10.1109/TCSVT.2013.2280849
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Lu XJ, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20080577
   Medioni G, 2009, IEEE T SYST MAN CY A, V39, P12, DOI 10.1109/TSMCA.2008.2007979
   Molchanov P., 2016, IEEE C COMPUTER VISI
   Molchanov P., 2015, IEEE T COMPUTER VISI, P17
   Munishwar VP, 2013, ACM T SENSOR NETWORK, V9, DOI 10.1145/2489253.2489262
   Papadopoulos K, 2021, INT C PATT RECOG, P452, DOI 10.1109/ICPR48806.2021.9413189
   Poon G, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P19, DOI 10.1109/SIPROCESS.2018.8600529
   Remelli E., 2020, IEEECVF C COMPUTER V
   Schuld M, 2014, LECT NOTES ARTIF INT, V8862, P208, DOI 10.1007/978-3-319-13560-1_17
   Sharma M., 2013, LECT NOTES COMPUTER, V7727, DOI [10.1007/978-3-642-37447-0_41, DOI 10.1007/978-3-642-37447-0_41]
   Song S., 2015, IEEE C COMPUTER VISI
   Song X.-H., 2014, J. Inf. Hiding Multimed. Signal Process, V5, P574
   Tao ZQ, 2020, IEEE T NEUR NET LEAR, V31, P600, DOI 10.1109/TNNLS.2019.2906867
   Tran D., 2015, IEEE INT C COMPUTER
   Vyas KK., 2013, INT J RECENT INNOVAT, V1, P632
   Wang L., 2019, IEEECVF INT C COMPUT
   Wang P., 2016, P 24 ACM INT C MULTI
   Xu YD, 2018, INT C PATT RECOG, P1833, DOI 10.1109/ICPR.2018.8545058
   Yin F, 2015, IET COMPUT VIS, V9, P354, DOI 10.1049/iet-cvi.2013.0301
   You H., 2018, P 26 ACM INT C MULTI
   Yuan SZ, 2019, INT J THEOR PHYS, V58, P2823, DOI 10.1007/s10773-019-04166-9
   Zhang C, 2013, IEEE ENG MED BIO, P3319, DOI 10.1109/EMBC.2013.6610251
   Zhang Y, 2013, QUANTUM INF PROCESS, V12, P2833, DOI 10.1007/s11128-013-0567-z
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhao J, 2008, IEEE J-STSP, V2, P464, DOI 10.1109/JSTSP.2008.2001430
   Zhou L.M., 2014, INT C DIG IM COMP TE, P18
   Zhou YJ, 2021, VISUAL COMPUT, V37, P2009, DOI 10.1007/s00371-020-01959-6
NR 67
TC 7
Z9 7
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2847
EP 2863
DI 10.1007/s00371-022-02497-z
EA MAY 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000791874100002
DA 2024-07-18
ER

PT J
AU Soroush, R
   Baleghi, Y
AF Soroush, Rahman
   Baleghi, Yasser
TI NIR/RGB image fusion for scene classification using deep neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural networks; Transfer learning; Image fusion; Infrared
   and visible images; Scene recognition
ID INFRARED IMAGES; FACE RECOGNITION; VISIBLE IMAGES; LIGHT
AB Near-infrared (NIR) imaging can add very useful data to many visible range image processing applications. In this paper, new fusion techniques are proposed to benefit from the data of both NIR/RGB sensors for the application of scene recognition and classification. Scene recognition and classification is an important and challenging branch of computer vision. Also, image fusion is a very well-known method in image processing. In this paper, fusion of RGB and NIR images is applied to improve the performance of scene classification. The proposed fusion technique is based on modified visual salient points (MVSP). Within this process, each RGB channel is fused with NIR channel based on their visual saliency maps. The fusion parameter space is then searched for the best fusion, based on an error function that is defined to measure the degree of conformity of the fused image with the input channels. This error function is defined on the basis of the distance between pixel intensities of NIR channel and fused image. On the contrary, the gradient difference is applied for the distance between RGB channels and fused image in the error function. After finding the most consent fused image, it is applied to deep convolutional neural networks (DCNNs) to perform scene category classification using the transfer learning method. The experimental results show that the proposed method in the scene classification outperforms the prior works. The results thus illustrate how an optimized NIR/RGB fusion can bring about better classification outcomes.
C1 [Soroush, Rahman; Baleghi, Yasser] Babol Noshirvani Univ Technol, Elect & Comp Engn Dept, Babol, Iran.
C3 Babol Noshirvani University of Technology
RP Baleghi, Y (corresponding author), Babol Noshirvani Univ Technol, Elect & Comp Engn Dept, Babol, Iran.
EM eng.soroush322@yahoo.com; y.baleghi@nit.ac.ir
FU Babol Noshirvani University of Technology [BNUT/370123/99]
FX The authors acknowledge the funding support of Babol Noshirvani
   University of Technology through Grant program No. BNUT/370123/99.
CR Alhichri H, 2021, IEEE ACCESS, V9, P14078, DOI 10.1109/ACCESS.2021.3051085
   Basu A, 2020, PROCEDIA COMPUT SCI, V167, P440, DOI 10.1016/j.procs.2020.03.253
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Bavirisetti DP, 2016, INFRARED PHYS TECHN, V76, P52, DOI 10.1016/j.infrared.2016.01.009
   Bayat A, 2018, IEEE COMPUT SOC CONF, P2073, DOI 10.1109/CVPRW.2018.00268
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bebis G, 2006, IMAGE VISION COMPUT, V24, P727, DOI 10.1016/j.imavis.2006.01.017
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517
   Brown M., 2011, CVPR 2011
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chen C, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE), P573, DOI 10.1109/ICMCCE.2018.00126
   Choe G, 2018, IEEE ROBOT AUTOM LET, V3, P1808, DOI 10.1109/LRA.2018.2801390
   Choi M, 2005, IEEE GEOSCI REMOTE S, V2, P136, DOI 10.1109/LGRS.2005.845313
   Farahzadeh E., 2014, TOOLS VISUAL SCENE R
   Farahzadeh E, 2015, SIGNAL IMAGE VIDEO P, V9, P1935, DOI 10.1007/s11760-014-0687-7
   Fei-Fei L., 2005, 2005 IEEE COMP VIS P
   Ghazali SM., 2019, J AI DATA MIN, V7, P1, DOI DOI 10.22044/JADM.2018.5742.1696
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Jhuo I.-H., 2010, 2010 20 INT C PATT R
   Jiang JH, 2019, IEEE ACCESS, V7, P20607, DOI 10.1109/ACCESS.2019.2896128
   Jingu, 2004, 2004 C COMP VIS PATT
   Kakooei M, 2020, EARTH SCI INFORM, V13, P459, DOI 10.1007/s12145-020-00449-6
   Khan A, 2021, NEUROCOMPUTING, V440, P111, DOI 10.1016/j.neucom.2021.01.085
   Khan MJ, 2018, IEEE ACCESS, V6, P14118, DOI 10.1109/ACCESS.2018.2812999
   Khan SH, 2016, IEEE T IMAGE PROCESS, V25, P3372, DOI 10.1109/TIP.2016.2567076
   Kong SG, 2005, COMPUT VIS IMAGE UND, V97, P103, DOI 10.1016/j.cviu.2004.04.001
   Kong WW, 2014, INFRARED PHYS TECHN, V65, P103, DOI 10.1016/j.infrared.2014.04.003
   Kumar WK, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01210-9
   Kwitt R., 2012, COMPUTER VISIONECCV
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li L.-j., 2010, NIPS
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ST, 2012, IEEE T BIO-MED ENG, V59, P3450, DOI 10.1109/TBME.2012.2217493
   Li ST, 2011, INFORM FUSION, V12, P74, DOI 10.1016/j.inffus.2010.03.002
   Li X., 2012, BMVC
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Liu J., 2007, 2007 IEEE 11 INT C C
   Liu J., 2009, 2009 IEEE C COMPUTER
   Liu YP, 2014, SIGNAL PROCESS, V97, P9, DOI 10.1016/j.sigpro.2013.10.010
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   López-Cifuentes A, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107256
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Mitchell HB, 2010, IMAGE FUSION: THEORIES, TECHNIQUES AND APPLICATIONS, P1, DOI 10.1007/978-3-642-11216-4
   Mou J, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1046, DOI 10.1109/CISP.2013.6745210
   Najafi M, 2020, 2020 6TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS), DOI 10.1109/ICSPIS51611.2020.9349599
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Omri F, 2014, LECT NOTES COMPUT SC, V8509, P549, DOI 10.1007/978-3-319-07998-1_63
   Pajares G, 2004, PATTERN RECOGN, V37, P1855, DOI 10.1016/j.patcog.2004.03.010
   Pandey M., 2011, 2011 INT C COMPUTER
   Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001
   Patterson G., 2012, 2012 IEEE C COMPUTER
   Quattoni A., 2009, 2009 IEEE C COMPUTER
   Quelhas P., 2005, 10 IEEE INT C COMPUT, V1
   Qun L., 2012, J CHINA U POSTS TELE, V19, P166, DOI [10.1016/S1005-8885(11)60426-3, DOI 10.1016/S1005-8885(11)60426-3]
   Rajkumar S, 2014, ADV INTELL SYST, V248, P93, DOI 10.1007/978-3-319-03107-1_11
   Reichl P, 2018, 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT RAIL TRANSPORTATION (ICIRT)
   Ren L, 2021, INFRARED PHYS TECHN, V114, DOI 10.1016/j.infrared.2021.103662
   Ren YZ, 2017, J VIS COMMUN IMAGE R, V42, P192, DOI 10.1016/j.jvcir.2016.11.004
   Saurabh S., 2004, P SPIE
   Shamsafar F, 2014, MACH VISION APPL, V25, P881, DOI 10.1007/s00138-013-0572-3
   Shojaiee F, 2022, OPTIK, V254, DOI 10.1016/j.ijleo.2022.168688
   Simonyan K., 2014, COMPUT VIS PAT RECOG
   Singh R, 2008, PATTERN RECOGN, V41, P880, DOI 10.1016/j.patcog.2007.06.022
   Singh S., 2012, EUROPEAN C COMPUTER
   Sun H, 2018, IEEE INT CONF ROBOT, P5875
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tighe J., 2011, 2011 INT C COMPUTER
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Wang C, 2020, INFORM FUSION, V63, P1, DOI 10.1016/j.inffus.2020.05.005
   Wang J, 2014, INFRARED PHYS TECHN, V67, P477, DOI 10.1016/j.infrared.2014.09.019
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xiang TZ, 2015, INFRARED PHYS TECHN, V69, P53, DOI 10.1016/j.infrared.2015.01.002
   Xie L, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107205
   Xie L, 2018, PATTERN RECOGN, V82, P118, DOI 10.1016/j.patcog.2018.04.025
   Yang CL, 2019, VISUAL COMPUT, V35, P473, DOI 10.1007/s00371-018-1475-0
   Zatout C, 2022, VISUAL COMPUT, V38, P2691, DOI 10.1007/s00371-021-02147-w
   Zhang XY, 2017, J OPT SOC AM A, V34, P1400, DOI 10.1364/JOSAA.34.001400
   Zhang Z, 1999, P IEEE, V87, P1315, DOI 10.1109/5.775414
   Zhao JF, 2017, INFRARED PHYS TECHN, V81, P201, DOI 10.1016/j.infrared.2017.01.012
   Zhao JF, 2014, INFRARED PHYS TECHN, V62, P86, DOI 10.1016/j.infrared.2013.11.008
   Zheng Y., 2011, Image fusion and its applications
   Zhou Y, 2011, RES NONDESTRUCT EVAL, V22, P76, DOI 10.1080/09349847.2011.553348
NR 91
TC 13
Z9 13
U1 7
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2725
EP 2739
DI 10.1007/s00371-022-02488-0
EA MAY 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000790216200002
DA 2024-07-18
ER

PT J
AU Yang, HS
   Fan, YY
   Lv, GY
   Liu, SY
   Guo, Z
AF Yang, Hansen
   Fan, Yangyu
   Lv, Guoyun
   Liu, Shiya
   Guo, Zhe
TI Exploiting emotional concepts for image emotion recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Image emotion recognition; Visual-semantic embedding; Knowledge graph;
   Emotion perception
AB With the increasing number of users express their emotions via images on social media, image emotion recognition attracts much attention of researchers. Different from conventional computer vision tasks, image emotion recognition is inherently more challenging for the ambiguity and subjectivity of emotion. Existing methods are limited to learn a direct mapping from image feature to emotion. However, emotion cognition mechanism in psychology demonstrates that human beings perceive emotion in a stepwise way. Therefore, we propose a novel image emotion recognition method that leverages emotional concepts as intermediary to bridge image and emotion. Specifically, we organize the relationship between concept and emotion in the form of knowledge graph. The relation between image and emotion is explored in the semantic embedding space where the knowledge is encoded into. Then, based on the hierarchical relation of emotions, we propose a multi-task learning deep model to recognize image emotion from visual perspective. Finally, a fusion strategy is proposed to merge the results of both visual-semantic stream and visual stream. Extensive experimental results show that our method outperforms state-of-the-art methods on two public image emotion datasets.
C1 [Yang, Hansen; Fan, Yangyu; Lv, Guoyun; Guo, Zhe] Northwestern Polytech Univ, Sch Elect & Informat, Xian, Peoples R China.
   [Liu, Shiya] Content Prod Ctr Virtual Real, Beijing, Peoples R China.
C3 Northwestern Polytechnical University
RP Lv, GY (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian, Peoples R China.
EM mikeyhs@126.com; fan_yangyu@nwpu.edu.cn; lvguoyun102@163.com;
   shiya.liu@cpcvr.org.cn; guozhe@nwpu.edu.cn
FU National Natural Science Foundation of China [62071384]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62071384.
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Ali AR, 2017, IEEE WINT CONF APPL, P679, DOI 10.1109/WACV.2017.81
   [Anonymous], 2014, Comput. Sci.
   Argyriou A., 2006, Advances in Neural Information Processing Systems, V19, P1
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Dellandrea Emmanuel, 2010, P INT WORKSHOP CONTE, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Goi MT, 2018, J MARK HIGH EDUC, V28, P90, DOI 10.1080/08841241.2018.1425231
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He XY, 2018, NEUROCOMPUTING, V291, P187, DOI 10.1016/j.neucom.2018.02.073
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Huang Y., 2017, IEEE T PATTERN ANAL, V42
   Kao YY, 2016, SIGNAL PROCESS-IMAGE, V47, P500, DOI 10.1016/j.image.2016.05.004
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LD, 2019, IEEE INT CON MULTI, P430, DOI 10.1109/ICME.2019.00081
   Li ZH, 2018, MULTIMED TOOLS APPL, V77, P1115, DOI 10.1007/s11042-016-4310-5
   Lim L, 2020, IEEE IMAGE PROC, P1886, DOI 10.1109/ICIP40778.2020.9191258
   Lin Honghuang., 2014, Proceedings of the 51st Design Automation Conference, P1, DOI [10.1109/ICME.2014.6890213, DOI 10.1109/ICME.2014.6890213]
   Lin LJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P847
   Liu X, 2019, J VIS COMMUN IMAGE R, V58, P576, DOI 10.1016/j.jvcir.2018.12.032
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mehrabian A., 1974, APPROACH ENV PSYCHOL, P222, DOI DOI 10.1016/J.ELERAP.2013.07.001
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Oliveira W. B. D., 2020, ACM T INFORM SYST, V38, P1, DOI DOI 10.1145/3385186
   Ortis A, 2021, MULTIMED TOOLS APPL, V80, P22323, DOI 10.1007/s11042-019-08312-7
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2016, IEEE IMAGE PROC, P634, DOI 10.1109/ICIP.2016.7532434
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song KK, 2018, NEUROCOMPUTING, V312, P218, DOI 10.1016/j.neucom.2018.05.104
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang PC, 2018, KNOWL-BASED SYST, V158, P43, DOI 10.1016/j.knosys.2018.05.029
   Wu, 2019, UNIVSE ROBUST VISUAL
   Xiong HT, 2019, AAAI CONF ARTIF INTE, P363
   Yamamoto T, 2021, IEICE T INF SYST, VE104D, P1691, DOI 10.1587/transinf.2020EDP7218
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zhang J, 2020, KNOWL-BASED SYST, V191, DOI 10.1016/j.knosys.2019.105245
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
NR 51
TC 10
Z9 10
U1 8
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2177
EP 2190
DI 10.1007/s00371-022-02472-8
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000786565700001
DA 2024-07-18
ER

PT J
AU Liang, PW
   Dong, PW
   Wang, F
   Ma, P
   Bai, JJ
   Wang, B
   Li, CY
AF Liang, Pengwei
   Dong, Pengwei
   Wang, Fan
   Ma, Peng
   Bai, Jiajing
   Wang, Bo
   Li, Chongyi
TI Learning to remove sandstorm for image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Sandstorm image enhancement; Color correction; Unsupervised learning
AB In the dusty weather, the suspended sand particles in the air lead to various degrees of degradation for the captured images. Commonly, severe color deviations, low contrast, and blurring details arise in the images. In this paper, a method equipped with color correction and an unsupervised learning network is proposed to enhance sandstorm images. First, the color correction is carried out by color compensation followed by the radical histogram equalization which enables the recovery of the natural color of an image and relieves the pressure of subsequent network that focuses on the enhancement of details. Second, the three sub-networks including atmospheric light with accurate initialization, transmission map, and clear image estimation layers are introduced to remove haze-like effects and reconstruct the clear image by unsupervised learning, which solves the lacking of paired sandstorm images and clear counterparts. To perform a comprehensive study of the state-of-the-art sandstorm image enhancement methods, we propose a Sandstorm Image Enhancement (SIE) dataset which can benchmark the performance of different methods and make it possible to train sandstorm image enhancement networks. The experimental results demonstrate that our method outperforms several state-of-the-arts both qualitatively and quantitatively.
C1 [Liang, Pengwei; Dong, Pengwei; Wang, Fan; Ma, Peng; Bai, Jiajing; Wang, Bo] Ningxia Univ, Sch Phys & Elect Elect Engn, Yinchuan, Ningxia, Peoples R China.
   [Li, Chongyi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Ningxia University; Nanyang Technological University
RP Wang, B (corresponding author), Ningxia Univ, Sch Phys & Elect Elect Engn, Yinchuan, Ningxia, Peoples R China.
EM tjuwb@nxu.edu.cn
RI Liang, Pengwei/JXM-4365-2024; Wang, Bo/D-9351-2013
FU higher education scientific research project of Ningxia [NGY2017009]
FX This research was supported by higher education scientific research
   project of Ningxia (Grant NGY2017009).
CR Al-Ameen Zohair, 2016, International Journal of Intelligent Systems and Applications, V8, P10, DOI 10.5815/ijisa.2016.08.02
   Al-Shakarji N.M., 2017, P APPL IMAGERY PATTE, P1, DOI [10.1109/AIPR.2017.8457935, DOI 10.1109/AIPR.2017.8457935]
   Barnard K, 2000, LECT NOTES COMPUT SC, V1842, P390
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   BRAINARD DH, 1986, J OPT SOC AM A, V3, P1651, DOI 10.1364/JOSAA.3.001651
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen YZ, 2016, OPTIK, V127, P517, DOI 10.1016/j.ijleo.2015.10.161
   Cheng YQ, 2020, IEEE ACCESS, V8, P66931, DOI 10.1109/ACCESS.2020.2985869
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu XY, 2014, IEEE INT WORKSH MULT
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Gao GX, 2020, IEEE PHOTONICS J, V12, DOI 10.1109/JPHOT.2020.2975833
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huang SC, 2015, IEEE T IND ELECTRON, V62, P2962, DOI 10.1109/TIE.2014.2364798
   Jian Wang, 2016, MultiMedia Modeling. 22nd International Conference, MMM 2016. Proceedings, P842, DOI 10.1007/978-3-319-27671-7_70
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kenk Mourad A., 2020, 2020 2nd Novel Intelligent and Leading Emerging Sciences Conference (NILES), P213, DOI 10.1109/NILES50944.2020.9257952
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Long MZ, 2018, IEEE T BIOMED CIRC S, V12, P993, DOI 10.1109/TBCAS.2018.2869530
   Manzanilla A, 2019, IEEE ROBOT AUTOM LET, V4, P1351, DOI 10.1109/LRA.2019.2895272
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   [潘海明 Pan Haiming], 2018, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V30, P992
   Park TH, 2021, IEEE ACCESS, V9, P19749, DOI 10.1109/ACCESS.2021.3054899
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi ZH, 2020, IET IMAGE PROCESS, V14, P747, DOI 10.1049/iet-ipr.2019.0992
   Shi ZH, 2019, IEEE ACCESS, V7, P116722, DOI 10.1109/ACCESS.2019.2936444
   Ting Yan, 2014, Journal of Software, V9, P2672, DOI 10.4304/jsw.9.10.2672-2677
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang B, 2021, SIGNAL IMAGE VIDEO P, V15, P637, DOI 10.1007/s11760-020-01786-1
   Wang Y, 2018, IEEE T CIRCUITS-I, V65, P992, DOI 10.1109/TCSI.2017.2751671
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang YH, 2019, IEEE ACCESS, V7, P10224, DOI [10.1109/ACCESS.2019.2891065, 10.1002/suco.201800261]
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2019, SPRINGER SER GEOMECH, P39, DOI 10.1007/978-981-10-7560-5_4
   Xiao L, 2018, IEEE T IMAGE PROCESS, V27, P4091, DOI 10.1109/TIP.2018.2831925
   Yu SY, 2016, J MOD OPTIC, V63, P2121, DOI 10.1080/09500340.2016.1184340
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
   [智宁 Zhi Ning], 2016, [中国图象图形学报, Journal of Image and Graphics], V21, P1585
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
NR 56
TC 10
Z9 10
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1829
EP 1852
DI 10.1007/s00371-022-02448-8
EA APR 2022
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781196500001
DA 2024-07-18
ER

PT J
AU Chen, SH
   Yu, JH
   Xu, XQ
   Chen, ZY
   Lu, L
   Hu, XL
   Yang, YQ
AF Chen, Shuhan
   Yu, Jinhao
   Xu, Xiuqi
   Chen, Zeyu
   Lu, Lu
   Hu, Xuelong
   Yang, Yuequan
TI Split-guidance network for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Split-guidance convolution; Unified decoder
AB Due to the large-scale variation in objects in practical scenes, multi-scale representation is of critical importance for salient object detection (SOD). Recent advances in multi-level feature fusion also demonstrate its contribution in consistent performance gains. Different from the existing layer-wise methods, we propose a simple yet efficient split-guidance convolution block to improve the multi-scale representation ability at a granular level in this paper. Specifically, the input feature is first split into different subsets; each of them is guided by all the subsets in front of it, in this way to increase the range of receptive fields for each network layer. By embedding it into each side-output stage of the encoder, we build a unified decoder for both RGB SOD and RGB-D SOD. Experimental results on five RGB datasets, five RGB-D datasets and three RGB-T datasets demonstrate that the proposed method without any attention mechanisms and other complex designs performs favorably against state-of-the-art approaches and also shows advantages in simplicity, efficiency and compactness.
C1 [Chen, Shuhan; Yu, Jinhao; Xu, Xiuqi; Chen, Zeyu; Lu, Lu; Hu, Xuelong; Yang, Yuequan] Yangzhou Univ, Sch Informat Engn, Yangzhou, Jiangsu, Peoples R China.
C3 Yangzhou University
RP Chen, SH (corresponding author), Yangzhou Univ, Sch Informat Engn, Yangzhou, Jiangsu, Peoples R China.
EM c.shuhan@gmail.com; yujinhao2020@foxmail.com; frequency.xu@foxmail.com;
   Czy1239858139@163.com; lulu0514@foxmail.com; xlhu@yzu.edu.cn;
   yang@yzu.edu.cn
FU Natural Science Foundation of China [61802336, 61806175, 62073322];
   Yangzhou University "Qinglan Project
FX This work is partially supported by the Natural Science Foundation of
   China (No. 61802336, No. 61806175, No. 62073322) and Yangzhou University
   "Qinglan Project."
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bovik A.C., 2020, ARXIV200514354
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Ding E., 2020, HS RESNET HIERARCHIC
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Li C, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P359
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Nguyen TV, 2019, IEEE T IMAGE PROCESS, V28, P3130, DOI 10.1109/TIP.2019.2894284
   Nguyen TV, 2018, INT J COMPUT VISION, V126, P86, DOI 10.1007/s11263-017-1042-6
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, 14091556 ARXIV
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Tu Z, 2021, IEEE OPEN J SIGNAL P, V2, P425, DOI 10.1109/OJSP.2021.3090333
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1541
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
NR 78
TC 5
Z9 5
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1437
EP 1451
DI 10.1007/s00371-022-02421-5
EA FEB 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000758950400001
DA 2024-07-18
ER

PT J
AU Guo, S
   Wang, W
   Wang, X
   Xu, X
AF Guo, Sheng
   Wang, Wei
   Wang, Xiao
   Xu, Xin
TI Low-light image enhancement with joint illumination and noise data
   distribution transformation
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light image enhancement; Generative adversarial network; Data
   distribution transformation
AB Images captured in low-light environments often face two problems: low contrast and high noise, which are caused by the low number of photons in the environment. Existing low-light image enhancement methods mainly focus on the previous problem to increase visibility while the latter one is usually addressed with a post-processing module. However, there is a coupling relationship between illumination and noise, and ignoring it will result in under-/over-smoothing of the enhanced images. To solve this problem, we propose a novel low-light image enhancement method based on simultaneous adjustment on illumination and noise using unpaired data. In other words, we consider illumination and noise as a joint data distribution. The proposed method consists of two main branches: a Distribution Extraction branch which is used to extract the joint distribution of illumination and noise in normal-light images, and a Distribution Transformation branch which transforms the low-light images in a spatial domain through the joint distribution. Extensive experiment results show that the proposed model can reach the network capability that trained with rich paired data and achieved satisfactory results.
C1 [Guo, Sheng; Wang, Wei; Wang, Xiao; Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.
   [Wang, Wei; Wang, Xiao; Xu, Xin] Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan 430065, Peoples R China.
C3 Wuhan University of Science & Technology
RP Wang, W (corresponding author), Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.; Wang, W (corresponding author), Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan 430065, Peoples R China.
EM wangwei8@wust.edu.cn
RI Xu, Xin/JRW-5800-2023
OI WANG, WEI/0000-0002-3733-3939
FU Natural Science Foundation of China [U1803262, 61602349, 61440016]
FX This work was supported by the Natural Science Foundation of China
   (U1803262, 61602349, 61440016).
CR Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   CHAN LC, 1983, IEEE T AERO ELEC SYS, V19, P71, DOI 10.1109/TAES.1983.309421
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hensel M, 2017, ADV NEUR IN, V30
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kurt M, 2021, VISUAL COMPUT, V37, P307, DOI 10.1007/s00371-020-01800-0
   Kwon D., 2020, ARXIV PREPRINT ARXIV
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li M., 2021, Vis. Comput
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2019, ARXIV190800682
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Radford A., 2015, ARXIV151106434
   Rahman Z, 2021, VISUAL COMPUT, V37, P865, DOI 10.1007/s00371-020-01838-0
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wang JY, 2019, IEEE INT CON MULTI, P1186, DOI 10.1109/ICME.2019.00207
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283
   Xiong Wei, 2020, arXiv
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yang KF, 2020, IEEE T IMAGE PROCESS, V29, P1493, DOI 10.1109/TIP.2019.2938310
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zhang XD, 2012, INT C PATT RECOG, P2034
   Zhang Y., 2020, arXiv
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhu AQ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102962
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zibo Meng, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P327, DOI 10.1007/978-3-030-67070-2_20
NR 56
TC 10
Z9 10
U1 14
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1363
EP 1374
DI 10.1007/s00371-022-02412-6
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000752807000002
DA 2024-07-18
ER

PT J
AU Prakash, SJ
   Mahalakshmi, K
AF Prakash, S. Jaya
   Mahalakshmi, K.
TI Improved reversible data hiding scheme employing dual image-based least
   significant bit matching for secure image communication using style
   transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Reversible data hiding; Improved least significant bit technique; Mosaic
   image security; Style transfer; Steganography
ID PREDICTION-ERROR; OPTIMIZATION; ALGORITHM; STEGANOGRAPHY; LOAD
AB The increasing use of multimedia applications often makes the information prone to leakage and makes the user's privacy at risk. The information hiding technique is mainly used to secure multimedia communication/information by modifying the secret information in a way that makes it useless for an unintended user. In this way, the secret information is secured from unauthorized access. To create a mosaic image, initially, a style transfer is applied using the Convolutional Neural Network (CNN) architecture. The style transfer alters the style of an image using another image still preserving the original content. The CNN'S optimization problem is to optimize the content loss, style loss, and total variation loss done using an Archimedes Optimization Algorithm (AOA). The AOA algorithm also alters the block in the target image by selecting an appropriate tile in the secret image. The tile fitting information is randomly inserted into the selected pixels using an improved dual image-based LSB matching (DILSBM) scheme. The DILSBM scheme can retrieve both the secret and target images from the mosaic image by offering a high PSNR value. The proposed DILSBM and CNN optimized AOA architecture is primarily intended to provide high capacity, improved visual quality, and solve the source image selection problem. The feasibility of the proposed technique can be observed from the experimental analysis where it is analyzed using different performance metrics such as Embedding Rate, Structural Similarity Method (SSIM), and peak signal-to-noise ratio (PSNR).
C1 [Prakash, S. Jaya] Anna Univ, Dept Informat & Commun, Chennai, Tamil Nadu, India.
   [Prakash, S. Jaya] Idhaya Engn Coll Women, Dept Comp Sci & Engn, Chinnasalem, Tamil Nadu, India.
   [Mahalakshmi, K.] KalaignarKarunanidhi Inst Technol, Dept Comp Sci & Engn, Coimbatore, Tamil Nadu, India.
C3 Anna University; Anna University Chennai
RP Prakash, SJ (corresponding author), Anna Univ, Dept Informat & Commun, Chennai, Tamil Nadu, India.; Prakash, SJ (corresponding author), Idhaya Engn Coll Women, Dept Comp Sci & Engn, Chinnasalem, Tamil Nadu, India.
EM sjayaprakash.id@gmail.com
CR Ahuja S, 2021, APPL INTELL, V51, P571, DOI 10.1007/s10489-020-01826-w
   Alam MMG, 2019, KNOWL INF SYST, V60, P971, DOI 10.1007/s10115-018-1263-1
   Alam MMG, 2019, SOFT COMPUT, V23, P1079, DOI 10.1007/s00500-018-3124-y
   Albawi S, 2017, I C ENG TECHNOL
   Ali ZM, 2021, AIN SHAMS ENG J, V12, P1923, DOI 10.1016/j.asej.2020.12.006
   Arulpandy P., 2020, COMPUT ASSIST METHOD, V27, P221
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Chen YC, 2019, IEEE T INF FOREN SEC, V14, P3332, DOI 10.1109/TIFS.2019.2914557
   Douglas M, 2018, MULTIMED TOOLS APPL, V77, P17333, DOI 10.1007/s11042-017-5308-3
   Dudul SV, 2021, TURK J COMPUT MATH E, V12, P4478
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gowthul Alam M. M., 2017, International Journal of Business Intelligence and Data Mining, V12, P299
   Green RC, 2012, EXPERT SYST APPL, V39, P555, DOI 10.1016/j.eswa.2011.07.046
   Gupta S., 2012, Int. J. Mod. Educ. Comput. Sci., V4, P27
   Hashim FA, 2021, APPL INTELL, V51, P1531, DOI 10.1007/s10489-020-01893-z
   Hassan BA, 2021, NEURAL COMPUT APPL, V33, P7011, DOI 10.1007/s00521-020-05474-6
   Hassan BA, 2020, DATA BRIEF, V28, DOI 10.1016/j.dib.2019.105046
   He WG, 2021, IEEE T IMAGE PROCESS, V30, P5045, DOI 10.1109/TIP.2021.3078088
   Hou JC, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116118
   Johnson NF., 2001, INFORM HIDING STEGAN, DOI [10.1007/978-1-4615-4375-6, DOI 10.1007/978-1-4615-4375-6]
   Lai IJ, 2011, IEEE T INF FOREN SEC, V6, P936, DOI 10.1109/TIFS.2011.2135853
   Li L, 2021, SIGNAL PROCESS, V181, DOI 10.1016/j.sigpro.2020.107920
   Li S, 2020, IEEE ACCESS, V8, P214732, DOI 10.1109/ACCESS.2020.3040048
   Lu TC, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23050577
   Mishra A, 2014, EXPERT SYST APPL, V41, P7858, DOI 10.1016/j.eswa.2014.06.011
   Omiotek Z, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020500
   Pradhan M, 2016, INT J ELEC POWER, V83, P325, DOI 10.1016/j.ijepes.2016.04.034
   Rejeesh MR, 2019, MULTIMED TOOLS APPL, V78, P22691, DOI 10.1007/s11042-019-7577-5
   Sahu AK, 2019, 3D RES, V10, DOI 10.1007/s13319-018-0211-x
   Sheidani S, 2021, IEEE T INF FOREN SEC, V16, P3647, DOI 10.1109/TIFS.2021.3080497
   Sundararaj, 2016, INT J INTELL ENG SYS, V9, P117, DOI [10.22266/ijies2016.0930.12, DOI 10.22266/IJIES2016.0930.12]
   Sundararaj V, 2020, PROG PHOTOVOLTAICS, V28, P1128, DOI 10.1002/pip.3315
   Sundararaj V, 2019, INT J BIOMED ENG TEC, V31, P325, DOI 10.1504/IJBET.2019.103242
   Sundararaj V, 2019, WIRELESS PERS COMMUN, V104, P173, DOI 10.1007/s11277-018-6014-9
   Sundararaj V, 2018, COMPUT SECUR, V77, P277, DOI 10.1016/j.cose.2018.04.009
   Swain G., 2019, J KING SAUD UNIV-COM, V31, P275
   Yang LN, 2020, IEEE ACCESS, V8, P108579, DOI 10.1109/ACCESS.2020.3000993
   Yao H, 2017, J VIS COMMUN IMAGE R, V43, P152, DOI 10.1016/j.jvcir.2017.01.004
   Yin JJ, 2018, DIGIT SIGNAL PROCESS, V81, P173, DOI 10.1016/j.dsp.2018.06.014
   Yu-Lun Wang, 2017, International Journal of Network Security, V19, P858, DOI 10.6633/IJNS.201709.19(5).24
   Zang HX, 2020, RENEW ENERG, V160, P26, DOI 10.1016/j.renene.2020.05.150
   Zhou ZL, 2019, IEEE ACCESS, V7, P179891, DOI 10.1109/ACCESS.2019.2955990
NR 42
TC 3
Z9 3
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4129
EP 4150
DI 10.1007/s00371-021-02285-1
EA SEP 2021
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000698559800001
DA 2024-07-18
ER

PT J
AU Yang, J
   Ning, T
   Shen, YC
AF Yang, Jiong
   Ning, Tao
   Shen, YunChao
TI Planar <i>G</i><SUP>3</SUP> Hermite interpolation by quintic Bezier
   curves
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric Hermite interpolation; G(3) continuity; quintic Bezier curve
ID EXISTENCE
AB To achieve G(3) Hermite interpolation with a lower degree curve, this paper studies planar G(3) Hermite interpolation using a quintic Bdzier curve. First, the first and second derivatives of the quintic Bdzier curve satisfying G(2) condition are constructed according to the interpolation conditions. Four parameters are introduced into the construction. Two of them are set as free design parameters, which represent the tangent vector module length of the quintic Bdzier curve at the two endpoints, and the other two parameters are derived from G(3) condition. Then, to match G(3) condition, it is necessary to ensure that the first derivative of curvature with respect to arc length is equal. Nevertheless, the direct calculation of the derivative of curvature involves the calculation of square root. Alternatively, an equivalent condition is derived by investigating the first derivative of curvature square. Based on this condition, the two parameters can be computed as the solutions of linear systems. Finally, the control points of the quintic Bdzier curve are obtained. Several comparative examples are provided to demonstrate the effectiveness of the proposed method. A variety of complex shape curves can be obtained by adjusting the two free design parameters. Applications to shape design are also shown.
C1 [Yang, Jiong] Zhengzhou Univ, Sch Mech & Power Engn, Zhengzhou 450001, Peoples R China.
   [Ning, Tao] Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.
   [Shen, YunChao] Aero Engine Corp China, Sichuan Gas Turbine Res Estab, Chengdu 610500, Peoples R China.
C3 Zhengzhou University; Beihang University
RP Ning, T (corresponding author), Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.
EM jiong_yang@foxmail.com; ningtao@buaa.edu.cn; shenych624@126.com
OI Ning, Tao/0000-0002-6678-3672; yang, jiong/0000-0001-6202-0996
FU National Natural Science Foundation of China [51705469]; National Key
   Project [GJXM92579]; Key Scientific Research Projects of Colleges and
   Universities in Henan Province [19A460028]
FX This research was supported by the National Natural Science Foundation
   of China (Grant No. 51705469), the National Key Project (Grant No.
   GJXM92579) and the Key Scientific Research Projects of Colleges and
   Universities in Henan Province (Grant No. 19A460028).
CR de Boor C., 1987, Computer-Aided Geometric Design, V4, P269, DOI 10.1016/0167-8396(87)90002-1
   Farin G.E., 2002, Curves and Surfaces for CAGD: A Practical Guide
   Farin G, 2008, COMPUT AIDED DESIGN, V40, P476, DOI 10.1016/j.cad.2008.01.003
   Femiani JC, 2012, COMPUT AIDED GEOM D, V29, P141, DOI 10.1016/j.cagd.2011.10.006
   Han CY, 2010, COMPUT AIDED GEOM D, V27, P713, DOI 10.1016/j.cagd.2010.09.002
   HOLLIG K, 1995, COMPUT AIDED GEOM D, V12, P567, DOI 10.1016/0167-8396(94)00034-P
   Hollig K, 1996, COMPUT AIDED GEOM D, V13, P681, DOI 10.1016/0167-8396(96)00004-0
   Jaklic G, 2011, J COMPUT APPL MATH, V235, P2758, DOI 10.1016/j.cam.2010.11.025
   Juhasz I, 1998, COMPUT AIDED DESIGN, V30, P1, DOI 10.1016/S0010-4485(97)00046-8
   Krajnc M, 2009, NONLINEAR ANAL-THEOR, V70, P2614, DOI 10.1016/j.na.2008.03.048
   Lu LZ, 2018, COMPUT GRAPH-UK, V70, P92, DOI 10.1016/j.cag.2017.07.007
   Lu LZ, 2015, J COMPUT APPL MATH, V274, P109, DOI 10.1016/j.cam.2014.07.015
   Meek DS, 2012, J COMPUT APPL MATH, V236, P4485, DOI 10.1016/j.cam.2012.04.021
   Meek DS, 1997, COMPUT AIDED GEOM D, V14, P619, DOI 10.1016/S0167-8396(96)00050-7
   Meek DS, 1997, J COMPUT APPL MATH, V81, P299, DOI 10.1016/S0377-0427(97)00066-6
   Reif U, 1999, COMPUT AIDED GEOM D, V16, P217, DOI 10.1016/S0167-8396(98)00046-6
   SCHACHTER-MORGENTHAU Ruth., 1998, Le multipartisme en Afrique de l'Ouest francophone jusqu 'aux independances. La periode nationaliste, P1
   Shi, 2013, CAGD NURBS
   Wang WP, 2000, VISUAL COMPUT, V16, P187, DOI 10.1007/s003710050207
   Wu WD, 2016, COMPUT AIDED DESIGN, V77, P86, DOI 10.1016/j.cad.2016.04.001
   Xu LH, 2001, COMPUT AIDED GEOM D, V18, P817, DOI 10.1016/S0167-8396(01)00053-X
   Yang XN, 2019, COMPUT AIDED DESIGN, V114, P112, DOI 10.1016/j.cad.2019.05.009
   Yang XNA, 2014, COMPUT AIDED GEOM D, V31, P701, DOI 10.1016/j.cagd.2014.09.001
   Yong JH, 2004, COMPUT AIDED GEOM D, V21, P281, DOI 10.1016/j.cagd.2003.08.003
   Ziatdinov R, 2012, COMPUT AIDED DESIGN, V44, P591, DOI 10.1016/j.cad.2012.01.007
NR 25
TC 1
Z9 1
U1 7
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4319
EP 4328
DI 10.1007/s00371-021-02298-w
EA SEP 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000696768500001
DA 2024-07-18
ER

PT J
AU Saeidi, M
   Arabsorkhi, A
AF Saeidi, Mahmoud
   Arabsorkhi, Abouzar
TI A novel backbone architecture for pedestrian detection based on the
   human visual system
SO VISUAL COMPUTER
LA English
DT Article
DE Pedestrian detection; DCNN; Backbone; Human visual system
ID PERFORMANCE
AB Pedestrian detection using deep convolutional neural networks (DCNNs) has made a breakthrough in the last few years and researchers have proposed different DCNN architectures to detect pedestrians more accurately. Most of these architectures have a backbone based on previous state-of-the-art architectures for classification tasks and just tried to adapt them for their detection task. They are improving their performance with some heuristics, trial and error techniques, and sometimes with grid search on a space of various architectures. However, there is no research in which, firstly, the visual detection system of human has been studied, and then tried to propose a backbone architecture based on that. In this paper, we first review the state-of-the-art methods and then, having a preliminary on visual detection system in the human brain and finally, propose our architecture based on that. The intuition behind our idea can justify the evolutionary course of detection architectures from the first fully convolutional neural networks (FCNNs), like Faster R-CNN, to the modern state-of-the-art methods nowadays and give us a better understanding of why some architectures are superior to the others. The advantage of our idea is that it can be applied to most of the existing architectures with some manipulations, although it is much easier on some methods than others. We have implemented our idea based on an anchor-free method called CSP and could achieve better performance on Caltech-USA and INRIA, which are two of the most popular pedestrian detection datasets.
C1 [Saeidi, Mahmoud; Arabsorkhi, Abouzar] Iran Telecommun Res Ctr, Tehran, Iran.
RP Saeidi, M (corresponding author), Iran Telecommun Res Ctr, Tehran, Iran.
EM msaeidi40@itrc.ac.ir
FU Iran Telecommunication Research Center
FX Authors would like to acknowledge Iran Telecommunication Research
   Center, for supports throughout this research.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.141
   [Anonymous], CoRR abs/1511.07122
   [Anonymous], COMPUT VIS PATTERN R
   Baars B.J., 2013, Fundamentals of cognitive neuroscience: A beginner's guide
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Du XZ, 2017, IEEE WINT CONF APPL, P953, DOI 10.1109/WACV.2017.111
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fei-Fei Li R.K., CNN ARCHITECTURES
   Gage N., 2018, Fundamentals of Cognitive Neuroscience-2nd Edition
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Bui HM, 2016, IEEE ACCESS, V4, P10059, DOI 10.1109/ACCESS.2016.2639543
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kanade T., 2012, 3 DIMENSIONAL MACHIN, V21
   Kingma D. P., 2014, arXiv
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin CZ, 2020, IEEE T IMAGE PROCESS, V29, P3820, DOI 10.1109/TIP.2020.2966371
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YD, 2020, AAAI CONF ARTIF INTE, V34, P11653
   Liu ZM, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/3518959
   Marín J, 2013, IEEE I CONF COMP VIS, P2592, DOI 10.1109/ICCV.2013.322
   Neuroscience F., 2003, FUNDAMENTAL NEUROSCI
   Paisitkriangkrai S, 2016, IEEE T PATTERN ANAL, V38, P1243, DOI 10.1109/TPAMI.2015.2474388
   Pang YW, 2019, IEEE I CONF COMP VIS, P4966, DOI 10.1109/ICCV.2019.00507
   Perreault H, 2020, 2020 17TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2020), P230, DOI 10.1109/CRV50864.2020.00038
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schieber M., 2008, FUNDAMENTAL NEUROSCI
   Sebe N., 2005, Machine Learning in Computer Vision, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh B, 2018, 32 C NEURAL INFORM P
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Song T, 2018, LECT NOTES COMPUT SC, V11211, P554, DOI 10.1007/978-3-030-01234-2_33
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szeliski R, 2011, TEXTS COMPUT SCI, P181, DOI 10.1007/978-1-84882-935-0_4
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   VAILLANT R, 1994, IEE P-VIS IMAGE SIGN, V141, P245, DOI 10.1049/ip-vis:19941301
   Viola P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P747
   Wang S., 2018, DEEP REINFORCEMENT L
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Wojek C, 2008, LECT NOTES COMPUT SC, V5096, P82, DOI 10.1007/978-3-540-69321-5_9
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang JW, 2003, PATTERN RECOGN LETT, V24, P1805, DOI 10.1010/S0167-8655(03)00005-9
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang S, 2014, CVPR
   Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
NR 64
TC 7
Z9 7
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2223
EP 2237
DI 10.1007/s00371-021-02280-6
EA AUG 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000686426400001
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Liu, T
   Hu, L
   Shang, YY
   Liu, XR
AF Liu, Shengjun
   Liu, Tao
   Hu, Ling
   Shang, Yuanyuan
   Liu, Xinru
TI Variational progressive-iterative approximation for RBF-based surface
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Implicit curve and surface; Progressive-iterative approximation;
   Conjugate gradient method
ID B-SPLINE CURVE; QUASI-INTERPOLATION; SCATTERED DATA; REPRESENTATION;
   PARTITION
AB RBF-based methods play a very important role in the point cloud reconstruction field. However, solving a linear system is the bottleneck of such methods, especially when there are a large number of points and lead the computing to be time-consuming and unstable. In this paper, we firstly construct a novel implicit progressive-iterative approximation framework based on RBFs, which could elegantly reconstruct curves and surfaces or even higher dimensional data in an approximation or interpolation way, avoiding expensive computational cost on solving linear systems. Then, we further accelerate the proposed method with a strategy inspired from the conjugate gradient algorithm. In our framework, using proper RBFs allows to simply transform the iteration matrix to be symmetrical and positive definite. Such a property contributes to reduce the computational cost greatly and produce high-quality reconstruction results. Plenty of numerical examples on various challenging data are provided to demonstrate our efficiency, effectiveness, and superiority to other methods.
C1 [Liu, Shengjun; Liu, Tao; Shang, Yuanyuan; Liu, Xinru] Cent South Univ, Sch Math & Stat, Changsha, Peoples R China.
   [Hu, Ling] Hunan First Normal Univ, Sch Math & Comp Sci, Changsha, Peoples R China.
C3 Central South University; Hunan First Normal University
RP Liu, XR (corresponding author), Cent South Univ, Sch Math & Stat, Changsha, Peoples R China.
EM shjliu.cg@csu.edu.cn; ltaocg@csu.edu.cn; liuxinru@csu.edu.cn
RI Liu, Xinru/KEH-2341-2024; Hu, Ling/AAA-5764-2020
OI Liu, Tao/0000-0003-1016-4191
FU Hunan Science Fund for Distinguished Young Scholars [2019JJ20027]
FX Funding is provided by Hunan Science Fund for Distinguished Young
   Scholars (Grant No. 2019JJ20027).
CR [Anonymous], 1979, P 1979 ARM NUM AN CO
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Calakli F, 2011, COMPUT GRAPH FORUM, V30, P1993, DOI 10.1111/j.1467-8659.2011.02058.x
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   [陈甜甜 Chen Tiantian], 2013, [图学学报, Journal of Graphics], V34, P66
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Cheng FH, 2009, J COMPUT SCI TECH-CH, V24, P39, DOI 10.1007/s11390-009-9199-2
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Dey T.K., 2006, CURVE SURFACE RECONS
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Gois JP, 2008, VISUAL COMPUT, V24, P1013, DOI 10.1007/s00371-008-0297-x
   Hamza YF, 2020, COMPUT AIDED GEOM D, V77, DOI 10.1016/j.cagd.2020.101817
   Han XL, 2008, LECT NOTES COMPUT SC, V4975, P541
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Le-Thi-Thu N., 2014, INT C NAT COMP COMM
   Lin H., 2011, TECHNICAL SECTION EX
   Lin HW, 2018, J SYST SCI COMPLEX, V31, P1618, DOI 10.1007/s11424-018-7443-y
   Lin HW, 2011, COMPUT GRAPH-UK, V35, P967, DOI 10.1016/j.cag.2011.07.003
   Lin HW, 2010, COMPUT AIDED GEOM D, V27, P322, DOI 10.1016/j.cagd.2010.01.003
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Liu M., 2017, J COMPUT APPL MATH, V327
   Liu SJ, 2016, COMPUT AIDED DESIGN, V78, P147, DOI 10.1016/j.cad.2016.05.001
   Liu SJ, 2013, VISUAL COMPUT, V29, P627, DOI 10.1007/s00371-013-0801-9
   Liu SJ, 2012, COMPUT AIDED GEOM D, V29, P435, DOI 10.1016/j.cagd.2012.03.011
   Liu SJ, 2012, IEEE COMPUT GRAPH, V32, P70, DOI 10.1109/MCG.2011.14
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Luenberger DG, 1984, LINEAR NONLINEAR PRO
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   Ohtake Y, 2005, GRAPH MODELS, V67, P150, DOI 10.1016/j.gmod.2004.06.003
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pan RJ, 2009, SCI CHINA SER F, V52, P308, DOI 10.1007/s11432-009-0032-x
   Qi D., 1975, THESIS
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Wendland H., 2004, Cambridge Monographs on Applied and Computational Mathematics, DOI [DOI 10.1017/CBO9780511617539, 10.1017/CBO9780511617539]
   [张莉 Zhang Li], 2017, [浙江大学学报. 理学版, Journal of Zhejiang University. Sciences Edition], V44, P22
NR 39
TC 6
Z9 7
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2485
EP 2497
DI 10.1007/s00371-021-02213-3
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000679759600002
DA 2024-07-18
ER

PT J
AU Xiao, K
   Tian, YT
   Lu, YY
   Lai, YP
   Wang, XC
AF Xiao, Ke
   Tian, Yutong
   Lu, Yuanyao
   Lai, Yuping
   Wang, Xunchang
TI Quality assessment-based iris and face fusion recognition with dynamic
   weight
SO VISUAL COMPUTER
LA English
DT Article
DE Iris recognition; Face recognition; Multi-biometric fusion; Quality
   assessment; Dynamic weight
AB Image quality is one of the most crucial influence factors when conducting biometric image-based human identification. However, it is not considered in most existing multi-biometric fusion algorithms. In this paper, a quality assessment-based dynamic weighted fusion algorithm is proposed, which applies a method of using image quality scores, and the scores are evaluated by integrating various image quality metrics to assess the quality of the feature matching process. According to the classification of the dual-modality feature matching quality, a dynamic weighted fusion strategy is proposed to increase the weight of biometric traits with better quality and weaken the impact of low-quality biometric traits on the recognition results, to achieve the adaptive optimization fusion of the biometric score level. Finally, the fused scores are used to make decisions. Experimental results reveal that the proposed algorithm is more robust and has better achievement than unimodal biometrics and traditional fusion algorithms.
C1 [Xiao, Ke; Tian, Yutong; Lu, Yuanyao; Lai, Yuping; Wang, Xunchang] North China Univ Technol, Sch Informat Sci & Technol, Beijing, Peoples R China.
C3 North China University of Technology
RP Tian, YT (corresponding author), North China Univ Technol, Sch Informat Sci & Technol, Beijing, Peoples R China.
EM xiaoke@ncut.edu.cn; tianyutong1994@126.com
RI Yang, Shu/JUU-4592-2023; wang, jiajun/JRW-6032-2023; Zhang,
   Shiwei/JIY-4344-2023; Tian, Yutong/KOC-6596-2024; xiao, ke/HSB-5863-2023
FU National Key R&D Program of China [2017YFB0802300]
FX This work was supported by the National Key R&D Program of China under
   Grant 2017YFB0802300.
CR Abaza A, 2012, INT C PATT RECOG, P3103
   [Anonymous], 2006, Handbook of Multibiometrics
   Anzar SM, 2013, ADV INTELL SYST, V177, P833
   Azom V, 2015, PROCEEDINGS OF THE 2015 PATTERN RECOGNITION ASSOCIATION OF SOUTH AFRICA AND ROBOTICS AND MECHATRONICS INTERNATIONAL CONFERENCE (PRASA-ROBMECH), P207, DOI 10.1109/RoboMech.2015.7359524
   Bouzouina Y, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON BIO-ENGINEERING FOR SMART TECHNOLOGIES (BIOSMART)
   Burge M.J., 2013, Handbook of Iris Recognition, DOI DOI 10.1007/978-1-4471-4402-1
   Chinese Academy of Science-Institute of Automation, CASIA IRIS V2 IR IM
   Dinca LM, 2017, IEEE ACCESS, V5, P6247, DOI 10.1109/ACCESS.2017.2694050
   Eskandari M, 2017, IET BIOMETRICS, V6, P334, DOI 10.1049/iet-bmt.2016.0060
   Guofeng Z., 2016, J SHANDONG U ENG SCI
   Gupta K, 2021, VISUAL COMPUT, V37, P1401, DOI 10.1007/s00371-020-01873-x
   Hosseini MS., 2015, J MATH IMAG VIS, V52, P1, DOI [10.1007/s10851-015-0579-7, DOI 10.1007/S10851-015-0579-7]
   Jang J, 2008, PATTERN RECOGN LETT, V29, P1759, DOI 10.1016/j.patrec.2008.05.005
   Kamble V, 2015, OPTIK, V126, P1090, DOI 10.1016/j.ijleo.2015.02.093
   Kihal N, 2014, INT CONF IMAG PROC, P313
   Liau HF, 2011, EXPERT SYST APPL, V38, P11105, DOI 10.1016/j.eswa.2011.02.155
   Liu X, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P106, DOI 10.1109/SPIN.2016.7566671
   Masek L., 2003, THESIS CITESEER
   Matin A, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND COMMUNICATION ENGINEERING (ECCE), P1, DOI 10.1109/ECACE.2017.7912868
   Merati A, 2012, IEEE T INF FOREN SEC, V7, P1270, DOI 10.1109/TIFS.2012.2198469
   Miao D, 2017, NEUROCOMPUTING, V224, P105, DOI 10.1016/j.neucom.2016.10.048
   Miao D, 2014, INT C PATT RECOG, P291, DOI 10.1109/ICPR.2014.59
   Murakami T., 2011, 2011 INT JOINT C BIO, P1
   Phillips P. J., 2013, IEEE INT C BIOM THEO, P1
   Sim HM, 2014, EXPERT SYST APPL, V41, P5390, DOI 10.1016/j.eswa.2014.02.051
   Sutra G., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P310, DOI 10.1109/ICB.2012.6199825
   Wan J., 2017, IAPR C MACH VIS APPL, P248
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z., 2012, INT J DIGIT CONTENT, V6, P111
   Yuling R., 2015, J SHANDONG U ENG SCI
   Zhu T, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-5
NR 31
TC 5
Z9 5
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1631
EP 1643
DI 10.1007/s00371-021-02093-7
EA MAR 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000628459700001
DA 2024-07-18
ER

PT J
AU Cheng, ZM
   Qu, AP
   He, XF
AF Cheng, Zhiming
   Qu, Aiping
   He, Xiaofeng
TI Contour-aware semantic segmentation network with spatial attention
   mechanism for medical image
SO VISUAL COMPUTER
LA English
DT Article
DE Medical image segmentation; Semantic segmentation; Neural network
ID CONVOLUTIONAL NEURAL-NETWORKS; NET
AB Medical image segmentation is a critical and important step for developing computer-aided system in clinical situations. It remains a complicated and challenging task due to the large variety of imaging modalities and different cases. Recently, Unet has become one of the most popular deep learning frameworks because of its accurate performance in biomedical image segmentation. In this paper, we propose a contour-aware semantic segmentation network, which is an extension of Unet, for medical image segmentation. The proposed method includes a semantic branch and a detail branch. The semantic branch focuses on extracting the semantic features from shallow and deep layers; the detail branch is used to enhance the contour information implied in the shallow layers. In order to improve the representation capability of the network, a MulBlock module is designed to extract semantic information with different receptive fields. Spatial attention module (CAM) is used to adaptively suppress the redundant features. In comparison with the state-of-the-art methods, our method achieves a remarkable performance on several public medical image segmentation challenges.
C1 [Cheng, Zhiming; Qu, Aiping; He, Xiaofeng] Univ South China, Sch Comp, Hengyang 421001, Peoples R China.
   [Qu, Aiping] Hunan Prov Base Sci & Technol Innovat Cooperat, Hengyang 421001, Peoples R China.
C3 University of South China
RP Qu, AP (corresponding author), Univ South China, Sch Comp, Hengyang 421001, Peoples R China.; Qu, AP (corresponding author), Hunan Prov Base Sci & Technol Innovat Cooperat, Hengyang 421001, Peoples R China.
EM qap@usc.edu.cn
RI Cheng, Zhiming/GQG-8732-2022
OI Cheng, Zhiming/0000-0003-2671-9411; Qu, Aiping/0000-0003-3648-4556
FU NationalNatural Science Foundation ofChina [61701218]; Natural Science
   Foundation of Hunan Province ofChina [2018JJ3449]; Education Commission
   of Hunan Province of China [17B229, 18A253]
FX This work has been partially supported by the NationalNatural Science
   Foundation ofChina (No. 61701218), the Natural Science Foundation of
   Hunan Province ofChina (No. 2018JJ3449), Education Commission of Hunan
   Province of China (No. 17B229, 18A253).
CR Alom MZ, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.1.014006
   Asadi-Aghbolaghi, 2020, ARXIV PREPRINT ARXIV
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Azad R, 2019, IEEE INT CONF COMP V, P406, DOI 10.1109/ICCVW.2019.00052
   Baldeon-Calisto M, 2020, NEUROCOMPUTING, V392, P325, DOI 10.1016/j.neucom.2019.01.110
   Bazin PL, 2008, MED IMAGE ANAL, V12, P616, DOI 10.1016/j.media.2008.06.008
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Carballido-Gamio J, 2004, IEEE T MED IMAGING, V23, P36, DOI 10.1109/TMI.2003.819929
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2019, PROC CVPR IEEE, P11624, DOI 10.1109/CVPR.2019.01190
   Choudhury AR, 2019, LECT NOTES COMPUT SC, V11384, P154, DOI 10.1007/978-3-030-11726-9_14
   Christ PF, 2017, Case Studies in Clinical Psychological Science: Bridging the Gap from Science to Practice
   Chung DH, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P404, DOI 10.1109/ICIP.2000.899419
   Ciresan D., 2012, NIPS, P2843
   Codella NCF, 2018, I S BIOMED IMAGING, P168, DOI 10.1109/ISBI.2018.8363547
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Guo C, 2020, PROCEEDINGS OF SC20: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SC20), DOI 10.1109/SC41405.2020.00020
   Hatamizadeh A, 2019, LECT NOTES COMPUT SC, V11861, P187, DOI 10.1007/978-3-030-32692-0_22
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Li H, 2019, IEEE J BIOMED HEALTH, V23, P527, DOI 10.1109/JBHI.2018.2859898
   Liao FZ, 2019, IEEE T NEUR NET LEAR, V30, P3484, DOI 10.1109/TNNLS.2019.2892409
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nguyen HT, 2003, IEEE T PATTERN ANAL, V25, P330, DOI 10.1109/TPAMI.2003.1182096
   NUNZIO GD, 2011, J DIGIT IMAGING, V24, P11
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Pereira S, 2016, IEEE T MED IMAGING, V35, P1240, DOI 10.1109/TMI.2016.2538465
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sánchez-González A, 2018, COMPUT BIOL MED, V100, P152, DOI 10.1016/j.compbiomed.2018.07.002
   Sun Jesse, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12264), P797, DOI 10.1007/978-3-030-59719-1_77
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Valindria V.V., 2018, ARXIV PREPRINT ARXIV
   Wang R, 2020, J CHEM INF MODEL, V60, P5853, DOI 10.1021/acs.jcim.0c00501
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xia YD, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101766
   Xie J, 2005, IEEE T MED IMAGING, V24, P45, DOI 10.1109/TMI.2004.837792
   Yang YY, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115907
   Yang YY, 2020, VISUAL COMPUT, V36, P717, DOI 10.1007/s00371-019-01651-4
   Zhou X.Y., 2017, ARXIV171101506
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 44
TC 38
Z9 39
U1 10
U2 132
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 749
EP 762
DI 10.1007/s00371-021-02075-9
EA FEB 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000620455500001
PM 33642659
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Ouerghi, H
   Mourali, O
   Zagrouba, E
AF Ouerghi, Hajer
   Mourali, Olfa
   Zagrouba, Ezzeddine
TI Glioma classification via MR images radiomics analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Glioma; Radiomics; Multi-modal medical image fusion; Machine learning
ID FUSION; SELECTION; SYSTEM; STATE
AB Accurate glioma classification before surgery is of the utmost important in clinical decision making and prognosis prediction. In this paper, we investigate the impact of multi-modal MR image fusion for the differentiation of low-grade gliomas (LGG) versus high-grade gliomas (HGG) via integrative analyses of radiomic features and machine learning approaches. A set of 80 histologically confirmed gliomas patients (40 HGG and 40 LGG) obtained from the MICCAI BraTS 2019 data were involved in this study. To achieve this work, we propose to combine T1 with T2 or FLAIR modality in the non-subsampled shearlet domain. Firstly, the pre-processed source MR images are decomposed into low-frequency (LF) and several high-frequency (HF) sub-images. LF sub-images are fused using the proposed weight local features fusion rule while HF sub-images are combined based on the novel sum-modified-laplacian technique. Experimental results demonstrate that the proposed fusion approach outperformed the recent state-of-the-art approaches in terms of entropy and feature mutual information. Subsequently, a key radiomics signature was retrieved by the least absolute shrinkage and selection operator regression algorithm. Five machine learning classifiers were established and evaluated with the retrieved dataset, then with the fused dataset using tenfold cross-validation scheme. As a result, the random forest had the highest accuracy of 96.5% with 21 features selected from the raw data and 96.1% with 16 features selected from the fused data. Finally, the experimental findings confirm that the proposed aided diagnosis framework represents a promising tool to aid radiologists in differentiating HGG and LGG.
C1 [Ouerghi, Hajer; Mourali, Olfa; Zagrouba, Ezzeddine] Univ Tunis Manar, LR16ES06 Lab Rech Informat Modelisat & Traitement, Inst Super Informat Manar, Res Team SIIVA, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
C3 Universite de Tunis-El-Manar
RP Ouerghi, H (corresponding author), Univ Tunis Manar, LR16ES06 Lab Rech Informat Modelisat & Traitement, Inst Super Informat Manar, Res Team SIIVA, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
EM hajer.ouerghi@fst.utm.tn; olfa.mourali@isi.utm.tn;
   ezzeddine.zagrouba@uvt.tn
RI Zagrouba, Ezzeddine/D-7896-2014
OI Zagrouba, Ezzeddine/0000-0002-2574-9080; ouerghi,
   hajer/0000-0003-4896-0433
CR Aerts HJWL, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5006
   Ali H, 2021, VISUAL COMPUT, V37, P939, DOI 10.1007/s00371-020-01845-1
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bakas Spyridon, 2018, ARXIV181102629, DOI DOI 10.17863/CAM.38755
   Brunese L, 2020, COMPUT METH PROG BIO, V185, DOI 10.1016/j.cmpb.2019.105134
   Cao H, 2020, EUR RADIOL, V30, P3073, DOI 10.1007/s00330-019-06632-8
   Cho HH, 2018, PEERJ, V6, DOI 10.7717/peerj.5982
   Dogra J, 2020, VISUAL COMPUT, V36, P875, DOI 10.1007/s00371-019-01698-3
   Du J, 2016, NEUROCOMPUTING, V215, P3, DOI 10.1016/j.neucom.2015.07.160
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Gillies RJ, 2016, RADIOLOGY, V278, P563, DOI 10.1148/radiol.2015151169
   Gore S, 2021, ACAD RADIOL, V28, P1599, DOI 10.1016/j.acra.2020.06.016
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P744, DOI 10.1016/j.compeleceng.2011.07.012
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Hu JP, 2020, EUR J RADIOL, V126, DOI 10.1016/j.ejrad.2020.108929
   Jagalingam P, 2015, AQUAT PR, V4, P133, DOI 10.1016/j.aqpro.2015.02.019
   James AP, 2014, INFORM FUSION, V19, P4, DOI 10.1016/j.inffus.2013.12.002
   Lambin P, 2017, NAT REV CLIN ONCOL, V14, P749, DOI 10.1038/nrclinonc.2017.141
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Liu XB, 2018, BIOMED SIGNAL PROCES, V40, P343, DOI 10.1016/j.bspc.2017.10.001
   Liu XB, 2017, NEUROCOMPUTING, V235, P131, DOI 10.1016/j.neucom.2017.01.006
   Lotan E, 2019, AM J ROENTGENOL, V212, P26, DOI 10.2214/AJR.18.20218
   Louis DN, 2016, ACTA NEUROPATHOL, V131, P803, DOI 10.1007/s00401-016-1545-1
   Mahajan S., 2014, IPASJ INT J COMPUTER, V2, P8
   Manchanda M, 2018, J VIS COMMUN IMAGE R, V51, P76, DOI 10.1016/j.jvcir.2017.12.011
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Mohammed A, 2016, PROCEEDINGS OF THE 2016 IEEE REGION 10 CONFERENCE (TENCON), P2147, DOI 10.1109/TENCON.2016.7848406
   Mohan G, 2018, BIOMED SIGNAL PROCES, V39, P139, DOI 10.1016/j.bspc.2017.07.007
   Ostrom QT, 2014, NEURO-ONCOLOGY, V16, P896, DOI 10.1093/neuonc/nou087
   Ouerghi Hajer, 2017, International Journal of Computer and Communication Engineering, V6, P201, DOI 10.17706/ijcce.2017.6.3.201-211
   Ouerghi H, 2018, IET IMAGE PROCESS, V12, P1873, DOI 10.1049/iet-ipr.2017.1298
   Qi XX, 2018, EUR RADIOL, V28, P1748, DOI 10.1007/s00330-017-5108-1
   Rathore S, 2020, CANCERS, V12, DOI 10.3390/cancers12030578
   Rohlfing T, 2010, HUM BRAIN MAPP, V31, P798, DOI 10.1002/hbm.20906
   Saba T, 2020, COGN SYST RES, V59, P221, DOI 10.1016/j.cogsys.2019.09.007
   Singh R, 2021, VISUAL COMPUT, V37, P2157, DOI 10.1007/s00371-020-01977-4
   Su CL, 2019, EUR RADIOL, V29, P1986, DOI 10.1007/s00330-018-5704-8
   Sudre CH, 2020, BMC MED INFORM DECIS, V20, DOI 10.1186/s12911-020-01163-5
   Tian Q, 2018, J MAGN RESON IMAGING, V48, P1518, DOI 10.1002/jmri.26010
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Ullah H, 2020, BIOMED SIGNAL PROCES, V57, DOI 10.1016/j.bspc.2019.101724
   Vamvakas A, 2019, PHYS MEDICA, V60, P188, DOI 10.1016/j.ejmp.2019.03.014
   van Griethuysen JJM, 2017, CANCER RES, V77, pE104, DOI 10.1158/0008-5472.CAN-17-0339
   Wang QY, 2019, J MAGN RESON IMAGING, V49, P825, DOI 10.1002/jmri.26265
   Wang ZB, 2020, COMPUT BIOL MED, V123, DOI 10.1016/j.compbiomed.2020.103823
   Wesseling P, 2018, NEUROPATH APPL NEURO, V44, P139, DOI 10.1111/nan.12432
   Wu YP, 2018, J AMB INTEL HUM COMP, V9, P1671, DOI 10.1007/s12652-018-0883-3
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yang Z, 2019, ARCH COMPUT METHOD E, V26, P491, DOI 10.1007/s11831-018-9253-8
   Yin M, 2014, OPTIK, V125, P2274, DOI 10.1016/j.ijleo.2013.10.064
   Zhou M, 2018, AM J NEURORADIOL, V39, P208, DOI 10.3174/ajnr.A5391
   Zhou P, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031028
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 53
TC 12
Z9 12
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1427
EP 1441
DI 10.1007/s00371-021-02077-7
EA FEB 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000618136000001
DA 2024-07-18
ER

PT J
AU Rao, ZB
   He, MY
   Dai, YC
   Shen, ZL
AF Rao, Zhibo
   He, Mingyi
   Dai, Yuchao
   Shen, Zhelun
TI Patch attention network with generative adversarial model for
   semi-supervised binocular disparity prediction
SO VISUAL COMPUTER
LA English
DT Article
DE Binocular disparity estimation; Semi-supervised learning; Patch
   attention mechanism; Generative adversarial model
ID STEREO; NET
AB In this paper, we address the challenging points of binocular disparity estimation: (1) unsatisfactory results in the occluded region when utilizing warping function in unsupervised learning; (2) inefficiency in running time and the number of parameters as adopting a lot of 3D convolutions in the feature matching module. To solve these drawbacks, we propose a patch attention network for semi-supervised stereo matching learning. First, we employ a channel-attention mechanism to aggregate the cost volume by selecting its different surfaces for reducing a large number of 3D convolution, called the patch attention network (PA-Net). Second, we use our proposed PA-Net as a generator and then combine it, traditional unsupervised learning loss, and the adversarial learning model to construct a semi-supervised learning framework for improving performance in the occluded areas. We have trained our PA-Net in supervised learning, semi-supervised learning, and unsupervised learning manners. Extensive experiments show that (1) our semi-supervised learning framework can overcome the drawbacks of unsupervised learning and significantly improve the performance in the ill-posed region by using only a few or inaccurate ground truths; (2) our PA-Net can outperform other state-of-the-art approaches in supervised learning and use fewer parameters.
C1 [Rao, Zhibo; He, Mingyi; Dai, Yuchao] Northwestern Polytech Univ, Xian 710129, Peoples R China.
   [Shen, Zhelun] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Northwestern Polytechnical University; Peking University
RP Rao, ZB; He, MY (corresponding author), Northwestern Polytech Univ, Xian 710129, Peoples R China.
EM raoxi36@foxmail.com; myhe@nwpu.edu.cn
RI He, Mingyi/B-4138-2011; Dai, Yuchao/F-7832-2015; Mingyi,
   HE/IXN-2319-2023; Rao, Zhibo/ACT-2427-2022; He, Mingyi/AGX-2464-2022
OI He, Mingyi/0000-0003-2051-6955; Dai, Yuchao/0000-0002-4432-7406; Mingyi,
   HE/0000-0003-2051-6955; He, Mingyi/0000-0003-2051-6955; Rao,
   Zhibo/0000-0001-7832-2913
FU Natural Science Foundation of China [61671387, 61420106007, 61871325]
FX This work was supported in part by Natural Science Foundation of China
   (61671387, 61420106007, 61871325).
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   [Anonymous], 2015, PROC CVPR IEEE
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen ST, 2021, VISUAL COMPUT, V37, P411, DOI 10.1007/s00371-020-01811-x
   Cheng XL, 2019, PROC CVPR IEEE, P6332, DOI 10.1109/CVPR.2019.00650
   Dai Y, 2020, IEEE J OCEANIC ENG, V45, P699, DOI 10.1109/JOE.2019.2899689
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji RR, 2020, IEEE T PATTERN ANAL, V42, P2410, DOI 10.1109/TPAMI.2019.2936024
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Li B, 2018, PATTERN RECOGN, V83, P328, DOI 10.1016/j.patcog.2018.05.029
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719
   Li YJ, 2019, VISUAL COMPUT, V35, P257, DOI 10.1007/s00371-018-1491-0
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   MENZE M, 2015, PROC CVPR IEEE, P3061, DOI DOI 10.1109/CVPR.2015.7298925
   Ramirez PZ, 2019, LECT NOTES COMPUT SC, V11363, P298, DOI 10.1007/978-3-030-20893-6_19
   Rao ZB, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.16
   Rao ZB, 2019, ASIAPAC SIGN INFO PR, P578, DOI 10.1109/APSIPAASC47483.2019.9023237
   Rao ZB, 2019, ASIAPAC SIGN INFO PR, P438, DOI 10.1109/APSIPAASC47483.2019.9023223
   Rasmuson S, 2021, VISUAL COMPUT, V37, P553, DOI 10.1007/s00371-020-01823-7
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703
   Seki Akihito, 2016, BMVC
   Shaked A, 2017, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2017.730
   Smolyanskiy N, 2018, IEEE COMPUT SOC CONF, P1120, DOI 10.1109/CVPRW.2018.00147
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Tian L, 2019, VISUAL COMPUT, V35, P1427, DOI 10.1007/s00371-018-01622-1
   Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu ZY, 2019, IEEE I CONF COMP VIS, P7483, DOI 10.1109/ICCV.2019.00758
   Xie L, 2019, VISUAL COMPUT, V35, P99, DOI 10.1007/s00371-018-1507-9
   Yamaguchi K, 2014, LECT NOTES COMPUT SC, V8693, P756, DOI 10.1007/978-3-319-10602-1_49
   Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhong Y., 2017, SELFSUPERVISED LEARN
   Zhong YR, 2019, PROC CVPR IEEE, P12087, DOI 10.1109/CVPR.2019.01237
   Zhu ZD, 2019, C IND ELECT APPL, P1789, DOI [10.1109/iciea.2019.8834193, 10.1109/ICIEA.2019.8834193]
NR 53
TC 9
Z9 9
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 77
EP 93
DI 10.1007/s00371-020-02001-5
EA NOV 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000588855100001
DA 2024-07-18
ER

PT J
AU Huang, L
   Wang, YZ
   Bai, T
AF Huang, Lan
   Wang, Yuzhao
   Bai, Tian
TI Recognizing art work image from natural type: a deep adaptive depiction
   fusion method
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Domain adaptation; Classification; Cross-depiction
AB As the big difference between natural type and art type, recognizing visual objects from photos to art paintings, cartoon pictures, or sketches introduces a great challenge. Domain adaptation focuses on overcoming the differences between different fields. It is an effective technology to bridge the cross-domain discrepancy by transferable features, while the existing domain-adaptive methods all need target domain images of the same category as source domain images to reduce domain shifts, which leads to limitations on target domain images. To solve this problem, we constructed an end-to-end unsupervised model called adaptive depiction fusion network (ADFN). Compared with other domain adaption methods, ADFN recognizes visual objects in art works by using only their natural type. It reinforces adaptive instance normalization technology to embed the depiction offset into the source domain features. At the meantime, we also provide a complete benchmark, cross-depiction-net, which is large and various enough to overcome the lack of data for this problem. To properly evaluate the performance of the ADFN, we compared it to different state-of-the-art methods (DAN, DDC, Deep-coral, and MRAN) on cross-depiction-net dataset. The results show that our model is superior to the state-of-the-art methods.
C1 [Huang, Lan; Wang, Yuzhao; Bai, Tian] Jilin Univ, Coll Comp Sci & Technol, Changchun 130000, Peoples R China.
   [Huang, Lan; Wang, Yuzhao; Bai, Tian] Jilin Univ, Minist Educ, Key Lab Symbol Computat & Knowledge Engn, Changchun 130000, Peoples R China.
C3 Jilin University; Jilin University
RP Bai, T (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130000, Peoples R China.; Bai, T (corresponding author), Jilin Univ, Minist Educ, Key Lab Symbol Computat & Knowledge Engn, Changchun 130000, Peoples R China.
EM baitian@jlu.edu.cn
FU National Natural Science Foundation of China [61702214]; Development
   Project of Jilin Province of China [202008 01033GH]; Jilin Provincial
   Key Laboratory of Big Date Intelligent Computing [20180622002JC];
   Fundamental Research Funds for the Central University, JLU
FX This work is supported by the National Natural Science Foundation of
   China (No. 61702214), Development Project of Jilin Province of China
   (No. 202008 01033GH), Jilin Provincial Key Laboratory of Big Date
   Intelligent Computing (No. 20180622002JC), and the Fundamental Research
   Funds for the Central University, JLU.
CR [Anonymous], 2016, BMC BIOINFORMATIC S9, DOI DOI 10.1186/s12859-016-0902-3
   Bai T, 2020, CONCURR COMP-PRACT E, V32, DOI 10.1002/cpe.5005
   Bousmalis K, 2016, ADV NEUR IN, V29
   Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288
   Crowley Elliot J., 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P721, DOI 10.1007/978-3-319-46604-0_50
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Florea C, 2017, LECT NOTES COMPUT SC, V10269, P337, DOI 10.1007/978-3-319-59126-1_28
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hall P., 2015, Computational Visual Media, V1, P91, DOI DOI 10.1007/S41095-015-0017-1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu M.-Y., 2017, NIPS
   Liu YF, 2014, ADV SOC SCI EDUC HUM, V6, P313
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Peng X., 2018, ARXIV180609755
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Taigman Y., 2014, ARXIV161102200
   Tzeng E., 2014, ARXIV14123474
   Wang L, 2020, VISUAL COMPUT, V36, P317, DOI 10.1007/s00371-018-1609-4
   Wang Y, 2019, J VIS COMMUN IMAGE R, V58, P130, DOI 10.1016/j.jvcir.2018.11.022
   Wu Q, 2014, LECT NOTES COMPUT SC, V8695, P313, DOI 10.1007/978-3-319-10584-0_21
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhou F, 2019, VISUAL COMPUT, V35, P1583, DOI 10.1007/s00371-018-1559-x
   Zhu YC, 2019, NEURAL NETWORKS, V119, P214, DOI 10.1016/j.neunet.2019.07.010
NR 36
TC 4
Z9 4
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1221
EP 1232
DI 10.1007/s00371-020-01995-2
EA NOV 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000583685000002
DA 2024-07-18
ER

PT J
AU Bai, J
   Chen, R
   Liu, M
AF Bai, Jing
   Chen, Ran
   Liu, Min
TI Feature-attention module for context-aware image-to-image translation
SO VISUAL COMPUTER
LA English
DT Article
DE Feature-attention; Unsupervised image-to-image translation;
   Context-aware
AB In a summer2winter image-to-image translation, trees should be transformed from green to gray, but the colors of houses or girls should not be changed. However, current unsupervised one-to-one image translation techniques failed to focus the translation on individual objects. To tackle this issue, we propose a novel feature-attention module for capturing the mutual influences of various features, so as to automatically attend only to specific scene objects in unsupervised image-to-image translation. The proposed module can be integrated into different image translation networks and improve their context-aware translation ability. The qualitative and quantitative experiments on horse2zebra, apple2orange and summer2winter datasets based on DualGAN, CycleGAN and UNIT demonstrate a significant improvement in our proposed module over the state-of-the-art methods. In addition, the experiments on apple2orange dataset based on MUNIT and DRIT further indicate the effectiveness of FA module in multimodal translation tasks. We also show that the computation complexity of the proposed module is linear to the image size; moreover, the experiments on the day2night dataset prove that the proposed module is insensitive to the growth of image resolution. The source code and trained models are available at https://github.com/gaoyu ainshuyi/fa..
C1 [Bai, Jing; Chen, Ran] North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Ningxia, Peoples R China.
   [Bai, Jing] Ningxia Prov Key Lab Intelligent Informat & Data, Yinchuan 750021, Ningxia, Peoples R China.
   [Liu, Min] Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA.
C3 North Minzu University; Purdue University System; Purdue University
RP Bai, J (corresponding author), North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Ningxia, Peoples R China.; Bai, J (corresponding author), Ningxia Prov Key Lab Intelligent Informat & Data, Yinchuan 750021, Ningxia, Peoples R China.
EM baijing@nun.edu.cn
RI Bai, Jing/HGD-3571-2022
OI Bai, Jing/0000-0003-4247-6210
FU Natural Science Foundation of China [61762003]; CAS "Light of West
   China" Program [2018QNXZ0024]; First-class discipline construction in
   Ningxia universities (Electronic Science and Technology) [NXYLXK2017A07]
FX We thank the reviewers for their valuable feedback. This work was partly
   supported by the Natural Science Foundation of China (61762003), CAS
   "Light of West China" Program (2018QNXZ0024) and First-class discipline
   construction in Ningxia universities (Electronic Science and Technology:
   NXYLXK2017A07).
CR [Anonymous], 2016, ECCV
   [Anonymous], 2018, NIPS
   [Anonymous], 2017, ICCV
   [Anonymous], 2018, ICLR
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Deepak P., 2016, CVP
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao ZL, 2019, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR.2019.00314
   Gatys Leon A., 2016, CVPR
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hensel M, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kautz J., 2018, ECCV
   Lee H., 2018, ECCV, V11129, P100, DOI DOI 10.1007/978-3-030-11009-3_5
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu MY, 2017, ADV NEUR IN, V30
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Matthew A., 2019, CVPR
   Mikaeli E, 2020, VISUAL COMPUT, V36, P1573, DOI 10.1007/s00371-019-01756-w
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Qifeng C., 2017, ICCV
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sangwoo M., 2018, ICRL
   Shen ZQ, 2019, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2019.00380
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z., 2019, ARXIV190508008
   Wayne W., 2019, CVPR
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 37
TC 15
Z9 15
U1 5
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2145
EP 2159
DI 10.1007/s00371-020-01943-0
EA SEP 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000572329100001
DA 2024-07-18
ER

PT J
AU Fernandez-Ramírez, J
   Alvarez-Meza, A
   Pereira, EM
   Orozco-Gutiérrez, A
   Castellanos-Dominguez, G
AF Fernandez-Ramirez, J.
   alvarez-Meza, A.
   Pereira, E. M.
   Orozco-Gutierrez, A.
   Castellanos-Dominguez, G.
TI Video-based social behavior recognition based on kernel relevance
   analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Social behavior recognition; Relevance analysis; Kernel methods; Video
   surveillance
ID DIMENSIONALITY REDUCTION; FRAMEWORK
AB This paper presents a kernel-based relevance analysis for video data to support social behavior recognition. Our approach, termed KRAV, is twofold: (i) A feature ranking based on centered kernel alignment (CKA) is carried out to match social semantic features with the output labels (individual and group behaviors). The employed method is an extension of the conventional CKA to mitigate the imbalance effect of unusual human behaviors. (ii) A classification stage to perform the behavior prediction. For concrete testing, the Israel Institute of Technology social behavior database is employed to assess the KRAVunder a tenfold cross-validation scheme. Attained results showthat the proposed approach for the individual recognition task obtains 0.5925 F1 measure using 50 relevant features. Likewise, for the group recognition task obtains 0.8094 F1 measure using 12 relevant features, which in both cases outperforms state-of-the-art results concerning the classification performance and number of employed features. Also, our video-based approach would assist further social behavior analysis from the set of features selected regarding the recognition of individual profiles and group behaviors.
C1 [Fernandez-Ramirez, J.; Orozco-Gutierrez, A.] Univ Tecnol Pereira, Automat Res Grp, Pereira, Colombia.
   [alvarez-Meza, A.; Castellanos-Dominguez, G.] Univ Nacl Colombia, Signal Proc & Recognit Grp, Manizales, Colombia.
   [Pereira, E. M.] Deloitte & Associados SROC SA, Porto, Portugal.
   [Pereira, E. M.] Deloitte & Associados SROC SA, Regiao, Portugal.
C3 Universidad Tecnologica de Pereira; Universidad Nacional de Colombia;
   Deloitte Touche Tohmatsu Limited; Deloitte Touche Tohmatsu Limited
RP Fernandez-Ramírez, J (corresponding author), Univ Tecnol Pereira, Automat Res Grp, Pereira, Colombia.
EM jorgeferram17@utp.edu.co
RI Orozco, Alvaro/HNJ-0514-2023
OI Alvarez-Meza, Andres/0000-0003-0308-9576; Fernandez-Ramirez, Jorge
   Luis/0000-0001-8606-9604; Orozco-Gutierrez, Alvaro/0000-0002-1167-1446
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Ai S., 2017, P MIPPR, V10609, P337
   alvarez-Meza A, 2014, UNSUPERVISED KERNEL, P335
   Alvarez-Meza AM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00550
   [Anonymous], 2002, ADV NEURAL INF PROCE
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Ben Mabrouk A, 2018, EXPERT SYST APPL, V91, P480, DOI 10.1016/j.eswa.2017.09.029
   Bloom V, 2017, PATTERN RECOGN, V72, P532, DOI 10.1016/j.patcog.2017.07.003
   Brockmeier AJ, 2014, NEURAL COMPUT, V26, P1080, DOI 10.1162/NECO_a_00591
   Chu C, 2011, NEUROIMAGE, V56, P662, DOI 10.1016/j.neuroimage.2010.03.058
   Climent-Perez P, 2013, OPTIMAL JOINT SELECT, P163
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Daza-Santacoloma G, 2009, INTELL AUTOM SOFT CO, V15, P667
   Fan WT, 2019, PATTERN ANAL APPL, V22, P63, DOI 10.1007/s10044-018-00767-y
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622
   Guo YN, 2017, IEEE T SYST MAN CY-S, V47, P617, DOI 10.1109/TSMC.2016.2617465
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048
   Iosifidis A, 2015, NEUROCOMPUTING, V161, P47, DOI 10.1016/j.neucom.2014.10.088
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karim S, 2018, IMAGE PROCESSING BAS, P1725
   Lee JA, 2013, NEUROCOMPUTING, V112, P92, DOI 10.1016/j.neucom.2012.12.036
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Molina-Giraldo S, 2015, ADV INTELL SYST, V318, P273, DOI 10.1007/978-3-319-12610-4_17
   Negin F, 2013, DECISION FOREST BASE, P648
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   Pei LS, 2016, VISUAL COMPUT, V32, P1395, DOI 10.1007/s00371-015-1090-2
   Pereira EM, 2017, NEURAL COMPUT APPL, V28, P2425, DOI 10.1007/s00521-016-2282-z
   Ribeiro A, 2005, INT CONF ACOUST SPEE, P61
   Robnik-Sikonja M, 2003, MACH LEARN, V53, P23, DOI 10.1023/A:1025667309714
   Soviany S, 2017, E-HEALTH BIOENG CONF, P683, DOI 10.1109/EHB.2017.7995516
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Weng JW, 2019, IEEE T CIRC SYST VID, V29, P1077, DOI 10.1109/TCSVT.2018.2818151
   Weng JW, 2018, LECT NOTES COMPUT SC, V11211, P142, DOI 10.1007/978-3-030-01234-2_9
   Xiao YL, 2016, MULTIMED TOOLS APPL, V75, P13041, DOI 10.1007/s11042-015-2569-6
   Zhao F, 2016, INT J COMPUT VISION, V119, P329, DOI 10.1007/s11263-016-0896-3
   Zhao SC, 2018, IEEE T CIRC SYST VID, V28, P1839, DOI 10.1109/TCSVT.2017.2682196
NR 40
TC 6
Z9 6
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1535
EP 1547
DI 10.1007/s00371-019-01754-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700002
DA 2024-07-18
ER

PT J
AU Peng, H
   Xian, CH
   Zhang, YB
AF Peng, Hao
   Xian, Chuhua
   Zhang, Yunbo
TI 3D hand mesh reconstruction from a monocular RGB image
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based modeling; 3D hand mesh reconstruction; Hand dataset; Hand
   pose estimation
AB Most of the existing methods for 3D hand analysis based on RGB images mainly focus on estimating hand keypoints or poses, which cannot capture geometric details of the 3D hand shape. In this work, we propose a novel method to reconstruct a 3D hand mesh from a single monocular RGB image. Different from current parameter-based or pose-based methods, our proposed method directly estimates the 3D hand mesh based on graph convolution neural network (GCN). Our network consists of two modules: the hand localization and mask generation module, and the 3D hand mesh reconstruction module. The first module, which is a VGG16-based network, is applied to localize the hand region in the input image and generate the binary mask of the hand. The second module takes the high-order features from the first and uses a GCN-based network to estimate the coordinates of each vertex of the hand mesh and reconstruct the 3D hand shape. To achieve better accuracy, a novel loss based on the differential properties of the discrete mesh is proposed. We also use professional software to create a large synthetic dataset that contains both ground truth 3D hand meshes and poses for training. To handle the real-world data, we use the CycleGAN network to transform the data domain of real-world images to that of our synthesis dataset. We demonstrate that our method can produce accurate 3D hand mesh and achieve an efficient performance for real-time applications.
C1 [Peng, Hao; Xian, Chuhua] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
   [Zhang, Yunbo] Rochester Inst Technol, Dept Ind & Syst Engn, New York, NY USA.
C3 South China University of Technology; Rochester Institute of Technology
RP Xian, CH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
EM 201820132530@mail.scut.edu.cn; chhxian@scut.edu.cn; ywzeie@rit.edu
OI Peng, Hao/0000-0003-4923-1240; Xian, Chuhua/0000-0001-7656-4652
CR [Anonymous], 1989, Differential Geometry
   [Anonymous], 2018, ANIMATED 3D CHARACTE
   [Anonymous], CHI 13 HUM FACT COMP
   [Anonymous], 2018, FLICKR COMMUNITY
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Chung F. R., 1997, Spectral Graph Theory, V92, DOI DOI 10.1090/CBMS/092
   Defferrard Michael, 2016, ADV NEURAL INFORM PR, P3837, DOI DOI 10.5555/3157382.3157527
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Fan Q, 2018, VISUAL COMPUT, V34, P1145, DOI 10.1007/s00371-018-1546-2
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jang Y, 2015, IEEE T VIS COMPUT GR, V21, P501, DOI 10.1109/TVCG.2015.2391860
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869
   Lassner C., 2017, PROC CVPR IEEE, V2, P3, DOI DOI 10.1109/CVPR.2017.500
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Malik J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173784
   Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Pavlakos G., 2019, ARXIV190405866
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Rahimi A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2009
   Remelli E, 2017, IEEE I CONF COMP VIS, P2554, DOI 10.1109/ICCV.2017.277
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sridhar Srinath, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P319, DOI 10.1109/3DV.2014.37
   Tan V., 2018, INDIRECT DEEP STRUCT
   Tkach A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980226
   Tzionas D, 2016, INT J COMPUT VISION, V118, P172, DOI 10.1007/s11263-016-0895-4
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wu XK, 2018, LECT NOTES COMPUT SC, V11220, P246, DOI 10.1007/978-3-030-01270-0_15
   Yao Pengfei, 2019, ARXIV190310153
   Zhang X., 2019, ARXIV190209305
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
   Zitnik M, 2018, BIOINFORMATICS, V34, P457, DOI 10.1093/bioinformatics/bty294
NR 47
TC 7
Z9 7
U1 1
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2227
EP 2239
DI 10.1007/s00371-020-01908-3
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000548484700001
DA 2024-07-18
ER

PT J
AU Yang, WC
   Gao, CY
AF Yang, Wencong
   Gao, Chengying
TI A completely parallel surface reconstruction method for particle-based
   fluids
SO VISUAL COMPUTER
LA English
DT Article
DE Smoothed particle hydrodynamics; Fluid simulation; Surface
   reconstruction; Narrow band
AB We present a novel surface reconstruction pipeline that significantly improves reconstructing efficiency while preserves high-quality details for particle-based liquid simulation. Our surface reconstruction algorithm is a sort of completely parallel narrow band method. At the beginning of reconstruction, we develop a spatial hashing grid-based strategy to identify surface particles, which is much more precise and simpler than the smoothed color field. Consequently, those precise surface particles ensure accurate extraction of scalar field in the narrow band around surface without any redundancy, which brings great performance improvement for subsequent reconstruction stages. Furthermore, in order to obtain a better computation performance, we carefully analyze the potential race conditions and conditional branches of each reconstruction step between parallel threads and come up with a completely parallel reconstruction method combined with the exclusive prefix sum algorithm. Our method is pretty straightforward to implement. Experimental results demonstrate that our method runs up to dozen times faster than the state-of-the-art of narrow band-based fluid surface reconstruction, especially for large-scale particle-based fluid.
C1 [Yang, Wencong; Gao, Chengying] Sun Yat Sen Univ, Sch Data Sci & Comp Sci, Guangzhou, Peoples R China.
C3 Sun Yat Sen University
RP Gao, CY (corresponding author), Sun Yat Sen Univ, Sch Data Sci & Comp Sci, Guangzhou, Peoples R China.
EM mcsgcy@mail.sysu.edu.cn; yangwc3@mail2.sysu.edu.cn
FU Natural Science Foundation of Guangdong Province, China
   [2019A1515011075]
FX This work was supported by the Natural Science Foundation of Guangdong
   Province, China (Grant No. 2019A1515011075).
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   Akinci G, 2012, COMPUT GRAPH FORUM, V31, P1797, DOI 10.1111/j.1467-8659.2012.02096.x
   Akinci G, 2013, WSCG 2013, FULL PAPERS PROCEEDINGS, P195
   [Anonymous], THESIS
   [Anonymous], 2011, PROC 27 SPRING C COM
   [Anonymous], 2003, P ACM SIGGRAPH EUR S
   [Anonymous], 2012, WORKSH VIRT REAL INT, DOI DOI 10.2312/PE/VRIPHYS/VRIPHYS12/061-068
   [Anonymous], THESIS
   [Anonymous], TECH REP
   Bhattacharya H, 2015, IEEE T VIS COMPUT GR, V21, P315, DOI 10.1109/TVCG.2014.2362546
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Canezin F, 2016, P ACM SIGGR EUR S CO, P37
   Gao* M., 2018, ACM T GRAPH P SIGGRA, V37
   Hadi NA, 2019, J PHYS CONF SER, V1192, DOI 10.1088/1742-6596/1192/1/012006
   Hadi NA, 2019, PROCEEDINGS OF 2019 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION TECHNOLOGY (ICIIT 2019), P16, DOI 10.1145/3321454.3321476
   Houston B., 2004, ACM SIGGRAPH TECHNIC, P137, DOI DOI 10.1145/1186223.1186394
   Ju T., 2006, P 14 PAC C COMP GRAP, V3
   Koren Y, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P121, DOI 10.1109/INFVIS.2003.1249017
   Lee H., 2004, 14 INT C ART REAL TE
   Lee W., 2017, INT J ADV SOFT COMPU, V9
   Loop C.T, 2018, US Patent, Patent No. [9,984,498, 9984498]
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Manson J, 2010, COMPUT GRAPH FORUM, V29, P377, DOI 10.1111/j.1467-8659.2009.01607.x
   Nielsen MB, 2006, J SCI COMPUT, V26, P261, DOI 10.1007/s10915-005-9062-8
   Nielsen MB, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289607
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Velasco F., 2001, P VIS MOD VIS C, P151
   Wang XK, 2017, 2017 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P118, DOI 10.1109/CW.2017.30
   Wiemann T, 2018, 2018 SECOND IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC), P278, DOI 10.1109/IRC.2018.00059
   Wu W, 2017, VISUAL COMPUT, V33, P1429, DOI 10.1007/s00371-016-1289-x
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Zhou K, 2011, IEEE T VIS COMPUT GR, V17, P669, DOI 10.1109/TVCG.2010.75
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 34
TC 5
Z9 7
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2313
EP 2325
DI 10.1007/s00371-020-01898-2
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000545934000001
DA 2024-07-18
ER

PT J
AU Agarwal, R
   Jalal, AS
   Arya, KV
AF Agarwal, Rohit
   Jalal, Anand Singh
   Arya, K. V.
TI Local binary hexagonal extrema pattern (LBH<sub>X</sub>EP): a new
   feature descriptor for fake iris detection
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; Iris; Liveness detection; Spoof; Feature descriptor
ID LIVENESS DETECTION; FINGERPRINT; CLASSIFICATION; SCHEME
AB Security agencies frequently use biometric traits for automatic recognition of a person. The human iris is the most hopeful biometric authentication that can accurately identify a person from their exclusive features. However, in recent years, different types of spoofing attacks are used to violate the security of a biometric system. Biometrics liveness detection system used to recognize persons in a fast and trustworthy way through the use of unique biological distinctiveness. Presentation of a manufactured article of a human iris in the form of photo attack and contact lens attack could hamper the projected policy of a biometric system. The quality of real and fake iris images shows different textural characteristics. In this paper, we have proposed a novel and proficient feature descriptor, i.e., local binary hexagonal extrema pattern for fake iris detection. The proposed descriptor exploits the relationship between the center pixel and its Hexa neighbor. Hexagonal shape using "six-neighbor approach" is preferable to the rectangular structure due to its higher symmetry, consistent connectivity, and efficient use of space. The proposed consideration also solves the "curse of dimensionality" problem in liveness detection. The proposed descriptor is evaluated on ATVS-FIr DB and IIIT-D CLI databases for iris liveness detection and show promising performance for liveness detection in terms green, brown, etc. of accuracy and average error rate.
C1 [Agarwal, Rohit; Jalal, Anand Singh] GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, Uttar Pradesh, India.
   [Arya, K. V.] ABV IIITM, ICT, Gwalior 474015, Madhya Pradesh, India.
C3 GLA University; ABV-Indian Institute of Information Technology &
   Management, Gwalior
RP Agarwal, R (corresponding author), GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, Uttar Pradesh, India.
EM rohit.agrwal@gla.ac.in; asjalal@gla.ac.in; kvarya@iiitm.ac.in
OI Jalal, Anand/0000-0002-7469-6608; , Rohit/0000-0003-4192-726X
CR [Anonymous], 2015, INT J SCI TECHNICAL
   [Anonymous], 301071 ISOIEC CD
   Bhogal APS, 2017, 2017 5 INT WORKSH BI, P1
   Chatterjee, 2019, P SPRING C SEC PRIV, V11637, DOI [10.1007/978-3-030-24900-7_7, DOI 10.1007/978-3-030-24900-7_7]
   Chen CJ, 2018, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW.2018.00011
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chen R, 2012, PATTERN RECOGN LETT, V33, P1513, DOI 10.1016/j.patrec.2012.04.002
   Choudhary M, 2019, FUTURE GENER COMP SY, V101, P1259, DOI 10.1016/j.future.2019.07.003
   Connell J, 2013, INT CONF ACOUST SPEE, P8692, DOI 10.1109/ICASSP.2013.6639363
   Daugman J, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P715, DOI 10.1016/B978-0-12-374457-9.00025-1
   Dubey SR, 2015, IEEE SIGNAL PROC LET, V22, P1215, DOI 10.1109/LSP.2015.2392623
   Fathy WSA, 2018, WIRELESS PERS COMMUN, V102, P2331, DOI 10.1007/s11277-017-5089-z
   Galbally J., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P271, DOI 10.1109/ICB.2012.6199819
   Galbally J., 2016, Biometrics and Forensics (IWBF), 2016 4th International Workshop on, P1, DOI DOI 10.1109/IWBF.2016.7449676
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   He X., 2008, Chinese Conference on Pattern Recognition (CCPR), P1
   He XF, 2007, LECT NOTES COMPUT SC, V4642, P540
   He XF, 2009, LECT NOTES COMPUT SC, V5558, P1132
   He ZF, 2009, LECT NOTES COMPUT SC, V5558, P1080
   Hu Y, 2016, PATTERN RECOGN LETT, V82, P242, DOI 10.1016/j.patrec.2015.10.010
   Hui Zhang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4279, DOI 10.1109/ICPR.2010.1040
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Kannala J, 2012, INT C PATT RECOG, P1363
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Liu M, 2020, IEEE T FUZZY SYST, V28, P92, DOI 10.1109/TFUZZ.2019.2912576
   Long M, 2019, CMC-COMPUT MATER CON, V58, P493, DOI 10.32604/cmc.2019.04378
   Nguyen K, 2017, PATTERN RECOGN, V72, P123, DOI 10.1016/j.patcog.2017.05.021
   Nosaka R, 2011, LECT NOTES COMPUT SC, V7088, P82, DOI 10.1007/978-3-642-25346-1_8
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V, 2008, INT C PATT RECOG, P3596
   Pala F, 2017, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP.2017.8296254
   Raghavendra R, 2015, IEEE T INF FOREN SEC, V10, P703, DOI 10.1109/TIFS.2015.2400393
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Yadav D, 2014, IEEE T INF FOREN SEC, V9, P851, DOI 10.1109/TIFS.2014.2313025
   Yan C, 2005, PROCEEDINGS OF THE 2005 IEEE INTERNATIONAL CONFERENCE ON NATURAL LANGUAGE PROCESSING AND KNOWLEDGE ENGINEERING (IEEE NLP-KE'05), P769
   Zhang HY, 2016, IEEE IMAGE PROC, P1, DOI 10.1109/ICIP.2016.7532307
NR 38
TC 22
Z9 24
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1357
EP 1368
DI 10.1007/s00371-020-01870-0
EA JUN 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539183200001
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Tan, F
   Wang, SF
   Yin, BC
AF Zhang, Yong
   Tan, Fei
   Wang, Shaofan
   Yin, Baocai
TI 3D human body skeleton extraction from consecutive surfaces using a
   spatial-temporal consistency model
SO VISUAL COMPUTER
LA English
DT Article
DE Human body skeletons; Consecutive surfaces; Spatio-temporal consistency
   model
ID POINT CLOUDS; OPTIMIZATION
AB Current approaches of human body skeleton extraction mainly suffer from following problems: insufficient temporal and spatial continuity, unrobust to background, ambient noise, etc. This paper proposes a three-dimensional human body skeleton extraction method from consecutive meshes. We extract the consistent skeletons from consecutive surfaces based on shape segmentation and skeleton sequences; then, we present a spatiotemporal skeleton optimization model to adjust the skeleton sequences. Experiments on multiview images captured from a light field device demonstrate that our method captures more complete and accurate skeletons compared to state-of-the-art methods.
C1 [Zhang, Yong; Tan, Fei; Wang, Shaofan; Yin, Baocai] Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Wang, SF (corresponding author), Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing 100124, Peoples R China.
EM wangshaofan@bjut.edu.cn
RI tan, fei/KOD-4737-2024; Zhang, Yong/AAW-8880-2021
OI Zhang, Yong/0000-0001-6650-6790
FU National Natural Science Foundation of China [61772049, 61632006,
   61876012, U19B2039, 61906011]; Beijing Natural Science Foundation
   [4202003]; Beijing Municipal Science and Technology Project
   [Z171100004417023]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61772049, 61632006, 61876012, U19B2039, 61906011, the
   Beijing Natural Science Foundation under Grant 4202003, and the Beijing
   Municipal Science and Technology Project under Grant Z171100004417023.
CR Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Bærentzen JA, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601226
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Cao J., 2010, SHAP MOD INT C SMI 2, P187, DOI [DOI 10.1109/SMI.2010.25, 10.1109/SMI.2010.25]
   Chu CW, 2003, PROC CVPR IEEE, P475
   Chuang JH, 2004, COMPUT GRAPH-UK, V28, P907, DOI 10.1016/j.cag.2004.08.004
   Chuang M, 2011, COMPUT GRAPH FORUM, V30, P1750, DOI 10.1111/j.1467-8659.2011.01899.x
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Dey TK, 2006, S GEOMETRY PROCESSIN, P143
   Fedor M., 2003, P 19 SPRING C COMPUT, P203
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Jiang W, 2013, GRAPH MODELS, V75, P137, DOI 10.1016/j.gmod.2012.10.005
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kleiman Y, 2019, COMPUT GRAPH FORUM, V38, P7, DOI 10.1111/cgf.13389
   Le BH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601161
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   Liang J, 2012, PROC CVPR IEEE, P214, DOI 10.1109/CVPR.2012.6247678
   Mehrizi R, 2018, IEEE INT CONF AUTOMA, P485, DOI 10.1109/FG.2018.00078
   Mei J, 2017, INT J GEOGR INF SCI, V31, P999, DOI 10.1080/13658816.2016.1264075
   Pan H. W., 2011, INT C VIRT REAL CONT, P243
   Pang ZQ, 2015, COMPUT ANIMAT VIRT W, V26, P301, DOI 10.1002/cav.1658
   Pantuwong N, 2012, COMPUT ANIMAT VIRT W, V23, P125, DOI 10.1002/cav.1429
   Schwarcz S, 2018, INT C PATT RECOG, P2326, DOI 10.1109/ICPR.2018.8545631
   Sharf A, 2007, COMPUT GRAPH FORUM, V26, P323, DOI 10.1111/j.1467-8659.2007.01054.x
   Storti D. W., 1997, Proceedings. Fourth Symposium on Solid Modeling and Applications, P141, DOI 10.1145/267734.267771
   Straka M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.69
   TAGLIASACCHI A, 2009, ACM T GRAPHIC, V28
   Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x
   Vejanovski R, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P45
   Wang K., 2015, MULTIMED TOOLS APPL, V75, P1
   Zhang Dejia, 2015, Journal of Computer Aided Design & Computer Graphics, V27, P1247
   Zhang Y, 2018, ISPRS J PHOTOGRAMM, V143, P124, DOI 10.1016/j.isprsjprs.2018.04.016
   Zheng Q, 2010, COMPUT GRAPH FORUM, V29, P635, DOI 10.1111/j.1467-8659.2009.01633.x
   Zimovnov Andrey, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P666
NR 38
TC 2
Z9 3
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1045
EP 1059
DI 10.1007/s00371-020-01851-3
EA MAY 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000533801800001
DA 2024-07-18
ER

PT J
AU Dogra, J
   Jain, S
   Sood, M
AF Dogra, Jyotsna
   Jain, Shruti
   Sood, Meenakshi
TI Glioma extraction from MR images employing Gradient Based Kernel
   Selection Graph Cut technique
SO VISUAL COMPUTER
LA English
DT Article
DE Gradient based kernel selection; Graph cut; Shrinkage behavior;
   High-grade glioma; Low-grade glioma
ID BRAIN-TUMOR SEGMENTATION; AUTOMATED SEGMENTATION; NETWORKS; METRICS;
   LESIONS; MODEL
AB Medical imaging is one of the most daunting, challenging, and emerging research topics in image processing. Segmenting the glioma from the brain magnetic resonance images (MRI) is an important and demanding task, as it assists the medical experts for the disease diagnosis process. Recent research methods in image segmentation have highlighted the prospective of graph-based techniques for medical applications. As graph cut (GC) method is interactive in nature, it requires manual selection of the initial kernels for processing. The popularity of the GC method is limited by the occurrence of small cuts due to its shrinkage behavior leading to inaccurate extraction causing erroneous regions. This paper addresses the open research issue of shrinkage behavior by proposing the gradient based kernel selection (GBKS) GC method emphasizing on the directive inclination of the intensity scales. The proposed technique aids in the initialization of GC, removes the shrinkage problem, and locates the tumor in brain images without any human intervention. The performance results of the proposed GBKS GC method are evaluated on high-grade glioma and low-grade glioma MRI images and are analyzed and compared by using various measures. All the results present a remarkable improvement with GBKS GC technique over other existing techniques.
C1 [Dogra, Jyotsna; Jain, Shruti; Sood, Meenakshi] Jaypee Univ Informat Technol, Dept Elect & Commun Engn, Solan, Himachal Prades, India.
C3 Jaypee University of Information Technology
RP Dogra, J (corresponding author), Jaypee Univ Informat Technol, Dept Elect & Commun Engn, Solan, Himachal Prades, India.
EM jyotsnadogra1989@gmail.com; shruti.jain@juit.ac.in;
   meenakshi.sood@juit.ac.in
RI Jain, Shruti/JFK-9972-2023; Jain, Shruti/C-8263-2015
OI Jain, Shruti/0000-0002-7538-0584; Dogra, Jyotsna/0000-0001-8135-7535
CR Alemán-Flores M, 2007, J MATH IMAGING VIS, V28, P81, DOI 10.1007/s10851-007-0015-8
   [Anonymous], P BRATS MICCAI
   Averbuch-Elor H, 2018, VISUAL COMPUT, V34, P1761, DOI 10.1007/s00371-017-1467-5
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bauer S, 2013, PHYS MED BIOL, V58, pR97, DOI 10.1088/0031-9155/58/13/R97
   Benhabiles H, 2010, VISUAL COMPUT, V26, P1451, DOI 10.1007/s00371-010-0494-2
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chen Xinjian, 2018, IEEE Rev Biomed Eng, V11, P112, DOI 10.1109/RBME.2018.2798701
   Despotovic I, 2015, COMPUT MATH METHOD M, V2015, DOI 10.1155/2015/450341
   Dogra Jyotsna, 2018, Procedia Computer Science, V132, P775, DOI 10.1016/j.procs.2018.05.089
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Gao L, 2012, MED PHYS, V39, P3299, DOI 10.1118/1.4718565
   Grady L, 2005, LECT NOTES COMPUT SC, V3750, P773, DOI 10.1007/11566489_95
   Hanaoka S, 2011, LECT NOTES COMPUT SC, V6893, P554, DOI 10.1007/978-3-642-23626-6_68
   Hao Z, 2012, IEEE IMAGE PROC, P2817, DOI 10.1109/ICIP.2012.6467485
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Ilunga-Mbuyamba E, 2017, NEUROCOMPUTING, V220, P84, DOI 10.1016/j.neucom.2016.07.057
   Ishikawa Hiroshi, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2993, DOI 10.1109/CVPRW.2009.5206689
   Jermyn IH, 2001, IEEE T PATTERN ANAL, V23, P1075, DOI 10.1109/34.954599
   Jiang J, 2013, COMPUT MED IMAG GRAP, V37, P512, DOI 10.1016/j.compmedimag.2013.05.007
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Kanas VG, 2015, BIOMED SIGNAL PROCES, V22, P19, DOI 10.1016/j.bspc.2015.06.004
   Kolmogorov V, 2005, IEEE I CONF COMP VIS, P564
   Ledig C, 2015, MED IMAGE ANAL, V21, P40, DOI 10.1016/j.media.2014.12.003
   Li RJ, 2014, LECT NOTES COMPUT SC, V8675, P305, DOI 10.1007/978-3-319-10443-0_39
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Li YH, 2016, ARTIF INTELL MED, V73, P1, DOI 10.1016/j.artmed.2016.08.004
   Linares OC, 2019, VISUAL COMPUT, V35, P1461, DOI 10.1007/s00371-018-1511-0
   Liu XX, 2014, LECT NOTES COMPUT SC, V8675, P97, DOI 10.1007/978-3-319-10443-0_13
   Liu X, 2005, P ANN INT IEEE EMBS, P7433
   Madabhushi A, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P601, DOI 10.1109/ISBI.2002.1029329
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Menze BH, 2010, LECT NOTES COMPUT SC, V6362, P151
   Pan RJ, 2016, VISUAL COMPUT, V32, P601, DOI 10.1007/s00371-015-1076-0
   Park C, 2013, IEEE T PATTERN ANAL, V35, P669, DOI 10.1109/TPAMI.2012.163
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Pereira S, 2016, IEEE T MED IMAGING, V35, P1240, DOI 10.1109/TMI.2016.2538465
   Popuri K, 2012, INT J COMPUT ASS RAD, V7, P493, DOI 10.1007/s11548-011-0649-2
   Prastawa M, 2004, MED IMAGE ANAL, V8, P275, DOI 10.1016/j.media.2004.06.007
   Rao B. D., 2017, P POW ADV COMP TECHN, P1
   ROSENFEL.A, 1966, J ACM, V13, P471
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rother C, 2009, PROC CVPR IEEE, P1382, DOI 10.1109/CVPRW.2009.5206739
   Schmidt P, 2012, NEUROIMAGE, V59, P3774, DOI 10.1016/j.neuroimage.2011.11.032
   Shan J., 2008, 2008 19 INT C PATT R, P1, DOI DOI 10.1109/ICPR.2008.4761336
   Shan J, 2012, ULTRASOUND MED BIOL, V38, P262, DOI 10.1016/j.ultrasmedbio.2011.10.022
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shen JB, 2017, IEEE T IMAGE PROCESS, V26, P4911, DOI 10.1109/TIP.2017.2722691
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sinop AK, 2007, IEEE I CONF COMP VIS, P1016, DOI 10.1109/iccv.2007.4408927
   Sobhaninia Z., 2018, Brain tumor segmentation using deep learning by type specific sorting of images
   Szwarc P, 2015, COMPUT MED IMAG GRAP, V46, P178, DOI 10.1016/j.compmedimag.2015.06.002
   Tustison NJ, 2015, NEUROINFORMATICS, V13, P209, DOI 10.1007/s12021-014-9245-2
   Vicente S, 2009, IEEE I CONF COMP VIS, P755, DOI 10.1109/ICCV.2009.5459287
   Wan CK, 2008, VISUAL COMPUT, V24, P373, DOI 10.1007/s00371-007-0195-7
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Ye DH, 2013, LECT NOTES COMPUT SC, V8149, P606, DOI 10.1007/978-3-642-40811-3_76
   Yuan CA, 2018, J ENG-JOE, P1704, DOI 10.1049/joe.2018.8320
   Zhang S, 2018, I S BIOMED IMAGING, P1, DOI 10.1109/ISBI.2018.8363510
   Zhao L, 2013, MULTIMODAL BRAIN TUM, V51, P54
   Zheng C, 2018, VISUAL COMPUT, V2018, P1
   Zikic D., 2014, Proc. MICCAI-BRATS, V36, P36
   Zikic D, 2012, LECT NOTES COMPUT SC, V7512, P369, DOI 10.1007/978-3-642-33454-2_46
NR 71
TC 11
Z9 12
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 875
EP 891
DI 10.1007/s00371-019-01698-3
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100002
DA 2024-07-18
ER

PT J
AU Lian, GY
   Zhang, K
AF Lian, Guanyu
   Zhang, Kang
TI Transformation of portraits to Picasso's cubism style
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing techniques; Cubism; Generative art; Deep learning
AB This paper presents an approach to the transformation of portrait photographs to Picasso's cubism style using deep learning and image processing techniques. We obtain the side-view face by rotating the face model constructed from a frontal portrait image 90 degrees and then replace the left half of the portrait by the side-view face. Our approach is applicable to online transformation of selfie photographs and potentially extendable to broader categories of images and artistic styles.
C1 [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.
   [Zhang, Kang] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.
C3 University of Texas System; University of Texas Dallas; Macau University
   of Science & Technology
EM lian_seu@163.com; kzhang@utdallas.edu
OI Lian, Guanyu/0000-0001-9149-2802
CR [Anonymous], 2017, Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses
   [Anonymous], 2017, PROC IEEE C COMPUT V
   [Anonymous], 2015, P IEEE INT C COMP VI
   [Anonymous], 2015, P INT C LEARN REP IC
   [Anonymous], 2016, ICML
   Arpa S, 2012, COMPUT GRAPH-UK, V36, P991, DOI 10.1016/j.cag.2012.06.003
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chen L. C., 2015, Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs, P357, DOI [10.1080/17476938708814211, DOI 10.1080/17476938708814211]
   Collomosse JP, 2003, IEEE T VIS COMPUT GR, V9, P443, DOI 10.1109/TVCG.2003.1260739
   Cristinacce D., 2007, BMVC
   Fishwick P, 2006, AESTHETIC COMPUTING, P2
   Gatys LA, 2015, COMPUT SCI, V35, P10005
   Huang X., 2017, ICCV
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jing Y., 2017, ARXIV170504058 CORR
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lea D, 1997, INT CON DISTR COMP S, P421
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li Y., 2017, Adv. Neural Inf. Process. Syst
   Lian G., 2016, SIGGRAPH ASIA 2016, P53, DOI 10.1145/3005274.3005306.
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   [龙晓苑 Long Xiaoyuan], 2005, [计算机辅助设计与图形学学报, Journal of Compute-Aided Design and Graphics], V17, P623
   Maciejewski R, 2008, IEEE COMPUT GRAPH, V28, P62, DOI 10.1109/MCG.2008.35
   Osborne H, 1978, ABSTRACT SKILLS 20 C, P87
   Qu YD, 2005, IMAGE VISION COMPUT, V23, P11, DOI 10.1016/j.imavis.2004.07.003
   Sadhu S, 2013, PROC TECH, V10, P356, DOI 10.1016/j.protcy.2013.12.371
   Seitz SM, 2003, IEEE COMPUT GRAPH, V23, P16, DOI 10.1109/MCG.2003.1242377
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   SOBEL I, 1978, COMPUT VISION GRAPH, V8, P127, DOI 10.1016/S0146-664X(78)80020-3
   Tao WY, 2014, 2014 IEEE/ACIS 13TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS), P201, DOI 10.1109/ICIS.2014.6912134
   Wang Z, 1999, IEEE T CIRCUITS-II, V46, P78, DOI 10.1109/82.749102
   Xuemei Y, 2002, PICASSO ART THEORY
   Zhang H., 2017, MULTISTYLE GENERATIV
   Zhang K, 2013, COMPUT AESTHET, V45, P243
   Zheng Y, 2015, VISUAL COMPUT, V31, P589, DOI 10.1007/s00371-014-0985-7
NR 37
TC 3
Z9 3
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 799
EP 807
DI 10.1007/s00371-019-01661-2
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800011
DA 2024-07-18
ER

PT J
AU Chen, ST
   Zhang, JL
   Jin, M
AF Chen, Suting
   Zhang, Jinglin
   Jin, Meng
TI A simplified ICA-based local similarity stereo matching
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo matching; Cost aggregation; Independent component correlation;
   Region-wise loss function
AB Since the existing stereo matching methods may fail in the regions of non-textures, boundaries and tiny details, a simplified independent component correlation algorithm (ICA)-based local similarity stereo matching algorithm is proposed. In order to improve the DispNetC, the proposed algorithm first offers the simplified independent component correlation algorithm (SICA) cost aggregation. Then, the algorithm introduces the matching cost volume pyramid, which simplifies the pre-processing process for the ICA. Also, the SICA loss function is defined. Next, the region-wise loss function combined with the pixel-wise loss function is defined as a local similarity loss function to improve the spatial structure of the disparity map. Finally, the SICA loss function is combined with the local similarity loss function, which is defined to estimate the disparity map and to compensate the edge information of the disparity map. Experimental results on KITTI dataset show that the average absolute error of the proposed algorithm is about 37% lower than that of the DispNetC, and its runtime consuming is about 0.6 s lower than that of GC-Net.
C1 [Chen, Suting; Zhang, Jinglin; Jin, Meng] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Meteorol Observat & Informat Proc, Nanjing 210044, Peoples R China.
   [Chen, Suting] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology
RP Chen, ST (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Meteorol Observat & Informat Proc, Nanjing 210044, Peoples R China.; Chen, ST (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
EM sutingchen@nuist.edu.cn; 574428734@qq.com; jinmeng0722@gmail.com
FU National Natural Science Foundation of China [61906097]
FX This work is supported by National Natural Science Foundation of China
   (61906097).
CR [Anonymous], AAAI
   [Anonymous], AS C COMP VIS ACCV
   Baldacci A, 2016, VISUAL COMPUT, V32, P1605, DOI 10.1007/s00371-015-1144-5
   Chang R, 2011, IEEE IMAGE PROC
   Cheng FY, 2015, PATTERN RECOGN, V48, P2269, DOI 10.1016/j.patcog.2015.01.002
   Du Y, 2018, LECT NOTES COMPUT SC, V11220, P388, DOI 10.1007/978-3-030-01270-0_23
   Fischer P., 2015, ICCV
   GEIGER A, 2010, AS C COMP VIS ACCV20
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Hwang J.-J., 2018, ECCV
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   TAO H, 2001, INT C COMP VIS ICCV2
   [芮挺 Rui Ting], 2005, [模式识别与人工智能, Pattern recognition and artificial intelligence], V18, P124
   Wang ES, 2017, C IND ELECT APPL, P2017, DOI 10.1109/ICIEA.2017.8283169
   Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zagoruyko S., 2015, P CVPR
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhou C., 2017, P ICCV
   Zhu SP, 2017, VISUAL COMPUT, V33, P1087, DOI 10.1007/s00371-016-1264-6
NR 28
TC 6
Z9 7
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 411
EP 419
DI 10.1007/s00371-020-01811-x
EA FEB 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000520062900001
OA hybrid
DA 2024-07-18
ER

PT J
AU Gong, KC
   Cao, ZG
   Xiao, Y
   Fang, ZW
AF Gong, Kaicheng
   Cao, Zhiguo
   Xiao, Yang
   Fang, Zhiwen
TI Abrupt-motion-aware lightweight visual tracking for unmanned aerial
   vehicles
SO VISUAL COMPUTER
LA English
DT Article
DE Abrupt motion estimation; Object tracking; Keypoint matching;
   Correlation filter
ID OBJECT TRACKING
AB Visual tracking for unmanned aerial vehicles (UAVs) is a hot research topic for the wide applications of UAVs. As UAVs are high-altitude and high-freedom platforms, small targets in UAV tracking sequences are often under the attribute of large-scale location change due to the abrupt motion of the platform. Currently, many visual tracking methods based on the local search hypothesis have been widely researched on low-speed moving platforms. However, these methods cannot be directly used on the UAV platform, because targets will appear in any position of the new frame. To address this problem, we propose an abrupt-motion-aware visual tracking method in this paper. Because of the high power consumption of deep learning models, the proposed method is a lightweight tracker for small UAVs without the deep learning framework. Our method consists of three major components: abrupt motion estimation, object tracking and model updating. Abrupt motion often leads to abnormal changes in the response map of trackers. Thus, by analyzing the changes of tracking response maps, the abrupt motion can be detected efficiently. When abrupt motion happens, keypoint matching will be adaptively implemented to estimate the ego-motion and skipped otherwise. Then, the target location is predicted by the correlation filter tracker in a local search region. Moreover, according to the confidence analysis, an adaptive model update strategy is designed to alleviate the model noise caused by the short-term occlusion. Experimental results confirm the robustness and the accuracy of our method on challenging sequences and show the comparative performance of the proposed method against several state-of-the-art lightweight methods.
C1 [Gong, Kaicheng; Cao, Zhiguo; Xiao, Yang] Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multispectral Informat, Sch Automat, Wuhan, Peoples R China.
   [Fang, Zhiwen] Hunan Univ Humanities Sci & Technol, Sch Energy & Mech Elect Engn, Loudi, Peoples R China.
C3 Huazhong University of Science & Technology; Hunan University Of
   Humanities, Science & Technology
RP Cao, ZG (corresponding author), Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multispectral Informat, Sch Automat, Wuhan, Peoples R China.
EM zgcao@hust.edu.cn
RI yang, xiao/HJI-7815-2023; XIAO, YANG/GPW-5529-2022; Yang,
   Xiao/JCD-7233-2023; xiao, yang/JCD-7195-2023; Cao, Zhiguo/ABA-6803-2020
OI Cao, Zhiguo/0000-0002-9223-1863
FU National Natural Science Foundation of China [61702182]; Hunan
   Provincial Natural Science Foundation of China [2018JJ3254]
FX This work is jointly supported by the National Natural Science
   Foundation of China (Grant No. 61702182) and the Hunan Provincial
   Natural Science Foundation of China (Grant No. 2018JJ3254).
CR [Anonymous], 2017, ARXIV170305020
   [Anonymous], 2014, ECCV
   [Anonymous], 2015, BRIT MACH VIS C
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Belagiannis V, 2012, LECT NOTES COMPUT SC, V7575, P842, DOI 10.1007/978-3-642-33765-9_60
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Fang ZW, 2016, PROC SPIE, V9988, DOI 10.1117/12.2241354
   Fang ZW, 2016, IEEE T IMAGE PROCESS, V25, P4116, DOI 10.1109/TIP.2016.2579311
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Floreano D, 2015, NATURE, V521, P460, DOI 10.1038/nature14542
   Grandvalet Y., 2005, CAP, V367, P281
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Hare S, 2012, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2012.6247889
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Liang PP, 2016, IEEE SIGNAL PROC LET, V23, P949, DOI 10.1109/LSP.2016.2556706
   Lin SFD, 2015, IET IMAGE PROCESS, V9, P959, DOI 10.1049/iet-ipr.2014.0666
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Liu T, 2015, IEEE SIGNAL PROC LET, V22, P1452, DOI 10.1109/LSP.2014.2365363
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2015, IEEE GEOSCI REMOTE S, V12, P43, DOI 10.1109/LGRS.2014.2325970
   Xing JL, 2013, IEEE I CONF COMP VIS, P665, DOI 10.1109/ICCV.2013.88
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang K, 2014, IEEE GEOSCI REMOTE S, V11, P469, DOI 10.1109/LGRS.2013.2267771
   Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 34
TC 3
Z9 3
U1 0
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 371
EP 383
DI 10.1007/s00371-020-01805-9
EA JAN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000510306100001
DA 2024-07-18
ER

PT J
AU Hashimoto, N
   Yoshimura, K
AF Hashimoto, Naoki
   Yoshimura, Kazuma
TI Radiometric compensation for non-rigid surfaces by continuously
   estimating inter-pixel correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Radiometric compensation; Inter-pixel correspondence; Non-rigid surface;
   Response function; ProCam system
AB In recent years, radiometric compensation techniques for realizing ideal image projection, even for a patterned surface, have attracted attention. In radiometric compensation, the influence of the pattern can be canceled based on the input and output relation, referred to as the response function, which is determined by a projector-camera system. However, since the response function strongly depends on the inter-pixel correspondence between the projector and the camera, the projection surface is restricted to being a rigid body. In the present study, we achieve radiometric compensation for a non-rigid surface, such as a swaying curtain in a normal room, by estimating the inter-pixel correspondence in real time. The reflectance of the projection surface is estimated based on observation of the projected image by the camera without using special equipment, and the offset of the correspondence is estimated based on its validity. We evaluate the effectiveness of the proposed method for various combinations of curtain patterns and projected images.
C1 [Hashimoto, Naoki; Yoshimura, Kazuma] Univ Electro, Communicat, W9-603, 1-5-1, Chofugaoka, Chofu, Tokyo 1828585, Japan.
RP Hashimoto, N (corresponding author), Univ Electro, Communicat, W9-603, 1-5-1, Chofugaoka, Chofu, Tokyo 1828585, Japan.
EM naoki@cs.uec.ac.jp
FU Japan Society for the Promotion of Science, Kakenhi [JP16K00267,
   JP19H04152]
FX This study was funded by the Japan Society for the Promotion of Science,
   Kakenhi Grant Numbers JP16K00267 and JP19H04152.
CR Amano T., 2014, COMPUTER VISION PATT, V2014, P443
   [Anonymous], 2015, 22 INT DISPL WORKSH
   [Anonymous], 2003, IEEE INT WORKSH PROJ
   [Anonymous], 2005, Spatial Augmented Reality: Merging Real and Virtual Worlds
   Ashdown M., 2006, 2006 C COMP VIS PATT, P6
   Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17
   Bimber O., 2008, ACM SIGGRAPH 2008 CL
   Cotting D., 2004, 3 IEEE ACM INT S MIX
   Fujii K, 2005, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.2005.41
   Grundhöfer A, 2015, IEEE T IMAGE PROCESS, V24, P5086, DOI 10.1109/TIP.2015.2478388
   Grundhöfer A, 2008, IEEE T VIS COMPUT GR, V14, P97, DOI 10.1109/TVCG.2007.1052
   Hashimoto N., 2015, ACM SIGGRAPH2015 POS
   Hashimoto N., 2017, ACM SIGGRAPH 2017 PO, P72
   HASTINGSJAMES R, 1969, P I ELECTR ENG, V116, P2057, DOI 10.1049/piee.1969.0378
   Lee J.C., 2004, 17 ANN ACM S US INT
   Li YQ, 2018, COMPUT GRAPH FORUM, V37, P365, DOI 10.1111/cgf.13368
   Majumder A, 2005, ACM T GRAPHIC, V24, P118, DOI 10.1145/1037957.1037964
   Majumder A, 2004, IEEE T VIS COMPUT GR, V10, P177, DOI 10.1109/TVCG.2004.1260769
   Majumder A, 2000, IEEE VISUAL, P117, DOI 10.1109/VISUAL.2000.885684
   Mihara S, 2014, IEEE T CIRC SYST VID, V24, P1631, DOI 10.1109/TCSVT.2014.2309832
   Nakamura T., 2012, ACM SIGGRAPH2012 POS
   Peng XH, 2012, INT C INTEL HUM MACH, P7, DOI 10.1109/IHMSC.2012.98
   Sajadi B, 2010, LECT NOTES COMPUT SC, V6314, P72, DOI 10.1007/978-3-642-15561-1_6
   Wang D., 2005, P IEEE COMP SOC C CO, P100
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wetzstein G, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P391, DOI 10.1109/PG.2007.47
   Yoshida T., 2003, VSMM P INT C VIRT SY, V3, P1
   Zhang S., 2004, IEEE computer Vision Pattern Recognition Workshop (CVPRW'04), V3, P28, DOI DOI 10.1109/CVPR.2004.86
   Zollmann S, 2007, EG SHORT PAPERS
   Zollmann S., 2007, JVRB J VIRTUAL REAL, V4
NR 30
TC 4
Z9 5
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 175
EP 187
DI 10.1007/s00371-019-01790-8
EA JAN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574081100003
DA 2024-07-18
ER

PT J
AU Tan, JS
   Liao, IY
   Venkat, I
   Belaton, B
   Jayaprakash, PT
AF Tan, Joi San
   Liao, Iman Yi
   Venkat, Ibrahim
   Belaton, Bahari
   Jayaprakash, P. T.
TI Computer-aided superimposition via reconstructing and matching 3D faces
   to 3D skulls for forensic craniofacial identifications
SO VISUAL COMPUTER
LA English
DT Article
DE Craniofacial superimposition; 3D face reconstruction; Generic elastic
   model; Curve registration; B-spline
ID SOFT-TISSUE THICKNESS; OVERLAY UNCERTAINTY; MORPHABLE MODEL; RELIABILITY
AB Identification of human remains via craniofacial superimposition (CS) is one of the prominent research areas in the forensic sciences. CS makes use of imaging techniques to identify an unknown skull by matching it with the available face photographs of missing individuals. Life-size enlargement of the face image and orientating the skull to correspond to the posture seen in the face photograph are the two main problems that affect identification accuracy with both the conventional and the computer-aided methods. Unlike the existing techniques, this research proposes a 3D skull-3D face model superimposition (3D-3D) approach to address the above two issues. The proposed method commences by reconstructing the 3D face model from a given 2D face image using the mean simplified generic elastic model, followed by registering the face model to a 3D skull along the jaw line using the analytical curvature B-spline (AC B-spline). The accuracy index of the registration is then evaluated to suggest the degree to which the face image corresponds to a skull. The superimpositions of positive and negative cases were conducted on a set of 3D skulls versus a set of 2D face images. The accuracy indices of the registration results suggest that the AC B-spline is more robust in 3D-3D superimposition compared to the other existing methods. The full experimental results have demonstrated the potential of the proposed method as an assistive tool to the forensic scientists for craniofacial identifications.
C1 [Tan, Joi San] Univ Tunku Abdul Rahman, Dept Comp Sci, Kampar, Malaysia.
   [Liao, Iman Yi] Univ Nottingham Malaysia Campus, Sch Comp Sci, Semenyih, Malaysia.
   [Venkat, Ibrahim] Univ Teknol Brunei, Sch Comp & Informat, Bandar Seri Begawan, Brunei.
   [Belaton, Bahari] Univ Sains Malaysia, Sch Comp Sci, Gelugor, Pulau Pinang, Malaysia.
   [Jayaprakash, P. T.] Univ Sains Malaysia, Forens Scienceprogram, Kubang Kerian, Malaysia.
C3 Universiti Tunku Abdul Rahman (UTAR); University of Nottingham Malaysia;
   University of Technology Brunei; Universiti Sains Malaysia; Universiti
   Sains Malaysia
RP Tan, JS (corresponding author), Univ Tunku Abdul Rahman, Dept Comp Sci, Kampar, Malaysia.
EM jstan@utar.edu.my; Iman.Liao@nottingham.edu.my;
   ibrahim.venkat@utb.edu.bn; bahari@usm.my; ptjaya@usm.my
RI Liao, Iman/HOC-9644-2023; Belaton, Bahari/AAO-9869-2021; tan,
   joisan/ABA-3666-2020
OI Belaton, Bahari/0000-0002-9099-1498; Tan, Joi San/0000-0002-8830-269X
FU Department of Radiology Hospital Universiti Sains Malaysia (RUT Grant)
   [1001/PPSG/852004]
FX This research is supported by Department of Radiology Hospital
   Universiti Sains Malaysia (RUT Grant, 1001/PPSG/852004).
CR AUSTINSMITH D, 1994, J FORENSIC SCI, V39, P446
   Ballerini Lucia, 2007, 2007 3rd International Symposium on Information Assurance and Security, P429
   Basauri C., 1967, Int. Criminal Police Review, V204, P37
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Campomanes AC, 2016, FUZZY SETS SYST, V318, P1
   Campomanes-Alvarez BR, 2014, FORENSIC SCI INT, V245, P77, DOI 10.1016/j.forsciint.2014.10.009
   Campomanes-Alvarez C, 2016, APPL SOFT COMPUT, V46, P596, DOI 10.1016/j.asoc.2015.11.006
   Campomanes-Alvarez C, 2015, ADV INTEL SYS RES, V89, P1612
   Campornanes-Alvarez C, 2018, INFORM FUSION, V39, P25, DOI 10.1016/j.inffus.2017.03.004
   Clement J.G., 1998, Craniofacial identification in forensic medicine
   Dong Y, 2012, FORENSIC SCI INT, V222, DOI 10.1016/j.forsciint.2012.06.004
   DORION RBJ, 1983, J FORENSIC SCI, V28, P724
   Fenton TW, 2008, J FORENSIC SCI, V53, P34, DOI 10.1111/j.1556-4029.2007.00624.x
   Glaister J., 1937, MED LEGAL ASPECTS RU, V33
   Gomez O, 2017, PROG ARTIF INTELL, V4, P1
   Hagemeier H., 1983, INT CRIM POLICE REV, V373, P286
   HELMER R, 1977, Z RECHTSMED, V80, P183, DOI 10.1007/BF02114613
   Hermann S., 2006, TECHNICAL REPORT
   Ibáñez O, 2012, SOFT COMPUT, V16, P797, DOI 10.1007/s00500-011-0770-8
   Ibanez O., 2009, NATURE INSPIRED INFO, P119
   Ibáñez O, 2008, LECT NOTES ARTIF INT, V5271, P599, DOI 10.1007/978-3-540-87656-4_74
   Ibáñez O, 2011, IEEE T FUZZY SYST, V19, P946, DOI 10.1109/TFUZZ.2011.2158220
   Ibáñez O, 2009, PROCEEDINGS OF THE JOINT 2009 INTERNATIONAL FUZZY SYSTEMS ASSOCIATION WORLD CONGRESS AND 2009 EUROPEAN SOCIETY OF FUZZY LOGIC AND TECHNOLOGY CONFERENCE, P195
   Ibáñez O, 2009, INFORM SCIENCES, V179, P3998, DOI 10.1016/j.ins.2008.12.029
   ITEN PX, 1987, J FORENSIC SCI, V32, P173
   Jain V., 2002, The Indian Face Database
   Jayaprakash PT, 2001, FORENSIC SCI INT, V117, P121, DOI 10.1016/S0379-0738(00)00455-2
   Jiang L, 2018, IEEE T IMAGE PROCESS, V27, P4756, DOI 10.1109/TIP.2018.2845697
   Joi San Tan, 2015, Advances in Visual Informatics. 4th International Visual Informatics Conference, IVIC 2015. Proceedings: LNCS 9429, P283, DOI 10.1007/978-3-319-25939-0_25
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu Celong, 2018, ARXIV181000107
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Ma MY, 2016, VISUAL COMPUT, V32, P1223, DOI 10.1007/s00371-015-1158-z
   MAAT GJR, 1989, FORENSIC SCI INT, V41, P225, DOI 10.1016/0379-0738(89)90215-6
   Maghari A.Y., 2013, INT J ADV SOFT COMPU, V5, P1
   Maghari Ashraf Y. A., 2015, Advances in Visual Informatics. 4th International Visual Informatics Conference, IVIC 2015. Proceedings: LNCS 9429, P476, DOI 10.1007/978-3-319-25939-0_42
   Maghari A, 2014, IET COMPUT VIS, V8, P441, DOI 10.1049/iet-cvi.2013.0220
   Manheim MH, 2000, J FORENSIC SCI, V45, P48
   MCKENNA JJI, 1988, J FORENSIC SCI, V33, P751
   NICKERSON BA, 1991, J FORENSIC SCI, V36, P480
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Campomanes-Alvarez BR, 2015, IEEE T INF FOREN SEC, V10, P2057, DOI 10.1109/TIFS.2015.2441000
   Sahni D, 2008, FORENSIC SCI INT, V176, P137, DOI 10.1016/j.forsciint.2007.07.012
   Santamaria J., 2009, 9 INT C INFORM TECHN, P1
   Schroeder W., 1998, The visualization toolkit an object-oriented approach to 3D graphics
   Sekharan C, 1993, POSITIONING SKULL SU
   SEKHARAN PC, 1971, J CRIM LAW CRIMINOL, V62, P107, DOI 10.2307/1142133
   SEKHARAN PC, 1973, J POLICE SCI ADMIN, V1, P232
   Sen N.K., 1962, Int. Criminal Police Review, P284
   Sipahioglu S, 2012, FORENSIC SCI INT, V219, DOI 10.1016/j.forsciint.2011.11.017
   Smith WAP, 2005, LECT NOTES COMPUT SC, V3691, P153
   Tan J.S., 2016, BMVC, P1
   Tan JS, 2016, SCIENCEASIA, V42, P136, DOI 10.2306/scienceasia1513-1874.2016.42.136
   Yoshino M, 1997, FORENSIC SCI INT, V90, P231, DOI 10.1016/S0379-0738(97)00168-0
   YOSHINO M, 1995, FORENSIC SCI INT, V74, P125, DOI 10.1016/0379-0738(95)01742-2
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 57
TC 5
Z9 5
U1 3
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1739
EP 1753
DI 10.1007/s00371-019-01767-7
EA OCT 2019
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000494074100001
DA 2024-07-18
ER

PT J
AU Rahim, MA
   Shin, J
   Islam, MR
AF Rahim, Md Abdur
   Shin, Jungpil
   Islam, Md Rashedul
TI Gestural flick input-based non-touch interface for character input
SO VISUAL COMPUTER
LA English
DT Article
DE Human-computer interaction; Hand gestures; Non-touch input; Gesture
   recognition; Kinect sensor
AB A non-touch character input is a modern system for communication between humans and computers that can help the user to interact with a computer, a machine, or a robot in unavoidable circumstances or industrial life. There have been many studies in the field of touch and non-touch character input systems (i.e., hand gesture languages), such as aerial handwriting, sign languages, and the finger alphabet. However, many previously developed systems require substantial effort in terms of learning and overhead processing for character recognition. To address this issue, this paper proposes a gesture flick input system that offers a quick and easy input method using a hygienic and safe non-touch character input system. In the proposed model, the position and state of the hands (i.e., open or closed) are recognized to enable flick input and to relocate and resize the on-screen virtual keyboard for the user. In addition, this system recognizes hand gestures that perform certain motion functions, such as delete, add a space, insert a new line, and select language, an approach which reduces the need for recognition of a large number of overhead gestures for the characters. To reduce the image-processing overhead and eliminate the surrounding noise and light effects, body index skeleton information from the Kinect sensor is used. The proposed system is evaluated based on the following factors: (a) character selection, recognition and speed of character input (in Japanese hiragana, English, and numerals); and (b) accuracy of gestures for the motion functions. The system is then compared to state-of-the-art algorithms. A questionnaire survey was also conducted to measure the user acceptance and usability of this system. The experimental results show that the average recognition rates for characters and motion functions were 98.61% and 97.5%, respectively, thus demonstrating the superiority of the proposed model compared to the state-of-the-art algorithms.
C1 [Rahim, Md Abdur; Shin, Jungpil; Islam, Md Rashedul] Univ Aizu, Sch Comp Sci & Engn, Fukushima 9658580, Japan.
C3 University of Aizu
RP Shin, J (corresponding author), Univ Aizu, Sch Comp Sci & Engn, Fukushima 9658580, Japan.
EM rahim_bds@yahoo.com; jpshin@u-aizu.ac.jp; rashed.cse@gmail.com
RI Rahim, Abdur/X-3408-2019; Rahim, Dr Md Abdur/JJD-7305-2023; Islam, Dr.
   Md. Rashedul/D-6227-2019
OI Rahim, Abdur/0000-0003-2300-1420; Rahim, Dr Md
   Abdur/0000-0003-2300-1420; SHIN, Jungpil/0000-0002-7476-2468; Islam, Dr.
   Md. Rashedul/0000-0001-8676-6338
CR Ben Jmaa A, 2016, COMPUT SIST, V20, P709, DOI [10.13053/cys-20-4-2390, 10.13053/CyS-20-4-2390]
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P436, DOI 10.1109/THMS.2015.2492599
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P403, DOI 10.1109/THMS.2015.2492598
   Fujii Y., 2009, SPECIAL INTEREST GRO, V50, P1
   Fukatsu Yoshitomo, 2013, P 15 INT C HUM COMP, P161
   Gajjar V, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON POWER, CONTROL, SIGNALS AND INSTRUMENTATION ENGINEERING (ICPCSI), P856, DOI 10.1109/ICPCSI.2017.8391833
   Kane L, 2017, IEEE T HUM-MACH SYST, V47, P1077, DOI 10.1109/THMS.2017.2706695
   Kawauchi Makiko, 2012, DIMENSION DATA HANDS
   Khan Z. H., 2018, INNOV FOOD SCI EMERG
   Kumar P, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P157, DOI 10.23919/MVA.2017.7986825
   Kuramochi K, 2017, 2017 56TH ANNUAL CONFERENCE OF THE SOCIETY OF INSTRUMENT AND CONTROL ENGINEERS OF JAPAN (SICE), P116, DOI 10.23919/SICE.2017.8105492
   Maehatake M., 2007, P WORKSH INT SYST SO P WORKSH INT SYST SO, P129
   Miyake T., 2012, TECH REP, V20, P7
   Nakamura K, 2012, KINECT WINDOWS SDK P
   Nishimura Y., 2012, TECH REP, V111, P161
   Ohkura M., 2011, J IPSJ, V52, P910
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Sauro J, 2016, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, 2ND EDITION, P1
   Shin J, 2017, IEEE ACCESS, V5, P10496, DOI 10.1109/ACCESS.2017.2703783
   Sonoda T., 2003, DIIJ86DII IEICE, P1015
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Takabayashi D., 2013, TECH REP, V112, P79
   Tanaka S., 2014, 76 NAT CONV IPSJ, V76, P2275
   Tojo T., 2018, P 20 INT C HUM COMP, P44
   Ye Z, 2013, C IND ELECT APPL, P501
   Zhang X, 2013, IEEE MULTIMEDIA, V20, P85, DOI 10.1109/MMUL.2013.50
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 29
TC 4
Z9 4
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1559
EP 1572
DI 10.1007/s00371-019-01758-8
EA OCT 2019
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000488921500001
DA 2024-07-18
ER

PT J
AU Lu, SF
   Jiang, W
   Ding, XF
   Kaplan, CS
   Jin, XG
   Gao, F
   Chen, JZ
AF Lu, Shufang
   Jiang, Wei
   Ding, Xuefeng
   Kaplan, Craig S.
   Jin, Xiaogang
   Gao, Fei
   Chen, Jiazhou
TI Depth-aware image vectorization and editing
SO VISUAL COMPUTER
LA English
DT Article
DE Image vectorization; RGB-D images; Depth aware; Diffusion curves; Object
   segmentation and editing
ID EDGE-DETECTION
AB Image vectorization is one of the primary means of creating vector graphics. The quality of a vectorized image depends crucially on extracting accurate features from input raster images. However, correct object edges can be difficult to detect when color gradients are weak. We present an image vectorization technique that operates on a color image augmented with a depth map and uses both color and depth edges to define vectorized paths. We output a vectorized result as a diffusion curve image. The information extracted from the depth map allows us more flexibility in the manipulation of the diffusion curves, in particular permitting high-level object segmentation. Our experimental results demonstrate that this method achieves high reconstruction quality and provides greater control in the organization and editing of vectorized images than existing work based on diffusion curves.
C1 [Lu, Shufang; Ding, Xuefeng; Gao, Fei] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Chen, Jiazhou] Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.
   [Jiang, Wei] Univ Victoria, Visual Comp Grp, Victoria, BC, Canada.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Kaplan, Craig S.] Univ Waterloo, Comp Sci, Waterloo, ON, Canada.
C3 Zhejiang University of Technology; Zhejiang University of Technology;
   University of Victoria; Zhejiang University; University of Waterloo
RP Chen, JZ (corresponding author), Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.
EM cjz@zjut.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [LY19F020027];
   National Natural Science Foundation ofChina [61402410]; Key Research and
   Development Program of Zhejiang Province [2018C03055]; National Natural
   Science Foundation of China [61732015]
FX This work is supported by Zhejiang Provincial Natural Science Foundation
   of China (No. LY19F020027), the National Natural Science Foundation
   ofChina (No. 61402410), the Key Research and Development Program of
   Zhejiang Province (No. 2018C03055), and the National Natural Science
   Foundation of China (No. 61732015).
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Boyé S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366192
   Finch M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024200
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hou F., 2018, IEEE T VISUALIZATION
   Ilbery P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508426
   Jeschke S, 2011, COMPUT GRAPH FORUM, V30, P523, DOI 10.1111/j.1467-8659.2011.01877.x
   Jeschke S, 2016, COMPUT GRAPH FORUM, V35, P71, DOI 10.1111/cgf.12812
   Jeschke S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618462
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Liao ZC, 2012, IEEE T VIS COMPUT GR, V18, P1858, DOI 10.1109/TVCG.2012.76
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P117, DOI 10.1023/A:1008097225773
   Mao CY, 2009, 2009 WRI WORLD CONGRESS ON SOFTWARE ENGINEERING, VOL 4, PROCEEDINGS, P13, DOI 10.1109/WCSE.2009.182
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Orzan A, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P103
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Price B, 2006, VISUAL COMPUT, V22, P661, DOI 10.1007/s00371-006-0051-1
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schneider P.J., 1990, Graphics gems, V1, P612
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P1453, DOI 10.1109/TCYB.2013.2273270
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Sun D, 2015, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2015.7298653
   Sun T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601187
   WANG C, 1944, IEEE T IMAGE PROCESS, V26, P1833, DOI DOI 10.1109/TIP.2017.2666742
   Wang TC, 2016, PROC CVPR IEEE, P3717, DOI 10.1109/CVPR.2016.404
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
   Xie GF, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661275
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Ye XC, 2014, IEEE T BROADCAST, V60, P540, DOI 10.1109/TBC.2014.2345931
   Zhao S, 2018, IEEE T VIS COMPUT GR, V24, P2153, DOI 10.1109/TVCG.2017.2721400
NR 35
TC 7
Z9 7
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1027
EP 1039
DI 10.1007/s00371-019-01671-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200021
OA Bronze
DA 2024-07-18
ER

PT J
AU Testa, E
   Barros, RC
   Musse, SR
AF Testa, Estevao
   Barros, Rodrigo C.
   Musse, Soraia Raupp
TI CrowdEst: a method for estimating (and not simulating) crowd evacuation
   parameters in generic environments
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Crowd estimation; Neural networks
ID EVOLUTION
AB Evacuation plans have been historically used as a safety measure for the construction of buildings. The existing crowd simulators require fully modeled 3D environments and enough time to prepare and simulate scenarios, where the distribution and behavior of the crowd needs to be controlled. In addition, its population, routes or even doors and passages may change, so the 3D model and configurations have to be updated accordingly. This is a time-consuming task that commonly has to be addressed within the crowd simulators. With that in mind, we present a novel approach to estimate the resulting data of a given evacuation scenario without actually simulating it. For such, we divide the environment into smaller modular rooms with different configurations, in a divide-and-conquer fashion. Next, we train an artificial neural network to estimate all required data regarding the evacuation of a single room. After collecting the estimated data from each room, we develop a heuristic capable of aggregating per-room information so the full environment can be properly evaluated. Our method presents an average error of 5% when compared to evacuation time in a real-life environment. Our crowd estimator approach has several advantages, such as not requiring to model the 3D environment, nor learning how to use and configure a crowd simulator, which means any user can easily use it. Furthermore, the computational time to estimate evacuation data (inference time) is virtually zero, which is much better even when compared to the best-case scenario in a real-time crowd simulator.
C1 [Testa, Estevao; Barros, Rodrigo C.; Musse, Soraia Raupp] Pontificia Univ Catolica Rio Grande do Sul, Sch Technol, Grad Program Comp Sci, Porto Alegre, RS, Brazil.
C3 Pontificia Universidade Catolica Do Rio Grande Do Sul
RP Musse, SR (corresponding author), Pontificia Univ Catolica Rio Grande do Sul, Sch Technol, Grad Program Comp Sci, Porto Alegre, RS, Brazil.
EM estevaostesta@gmail.com; rodrigo.barros@pucrs.br; soraia.musse@pucrs.br
RI Barros, Rodrigo Coelho/M-2588-2019; Barros, Rodrigo Coelho/I-4319-2012;
   Alidadi, Mehdi/HJZ-0235-2023; Musse, Soraia Raupp R/G-4801-2012; Musse,
   Soraia Raupp/AAS-3787-2021
OI Barros, Rodrigo Coelho/0000-0002-0782-9482; Barros, Rodrigo
   Coelho/0000-0002-0782-9482; Alidadi, Mehdi/0000-0001-5183-7829; 
FU Brazilian Research Agency CNPq [305084/2016-0]
FX This work was partially funded by Brazilian Research Agency CNPq (Grant
   Number: 305084/2016-0).
CR Aik LE, 2012, J APPL MATH, DOI 10.1155/2012/765270
   [Anonymous], CASA 2010 P COMP AN
   Cassol Vincius, 2016, 2016 IEEE Virtual Humans and Crowds for Immersive Environments (VHCIE), P1, DOI 10.1109/VHCIE.2016.7563565
   Cassol VJ, 2017, IEEE COMPUT GRAPH, V37, P60, DOI 10.1109/MCG.2017.3271454
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chu ML, 2014, COMPUT ANIMAT VIRT W, V25, P375, DOI 10.1002/cav.1595
   Fradi H, 2013, IEEE INT WORKSH MULT, P40, DOI 10.1109/MMSP.2013.6659261
   FRUIN J.J., 1971, Designing for pedestrians: A level-of-service concept. Highway Research Record
   FRUIN J.J., 1971, Metropolitan Association of Urban Designers and Environmental Planners
   Fu LB, 2014, TRANSP RES PROC, V2, P533, DOI 10.1016/j.trpro.2014.09.093
   Galea ER, 1998, J FIRE SCI, V16, P414, DOI 10.1177/073490419801600603
   Garrett A, 2006, IEEE C EVOL COMPUTAT, P157, DOI 10.1109/CEC.2006.1688303
   Gwynne S, 1999, BUILD ENVIRON, V34, P741, DOI 10.1016/S0360-1323(98)00057-2
   Hansen M, 1996, IEEE C EVOL COMPUTAT, P312, DOI 10.1109/ICEC.1996.542381
   Hansen Nikolaus, 2011, RES REPORT
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   Helbing D, 1997, NATURE, V388, P47, DOI 10.1038/40353
   HENDERSON LF, 1971, NATURE, V229, P381, DOI 10.1038/229381a0
   HENDERSON LF, 1972, NATURE, V240, P353, DOI 10.1038/240353a0
   HENDERSON LF, 1974, TRANSPORT RES, V8, P509, DOI 10.1016/0041-1647(74)90027-6
   Ji LQ, 2013, J APPL MATH, DOI 10.1155/2013/284721
   Liu W., 2017, CHARACTERIZING RELAT
   Ma WB, 2015, J URBAN PLAN DEV, V141, DOI 10.1061/(ASCE)UP.1943-5444.0000232
   Musse, 2013, CROWD SIMULATION, P195
   Musse SR, 2012, COMPUT ANIMAT VIRT W, V23, P49, DOI 10.1002/cav.1423
   Nasirpouri F., 2015, 2015 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2015.7157712
   Pauls J.L., 1980, Fires and Human Behaviour, P227
   POLUS A, 1983, J TRANSP ENG-ASCE, V109, P46, DOI 10.1061/(ASCE)0733-947X(1983)109:1(46)
   Schadschneider A, 2011, NETW HETEROG MEDIA, V6, P545, DOI 10.3934/nhm.2011.6.545
   Still G.K, 2000, Crowd dynamic
   Tilley A. R., 2001, The Measure of Man and Woman: Human Factors in Design
   Tripathi G, 2018, VIS COMPUT
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   Van DenBerg J., OPTIMAL RECIPROCAL C
NR 34
TC 4
Z9 5
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1119
EP 1130
DI 10.1007/s00371-019-01684-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200028
OA Bronze
DA 2024-07-18
ER

PT J
AU Ben Boudaoud, L
   Solaiman, B
   Tari, A
AF Ben Boudaoud, Lynda
   Solaiman, Basel
   Tari, Abdelkamel
TI A modified ZS thinning algorithm by a hybrid approach
SO VISUAL COMPUTER
LA English
DT Article
DE Thinning algorithm; Binary image; Thinning rate; Thinning speed;
   Connectivity; Noise sensitivity; GPU
ID FAST PARALLEL ALGORITHM; DIGITAL PATTERNS; SKELETONIZATION
AB Thinning is one of the most important techniques in the field of image processing. It is applied to erode the image of an object layer-by-layer until a skeleton is left. Several thinning algorithms allowing to get a skeleton of a binary image are already proposed in the literature. This paper investigates several well-known parallel thinning algorithms and proposes a modified version of the most widely used thinning algorithm, called the ZS algorithm. The proposed modified ZS (MZS) algorithm is implemented and compared against seven existing algorithms. Experimental results and performances evaluation, using different image databases, confirm the proposed MZS algorithm improvement over the seven examined algorithms both in terms of the obtained results quality and the computational speed. Moreover, for an efficient implementation (on Graphics Processing Units), a parallel model of the MZS algorithm is proposed (using the Compute Unified Device Architecture, CUDA, as a parallel programming model). Evaluation results have shown that the parallel version of the proposed algorithm is, on average, more than 21 times faster than the traditional CPU sequential version.
C1 [Ben Boudaoud, Lynda; Tari, Abdelkamel] Univ Bejaia, Fac Sci Exactes, LIMED, Lab Informat Med, Bejaia 06000, Algeria.
   [Solaiman, Basel] IMT Atlantique Inst, Image & Informat Proc Dept, F-29238 Brest, France.
C3 Universite de Bejaia; IMT - Institut Mines-Telecom; IMT Atlantique
RP Ben Boudaoud, L (corresponding author), Univ Bejaia, Fac Sci Exactes, LIMED, Lab Informat Med, Bejaia 06000, Algeria.
EM lynda.benboudaoud@gmail.com
CR Ahmed M, 2002, IEEE T PATTERN ANAL, V24, P1672, DOI 10.1109/TPAMI.2002.1114862
   [Anonymous], 2013, DESIGNING SCI APPL G
   [Anonymous], 1996, TOPOLOGICAL ALGORITH
   Boudaoud L. B., 2015, 3 INT C CONTR ENG IN, P1, DOI DOI 10.1109/CEIT.2015.7233099
   CHENG J., 2014, Professional CUDA c Programming
   Deng W, 2000, INT J HIGH PERFORM C, V14, P65, DOI 10.1177/109434200001400105
   Jagna A., 2010, J ENG APPL SCI, V5, P64
   Kocharyan D., 2013, MATHEMATICS-BASEL, V2, P1, DOI [10.11648/j.ajsea.20130201.11, DOI 10.11648/J.ENGMATH.20180201.11]
   LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346
   LU HE, 1986, COMMUN ACM, V29, P239, DOI 10.1145/5666.5670
   Palágyi K, 2002, PATTERN RECOGN LETT, V23, P663, DOI 10.1016/S0167-8655(01)00142-8
   Rosenfeld A., 1976, DIGITAL PICTURE PROC
   Sanders J, 2010, CUDA EXAMPLE INTRO G
   Song CF, 2018, VISUAL COMPUT, V34, P243, DOI 10.1007/s00371-016-1331-z
   Tang YY, 2003, IEEE T PATTERN ANAL, V25, P1118, DOI 10.1109/TPAMI.2003.1227987
   Tarabek P., 2012, Proceedings of the 2012 7th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI), P75, DOI 10.1109/SACI.2012.6249979
   Wei Chen, 2012, 2012 International Conference on Systems and Informatics (ICSAI 2012), P1947, DOI 10.1109/ICSAI.2012.6223430
   Wolberg G., 1989, Visual Computer, V5, P95, DOI 10.1007/BF01901485
   WU RY, 1992, PATTERN RECOGN LETT, V13, P715, DOI 10.1016/0167-8655(92)90101-5
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
   Zhou RW, 1995, PATTERN RECOGN LETT, V16, P1267, DOI 10.1016/0167-8655(95)00078-X
NR 21
TC 17
Z9 17
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 689
EP 706
DI 10.1007/s00371-017-1407-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100008
DA 2024-07-18
ER

PT J
AU Cheggoju, N
   Satpute, VR
AF Cheggoju, Naveen
   Satpute, Vishal R.
TI INPAC: INdependent PAss Coding algorithm for robust image data
   transmission through low SNR channels
SO VISUAL COMPUTER
LA English
DT Article
DE EZW; INdependent PAss Coding (INPAC); Compression; Noise; Modulation
ID NOISY CHANNELS; COMPRESSION; JPEG2000
AB A new strategy of data representation called INdependent PAss Coding (INPAC) is proposed in this paper for robust and reliable transmission of embedded zero wavelet (EZW) bit stream through noisy channels. Error handling capacity of the EZW is improved by incorporating the proposed INPAC algorithm to EZW algorithm. It suppress the effect of noise occurred in one pass on to another by restricting the data of corrupted pass to mix with the data of uncorrupted pass. For the robust and reliable reconstruction, it is essential to have some of the most important header data without error. This kind of data representation achieves a better quality of reconstruction with a slight increase in overhead of maximum 32 B. To evaluate the modified INPAC-EZW algorithm, the results are compared mathematically and perceptually (visual) with the robust block-based EZW in AWGN channel environment with 4- and 16-QAM modulation techniques and by considering various SNR values. Mathematical parameters peak signal-to-noise ratio (PSNR), compression ratio (CR) and structural similarity index (SSIM) are used to evaluate the performance of the modified algorithm. By considering the aforementioned channel conditions, the average percentage increase in PSNR is observed to be 26.97%, average improvement in CR is observed to be 1.129% and average percentage increase in SSIM is observed to be 12.4%.
C1 [Cheggoju, Naveen; Satpute, Vishal R.] VNIT Nagpur, Dept ECE, Image Anal & Comp Vis Lab, Nagpur, Maharashtra, India.
C3 National Institute of Technology (NIT System); Visvesvaraya National
   Institute of Technology, Nagpur
RP Cheggoju, N (corresponding author), VNIT Nagpur, Dept ECE, Image Anal & Comp Vis Lab, Nagpur, Maharashtra, India.
EM naninaveen.ch@gmail.com; vrsatpute@ece.vnit.ac.in
RI Cheggoju, Naveen/L-3006-2019; Cheggoju, Naveen/M-9444-2019; SATPUTE,
   VISHAL RAMESH/AAA-7713-2022
OI Cheggoju, Naveen/0000-0002-4734-9344; SATPUTE, VISHAL
   RAMESH/0000-0001-9944-9489
CR [Anonymous], OBJ PERC ASS VID QUA
   Aulí-Llinàs F, 2008, IEEE T CIRC SYST VID, V18, P923, DOI 10.1109/TCSVT.2008.920748
   Baqai S., 2002, P IEEE 4 INT S MULT
   Boulgouris NV, 2003, IEEE T CIRC SYST VID, V13, P1170, DOI 10.1109/TCSVT.2003.819187
   Creusere CD, 1997, IEEE T IMAGE PROCESS, V6, P1436, DOI 10.1109/83.624967
   Cui J., 1998, P IEEE INT S CIRC SY
   Gaol L., 2002, IEEE INT S CIRC SYST
   Jen-Chang Liu, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P560
   Lee PJ, 2003, IEEE T CONSUM ELECTR, V49, P1395, DOI 10.1109/TCE.2003.1261246
   Lee YS, 2003, IEEE T CIRC SYST VID, V13, P176, DOI 10.1109/TCSVT.2002.808430
   Man H, 1997, IEEE SIGNAL PROC LET, V4, P227, DOI 10.1109/97.611284
   Manji S., 1999, 49 IEEE VEH TECHN C
   Naveen Ch., 2015, WORKSH COMP INT THEO, V2015, P1, DOI [10.1109/WCI.2015.7495505, DOI 10.1109/WCI.2015.7495505]
   Ogura K., 1999, 49 IEEE VEH TECHN C
   Redmill DW, 1996, IEEE T IMAGE PROCESS, V5, P565, DOI 10.1109/83.491333
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sanchez V. S., 2002, INT C CONS EL ICCE 2
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman DS, 2002, P IEEE, V90, P1336, DOI 10.1109/JPROC.2002.800725
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wei H.-K., 2002, INT C IM PROC ICIP
   Wu AY, 2007, IEEE T CIRC SYST VID, V17, P1752, DOI 10.1109/TCSVT.2007.904534
   Yang SH, 2000, ELECTRON LETT, V36, P208, DOI 10.1049/el:20000216
NR 25
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 563
EP 573
DI 10.1007/s00371-017-1361-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400009
DA 2024-07-18
ER

PT J
AU Deng, WX
   Zou, HX
   Guo, F
   Lei, L
   Zhou, SL
AF Deng, Wanxia
   Zou, Huanxin
   Guo, Fang
   Lei, Lin
   Zhou, Shilin
TI Point-pattern matching based on point pair local topology and
   probabilistic relaxation labeling
SO VISUAL COMPUTER
LA English
DT Article
DE Point-pattern matching (PPM); Point pair local topology (PPLT);
   Probabilistic relaxation labeling; Compatibility coefficients
ID IMAGE REGISTRATION
AB This paper presents a robust point-pattern matching (PPM) algorithm, in which the invariant feature and probabilistic relaxation labeling are combined to improve the assignment accuracy and efficiency. A local feature descriptor, namely, point pair local topology (PPLT), is proposed first. The feature descriptor is defined by histogram which is constructed using the weighting of distance measures and angle measures based on local point pair. We use the matching scores of point pair local topology descriptor's statistic test to define new compatibility coefficients. Then, the robust support functions are constructed based on the obtained compatibility coefficients. Finally, according to the relaxed iterations of matching probability matrix and the mapping constraints required by the bijective correspondence, the correct matching results are obtained. A number of comparison and evaluation experiments on both synthetic point sets and real-world data demonstrate that the proposed algorithm performs better in the presence of outliers and positional jitter. In addition, it achieves the superior performance under similarity and even nonrigid transformation among point sets in the meantime compared with state-of-the-art approaches.
C1 [Deng, Wanxia; Zou, Huanxin; Guo, Fang; Lei, Lin; Zhou, Shilin] Natl Univ Def Technol, 109 Deya Rd,Yanwachi St, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Zou, HX (corresponding author), Natl Univ Def Technol, 109 Deya Rd,Yanwachi St, Changsha 410073, Hunan, Peoples R China.
EM wanxiadeng@163.com
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Dou H., 2015, VIS COMP, V967
   Jain AK, 2009, IEEE IMAGE PROC, P2745, DOI 10.1109/ICIP.2009.5414140
   Jian Zhao, 2011, 2011 3rd International Conference on Computer Research and Development (ICCRD 2011), P508, DOI 10.1109/ICCRD.2011.5764185
   Jiang TT, 2009, PROC CVPR IEEE, P848, DOI 10.1109/CVPRW.2009.5206568
   Kittler J, 2000, PATTERN RECOGN, V33, P705, DOI 10.1016/S0031-3203(99)00081-3
   Lee JH, 2011, IEEE T PATTERN ANAL, V33, P427, DOI 10.1109/TPAMI.2010.179
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li H., 2010, 23 IEEE C COMP VIS P
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   McKeon RT, 2010, IEEE T INSTRUM MEAS, V59, P774, DOI 10.1109/TIM.2009.2037874
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3
   VELTKAMP RC, 1999, UUCS199927
   Wang H, 2008, PATTERN RECOGN, V41, P3393, DOI 10.1016/j.patcog.2008.03.030
   Xiong Z, 2009, IEEE T GEOSCI REMOTE, V47, P4189, DOI 10.1109/TGRS.2009.2023794
   Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81
NR 27
TC 5
Z9 7
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 55
EP 65
DI 10.1007/s00371-016-1311-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200007
DA 2024-07-18
ER

PT J
AU Lv, P
   Zhao, QJ
   Chen, YM
   Zhao, LJ
AF Lv, Peng
   Zhao, Qingjie
   Chen, Yanming
   Zhao, Liujun
TI Multiple cues-based active contours for target contour tracking under
   sophisticated background
SO VISUAL COMPUTER
LA English
DT Article
DE Object contour tracking; Active contours; Level sets; Segmentation
ID OBJECT TRACKING; VIDEO SEGMENTATION; EVOLUTION; REGION; IMAGE
AB In this paper, we propose a novel target contour tracking method under sophisticated background using the multiple cues-based active contour model. To locate the target position, a contour-based mean-shift tracker is designed which combines both color and texture information. To reduce the adverse impact of sophisticated background and also accelerate the curve motion, we propose a twolayer-based target appearance model that combines both discriminative pre-learned-based global layer and votingbased local layer. The proposed appearance model is able to extract rough target region from the complex background, which provides important target region information for our active contour model. We subsequently introduce a dynamical shape model to provide prior target shape information for more stable segmentation. To obtain accurate target boundaries, we design a new multiple cues-based active contour model which integrates with target edge, discriminative region, and shape information. The experimental results on 30 video sequences demonstrate that the proposed method outperforms other competitive contour tracking methods under various tracking environment.
C1 [Lv, Peng; Zhao, Qingjie; Chen, Yanming; Zhao, Liujun] Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligence Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Lv, P (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligence Informat Technol, Beijing 100081, Peoples R China.
EM p1v@bit.edu.cn
FU National Natural Science Foundation of China [61175096, 61303245];
   Specialized Fund for Joint Building Program of Beijing municipal
   Education Commission
FX This work was supported in part by the National Natural Science
   Foundation of China under Grand (Nos. 61175096 and 61303245) and
   Specialized Fund for Joint Building Program of Beijing municipal
   Education Commission. The authors would also like to thank C. Li, J.
   Fan, M. Godec, S. Wang, Z. Cai, X. Jia, and M. Yang et al. for providing
   their source codes for comparisons in our experiments.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Allili Mohand Said, 2007, Signal, Image and Video Processing, V1, P101, DOI 10.1007/s11760-007-0021-8
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Bibby C, 2008, LECT NOTES COMPUT SC, V5303, P831, DOI 10.1007/978-3-540-88688-4_61
   Bibby C, 2010, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2010.5539818
   Cai L, 2011, IEEE T CIRC SYST VID, V21, P1784, DOI 10.1109/TCSVT.2011.2133550
   Cai ZW, 2014, IEEE T IMAGE PROCESS, V23, P5497, DOI 10.1109/TIP.2014.2364919
   CASELLES V, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P694, DOI 10.1109/ICCV.1995.466871
   Chen L, 2006, PATTERN RECOGN, V39, P1391, DOI 10.1016/j.patcog.2006.01.017
   Chiverton J., 2008, BRIT MACH VIS C BMVC, P253
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Cremers D, 2006, IEEE T PATTERN ANAL, V28, P1262, DOI 10.1109/TPAMI.2006.161
   Fan JL, 2012, IEEE T PATTERN ANAL, V34, P1633, DOI 10.1109/TPAMI.2011.257
   Godec M, 2013, COMPUT VIS IMAGE UND, V117, P1245, DOI 10.1016/j.cviu.2012.11.005
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Hoch M, 1996, VISUAL COMPUT, V12, P75
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Khoreva A, 2015, PROC CVPR IEEE, P951, DOI 10.1109/CVPR.2015.7298697
   Krotosky SJ, 2008, IEEE T CIRC SYST VID, V18, P1096, DOI 10.1109/TCSVT.2008.928217
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Mahmoodi S, 2009, IEEE SIGNAL PROC LET, V16, P857, DOI 10.1109/LSP.2009.2025924
   Mansouri AR, 2002, IEEE T PATTERN ANAL, V24, P947, DOI 10.1109/TPAMI.2002.1017621
   Niethammer M, 2006, IEEE T AUTOMAT CONTR, V51, P562, DOI 10.1109/TAC.2006.872837
   Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758
   Pirsiavash H, 2011, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2011.5995604
   Ramakanth SA, 2014, PROC CVPR IEEE, P376, DOI 10.1109/CVPR.2014.55
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Vaswani N, 2010, IEEE T IMAGE PROCESS, V19, P841, DOI 10.1109/TIP.2009.2037465
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Xin Sun, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3393, DOI 10.1109/CVPR.2011.5995656
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yilmaz A, 2011, MACH VISION APPL, V22, P255, DOI 10.1007/s00138-009-0237-4
   Zhang T, 2005, IEEE T PATTERN ANAL, V27, P282, DOI 10.1109/TPAMI.2005.31
   Zou DQ, 2015, IEEE I CONF COMP VIS, P1564, DOI 10.1109/ICCV.2015.183
NR 41
TC 2
Z9 2
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1103
EP 1119
DI 10.1007/s00371-016-1268-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600003
DA 2024-07-18
ER

PT J
AU Hermosilla, P
   Krone, M
   Guallar, V
   Vázquez, PP
   Vinacua, A
   Ropinski, T
AF Hermosilla, Pedro
   Krone, Michael
   Guallar, Victor
   Vazquez, Pere-Pau
   Vinacua, Alvar
   Ropinski, Timo
TI Interactive GPU-based generation of solvent-excluded surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Molecular visualization; Surface representation; Distance field
ID VISUALIZATION; EFFICIENT
AB The solvent-excluded surface (SES) is a popular molecular representation that gives the boundary of the molecular volume with respect to a specific solvent. SESs depict which areas of a molecule are accessible by a specific solvent, which is represented as a spherical probe. Despite the popularity of SESs, their generation is still a compute-intensive process, which is often performed in a preprocessing stage prior to the actual rendering (except for small models). For dynamic data or varying probe radii, however, such a preprocessing is not feasible as it prevents interactive visual analysis. Thus, we present a novel approach for the on-the-fly generation of SESs, a highly parallelizable, grid-based algorithm where the SES is rendered using ray-marching. By exploiting modern GPUs, we are able to rapidly generate SESs directly within the mapping stage of the visualization pipeline. Our algorithm can be applied to large time-varying molecules and is scalable, as it can progressively refine the SES if GPU capabilities are insufficient. In this paper, we show how our algorithm is realized and how smooth transitions are achieved during progressive refinement. We further show visual results obtained from real-world data and discuss the performance obtained, which improves upon previous techniques in both the size of the molecules that can be handled and the resulting frame rate.
C1 [Hermosilla, Pedro; Vazquez, Pere-Pau; Vinacua, Alvar] Univ Politecn Cataluna, Visualizat Virtual Real & Graph Interact ViRVIG, Barcelona, Spain.
   [Krone, Michael] Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany.
   [Guallar, Victor] Barcelona Supercomp Ctr, Barcelona, Spain.
   [Ropinski, Timo] Ulm Univ, Res Grp Visual Comp, Ulm, Germany.
C3 Universitat Politecnica de Catalunya; University of Stuttgart;
   Universitat Politecnica de Catalunya; Barcelona Supercomputer Center
   (BSC-CNS); Ulm University
RP Hermosilla, P (corresponding author), Univ Politecn Cataluna, Visualizat Virtual Real & Graph Interact ViRVIG, Barcelona, Spain.
EM pedro.hermosilla@cgstarad.com
RI Guallar, Victor/B-1579-2013; Vinacua, Àlvar/K-9392-2014; Krone,
   Michael/HJI-9309-2023; Vinacua, Àlvar/JAN-9485-2023; Vázquez,
   Pere-Pau/HTP-9691-2023
OI Vinacua, Àlvar/0000-0001-8984-4311; Vázquez,
   Pere-Pau/0000-0003-4638-4065; Ropinski, Timo/0000-0002-7857-5512; Krone,
   Michael/0000-0002-1445-7568
FU Spanish Ministerio de Economia y Competitividad [TIN2014-52211-C2-1-R,
   CTQ2016-79138-R]; FEDER; German Research Foundation (DFG) as part of
   Collaborative Research Center [SFB 716]
FX This work has been partially supported by Grant TIN2014-52211-C2-1-R and
   Grant CTQ2016-79138-R from the Spanish Ministerio de Economia y
   Competitividad with FEDER funds, and by the German Research Foundation
   (DFG) as part of Collaborative Research Center SFB 716.
CR [Anonymous], 2012, EuroVis-short papers, DOI [DOI 10.2312/PE/EUROVISSHORT/EUROVISSHORT2012/067-071, 10.2312/pe/eurovisshort/eurovisshort2012/067-071, 10.2312/PE/EuroVisShort/EuroVisShort2012/067-071, DOI 10.2312/PE/EUR0VISSH0RT/EUR0VISSH0RT2012/067-071]
   Behley J, 2015, IEEE INT CONF ROBOT, P3625, DOI 10.1109/ICRA.2015.7139702
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Can T, 2006, J MOL GRAPH MODEL, V25, P442, DOI 10.1016/j.jmgm.2006.02.012
   CONNOLLY ML, 1983, J APPL CRYSTALLOGR, V16, P548, DOI 10.1107/S0021889883010985
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Green S., 2007, TECHNICAL REPORTS
   GREER J, 1978, P NATL ACAD SCI USA, V75, P303, DOI 10.1073/pnas.75.1.303
   Grottel S, 2015, IEEE T VIS COMPUT GR, V21, P201, DOI 10.1109/TVCG.2014.2350479
   Hadwiger M, 2005, COMPUT GRAPH FORUM, V24, P303, DOI 10.1111/j.1467-8659.2005.00855.x
   Hermosilla P., 2015, COMPUT GRAPH, V24, P113
   Hoetzlein R.C., 2014, GPU TECHN C
   Jurcík A, 2016, IEEE PAC VIS SYMP, P112, DOI 10.1109/PACIFICVIS.2016.7465258
   Kozlikova B., 2016, COMPUT GRAPH FORUM
   Krone M., 2011, 2011 IEEE Symposium on Biological Data Visualization, P17, DOI 10.1109/BioVis.2011.6094043
   Krone M, 2009, IEEE T VIS COMPUT GR, V15, P1391, DOI 10.1109/TVCG.2009.157
   Lindow N, 2014, IEEE T VIS COMPUT GR, V20, P2486, DOI 10.1109/TVCG.2014.2346404
   Lindow N, 2010, COMPUT GRAPH FORUM, V29, P943, DOI 10.1111/j.1467-8659.2009.01693.x
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Parulek J, 2012, IEEE PAC VIS SYMP, P217, DOI 10.1109/PacificVis.2012.6183594
   Pettersen EF, 2004, J COMPUT CHEM, V25, P1605, DOI 10.1002/jcc.20084
   RICHARDS FM, 1977, ANNU REV BIOPHYS BIO, V6, P151, DOI 10.1146/annurev.bb.06.060177.001055
   Sanner MF, 1996, BIOPOLYMERS, V38, P305, DOI 10.1002/(SICI)1097-0282(199603)38:3<305::AID-BIP4>3.0.CO;2-Y
   Skånberg R, 2016, IEEE T VIS COMPUT GR, V22, P718, DOI 10.1109/TVCG.2015.2467293
   Tarini M, 2006, IEEE T VIS COMPUT GR, V12, P1237, DOI 10.1109/TVCG.2006.115
   Totrov M, 1996, J STRUCT BIOL, V116, P138, DOI 10.1006/jsbi.1996.0022
   VARSHNEY A, 1994, IEEE COMPUT GRAPH, V14, P19, DOI 10.1109/38.310720
   Xu D, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0008140
   Yu ZY, 2009, IEEE ENG MED BIO, P5909, DOI 10.1109/IEMBS.2009.5334843
NR 29
TC 10
Z9 11
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 869
EP 881
DI 10.1007/s00371-017-1397-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, YL
   Li, GQ
   Wu, SH
   Liu, YX
   Gao, YF
AF Zheng, Yinglong
   Li, Guiqing
   Wu, Shihao
   Liu, Yuxin
   Gao, Yuefang
TI Guided point cloud denoising via sharp feature skeletons
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Point cloud; Denoising; Sharp feature analysis
ID EXTRACTION
AB Feature-preserving filtering of noisy point clouds plays a fundamental role in geometric processing. Though the guided filter is known to be a powerful tool for edge-aware image processing and mesh denoising, extending it to point clouds is not a trivial task due to the difficulty of defining a piecewise smooth normal field on point clouds with sharp features. Our key idea to address the issue is to assign feature points with multiple normals according to their feature type. Specifically, our approach consists of four stages. It first screens out candidate feature points according to normal variation and then employs the -medial skeleton to extract a sharp feature structure. Following that, multiple normals are computed for each feature point by using k-means clustering. It then computes the guidance normals by using a k-nearest neighbor patch whose normals are most consistent. Point positions are finally updated according to the filtered normals. A variety of experiments suggest that our approach can robustly filter out high level of noise while keeping the important geometric features intact.
C1 [Zheng, Yinglong; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Liu, Yuxin] South China Univ Technol, Dept Comp Sci, Guangzhou, Peoples R China.
   [Wu, Shihao] Univ Bern, CGG Grp, Bern, Switzerland.
   [Gao, Yuefang] South China Agr Univ, Coll Math & Informat, Guangzhou, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; University of Bern; South China Agricultural University
RP Li, GQ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
EM ylong.zh@gmail.com; ligq@scut.edu.cn; shihao.wu312@gmail.com
RI Wu, Shihao/ADU-7023-2022
OI Wu, Shihao/0000-0001-6147-4291
FU NSFC [61572202, 61300136]; NSF of Guangdong [S2013020012795,
   2015A030313220]; Ministry of Education of China [20130172110041]
FX The authors would like to thank all the reviewers for their valuable
   comments. We are grateful to Enrico Mattei for sharing the data about
   MRPCA and Alexandre Boulch for the source code of the method in [5].
   This work is supported by NSFC (61572202, 61300136), Key Program of NSF
   of Guangdong (S2013020012795), NSF of Guangdong(2015A030313220), and
   Doctoral Funds of Ministry of Education of China (20130172110041).
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Altantsetseg E, 2013, VISUAL COMPUT, V29, P617, DOI 10.1007/s00371-013-0800-x
   Berger Matthew, 2016, SURVEY SURFACE RECON
   Bian Z, 2011, COMPUT AIDED GEOM D, V28, P50, DOI 10.1016/j.cagd.2010.10.001
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Cao Y., 2016, ARXIV160308753
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Fan HQ, 2010, IEEE T VIS COMPUT GR, V16, P312, DOI 10.1109/TVCG.2009.70
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kim HS, 2009, COMPUT AIDED DESIGN, V41, P47, DOI 10.1016/j.cad.2008.12.003
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222
   Mattei E., 2016, POINT CLOUD DENOISIN, P1
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Park MK, 2012, GRAPH MODELS, V74, P197, DOI 10.1016/j.gmod.2012.04.008
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Solomon J., 2014, ARXIV14054734V1CSGR
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Taubin Gabriel., 2001, Res. Rep. RC2213 IBM, V1, P4
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang J., 2013, CONSOLIDATION LOW QU, P207
   Wang PS, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818068
   Weber C, 2012, GRAPH MODELS, V74, P335, DOI 10.1016/j.gmod.2012.04.012
   Wei MQ, 2015, IEEE T VIS COMPUT GR, V21, P43, DOI 10.1109/TVCG.2014.2326872
   Yadav SK, 2016, CORR
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
NR 40
TC 37
Z9 44
U1 3
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 857
EP 867
DI 10.1007/s00371-017-1391-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800017
DA 2024-07-18
ER

PT J
AU Yasseen, Z
   Verroust-Blondet, A
   Nasri, A
AF Yasseen, Zahraa
   Verroust-Blondet, Anne
   Nasri, Ahmad
TI View selection for sketch-based 3D model retrieval using visual part
   shape description
SO VISUAL COMPUTER
LA English
DT Article
DE Sketch-based 3D object retrieval; 2D Shape description; Best view
   selection; Symmetry estimation; Side view
ID TIME
AB Hand drawings are the imprints of shapes in human's mind. How a human expresses a shape is a consequence of how he or she visualizes it. A query-by-sketch 3D object retrieval application is closely tied to this concept from two aspects. First, describing sketches must involve elements in a figure that matter most to a human. Second, the representative 2D projection of the target 3D objects should be limited to "the canonical views" from a human cognition perspective. We advocate for these two rules by presenting a new approach for sketch-based 3D object retrieval that describes a 2D shape by the visual protruding parts of its silhouette. Furthermore, we present a list of candidate 2D projections that represent the canonical views of a 3D object. The general rule is that humans would visually avoid part occlusion and symmetry. We quantify the extent of part occlusion of the projected silhouettes of 3Dobjects by skeletal length computations. Sorting the projected views in the decreasing order of skeletal lengths gives access to a subset of the best representative views. We experimentally show how views that cause misinterpretation and mismatching can be detected according to the part occlusion criteria. We also propose criteria for locating side, off axis, or asymmetric views.
C1 [Yasseen, Zahraa; Verroust-Blondet, Anne] Inria Paris, 2 Rue Simone Iff CS 42112, F-75589 Paris 12, France.
   [Nasri, Ahmad] CNRS, Natl Council Sci Res, Jnah, 59,Zahia Salmane St,POB 11-8281, Beirut, Lebanon.
C3 Centre National de la Recherche Scientifique (CNRS) - Lebanon
RP Yasseen, Z (corresponding author), Inria Paris, 2 Rue Simone Iff CS 42112, F-75589 Paris 12, France.
EM zyasseen@gmail.com; anne.verroust@inria.fr; anasri.82@gmail.com
OI Nasri, Ahmad/0000-0002-2047-6693; Verroust-Blondet,
   Anne/0000-0002-3365-5463; Yasseen, Zahraa/0000-0002-6808-7734
FU Lebanese National Council for Scientific Research
FX This work is supported by the Lebanese National Council for Scientific
   Research through a Ph.d. Scholarship given to Zahraa Yasseen and a
   research grant awarded to Ahmad Nasri.
CR Al-Naymat G, 2009, AUSTRALASIAN DATA MI, V101, P117, DOI DOI 10.1007/s10115-004-0154-9
   [Anonymous], 2013, P EUR WORKSH 3D OBJ, DOI DOI 10.2312/3DOR/3DOR13/049-056
   [Anonymous], 1981, Attention and Performance
   [Anonymous], P 1 ACM INT C MULT R
   Aono M, 2012, SIGN INF PROC ASS AN, V3
   Bertamini M, 2013, PSYCHON B REV, V20, P191, DOI 10.3758/s13423-012-0347-2
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Cohen EH, 2007, VISION RES, V47, P2825, DOI 10.1016/j.visres.2007.06.021
   De Winter J, 2008, PERCEPTION, V37, P245, DOI 10.1068/p5429
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Furuya T, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P274, DOI 10.1109/CW.2013.60
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Lemire D, 2009, PATTERN RECOGN, V42, P2169, DOI 10.1016/j.patcog.2008.11.030
   LI B., 2012, EurographicsWorkshop on 3D Object Retrieval, P109
   Li B., 2013, 2013 IEEE International Symposium on Sensorless Control for Electrical Drives and Predictive Control of Electrical Drives and Power Electronics (SLED/PRECEDE), P1, DOI DOI 10.1109/ICMEW.2013.6618316
   Li B., 2013, EUR WORKSH 3D OBJ RE, P89, DOI DOI 10.2312/3DOR/3DOR13/089
   Li B, 2012, MULTIMED TOOLS APPL, V61
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li B, 2014, COMPUT VIS IMAGE UND, V119, P57, DOI 10.1016/j.cviu.2013.11.008
   Mezuman E., 2012, Advances in neural information processing systems, P719
   Napoléon T, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/367181
   Neri P, 2009, P ROY SOC B-BIOL SCI, V276, P861, DOI 10.1098/rspb.2008.1363
   Ohbuchi Ryutarou, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P63, DOI 10.1109/ICCVW.2009.5457716
   Prasad L, 2007, IMAGE VISION COMPUT, V25, P1557, DOI 10.1016/j.imavis.2006.06.025
   Saavedra J.M., 2012, 3DOR, P47
   Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Shao TJ, 2011, COMPUT GRAPH FORUM, V30, P2011, DOI 10.1111/j.1467-8659.2011.02050.x
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Vintsyuk T. K., 1968, Cybernetics, V4, P52, DOI 10.1007/BF01074755
   Wang XY, 2013, DATA MIN KNOWL DISC, V26, P275, DOI 10.1007/s10618-012-0250-5
   Yasseen Z, 2016, PATTERN RECOGN, V57, P115, DOI 10.1016/j.patcog.2016.03.022
   Yasseen Z., 2015, 3DOR 2015, P8, DOI [10.2312/3dor.20151053, DOI 10.2312/3DOR.20151053]
   YOON S.M., 2010, Proceedings of the international conference on Multimedia, P193
   Zhao L, 2015, VISUAL COMPUT, V31, P765, DOI 10.1007/s00371-015-1091-1
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zou CQ, 2014, IEEE SIGNAL PROC LET, V21, P966, DOI 10.1109/LSP.2014.2321764
NR 41
TC 14
Z9 19
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 565
EP 583
DI 10.1007/s00371-016-1328-7
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4KB
UT WOS:000398767300003
DA 2024-07-18
ER

PT J
AU Banerjee, I
   Agibetov, A
   Catalano, CE
   Patané, G
   Spagnuolo, M
AF Banerjee, Imon
   Agibetov, Asan
   Catalano, Chiara Eva
   Patane, Giuseppe
   Spagnuolo, Michela
TI Semantics-driven annotation of patient-specific 3D data: a step to
   assist diagnosis and treatment of rheumatoid arthritis
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic annotation; Patient-specific 3D model; Computer aided
   diagnosis; Rheumatoid arthritis; 3D indexing
ID SYSTEM
AB In the digital era, patient-specific 3D models (3D-PSMs) are becoming increasingly relevant in computer-assisted diagnosis, surgery training on digital models, or implant design. While advanced imaging and reconstruction techniques can create accurate and detailed 3D models of patients' anatomy, software tools that are able to fully exploit the potential of 3D-PSMs are still far from being satisfactory. In particular, there is still a lack of integrated approaches for extracting, coding, sharing and retrieving medically relevant information from 3D-PSMs and use it concretely as a support to diagnosis and treatment. In this article, we propose the SemAnatomy3D framework, which demonstrates how the ontology-driven annotation of 3D-PSMs and of their anatomically relevant features (parts of relevance) can assist clinicians to document more effectively pathologies and their evolution. We exemplify the idea in the context of the diagnosis of rheumatoid arthritis of the hand district, and show how feature extraction tools and semantic 3D annotation can provide a rich characterization of anatomical landmarks (e.g., articular facets, prominent features, ligament attachments) and pathological markers (erosions, bone loss). The core contributions are an ontology-driven part-based annotation method for the 3D-PSMs and a novel automatic localization of erosion and quantification of the OMERACT RAMRIS erosion score. Finally, our results have been compared against a medical ground truth.
C1 [Banerjee, Imon] CNR IMATI, Shape Modeling Grp, Via Marini 6, Genoa, Italy.
   [Agibetov, Asan; Catalano, Chiara Eva; Patane, Giuseppe; Spagnuolo, Michela] CNR IMATI, Via Marini 6, Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR);
   Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Banerjee, I (corresponding author), CNR IMATI, Shape Modeling Grp, Via Marini 6, Genoa, Italy.
EM imon.banerjee@ge.imati.cnr.it; asan.agibetov@ge.imati.cnr.it;
   chiara.catalano@ge.imati.cnr.it; giuseppe.patane@ge.imati.cnr.it;
   michela.spagnuolo@ge.imati.cnr.it
RI Agibetov, Asan/AAE-5207-2021; Patane', Giuseppe/O-1322-2013; Spagnuolo,
   Michela/ABA-1927-2021
OI Agibetov, Asan/0000-0003-0096-0143; Spagnuolo,
   Michela/0000-0002-5682-6990
FU FP7 Marie Curie Initial Training Network "MultiScaleHuman": Multi-scale
   Biological Modalities for Physiological Human Articulation; Project
   FAS-MEDIARE "Nuove metodologie di Imaging Diagnostico per patologie
   reumatiche"
FX This work is supported by the FP7 Marie Curie Initial Training Network
   "MultiScaleHuman": Multi-scale Biological Modalities for Physiological
   Human Articulation (2011-2015). This work is also partially supported by
   the Project FAS-MEDIARE "Nuove metodologie di Imaging Diagnostico per
   patologie reumatiche". The carpal data set is provided by Softeco Sismat
   Srl in collaboration with DIMI (Dipartimento di Medicina Interna,
   Clinica Reumatologica, Universita degli Studi di Genova).
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Banerjee I., 2015, INT C CYB CW
   Banerjee I., 2015, SMART TOOLS APPS GRA, DOI [10.2312/stag.20151292, DOI 10.2312/STAG.20151292]
   Banerjee I., 2015, INT J COMPUT ASSIST, P1
   Banerjee I, 2015, LECT NOTES COMPUT SC, V9281, P167, DOI 10.1007/978-3-319-23222-5_21
   Boeker M., 2012, P ICBO 12 GRAZ AUSTR
   Ejbjerg B, 2005, ANN RHEUM DIS, V64, P23, DOI 10.1136/ard.2004.031823
   Hunter J., 2010, The Drucker difference: What the world's greatest management thinker means to today's business leaders, P175
   Jimnez-Ruiz E., 2008, LECT NOTES COMPUTER, V5021, P185
   Kahan J, 2002, COMPUT NETW, V39, P589, DOI 10.1016/S1389-1286(02)00220-7
   Langlotz CP, 2006, RADIOGRAPHICS, V26, P1595, DOI 10.1148/rg.266065168
   Lukovic V., 2009, UBIQUITOUS COMPUTING, V4, P664
   Mejino Jose L V, 2008, AMIA Annu Symp Proc, P465
   Möller M, 2009, LECT NOTES COMPUT SC, V5554, P21, DOI 10.1007/978-3-642-02121-3_6
   Mork P, 2003, J BIOMED INFORM, V36, P501, DOI 10.1016/j.jbi.2003.11.004
   MSH, MSH ONT DEL REP D8 2
   Ostergaard M, 2003, J RHEUMATOL, V30, P1385
   Parascandolo P, 2015, LECT NOTES COMPUT SC, V9281, P135, DOI 10.1007/978-3-319-23222-5_17
   Rubin Daniel L, 2008, AMIA Annu Symp Proc, P626
   Sanderson R., 2013, OPEN ANNOTA IN PRESS
   Seifert S., 2010, SPIE MED IMAGING, V7628, P762
   Tomatis V, 2015, LECT NOTES COMPUT SC, V9281, P143, DOI 10.1007/978-3-319-23222-5_18
   Woolf AD, 2003, B WORLD HEALTH ORGAN, V81, P646
   World Health Organization, 1992, ICD 10 CLASS MENT BE
NR 24
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1337
EP 1349
DI 10.1007/s00371-016-1226-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800011
DA 2024-07-18
ER

PT J
AU Guo, YH
   Lin, SJ
   Su, Z
   Luo, XN
   Wang, RM
   Kang, Y
AF Guo, Yihui
   Lin, Shujin
   Su, Zhuo
   Luo, Xiaonan
   Wang, Ruomei
   Kang, Yang
TI A 3D model perceptual feature metric based on global height field
SO VISUAL COMPUTER
LA English
DT Article
DE Perceptual feature points metric; Human visual attention; Global height
   field; Spectral method
ID GEOMETRIC FEATURES; MESH SALIENCY; REGIONS; POINT
AB Human visual attention system tends to be attracted to perceptual feature points on 3D model surfaces. However, purely geometric-based feature metrics may be insufficient to extract perceptual features, because they tend to detect local structure details. Intuitively, the perceptual importance degree of vertex is associated with the height of its geometry position between original model and a datum plane. So, we propose a novel and straightforward method to extract perceptually important points based on global height field. Firstly, we construct spectral domain using Laplace-Beltrami operator, and we perform spectral synthesis to reconstruct a rough approximation of the original model by adopting low-frequency coefficients, and make it as the 3D datum plane. Then, to build global height field, we calculate the Euclidean distance between vertex geometry position on original surface and the one on 3D datum plane. Finally, we set a threshold to extract perceptual feature vertices. We implement our technique on several 3D mesh models and compare our algorithm to six state-of-the-art interest points detection approaches. Experimental results demonstrate that our algorithm can accurately capture perceptually important points on arbitrary topology 3D model.
C1 [Guo, Yihui; Lin, Shujin; Su, Zhuo; Luo, Xiaonan; Wang, Ruomei; Kang, Yang] Sun Yat Sen Univ, Sch Data & Comp Sci, Natl Engn Res Ctr Digital Life, Guangzhou 510006, Guangdong, Peoples R China.
   [Guo, Yihui] Guangdong Univ Finance, Dept Internet Finance & Informat Technol, Guangzhou 510521, Guangdong, Peoples R China.
   [Lin, Shujin] Sun Yat Sen Univ, Sch Commun & Design, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Guangdong University of Finance; Sun Yat Sen
   University
RP Lin, SJ (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Natl Engn Res Ctr Digital Life, Guangzhou 510006, Guangdong, Peoples R China.; Lin, SJ (corresponding author), Sun Yat Sen Univ, Sch Commun & Design, Guangzhou 510006, Guangdong, Peoples R China.
EM gzgdufguo@126.com
RI Su, Zhuo/AAO-4506-2020
OI Su, Zhuo/0000-0002-6090-0110
FU Natural Science Foundation of China (NSFC) [61232011, 61320106008];
   NSFC-Guangdong Joint Fund [U1135003]; National Natural Science
   Foundation of China [61502541]
FX This research is supported by Natural Science Foundation of China (NSFC)
   (Nos. 61232011, 61320106008), NSFC-Guangdong Joint Fund (No. U1135003),
   National Natural Science Foundation of China (No. 61502541). The 3D
   models are courtesy of Stanford University, Princeton University and the
   Aim@Shape.
CR [Anonymous], VIS MATH
   [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], IS T SPIE ELECT IMAG
   [Anonymous], COMPUTER GRAPHICS
   [Anonymous], 2011, P EUROGRAPHICS WORKS
   [Anonymous], 2011, VISUAL COMPUT, DOI DOI 10.1007/s00371-011-0610-y
   Bronstein A.M., 2010, EUROGRAPHICS Workshop on 3D Object Retrieval, V2, P6
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Laga H, 2011, VISUAL COMPUT, V27, P977, DOI 10.1007/s00371-011-0628-1
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Levy Bruno., 2010, ACM SIGGRAPH 2010 Courses, P8
   Litman R, 2011, COMPUT GRAPH-UK, V35, P549, DOI 10.1016/j.cag.2011.03.011
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Miao YW, 2011, COMPUT GRAPH-UK, V35, P706, DOI 10.1016/j.cag.2011.03.017
   Novatnack J., 2007, IEEE 11 INT C COMPUT, P1
   Öztireli AC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866190
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Rong GD, 2008, VISUAL COMPUT, V24, P787, DOI 10.1007/s00371-008-0260-x
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Sipiran I, 2013, VISUAL COMPUT, V29, P1319, DOI 10.1007/s00371-013-0870-9
   Song R, 2013, VISUAL COMPUT, V29, P695, DOI 10.1007/s00371-013-0806-4
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
NR 34
TC 3
Z9 3
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1151
EP 1164
DI 10.1007/s00371-015-1199-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400008
DA 2024-07-18
ER

PT J
AU Li, Y
   Ye, JY
   Wang, TQ
   Huang, SJ
AF Li, Yang
   Ye, Junyong
   Wang, Tongqing
   Huang, Shijian
TI Augmenting bag-of-words: a robust contextual representation of
   spatiotemporal interest points for action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Contextual features; Cumulative probability
   histogram; Sparse coding
ID APPEARANCE; FEATURES
AB Although traditional bag-of-words model, together with local spatiotemporal features, has shown promising results for human action recognition, it ignores all structural information of features, which carries important information of motion structures in videos. Recent methods usually characterize the relationship of quantized spatiotemporal features to overcome this drawback. However, the propagation of quantization error leads to an unreliable representation. To alleviate the propagation of quantization error, we present a coding method, which considers not only the spatial similarity but also the reconstruction ability of visual words after giving a probabilistic interpretation of coding coefficients. Based on our coding method, a new type of feature called cumulative probability histogram is proposed to robustly characterize contextual structural information around interest points, which are extracted from multi-layered contexts and assumed to be complementary to local spatiotemporal features. The proposed method is verified on four benchmark datasets. Experiment results show that our method can achieve better performance than previous methods in action recognition.
C1 [Li, Yang; Ye, Junyong; Wang, Tongqing; Huang, Shijian] Chongqing Univ, Minist Educ, Key Lab Optoelect Technol & Syst, Chongqing 630044, Peoples R China.
C3 Chongqing University
RP Ye, JY (corresponding author), Chongqing Univ, Minist Educ, Key Lab Optoelect Technol & Syst, Chongqing 630044, Peoples R China.
EM liyheartdarling@gmail.com; ygyocr@cqu.edu.cn; ocr@cqu.edu.cn;
   huangshijian@cqu.edu.cn
RI Li, Yang/B-2227-2012; junyong, ye/E-5576-2010; wang, tong/HTR-5412-2023
OI Li, Yang/0000-0002-0090-2894; junyong, ye/0000-0001-8864-9987
FU Fundamental Research Funds for the Central Universities of China
   [106112013CDJZR120014]; Scientific and Technological Research Program of
   Chongqing Municipal Education Commission of China [KJ1401207]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities of China under Grant 106112013CDJZR120014 and
   Scientific and Technological Research Program of Chongqing Municipal
   Education Commission of China under Grant KJ1401207.
CR Ahad MAR, 2012, MACH VISION APPL, V23, P255, DOI 10.1007/s00138-010-0298-4
   [Anonymous], P WMVC
   [Anonymous], 2008, PROC BRIT MACH VIS C
   [Anonymous], 2009, P BRIT MACH VIS C
   Bilinski P, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P228, DOI 10.1109/AVSS.2012.29
   Bregonzio M, 2012, PATTERN RECOGN, V45, P1220, DOI 10.1016/j.patcog.2011.08.014
   Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779
   Choi J., 2008, ACM ICMR, P291
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31
   Kliper-Gross O, 2012, LECT NOTES COMPUT SC, V7577, P256, DOI 10.1007/978-3-642-33783-3_19
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu JE, 2012, COMPUT VIS IMAGE UND, V116, P361, DOI 10.1016/j.cviu.2011.08.010
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Raptis M, 2010, LECT NOTES COMPUT SC, V6311, P577, DOI 10.1007/978-3-642-15549-9_42
   Rodriguez M. D., 2008, P IEEE INT C COMPUTE, P1
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995493
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu XX, 2011, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2011.5995624
   Yuan CF, 2013, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2013.99
   Zhao DJ, 2013, NEUROCOMPUTING, V113, P88, DOI 10.1016/j.neucom.2013.01.022
NR 41
TC 16
Z9 16
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1383
EP 1394
DI 10.1007/s00371-014-1020-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800007
DA 2024-07-18
ER

PT J
AU Li, ZY
   He, S
   Hashem, M
AF Li, Zhiyong
   He, Shuang
   Hashem, Mervat
TI Robust object tracking via multi-feature adaptive fusion based on
   stability: contrast analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Feature fusion; Multi-feature joint descriptor;
   Stability; Contrast
ID VISUAL TRACKING; SELECTION
AB Object tracking under complex circumstances is a challenging task because of background interference, obstacle occlusion, object deformation, etc. Given such conditions, robustly detecting, locating, and analyzing a target through single-feature representation are difficult tasks. Global features, such as color, are widely used in tracking, but may cause the object to drift under complex circumstances. Local features, such as HOG and SIFT, can precisely represent rigid targets, but these features lack the robustness of an object in motion. An effective method is adaptive fusion of multiple features in representing targets. The process of adaptively fusing different features is the key to robust object tracking. This study uses a multi-feature joint descriptor (MFJD) and the distance between joint histograms to measure the similarity between a target and its candidate patches. Color and HOG features are fused as the tracked object of the joint representation. This study also proposes a self-adaptive multi-feature fusion strategy that can adaptively adjust the joint weight of the fused features based on their stability and contrast measure scores. The mean shift process is adopted as the object tracking framework with multi-feature representation. The experimental results demonstrate that the proposed MFJD tracking method effectively handles background clutter, partial occlusion by obstacles, scale changes, and deformations. The novel method performs better than several state-of-the-art methods in real surveillance scenarios.
C1 [Li, Zhiyong; He, Shuang; Hashem, Mervat] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
C3 Hunan University
RP Li, ZY (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM zhiyong.li@hnu.edu.cn
RI li, zy/HZM-1892-2023
FU National Natural Science Foundation of China [91320103]; National high
   technology research and development program (863) [2012AA01A301-01];
   Research Foundation of Industry-education-research Cooperation among
   Guangdong Province; Ministry of Education and Ministry of science and
   Technology, China [2011A091000027]; Research Foundation of
   Industry-education-research Cooperation of Huizhou, Guangdong
   [2012C050012012]
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant No. 91320103), National high technology
   research and development program (863) (Grant No. 2012AA01A301-01), the
   Research Foundation of Industry-education-research Cooperation among
   Guangdong Province, Ministry of Education and Ministry of science and
   Technology, China (Grant No. 2011A091000027) and the Research Foundation
   of Industry-education-research Cooperation of Huizhou, Guangdong (Grant
   No. 2012C050012012).
CR [Anonymous], BMVC
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Badrinarayanan V, 2007, IEEE I CONF COMP VIS, P1000
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Leichter I, 2006, INT J COMPUT VISION, V67, P343, DOI 10.1007/s11263-006-5568-2
   Li G., 2011, ONLINE SELECTION BES, V23, P254
   Luo HY, 2010, IEEE WIREL COMMUN, V17, P44, DOI 10.1109/MWC.2010.5416349
   NEDOVIC V, 2005, KERNEL BASED OBJECT
   Nguyen THD, 2005, IEEE T VIS COMPUT GR, V11, P706, DOI 10.1109/TVCG.2005.105
   Ning J, 2012, IET COMPUT VIS, V6, P62, DOI 10.1049/iet-cvi.2009.0075
   Spengler M, 2003, MACH VISION APPL, V14, P50, DOI 10.1007/s00138-002-0095-9
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Stern H, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P249, DOI 10.1109/AFGR.2002.1004162
   Sun L, 2011, IEEE T CIRC SYST VID, V21, P408, DOI 10.1109/TCSVT.2010.2087815
   Sun L, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2009.5204358
   Wang J.X., 2006, OTCBVS, P137
   Wang JQ, 2008, IEEE T IMAGE PROCESS, V17, P235, DOI 10.1109/TIP.2007.914150
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P3296, DOI 10.1109/TIP.2012.2190085
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wu Bo., 2008, PROC IEEE C COMPUTER, P1, DOI DOI 10.1109/GLOCOM.2008.ECP.519
   Yang F, 2014, IEEE T CIRC SYST VID, V24, P242, DOI 10.1109/TCSVT.2013.2276145
   Yoon JH, 2012, LECT NOTES COMPUT SC, V7575, P28, DOI 10.1007/978-3-642-33765-9_3
   Zhang KH, 2013, IEEE T CIRC SYST VID, V23, P1957, DOI 10.1109/TCSVT.2013.2269772
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zhou HY, 2009, COMPUT VIS IMAGE UND, V113, P345, DOI 10.1016/j.cviu.2008.08.006
   Zoidi O, 2013, IEEE T CIRC SYST VID, V23, P870, DOI 10.1109/TCSVT.2012.2226527
NR 33
TC 19
Z9 22
U1 0
U2 32
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1319
EP 1337
DI 10.1007/s00371-014-1014-6
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800003
DA 2024-07-18
ER

PT J
AU Company, P
   Plumed, R
   Varley, PAC
AF Company, Pedro
   Plumed, Raquel
   Varley, Peter A. C.
TI A fast approach for perceptually-based fitting strokes into elliptical
   arcs
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Computer-aided sketching; Sketch strokes; Fitting primitives to strokes;
   Elliptical arcs; Perceptual fit; Fast fit
ID CORNER DETECTION
AB Fitting elliptical arcs to strokes of an input sketch is discussed. We describe an approach which automatically combines existing algorithms to get a balance of speed and precision. For measuring precision, we introduce fast metrics which are based on perceptual criteria and are tolerant of sketching imperfections. We return a likelihood estimate based on these metrics rather than deterministic yes/no result, in order that the approach can be used in higher-level collaborative-decision recognition flows.
C1 [Company, Pedro; Varley, Peter A. C.] Univ Jaume 1, Inst New Imaging Technol, Castellon de La Plana 12071, Spain.
   [Plumed, Raquel] Univ Jaume 1, Dept Mech Engn & Construct, Castellon de La Plana 12071, Spain.
C3 Universitat Jaume I; Universitat Jaume I
RP Company, P (corresponding author), Univ Jaume 1, Inst New Imaging Technol, Av Sos Baynat S-N, Castellon de La Plana 12071, Spain.
EM pcompany@uji.es
RI Company, Pedro/G-2567-2016; Plumed, Raquel/C-4873-2015
OI Company, Pedro/0000-0001-6399-4717; Plumed, Raquel/0000-0001-8018-8039
FU Ramon y Cajal Scholarship Programme; Pla de Promocio de la Investigacio
   de la Universitat Jaume I [P1 1B2010-01]
FX This work was partially funded by financial support from the Ramon y
   Cajal Scholarship Programme and by the "Pla de Promocio de la
   Investigacio de la Universitat Jaume I", Project P1 1B2010-01. We wish
   to thank Salvador Mondragon, who collected many questionnaires from his
   students, and Margarita Vergara, for her contribution to statistical
   data treatment.
CR [Anonymous], 2008, ADV DIGITAL SIGNAL P
   Boyer KL, 1999, COMPUT VIS IMAGE UND, V76, P1, DOI 10.1006/cviu.1999.0797
   Chernov N., 2014, Br. J. Math. Comput. Sci, V4, P33, DOI DOI 10.9734/BJMCS/2014/6016
   Company P., 2015, 092015 GEOM REC GROU
   Company P., 2015, SOURC COD FITT ELL
   Davis T., 1996, CODE FIND EQUATION C
   DURBIN J, 1959, BIOMETRIKA, V46, P306, DOI 10.1093/biomet/46.3-4.306
   Eberly D., 2004, 3D GAME ENGINE DESIG
   Elsen C, 2012, AI EDAM, V26, P281, DOI 10.1017/S0890060412000157
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Halir R, 1998, WSCG '98, VOL 1, P125
   Hoffmann D.D., 1998, VISUAL INTELLIGENCE
   Kotagiri S., 2010, C IMPLEMENTATION FIT
   Kumar P, 2013, IEEE IMAGE PROC, P815, DOI 10.1109/ICIP.2013.6738168
   Masood A, 2007, COMPUT GRAPH-UK, V31, P440, DOI 10.1016/j.cag.2007.01.021
   Press W. H., 2007, ART SCI COMPUTING
   Pusch R, 2007, VISUAL COMPUT, V23, P955, DOI 10.1007/s00371-007-0160-5
   ROSIN PL, 1993, PATTERN RECOGN LETT, V14, P661, DOI 10.1016/0167-8655(93)90052-F
   Saund E., 1994, UIST '94. Seventh Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P175, DOI 10.1145/192426.192494
   Shpitalni M, 1997, J MECH DESIGN, V119, P131, DOI 10.1115/1.2828775
   Szpak Z. L., RES THEMES ELLIPSE F
   Szpak Z.L., 2012, P 12 EUR C COMP VIS, V7576, P87
   Wang SX, 2014, VISUAL COMPUT, V30, P285, DOI 10.1007/s00371-013-0844-y
   Wolin A., 2008, P EUR WORKSH SKETCH, P33, DOI DOI 10.2312/SBM/SBM08/033-040
   Xiong YY, 2010, COMPUT GRAPH-UK, V34, P513, DOI 10.1016/j.cag.2010.06.008
   Yu JQ, 2012, PATTERN RECOGN LETT, V33, P492, DOI 10.1016/j.patrec.2011.11.025
NR 26
TC 7
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 775
EP 785
DI 10.1007/s00371-015-1099-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500004
DA 2024-07-18
ER

PT J
AU Dou, H
   Baker, ML
   Ju, T
AF Dou, Hang
   Baker, Matthew L.
   Ju, Tao
TI Graph-based deformable matching of 3D line with application in protein
   fitting
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Non-rigid; Graph matching; Quadratic assignment; Line feature
ID ISOMETRIC REGISTRATION; ALGORITHM
AB We present an algorithm for matching two sets of line segments in 3D that have undergone non-rigid deformations. This problem is motivated by a biology application that seeks a correspondence between the alpha-helices from two proteins, so that matching helices have similar lengths and these can be aligned by some low-distortion deformation. While matching between two feature sets have been extensively studied, particularly for point features, matching line segments has received little attention so far. As typical in point-matching methods, we formulate a graph matching problem and solve it using continuous relaxation. We make two technical contributions. First, we propose a graph construction for undirected line segments such that the optimal matching between two graphs represents an as-rigid-as-possible deformation between the two sets of segments. Second, we propose a novel heuristic for discretizing the continuous solution in graph matching. Our heuristic can be applied to matching problems (such as ours) that are not amenable to certain heuristics, and it produces better solutions than those applicable heuristics. Our method is compared with a state-of-art method motivated by the same biological application and demonstrates improved accuracy.
C1 [Dou, Hang; Ju, Tao] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
   [Baker, Matthew L.] Baylor Coll Med, Dept Biochem & Mol Biol, Houston, TX 77030 USA.
C3 Washington University (WUSTL); Baylor College of Medicine
RP Dou, H (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM hangdou@gmail.com
FU NSF [DBI-1356388, DBI-1356306]; NIH [5P41GM103832, 2R01GM079429,
   R21GM100229]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1319944] Funding Source: National
   Science Foundation; Div Of Biological Infrastructure; Direct For
   Biological Sciences [1356306, 1356388] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1319573] Funding Source: National Science
   Foundation
FX The work is supported in part by the NSF grants (DBI-1356388,
   DBI-1356306) and the NIH grants (5P41GM103832, 2R01GM079429,
   R21GM100229).
CR Abeysinghe SS, 2010, COMPUT GRAPH FORUM, V29, P2243, DOI 10.1111/j.1467-8659.2010.01813.x
   Anguelov Dragomir., 2004, Advances in Neural Information Processing Systems
   Baker ML, 2007, STRUCTURE, V15, P7, DOI 10.1016/j.str.2006.11.008
   Berman H, 2003, NAT STRUCT BIOL, V10, P980, DOI 10.1038/nsb1203-980
   Chang W, 2009, COMPUT GRAPH FORUM, V28, P447, DOI 10.1111/j.1467-8659.2009.01384.x
   Chertok M, 2010, IEEE T PATTERN ANAL, V32, P2205, DOI 10.1109/TPAMI.2010.51
   Chiu W, 2006, TRENDS CELL BIOL, V16, P144, DOI 10.1016/j.tcb.2006.01.002
   Cour T., 2007, Advances in Neural Information Processing Systems, V19, P313
   Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110
   Feng W, 2013, VISUAL COMPUT, V29, P53, DOI 10.1007/s00371-012-0674-3
   Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619
   HORAUD R, 1989, IEEE T PATTERN ANAL, V11, P1168, DOI 10.1109/34.42855
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Leordeanu M., 2009, NIPS, P1114
   Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032
   Lu Wang, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2623, DOI 10.1109/CVPRW.2009.5206600
   Schellewald C, 2005, LECT NOTES COMPUT SC, V3757, P171, DOI 10.1007/11585978_12
   Sheng-Lin Chou, 1993, Machine Vision and Applications, V6, P191, DOI 10.1007/BF01212297
   Tam GKL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2517967
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tevs A, 2009, PROC CVPR IEEE, P1185, DOI 10.1109/CVPRW.2009.5206775
   Topf M, 2008, STRUCTURE, V16, P295, DOI 10.1016/j.str.2007.11.016
   Trabuco LG, 2008, STRUCTURE, V16, P673, DOI 10.1016/j.str.2008.03.005
   ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925
   van Kaick O, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P271, DOI 10.1109/PG.2007.56
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Yu ZY, 2006, IEEE IMAGE PROC, P2513, DOI 10.1109/ICIP.2006.312804
   Zheng WS, 2011, BIOPHYS J, V100, P478, DOI 10.1016/j.bpj.2010.12.3680
   Zheng WJ, 2005, BIOPHYS J, V88, P3109, DOI 10.1529/biophysj.104.058453
   Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81
NR 30
TC 4
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 967
EP 977
DI 10.1007/s00371-015-1115-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500022
DA 2024-07-18
ER

PT J
AU Paulus, CJ
   Untereiner, L
   Courtecuisse, H
   Cotin, S
   Cazier, D
AF Paulus, Christoph J.
   Untereiner, Lionel
   Courtecuisse, Hadrien
   Cotin, Stephane
   Cazier, David
TI Virtual cutting of deformable objects based on efficient topological
   operations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Real-time; Simulation; Virtual reality; Topological changes; Cutting and
   tearing; Deformable models; Medical applications
ID COMPOSITE FINITE-ELEMENTS
AB Virtual cutting of deformable objects is at the core of many applications in interactive simulation and especially in computational medicine. The ability to simulate surgical cuts, dissection, soft tissue tearing ormicro-fractures is essential for augmenting the capabilities of existing or future simulation systems. To support such features, we combine a new remeshing algorithm with a fast finite element approach. The proposed method is generic enough to support a large variety of applications. We show the benefits of our approach evaluating the impact of cuts on the number of nodes and the numerical quality of the mesh. These points are crucial to ensure accurate and stable real-time simulations.
C1 [Paulus, Christoph J.; Untereiner, Lionel; Cotin, Stephane] Inria Nancy Grand Est, F-54600 Villers Les Nancy, France.
   [Paulus, Christoph J.; Untereiner, Lionel; Courtecuisse, Hadrien; Cazier, David] Univ Strasbourg, ICube Lab, F-67412 Illkirch Graffenstaden, France.
   [Paulus, Christoph J.; Untereiner, Lionel; Courtecuisse, Hadrien; Cazier, David] CNRS, ICube Lab, F-67412 Illkirch Graffenstaden, France.
C3 Universite de Lorraine; Universites de Strasbourg Etablissements
   Associes; Universite de Strasbourg; Centre National de la Recherche
   Scientifique (CNRS)
RP Paulus, CJ (corresponding author), Inria Nancy Grand Est, F-54600 Villers Les Nancy, France.
EM christoph.paulus@inria.fr
OI Cazier, David/0000-0001-5247-6404; Untereiner,
   Lionel/0000-0002-8025-2616
CR [Anonymous], 2004, SGP '04: Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing
   Bey J, 1995, COMPUTING, V55, P355, DOI 10.1007/BF02238487
   Bielser D, 2004, GRAPH MODELS, V66, P398, DOI 10.1016/j.gmod.2004.05.009
   Bielser D, 1999, COMPUT GRAPH FORUM, V18, pC31, DOI 10.1111/1467-8659.00325
   Burkhart D, 2010, COMPUT GRAPH FORUM, V29, P117, DOI 10.1111/j.1467-8659.2009.01581.x
   Chentanez N, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531394
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P1663, DOI 10.1109/TVCG.2010.268
   Faure F., 2012, ser. Studies in Mechanobiology, Tissue Engineering and Biomaterials, P283
   Felippa CA, 2005, COMPUT METHOD APPL M, V194, P2285, DOI 10.1016/j.cma.2004.07.035
   Ganovelli F, 2000, COMPUT GRAPH FORUM, V19, pC271, DOI 10.1111/1467-8659.00419
   Hackbusch W, 1997, NUMER MATH, V75, P447, DOI 10.1007/s002110050248
   Kohn L.T., 2000, To err is human: Building a safer health system, V627
   Koschier D., 2014, P 2014 ACM SIGGRAPH
   Kraemer P., 2013, P 22 INT MESH ROUNDT, P485, DOI DOI 10.1007/978-3-319-02335-9_27
   Molino N., 2005, "ACM SIGGRAPH 2005 Courses," SIGGRAPH '05
   Mor AB, 2000, LECT NOTES COMPUT SC, V1935, P598
   Nienhuys H.-W., 2001, MICCAI 01, P145
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Payan Y., 2012, SOFT TISSUE BIOMECHA, V11
   RIVARA MC, 1991, J COMPUT APPL MATH, V36, P79, DOI 10.1016/0377-0427(91)90227-B
   Sauter S, 2006, COMPUTING, V77, P29, DOI 10.1007/s00607-005-0150-2
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   Stanculescu L, 2013, COMPUT GRAPH-UK, V37, P753, DOI 10.1016/j.cag.2013.05.010
   Steinemann D, 2006, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2006.74
   Wu J., 2014, EUROGRAPHICS 2014 ST, P1
   Wu J., 2011, VRIPHYS, P29
   Wu Jun, 2014, Stud Health Technol Inform, V196, P469
   Zhang YJ, 2010, COMPUT METHOD APPL M, V199, P405, DOI 10.1016/j.cma.2009.06.007
NR 30
TC 33
Z9 37
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 831
EP 841
DI 10.1007/s00371-015-1123-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500009
DA 2024-07-18
ER

PT J
AU Papageorgiou, A
   Platis, N
AF Papageorgiou, Alexandros
   Platis, Nikos
TI Triangular mesh simplification on the GPU
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Mesh simplification; Edge collapse; GPU; OpenCL
AB We present a simplification algorithm for triangular meshes, implemented on the GPU. The algorithm performs edge collapses on manifold triangular meshes driven by a quadric error metric. It uses data parallelism as provided by OpenCL and has no sequential segments in its main iterative structure in order to fully exploit the processing power of the GPU. Our implementation produces results faster than a corresponding sequential implementation and the resulting models are of comparable quality.
C1 [Papageorgiou, Alexandros; Platis, Nikos] Univ Peloponnese, Dept Informat & Telecommun, Tripolis, Greece.
C3 University of Peloponnese
RP Platis, N (corresponding author), Univ Peloponnese, Dept Informat & Telecommun, Tripolis, Greece.
EM alexp@uop.gr; nplatis@uop.gr
OI PAPAGEORGIOU, ALEXANDROS/0000-0002-3460-5476
CR [Anonymous], 1973, The art of computer programming
   [Anonymous], 2008, MESHLAB OPEN SOURCE, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136
   DeCoro C, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P161
   Franc M., 2001, P INT C SCCG, P164
   Garland M., 1999, THESIS CARNEGIE MELL
   Hjelmervik J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P91, DOI 10.1109/SMI.2007.17
   Munshi A., 2009, OPENCL SPECIFICATION
   Peters H, 2010, LECT NOTES COMPUT SC, V6067, P403
   Silva S, 2009, COMPUT GRAPH-UK, V33, P181, DOI 10.1016/j.cag.2008.09.014
NR 9
TC 22
Z9 26
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 235
EP 244
DI 10.1007/s00371-014-1039-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800011
DA 2024-07-18
ER

PT J
AU Zhou, LY
   Lu, ZW
   Leung, H
   Shang, LF
AF Zhou, Liuyang
   Lu, Zhiwu
   Leung, Howard
   Shang, Lifeng
TI Spatial temporal pyramid matching using temporal sparse representation
   for human motion retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Motion retrieval; Temporal sparse representation; Spatial temporal
   pyramid matching; Sparse coding; Motion capture
AB An efficient retrieval mechanism is essential to search for a particular motion from a large corpus. This has proven to be a challenging task as human motion is high dimensional in both spatial and temporal domains. Besides, semantically similar motions are not necessary numerically similar because of the speed variations. In this paper, we propose a temporal sparse representation (TSR) for human motion retrieval. Compared with existing methods that adopt sparse representation, our TSR encodes the temporal information within motions and thus generates a more compact and discriminative representation. In addition, we propose a spatial temporal pyramid matching kernel based on TSR, which can be used for logical comparison between motions. Moreover, it improves the effectiveness of motion retrieval in terms of accuracy and speed. Through our experimental evaluations, we demonstrate that the proposed human motion retrieval system has better performance and allows the user to retrieve desired motions from the motion capture database.
C1 [Zhou, Liuyang; Leung, Howard] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Lu, Zhiwu] Renmin Univ China, Sch Informat, Key Lab Data Engn & Knowledge Engn MOE, Beijing 100872, Peoples R China.
   [Shang, Lifeng] Huawei, Noahs Ark Lab, Shatin, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Renmin University of China; Huawei
   Technologies
RP Leung, H (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM zhiwu.lu@gmail.com; howard@cityu.edu.hk; lifengshang@gmail.com
OI Lu, Zhiwu/0000-0003-0280-7724; LEUNG, Wing Ho
   Howard/0000-0002-2633-2965; Lu, Zhiwu/0000-0001-6429-7956
FU City University of Hong Kong [7004045]; National Natural Science
   Foundation of China [61202231]; Beijing Natural Science Foundation of
   China [4132037]; Ph.D. Programs Foundation of Ministry of Education of
   China [20120001120130]
FX The work described in this paper was supported by a grant from City
   University of Hong Kong (Project No. 7004045), National Natural Science
   Foundation of China under Grant 61202231, Beijing Natural Science
   Foundation of China under Grant 4132037, and Ph.D. Programs Foundation
   of Ministry of Education of China under Grant 20120001120130.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2009, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, I3D'09, DOI DOI 10.1145/1507149.1507181
   [Anonymous], 2009, IEEE C COMP VIS PATT
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   [Anonymous], 2007, Computer Graphics Technical Report CG-2007-2
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Corporation M, 2013, KIN WIND SDK BET PRO
   Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   HUANG T, 2012, P 14 ACM INT C MULT, P209
   Jin YH, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/1925101.1925104
   Komura T, 2005, COMPUT ANIMAT VIRT W, V16, P213, DOI 10.1002/cav.101
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Lai R.Y.Q, 2012, ABS12011409 CORR
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lu ZW, 2011, IEEE T SYST MAN CY B, V41, P976, DOI 10.1109/TSMCB.2010.2102749
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mou LT, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542208
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Pradhan GN, 2009, IEEE T INF TECHNOL B, V13, P802, DOI 10.1109/TITB.2009.2021262
   Qi T, 2013, COMPUT ANIMAT VIRT W, V24, P399, DOI 10.1002/cav.1505
   Shum H. P. H., 2012, P ACM S VIRT REAL SO, P17
   Sun C, 2011, COMPUT GRAPH FORUM, V30, P1953, DOI 10.1111/j.1467-8659.2011.02048.x
   Tang JKT, 2012, PATTERN RECOGN LETT, V33, P420, DOI 10.1016/j.patrec.2011.06.005
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu SY, 2009, VISUAL COMPUT, V25, P499, DOI 10.1007/s00371-009-0345-1
   Zhu M Y, 2012, P 2012 ACM SIGGRAPH, P183, DOI [10.2312/SCA/SCA12/183-192, DOI 10.2312/SCA/SCA12/183-192]
NR 29
TC 14
Z9 15
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 845
EP 854
DI 10.1007/s00371-014-0957-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700026
DA 2024-07-18
ER

PT J
AU Quan, W
   Chen, JX
   Yu, NY
AF Quan, Wei
   Chen, Jim X.
   Yu, Nanyang
TI Robust object tracking using enhanced random ferns
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Enhanced random ferns; Hidden classes; On-line
   clustering
AB This paper presents a method to address the problem of long-term robust object tracking in unconstrained environments. An enhanced random fern is proposed and integrated into our tracking framework as the object detector, whose main idea is to exploit the potential distribution properties of feature vectors which are here called hidden classes by on-line clustering of feature space for each leaf-node of ferns. The kernel density estimation technique is then used to evaluate unlabeled samples based on the hidden classes which are set as the data points of the kernel function. Experimental results on challenging real-world video sequences demonstrate the effectiveness and robustness of our approach. Comparisons with several state-of-the-art approaches are provided.
C1 [Quan, Wei] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Sichuan, Peoples R China.
   [Chen, Jim X.] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Yu, Nanyang] Southwest Jiaotong Univ, Sch Mech Engn, Chengdu 610031, Sichuan, Peoples R China.
C3 Southwest Jiaotong University; George Mason University; Southwest
   Jiaotong University
RP Quan, W (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Sichuan, Peoples R China.
EM xweiquan@gmail.com; jchen@gmu.edu; yunanyang@hotmail.com
FU National Natural Science Foundation of China [40672203]; Doctoral
   Innovation Foundation of Southwest Jiaotong University
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No. 40672203) and the Doctoral Innovation
   Foundation of Southwest Jiaotong University.
CR ABUMOSTAFA YS, 1995, SCI AM, V272, P64, DOI 10.1038/scientificamerican0495-64
   Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2005, 2005 IEEE COMPUTER S, DOI DOI 10.1109/CVPR.2005.288
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Grabner H, 2008, EUR C COMP VIS ECCV
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kalal Z., 2009, IEEE INT C COMP VIS
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Leistner C., 2010, EUR C COMP VIS ECCV
   Leistner C, 2010, LECT NOTES COMPUT SC, V6376, P493
   Leistner C, 2009, IEEE I CONF COMP VIS, P506, DOI 10.1109/ICCV.2009.5459198
   Lim J., 2005, NEURAL INFORM PROCES
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Ozuysal M., 2006, EUR C COMP VIS ECCV
   Ozuysal M., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383123
   Q Yu, 2008, EUR C COMP VIS ECCV
   Ramanan D., 2005, IEEE C COMP VIS PATT
   Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29
   Roth P., 2006, INT C COMP VIS SYST
   Saffari Amir, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1393, DOI 10.1109/ICCVW.2009.5457447
   Stalder S., 2009, IEEE INT C COMP VIS
   Stenger B, 2009, PROC CVPR IEEE, P2639
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang AP, 2009, IEEE IMAGE PROC, P1449, DOI 10.1109/ICIP.2009.5414559
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
NR 32
TC 7
Z9 8
U1 1
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 351
EP 358
DI 10.1007/s00371-013-0860-y
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500001
DA 2024-07-18
ER

PT J
AU Hsieh, TJ
   Yang, YS
   Wang, JH
   Shen, WJ
AF Hsieh, Tung-Ju
   Yang, Yuan-Sen
   Wang, Jenq-Haur
   Shen, Wen-Jay
TI Feature extraction using bionic particle swarm tracing for transfer
   function design in direct volume rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Volume rendering; Transfer function; Computational intelligence;
   Particle swarm optimization
ID CLASSIFICATION; OPTIMIZATION
AB In direct volume rendering, features of interest are still typically classified by a transfer function based on the volume data's intensity and the derived properties. Despite the efforts of previous research, classification remains a challenge. This paper presents a framework for designing new transfer functions that use bionic algorithms to map the frequency of particle occurrences to the color and opacity values. This allows us to extract features from the volume data. In particular, a novel approach is presented to allow a user to design a transfer function using the techniques of swarm intelligence. This approach consists of a population of simple agents interacting locally with one another and with the volume data. The agents scatter around the volume data and approach areas that contain features. Their movements are not only based on solution optimization, but are also governed by global optimization. After the agents have finished searching for features in the volume data, they can automatically modify the transfer function according to agents' behavior. With these agents, we do not have to preprocess the volume data for visualizing and exploring the features.
C1 [Hsieh, Tung-Ju; Wang, Jenq-Haur; Shen, Wen-Jay] Natl Taipei Univ Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Yang, Yuan-Sen] Natl Taipei Univ Technol, Dept Civil Engn, Taipei 106, Taiwan.
C3 National Taipei University of Technology; National Taipei University of
   Technology
RP Hsieh, TJ (corresponding author), Natl Taipei Univ Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM tjh@csie.ntut.edu.tw
RI Wang, Jenq-Haur/HJJ-1020-2023
OI Wang, Jenq-Haur/0000-0002-6076-7380; Yang, Yuan-Sen/0000-0003-1547-317X
FU Taiwan National Science Council [NSC 101-2221-E-027-131-, NSC
   101-2218-E-027-001-, NSC 101-2221-E-027-126-MY3]
FX This work was supported by the Taiwan National Science Council under
   Grants NSC 101-2221-E-027-131-, NSC 101-2218-E-027-001-, and NSC
   101-2221-E-027-126-MY3. Data sets are courtesy of the University of
   Erlangen, the Lawrence Berkeley Laboratory, the University of Utah, and
   the OsiriX Foundation.
CR Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Chan MY, 2009, IEEE T VIS COMPUT GR, V15, P1283, DOI 10.1109/TVCG.2009.172
   Chen M., 2001, VOLUME GRAPHICS
   Clerc M, 2002, IEEE T EVOLUT COMPUT, V6, P58, DOI 10.1109/4235.985692
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Correa CD, 2009, IEEE T VIS COMPUT GR, V15, P1465, DOI 10.1109/TVCG.2009.189
   Correa CD, 2009, IEEE PAC VIS SYMP, P177, DOI 10.1109/PACIFICVIS.2009.4906854
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Elvins T. T., 1992, Computer Graphics, V26, P194, DOI 10.1145/142413.142427
   Engelbrecht A., 2007, Computational Intelligence: An Introduction, Vsecond
   Fujishiro I., 1999, P IEEE VIS 1999, P184
   Ge Y, 2005, LECT NOTES COMPUT SC, V3612, P553
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   Hadwiger M, 2008, IEEE T VIS COMPUT GR, V14, P1507, DOI 10.1109/TVCG.2008.147
   He TS, 1996, IEEE VISUAL, P227, DOI 10.1109/VISUAL.1996.568113
   Johnson Christopher., 2004, VISUALIZATION HDB
   KAUFMAN A, 1993, COMPUTER, V26, P51, DOI 10.1109/MC.1993.274942
   Kennedy J, 1997, PROCEEDINGS OF 1997 IEEE INTERNATIONAL CONFERENCE ON EVOLUTIONARY COMPUTATION (ICEC '97), P303, DOI 10.1109/ICEC.1997.592326
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Krohling RA, 2005, IEEE C EVOL COMPUTAT, P1226
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887
   Niu B, 2006, LECT NOTES COMPUT SC, V4115, P61
   Pfister H, 2001, IEEE COMPUT GRAPH, V21, P16, DOI 10.1109/38.920623
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Roettger S., 2005, EUR IEEE VGTC S VIS
   Sereda P, 2006, IEEE T VIS COMPUT GR, V12, P208, DOI 10.1109/TVCG.2006.39
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   Takahashi S, 2004, GRAPH MODELS, V66, P24, DOI 10.1016/j.gmod.2003.08.002
   Takahashi S., 2006, SCI VISUALIZATION VI, VMathematics and Visualization, P185
   Tzeng Fan-Yin., 2003, VIS 03, P66
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Yen GG, 2009, IEEE T SYST MAN CY A, V39, P890, DOI 10.1109/TSMCA.2009.2013915
   Yuhui Shi, 1998, Evolutionary Programming VII. 7th International Conference, EP98. Proceedings, P591, DOI 10.1007/BFb0040810
NR 38
TC 2
Z9 3
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 33
EP 44
DI 10.1007/s00371-013-0777-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500003
DA 2024-07-18
ER

PT J
AU Chen, XW
   Guo, Y
   Zhou, B
   Zhao, QP
AF Chen, Xiaowu
   Guo, Yu
   Zhou, Bin
   Zhao, Qinping
TI Deformable model for estimating clothed and naked human shapes from a
   single image
SO VISUAL COMPUTER
LA English
DT Article
DE Human shape; Deformable model; Shape-from-contour; Shape estimation
AB Estimation of human shape from images has numerous applications ranging from graphics to surveillance. A single image provides insufficient constraints (e.g. clothing), making human shape estimation more challenging. We propose a method to simultaneously estimate a person's clothed and naked shapes from a single image of that person wearing clothing. The key component of our method is a deformable model of clothed human shape. We learn our deformable model, which spans variations in pose, body, and clothes, from a training dataset. These variations are derived by the non-rigid surface deformation, and encoded in various low-dimension parameters. Our deformable model can be used to produce clothed 3D meshes for different people in different poses, which neither appears in the training dataset. Afterward, given an input image, our deformable model is initialized with a few user-specified 2D joints and contours of the person. We optimize the parameters of the deformable model by pose fitting and body fitting in an iterative way. Then the clothed and naked 3D shapes of the person can be obtained simultaneously. We illustrate our method for texture mapping and animation. The experimental results on real images demonstrate the effectiveness of our method.
C1 [Chen, Xiaowu; Guo, Yu; Zhou, Bin; Zhao, Qinping] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Chen, XW (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM chen@vrlab.buaa.edu.cn; guoyu@vrlab.buaa.edu.cn;
   zhoubin@vrlab.buaa.edu.cn; zhaoqp@vrlab.buaa.edu.cn
FU NSFC [60933006]; 863 Program [2012AA011504]; ITER [2012GB102008]
FX We would like to acknowledge the indispensable help of Dragomir Anguelov
   and Nils Hasler who provided us with the scan data. This work was
   partially supported by NSFC (60933006), 863 Program (2012AA011504), and
   ITER (2012GB102008).
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2007, IEEE C COMPUTER VISI
   [Anonymous], 2009, P ICCV
   [Anonymous], 2009, P 6 EUR S SKETCH BAS, DOI DOI 10.1145/1572741.1572749]
   Balan AO, 2008, LECT NOTES COMPUT SC, V5303, P15, DOI 10.1007/978-3-540-88688-4_2
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Chai ML, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185612
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen X., 2012, P ECCV, P554
   Chen Y, 2010, LECT NOTES COMPUT SC, V6313, P300
   Ghahramani Z., 1997, CRGTR961
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Guo Y., 2012, COMPUTATIONAL VISUAL, P1
   Hasler N., 2009, CGF P EG2008, V2
   Hasler N, 2010, PROC CVPR IEEE, P1823, DOI 10.1109/CVPR.2010.5539853
   Huang TJ, 2011, SCI CHINA INFORM SCI, V54, P2461, DOI 10.1007/s11432-011-4487-1
   LOURAKIS M. I. A., 2009, LEVENBERG MARQUARDT
   Ma Y., 2003, INVITATION 3 D VISIO
   Rose CF, 2001, COMPUT GRAPH FORUM, V20, pC239
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sigal L., 2007, P NIPS
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Teramoto O, 2010, VISUAL COMPUT, V26, P1339, DOI 10.1007/s00371-009-0405-6
   Wei XLK, 2011, IEEE COMPUT GRAPH, V31, P78, DOI 10.1109/MCG.2009.132
   Xu K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024207
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
NR 27
TC 25
Z9 34
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1187
EP 1196
DI 10.1007/s00371-013-0775-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300007
DA 2024-07-18
ER

PT J
AU Wang, CB
   Zhang, Q
   Kong, FL
   Qin, H
AF Wang, Chang-bo
   Zhang, Qiang
   Kong, Fan-long
   Qin, Hong
TI Hybrid particle-grid fluid animation with enhanced details
SO VISUAL COMPUTER
LA English
DT Article
DE Hybrid particle-grid simulation; Fluid animation; Two-way interaction;
   Enhanced details
ID LEVEL SET METHOD; WATER
AB Simulating large-scale fluid while retaining and rendering details still remains to be a difficult task in spite of rapid advancements of computer graphics during the last two decades. Grid-based methods can be easily extended to handle large-scale fluid, yet they are unable to preserve sub-grid surface details like spray and foam without multi-level grid refinement. On the other hand, the particle-based methods model details naturally, but at the expense of increasing particle densities. This paper proposes a hybrid particle-grid coupling method to simulate fluid with finer details. The interaction between particles and fluid grids occurs in the vicinity of "coupling band" where multiple particle level sets are introduced simultaneously. First, fluids free of interaction could be modeled by grids and SPH particles independently after initialization. A coupling band inside and near the interface is then identified where the grids interact with the particles. Second, the grids inside and far away from the interface are adaptively sampled for large-scale simulation. Third, the SPH particles outside the coupling band are enhanced by diffuse particles which render little computational cost to simulate spray, foam, and bubbles. A distance function is continuously updated to adaptively coarsen or refine the grids near the coupling band and provides the coupling weights for the two-way coupling between grids and particles. One characteristic of our hybrid approach is that the two-way coupling between these particles of spray and foam and the grids of fluid volume can retain details with little extra computational cost. Our rendering results realistically exhibit fluids with enhanced details like spray, foam, and bubbles. We make comprehensive comparisons with existing works to demonstrate the effectiveness of our new method.
C1 [Wang, Chang-bo; Zhang, Qiang; Kong, Fan-long] E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, CB (corresponding author), E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
EM cbwangcg@gmail.com; qin@cs.sunysb.edu
FU Natural Science Foundation of China [61070128, 61272199]; National
   Natural Science Foundation of China [61190120, 61190121, 61190125];
   National Science Foundation of USA [IIS0949467, IIS1047715, IIS1049448];
   Innovation Program of the Shanghai Municipal Education Commission
   [12ZZ042]; Fundamental Research Funds for the Central Universities;
   Shanghai Knowledge Service Platform for Trustworthy Internet of Things
   [ZF1213]; Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1047715] Funding Source: National Science
   Foundation; Direct For Computer & Info Scie & Enginr; Div Of Information
   & Intelligent Systems [1049448] Funding Source: National Science
   Foundation
FX This paper was partially supported by Natural Science Foundation of
   China (Grant Nos. 61070128, 61272199), National Natural Science
   Foundation of China (Grant Nos. 61190120, 61190121, and 61190125) and
   National Science Foundation of USA (Grant Nos. IIS0949467, IIS1047715,
   and IIS1049448), Innovation Program of the Shanghai Municipal Education
   Commission (Grant No. 12ZZ042), Fundamental Research Funds for the
   Central Universities, and Shanghai Knowledge Service Platform for
   Trustworthy Internet of Things (Grant No. ZF1213).
CR ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098
   Adalsteinsson D, 1999, J COMPUT PHYS, V148, P2, DOI 10.1006/jcph.1998.6090
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   BICKNELL GV, 1991, SIAM J SCI STAT COMP, V12, P1198, DOI 10.1137/0912064
   Chentanez Nuttapong., 2010, P 2010 ACM SIGGRAPH, P197
   Enright D, 2005, COMPUT STRUCT, V83, P479, DOI 10.1016/j.compstruc.2004.04.024
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Enright D, 2002, J COMPUT PHYS, V183, P83, DOI 10.1006/jcph.2002.7166
   Filippova O, 1998, J COMPUT PHYS, V147, P219, DOI 10.1006/jcph.1998.6089
   Ihmsen M, 2012, VISUAL COMPUT, V28, P669, DOI 10.1007/s00371-012-0697-9
   Irving G, 2006, ACM T GRAPHIC, V25, P805, DOI 10.1145/1141911.1141959
   Jiang GS, 2000, SIAM J SCI COMPUT, V21, P2126, DOI 10.1137/S106482759732455X
   Kwak Y., 2009, International Journal of Computational Science, V3, P579
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Sethian JA, 1999, SIAM REV, V41, P199, DOI 10.1137/S0036144598347059
   Solenthaler B, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964976
   Thuerey N, 2003, THESIS U ERLANGEN NU
   Thuerey N., 2004, VISION MODELING VISU, P199
   Thürey N, 2009, COMPUT VIS SCI, V12, P247, DOI 10.1007/s00791-008-0090-4
   Thurey Nils, 2006, P 2006 ACM SIGGRAPHE, P157
NR 21
TC 19
Z9 23
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 937
EP 947
DI 10.1007/s00371-013-0849-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100009
DA 2024-07-18
ER

PT J
AU Ricks, BC
   Egbert, PK
AF Ricks, Brian C.
   Egbert, Parris K.
TI More realistic, flexible, and expressive social crowds using
   transactional analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Crowd simulation; Social crowd simulation; Transactional analysis; Pair
   walking
AB Recent algorithms have been able to simulate "social crowds" that allow agents to interact socially as opposed to only treating other agents as obstacles. Unfortunately, past social crowd algorithms lack realism and flexibility because they do not allow agents to move in and out of different and repeated social interactions, are built around a specific obstacle avoidance algorithm, or are tuned only for a specific social setting and do not allow for artist directed changes. We propose a new, simplified social crowd algorithm that focuses on the evolving social needs of agents and allows each agent to join and leave different encounters as desired. Our algorithm is based on the psychology research area of transactional analysis, does not require a specific obstacle avoidance algorithm, and allows for easy artist direction for determining the precise social environment being simulated. Our algorithm runs in real-time with 3,000 to 4,000 agents without the restrictions of previous research.
C1 [Ricks, Brian C.; Egbert, Parris K.] Brigham Young Univ, Provo, UT 84602 USA.
C3 Brigham Young University
RP Ricks, BC (corresponding author), Brigham Young Univ, Provo, UT 84602 USA.
EM brianricks@byu.edu; egbert@cs.byu.edu
RI Ricks, Brian/AAG-9889-2020
CR [Anonymous], 2010, GAMES PEOPLE PLAY PS
   [Anonymous], 1997, COMPUTER ANIMATION S, DOI DOI 10.1007/978-3-7091-6874-5_3
   BERNE E, 1958, AM J PSYCHOTHER, V12, P735, DOI 10.1176/appi.psychotherapy.1958.12.4.735
   Berne E., 1981, BEYOND GAMES SCRIPTS
   Berne E., 1984, WHAT DO YOU SAY YOU
   Carstensdottir Elin, 2011, Intelligent Virtual Agents. Proceedings 11th International Conference, IVA 2011, P48, DOI 10.1007/978-3-642-23974-8_6
   COLEMAN JS, 1961, SOCIOMETRY, V24, P36, DOI 10.2307/2785927
   Durupinar F., 2008, P 7 INT JOINT C AUT, P1217
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Groschel A., 2011, THESIS HOCHSHULE TEC
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Harris T., 2004, I M OKAY YOU RE OKAY
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   James John, 1953, AM SOCIOLOGICAL REV
   Jan D, 2005, LECT NOTES ARTIF INT, V3661, P65
   Jan D., 2007, Proceedings of the Workshop on Embodied Language Processing, P59, DOI DOI 10.1145/1329125.1329142
   Karamouzas I., 2010, Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology, VRST '10, P183, DOI DOI 10.1145/1889863
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Padilha E., 2002, P EDILOG 2002 6 WORK, P117
   Patel J., 2004, SIMULATION SMALL GRO
   Pedica C, 2010, LECT NOTES ARTIF INT, V6356, P336, DOI 10.1007/978-3-642-15892-6_35
   Pedica C, 2009, LECT NOTES ARTIF INT, V5773, P344, DOI 10.1007/978-3-642-04380-2_38
   Pelechano Nuria., 2008, P 7 INT JOINT C AUTO, V1, P136
   Popelova Marketa, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P278, DOI 10.1007/978-3-642-25090-3_24
   Reynolds Craig W, 1999, OPENSTEER
   Ricks B., 2012, EUROGRAPHICS
   Scheflen A., 1976, Human territories: How we behave in space-time
   Singh S, 2009, COMPUT ANIMAT VIRT W, V20, P533, DOI 10.1002/cav.277
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   YEH H., 2008, SCA 08 P 2005 ACM SI, P39
NR 32
TC 3
Z9 6
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 889
EP 898
DI 10.1007/s00371-012-0712-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500036
DA 2024-07-18
ER

PT J
AU Dinesha, V
   Adabala, N
   Natarajan, V
AF Dinesha, Vijeth
   Adabala, Neeharika
   Natarajan, Vijay
TI Uncertainty visualization using HDR volume rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Uncertainty visualization; Transfer function design; Ray casting; Volume
   rendering; High dynamic range imaging; Tone mapping
AB In this paper, we explore a novel idea of using high dynamic range (HDR) technology for uncertainty visualization. We focus on scalar volumetric data sets where every data point is associated with scalar uncertainty. We design a transfer function that maps each data point to a color in HDR space. The luminance component of the color is exploited to capture uncertainty. We modify existing tone mapping techniques and suitably integrate them with volume ray casting to obtain a low dynamic range (LDR) image. The resulting image is displayed on a conventional 8-bits-per-channel display device. The usage of HDR mapping reveals fine details in uncertainty distribution and enables the users to interactively study the data in the context of corresponding uncertainty information. We demonstrate the utility of our method and evaluate the results using data sets from ocean modeling.
C1 [Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automat, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
   [Adabala, Neeharika] CybULab, Bangalore, Karnataka, India.
C3 Indian Institute of Science (IISC) - Bangalore
RP Natarajan, V (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Supercomp Educ & Res Ctr, Bangalore 560012, Karnataka, India.
EM vijeth.d@csa.iisc.ernet.in; neeharik@gmail.com; vijayn@csa.iisc.ernet.in
FU Microsoft Research India; Department of Science and Technology, India
   [SR/S3/EECE/048/2007]
FX We thank Professor Vinayachandran for providing the Bay of Bengal data
   set and for his input on the study of the ocean data. We thank Professor
   Lermusiaux for sharing the MAB data set. This work was supported by
   Microsoft Research India and by Department of Science and Technology,
   India, under Grant SR/S3/EECE/048/2007.
CR Cedilnik A, 2000, IEEE VISUAL, P77, DOI 10.1109/VISUAL.2000.885679
   Choudhury, 2005, ACM SIGGRAPH 2005 Courses, P5, DOI 10.1145/1198555.1198565
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Dinesha V., 2010, EUR IEEE VGTC S VIS
   Djurcilov S, 2002, COMPUT GRAPH-UK, V26, P239, DOI 10.1016/S0097-8493(02)00055-9
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Durand F, 2000, SPRING COMP SCI, P219
   Fairchild M.D., 2005, Colour appearance models, V2nd
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Ghosh A, 2005, VOLUME GRAPHICS 2005, P91
   Grigoryan G, 2004, IEEE T VIS COMPUT GR, V10, P564, DOI 10.1109/TVCG.2004.30
   Haroz S, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P207
   Hengl T., 2003, P 7 INT C GEO COMPUT, P8
   Johnson CR, 2003, IEEE COMPUT GRAPH, V23, P6, DOI 10.1109/MCG.2003.1231171
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Lee CH, 2002, PROC SPIE, V4665, P80, DOI 10.1117/12.482423
   Lermusiaux P.F.J., 2008, MSEAS REANALYSES AWA
   Lodha SK, 1996, PROC GRAPH INTERF, P238
   Lundström C, 2007, IEEE T VIS COMPUT GR, V13, P1648, DOI 10.1109/TVCG.2007.70518
   Pang AT, 1997, VISUAL COMPUT, V13, P370, DOI 10.1007/s003710050111
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Sabella P., 1988, Computer Graphics, V22, P51, DOI 10.1145/378456.378476
   Sanyal J, 2009, IEEE T VIS COMPUT GR, V15, P1209, DOI 10.1109/TVCG.2009.114
   Taylor B.N., 1993, 1297 NAT I STAND TEC
   Wang L., 2008, VOLUME GRAPHICS, P33
   Wittenbrink CM, 1996, IEEE T VIS COMPUT GR, V2, P266, DOI 10.1109/2945.537309
   Yuan XR, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P327
   Yuan XR, 2006, IEEE T VIS COMPUT GR, V12, P433, DOI 10.1109/TVCG.2006.72
NR 31
TC 6
Z9 13
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2012
VL 28
IS 3
BP 265
EP 278
DI 10.1007/s00371-011-0614-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 896QS
UT WOS:000300585400003
DA 2024-07-18
ER

PT J
AU Pan, RJ
   Skala, V
AF Pan, Rongjiang
   Skala, Vaclav
TI Surface reconstruction with higher-order smoothness
SO VISUAL COMPUTER
LA English
DT Article
DE Surface reconstruction; Higher-order smoothness; Convex optimization
AB This work proposes a method to reconstruct surfaces with higher-order smoothness from noisy 3D measurements. The reconstructed surface is implicitly represented by the zero-level set of a continuous valued embedding function. The key idea is to find a function whose higher-order derivatives are regularized and whose gradient is best aligned with a vector field defined by the input point set. In contrast to methods based on the first-order variation of the function that are biased toward the constant functions and treat the extraction of the isosurface without aliasing artifacts as an afterthought, we impose a higher-order smoothness directly on the embedding function. After solving a convex optimization problem with a multiscale iterative scheme, a triangulated surface can be extracted using the marching cubes algorithm. We demonstrated the proposed method on several data sets obtained from raw laser-scanners and multiview stereo approaches. Experimental results confirm that our approach allows us to reconstruct smooth surfaces from points in the presence of noise, outliers, large missing parts, and very coarse orientation information.
C1 [Pan, Rongjiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Skala, Vaclav] Univ W Bohemia, Dept Comp Sci & Engn, Ctr Comp Graph & Data Visualizat, Plzen 30614, Czech Republic.
C3 Shandong University; University of West Bohemia Pilsen
RP Pan, RJ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM panrj@sdu.edu.cn
RI Skala, Vaclav/F-9141-2011
OI Skala, Vaclav/0000-0001-8886-4281
FU Key Project in the National Science & Technology Pillar Program of China
   [2008BAH29B02]; Shandong Natural Science Foundation of China
   [ZR2010FM046]; State Key Program of National Natural Science of China
   [U1035004]; National Natural Science of China [60970046]; Independent
   Innovation Foundation of Shandong University; Ministry of Education of
   the Czech Republic [2C06002, ME10060]
FX The authors would like to express their thanks to Victor Lempitsky and
   Michael Misha Kazhdan for fruitful discussions. The authors would also
   like to thank Rasmus R. Paulsen, Noah Snavely, and Yasutaka Furukawa for
   making their algorithms publicly available. This work was supported by
   the Key Project in the National Science & Technology Pillar Program of
   China (Grant No. 2008BAH29B02), Shandong Natural Science Foundation of
   China (Grant No. ZR2010FM046), the State Key Program of National Natural
   Science of China (Grant No. U1035004), the National Natural Science of
   China (Grant No. 60970046), Independent Innovation Foundation of
   Shandong University, and projects of the Ministry of Education of the
   Czech Republic (Nos. 2C06002 and ME10060).
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   [Anonymous], IEEE COMPUTER VISION
   Bajaj C. L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P109, DOI 10.1145/218380.218424
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286
   Chuang M, 2009, COMPUT GRAPH FORUM, V28, P1475, DOI 10.1111/j.1467-8659.2009.01524.x
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Desbrun, 2007, P 5 EUROGRAPHICS S G, V7, P39
   Furukawa Y., 2007, CVPR 07
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hornung Alexander, 2006, ACM INT C PROCEED IN, P41, DOI DOI 10.2312/SGP/SGP06
   Jalba AC, 2007, IEEE T VIS COMPUT GR, V13, P1512, DOI 10.1109/TVCG.2007.70553
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Kolev K, 2009, INT J COMPUT VISION, V84, P80, DOI 10.1007/s11263-009-0233-1
   Lempitsky V., 2007, IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [10.1109/CVPR.2007.383293, DOI 10.1109/CVPR.2007.383293]
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Ohtake Y, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P153
   Pan RJ, 2009, SCI CHINA SER F, V52, P308, DOI 10.1007/s11432-009-0032-x
   Paulsen RR, 2010, IEEE T VIS COMPUT GR, V16, P636, DOI 10.1109/TVCG.2009.208
   Saleem W, 2007, VISUAL COMPUT, V23, P381, DOI 10.1007/s00371-006-0094-3
   SAMOZINO M., 2006, SGP 06, P51
   Sharf A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239494
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
NR 29
TC 14
Z9 17
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 155
EP 162
DI 10.1007/s00371-011-0604-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100002
DA 2024-07-18
ER

PT J
AU Wang, J
   Yang, ZW
   Chen, FL
AF Wang, Jun
   Yang, Zhouwang
   Chen, Falai
TI A variational model for normal computation of point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Normal vector field; Consistent orientation; Variational
   model; Eigenvalue problem
ID APPROXIMATION
AB In this paper we present a novel model for computing the oriented normal field on a point cloud. Differently from previous two-stage approaches, our method integrates the unoriented normal estimation and the consistent normal orientation into one variational framework. The normal field with consistent orientation is obtained by minimizing a combination of the Dirichlet energy and the coupled-orthogonality deviation, which controls the normals perpendicular to and continuously varying on the underlying shape. The variational model leads to solving an eigenvalue problem. If unoriented normal field is provided, the model can be modified for consistent normal orientation. We also present experiments which demonstrate that our estimates of oriented normal vectors are accurate for smooth point clouds, and robust in the presence of noise, and reliable for surfaces with sharp features, e.g., corners, ridges, close-by sheets and thin structures.
C1 [Yang, Zhouwang; Chen, Falai] Univ Sci & Technol China, Dept Math, Hefei 230026, Peoples R China.
   [Wang, Jun] Univ Sci & Technol China, Sch Comp Sci, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Yang, ZW (corresponding author), Univ Sci & Technol China, Dept Math, Hefei 230026, Peoples R China.
EM junwang0211@gmail.com; yangzw@ustc.edu.cn; chenfl@ustc.edu.cn
RI Yang, Zhouwang/C-4339-2013; Chen, Falai/C-3846-2013
FU 973 Program [2011CB302400]; NSF of China [11031007, 61073108, 61073111,
   60803066, 60873109]; Chinese Universities
FX We would like to thank the anonymous reviewers for their comments and
   suggestions which greatly improve the manuscript. Thanks also go to
   Aim@Shape project and the Stanford Computer Graphics Laboratory for
   providing the models in the paper. The work is supported by 973 Program
   2011CB302400, the NSF of China (No. 11031007, 61073108, 61073111,
   60803066 and 60873109). Zhouwang Yang is supported in part by Chinese
   Universities Scientific Fund.
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Alliez P., 2007, P S GEOM PROC 07
   Amenta N., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P39, DOI 10.1145/276884.276889
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Dey T., 2005, NORMAL ESTIMATION PO, P39
   GROSS MH, 2007, POINT BASED GRAPHICS
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Levin D, 2004, MATH VISUAL, P37
   Li B, 2010, COMPUT GRAPH-UK, V34, P94, DOI 10.1016/j.cag.2010.01.004
   Liu SJ, 2010, COMPUT GRAPH-UK, V34, P209, DOI 10.1016/j.cag.2010.03.003
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Mount D.M., 2006, Ann: A library for approximate nearest neighbor searching
   Ohtake Y, 2005, GRAPH MODELS, V67, P150, DOI 10.1016/j.gmod.2004.06.003
   OuYang D, 2005, COMPUT AIDED DESIGN, V37, P1071, DOI 10.1016/j.cad.2004.11.005
   Pauly M, 2003, ACM T GRAPHIC, V22, P641, DOI 10.1145/882262.882319
   STEWART G. W., 2001, Eigensystems, VII
   Sun J, 2007, IMAGE VISION COMPUT, V25, P1073, DOI 10.1016/j.imavis.2006.04.024
   Yang ZW, 2007, COMPUT AIDED DESIGN, V39, P1091, DOI 10.1016/j.cad.2007.08.005
NR 21
TC 18
Z9 22
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 163
EP 174
DI 10.1007/s00371-011-0607-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100003
DA 2024-07-18
ER

PT J
AU Raman, S
   Chaudhuri, S
AF Raman, Shanmuganathan
   Chaudhuri, Subhasis
TI Reconstruction of high contrast images for dynamic scenes
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range imaging; De-ghosting; Low dynamic range image
   generation; Computational photography
ID FUSION
AB High Dynamic Range (HDR) imaging requires one to composite multiple, differently exposed images of a scene in the irradiance domain and perform tone mapping of the generated HDR image for displaying on Low Dynamic Range (LDR) devices. In the case of dynamic scenes, standard techniques may introduce artifacts called ghosts if the scene changes are not accounted for. In this paper, we consider the blind HDR problem for dynamic scenes. We develop a novel bottom-up segmentation algorithm through superpixel grouping which enables us to detect scene changes. We then employ a piecewise patch-based compositing methodology in the gradient domain to directly generate the ghost-free LDR image of the dynamic scene. Being a blind method, the primary advantage of our approach is that we do not assume any knowledge of camera response function and exposure settings while preserving the contrast even in the non-stationary regions of the scene. We compare the results of our approach for both static and dynamic scenes with that of the state-of-the-art techniques.
C1 [Raman, Shanmuganathan; Chaudhuri, Subhasis] Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay
RP Raman, S (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
EM shanmuga@ee.iitb.ac.in; sc@ee.iitb.ac.in
RI Raman, Shanmuganathan/AAV-2186-2020
OI Raman, Shanmuganathan/0000-0003-2718-7891
FU Microsoft Research India; JC Bose National fellowship; Bharti Center for
   Communication
FX The first author would like to acknowledge the financial support
   provided by Microsoft Research India through a Ph.D. fellowship. The
   second author would like to thank JC Bose National fellowship and Bharti
   Center for Communication for the financial support.
CR *AD SYST INC, 2010, AD PHOT CS5 US GUID
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578
   [Anonymous], 2010, P 7 IND C COMP VIS G
   [Anonymous], 2002, P 13 EUR WORKSH REND
   [Anonymous], CVPR
   [Anonymous], IEEE INT C COMP PHOT
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], P IEEE C COMP VIS PA
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   BOGART R, 2003, SIGGRAPH TECHNICAL S
   Borenstein E, 2004, LECT NOTES COMPUT SC, V3023, P315
   BORENSTEIN E, 2002, LECT NOTES COMPUTER, V2351, P639
   Brinkmann R, 2008, The Art And Science Of Digital Compositing: Techniques For Visual Effects, Animation And Motion Graphics, DOI 10.1016/B978-0-12-370638-6.X0001-6
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   FULKERSON B, 2010, COMP VIS 2009 IEEE 1, P670
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   Granados M, 2010, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2010.5540208
   Grossberg MD, 2003, IEEE T PATTERN ANAL, V25, P1455, DOI 10.1109/TPAMI.2003.1240119
   Guo D, 2010, PROC CVPR IEEE, P515, DOI 10.1109/CVPR.2010.5540170
   Heidrich Wolfgang, ERIK REINHARD
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   Khan EA, 2006, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2006.312892
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Lin S, 2004, PROC CVPR IEEE, P938
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   Mann S, 2000, IEEE T IMAGE PROCESS, V9, P1389, DOI 10.1109/83.855434
   MANTIUK, 2006, ACM T APPL PERCEPT, V3, P286
   MANTIUK R, 2007, P SPIE
   Mantiuk R, 2008, COMPUT GRAPH FORUM, V27, P699, DOI 10.1111/j.1467-8659.2008.01168.x
   Masood SZ, 2009, COMPUT GRAPH FORUM, V28, P1861, DOI 10.1111/j.1467-8659.2009.01564.x
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Mori G, 2005, IEEE I CONF COMP VIS, P1417
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Porter T., 1984, Computers & Graphics, V18, P253
   RAMAN S, 2009, EUROGRAPHICS SHORT P
   Raman S., 2007, ICCV '07: Proceedings of the 11th IEEE International Conference on Computer Vision, P1
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   *SPHER, SPHER HDR
   SZELISKI R, 2008, FDN TRENDS COMPUT GR, V2
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Ward G., 2003, Journal of Graphics Tools, V8, P17, DOI 10.1080/10867651.2003.10487583
   Ward G. J., 1994, P 21 ANN C COMP GRAP, P459, DOI [DOI 10.1145/192161.192286, 10.1145/192161.192286]
   Zhang W, 2010, PROC CVPR IEEE, P530, DOI 10.1109/CVPR.2010.5540168
NR 57
TC 47
Z9 51
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1099
EP 1114
DI 10.1007/s00371-011-0653-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500006
DA 2024-07-18
ER

PT J
AU Wang, D
   Li, GQ
   Jia, WJ
   Luo, XN
AF Wang, Dong
   Li, Guiqing
   Jia, Weijia
   Luo, Xiaonan
TI Saliency-driven scaling optimization for image retargeting
SO VISUAL COMPUTER
LA English
DT Article
DE Image retargeting; Scaling factor energy; Visual saliency; Mesh
   deformation
ID VIDEO
AB This paper proposes a saliency-weighted scaling factor energy for image retargeting. Considering that salient objects should be scaled with a larger scaling factor with respect to nonsalient regions, we define a quadric energy to establish the relation between the scaling factor of a local region and its saliency. The quadric energy is the weighted sum of the square of scaling factors, where the weight of each scaling factor is inversely proportional to its corresponding saliency. Furthermore, a triangle similarity quadric energy is introduced to prevent salient regions from distortion. Compared to previous methods, our approach not only preserves the shapes of salient objects and the integrity of the whole image well, but also reserves more resolution to salient objects in target image even when the aspect ratio is unchanged.
C1 [Jia, Weijia] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Li, Guiqing] S China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Luo, Xiaonan] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.
   [Luo, Xiaonan] Sun Yat Sen Univ, Comp Applicat Inst, Guangzhou 510275, Guangdong, Peoples R China.
C3 City University of Hong Kong; South China University of Technology; Sun
   Yat Sen University; Sun Yat Sen University
RP Jia, WJ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM donwang@student.cityu.edu.hk; ligq@scut.edu.cn; wei.jia@cityu.edu.hk;
   syslxn@mail.sysu.edu.cn
RI Jia, Weijia/W-6152-2019; Jia, Weijia/AAJ-4178-2020
OI Jia, Weijia/0000-0002-0231-3196
FU Research Grants Council of the Hong Kong SAR, China [CityU 114908,
   114609]; NSFC [60973084]; NSF of Guangdong [915106410-1000106, U0835004,
   U0935004]; Fundamental Research Funds for the Central Universities [2009
   zz0016]
FX We thank the anonymous reviewers for their valuable comments and Dr.
   Hong-bo Fu for helpful discussion. Thanks also go to Jonathan Harel for
   providing source code for saliency detection; Yu-shuen Wang for their
   executable program for producing OSS results, Ligang Liu and Yong Jin
   for generating NSO results, and Yan-wen Guo for generating
   parametrization based results. Wei-jia Jia is supported by the Research
   Grants Council of the Hong Kong SAR, China (CityU 114908, 114609).
   Guiqing Li is supported by NSFC(60973084), NSF of Guangdong
   (915106410-1000106), and Fundamental Research Funds for the Central
   Universities (2009 zz0016). Xiao-nan Luo's research is supported by the
   Joint Funds of NSFC-Guangdong (U0835004, U0935004).
CR ACHANTA R, ICIP 2009, P1001
   [Anonymous], P SPIE
   [Anonymous], P 17 ACM INT C MULT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   DONG W, 2009, COMPUT GRAPH FORUM, V28, P1
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   LAM H, CHI 2005, P681
   Li Z, 2009, IEEE T IMAGE PROCESS, V18, P2572, DOI 10.1109/TIP.2009.2026677
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Pavic D, 2010, COMPUT GRAPH FORUM, V29, P743, DOI 10.1111/j.1467-8659.2009.01644.x
   PHILIPP K, 2009, ACM T GRAPHIC, V28, P1
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   SHAMIR A, 2009, P ACM SIGGRAPH AS CO
   Shamir A, 2009, COMMUN ACM, V52, P77, DOI 10.1145/1435417.1435437
   STEPHAN K, ACM MULT 2009, P321
   Wang YS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778827
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
NR 25
TC 27
Z9 28
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2011
VL 27
IS 9
BP 853
EP 860
DI 10.1007/s00371-011-0559-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 807TK
UT WOS:000293922400005
DA 2024-07-18
ER

PT J
AU Andersson, M
   Johnsson, B
   Munkberg, J
   Clarberg, P
   Hasselgren, J
   Akenine-Möller, T
AF Andersson, Magnus
   Johnsson, Bjorn
   Munkberg, Jacob
   Clarberg, Petrik
   Hasselgren, Jon
   Akenine-Moller, Tomas
TI Efficient multi-view ray tracing using edge detection and shader reuse
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Multi-view; Ray tracing; Adaptive sampling; Edge detection
AB Stereoscopic rendering and 3D stereo displays are quickly becoming mainstream. The natural extension is autostereoscopic multi-view displays which, by the use of parallax barriers or lenticular lenses, can accommodate many simultaneous viewers without the need for active or passive glasses. As these displays, for the foreseeable future, will support only a rather limited number of views, there is a need for high-quality interperspective antialiasing. We present a specialized algorithm for efficient multi-view image generation from a camera line using ray tracing, which builds on previous methods for multi-dimensional adaptive sampling and reconstruction of light fields. We introduce multi-view silhouette edges to detect sharp geometrical discontinuities in the radiance function. These are used to significantly improve the quality of the reconstruction. In addition, we exploit shader coherence by computing analytical visibility between shading points and the camera line, and by sharing shading computations over the camera line.
C1 [Andersson, Magnus; Munkberg, Jacob; Clarberg, Petrik] Lund Univ, Graph Grp, Lund, Sweden.
   [Andersson, Magnus; Johnsson, Bjorn; Munkberg, Jacob; Clarberg, Petrik; Hasselgren, Jon; Akenine-Moller, Tomas] Intel Corp, Lund, Sweden.
C3 Lund University; Intel Corporation
RP Andersson, M (corresponding author), Lund Univ, Graph Grp, Lund, Sweden.
EM magnusa@cs.lth.se
CR Adelson S. J., 1993, Visual Computer, V10, P127, DOI 10.1007/BF01900903
   Apodaca Anthony, 2000, ADV RENDERMAN CREATI
   Badt S.  Jr., 1988, Visual Computer, V4, P123, DOI 10.1007/BF01908895
   Bala K, 2003, ACM T GRAPHIC, V22, P631, DOI 10.1145/882262.882318
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   CROW F, 1977, COMPUT GRAPH, V11, P242
   Drettakis G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P223, DOI 10.1145/192161.192207
   Egan K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531399
   EZELL JD, 1990, P SOC PHOTO-OPT INS, V1256, P298, DOI 10.1117/12.19912
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Halle M.W., 1997, THESIS MIT
   HALLE MW, 1994, P SOC PHOTO-OPT INS, V2176, P73, DOI 10.1117/12.172620
   HASSELGREN J., 2006, P EUROGRAPHICS S REN, P61
   HAVRAN V, 2003, ACM SIGGRAPH SKETCHE
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Javidi B., 2002, 3 DIMENSIONAL TELEVI
   KARTCH D, 2000, THESIS CORNELL U
   LEVOY M, 1996, P ACM SIGGRAPH, P13
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   MAX N., 1995, 6th Eurographics Workshop on Rendering, P45
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Ramachandra V, 2008, IEEE IMAGE PROC, P2436, DOI 10.1109/ICIP.2008.4712285
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Stewart J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P150
   Sung K, 2002, IEEE T VIS COMPUT GR, V8, P144, DOI 10.1109/2945.998667
   Zhang C, 2003, IEEE T CIRC SYST VID, V13, P1038, DOI 10.1109/TCSVT.2003.817350
   ZHANG C, 2001, AMP0106
   ZWICKER M, 2007, INT C MULT ACM MULT, P1046
   Zwicker Matthias., 2006, P 17 EUROGRAPHICS C, P73
NR 32
TC 3
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 665
EP 676
DI 10.1007/s00371-011-0560-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Runions, A
   Samavati, FF
AF Runions, Adam
   Samavati, Faramarz F.
TI Partition of unity parametrics: a framework for meta-modeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Meta-modeling; Parametric curves and surfaces; Sketch-based modeling;
   Geometric modeling
ID SPLINES
AB We propose Partition of Unity Parametrics (PUPs), a natural extension of NURBS that maintains affine invariance. PUPs replace the weighted basis functions of NURBS with arbitrary weight-functions (WFs). By choosing appropriate WFs, PUPs yield a comprehensive geometric modeling framework, accounting for a variety of beneficial properties, such as local support, specified smoothness, arbitrary sharp features and approximating or interpolating curves. Additionally, we consider interactive specification of WFs to fine-tune the character of curves and generate non-trivial effects. This serves as a basis for a system where users model the tools used for modeling, here weight-functions, in tandem with the model itself, which we dub a meta-modeling system. PUP curves and surfaces are considered in detail. Curves illustrate basic concepts that apply directly to surfaces. For surfaces, the advantages of PUPs are more pronounced; permitting non-tensor WFs and direct parameter space manipulations. These features allow us to address two difficult geometric modeling problems (sketching features onto surfaces and converting planar meshes into parametric surfaces) in a conceptually and computationally simple way.
C1 [Runions, Adam; Samavati, Faramarz F.] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Runions, A (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM runionsa@cpsc.ucalgary.ca
RI Runions, Adam/Z-6049-2019
OI Runions, Adam/0000-0002-7758-7423
CR [Anonymous], CONSTRUCTIVE THEORY
   [Anonymous], SIGGRAPH 04
   [Anonymous], 1996, PRIOR ANAL
   [Anonymous], P SKETCH BAS INT MOD
   [Anonymous], SIGGRAPH 09
   Barsky B.A., 1988, COMPUTER GRAPHICS GE
   Brunn M, 2007, INT J IMAGE GRAPH, V7, P593, DOI 10.1142/S0219467807002829
   Buhmann MD, 2001, ACT NUMERIC, V9, P1, DOI 10.1017/S0962492900000015
   De Boor C, 1978, A Pratical Guide to Splines, V27
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FRANKE R, 1980, INT J NUMER METH ENG, V15, P1691, DOI 10.1002/nme.1620151110
   Grimm C., 1995, PROC SIGGRAPH 1995, P359
   Hormann K., 2007, SIGGRAPH 2007 COURSE, P1, DOI 10.1145/1281500.1281510
   Li QD, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516524
   Meyer M., 2002, Journal of Graphics Tools, V7, P13, DOI 10.1080/10867651.2002.10487551
   Nealen Andrew., 2007, SIGGRAPH 07, P41
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Piegl L., 1997, The Nurbs Book, Vsecond
   Piegl LA, 1999, VISUAL COMPUT, V15, P77, DOI 10.1007/s003710050163
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Shepard D., 1968, P 1968 23 ACM NAT C, P517, DOI DOI 10.1145/800186.810616
   Turk G., 1999, Variational implicit surfaces
   Wang Q, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P365
NR 25
TC 6
Z9 7
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 495
EP 505
DI 10.1007/s00371-011-0567-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600009
DA 2024-07-18
ER

PT J
AU Öhman, C
   Espino, DM
   Heinmann, T
   Baleani, M
   Delingette, H
   Viceconti, M
AF Oehman, C.
   Espino, D. M.
   Heinmann, T.
   Baleani, M.
   Delingette, H.
   Viceconti, M.
TI Subject-specific knee joint model: Design of an experiment to validate a
   multi-body finite element model
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Knee joint; FE model; Subject-specific; Experimental validation
ID ANTERIOR CRUCIATE LIGAMENT; STRAIN; FLEXION; LAXITY; FORCES; MOTION
AB The availability of a validated subject-specific model of the knee joint would be extremely useful for the orthopaedic surgeon in evaluating the biomechanics of the joint of a patient, especially when suspecting an injury of one or more components.
   The aim of this paper was to describe a procedure designed and developed to validate a subject-specific model of the human knee. The proposed approach considers the use of clinical images to create a multi-body finite element model of a healthy knee. The same joint must undergo an experimental test aimed at collecting the data necessary to validate the model predictions. Therefore, the experimental set-up must be designed to monitor all the degrees of freedom of the joint, allowing the replication of the loading conditions in silico with a finite element (FE) model.
   At the moment, an animal model is used to verify the accuracy and repeatability of the developed procedure.
C1 [Oehman, C.; Espino, D. M.; Baleani, M.; Viceconti, M.] Ist Ortoped Rizzoli, Lab Tecnol Med, Bologna, Italy.
   [Oehman, C.] Univ Bologna, Fac Engn, Bologna, Italy.
   [Heinmann, T.; Delingette, H.] INRIA, Asclepios Res Project, Sophia Antipolis, France.
   [Heinmann, T.] German Canc Res Ctr, Div Med & Biol Informat, D-6900 Heidelberg, Germany.
C3 IRCCS Istituto Ortopedico Rizzoli; University of Bologna; Inria;
   Helmholtz Association; German Cancer Research Center (DKFZ)
RP Öhman, C (corresponding author), Ist Ortoped Rizzoli, Lab Tecnol Med, Bologna, Italy.
EM ohman@tecno.ior.it
RI Viceconti, Marco/ABG-5546-2020; Baleani, Massimiliano/K-4757-2018;
   Viceconti, Marco/N-9164-2013; Viceconti, Marco/HOF-5985-2023; Viceconti,
   Marco/HNQ-6918-2023
OI Viceconti, Marco/0000-0002-2293-1530; Baleani,
   Massimiliano/0000-0002-9759-6474; Viceconti, Marco/0000-0002-2293-1530;
   Viceconti, Marco/0000-0002-2293-1530; Viceconti,
   Marco/0000-0002-2293-1530; Ohman Magi, Caroline/0000-0003-2709-9541;
   Delingette, Herve/0000-0001-6050-5949
CR Amis AA, 2003, KNEE SURG SPORT TR A, V11, P271, DOI 10.1007/s00167-003-0410-7
   Bull AMJ, 1998, P I MECH ENG H, V212, P357, DOI 10.1243/0954411981534132
   Bull AMJ, 2008, CLIN ORTHOP RELAT R, V466, P2491, DOI 10.1007/s11999-008-0440-z
   Caruntu DI, 2004, J BIOMECH ENG-T ASME, V126, P44, DOI 10.1115/1.1644565
   Chen E, 2001, MED IMAGE ANAL, V5, P317, DOI 10.1016/S1361-8415(01)00049-4
   Clavert P, 2001, SURG RADIOL ANAT, V23, P259, DOI 10.1007/s00276-001-0259-8
   Elias JJ, 2003, AM J SPORT MED, V31, P241, DOI 10.1177/03635465030310021401
   FUSS FK, 1991, J ANAT, V178, P11
   Gabriel MT, 2004, J ORTHOP RES, V22, P85, DOI 10.1016/S0736-0266(03)00133-5
   Griffith CJ, 2009, AM J SPORT MED, V37, P1762, DOI 10.1177/0363546509333852
   Griffith CJ, 2009, AM J SPORT MED, V37, P140, DOI 10.1177/0363546508322890
   Heimann T, 2010, COMPUTATIONAL BIOMECHANICS FOR MEDICINE, P107, DOI 10.1007/978-1-4419-5874-7_12
   Iwaki H, 2000, J BONE JOINT SURG BR, V82B, P1189, DOI 10.1302/0301-620X.82B8.10717
   Kanamori A, 2000, J Orthop Sci, V5, P567, DOI 10.1007/s007760070007
   Kato Y, 2010, KNEE SURG SPORT TR A, V18, P20, DOI 10.1007/s00167-009-0893-y
   Leitschuh PH, 1996, J ORTHOPAED RES, V14, P830, DOI 10.1002/jor.1100140522
   MARKOLF KL, 1976, J BONE JOINT SURG AM, V58, P583, DOI 10.2106/00004623-197658050-00001
   NIKOLAOU PK, 1986, AM J SPORT MED, V14, P348, DOI 10.1177/036354658601400502
   Noyes Frank R., 1983, The Iowa Orthopaedic Journal, V3, P32
   Öhman C, 2009, ACTA BIOENG BIOMECH, V11, P19
   Parenti-Castelli V, 2004, AUTON ROBOT, V16, P219, DOI 10.1023/B:AURO.0000016867.17664.b1
   RALIS ZA, 1989, J BONE JOINT SURG BR, V71, P55, DOI 10.1302/0301-620X.71B1.2915006
   Roos PJ, 2005, J ORTHOP RES, V23, P327, DOI 10.1016/j.orthres.2004.08.002
   Shin CS, 2007, J BIOMECH, V40, P1145, DOI 10.1016/j.jbiomech.2006.05.004
   Wijdicks CA, 2009, AM J SPORT MED, V37, P1771, DOI 10.1177/0363546509335191
   Withrow TJ, 2008, J BONE JOINT SURG AM, V90A, P815, DOI 10.2106/JBJS.F.01352
   Withrow TJ, 2006, AM J SPORT MED, V34, P269, DOI 10.1177/0363546505280906
   Woo S L, 1999, J Sci Med Sport, V2, P283, DOI 10.1016/S1440-2440(99)80002-4
   WOO SLY, 1986, J BIOMECH, V19, P399, DOI 10.1016/0021-9290(86)90016-3
NR 29
TC 14
Z9 16
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 153
EP 159
DI 10.1007/s00371-010-0537-8
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900008
DA 2024-07-18
ER

PT J
AU Hu, MX
   Feng, JQ
   Zheng, JM
AF Hu, Mingxiao
   Feng, Jieqing
   Zheng, Jianmin
TI An additional branch free algebraic B-spline curve fitting method
SO VISUAL COMPUTER
LA English
DT Article
DE Curve reconstruction; Curve fitting; Algebraic B-spline curve;
   Additional branch; Geometric distance; Distance field; Local calibration
ID POINT CLOUDS
AB Algebraic curve fitting based on the algebraic distance is simple, but it has the disadvantage of inclining to a trivial solution. Researchers therefore introduce some constraints into the objective function in order to avoid the trivial solution. However, this often causes additional branches. Fitting based on geometric distance can avoid additional branches, but it does not offer sufficient fitting precision. In this paper we present a novel algebraic B-spline curve fitting method which combines both geometric distance and algebraic distance. The method first generates an initial curve by a distance field fitting that takes geometric distance as the objective function. Then local topology-preserving calibrations based on algebraic distance are performed so that each calibration does not produce any additional branches. In this way, we obtain an additional branch free fitting result whose precision is close to or even better than that produced by purely algebraic distance based methods. The adopted precision criterion is the geometric distance error rather than the algebraic one. In addition, we find a calibration fatigue phenomenon about calibrating strategy and propose a hybrid mode to solve it.
C1 [Hu, Mingxiao; Feng, Jieqing] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Zhejiang University; Nanyang Technological University
RP Feng, JQ (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM jsj_hmx@wzu.edu.cn
RI Zheng, Jianmin/A-3717-2011
OI Zheng, Jianmin/0000-0002-5062-6226
FU NSF of China [60933007, 60736019]; 973 program of China [2009CB320801]
FX This work is jointly supported by the NSF of China (60933007, 60736019),
   the 973 program of China (2009CB320801).
CR Ahn SJ, 2004, LECT NOTES COMPUT SC, V3151, P1, DOI 10.1007/978-3-540-28627-1_1
   Ahn SJ, 2001, PATTERN RECOGN, V34, P2283, DOI 10.1016/S0031-3203(00)00152-7
   AIGNER M, 2005, MATH METHODS CURVES
   Elber G., 2001, P 6 ACM S SOLID MODE, P1, DOI [10.1145/376957.376958, DOI 10.1145/376957.376958]
   FARIN G, 2002, CURVES SURFACES CAGD, P119
   HUDSON J, 2003, PROCESSING LARGE POI
   Jüttler B, 2002, ADV COMPUT MATH, V17, P135, DOI 10.1023/A:1015200504295
   JUTTLER B, 2005, P 21 SPRING C COMP G, P13
   Krystek M, 2007, MEAS SCI TECHNOL, V18, P3438, DOI 10.1088/0957-0233/18/11/025
   Li Yun-Xi, 2007, Journal of Software, V18, P2306, DOI 10.1360/jos182306
   Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002
   Pratt Vaughan, 1987, Computer Graphics Vol, V21, P145, DOI DOI 10.1145/37402.37420
   Redding NJ, 2000, IEEE T PATTERN ANAL, V22, P191, DOI 10.1109/34.825757
   Sederberg TW, 1999, GRAPH MODEL IM PROC, V61, P177, DOI 10.1006/gmip.1999.0497
   Stamati V., 2007, P 2007 ACM S SOL PHY, P347
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Yang ZW, 2005, VISUAL COMPUT, V21, P831, DOI 10.1007/s00371-005-0340-0
NR 18
TC 6
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 801
EP 811
DI 10.1007/s00371-010-0476-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800040
DA 2024-07-18
ER

PT J
AU Ruiz, M
   Szirmay-Kalos, L
   Umenhoffer, T
   Boada, I
   Feixas, M
   Sbert, M
AF Ruiz, Marc
   Szirmay-Kalos, Lazlo
   Umenhoffer, Tamas
   Boada, Imma
   Feixas, Miquel
   Sbert, Mateu
TI Volumetric ambient occlusion for volumetric models
SO VISUAL COMPUTER
LA English
DT Article
DE Ambient occlusion; Obscurances; Volume rendering
AB This paper presents new algorithms to compute ambient occlusion for volumetric data. Ambient occlusion is used in video-games and film animation to mimic the indirect lighting of global illumination. We extend a novel interpretation of ambient occlusion to volumetric models that measures how big portion of the tangent sphere of a surface belongs to the set of occluded points, and propose statistically robust estimates for the ambient occlusion value. The data needed by this estimate can be obtained by separable filtering of the voxel array. As ambient occlusion is meant to obtain global illumination effects, it can provide decisive clues in interpreting the data. The new algorithms obtain smooth shading and can be computed at interactive rates, being thus appropriate for dynamic models exploration.
C1 [Ruiz, Marc; Boada, Imma; Feixas, Miquel; Sbert, Mateu] Univ Girona, Graph & Imaging Lab, Girona 17071, Spain.
   [Szirmay-Kalos, Lazlo; Umenhoffer, Tamas] Budapest Univ Technol & Econ, H-1117 Budapest, Hungary.
C3 Universitat de Girona; Budapest University of Technology & Economics
RP Ruiz, M (corresponding author), Univ Girona, Graph & Imaging Lab, Campus Montilivi, Girona 17071, Spain.
EM mruiz@ima.udg.cat; szirmay@iit.bme.hu; umitomi@freemail.hu;
   imma.boada@udg.edu; miquel.feixas@udg.edu; mateu.sbert@udg.edu
RI Szirmay-Kalos, Laszlo/H-3853-2012; Umenhoffer, Tamás/H-3732-2012; Boada,
   Imma/M-2877-2018; Tóth, Balázs/H-3704-2012; Sbert, Mateu/G-6711-2011;
   Feixas, Miquel/F-6762-2016
OI Szirmay-Kalos, Laszlo/0000-0002-8523-2315; Boada,
   Imma/0000-0002-0001-8193; Ruiz, Marc/0000-0001-6490-4170; Sbert,
   Mateu/0000-0003-2164-6858; Feixas, Miquel/0000-0001-6512-7588
FU Ministry of Education and Science [TIN2007-68066-C04-01,
   TIN2007-67982-C02]; Catalan Government [2009 SGR 643]; National Office
   for Research and Technology (Hungary) [OTKA K-719922]
FX This work was supported by projects TIN2007-68066-C04-01 and
   TIN2007-67982-C02 of the Ministry of Education and Science (Spanish
   Government), project 2009 SGR 643 from the Catalan Government, and
   projects Teratomo of the National Office for Research and Technology
   (Hungary) and OTKA K-719922. The CT-head data set is courtesy of North
   Carolina Memorial Hospital, US
   (http://www9.informatik.uni-erlangen.de/External/vollib/). The CT-body
   data set is courtesy of the Visible Human Project, National Library of
   Medicine, US (http://www.nlm.nih.gov/research/visible/).
CR DESGRANGES, 2007, Patent No. 20070013696
   GRUN H, 2008, SHADER X 7
   HERNELL F., 2007, IEEEEG S VOLUME GRAP, P1, DOI [10.2312/VG/VG07/001-008, DOI 10.2312/VG/VG07/001-008]
   Iones A, 2003, IEEE COMPUT GRAPH, V23, P54, DOI 10.1109/MCG.2003.1198263
   JAINEK W, 2008, ILLUSTRATIVE HYBRID, P855
   LANDIS H, 2002, COURS 16 NOT SIGGRAP
   Méndez-Feliu A, 2009, VISUAL COMPUT, V25, P181, DOI 10.1007/s00371-008-0213-4
   MENDEZFELIU A, 2003, EUROGRAPHICS 2003 GR
   Neumann L, 2000, COMPUT GRAPH FORUM, V19, pC351, DOI 10.1111/1467-8659.00427
   Penner E. V., 2008, Industrial Automated Control Systems and Controllers, P57
   Ropinski T, 2008, COMPUT GRAPH FORUM, V27, P567, DOI 10.1111/j.1467-8659.2008.01154.x
   Ruiz M., 2008, IEEE EG S VOLUME POI, P113, DOI [10.2312/VG/VG-PBG08/113-1202, 10.2312/VG/VG-PBG08/113-120, DOI 10.2312/VG/VG-PBG08/113-1202]
   Stewart AJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2003.1250394
   Szirmay-Kalos L, 2010, IEEE COMPUT GRAPH, V30, P70, DOI 10.1109/MCG.2010.19
   Winitzki S, 2008, A Handy approximation for the Error Function and its Inverse
   Wyman C, 2006, IEEE T VIS COMPUT GR, V12, P186, DOI 10.1109/TVCG.2006.33
   ZHUKOV S, 1998, P EUR WORKSH REND TE, P45
NR 17
TC 7
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 687
EP 695
DI 10.1007/s00371-010-0497-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800029
DA 2024-07-18
ER

PT J
AU Castro, GG
   Athanasopoulos, M
   Ugail, H
AF Castro, G. Gonzalez
   Athanasopoulos, M.
   Ugail, H.
TI Cyclic animation using partial differential equations
SO VISUAL COMPUTER
LA English
DT Article
DE Cyclic animation; PDE method; spine based animation
ID SURFACES
AB This work presents an efficient and fast method for achieving cyclic animation using partial differential equations (PDEs). The boundary-value nature associated with elliptic PDEs offers a fast analytic solution technique for setting up a framework for this type of animation. The surface of a given character is thus created from a set of pre-determined curves, which are used as boundary conditions so that a number of PDEs can be solved. Two different approaches to cyclic animation are presented here. The first of these approaches consists of attaching the set of curves to a skeletal system, which is responsible for holding the animation for cyclic motions through a set mathematical expressions. The second approach exploits the spine associated with the analytic solution of the PDE as a driving mechanism to achieve cyclic animation. The spine is also manipulated mathematically. In the interest of illustrating both approaches, the first one has been implemented within a framework related to cyclic motions inherent to human-like characters. Spine-based animation is illustrated by modelling the undulatory movement observed in fish when swimming. The proposed method is fast and accurate. Additionally, the animation can be either used in the PDE-based surface representation of the model or transferred to the original mesh model by means of a point to point map. Thus, the user is offered with the choice of using either of these two animation representations of the same object, the selection depends on the computing resources such as storage and memory capacity associated with each particular application.
C1 [Castro, G. Gonzalez; Athanasopoulos, M.; Ugail, H.] Univ Bradford, Sch Comp Informat & Media, Bradford BD1 5HD, W Yorkshire, England.
C3 University of Bradford
RP Castro, GG (corresponding author), Univ Bradford, Sch Comp Informat & Media, Richmond Rd, Bradford BD1 5HD, W Yorkshire, England.
EM g.gonzalezcastro1@bradford1.ac.uk
FU EPSRC [EP/G067732/1] Funding Source: UKRI
CR ALASKARI H, 2005, CYCLIC ANIMATION USE
   Allan C., 2003, NATURAL RESOURCE MAN, V6, P23
   [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Bertalmío M, 2001, J COMPUT PHYS, V174, P759, DOI 10.1006/jcph.2001.6937
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Castro GG, 2008, VISUAL COMPUT, V24, P213, DOI 10.1007/s00371-007-0190-z
   Du HX, 2005, GRAPH MODELS, V67, P43, DOI 10.1016/j.gmod.2004.06.002
   Du HX, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P213, DOI 10.1109/PCCGA.2000.883943
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Ieronutti L, 2007, COMPUT EDUC, V49, P93, DOI 10.1016/j.compedu.2005.06.007
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Kalra P, 1998, IEEE COMPUT GRAPH, V18, P42, DOI 10.1109/38.708560
   Lee HC, 2006, INT J COMPUT SCI NET, V6, P58
   LIGHTHILL MJ, 1969, ANNU REV FLUID MECH, V1, P413, DOI 10.1146/annurev.fl.01.010169.002213
   Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2
   Ormoneit D, 2005, IMAGE VISION COMPUT, V23, P1264, DOI 10.1016/j.imavis.2005.09.004
   ORMONEIT D, 2001, ENCY LIB INFORM SCI
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   PRATSCHER M, 2005, SCA 05, P329
   STEPHENSON SL, 2003, FUN NEW ZEA, V3, P1
   Terzopoulos D, 1999, COMMUN ACM, V42, P32, DOI 10.1145/310930.310966
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Tost D., 1988, Visual Computer, V3, P254, DOI 10.1007/BF01914860
   TU X, 1994, SIGGRAPH 94, P43
   Ugail H, 2003, LECT NOTES COMPUT SC, V2768, P366
   Ugail H, 1999, COMPUT GRAPH-UK, V23, P525, DOI 10.1016/S0097-8493(99)00071-0
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   Xu GL, 2006, COMPUT AIDED GEOM D, V23, P125, DOI 10.1016/j.cagd.2005.05.004
   Yang XL, 2005, IEEE T INSTRUM MEAS, V54, P1333, DOI 10.1109/TIM.2005.847134
   You LH, 2004, COMPUT GRAPH-UK, V28, P895, DOI 10.1016/j.cag.2004.08.003
   Yu QX, 1999, VISUAL COMPUT, V15, P377, DOI 10.1007/s003710050186
   Zhang YC, 2004, ORG LETT, V6, P23, DOI 10.1021/ol036020y
   Zhu Q, 2002, J FLUID MECH, V468, P1, DOI 10.1017/S002211200200143X
NR 35
TC 10
Z9 10
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2010
VL 26
IS 5
BP 325
EP 338
DI 10.1007/s00371-010-0422-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 587GS
UT WOS:000276978800004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Jacobs, K
   Nielsen, AH
   Vesterbaek, J
   Loscos, C
AF Jacobs, Katrien
   Nielsen, Anders Hjorth
   Vesterbaek, Jeppe
   Loscos, Celine
TI Coherent radiance capture of scenes under changing illumination
   conditions for relighting applications
SO VISUAL COMPUTER
LA English
DT Article
DE Radiance calibration; Relighting; Inverse illumination
AB Relighting algorithms make it possible to take a model of a real-world scene and virtually modify its lighting. Unlike other methods that require controlled conditions, we introduce a new radiance capture method that allows the user to capture parts of the scene under different lighting conditions. A novel calibration method is presented that finds the positions of reflective spheres and their mathematically accurate projection onto the scene geometry. The resulting radiance distribution is used to estimate a diffuse reflectance for each object, computed coherently using the appropriate light probe image. Finally, the scene is relit using a novel illumination pattern.
C1 [Loscos, Celine] Univ Girona, IIiA GGG, Girona 17071, Spain.
   [Jacobs, Katrien] UCL, London WC1E 6BT, England.
   [Nielsen, Anders Hjorth; Vesterbaek, Jeppe] Aalborg Univ, Aalborg, Denmark.
C3 Universitat de Girona; University of London; University College London;
   Aalborg University
RP Loscos, C (corresponding author), Univ Girona, IIiA GGG, Girona 17071, Spain.
EM celine.loscos@ima.udg.edu
RI Loscos, Celine/F-5973-2017
OI Loscos, Celine/0000-0002-0520-6249
CR Agusanto K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P208, DOI 10.1109/ISMAR.2003.1240704
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   [Anonymous], P SPIE C HUM VIS EL
   [Anonymous], THESIS CORNELL U ITH
   Boivin S, 2001, COMP GRAPH, P107, DOI 10.1145/383259.383270
   Debevec P., 1997, P ACM SIGGRAPH, P369, DOI DOI 10.1145/258734.258884
   DEBEVEC P, 2004, ICTTR062004 USC
   Debevec P., 1998, SIGGRAPH98, P189, DOI DOI 10.1145/280814.280864
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Elkin JackM., 1965, MATH TEACHER, V58, P194, DOI DOI 10.5951/MT.58.3.0194
   Hu GH, 2007, COMPUT GRAPH-UK, V31, P262, DOI 10.1016/j.cag.2007.01.002
   Jacobs K, 2006, COMPUT GRAPH FORUM, V25, P29, DOI 10.1111/j.1467-8659.2006.00816.x
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   *LEIC, LEIC HDS 3000
   Li YZ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P483, DOI 10.1109/PCCGA.2002.1167909
   Loscos C, 2000, IEEE T VIS COMPUT GR, V6, P289, DOI 10.1109/2945.895874
   LOSCOS C, 1999, P 10 EUR WORKSH REND, V10, P235
   *METACREATIONS, METACREATIONS CAN
   MILLER AR, 1991, COMPUTING GRAZING AN
   Neumann PM, 1998, AM MATH MON, V105, P523, DOI 10.2307/2589403
   OKATANI T, 2000, P 15 INT C PATT REC, V3
   Patow G, 2003, COMPUT GRAPH FORUM, V22, P663, DOI 10.1111/j.1467-8659.2003.00716.x
   Pharr Matt, PHYS BASED RENDERING
   *REALV, REALV IM MOD
   Sato I, 1999, IEEE T VIS COMPUT GR, V5, P1, DOI 10.1109/2945.764865
   Troccoli A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P655
   Troccoli Alejandro., 2005, 3DIM
   Waldvogel J., 1992, ELEM MATH, V47, P108
   Yu Y., 1998, P 25 INT C COMPUTER, P207
   YU Y, 1999, SIGGRAPH, V99, P215
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   PHOTOMATIX MULTIMEDI
NR 32
TC 3
Z9 3
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2010
VL 26
IS 3
BP 171
EP 185
DI 10.1007/s00371-009-0360-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 558CW
UT WOS:000274719200002
DA 2024-07-18
ER

PT J
AU Aigner, M
   Jüttler, B
AF Aigner, Martin
   Juettler, Bert
TI Robust fitting of implicitly defined surfaces using Gauss-Newton-type
   techniques
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Shape Modeling and Applications
CY JUN 04-06, 2008
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH, EUROGRAPHICS, Comp Graph Soc
DE Surface fitting; Implicitly defined surfaces; Gauss-Newton method;
   General error function
ID EVOLUTION; CURVES
AB We describe Gauss-Newton-type methods for fitting implicitly defined curves and surfaces to given unorganized data points. The methods are suitable not only for least-squares approximation, but they can also deal with general error functions, such as approximations to the a"" (1) or a"" (a) norm of the vector of residuals. Two different definitions of the residuals will be discussed, which lead to two different classes of methods: direct methods and data-based ones. In addition we discuss the continuous versions of the methods, which furnish geometric interpretations as evolution processes.
   It is shown that the data-based methods-which are less costly, as they work without the computation of the closest points-can efficiently deal with error functions that are adapted to noisy and uncertain data. In addition, we observe that the interpretation as evolution process allows to deal with the issues of regularization and with additional constraints.
C1 [Aigner, Martin; Juettler, Bert] Johannes Kepler Univ Linz, Inst Appl Geometry, A-4040 Linz, Austria.
C3 Johannes Kepler University Linz
RP Aigner, M (corresponding author), Johannes Kepler Univ Linz, Inst Appl Geometry, A-4040 Linz, Austria.
EM martin.aigner@jku.at; bert.juettler@jku.at
OI Juttler, Bert/0000-0002-5518-7795
CR Aigner M, 2007, COMPUTING, V79, P237, DOI 10.1007/s00607-006-0201-3
   AIGNER M, 2007, CURVE SURFACE DESIGN, P1
   AIGNER M, 2007, 54 FSP S92 IND GEOM
   Aigner M., 2005, MATH METHODS CURVES, P1
   Aigner M, 2007, COMPUT AIDED GEOM D, V24, P310, DOI 10.1016/j.cagd.2007.04.001
   Al-Subaihi I., 2004, Applied Numerical Analysis and Computational Mathematics, V1, P363, DOI 10.1002/anac.200410003
   Alhanaty M, 2001, COMPUT AIDED DESIGN, V33, P167, DOI 10.1016/S0010-4485(00)00089-0
   [Anonymous], 2002, SURFACES
   Atieg A., 2004, Australian and New Zealand Industrial and Applied Mathematics Journal, V45, pC187
   Blake Andrew., 2000, Active Contours
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Feichtinger R, 2008, COMPUT AIDED DESIGN, V40, P13, DOI 10.1016/j.cad.2007.08.003
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Hoschek J, 1996, Fundamentals of computer aided geometric design
   Huber P., 1981, Robust Statistics
   Juttler B., 1998, International Journal of Shape Modeling, V4, P21, DOI 10.1142/S0218654398000039
   Jüttler B, 2000, MATHEMATICS OF SURFACES IX, P263
   Jüttler B, 2002, ADV COMPUT MATH, V17, P135, DOI 10.1023/A:1015200504295
   Mahadevan V, 2004, IEEE T INF TECHNOL B, V8, P360, DOI 10.1109/TITB.2004.834410
   Ohtake Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P31, DOI 10.1109/SMI.2004.1314491
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Osborne M. R., 1985, FINITE ALGORITHMS OP
   Pottmann H, 2005, COMPUT AIDED DESIGN, V37, P751, DOI 10.1016/j.cad.2004.08.013
   Pottmann H, 2003, COMPUT AIDED GEOM D, V20, P343, DOI 10.1016/S0167-8396(03)00078-5
   PRATT V, 1987, SIGGRAPH 87, P145
   Raviv A., 1999, Proceedings of Fifth ACM Symposium on Solid Modeling and Applications, P246
   ROGERS DF, 1989, COMPUT AIDED DESIGN, V21, P641, DOI 10.1016/0010-4485(89)90162-0
   SAMPSON PD, 1982, COMPUT VISION GRAPH, V18, P97, DOI 10.1016/0146-664X(82)90101-0
   SARKAR B, 1991, COMPUT AIDED GEOM D, P267, DOI DOI 10.1016/0167-8396(91)90016-5
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   SHEN C, 2004, SIGGRAPH 04, P896
   Speer T, 1998, COMPUT AIDED GEOM D, V15, P869, DOI 10.1016/S0167-8396(98)00024-7
   Tasdizen T, 2000, IEEE T IMAGE PROCESS, V9, P405, DOI 10.1109/83.826778
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Velho L., 2002, IMPLICIT OBJECTS COM, DOI DOI 10.1007/B97350
   WANG W, 2006, ACM T GRAPH, V25
   Watson GA, 2002, IMA J NUMER ANAL, V22, P345, DOI 10.1093/imanum/22.3.345
   WATSON GA, 1977, COMPUTING, V18, P263, DOI 10.1007/BF02253212
   Yang HP, 2008, VISUAL COMPUT, V24, P435, DOI 10.1007/s00371-008-0222-3
   Yang HP, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P247
   Yang ZW, 2005, VISUAL COMPUT, V21, P831, DOI 10.1007/s00371-005-0340-0
   Zeng HF, 2005, Computer Graphics, Imaging and Vision: New Trends, P251
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
NR 43
TC 6
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2009
VL 25
IS 8
BP 731
EP 741
DI 10.1007/s00371-009-0361-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 465NI
UT WOS:000267593700002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Erleben, K
AF Erleben, Kenny
TI Maximal independent set graph partitions for representations of
   body-centered cubic lattices
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Graph; Refinement; Coarsening; Body-centered cubic lattice
AB A maximal independent set graph data structure for a body-centered cubic lattice is presented. Refinement and coarsening operations are defined in terms of set-operations resulting in robust and easy implementation compared to a quad-tree-based implementation. The graph only stores information corresponding to the leaves of a quad-tree thus has a smaller memory foot-print. The adjacency information in the graph relieves one from going up and down the quad-tree when searching for neighbors. This results in constant time complexities for refinement and coarsening operations.
C1 Univ Copenhagen, eSci Ctr, Copenhagen, Denmark.
C3 University of Copenhagen
RP Erleben, K (corresponding author), Univ Copenhagen, eSci Ctr, Copenhagen, Denmark.
EM kenny@diku.dk
RI Erleben, Kenny/AAZ-6556-2020
OI Erleben, Kenny/0000-0001-6808-4747
CR Cormen Thomas H., 2001, INTRO ALGORITHMS
   De Floriani L, 2002, TUTORIALS ON MULTIRESOLUTION IN GEOMETRIC MODELLING, P363
   DELGADOFRIEDRIC.O, 2005, J SOLID STATE CHEM, V178
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   Entezari A, 2008, IEEE T VIS COMPUT GR, V14, P313, DOI 10.1109/TVCG.2007.70429
   Labelle F, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239508
   MOLINO N, 2003, P 12 INT MESH ROUNDT, P103
   SAMET H, 2007, SIGGRAPH 07 ACM SIGG
   Velho L, 2001, COMPUT AIDED GEOM D, V18, P397, DOI 10.1016/S0167-8396(01)00039-5
   WOJTAN C, 2008, SIGGRAPH 08, P1
NR 10
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 423
EP 430
DI 10.1007/s00371-009-0330-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300006
DA 2024-07-18
ER

PT J
AU Teng, CH
   Chen, YS
AF Teng, Chin-Hung
   Chen, Yung-Sheng
TI Image-based tree modeling from a few images with very narrow viewing
   range
SO VISUAL COMPUTER
LA English
DT Article
DE Tree modeling; Image-based modeling; 3D reconstruction
ID INTERACTIVE DESIGN; PHOTOGRAPHS
AB Creating 3D tree models from actual trees is a task receiving increasing attention. Some approaches have been developed to reconstruct a tree based on a number of photographs around the tree, typically spanning a wide viewing range. However, due to the environmental restrictions, sometimes it is quite difficult to capture so many acceptable images from so many different viewpoints. In this paper, we propose a tree modeling system which is capable of reconstructing the 3D model of a tree from a few images with very narrow viewing ranges. Because only a few images are required to generate the model, our system has the distinct advantage of fewer environmental restrictions, resulting in the extended usability and flexibility for real applications.
C1 [Teng, Chin-Hung] Yuan Ze Univ, Dept Informat Commun, Chungli 32003, Taiwan.
   [Chen, Yung-Sheng] Yuan Ze Univ, Dept Elect Engn, Chungli 32003, Taiwan.
C3 Yuan Ze University; Yuan Ze University
RP Teng, CH (corresponding author), Yuan Ze Univ, Dept Informat Commun, Chungli 32003, Taiwan.
EM chteng@saturn.yzu.edu.tw; eeyschen@saturn.yzu.edu.tw
CR Boudon F, 2003, COMPUT GRAPH FORUM, V22, P591, DOI 10.1111/1467-8659.t01-2-00707
   de Reffye P., 1988, Computer Graphics, V22, P151, DOI 10.1145/378456.378505
   Hartley R., 2002, MULTIPLE VIEW GEOMET
   INGBER L, 1993, MATH COMPUT MODEL, V18, P29, DOI 10.1016/0895-7177(93)90204-C
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Lintermann B, 1999, IEEE COMPUT GRAPH, V19, P56, DOI 10.1109/38.736469
   MECH R, 1996, ANN C SERIES, P397
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Okabe M, 2005, COMPUT GRAPH FORUM, V24, P487, DOI 10.1111/j.1467-8659.2005.00874.x
   Power J. L., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P175, DOI 10.1145/300523.300548
   Press W. H., 1999, NUMERICAL RECIPES C
   PRUSINKIEWICZ P, 1994, ANN C SERIES, V94, P351
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929
   Reche A, 2004, ACM T GRAPHIC, V23, P720, DOI 10.1145/1015706.1015785
   Shlyakhter I, 2001, IEEE COMPUT GRAPH, V21, P53, DOI 10.1109/38.920627
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Teng CH, 2006, APPL OPTICS, V45, P688, DOI 10.1364/AO.45.000688
   Teng CH, 2005, COMPUT VIS IMAGE UND, V97, P315, DOI 10.1016/j.cviu.2004.08.002
   WEBER J, 1995, ANN C SERIES, P119
NR 20
TC 16
Z9 23
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2009
VL 25
IS 4
BP 297
EP 307
DI 10.1007/s00371-008-0269-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 413YH
UT WOS:000263830600001
DA 2024-07-18
ER

PT J
AU Aprile, WA
   Ruffaldi, E
   Sotgiu, E
   Frisoli, A
   Bergamasco, M
AF Aprile, Walter A.
   Ruffaldi, Emanuele
   Sotgiu, Edoardo
   Frisoli, Antonio
   Bergamasco, Massimo
TI A dynamically reconfigurable stereoscopic/panoramic vision mobile robot
   head controlled from a virtual environment
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Panoramic computer vision; Stereoscopic computer vision; Robot head;
   Mobile robot; Telerobotics; Spherical mirror
AB We have built a mobile robotic platform that features an Active Robotic Head (ARH) with two high-resolution cameras that can be switched during robot operation between two configurations that produce respectively panoramic and stereoscopic images. Image disparity is used for improving the quality of the texture. The robot head switches dynamically, based on robot operation between the stereoscopic configuration and the panoramic configuration.
C1 [Aprile, Walter A.; Ruffaldi, Emanuele; Sotgiu, Edoardo; Frisoli, Antonio; Bergamasco, Massimo] Scuola Super Sant Anna, Pisa, Italy.
   [Aprile, Walter A.] Delft Univ Technol, Delft, Netherlands.
C3 Scuola Superiore Sant'Anna; Delft University of Technology
RP Aprile, WA (corresponding author), Scuola Super Sant Anna, Pisa, Italy.
EM walter.aprile@sssup.it; emanuele.ruffaldi@sssup.it;
   edoardo.sotgiu@sssup.it; antony@sssup.it; m.bergamasco@sssup.it
RI Sotgiu, Edoardo/Y-4081-2019; Ruffaldi, Emanuele/A-2352-2009; Sotgiu,
   Edoardo/F-4412-2012
OI Sotgiu, Edoardo/0000-0003-2825-1672; BERGAMASCO,
   Massimo/0000-0002-7418-2332; Ruffaldi, Emanuele/0000-0001-6084-6938
CR BAKER S, 1998, P 6 INT C COMP VIS B
   BOULT TE, 1998, DARPA IM UND WORKSH, P1049
   CARROZZINO M, 2005, P ACE 2005 VAL SPAIN
   FIALA M, 2005, INT C INT ROB SYST I
   Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135
   RICKS BW, 2004, INT C INT ROB SYST I
   *VRMEDIA, 2007, EXTR VR
NR 7
TC 2
Z9 2
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 941
EP 946
DI 10.1007/s00371-008-0278-0
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600003
DA 2024-07-18
ER

PT J
AU Reif, R
   Walch, D
AF Reif, Rupert
   Walch, Dennis
TI Augmented & Virtual Reality applications in the field of logistics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Logistics; Order picking; Process planning; Virtual Reality; Augmented
   Reality
AB Changing basic conditions in the field of logistics requires intuitive planning methods as well as new ways of supporting and educating the operative staff. VR and AR show a great promise for that.
C1 [Reif, Rupert; Walch, Dennis] Tech Univ Munich, Dept Mat Handling Mat Flow & Logist, D-85748 Garching, Germany.
C3 Technical University of Munich
RP Reif, R (corresponding author), Tech Univ Munich, Dept Mat Handling Mat Flow & Logist, Boltzmannstr 15, D-85748 Garching, Germany.
EM reif@fml.mw.tum.de
CR Bowman D., 2006, 3D user interfaces Theory and practice
   DANGELMAIER W, 2006, P 7 INT C PROD ENG L
   DEDE C, 2006, MEDIATED IMMERSION S
   DOIL F, 2003, P 7 INT IMM PROJ TEC
   Friedrich W., 2004, Arvika-augmented reality in Entwicklung, Produktion und Service
   Gudehus T., 2007, Logistics. A handbook: Principles, strategies, operations
   KLINKER G, 2006, P 16 INT C ART REAL
   Ong SK, 2004, VIRTUAL AND AUGMENTED REALITY APPLICATIONS IN MANUFACTURING, P1
   Pentenrieder K, 2006, P 5 IEEE ACM INT S M
   TENHOMPEL M, 2004, WAREHOUSE MANAGEMENT
NR 10
TC 48
Z9 53
U1 3
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 987
EP 994
DI 10.1007/s00371-008-0271-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600009
DA 2024-07-18
ER

PT J
AU Böttcher, G
   Allerkamp, D
   Glöckner, D
   Wolter, FE
AF Boettcher, Guido
   Allerkamp, Dennis
   Gloeckner, Daniel
   Wolter, Franz-Erich
TI Haptic two-finger contact with textiles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds
CY OCT 24-27, 2007
CL Hannover, GERMANY
SP Welfenlab, Gottfried Wilhelm Leibniz Univ, EuroGraphics, ACM SIGWEB, ACM SIGART
DE virtual reality; haptics; deformable objects; tactile rendering
AB Real-time cloth simulation involves many computational challenges to be solved, particularly in the context of haptic applications, where high frame rates are necessary for obtaining a satisfying experience. In this paper, we present an interactive cloth simulation system that offers a compromise between a realistic physics-based simulation of fabrics and a haptic application meeting high requirements in terms of computation speed. Our system allows the user to interact with the fabric using two fingers. The required performance of the system is achieved by introducing an intermediate layer responsible for the simulation of the small part of the surface being in contact with the fingers. Additionally we separate the possible contact situations into different cases, each being individually handled by a specialised contact algorithm.
C1 [Boettcher, Guido; Allerkamp, Dennis; Gloeckner, Daniel; Wolter, Franz-Erich] Leibniz Univ Hannover, Inst Man Machine Commun, Welfenlab, Comp Graph Div, Hannover, Germany.
C3 Leibniz University Hannover
RP Böttcher, G (corresponding author), Leibniz Univ Hannover, Inst Man Machine Commun, Welfenlab, Comp Graph Div, Hannover, Germany.
EM boettcher@welfenlab.de; allerkam@welfenlab.de; glockner@welfenlab.de;
   few@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz - Erich/B-1672-2014;
   Wolter, Franz-Erich/AAV-3008-2020
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494
CR Allerkamp D, 2007, VISUAL COMPUT, V23, P97, DOI 10.1007/s00371-006-0031-5
   BALANIUK R, 1999, PUG99
   BARAFF D, 1925, SIGGRAPH 98, P43, DOI DOI 10.1145/280814.280821
   Barbagli F, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P109, DOI 10.1109/HAPTIC.2003.1191248
   Bergamasco M, 2007, VISUAL COMPUT, V23, P247, DOI 10.1007/s00371-007-0103-1
   BOTSCH M, 2002, P OPENSG S 2002
   Bro-Nielsen M, 1998, P IEEE, V86, P490, DOI 10.1109/5.662874
   CAVUSOGLU MC, 2000, ROBOTICS AUTOMATION, V3, P2458, DOI DOI 10.1109/ROBOT.2000.846397
   DURIEZ C, 2004, INT ROB SYST 2004 IR, V4, P3232, DOI DOI 10.1109/IR0S.2004.1389915
   Etzmuss O, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/PCCGA.2003.1238266
   FONTANA M, 2007, CYBERWORLDS 2007 CW, P277, DOI DOI 10.1109/CW.2007.40
   Hutchinson D., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P31
   Magnenat-Thalmann N., 2007, The International Journal of Virtual Reality, V6, P35
   MAHVASH M, 2002, COMPUT GRAPH APPL IE, V24, P48, DOI DOI 10.1109/MCG.2004.1274061
   Mark W.R., 1996, Proceedings of SIGGRAPH on Computer Graphics, P447, DOI DOI 10.1145/237170.237284
   MAZZELLA F, 2002, ROBOTICS AUTOMATION, V1, P939, DOI DOI 10.1109/ROBOT.2002.1013477
   Pawluk DTV, 1999, J BIOMECH ENG-T ASME, V121, P178, DOI 10.1115/1.2835100
   PEINECKE N, 2007, CYBERWORLDS 2007 CW, P308, DOI DOI 10.1109/CW.2007.38
   Ruspini D. C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P345, DOI 10.1145/258734.258878
   RUSPINI DC, 1997, IEEE INT C INT ROBOT, V1, P128, DOI DOI 10.1109/IROS.1997.649024
   SUMMERS IR, 2008, FINAL DEMONSTRATOR F
   VOLINO P, 2005, P HAPTEX 05 WORKSH H, P17
   VOLINO P, 2007, CYBERWORLDS 2007 CW, P300, DOI DOI 10.1109/CW.2007.11
   Volino P, 2007, VISUAL COMPUT, V23, P133, DOI 10.1007/s00371-006-0034-2
   Zhuang Y., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P2428, DOI 10.1109/ROBOT.2000.846391
   ZILLES CB, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P146, DOI 10.1109/IROS.1995.525876
NR 26
TC 7
Z9 11
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2008
VL 24
IS 10
BP 911
EP 922
DI 10.1007/s00371-008-0287-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 347IQ
UT WOS:000259134200007
DA 2024-07-18
ER

PT J
AU De Zutter, S
   Asbach, M
   De Bruyne, S
   Unger, M
   Wien, M
   Van de Walle, R
AF De Zutter, Saar
   Asbach, Mark
   De Bruyne, Sarah
   Unger, Michael
   Wien, Mathias
   Van de Walle, Rik
TI System architecture for semantic annotation and adaptation in content
   sharing environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE semantic annotation and adaptation; description-driven adaptation;
   system architecture; content sharing environment
AB This paper describes a system architecture, which enables the automatic semantic annotation and adaptation of multimedia content in context-aware content sharing environments. The discussed architecture is the result of research done in the EU FP6 IST INTERMEDIA project. Generating a common vision on user-centric multimedia services in shared content environments to provide users with content personalized to their user preferences and usage environment is one of the objectives of the project. The work presented in this paper describes how media formats with their related metadata are automatically annotated and dynamically adapted. Based on the architecture, a full-featured demonstrator is built.
C1 [De Zutter, Saar; De Bruyne, Sarah; Van de Walle, Rik] Univ Ghent, Dept Elect & Informat Syst, Multimedia Lab, B-9050 Ledeberg Ghent, Belgium.
   [Asbach, Mark; Unger, Michael; Wien, Mathias] Rhein Westfal TH Aachen, Inst Commun Engn IENT, D-52056 Aachen, Germany.
   [De Zutter, Saar; De Bruyne, Sarah; Van de Walle, Rik] IBBT, B-9050 Ledeberg Ghent, Belgium.
C3 Ghent University; RWTH Aachen University
RP De Zutter, S (corresponding author), Univ Ghent, Dept Elect & Informat Syst, Multimedia Lab, Gaston Crommenlaan 8 Bus 201, B-9050 Ledeberg Ghent, Belgium.
EM saar.dezutter@elis.ugent.be
CR ASBACH M, INTERMEDIA MULTIMEDI
   Bradski G, 2000, OPENCV LIB
   Burnett I.S., 2006, MPEG 21 BOOK, V1st
   Clark James, 1999, Xsl transformations (xslt)
   DEBRUYNE S, 2007, P 1 S COMP 6 INT IM, P380
   *EUR 6 FRAM PROGR, 2008, INTERMEDIA INT MED P
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   *IETF, 1998, 2326 RFC IETF RTSP
   *IETF, 1996, 1889 RFC IETF
   Le Feuvre J., 2007, P 15 ACM INT C MULT, P1009, DOI [10.1145/1291233.1291452, DOI 10.1145/1291233.1291452]
   MANJUNATH BS, 2003, INTRO MPEG 7 MULTIME
   Pereira F., 2002, IMSC Press multimedia series
   RANSBURG M, 2006, MOBIMEDIA 06
   Van Deursen D, 2007, IEEE INT SYM MULTIM, P131, DOI 10.1109/ISM.2007.13
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P16, DOI 10.1109/MSP.2003.1184335
   Zimmermann A, 2005, USER MODEL USER-ADAP, V15, P275, DOI 10.1007/s11257-005-1092-2
   2006, SOURCE FORGE STREAMI
NR 17
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 735
EP 743
DI 10.1007/s00371-008-0255-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800029
OA Green Published
DA 2024-07-18
ER

PT J
AU Okaichi, N
   Johan, H
   Imagire, T
   Nishita, T
AF Okaichi, Naoto
   Johan, Henry
   Imagire, Takashi
   Nishita, Tomoyuki
TI A virtual painting knife
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE non-photorealistic rendering; painting systems; painting knife model
ID MODEL
AB Recently, in the area of non-photorealistic rendering, there has been a significant effort in digital painting research to simulate traditional painting styles and painting pigments. In particular, the simulation of painting tools is very important because this enables an intuitive painting experience for a user and generates rich painting effects. Many painting systems have been proposed using brushes, but less for other painting tools. As a result, there is still a limitation in the variety of painting effects that can be generated. In this paper, we propose a method to simulate painting using a painting knife, which is an important tool in oil painting. We model a painting knife and model the pigments such that the system is suitable for realizing the impasto style. We also present a technique for simulating the interaction between a painting knife and pigments on a canvas in real-time.
C1 [Okaichi, Naoto; Imagire, Takashi; Nishita, Tomoyuki] Univ Tokyo, Tokyo 2778561, Japan.
   [Johan, Henry] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Imagire, Takashi] Namco Bandai Games Inc, Santa Clara, CA 95054 USA.
C3 University of Tokyo; Nanyang Technological University
RP Okaichi, N (corresponding author), Univ Tokyo, Tokyo 2778561, Japan.
EM okaichi@nis-lab.is.s.u-tokyo.ac.jp; henryjohan@ntu.edu.sg;
   imagire@nis-lab.is.s.u-tokyo.ac.jp; nis@nis-lab.is.s.u-tokyo.ac.jp
RI Johan, Henry/A-3707-2011
OI Okaichi, Naoto/0000-0001-9747-746X
CR Baxter B, 2001, COMP GRAPH, P461, DOI 10.1145/383259.383313
   Baxter W, 2004, COMPUT ANIMAT VIRT W, V15, P433, DOI 10.1002/cav.47
   Baxter William., 2004, Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering, P45
   Baxter WV, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P319
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   Chu NSH, 2004, IEEE COMPUT GRAPH, V24, P76, DOI 10.1109/MCG.2004.37
   CURTIS CJ, 1997, P SIGGRAPH 97, P461
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   LAERHOVEN TV, 2007, VISUAL COMPUT, V23, P763
   Matsui H, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P148
   Murakami K, 2006, VISUAL COMPUT, V22, P415, DOI 10.1007/s00371-006-0021-7
   Onoue K., 2003, Journal of the Institute of Image Electronics Engineers of Japan, V32, P328
   Rudolf D, 2005, COMPUT GRAPH FORUM, V24, P27, DOI 10.1111/j.1467-8659.2005.00826.x
   Rudolf D, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P163, DOI 10.1109/PCCGA.2003.1238258
   SAITO S, 2000, JYOUHOUSHORI GAKKAI, V41, P608
   SALISBURY MP, 1997, P SIGGRAPH 97, P401
   SOBCZYK D, 2003, P EUROGRAPHICS 2003
   TAKAGI S, 1999, PAC GRAPH 99 C P, P250
   Van Haevre W, 2007, VISUAL COMPUT, V23, P925, DOI 10.1007/s00371-007-0144-5
   Xu S, 2007, COMPUT GRAPH FORUM, V26, P609, DOI 10.1111/j.1467-8659.2007.01084.x
NR 21
TC 6
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 753
EP 763
DI 10.1007/s00371-008-0257-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800031
DA 2024-07-18
ER

PT J
AU Reuter, M
   Mikkelsen, TS
   Sherbrooke, EC
   Maekawa, T
   Patrikalakis, NM
AF Reuter, Martin
   Mikkelsen, Tarjei S.
   Sherbrooke, Evan C.
   Maekawa, Takashi
   Patrikalakis, Nicholas M.
TI Solving nonlinear polynomial systems in the barycentric Bernstein basis
SO VISUAL COMPUTER
LA English
DT Article
DE CAD; CAGD; CAM; geometric modeling; solid modeling; intersections;
   distance computation; engineering design
ID ALGORITHM
AB We present a method for solving arbitrary systems of N nonlinear polynomials in n variables over an n-dimensional simplicial domain based on polynomial representation in the barycentric Bernstein basis and subdivision. The roots are approximated to arbitrary precision by iteratively constructing a series of smaller bounding simplices. We use geometric subdivision to isolate multiple roots within a simplex. An algorithm implementing this method in rounded interval arithmetic is described and analyzed. We find that when the total order of polynomials is close to the maximum order of each variable, an iteration of this solver algorithm is asymptotically more efficient than the corresponding step in a similar algorithm which relies on polynomial representation in the tensor product Bernstein basis. We also discuss various implementation issues and identify topics for further study.
C1 [Reuter, Martin; Mikkelsen, Tarjei S.; Sherbrooke, Evan C.; Patrikalakis, Nicholas M.] MIT, Cambridge, MA 02139 USA.
   [Maekawa, Takashi] Yokohama Natl Univ, Yokohama, Kanagawa 2408501, Japan.
C3 Massachusetts Institute of Technology (MIT); Yokohama National
   University
RP Reuter, M (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM reuter@mit.edu
RI Patrikalakis, Nicholas M/G-9853-2011; Mikkelsen, Tarjei S/A-1306-2007;
   Maekawa, Takashi/AAI-4292-2020; Reuter, Martin/B-3456-2010
OI Reuter, Martin/0000-0002-2665-9693; Maekawa, Takashi/0000-0003-3896-8431
CR Abrams SL, 1998, COMPUT AIDED DESIGN, V30, P657, DOI 10.1016/S0010-4485(97)00086-9
   Farin G., 1986, Computer-Aided Geometric Design, V3, P83, DOI 10.1016/0167-8396(86)90016-6
   Goldman R. N., 1990, Computer-Aided Geometric Design, V7, P69, DOI 10.1016/0167-8396(90)90022-J
   GOLDMAN RN, 1983, COMPUT AIDED DESIGN, V15, P159, DOI 10.1016/0010-4485(83)90083-0
   Hanniel I, 2007, COMPUT AIDED DESIGN, V39, P369, DOI 10.1016/j.cad.2007.02.004
   Hu CY, 1997, COMPUT AIDED DESIGN, V29, P617, DOI 10.1016/S0010-4485(96)00099-1
   LANE JM, 1980, IEEE T PATTERN ANAL, V2, P35, DOI 10.1109/TPAMI.1980.4766968
   SABLONNIERE P, 1978, COMPUT AIDED DESIGN, V10, P257, DOI 10.1016/0010-4485(78)90061-1
   SEDERBERG TW, 1989, COMPUT AIDED DESIGN, V21, P547, DOI 10.1016/0010-4485(89)90015-8
   SHERBROOKE EC, 1993, COMPUT AIDED GEOM D, V10, P379, DOI 10.1016/0167-8396(93)90019-Y
   WAGGENSPACK WN, 1986, COMPUT AIDED DESIGN, V18, P529, DOI 10.1016/0010-4485(86)90040-0
   [No title captured]
NR 12
TC 19
Z9 19
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2008
VL 24
IS 3
BP 187
EP 200
DI 10.1007/s00371-007-0184-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 255ZA
UT WOS:000252695900003
DA 2024-07-18
ER

PT J
AU Iwasaki, K
   Dobashi, Y
   Yoshimoto, F
   Nishita, T
AF Iwasaki, Kei
   Dobashi, Yoshinori
   Yoshimoto, Fujiichi
   Nishita, Tomoyuki
TI GPU-based rendering of point-sampled water surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE real-time rendering; GPU; point-sampled geometry; caustics
AB Particle-based simulations are widely used to simulate fluids. We present a real-time rendering method for the results of particle-based simulations of water. Traditional approaches to visualize the results of particle-based simulations construct water surfaces that are usually represented by polygons. To construct water surfaces from the results of particle-based simulations, a density function is assigned to each particle and a density field is computed by accumulating the values of the density functions of all particles. However, the computation of the density field is time consuming. To address this problem, we propose an efficient calculation of density field using a graphics processing unit (GPU). We present a rendering method for water surfaces sampled by points. The use of the GPU permits efficient simulation of optical effects, such as refraction, reflection, and caustics.
C1 [Dobashi, Yoshinori] Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 060, Japan.
   [Nishita, Tomoyuki] Univ Tokyo, Grad Sch Frontier Sci, Tokyo, Japan.
C3 Hokkaido University; University of Tokyo
RP Iwasaki, K (corresponding author), Wakayama Univ, Dept Comp & Commun Sci, Wakayama, Japan.
EM iwasaki@sys.wakayama-u.ac.jp; doba@nis-ei.eng.hokudai.ac.jp;
   fuji@sys.wakayama-u.ac.jp; nis@is.s.u-tokyo.ac.jp
RI Iwasaki, Kei/GNH-6504-2022
OI Iwasaki, Kei/0000-0002-5235-536X
CR [Anonymous], P CG INT
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   Co CS, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2003.1238274
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Guennebaud G, 2004, COMPUT GRAPH FORUM, V23, P653, DOI 10.1111/j.1467-8659.2004.00797.x
   Iwasaki K, 2003, COMPUT GRAPH FORUM, V22, P601, DOI 10.1111/1467-8659.t01-2-00708
   Koshizuka S., 1995, Computational Fluid Dynamics Journal, V113, P134
   Kunimatsu A, 2001, COMPUT GRAPH FORUM, V20, pC57, DOI 10.1111/1467-8659.00498
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MATSUMURA M, 2003, P EL IM 2003, P145
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   Nishida Toshisada, 1994, P373, DOI 10.1145/192161.192261
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Premoze S, 2003, COMPUT GRAPH FORUM, V22, P401, DOI 10.1111/1467-8659.00687
   RECK F, 2004, P EUR 2004 SHORT PRE
   STAH M, 2007, IEEE T VIS COMPUT GR, V13, P272
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Takahashi T, 2003, COMPUT GRAPH FORUM, V22, P391, DOI 10.1111/1467-8659.00686
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 22
TC 3
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2008
VL 24
IS 2
BP 77
EP 84
DI 10.1007/s00371-007-0186-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 252ZH
UT WOS:000252486200001
DA 2024-07-18
ER

PT J
AU Dai, F
   Shen, YF
   Zhang, YD
   Lin, SX
AF Dai, Feng
   Shen, Yanfei
   Zhang, Yongdong
   Lin, Shouxun
TI Selection of the most efficient tile size in tile-based cylinder
   panoramic video coding and transmission
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE cylinder panoramic video; most efficient tile size
AB Panoramic videos are a 360 degree representation of a certain scene. The users can navigate interactively through the scene and change their view angles. Panoramic videos are often high-resolution and consume a significant amount of bandwidth for transmission. To resolve the problem, tile-based panoramic video coding and transmission is applied in some systems. With tile-based panoramic video coding and transmission, only the tiles involved with the perspective view are transmitted and decoded. Different tile sizes will bring different transmission bit rates for same video quality. In this paper, a two path coding method with H.264/AVC for cylinder panoramic video based on a hyperbolic model is proposed. With this method, the most efficient tile size can be selected and users can build the same quality perspective view with the smallest transmission bit rate.
C1 Chinese Acad Sci, Comp Technol Inst, Beijing 100080, Peoples R China.
   Chinese Acad Sci, Grad Univ, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Dai, F (corresponding author), Chinese Acad Sci, Comp Technol Inst, Beijing 100080, Peoples R China.
EM fdai@ict.ac.cn
CR Chen SN, 1995, SWIMMING THROUGH TROUBLED WATER, P29
   GRUNHEIT C, 2002, IM PROC INT C JUN, V3, P24
   HEYMANN S, 2005, 2 PAN PHOT WORKSH BE
   Neumann U., 2000, MULTIMEDIA 00 P 8 AC, P493
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   YAMAZAWA K, 2003, ISO IEC JTC 1 SC 29
NR 6
TC 0
Z9 0
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 891
EP 896
DI 10.1007/s00371-007-0145-4
PG 6
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206UE
UT WOS:000249207600029
DA 2024-07-18
ER

PT J
AU Banterle, F
   Ledda, P
   Debattista, K
   Chalmers, A
   Bloj, M
AF Banterle, Francesco
   Ledda, Patrick
   Debattista, Kurt
   Chalmers, Alan
   Bloj, Marina
TI A framework for inverse tone mapping
SO VISUAL COMPUTER
LA English
DT Article
DE inverse tone mapping; image enhancement; high dynamic range imaging;
   image editing; image based lighting
AB In recent years many tone mapping operators (TMOs) have been presented in order to display high dynamic range images (HDRI) on typical display devices. TMOs compress the luminance range while trying to maintain contrast. The inverse of tone mapping, inverse tone mapping, expands a low dynamic range image (LDRI) into an HDRI. HDRIs contain a broader range of physical values that can be perceived by the human visual system. We propose a new framework that approximates a solution to this problem. Our framework uses importance sampling of light sources to find the areas considered to be of high luminance and subsequently applies density estimation to generate an expand map in order to extend the range in the high luminance areas using an inverse tone mapping operator. The majority of today's media is stored in the low dynamic range. Inverse tone mapping operators (iTMOs) could thus potentially revive all of this content for use in high dynamic range display and image based lighting (IBL). Moreover, we show another application that benefits quick capture of HDRIs for use in IBL.
C1 Univ Warwick, Warwick Digital Lab, Coventry CV4 7AL, W Midlands, England.
   Univ Bradford, Dept Optometry, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Warwick; University of Bradford
RP Banterle, F (corresponding author), Univ Warwick, Warwick Digital Lab, Coventry CV4 7AL, W Midlands, England.
EM F.banterle@warwick.ac.uk
RI Banterle, Francesco/AAE-5953-2020; Bloj, Marina/F-1081-2010
OI Banterle, Francesco/0000-0002-6374-6657; Bloj,
   Marina/0000-0001-9251-0750
FU EPSRC [EP/D032148/1, EP/D032008/1] Funding Source: UKRI
CR AGARWAL S, 2003, SIGGRAPH 03, P605
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1997, SIGGRAPH, DOI DOI 10.1145/258734.258884
   Bennett EP, 2005, ACM T GRAPHIC, V24, P845, DOI 10.1145/1073204.1073272
   DALY S, 1993, VISIBLE DIFFERENCES
   DEBEVEC PE, 2005, ACM SIGGRAPH 2005
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kang SB, 2003, ACM T GRAPHIC, V22, P319, DOI 10.1145/882262.882270
   Landis H., 2002, ACM SIGGRAPH COURSE, V16
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Mantiuk R, 2004, IEEE SYS MAN CYBERN, P2763
   MEYLAN L, 2006, IS T SID 14 COL IM C
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   PAVICIC MJ, 1990, CONVENINENT ANTIALIA
   PHARR M, 2004, IMPROVED INFINITE AR
   Reinhard E., 2002, Journal of Graphics Tools, V7, P45, DOI 10.1080/10867651.2002.10487554
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Smith K, 2006, COMPUT GRAPH FORUM, V25, P427, DOI 10.1111/j.1467-8659.2006.00962.x
   Ward G., 2005, P 13 COL IM C
NR 22
TC 62
Z9 76
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 467
EP 478
DI 10.1007/s00371-007-0124-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200003
DA 2024-07-18
ER

PT J
AU Mura, G
AF Mura, Gianluca
TI The red and black semantics: a fuzzy language
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW 2006)
CY NOV 28-29, 2006
CL Lausanne, SWITZERLAND
SP EPFL, VRlab
DE virtual reality; visual semantics; fuzzy logic
AB The complexity of abstract art languages create new semantics in art. This model tries to implement its conceptual language within creating a new virtual environment. The gap between art and technology has been approached with a fuzzy logic engine which uses a red and black semantic codification. The examples include the application of this metalanguage to a virtual artwork experimentation.
C1 Politecn Milano Univ, Fac Ind Design, INDACO Dept, Milan, Italy.
C3 Polytechnic University of Milan
RP Mura, G (corresponding author), Politecn Milano Univ, Fac Ind Design, INDACO Dept, Milan, Italy.
EM gianluca.mura@polimi.it
CR BENEDIKT M, 1991, CYBERSPACE 1 STEP
   Carver CS., 1998, On the self-regulation of behavior, pxx, DOI [10.1017/CBO9781139174794, DOI 10.1017/CBO9781139174794]
   Chomsky N., 1959, INFORM CONTR, V2, P137, DOI [10.1016/S0019-9958(59)90362-6, DOI 10.1016/S0019-9958(59)90362-6]
   CORRAIN L., 2004, SEMIOTICHE PITTURA
   Dubois D., 2000, The Handbooks of Fuzzy Sets Series, V1
   GOODMAN N., 2003, LINGUAGGI ARTE
   ITTEN J, 1965, ARTE COLORE
   Kandinsky W, 1926, POINT LIGNE PLAN
   KLIR GJ, 1994, FUZZY SETS FUZZY LOG
   KOSKO B, 1995, FUZZY PENSIERO
   Sapper R, 1959, TEORIA FORMA FIGURAZ
   *VRML CONS, 1997, 1477211997 ISOIEC VR
   *VRML CONS, VIRT REAL MOD LANG
NR 13
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2007
VL 23
IS 5
BP 359
EP 368
DI 10.1007/s00371-007-0111-1
PG 10
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 154JS
UT WOS:000245503600007
DA 2024-07-18
ER

PT J
AU Liu, GD
   McMillan, L
AF Liu, Guodong
   McMillan, Leonard
TI Estimation of missing markers in human motion capture
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE missing markers; motion capture; principle component analysis; piecewise
   linear modeling
AB Motion capture is a prevalent technique for capturing and analyzing human articulations. A common problem encountered in motion capture is that some marker positions are often missing due to occlusions or ambiguities. Most methods for completing missing markers may quickly become ineffective and produce unsatisfactory results when a significant portion of the markers are missing for extended periods of time. We propose a data-driven, piecewise linear modeling approach to missing marker estimation that is especially beneficial in this scenario. We model motion sequences of a training set with a hierarchy of low-dimensional local linear models characterized by the principal components. For a new sequence with missing markers, we use a pre-trained classifier to identify the most appropriate local linear model for each frame and then recover the missing markers by finding the least squares solutions based on the available marker positions and the principal components of the associated model. Our experimental results demonstrate that our method is efficient in recovering the full-body motion and is robust to heterogeneous motion data.
C1 Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Liu, GD (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
EM liug@cs.unc.edu; mcmillan@cs.unc.edu
RI McMillan, Leonard/IQS-6191-2023
OI McMillan, Leonard/0000-0002-8453-0847
CR [Anonymous], 2003, ACM S VIRT REAL SOFT
   Barbic J, 2004, PROC GRAPH INTERF, P185
   BREGLER C, 1995, ADV NEURAL INFORMATI, P43
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   FUKUNAGA K, 1971, IEEE T COMPUT, VC 20, P176, DOI 10.1109/T-C.1971.223208
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   GUO S, 1996, COMPUTER ANIMATION S, P95
   Herda L, 2000, COMP ANIM CONF PROC, P77, DOI 10.1109/CA.2000.889046
   Hinton G.E., 1995, ADV NEURAL INFORM PR, V7, P1015
   Hornung A, 2005, P IEEE VIRT REAL ANN, P75
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Liu Guodong., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P35
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Roweis S, 1998, ADV NEUR IN, V10, P626
   SAFONOVA A, 2004, P SIGGRAPH, P514
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   WELCH G, 1999, S VIRT REAL SOFTW TE, P1
   Wiley DJ, 1997, IEEE COMPUT GRAPH, V17, P39, DOI 10.1109/38.626968
NR 20
TC 61
Z9 74
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 721
EP 728
DI 10.1007/s00371-006-0080-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000015
DA 2024-07-18
ER

PT J
AU Botha, CP
   Post, FH
AF Botha, CP
   Post, FH
TI Improved perspective visibility ordering for object-order volume
   rendering
SO VISUAL COMPUTER
LA English
DT Article
DE volume rendering; visibility ordering; splatting
ID DISPLAY
AB Finding a correct a priori back-to-front (BTF) visibility ordering for the perspective projection of the voxels of a rectangular volume poses interesting problems. The BTF ordering presented by Frieder et al. [6] and the permuted BTF presented by Westover [14] are correct for parallel projection but not for perspective projection [12]. Swan presented a constructive proof for the correctness of the perspective BTF (PBTF) ordering [12]. This was a significant improvement on the existing orderings. However, his proof assumes that voxel projections are not larger than a pixel, i.e. voxel projections do not overlap in screen space. Very often the voxel projections do overlap, e.g. with splatting algorithms. In these cases, the PBTF ordering results in highly visible and characteristic rendering artefacts.
   In this paper we analyse the PBTF and show why it yields these rendering artefacts. We then present an improved visibility ordering that remedies the artefacts. Our new ordering is as good as the PBTF, but it is also valid for cases where voxel projections are larger than a single pixel, i.e. when voxel projections overlap in screen space. We demonstrate why and how our ordering works at fundamental and implementation levels.
C1 Delft Univ Technol, Data Visualisat Grp, NL-2600 AA Delft, Netherlands.
C3 Delft University of Technology
RP Botha, CP (corresponding author), Delft Univ Technol, Data Visualisat Grp, NL-2600 AA Delft, Netherlands.
EM c.p.botha@ewi.tudelft.nl
RI Botha, Charl P./E-9506-2010
CR Anderson D. P., 1982, ACM Transactions on Graphics, V1, P274, DOI 10.1145/357311.357313
   [Anonymous], 1989, P 1989 CHAPEL HILL W, DOI DOI 10.1145/329129.329138
   [Anonymous], P ANN C COMP GRAPH I, DOI DOI 10.1145/54852.378484
   Botha C. P., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P105
   BRODLIE K, 2000, EUROGRAPHICS STATE A, P65
   Coconu L., 2002, PROC EUROGRAPHICS WO, P43
   FRIEDER G, 1985, IEEE COMPUT GRAPH, V5, P52, DOI 10.1109/MCG.1985.276273
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Max NelsonL., 1993, FOCUS SCI VISUALIZAT, P259
   MEISSNER M, 2000, P 2000 IEEE S VOL VI, P81
   Mueller K, 1999, IEEE T VIS COMPUT GR, V5, P116, DOI 10.1109/2945.773804
   Mueller K, 1998, VISUALIZATION '98, PROCEEDINGS, P239, DOI 10.1109/VISUAL.1998.745309
   SWAN JE, 1998, THESIS OHIO STATE U
   WESTOVER L, 1990, P SIGGRAPH 90, P367
   WILLIAMS PL, 1992, ACM T GRAPHIC, V11, P103, DOI 10.1145/130826.130899
NR 15
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 887
EP 896
DI 10.1007/s00371-005-0298-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600001
DA 2024-07-18
ER

PT J
AU Kang, HW
   He, WJ
   Chui, CK
   Chakraborty, UK
AF Kang, HW
   He, WJ
   Chui, CK
   Chakraborty, UK
TI Interactive sketch generation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE non-photorealistic rendering; interactive sketch; wavelet frame;
   multiresolution B-spline; livewire contour tracing
ID IMAGE SEGMENTATION; LIVE WIRE
AB In this paper, we propose an interactive system for generating artistic sketches from images, based on the stylized multiresolution B-spline curve model and the livewire contour tracing paradigm. Our multiresolution B-spline stroke model allows interactive and continuous control of style and shape of the stroke at any level of details. Especially, we introduce a novel mathematical paradigm called the wavelet frame to provide essential properties for multiresolution stroke editing, such as feature point preservation, locality, time-efficiency, good approximation, etc. The livewire stroke map construction leads the user-guided stroke to automatically lock on to the target contour, allowing fast and accurate sketch drawing. We classify the target contours as outlines and interior flow, and develop two respective livewire techniques based on extended graph formulation and vector flow field. Experimental results show that the proposed system facilitates quick and easy generation of artistic sketches of various styles.
C1 Univ Missouri, Dept Math & Comp Sci, St Louis, MO 63121 USA.
C3 University of Missouri System; University of Missouri Saint Louis
RP Univ Missouri, Dept Math & Comp Sci, 1 Univ Blvd, St Louis, MO 63121 USA.
EM kang@arch.umsl.edu; he@arch.umsl.edu; chui@arch.umsl.edu;
   uday@arch.umsl.edu
CR [Anonymous], P SIGGRAPH
   [Anonymous], P COMP GRAPH SIGGRAP, DOI DOI 10.1145/218380.218442
   Chui C. K., 1992, An Introduction to Wavelets, DOI 10.2307/2153134
   Chui CK, 2000, APPL COMPUT HARMON A, V8, P293, DOI 10.1006/acha.2000.0301
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   DAVIS J, 2003, ADOBE PHOTOSHOP 7 WO
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   Falcao AX, 1998, GRAPH MODEL IM PROC, V60, P233, DOI 10.1006/gmip.1998.0475
   Falcao AX, 2000, IEEE T MED IMAGING, V19, P55, DOI 10.1109/42.832960
   Finkelstein A., 1994, P SIGGRAPH, P261, DOI DOI 10.1145/192161.192223
   Gooch B., 2002, P 2 INT S NONPH AN R, P83
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   Hertzmann A., 2002, NPAR, P91, DOI 10.1145/508530.508546
   Kang HW, 2002, GRAPH MODELS, V64, P282, DOI 10.1016/S1077-3169(02)00007-2
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Ostromoukhov V, 1999, COMP GRAPH, P417, DOI 10.1145/311535.311604
   Ron A, 1997, J FUNCT ANAL, V148, P408, DOI 10.1006/jfan.1996.3079
   SALISBURY MP, 1994, P SIGGRAPH 94, P101
   SALISBURY MP, 1997, P SIGGRAPH 97, P401
   Siu Chi Hsu, 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P109
   Sousa MC, 2000, COMPUT GRAPH FORUM, V19, P27, DOI 10.1111/1467-8659.00386
   STEUER S, 2003, ADOBE ILLUSTRATOR CS
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
NR 25
TC 19
Z9 19
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 821
EP 830
DI 10.1007/s00371-005-0328-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400035
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, HW
   Qin, KH
AF Wang, HW
   Qin, KH
TI Surface modeling with ternary interpolating subdivision
SO VISUAL COMPUTER
LA English
DT Article
DE Interpolation; subdivision surface; quadrilateral mesh
ID B-SPLINE SURFACES
AB In this paper, a new interpolatory subdivision scheme, called ternary interpolating subdivision, for quadrilateral meshes with arbitrary topology is presented. It can be used to deal with not only extraordinary faces but also extraordinary vertices in polyhedral meshes of arbitrary topologies. It is shown that the ternary interpolating subdivision can generate a C-1-continuous interpolatory surface. Some applications with open boundaries and curves to be interpolated are also discussed.
C1 Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Wang, HW (corresponding author), Tsing Hua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM whw9418@yahoo.com.cn; qkh-dcs@tsinghua.edu.cn
CR BALL AA, 1988, ACM T GRAPHIC, V7, P83, DOI 10.1145/42458.42459
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Dyn N., 1987, Computer-Aided Geometric Design, V4, P257, DOI 10.1016/0167-8396(87)90001-X
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Hassel AW, 2002, ELECTROCHEM COMMUN, V4, P1, DOI 10.1016/S1388-2481(01)00260-0
   KOBBELT L, 2000, ACM COMPUT GRAPH, V34, P103
   Loop C, 1987, THESIS U UTAH
   REIF U, 1995, COMPUT AIDED GEOM D, V12, P153, DOI 10.1016/0167-8396(94)00007-F
   Zorin D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P189, DOI 10.1145/237170.237254
   [No title captured]
NR 12
TC 6
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 59
EP 70
DI 10.1007/s00371-004-0270-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300004
DA 2024-07-18
ER

PT J
AU Ota, S
   Tamura, M
   Fujimoto, T
   Muraoka, K
   Chiba, N
AF Ota, S
   Tamura, M
   Fujimoto, T
   Muraoka, K
   Chiba, N
TI A hybrid method for real-time animation of trees swaying in wind fields
SO VISUAL COMPUTER
LA English
DT Article
DE tree; motion; wind; 1/f(beta) noise; real-time animation
AB Trees are one of the most important elements of natural landscapes. Therefore, in computer graphics, there is a great demand for methods to realize the natural representation of trees in virtual landscapes in various fields such as the entertainment industry or environmental assessment in construction. Many studies have been made on techniques in which the shapes of trees are modeled but only a few studies have been reported on methods to incorporate the shapes with motions in a wind field. Most of these studies use physical simulation techniques based on the equations of motion to generate the branch motions and cannot realize the motions of individual leaves. In this paper, we propose a method to create the natural motions of individual leaves and branches swaying in a wind field. The proposed method uses a hybrid approach combining a stochastic method and a simulation method. The stochastic method is based on 1/f(beta) stop noise, which is observed in various natural phenomena, and provides natural motion to leaves and branches. In addition, a simple simulation method based on the spring model is applied to branches to enhance the reality of their motions. This method enables the real-time creation of the leaf and branch motions. Diverse motions according to tree species and shapes and wind conditions can be easily realized by controlling the parameters.
EM shin@isop.ne.jp; mati@isop.ne.jp; fujimoto@cis.iwate-u.ac.jp;
   muraoka@tohtech.ac.jp; nchiba@cis.iwate-u.ac.jp
CR [Anonymous], 1994, TEXTURING MODELING P
   AONO M, 1984, IEEE COMPUT GRAPH, V4, P10, DOI 10.1109/MCG.1984.276141
   Chiba N., 1993, Journal of the Institute of Image Electronics Engineers of Japan, V22, P475
   Di Giacomo T, 2001, SPRING EUROGRAP, P65
   Kanayama C., 1997, Transactions of the Institute of Electronics, Information and Communication Engineers D-II, VJ80D-II, P1843
   Mech Radomir., 1996, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. SIGGRAPH'96, P397, DOI [10.1145/237170.237279, DOI 10.1145/237170.237279]
   MUSHA T, 1994, IDEA MOTIONS REVEALI
   Peitgen H.-O., 1988, SCI FRACTAL IMAGE
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   REFFYE P, 1988, P SIGGRAPH 88, P151
   SAKAGUCHI T, 1999, S VIRT REAL SOFTW TE, P139
   SHINYA M, 1992, EUROGRAPHICS 92, pC119
   STAM J, 1997, EUROGRAPHICS 9M, P159
NR 13
TC 30
Z9 49
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 613
EP 623
DI 10.1007/s00371-004-0266-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000001
DA 2024-07-18
ER

PT J
AU Çiftçi, UA
   Demir, I
   Yin, LJ
AF Ciftci, Umur Aybars
   Demir, Ilke
   Yin, Lijun
TI Deepfake source detection in a heart beat
SO VISUAL COMPUTER
LA English
DT Article
DE Deepfakes; Detection; Source detection; Generative models; Biological
   detector; PPG; GAN fingerprint
ID NETWORKS
AB Fake portrait video generation techniques are posing a new threat to society as photorealistic deepfakes are being used for political propaganda, celebrity imitation, forged pieces of evidences, and other identity-related manipulations. Despite these generation techniques, some detection approaches have also been proven useful due to their high classification accuracy. Nevertheless, almost no effort has been spent tracking down the source of deepfakes. We propose an approach not only to separate deepfakes from real videos, but also to discover the specific generative model behind deepfakes. Some pure deep learning-based approaches try to classify deepfakes using CNNs which actually learn the residuals of the generator. Our key observation is that the spatiotemporal patterns in biological signals can be conceived as a representative projection of the residuals. To justify this observation, we extract PPG cells from real and fake videos and feed these to a state-of-the-art classification network, with an attempt to detect which generative model was used to create a certain fake video. Our results indicate that our approach can detect fake videos with 97.29% accuracy and the source model with 93.39% accuracy. We further evaluate and compare our approach on six datasets to assess its expansibility with new models and generalizability across skin tones and genders, run ablation studies for various components, and analyze its robustness toward compression, landmark noise, and postprocessing operations. The experiments show the superior performance of our proposed approach as compared to the state of the art.
C1 [Ciftci, Umur Aybars; Yin, Lijun] Binghamton Univ, Comp Sci Dept, Binghamton, NY USA.
   [Demir, Ilke] Intel Corp, Intel Labs, Santa Clara, CA 95054 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; Intel Corporation
RP Demir, I (corresponding author), Intel Corp, Intel Labs, Santa Clara, CA 95054 USA.
EM uciftci@binghamton.edu; ilke.demir@intel.com; lijun@cs.binghamton.edu
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Albright M., 2019, IEEE C COMP VIS PATT
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   amt, DEEPFAKE TECHNOLOGY
   [Anonymous], 2018, 000 FACES GENERATED
   Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Asnani V., 2021, ARXIV
   Baltrusaitis Tadas, 2018, 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), P59, DOI 10.1109/FG.2018.00019
   Barni M, 2017, J VIS COMMUN IMAGE R, V49, P153, DOI 10.1016/j.jvcir.2017.09.003
   Bellemare M. G., 2017, arXiv
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Bradski G, 2000, DR DOBBS J, V25, P120
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen ML, 2022, IEEE T INF FOREN SEC, V17, P457, DOI 10.1109/TIFS.2022.3142993
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chollet F, 2015, KERAS
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chu D., 2020, WHITE PAPER DEEP FAK
   Ciftci U.A., 2019, ISVC
   Ciftci UA, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   cnn, LAWMAKERS WARN DEEPF
   Conotter V, 2014, IEEE IMAGE PROC, P248, DOI 10.1109/ICIP.2014.7025049
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   DeepFakes, about us
   Demir I., 2021, ARXIV
   Demir I, 2021, PROCEEDINGS ETRA 2021: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3448017.3457387
   Dolhansky B, 2019, ARXIV
   Dolhansky B., 2020, arXiv
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dufour Nicholas, 2019, DeepFakes detection dataset by Google & JigSaw
   elle, DEEPFAKE PORN NEARLY
   FaceSwap, ABOUT US
   FakeApp, US
   Farid H, 2019, MIT PRESS ESSENT, P1, DOI 10.7551/mitpress/11736.001.0001
   Fortune, 1997, HDB DISCRETE COMPUTA, P377
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   github, Faceswap-GAN
   github, VIT PYTORCH
   gritdaily, ARE DEEPFAKES FUTURE
   Guarnera L, 2020, IEEE COMPUT SOC CONF, P2841, DOI 10.1109/CVPRW50498.2020.00341
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hsiao C., 1996, Logit and probit models. The Econometrics of Panel Data
   Huang G, 2016, arXiv
   Huang Y., 2020, P 28 ACM INT C MULT
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Jia Y, 2019, ARXIV
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Khalid H., 2021, arXiv
   Khodabakhsh A, 2018, 2018 INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP (BIOSIG)
   Korshunov P., 2019, ICML WORKSH SYNTH RE
   Korshunov P, 2018, EUR SIGNAL PR CONF, P2375, DOI 10.23919/EUSIPCO.2018.8553270
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Le N, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P202, DOI 10.1145/2964284.2967211
   Li HD, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107616
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li Y., 2018, 2018 IEEE INT WORKSH, P1, DOI DOI 10.1109/WIFS.2018.8630787
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu JR, 2021, INT J INTELL SYST, V36, P4990, DOI 10.1002/int.22499
   Trinh L, 2021, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV48630.2021.00202
   Lu CL, 2021, IEEE IMAGE PROC, P3572, DOI 10.1109/ICIP42928.2021.9506381
   Lu YF, 2019, IEEE IC COMP COM NET, DOI [10.1109/icccn.2019.8847083, 10.23919/apnoms.2019.8893042]
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Marra F, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035099
   Marra F, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P506, DOI 10.1109/MIPR.2019.00103
   Masi I., 2020, ARXIV
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   McCloskey S, 2019, IEEE IMAGE PROC, P4584, DOI [10.1109/icip.2019.8803661, 10.1109/ICIP.2019.8803661]
   McDuff DJ, 2015, IEEE ENG MED BIO, P6398, DOI 10.1109/EMBC.2015.7319857
   Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780
   Miyato T., 2018, ARXIV
   Mutegeki Ronald, 2020, 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), P362, DOI 10.1109/ICAIIC48513.2020.9065078
   Nadeem MS, 2019, IEEE ACCESS, V7, P84003, DOI 10.1109/ACCESS.2019.2924733
   Nawaz M, 2023, VISUAL COMPUT, V39, P6323, DOI 10.1007/s00371-022-02732-7
   Neves JC, 2020, IEEE J-STSP, V14, P1038, DOI 10.1109/JSTSP.2020.3007250
   newscientist, DEEPFAKES ARE BEING
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Pan D, 2020, 2020 IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING, APPLICATIONS AND TECHNOLOGIES (BDCAT 2020), P134, DOI 10.1109/BDCAT50828.2020.00001
   Perov Ivan, 2021, ARXIV
   popularmechanics, HERES HARRISON FORD
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pu J., 2021, ARXIV
   Qi H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4318, DOI 10.1145/3394171.3413707
   Rana MS, 2020, 2020 7TH IEEE INTERNATIONAL CONFERENCE ON CYBER SECURITY AND CLOUD COMPUTING (CSCLOUD 2020)/2020 6TH IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING AND SCALABLE CLOUD (EDGECOM 2020), P70, DOI 10.1109/CSCloud-EdgeCom49738.2020.00021
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rossler A., 2018, arXiv
   Roy A., 2018, IEEE C COMP VIS PATT
   Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21
   Saremsky S.R., 2022, IEEE CVF C COMP VIS
   Sebyakin A, 2021, ICONFERENCE LECT NOT
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Straub Jeremy., 2019, Real-Time Image Processing and Deep Learning 2019, V10996, DOI DOI 10.1117/12.2520546
   Sun YJ, 2021, VISUAL COMPUT, V37, P1015, DOI 10.1007/s00371-020-01849-x
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tariq S, 2018, MPS'18: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA PRIVACY AND SECURITY, P81, DOI 10.1145/3267357.3267367
   Thies J., 2016, P COMPUTER VISION PA
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Tolosana Ruben, 2020, Information Fusion, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Wang S.-Y., 2020, CVPR, P8695
   Wang SY, 2019, IEEE I CONF COMP VIS, P10071, DOI 10.1109/ICCV.2019.01017
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P2781, DOI 10.1109/TBME.2017.2676160
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Yadav D, 2019, PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICCS), P852, DOI [10.1109/iccs45141.2019.9065881, 10.1109/ICCS45141.2019.9065881]
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P15, DOI 10.1109/SIPROCESS.2017.8124497
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229
NR 120
TC 0
Z9 0
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2733
EP 2750
DI 10.1007/s00371-023-02981-0
EA OCT 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001083752500001
DA 2024-07-18
ER

PT J
AU Lewis, DA
   Chatoux, H
   Mansouri, A
AF Lewis, David A. A.
   Chatoux, Hermine
   Mansouri, Alamin
TI SFF-RTI: an active multi-light approach to shape from focus
SO VISUAL COMPUTER
LA English
DT Article
DE Shape from focus; Multi-light imaging; Full vector gradient
AB In this paper, we propose a methodology for the fusion of shape from focus and reflectance transformation imaging. This fusion of two seemingly disparate methods of computational imaging is proposed with the purpose of leveraging their strengths in understanding overall surface structure (low-frequency detail) and surface texture/micro-geometry (high-frequency detail), respectively. This fusion is achieved by our new proposal of the integration of varying light images at different focus distances. We compare three methods of integration: the mean gradient response, the maximum gradient response, and the full vector gradient (FVG). The validation of the tested methods was conducted using different focus measure window sizes and multi-light integration methods to provide a clear demonstration of the effectiveness of the proposed method. The FVG is determined to provide a higher-quality shape recovery of a complex object with the trade-off of increasing the scope of the image acquisition.
C1 [Lewis, David A. A.; Chatoux, Hermine; Mansouri, Alamin] Univ Burgundy, Dijon, France.
C3 Universite de Bourgogne
RP Lewis, DA (corresponding author), Univ Burgundy, Dijon, France.
EM david_lewis@etu.u-bourgogne.fr
OI Lewis, David/0000-0003-3277-6260
FU European Union [813789]
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under the Marie Sklodowska-Curie Grant
   Agreement No. 813789.
CR [Anonymous], 2021, MUZEUM PALACU KROLA
   [Anonymous], 2021, Blender
   [Anonymous], 2021, ARCHEOMATIQUE STATUE
   Asif M, 2001, IEEE T IMAGE PROCESS, V10, P1670, DOI 10.1109/83.967395
   Bueno-Ibarra MA, 2005, OPT ENG, V44, DOI 10.1117/1.1925119
   Chatoux H, 2019, J OPT SOC AM A, V36, pC154, DOI 10.1364/JOSAA.36.00C154
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Gautron Pascal., 2004, EUROGRAPHICS SYMPOSI, P321
   Kautsky J, 2002, PATTERN RECOGN LETT, V23, P1785, DOI 10.1016/S0167-8655(02)00152-6
   Kim HJ, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8091648
   Li ST, 2008, PATTERN RECOGN LETT, V29, P1295, DOI 10.1016/j.patrec.2008.02.002
   Malzbender T, 2001, COMP GRAPH, P519, DOI 10.1145/383259.383320
   Mutahira H, 2021, MICROSC RES TECHNIQ, V84, P656, DOI 10.1002/jemt.23623
   NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479
   Pitard G, 2017, MACH VISION APPL, V28, P607, DOI 10.1007/s00138-017-0856-0
   Raskar R, 2004, ACM T GRAPHIC, V23, P679, DOI 10.1145/1015706.1015779
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
NR 17
TC 0
Z9 0
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2067
EP 2079
DI 10.1007/s00371-023-02902-1
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001039828800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, ZY
   Wang, HJ
   Liu, Y
AF Li, Zhongyu
   Wang, Huajun
   Liu, Yang
TI Semantic segmentation of remote sensing image based on bilateral branch
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Feature extraction; Semantic segmentation; Deep neural network;
   Imbalance; Integrated learning
AB Due to the large intra-class differences between the same categories and the scale imbalance between different categories in the remote sensing image dataset, the semantic segmentation task presents the problem of small-scale object information loss, the imbalance between foreground and background, and simultaneously the background dominates, which seriously affects the performance of the network model. To solve the above problems, this paper proposes an efficient bilateral branch depth neural network model based on the U-Net depth neural network, named BBU-Net. Firstly, one branch of the network learns the distribution characteristics of the original data, and the other focuses on difficult samples. Then the two branches improve the representation and classification ability of the neural network by accumulating learning strategies. Finally, considering the geometric diversity of remote sensing images, this paper adopts test time augmentation and reflection padding strategies and proposes a balanced weighted loss function named CombineLoss to alleviate the imbalance in the training process. The depth neural network proposed in this paper was first tested on the Inria Aerial Image Labeling Dataset, and 87.53% of mean intersection over union and 97.4% of mean pixel accuracy were obtained, respectively. At the same time, to verify the model's complexity, the model proposed in this paper is compared with the neural network based on integrated learning. The comparison results show that the spatial complexity of the network proposed in this paper is much lower than the neural network obtained by integrated learning, and the parameters are also much smaller than the neural network based on integrated learning. Then use the satellite building dataset I in the WHU Building Dataset and mainstream semantic segmentation methods for multiple groups of comparative experiments. The experimental results show that the method proposed in this paper can effectively extract the semantic information of remote sensing images, significantly improve the imbalance of remote sensing image data, improve the performance of the network model, and achieve a good semantic segmentation effect, which fully proves the effectiveness of this method.
C1 [Li, Zhongyu; Wang, Huajun] Chengdu Univ Technol, Coll Geophys, Chengdu 610059, Peoples R China.
   [Li, Zhongyu; Liu, Yang] Chengdu Normal Univ, Coll Comp Sci, Chengdu 611130, Peoples R China.
   [Li, Zhongyu] Chengdu Normal Univ, Inst Higher Educ Sichuan Prov, Key Lab Interior Layout Optimizat & Secur, Chengdu 611130, Peoples R China.
C3 Chengdu University of Technology; Chengdu Normal University; Chengdu
   Normal University
RP Li, ZY (corresponding author), Chengdu Univ Technol, Coll Geophys, Chengdu 610059, Peoples R China.; Li, ZY (corresponding author), Chengdu Normal Univ, Coll Comp Sci, Chengdu 611130, Peoples R China.; Li, ZY (corresponding author), Chengdu Normal Univ, Inst Higher Educ Sichuan Prov, Key Lab Interior Layout Optimizat & Secur, Chengdu 611130, Peoples R China.
EM zyli@cdnu.edu.cn
RI ren, jun/KHG-7717-2024; Li, Zhongyu/HDO-2794-2022; zhu,
   hao/KHW-3813-2024; li, cheng/KCZ-0615-2024; su, lin/KHC-5034-2024; guo,
   yi/KHC-4669-2024
OI Liu, Yang/0009-0004-9364-8232
FU Artificial Intelligence Key Laboratory of Sichuan Province [2020RYJ02];
   Project of Key Laboratory of Pattern Recognition and Intelligent
   Information Processing of Sichuan [MSSB-2020-10]; Project of Key
   Research and Development Program of Sichuan Department of Science and
   Technology in 2022 [2022YFG0190]; Project of Information Materials and
   Devices Application Sichuan Key Laboratory [2022XXCL007]; Innovation
   Team of Chengdu Normal University Grant [CSCXTD2020B09]
FX This work was jointly funded by the project of Artificial Intelligence
   Key Laboratory of Sichuan Province (No. 2020RYJ02), Project of Key
   Laboratory of Pattern Recognition and Intelligent Information Processing
   of Sichuan (No. MSSB-2020-10), Project of Key Research and Development
   Program of Sichuan Department of Science and Technology in
   2022(2022YFG0190), and Project of Information Materials and Devices
   Application Sichuan Key Laboratory (2022XXCL007) and supported by the
   Innovation Team of Chengdu Normal University Grant (No. CSCXTD2020B09).
CR Abdollahi A, 2022, GEOCARTO INT, V37, P3355, DOI 10.1080/10106049.2020.1856199
   Agrawal T, 2023, VISUAL COMPUT, V39, P875, DOI 10.1007/s00371-021-02352-7
   Alom MZ, 2018, arXiv
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen XH, 2022, AAAI CONF ARTIF INTE, P356
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Cui XN, 2021, ADV STRUCT ENG, V24, P1859, DOI 10.1177/1369433220986638
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gowda SN, 2019, LECT NOTES COMPUT SC, V11364, P581, DOI 10.1007/978-3-030-20870-7_36
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hassan T, 2020, IEEE INT C BIOINF BI, P577, DOI 10.1109/BIBE50027.2020.00099
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ji SP, 2019, IEEE T GEOSCI REMOTE, V57, P574, DOI 10.1109/TGRS.2018.2858817
   Jia F, 2021, ANAL APPL, V19, P147, DOI 10.1142/S0219530519410148
   Li X., 2019, ARXIV
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maggiori E, 2017, INT GEOSCI REMOTE SE, P3226, DOI 10.1109/IGARSS.2017.8127684
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Nayem A.B.S., 2020, ARXIV
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saxena N, 2020, IEEE INT C SEMANT CO, P154, DOI 10.1109/ICSC.2020.00030
   Tian L, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/9491376
   Wang B., 2019, B SURV MAPP, V503, P108
   Wang ED., 2019, ACTA OPT SINICA, V39, P93
   Wang GT, 2019, LECT NOTES COMPUT SC, V11384, P61, DOI 10.1007/978-3-030-11726-9_6
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xie HB, 2021, J INDIAN SOC REMOTE, V49, P1257, DOI 10.1007/s12524-021-01312-x
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zheng Z, 2020, PROC CVPR IEEE, P4095, DOI 10.1109/CVPR42600.2020.00415
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   [朱节中 Zhu Jiezhong], 2021, [计算机应用研究, Application Research of Computers], V38, P3460
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 38
TC 1
Z9 1
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3069
EP 3090
DI 10.1007/s00371-023-03011-9
EA JUL 2023
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001037161700003
DA 2024-07-18
ER

PT J
AU Al-Jebrni, AH
   Ali, SG
   Li, HT
   Lin, X
   Li, P
   Jung, YHY
   Kim, J
   Feng, DD
   Sheng, B
   Jiang, LX
   Du, J
AF Al-Jebrni, Abdulrhman H.
   Ali, Saba Ghazanfar
   Li, Huating
   Lin, Xiao
   Li, Ping
   Jung, Younhyun
   Kim, Jinman
   Feng, David Dagan
   Sheng, Bin
   Jiang, Lixin
   Du, Jing
TI SThy-Net: a feature fusion-enhanced dense-branched modules network for
   small thyroid nodule classification from ultrasound images
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Papillary thyroid microcarcinoma; Small thyroid nodules;
   Image classification; Ultrasound imaging
ID DIAGNOSIS
AB Deep learning studies of thyroid nodule classification from ultrasound (US) images have focused mainly on nodules with diameters > 1 cm. However, small thyroid nodules measuring = 1 cm, especially nodules with high-risk stratification, are prevalent in the population but without enough focus, including papillary thyroid microcarcinoma (PTMC) as their common malignant type. Additionally, small nodules with high-risk stratification are difficult for physicians to diagnose from US images due to their atypical features. In this work, we propose a small thyroid nodule classification network (SThy-Net) to classify benign and PTMC small thyroid nodules with high-risk stratification from US images. We design two main components, a dense-branched module and a Gaussian-enhanced feature fusion module, to help recognize small thyroid nodules. To our knowledge, this work is the first to address the challenging task of classifying small thyroid nodules using US images. Our SThy-Net achieves as high accuracy as 87.4% compared to five state-of-the-art thyroid nodule diagnosis studies, several state-of-the-art deep learning models, and three radiologists. From visual explainability, our network shows an intuitive feature extraction method and consistency with US image analysis of radiologists. The results suggest that our network has the potential to be an affordable tool for radiologists to diagnose small nodules with high-risk stratification in clinical practice.
C1 [Al-Jebrni, Abdulrhman H.; Ali, Saba Ghazanfar; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Li, Huating] Shanghai Jiao Tong Univ Affiliated Peoples Hosp 6, Shanghai 200233, Peoples R China.
   [Lin, Xiao] Shanghai Normal Univ, Coll Informat Mech & Elect Engn, Shanghai 200234, Peoples R China.
   [Lin, Xiao] Shanghai Engn Res Ctr Intelligent Educ & Bigdata, Shanghai 200240, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Jung, Younhyun] Gachon Univ, Sch Comp, Seongnam 13120, South Korea.
   [Kim, Jinman; Feng, David Dagan] Univ Sydney, Sch Comp Sci, Biomed & Multimedia Informat Technol Res Grp, Sydney, NSW 2006, Australia.
   [Jiang, Lixin; Du, Jing] Shanghai Jiao Tong Univ, Renji Hosp, Sch Med, Dept Ultrasound, Shanghai 200127, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai
   Normal University; Hong Kong Polytechnic University; Gachon University;
   University of Sydney; Shanghai Jiao Tong University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Jung, YHY (corresponding author), Gachon Univ, Sch Comp, Seongnam 13120, South Korea.; Jiang, LX; Du, J (corresponding author), Shanghai Jiao Tong Univ, Renji Hosp, Sch Med, Dept Ultrasound, Shanghai 200127, Peoples R China.
EM younhyun.jung@gachon.ac.kr; shengbin@sjtu.edu.cn; jinger_28@sina.com;
   beautydujing@163.com
OI Al-Jebrni, ABDULRHMAN/0000-0001-7277-7095
FU National Natural Science Foundation of China [82071954, 81102014,
   82100879]; Joint Research Project of Health and Family Planning of the
   Shanghai Pudong New Area Health Committee [PW2019D-6]; Shanghai Pujiang
   Program [2020PJD044]; Korea Health Technology R amp;D Project through
   the Korea Health Industry Development Institute [HI22C1651]
FX AcknowledgementsThis work was supported in part by the National Natural
   Science Foundation of China (82071954, 81102014, 82100879), the Joint
   Research Project of Health and Family Planning of the Shanghai Pudong
   New Area Health Committee (PW2019D-6), the Shanghai Pujiang Program
   (2020PJD044), and the Korea Health Technology R &D Project through the
   Korea Health Industry Development Institute (grant number: HI22C1651)
CR Abdolali F., 2020, FRONT BIOMED TECHNOL, V7, P266, DOI DOI 10.18502/fbt.v7i4.5324
   Avola D, 2022, IEEE T CIRC SYST VID, V32, P2527, DOI 10.1109/TCSVT.2021.3074414
   Baran JM, 2009, IEEE ENG MED BIO, P792, DOI 10.1109/IEMBS.2009.5332754
   Chai YJ., 2020, Ann Thyroid, V5, P1, DOI [10.21037/aot.2020.04.01, DOI 10.21037/AOT.2020.04.01, 10.21037/AOT.2020.04.01]
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy Alexey, 2021, ICLR
   Du J, 2022, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.746776
   Gharib Hossein, 2006, Endocr Pract, V12, P63
   Guan Q, 2019, ANN TRANSL MED, V7, DOI 10.21037/atm.2019.04.34
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holzinger A., 2017, ARXIV171209923V1, Vabs/1712.09923, P1, DOI DOI 10.48550/ARXIV.1712.09923
   Hou YQ, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.614172
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iscan Y, 2019, EUR THYROID J, V8, P256, DOI 10.1159/000501613
   Kaliszewski K, 2019, WORLD J SURG ONCOL, V17, DOI 10.1186/s12957-019-1638-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon SW, 2020, J DIGIT IMAGING, V33, P1202, DOI 10.1007/s10278-020-00362-w
   Li XC, 2019, LANCET ONCOL, V20, P193, DOI 10.1016/S1470-2045(18)30762-9
   Liu TJ, 2017, INT CONF ACOUST SPEE, P919, DOI 10.1109/ICASSP.2017.7952290
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mendes GF, 2018, BRIT J RADIOL, V91, DOI 10.1259/bjr.20170642
   Nabahati M, 2022, EGYPT J RADIOL NUC M, V53, DOI 10.1186/s43055-022-00802-3
   Park VY, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54434-1
   Qin PL, 2020, IEEE J BIOMED HEALTH, V24, P1028, DOI 10.1109/JBHI.2019.2950994
   Redmon Joseph, 2013, Darknet: Open Source Neural Networks in C, DOI DOI 10.1109/CVPR.2016.91
   Sakorafas GH, 2010, ONKOLOGIE, V33, P61, DOI 10.1159/000264624
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J, 2019, MEDICINE, V98, DOI 10.1097/MD.0000000000015133
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tessler FN, 2017, J AM COLL RADIOL, V14, P587, DOI 10.1016/j.jacr.2017.01.046
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Z, 2021, CANCER MED-US, V10, P1614, DOI 10.1002/cam4.3743
   Yi KH, 2016, ENDOCRINOL METAB, V31, P373, DOI 10.3803/EnM.2016.31.3.373
   Yu C., 2018, EUR C COMP VIS, P34
   Zhou H, 2020, IEEE T BIO-MED ENG, V67, P2773, DOI 10.1109/TBME.2020.2971065
NR 39
TC 7
Z9 7
U1 15
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3675
EP 3689
DI 10.1007/s00371-023-02984-x
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035720700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YK
   Luo, YT
   Liu, YH
   Zhou, B
   Liu, XP
AF Zhang, Yankong
   Luo, Yuetong
   Liu, Yuhua
   Zhou, Bo
   Liu, Xiaoping
TI CCET: towards customized explanation of clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Interpretable clustering; Customized explanation; Cluster explanation
   tree; Visual analytics
AB Classical clustering algorithms use all features to partition a dataset, making it difficult for users to understand the clustering results. Some scholars have proposed interpretable clustering algorithms that use a few understandable features to explain clustering results. However, the existing algorithms can only generate one interpretation and fail to satisfy the diverse needs of different users. To address this challenge, the Clustering Customized Explanation Tree (CCET), a visual analytics system, was constructed in this paper. The system helps users modify existing explanations to obtain customized explanations. Firstly, a variety of views are designed to visualize the explanations and help users judge whether the existing explanations meet the requirements. Then, an explanations modification strategy based on cluster centroids splitting is proposed making it easy for users to revise explanations according to the requirement. We demonstrate the CCET using a case study and a user study. The results show that the system can deepen users' understanding of clustering results and make it easy for them to conduct further decision analysis.
C1 [Zhang, Yankong; Luo, Yuetong; Liu, Yuhua; Zhou, Bo; Liu, Xiaoping] Hefei Univ Technol, Hefei, Anhui, Peoples R China.
C3 Hefei University of Technology
RP Zhang, YK (corresponding author), Hefei Univ Technol, Hefei, Anhui, Peoples R China.
EM zhangyankong@hfut.edu.cn
FU Hefei University of Technology Academic Newcomer Enhancement A Program
   [JZ2021HGTA0140]; National Natural Science Foundation of China
   [62277014]; National Key Research and Development Program of China
   [2020YFC1523100]
FX AcknowledgementsThis work was supported by the Hefei University of
   Technology Academic Newcomer Enhancement A Program (JZ2021HGTA0140), the
   National Natural Science Foundation of China (62277014) and the National
   Key Research and Development Program of China (2020YFC1523100).
CR Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   [Anonymous], 2011, KDD
   Basak J, 2005, IEEE T KNOWL DATA EN, V17, P121, DOI 10.1109/TKDE.2005.11
   Bertsimas D, 2021, MACH LEARN, V110, P89, DOI 10.1007/s10994-020-05896-2
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Blockeel H., 2000, arXiv
   Chen JX, 2016, IEEE DATA MINING, P823, DOI [10.1109/ICDM.2016.166, 10.1109/ICDM.2016.0097]
   CLANCEY WJ, 1983, ARTIF INTELL, V20, P215, DOI 10.1016/0004-3702(83)90008-5
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fraiman R, 2013, ADV DATA ANAL CLASSI, V7, P125, DOI 10.1007/s11634-013-0129-3
   Henin Clement, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P5, DOI 10.1007/978-3-030-68796-0_1
   Kohavi R, 1995, LECT NOTES ARTIF INT, V912, P174
   Likas A, 2003, PATTERN RECOGN, V36, P451, DOI 10.1016/S0031-3203(02)00060-2
   Liu B, 2000, P 9 INT C INF KNOWL, P20, DOI DOI 10.1145/354756.354775
   Loyola-González O, 2020, IEEE ACCESS, V8, P52370, DOI 10.1109/ACCESS.2020.2980581
   Madhulatha TS, 2012, ARXIV
   Ming Y, 2019, IEEE T VIS COMPUT GR, V25, P342, DOI 10.1109/TVCG.2018.2864812
   Moshkovitz Michal, 2020, PMLR, P7055
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Rezaee MR, 1998, PATTERN RECOGN LETT, V19, P237, DOI 10.1016/S0167-8655(97)00168-2
   Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095
   Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1007/BF00058680
   Schulz HJ, 2011, IEEE COMPUT GRAPH, V31, P11, DOI 10.1109/MCG.2011.103
   Sokol K, 2020, KUNSTL INTELL, V34, P235, DOI 10.1007/s13218-020-00637-y
   van den Elzen S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P151, DOI 10.1109/VAST.2011.6102453
   Yang WK, 2021, IEEE T VIS COMPUT GR, V27, P3953, DOI 10.1109/TVCG.2020.2995100
   Zhao YG, 2016, SOIL TILL RES, V155, P363, DOI 10.1016/j.still.2015.08.019
NR 27
TC 0
Z9 0
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3169
EP 3181
DI 10.1007/s00371-023-02958-z
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035766800005
OA Bronze
DA 2024-07-18
ER

PT J
AU Diaz-Arias, A
   Shin, D
AF Diaz-Arias, Alec
   Shin, Dmitriy
TI ConvFormer: parameter reduction in transformer models for 3D human pose
   estimation by leveraging dynamic multi-headed convolutional attention
SO VISUAL COMPUTER
LA English
DT Article
DE Transformers; 3D human pose estimation; Dynamic convolutions; Monocular
   motion capture
AB Recently, fully-transformer architectures have replaced the defacto convolutional architecture for the 3D human pose estimation task. In this paper, we propose ConvFormer, a novel convolutional transformer that leverages a new dynamic multi-headed convolutional self-attention mechanism for monocular 3D human pose estimation. We designed a spatial and temporal convolutional transformer to comprehensively model human joint relations within individual frames and globally across the motion sequence. Moreover, we introduce a novel notion of temporal joints profile for our temporal ConvFormer that fuses complete temporal information immediately for a local neighborhood of joint features. We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6 M, MPI-INF-3DHP, and HumanEva. Extensive experiments have been conducted to identify the optimal hyper-parameter set. These experiments demonstrated that we achieved a significant parameter reduction relative to prior transformer models while attaining State-of-the-Art (SOTA) or near SOTA on all three datasets. Additionally, we achieved SOTA for Protocol III on H36M for both GT and CPN detection inputs. Finally, we obtained SOTA on all three metrics for the MPI-INF-3DHP dataset and for all three subjects on HumanEva under Protocol II.
C1 [Diaz-Arias, Alec; Shin, Dmitriy] Inseer, 2500 Cross Pk Rd, Coralville, IA 52241 USA.
RP Diaz-Arias, A (corresponding author), Inseer, 2500 Cross Pk Rd, Coralville, IA 52241 USA.
EM alec.diaz-arias@inseer.com; dmitriy.shin@inseer.com
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Banik S, 2021, IEEE IMAGE PROC, P924, DOI 10.1109/ICIP42928.2021.9506736
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Carion Nicolas, 2020, EUR C COMP VIS ECCV, DOI DOI 10.1007/978-3-030-58452-8_13
   Chaitanya K., 2021, ARXIV
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Diaz-Arias A., 2021, ARXIV
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   He Y., 2020, IEEECVF C COMPUTER V, P7776
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kitaev Nikita, 2020, INT C LEARN REPR
   Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117
   Li S, 2020, ARXIV
   Li W., 2022, P IEEE C COMPUTER VI
   Li W., 2022, IEEE TRANSAACTIONS M
   Liaw Richard, 2018, arXiv
   Lin J., 2019, BRIT MACH VIS C BMVC
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu JF, 2021, IEEE INT CONF ROBOT, P3374, DOI 10.1109/ICRA48506.2021.9561605
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Liu Z, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00708-7
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Michel P, 2019, ADV NEUR IN, V32
   Paaschalis P., 2021, ARXIV
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sebastian J., 2021, SPARSE IS ENOUGH SCA
   Shuai H., 2021, ARXIV
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun X, 2017, ARXIV
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang J, 2020, AAAI CONF ARTIF INTE, V34, P9169
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Wu Z, 2020, INT C LEARNING REPRE
   Yeh R., 2019, INT C NEUR INF PROC
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou K, 2021, INT J ADV MANUF TECH, V115, P1005, DOI 10.1007/s00170-021-07253-6
   Zhou K, 2019, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2019.00243
NR 52
TC 2
Z9 2
U1 7
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2555
EP 2569
DI 10.1007/s00371-023-02936-5
EA JUL 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001020426500001
DA 2024-07-18
ER

PT J
AU Li, F
   Xiao, K
   Hu, ZP
   Zhang, GZ
AF Li, Feng
   Xiao, Kang
   Hu, Zhengpeng
   Zhang, Guozheng
TI Fabric defect detection algorithm based on improved YOLOv5
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Fabric defect detection; YOLOv5; SIoU loss; GHM loss
AB Fabric defect detection is an important part of the textile industry, aiming at the problems of many types of fabric defects, small size defects and unbalanced samples, an improved YOLOv5 fabric defect detection algorithm, FD-YOLOv5, was proposed. First, the coordinate attention module is embedded in the YOLOv5 backbone network structure to replace the bottleneck structure in the original network model. While reducing the amount of parameters and calculation, it enhances the ability of the network to extract features and improves the model's ability to detect small target defects. Secondly, a smoother Mish activation function is used in the original model convolution structure for model training, which improves the nonlinear expression ability of the model; the SIoU loss function considering the direction of the anchor box is used to improve the convergence speed and detection accuracy of the model. Finally, combining the focal loss and GHM loss functions as the target confidence loss function to solve the problem of sample imbalance in the fabric defect dataset. The experimental results based on the public fabric defect dataset of Aliyun TianChi shows that the mAP@.5 and mAP@.5:.95 of the improved algorithm are 65.1% and 30.4%, respectively, which are 8.3% and 3.2% higher than the original model, respectively, and the parameter amount, calculation amount and weight of the model are reduced by 8.4%, 11.2% and 14.3%, respectively, compared with the original model. Even compared with the state-of-the-art YOLOv7 model, the mAP@.5 value of the proposed model is improved by 6.5%. Although the FPS value is lower than YOLOv7 model, it also achieves a detection speed of 79 frames per second, which can meet the real-time demand. The experimental results demonstrate the effectiveness of the method in this paper, which can provide a reference for the automatic detection method of fabric defects.
C1 [Li, Feng; Xiao, Kang] Donghua Univ, Sch Comp Sci & Technol, Shanghai 201600, Peoples R China.
   [Li, Feng; Hu, Zhengpeng; Zhang, Guozheng] Natl Innovat Ctr Adv Dyeing & Finishing Technol, Tai An 271000, Shandong, Peoples R China.
C3 Donghua University
RP Xiao, K (corresponding author), Donghua Univ, Sch Comp Sci & Technol, Shanghai 201600, Peoples R China.
EM lifeng@dhu.edu.cn; xiao_kang99@163.com; huzp2019@foxmail.com;
   1943220103@qq.com
OI Li, Feng/0000-0001-6128-3663; Xiao, Kang/0000-0002-4048-8392
FU National Innovation Center of Advanced Dyeing amp; Finishing Technology
FX This paper was funded and technically supported by the "National
   Innovation Center of Advanced Dyeing & Finishing Technology".
CR Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Gevorgyan Z, 2022, arXiv
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu SL, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14225853
   Misra D., 2019, ARXIV190808681
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tianchi, 2020, SMART DIAGNOSIS CLOT
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zhao Z, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-80179-3
   Zheng ZH, 2022, IEEE T CYBERNETICS, V52, P8574, DOI 10.1109/TCYB.2021.3095305
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 25
TC 4
Z9 4
U1 37
U2 80
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2309
EP 2324
DI 10.1007/s00371-023-02918-7
EA JUN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001018118800003
DA 2024-07-18
ER

PT J
AU Huang, ZJ
   Hui, BW
   Sun, SJ
   Ma, YX
AF Huang, Zhijian
   Hui, Bingwei
   Sun, Shujin
   Ma, Yanxin
TI Infrared image super-resolution method based on dual-branch deep neural
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Infrared image; Super-resolution; ESRGAN; Gradient guidance; Haze
   removal
AB Infrared image has lower resolution, lower contrast, and less detail than visible image, which causes its super-resolution (SR) more difficult than visible image. This paper presents an approach based on a deep neural network that comprises an image SR branch and a gradient SR branch to reconstruct high-quality SR image from single-frame infrared image. The image SR branch reconstructs the SR image from the initial low-resolution infrared image using a basic structure similar to the enhanced SR generative adversarial network (ESRGAN). The gradient SR branch removes haze, extracts the gradient map, and reconstructs the SR gradient map. To obtain more natural SR image, a fusion block based on attention mechanism is adopted between these branches. To preserve the geometric structure, gradient L1 loss and gradient GAN loss are defined and added. Experimental results on a public infrared image dataset demonstrate that, compared with the current SR methods, the proposed method is more natural and realistic, and can better preserve the structures.
C1 [Huang, Zhijian] Changsha Univ, Sch Comp Sci & Engn, Changsha 410003, Peoples R China.
   [Hui, Bingwei] Natl Univ Def Technol, Sch Elect Sci, ATR Key Lab, Changsha, Peoples R China.
   [Sun, Shujin] Hunan Prov Key Lab Ind Internet Technol & Secur, Changsha, Peoples R China.
   [Ma, Yanxin] Natl Univ Def Technol, Coll Meteorol & Oceanol, Changsha, Peoples R China.
C3 Changsha University; National University of Defense Technology - China;
   National University of Defense Technology - China
RP Huang, ZJ (corresponding author), Changsha Univ, Sch Comp Sci & Engn, Changsha 410003, Peoples R China.
EM 274652724@qq.com; huibingwei07@163.com
OI Zhijian, Huang/0000-0002-7948-425X
FU Key Laboratory Fund of Basic Strengthening Program [JKWATR-210503];
   Changsha Municipal Natural Science Foundation [kq2202067]; Basic Science
   and Technology Research Project of the National Key Laboratory of
   Science and Technology on Automatic Target Recognition of Scientific
   Research [WDZC20205500209]
FX AcknowledgementsThis work was supported by Key Laboratory Fund of Basic
   Strengthening Program (JKWATR-210503), Changsha Municipal Natural
   Science Foundation (kq2202067), and the Basic Science and Technology
   Research Project of the National Key Laboratory of Science and
   Technology on Automatic Target Recognition of Scientific Research under
   Grant (WDZC20205500209).
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gautam A, 2022, VISUAL COMPUT, V38, P4111, DOI 10.1007/s00371-021-02284-2
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Li SM, 2018, NEUROCOMPUTING, V275, P267, DOI 10.1016/j.neucom.2017.08.041
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Ma C, 2020, STRUCTURE PRESERVING
   Mao R., 2020, SINGLE INFRARED IMAG
   [南方哲 Nan Fangzhe], 2020, [计算机应用研究, Application Research of Computers], V37, P321
   Nayak R, 2020, ARAB J SCI ENG, V45, P10261, DOI 10.1007/s13369-020-04662-9
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Songchen H., 2018, ADV ENG SCI, V50, P347
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2020, ARAB J SCI ENG, V45, P3245, DOI 10.1007/s13369-020-04351-7
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiaojiao M., 2016, RES GATE, V6, P391
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou DY, 2022, VISUAL COMPUT, V38, P119, DOI 10.1007/s00371-020-02007-z
   Zhou FQ, 2018, NEUROCOMPUTING, V290, P34, DOI 10.1016/j.neucom.2018.02.027
NR 30
TC 1
Z9 1
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1673
EP 1684
DI 10.1007/s00371-023-02878-y
EA MAY 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000994983400002
DA 2024-07-18
ER

PT J
AU Sheng, QX
   Fu, C
   Lin, ZA
   Chen, JX
   Cao, L
   Sham, CW
AF Sheng, Qingxin
   Fu, Chong
   Lin, Zhaonan
   Chen, Junxin
   Cao, Lin
   Sham, Chiu-Wing
TI An efficient chaotic image encryption scheme using simultaneous
   permutation-diffusion operation
SO VISUAL COMPUTER
LA English
DT Article
DE Chaos; Image encryption; Permutation-diffusion; PRNG; Reuse strategy
ID ALGORITHM; DESIGN; PIXEL
AB This paper suggests a chaotic image encryption scheme using an efficient pseudo-random number generator (PRNG). The proposed PRNG can produce 1024 pseudo-random numbers by reusing 64 items of the time series of the chaotic system. Compared with the direct quantization method, the efficiency of our PRNG is improved by about 79% due to a 16-fold reduction in the total number of iterations of the chaotic system. By using the proposed PRNG, we present an efficient chaotic image encryption scheme. Unlike most other existing approaches that use independent permutation and diffusion operations, we combine these two operations into a single stage. In our scheme, row-wise and column-wise permutation operations are performed by utilizing two different Latin squares, and the diffusion operation is performed simultaneously. As the two steps of permutation and diffusion interact with each other, our suggested scheme can achieve the satisfactory security level with only one round of encryption. Compared with the AES algorithm, the encryption efficiency of our scheme is improved by more than 60% on both grayscale and color images. Moreover, we also demonstrate the encryption efficiency of our scheme by comparing it with several state-of-the-art algorithms. The detailed security analysis shows that our suggested scheme is robust against all common attacks.
C1 [Sheng, Qingxin; Fu, Chong; Lin, Zhaonan] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.
   [Fu, Chong] Minist Educ, Engn Res Ctr Secur Technol Complex Network Syst, Shenyang, Peoples R China.
   [Chen, Junxin] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
   [Cao, Lin] Beijing Informat Sci & Technol Univ, Sch Informat & Commun Engn, Beijing 100101, Peoples R China.
   [Sham, Chiu-Wing] Univ Auckland, Sch Comp Sci, Auckland, New Zealand.
C3 Northeastern University - China; Dalian University of Technology;
   Beijing Information Science & Technology University; University of
   Auckland
RP Fu, C (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.; Fu, C (corresponding author), Minist Educ, Engn Res Ctr Secur Technol Complex Network Syst, Shenyang, Peoples R China.
EM fuchong@mail.neu.edu.cn
RI Sham, Chiu-Wing/C-3819-2014
OI Sham, Chiu-Wing/0000-0001-7007-6746
FU National Natural Science Foundation of China [62171114]; National Key R
   &D Program of China [2021YFF0306405]; Fundamental Research Funds for the
   Central Universities [N2124006-1]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (No. 62171114) and the National Key R &D Program of
   China (No. 2021YFF0306405) and the Fundamental Research Funds for the
   Central Universities (No. N2124006-1).
CR Al-Khasawneh MA, 2022, CLUSTER COMPUT, V25, P999, DOI 10.1007/s10586-021-03466-2
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Bezerra JIM, 2021, CHAOS SOLITON FRACT, V151, DOI 10.1016/j.chaos.2021.111235
   Chen JX, 2015, NONLINEAR DYNAM, V81, P1151, DOI 10.1007/s11071-015-2057-6
   Chen JX, 2015, COMMUN NONLINEAR SCI, V23, P294, DOI 10.1016/j.cnsns.2014.11.021
   Chen L, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103424
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Fu C, 2018, INFORMATICA-LITHUAN, V29, P651, DOI 10.15388/Informatica.2018.186
   Fu C, 2013, COMPUT BIOL MED, V43, P1000, DOI 10.1016/j.compbiomed.2013.05.005
   Ge B, 2022, IEEE ACCESS, V10, P95986, DOI 10.1109/ACCESS.2022.3204873
   Gui X., 2022, MULTIMED TOOLS APPL, P1
   Hu GQ, 2017, NONLINEAR DYNAM, V88, P1305, DOI 10.1007/s11071-016-3311-2
   Hua ZY, 2021, NONLINEAR DYNAM, V104, P807, DOI 10.1007/s11071-021-06308-3
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Huang LQ, 2019, OPT LASER ENG, V115, P7, DOI 10.1016/j.optlaseng.2018.11.015
   Kang SW, 2022, MULTIMED TOOLS APPL, V81, P1209, DOI 10.1007/s11042-021-11424-8
   Kumar A, 2021, MULTIMED TOOLS APPL, V80, P21727, DOI 10.1007/s11042-021-10750-1
   Kumar CM, 2022, APPL INTELL, V52, P2556, DOI 10.1007/s10489-021-02508-x
   Li M, 2022, CHAOS SOLITON FRACT, V158, DOI 10.1016/j.chaos.2022.111989
   Liu S, 2021, IEEE MULTIMEDIA
   Ma YL, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102566
   Man ZL, 2021, CHAOS SOLITON FRACT, V152, DOI 10.1016/j.chaos.2021.111318
   Matthews R., 1989, Cryptologia, V13, P29, DOI [10.1080/0161-118991863745, DOI 10.1080/0161-118991863745]
   Qin C, 2019, IEEE T CIRC SYST VID, V29, P3341, DOI 10.1109/TCSVT.2018.2878026
   Sha YW, 2022, INT J BIFURCAT CHAOS, V32, DOI 10.1142/S0218127422501863
   Shahna KU, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116495
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Shrivastava M, 2021, NONLINEAR DYNAM, V106, P2679, DOI 10.1007/s11071-021-06923-0
   Song W, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116628
   Teng L, 2022, INFORM SCIENCES, V605, P71, DOI 10.1016/j.ins.2022.05.032
   Trujillo-Toledo DA, 2021, CHAOS SOLITON FRACT, V153, DOI 10.1016/j.chaos.2021.111506
   Tsafack N, 2020, INFORM SCIENCES, V515, P191, DOI 10.1016/j.ins.2019.10.070
   Wang H, 2019, SIGNAL PROCESS, V155, P218, DOI 10.1016/j.sigpro.2018.10.001
   Wang XY, 2015, OPT LASER ENG, V73, P53, DOI 10.1016/j.optlaseng.2015.03.022
   Wang XY, 2022, VISUAL COMPUT, V38, P3831, DOI 10.1007/s00371-021-02224-0
   Wang XY, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106393
   Wang XY, 2013, COMMUN NONLINEAR SCI, V18, P3075, DOI 10.1016/j.cnsns.2013.04.008
   Wang Y, 2009, CHAOS SOLITON FRACT, V41, P1773, DOI 10.1016/j.chaos.2008.07.031
   Wu XJ, 2015, APPL SOFT COMPUT, V37, P24, DOI 10.1016/j.asoc.2015.08.008
   Wu Y, 2014, INFORM SCIENCES, V264, P317, DOI 10.1016/j.ins.2013.11.027
   Xian YJ, 2020, OPT LASER ENG, V134, DOI 10.1016/j.optlaseng.2020.106202
   Xu L, 2017, OPT LASER ENG, V91, P41, DOI 10.1016/j.optlaseng.2016.10.012
   Xu M, 2019, INFORM SCIENCES, V478, P1, DOI 10.1016/j.ins.2018.11.010
   Zhang W, 2013, COMMUN NONLINEAR SCI, V18, P584, DOI 10.1016/j.cnsns.2012.08.010
   Zhang XJ, 2013, SIGNAL PROCESS, V93, P2422, DOI 10.1016/j.sigpro.2013.03.017
   Zhang Y., 2022, VISUAL COMPUT, P1
   Zhang Y, 2018, INFORM SCIENCES, V450, P361, DOI 10.1016/j.ins.2018.03.055
   Zhao HY, 2022, CHAOS SOLITON FRACT, V164, DOI 10.1016/j.chaos.2022.112742
NR 48
TC 3
Z9 3
U1 15
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1643
EP 1658
DI 10.1007/s00371-023-02876-0
EA MAY 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000991778200001
DA 2024-07-18
ER

PT J
AU Wu, CM
   Wu, W
AF Wu, Chengmao
   Wu, Wen
TI Master-slave hierarchy local information driven fuzzy C-means clustering
   for noisy image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Image segmentation; Fuzzy C-means clustering; Fuzzy local information
   factor; Master-slave neighborhood model; Anti-noise robustness
ID ALGORITHM; RECONSTRUCTION; TAXONOMY; FCM
AB Local neighborhood information plays an important role in robust fuzzy clustering-related segmentation algorithms, and how to construct local information items is the key to robust fuzzy clustering. Based on existing local information constraints, this paper proposes a model to describe the hierarchy relationship of local neighborhood windows in the master-slave neighborhood model, which combines spatial distance with gray information to suppress the influence of noise on current pixel clustering, and can also well control the balance of noise suppression and detail preservation. Based on this model, this paper proposes a robust fuzzy clustering segmentation algorithm with master-slave neighborhood information constraints. When constraining the neighborhood pixels of a pixel (that is the master neighborhood pixels), the algorithm will further constrain the pixels in the neighborhood window around the master neighborhood pixel (that is the salve neighborhood pixels), thus enhancing the robustness of the algorithm. Experiments results show that the proposed algorithm has good segmentation performance and strong anti-noise performance, even significantly outperforms existing state-of-the-art robust fuzzy clustering-related algorithms in the presence of high noise.
C1 [Wu, Chengmao; Wu, Wen] Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Wu, W (corresponding author), Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
EM wuchengmao123@sohu.com; ww70182@163.com
FU National Natural Science Foundation of China [61671377]; Natural Science
   Foundation of Shaanxi Province [2022JM-370]; School of Electronic
   Engineering, Xi'an University of Posts & Telecommunications, Xi'an,
   China
FX This work was supported by the National Natural Science Foundation of
   China (Grant Numbers 61671377), and the Natural Science Foundation of
   Shaanxi Province (2022JM-370). Wu and Wu would like to thank the
   anonymous reviewers for their constructive suggestions to improve the
   overall quality of the paper. Besides, Wu and Wu would like to thank the
   School of Electronic Engineering, Xi'an University of Posts &
   Telecommunications, Xi'an, China for financial support.
CR Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Bei HH, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/6687202
   BEZDEK JC, 1974, J MATH BIOL, V1, P57, DOI 10.1007/BF02339490
   BEZDEK JC, 1987, IEEE T SYST MAN CYB, V17, P873, DOI 10.1109/TSMC.1987.6499296
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bhagyalakshmi S., 2015, Int. J. Eng. Res. Technol. (IJERT), V4, P68, DOI [10.17577/IJERTV4IS050183, DOI 10.17577/IJERTV4IS050183]
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Celik T, 2013, IEEE T IMAGE PROCESS, V22, P1258, DOI 10.1109/TIP.2012.2226048
   Chen L, 2011, IEEE T SYST MAN CY B, V41, P1263, DOI 10.1109/TSMCB.2011.2124455
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   CONGALTON RG, 1991, REMOTE SENS ENVIRON, V37, P35, DOI 10.1016/0034-4257(91)90048-B
   eecs, About us
   Fahad A, 2014, IEEE T EMERG TOP COM, V2, P267, DOI 10.1109/TETC.2014.2330519
   Fan JL, 2003, PATTERN RECOGN LETT, V24, P1607, DOI 10.1016/S0167-8655(02)00401-4
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gao X, 2023, VISUAL COMPUT, V39, P1583, DOI 10.1007/s00371-022-02430-4
   Gao YL, 2022, KNOWL-BASED SYST, V237, DOI 10.1016/j.knosys.2021.107769
   github, about us
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Gong MG, 2012, IEEE T IMAGE PROCESS, V21, P2141, DOI 10.1109/TIP.2011.2170702
   Goyal S, 2017, INT J UNCERTAIN FUZZ, V25, P649, DOI 10.1142/S0218488517500283
   Guo YH, 2013, CIRC SYST SIGNAL PR, V32, P1699, DOI 10.1007/s00034-012-9531-x
   Hassan N.S., 2021, Asian Journal of Research in Computer Science, V9, P23, DOI [10.9734/ajrcos/2021/v9i130212, DOI 10.9734/AJRCOS/2021/V9I130212]
   He H, 2019, SCI CHINA EARTH SCI, V62, P438, DOI 10.1007/s11430-017-9224-6
   Hemalatha K. L., 2017, Journal of Theoretical and Applied Information Technology, V95, P3365
   host.robots.ox.ac.uk, About us
   Jha P, 2021, COMPUT BIOL CHEM, V92, DOI 10.1016/j.compbiolchem.2021.107454
   Ji J, 2014, IEEE J-STARS, V7, P4929, DOI 10.1109/JSTARS.2014.2308531
   Jia XH, 2020, IEEE ACCESS, V8, P146182, DOI 10.1109/ACCESS.2020.3015270
   Junwei Han, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P816, DOI 10.1109/TKDE.2020.2986201
   kaggle, US
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Li J., 2019, Matrix Sci. Pharma., V3, P9, DOI [10.4103/MTSP.MTSP_3_19, DOI 10.4103/MTSP.MTSP_3_19]
   Li MQ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102385
   Li N, 2013, IEEE GEOSCI REMOTE S, V10, P1124, DOI 10.1109/LGRS.2012.2231662
   Liu B, 2019, IEEE ACCESS, V7, P42169, DOI 10.1109/ACCESS.2019.2907573
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Lu ZY, 2019, J VIS COMMUN IMAGE R, V58, P269, DOI 10.1016/j.jvcir.2018.11.045
   Madhu A, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13204163
   Miloudi S, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23050553
   Muñoz X, 2003, PATTERN RECOGN LETT, V24, P375, DOI 10.1016/S0167-8655(02)00262-3
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Pei YL, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13179741
   Schultz T, 2013, IEEE T VIS COMPUT GR, V19, P2100, DOI 10.1109/TVCG.2013.181
   Sharma PK, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P109, DOI 10.1109/ICMLA.2015.86
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Szilágyi L, 2003, P ANN INT IEEE EMBS, V25, P724, DOI 10.1109/IEMBS.2003.1279866
   Tang YM, 2020, APPL SOFT COMPUT, V87, DOI 10.1016/j.asoc.2019.105928
   Wang C, 2021, IEEE-CAA J AUTOMATIC, V8, P876, DOI 10.1109/JAS.2020.1003420
   Wang QS, 2021, APPL SOFT COMPUT, V105, DOI 10.1016/j.asoc.2021.107245
   Wang QS, 2020, APPL SOFT COMPUT, V92, DOI 10.1016/j.asoc.2020.106318
   Wang W, 2008, FIFTH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY, VOL 1, PROCEEDINGS, P62, DOI 10.1109/FSKD.2008.160
   Wu CM, 2021, SOFT COMPUT, V25, P3751, DOI 10.1007/s00500-020-05403-8
   Xiang DL, 2014, IEEE GEOSCI REMOTE S, V11, P1290, DOI 10.1109/LGRS.2013.2292820
   Zaitoun NM, 2015, PROCEDIA COMPUT SCI, V65, P797, DOI 10.1016/j.procs.2015.09.027
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang H, 2017, IEEE T GEOSCI REMOTE, V55, P5057, DOI 10.1109/TGRS.2017.2702061
   Zhang XF, 2021, INFORM SCIENCES, V550, P129, DOI 10.1016/j.ins.2020.10.039
   Zhang XF, 2019, SOFT COMPUT, V23, P3081, DOI 10.1007/s00500-017-2955-2
   Zhao F, 2014, EXPERT SYST APPL, V41, P4083, DOI 10.1016/j.eswa.2014.01.003
   Zhou Y, 2019, INFORM SCIENCES, V472, P77, DOI 10.1016/j.ins.2018.08.064
   Zhou Y, 2016, INFORM SCIENCES, V360, P1, DOI 10.1016/j.ins.2016.03.027
NR 63
TC 0
Z9 0
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 865
EP 897
DI 10.1007/s00371-023-02821-1
EA APR 2023
PG 33
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000969747600003
DA 2024-07-18
ER

PT J
AU Yang, HL
   Yang, ZY
   Li, LJ
   Chen, JH
AF Yang, Hailong
   Yang, Zuyuan
   Li, Lingjiang
   Chen, Junhang
TI A method of smoothing laser spot deformation
SO VISUAL COMPUTER
LA English
DT Article
DE Spot size; Kalman filter; Prior information; Environmental factors
ID KALMAN FILTER; PERFORMANCE; DIAMETER
AB Spot size is an important parameter in laser-quality detection. However, the size of the spot is easily affected by environmental factors such as light. In this paper, we propose improved Kalman filter algorithms (IKF) with prior information to reduce the negative influence of noise on the spot size. At the same time, the filtering process and the method of determining the initial filter value are given. Finally, the experimental results on the synthetic dataset and real-world dataset confirm that the proposed IKF performs better than the compared methods.
C1 [Yang, Hailong; Yang, Zuyuan; Li, Lingjiang; Chen, Junhang] Guangdong Univ Technol, Sch Automat, Guangdong Key Lab IoT Informat Technol, Guangzhou 510006, Peoples R China.
   [Chen, Junhang] Minist Educ, Key Lab Intelligent Detect & Internet Things Mfg G, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology
RP Li, LJ (corresponding author), Guangdong Univ Technol, Sch Automat, Guangdong Key Lab IoT Informat Technol, Guangzhou 510006, Peoples R China.
EM hlyanggdut@aliyun.com; zuyuangdut@aliyun.com; shemmeenen@gmail.com;
   junhanggdgy@aliyun.com
RI zhao, yan/JNT-6961-2023; li, yifan/JHU-9272-2023; YI, J/JJE-7713-2023;
   Wang, Xin/JVE-0200-2024; Zhang, Yuan/JUF-7293-2023; Yu,
   Xiaohan/KCK-5462-2024; Li, Yan/JRW-0176-2023; zhang, xiao/JCN-8822-2023;
   LI, WEI/JUE-9796-2023; zhang, can/KHC-5357-2024; Li, J N/JXL-5833-2024;
   yuanyuan, Li/JEZ-6497-2023; Wang, yl/JNR-4963-2023; Li,
   Wen/JQI-4757-2023; liu, jingwen/JQW-9270-2023; wang, xin/JWA-3772-2024;
   wang, zhe/JNE-3510-2023; wang, xiaoxuan/JMP-6531-2023; Wang,
   Jing/JRW-1512-2023; li, wenjing/JMP-7498-2023; Zhang,
   Yusi/JNS-2335-2023; Li, siqi/KDN-4520-2024; wang, jun/JPY-3635-2023;
   zhao, lin/JJF-0406-2023; Liu, Yang/JVD-6777-2023; Wang,
   Xuechun/JRX-6509-2023; Wang, Yue/JRY-8962-2023; zhang,
   ying/JQX-1479-2023; ZHOU, YUE/KCJ-8790-2024; song, yu/KCZ-2003-2024; li,
   li/JVP-2971-2024; wang, wei/JYP-7819-2024; chang, yu/KFB-2822-2024; Li,
   YU/JQV-2716-2023; Li, Yan/JUU-5189-2023; Zhang, Yunyi/JHS-3626-2023;
   yang, yunfeng/KHT-9566-2024; Zhang, Tianxi/KEH-5921-2024; yang,
   rui/JHI-3328-2023; Zhou, Yue/JHS-8791-2023; yang, xiao/JLL-7721-2023;
   zheng, yan/JKJ-3632-2023; su, hang/KEH-2976-2024; Zhang,
   Youyou/KCY-0810-2024; wang, yi/JYO-8193-2024; Wang, Han/JJF-2614-2023;
   Zhang, Wei/JKI-3565-2023; li, yuan/KBQ-4200-2024
OI Wang, Yue/0000-0001-8673-6358; 
FU Guangdong Basic and Applied Basic Research Foundation [2022A1515010688];
   Key-Area Research and Development Program of Guangdong Province
   [2019B010154002]
FX AcknowledgementsThis work was supported in part by the Guangdong Basic
   and Applied Basic Research Foundation under Grant 2022A1515010688 and in
   part by the Key-Area Research and Development Program of Guangdong
   Province under Grant 2019B010154002.
CR Ansari-Shahrezaei S, 2011, GRAEF ARCH CLIN EXP, V249, P11, DOI 10.1007/s00417-010-1460-4
   Ayyildiz BC., 2020, PROC SPIE, V11538, P21
   Bonnett M., 2021, ARXIV
   Cai YL, 2020, IEEE T NUCL SCI, V67, P1861, DOI 10.1109/TNS.2020.3000275
   Dang LJ, 2022, IEEE-CAA J AUTOMATIC, V9, P450, DOI 10.1109/JAS.2021.1004350
   Fabry J, 2020, IEEE SIGNAL PROC LET, V27, P1854, DOI 10.1109/LSP.2020.3029703
   Jia BZ, 2015, VISUAL COMPUT, V31, P281, DOI 10.1007/s00371-014-0918-5
   Jia HM, 2019, IEEE ACCESS, V7, P44097, DOI 10.1109/ACCESS.2019.2908718
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Li ZM., 2011, OPT OPTELECTRON TECH, V9, P19
   Liu Y., 2021, WIREL COMMUN MOB COM, V2021, P1, DOI [10.1155/2021/7264264, DOI 10.1155/2021/7264264]
   Meng CL, 2018, IEEE PHOTONIC TECH L, V30, P1964, DOI 10.1109/LPT.2018.2873371
   Pang M., 2013, INT C OPTICAL INSTRU
   Ranjbar OA, 2020, APPL PHYS A-MATER, V126, DOI 10.1007/s00339-020-03504-7
   Shao T, 2021, IEEE SIGNAL PROC LET, V28, P902, DOI 10.1109/LSP.2021.3071979
   Stratan A, 2014, OPT ENG, V53, DOI 10.1117/1.OE.53.12.122513
   Sun DJ, 2021, IEEE J OCEANIC ENG, V46, P183, DOI 10.1109/JOE.2020.2976078
   Suzdaleva E, 2010, INT J ADAPT CONTROL, V24, P188, DOI 10.1002/acs.1106
   Virmontois C., 2019, 19 EUROPEAN C RAD IT, P1
   Wang FB, 2018, INT J ADV MANUF TECH, V94, P2605, DOI 10.1007/s00170-017-1007-5
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wang ZZ, 2018, IEEE T CIRC SYST VID, V28, P2220, DOI 10.1109/TCSVT.2017.2719122
   WOLFFROTTKE B, 1995, APPL PHYS A-MATER, V60, P13, DOI 10.1007/BF01577606
   Xie H, 2021, IEEE T GEOSCI REMOTE, V59, P9758, DOI 10.1109/TGRS.2020.3048042
   Yang Y, 2020, IEEE SENS J, V20, P6331, DOI 10.1109/JSEN.2020.2976579
   Zhang F.L, 2011, INT C OPTICAL INSTRU
   Zhang H, 2020, IEEE T CYBERNETICS, V50, P4346, DOI 10.1109/TCYB.2019.2901515
   Zhang Z.X., 2008, 5 INT S INSTRUMENTAT, V7133, P15
   Zheng ZX, 2021, I C COMM SOFTW NET, P192, DOI 10.1109/ICCSN52437.2021.9463668
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 30
TC 0
Z9 0
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6689
EP 6697
DI 10.1007/s00371-022-02756-z
EA MAR 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000949124700001
DA 2024-07-18
ER

PT J
AU Xin, FF
   Zhang, HP
   Pan, HG
AF Xin, Fangfang
   Zhang, Huipeng
   Pan, Hongguang
TI Hybrid dilated multilayer faster RCNN for object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Hybrid dilated convolution; Spatial pyramid pooling;
   Faster RCNN
ID SUPERRESOLUTION; REPRESENTATION; NETWORK
AB Faster region-based convolution neural network (Faster RCNN) architecture was proposed as an efficient object detection method, wherein a CNN is used to extract image features. However, CNNs require a large number of learning parameters, and an excessive amount of pooling layers lead to a loss of information on small objects, which may affect efficiency. In this study, we proposed a hybrid dilated multilayer Faster RCNN model to address this problem. The key contributions of this work are summarized as follows: (1) We substituted a hybrid dilated CNN (HDC) model for the VGG16 network used in the original Faster RCNN architecture to extract features and ensure portability. We also used a LeakyReLU activation function to improve the mapping ability of negative input information to detect objects rapidly and accurately. (2) We used a multilayer feature spatial pyramid to convert single-scale features into multi-scale features, and higher-resolution information was obtained through a deconvolutional network to achieve more accurate object detection. (3) We conducted experiments to verify the performance of the proposed HDMF-RCNN model using the Microsoft COCO data set. The results indicated that the accuracy of HDMF-RCNN was 8.12% greater than that of the traditional Faster RCNN model, and the training loss and training time were lower by 44.64% and 39.46% on average, respectively. Overall, the results verified that HDMF-RCNN can significantly improve on the efficiency of existing object detection methods. As an independent feature extraction network, HDC can be adapted to different network frameworks.
C1 [Xin, Fangfang; Zhang, Huipeng; Pan, Hongguang] Xian Univ Sci & Technol, Xian, Peoples R China.
C3 Xi'an University of Science & Technology
RP Xin, FF (corresponding author), Xian Univ Sci & Technol, Xian, Peoples R China.
EM xf9258@163.com
RI Zhang, Huipeng/GLT-9976-2022
OI Zhang, Huipeng/0000-0003-3759-9179
FU Natural Science Basic Research Program of Shaanxi [2021JQ-572,
   2021JQ-574]; National Natural Science Foundation of China [51804250,
   51905416, 51804249]; Xi'an Science and Technology Program
   [2022JH-RGZN-0041]; Qin Chuangyuan "Scientists + Engineers" Team
   Construction Program in Shaanxi Province [2022KXJ-38]; Scientific
   Research Plan Projects of Shaanxi Education Department [20JK0758]
FX This work was supported in part by the Natural Science Basic Research
   Program of Shaanxi (Grant No. 2021JQ-572, 2021JQ-574), in part by the
   National Natural Science Foundation of China (grant No. 51804250,
   51905416, 51804249), by the Xi'an Science and Technology Program (grant
   No. 2022JH-RGZN-0041), in part by the Qin Chuangyuan "Scientists +
   Engineers" Team Construction Program in Shaanxi Province (grant No.
   2022KXJ-38), and in part by the Scientific Research Plan Projects of
   Shaanxi Education Department (grant No. 20JK0758).
CR Albahli S, 2021, ARAB J SCI ENG, V46, P8509, DOI 10.1007/s13369-021-05471-4
   Chandra Mayank Arya, 2021, International Journal of Information Technology, V13, P1, DOI 10.1007/s41870-017-0080-1
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Jiang D, 2021, FUTURE GENER COMP SY, V123, P94, DOI 10.1016/j.future.2021.04.019
   Jiang XR, 2021, NEURAL NETWORKS, V144, P21, DOI 10.1016/j.neunet.2021.08.002
   Jiang XR, 2020, NEUROCOMPUTING, V402, P29, DOI 10.1016/j.neucom.2020.03.073
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1990, ADV NEURAL INFORM PR, P396
   Lei M, 2021, FUEL, V294, DOI 10.1016/j.fuel.2021.120475
   Li HY, 2021, NEUROCOMPUTING, V432, P159, DOI 10.1016/j.neucom.2020.12.076
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Zhidi, 2022, Proceedings of SPIE, DOI 10.1117/12.2643386
   Mansour RF, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104229
   Qiao LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8661, DOI 10.1109/ICCV48922.2021.00856
   Ren J., 2022, Journal of Computer and Communications, V10, P115
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tripathi M., 2021, J. Innov. Image Process. (JIIP), V3, P100, DOI [DOI 10.36548/JIIP.2021.2.003, 10.36548/jiip.2021.2.003]
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu CH, 2021, J CENT SOUTH UNIV, V28, P1765, DOI 10.1007/s11771-021-4731-9
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Yu F., 2015, ARXIV
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang H., 2022, arXiv
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhao ZP, 2021, NEURAL NETWORKS, V141, P52, DOI 10.1016/j.neunet.2021.03.013
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng WF, 2021, PEERJ COMPUT SCI, V7, DOI 10.7717/peerj-cs.613
NR 33
TC 4
Z9 6
U1 35
U2 77
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 393
EP 406
DI 10.1007/s00371-023-02789-y
EA MAR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000945286400001
DA 2024-07-18
ER

PT J
AU Bai, RY
AF Bai, Ruyi
TI A general image orientation detection method by feature fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Image orientation detection; LBP; Attention features; Rotation features
AB The automatic detection of image orientation is an important part of computer vision research. It is widely used in a variety of intelligent devices and application software. In the existing research on orientation detection, low-level features used in classification model cannot accurately express the high-level semantics of the image, and fine-tuning the existing deep learning network does not consider whether the extracted features can express the human visual perception of the orientation. As a result, the generalization ability of the model is not high. Based on the above shortcomings, we propose an automatic image orientation detection method based on the fusion of attention features (AF) and rotation features (RF). Firstly, the AF is obtained by fusing the attention mechanism features, which are extracted from the feature maps of different scales of ResNet50. It can quickly screen out high-value information from a large amount of information by using limited attention resources. Secondly, the "rotating LBP" features of different scales that can better reflect the direction attribute are extracted. The RF is obtained by residual dilated convolution combing with ResNet50. It can more accurately express the directional characteristics of the image and improve the generalization ability of the model. Finally, AF and RF are fused to realize the detection of four orientations of the image. The proposed method is verified on five different types of data sets. The results show that this method can more comprehensively express the directional semantics of images and improve the classification accuracy and wide application of the model.
C1 [Bai, Ruyi] Shanxi Univ, Coll Automat & Software, Taiyuan 030013, Shanxi, Peoples R China.
C3 Shanxi University
RP Bai, RY (corresponding author), Shanxi Univ, Coll Automat & Software, Taiyuan 030013, Shanxi, Peoples R China.
EM bry@sxu.edu.cn
FU Youth Program of the National Natural Science Foundation of China
   [61603228]; Fundamental Research Program of Shanxi Province
   [202103021223030]; Scientific and Technological Innovation Programs of
   Higher Education Institutions in Shanxi [2020L0036]
FX AcknowledgementsThis work is partially supported by the Youth Program of
   the National Natural Science Foundation of China (61603228), Fundamental
   Research Program of Shanxi Province (202103021223030), Scientific and
   Technological Innovation Programs of Higher Education Institutions in
   Shanxi (2020L0036).
CR ALAMEDAPINEDA X, 2016, PROC CVPR IEEE, P5240, DOI DOI 10.1109/CVPR.2016.566
   Bai R.Y., 2020, 3 INT C COMPUTER SCI
   Bai RY, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.107240
   Bai RY., 2021, COMPUT APPL SOFTW, V38, P239
   Borawski Mariusz, 2012, Machine Learning and Data Mining in Pattern Recognition. Proceedings 8th International Conference, MLDM 2012, P336, DOI 10.1007/978-3-642-31537-4_26
   Cao ZQ, 2016, IEEE T SYST MAN CY-S, V46, P1589, DOI 10.1109/TSMC.2015.2497253
   Cingovska I., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2985, DOI 10.1109/ICIP.2011.6116289
   Ciocca G., 2010, IMAGE ORIENTATION DE, V7537, P75370
   Ciocca G, 2015, MULTIMED TOOLS APPL, V74, P3013, DOI 10.1007/s11042-013-1766-4
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hazgui M, 2022, VISUAL COMPUT, V38, P457, DOI 10.1007/s00371-020-02028-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hollitt C., 2012, P IVCNZ 2012, V2012, P346, DOI [10.1145/2425836.2425904, DOI 10.1145/2425836.2425904]
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu P, 2022, IEEE T CYBERNETICS, V52, P12954, DOI 10.1109/TCYB.2021.3093626
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Joshi U, 2017, 2017 14TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2017), P103, DOI 10.1109/CRV.2017.59
   Li XR, 2023, VISUAL COMPUT, V39, P1307, DOI 10.1007/s00371-022-02407-3
   Liu J, 2017, MULTIMED TOOLS APPL, V76, P1017, DOI 10.1007/s11042-015-3104-5
   [刘丽 Liu Li], 2014, [中国图象图形学报, Journal of Image and Graphics], V19, P1696
   Lumini A, 2021, MACH LEARN APPL, V6, DOI 10.1016/j.mlwa.2021.100090
   Lyu, 2011, AUTOMATIC IMAGE ORIE, P491
   Morra L, 2019, 2019 IEEE 23RD INTERNATIONAL SYMPOSIUM ON CONSUMER TECHNOLOGIES (ISCT), P118, DOI [10.1109/isce.2019.8901005, 10.1109/ISCE.2019.8901005]
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P4857, DOI 10.1109/TNNLS.2019.2958324
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Prince M, 2019, IEEE ACCESS, V7, P185750, DOI 10.1109/ACCESS.2019.2959666
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Sartori A, 2015, ACM T INTERACT INTEL, V5, DOI 10.1145/2768209
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Swami Kunal, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P495, DOI 10.1109/ICMEW.2017.8026216
   Wang G., 2022, VISUAL COMPUT
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
NR 34
TC 2
Z9 2
U1 3
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 287
EP 302
DI 10.1007/s00371-023-02782-5
EA JAN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000918719100001
DA 2024-07-18
ER

PT J
AU Singh, K
   Parihar, AS
AF Singh, Kavinder
   Parihar, Anil Singh
TI Illumination estimation for nature preserving low-light image
   enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light image; Illumination estimation; Retinex; Image enhancement;
   Guided filtering
ID CONTRAST ENHANCEMENT; ALGORITHM
AB In retinex model, images are considered as a combination of two components: illumination and reflectance. However, decomposing an image into the illumination and reflectance is an ill-posed problem. This paper presents a new approach to estimate the illumination for low-light image enhancement. This work contains three major tasks: estimation of structure-aware initial illumination, refinement of the estimated illumination, and the final correction of lightness in refined illumination. We have proposed a novel approach for structure-aware initial illumination estimation leveraging a new multi-scale guided filtering approach. The algorithm refines proposed initial estimation by formulating a new multi-objective function for optimization. Further, we proposed a new adaptive illumination adjustment for correction of lightness using the estimated illumination. The qualitative and quantitative analysis on low-light images with varying illumination shows that the proposed algorithm performs image enhancement with color constancy and preserves the natural details. The performance comparison with state-of-the-art algorithms shows the superiority of the proposed algorithm.
C1 [Singh, Kavinder; Parihar, Anil Singh] Delhi Technol Univ, Dept Comp Sci & Engn, Machine Learning Res Lab, Delhi, India.
C3 Delhi Technological University
RP Parihar, AS (corresponding author), Delhi Technol Univ, Dept Comp Sci & Engn, Machine Learning Res Lab, Delhi, India.
EM kavinder85@gmail.com; parihar.anil@gmail.com
RI Parihar, Anil Singh/Z-4992-2019
OI Parihar, Anil Singh/0000-0001-5339-8671
CR Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Celik T, 2012, PATTERN RECOGN, V45, P3810, DOI 10.1016/j.patcog.2012.03.019
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Dhal Krishna Gopal, 2018, Pattern Recognition and Image Analysis, V28, P747, DOI 10.1134/S1054661818040211
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gautam A, 2022, VISUAL COMPUT, V38, P4111, DOI 10.1007/s00371-021-02284-2
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guo ZB, 2023, VISUAL COMPUT, V39, P4267, DOI 10.1007/s00371-022-02589-w
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Jiang Y., 2019, ARXIV191202178, P1
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   Ju MY, 2017, VISUAL COMPUT, V33, P1613, DOI 10.1007/s00371-016-1305-1
   Karr BA, 2021, VISUAL COMPUT, V37, P895, DOI 10.1007/s00371-020-01841-5
   Land Edwin H., 1985, Wenner-gren center international symposium series, P5
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1983, P NATL ACAD SCI USA, V80, P5163, DOI 10.1073/pnas.80.16.5163
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li M, 2022, VISUAL COMPUT, V38, P4203, DOI 10.1007/s00371-021-02289-x
   Lin R., 2021, VISUAL COMPUT, P1
   Lind D.A., 2014, STATIS TECHN BUSINES
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma L., 2021, IEEE T NEUR NET LEAR, V22, P3025
   Parihar AS, 2022, VISUAL COMPUT, V38, P295, DOI 10.1007/s00371-020-02016-y
   Parihar AS, 2021, IET IMAGE PROCESS, V15, P1410, DOI 10.1049/ipr2.12114
   Parihar AS, 2020, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS 2020), P823, DOI [10.1109/ICICCS48265.2020.9120999, 10.1109/iciccs48265.2020.9120999]
   Parihar AS, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P619, DOI 10.1109/ICISC.2018.8398874
   Parihar AS, 2017, IEEE T IMAGE PROCESS, V26, P1810, DOI 10.1109/TIP.2017.2665975
   Parihar AS, 2016, IET IMAGE PROCESS, V10, P799, DOI 10.1049/iet-ipr.2016.0242
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Rahman Z, 2021, VISUAL COMPUT, V37, P865, DOI 10.1007/s00371-020-01838-0
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen L., 2017, ARXIV171102488
   Singh K, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103241
   Song X., 2022, VISUAL COMPUT, P1
   Wang CM, 2021, VISUAL COMPUT, V37, P1233, DOI 10.1007/s00371-021-02079-5
   Wang CX, 2021, VISUAL COMPUT, V37, P77, DOI 10.1007/s00371-020-01888-4
   Wang C, 2021, VISUAL COMPUT, V37, P1851, DOI 10.1007/s00371-020-01944-z
   Wang G., 2022, VISUAL COMPUT, P1
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang YN, 2021, VISUAL COMPUT, V37, P1467, DOI 10.1007/s00371-020-01882-w
   Wang Y, 2019, J SCI COMPUT, V78, P29, DOI 10.1007/s10915-018-0757-z
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woodell G., RETINEX IMAGE PROCES
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180
   YU X, 2022, VISUAL COMPUT, P1
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang Q, 2019, COMPUT GRAPH FORUM, V38, P243, DOI 10.1111/cgf.13833
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 66
TC 8
Z9 8
U1 8
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 121
EP 136
DI 10.1007/s00371-023-02770-9
EA JAN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000914709900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, XT
   Wu, W
   Zhang, LL
   Wan, Y
AF Wu, Xian-Tao
   Wu, Wen
   Zhang, Lin-Lin
   Wan, Yi
TI Don't worry about noisy labels in soft shadow detection
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Noisy label; Transformer; Soft shadow
ID ILLUMINATION; NETWORKS; REMOVAL
AB Soft shadow is harder to detect than hard shadow as its complex characteristics (i.e., low-contrast, irregular shape, and ambiguous shadow boundaries). To improve the detecting capacity of these images, in this paper, we create a new benchmark for soft shadow detection and then design a reasonable supervision strategy to alleviate the effect of annotation noises. Next, we present a general shadow detection framework based on transformer to deal with complex scenes. Concretely, we combine the traditional channel attention and recent popular self-attention into our network. Moreover, we introduce a deep supervision mechanism that performs deep layer supervision to "guide " early classification results at each layer, which can further improve our detection performance. Finally, experimental results on three datasets show that our shadow transformer can be favorable against current state-of-the-art detectors.
C1 [Wu, Xian-Tao; Zhang, Lin-Lin] Xinjiang Univ, Sch Informat Sci & Engn, Urumqi 830046, Peoples R China.
   [Wu, Wen] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Wan, Yi] Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
C3 Xinjiang University; Hangzhou Dianzi University; Shaoxing University
RP Wu, XT; Zhang, LL (corresponding author), Xinjiang Univ, Sch Informat Sci & Engn, Urumqi 830046, Peoples R China.
EM xiantao.cs@gmail.com; wuwen.hdu.cs@gmail.com; zllnadasha@xju.edu.com;
   19945853@qq.com
RI Wu, Wen/HKW-7234-2023; ZHANG, Linlin/KIB-7136-2024
OI Wu, Wen/0000-0003-0919-3948; 
FU Natural Science Foundation of Xinjiang Uygur Autonomous Region
   [2019D01C062, 2019D01C041, 2019D01C205, 2020D01C028]; National Natural
   Science Foundation of China [12061071]; Higher Education of Xinjiang
   Uygur Autonomous Region [XJEDU2019Y006, XJEDU2020Y003]; Tianshan
   Innovation Team Plan Project of Xinjiang Uygur Autonomous Region
   [202101642]; SichuanRegional InnovationCooperation Project
   [2020YFQ0018]; National Social Science Foundation in China [20XGL029]
FX This work is supported by the Natural Science Foundation of Xinjiang
   Uygur Autonomous Region (Nos. 2019D01C062, 2019D01C041, 2019D01C205,
   2020D01C028), the National Natural Science Foundation of China (No.
   12061071), the Higher Education of Xinjiang Uygur Autonomous Region
   (XJEDU2019Y006, XJEDU2020Y003), Tianshan Innovation Team Plan Project of
   Xinjiang Uygur Autonomous Region under Grant (No. 202101642), and
   SichuanRegional InnovationCooperation Project (No. 2020YFQ0018), the
   National Social Science Foundation in China (No. 20XGL029).
CR Banerjee A, 2022, VISUAL COMPUT, V38, P321, DOI 10.1007/s00371-020-02017-x
   Brendel W., 2019, arXiv
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2011, PROC CVPR IEEE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Junejo IN, 2008, LECT NOTES COMPUT SC, V5302, P318, DOI 10.1007/978-3-540-88682-2_25
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Khan SH, 2014, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2014.249
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mo J, 2017, INT J COMPUT ASS RAD, V12, P2181, DOI 10.1007/s11548-017-1619-0
   Mohammadi SM, 2018, IEEE ENG MED BIO, P3501, DOI 10.1109/EMBC.2018.8513009
   Nielsen M, 2007, LECT NOTES COMPUT SC, V4522, P918
   Okabe T, 2009, IEEE I CONF COMP VIS, P1693
   Panagopoulos A, 2013, IEEE T PATTERN ANAL, V35, P437, DOI 10.1109/TPAMI.2012.110
   Panagopoulos A, 2009, PROC CVPR IEEE, P651, DOI 10.1109/CVPRW.2009.5206665
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tian JD, 2016, PATTERN RECOGN, V51, P85, DOI 10.1016/j.patcog.2015.09.006
   Vaswani A, 2017, ADV NEUR IN, V30
   Vicente TFY, 2015, IEEE I CONF COMP VIS, P3388, DOI 10.1109/ICCV.2015.387
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wu W, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103397
   Wu W, 2022, COMPUT VIS IMAGE UND, V216, DOI 10.1016/j.cviu.2021.103341
   Wu W, 2022, VISUAL COMPUT, V38, P1677, DOI 10.1007/s00371-021-02096-4
   Yao Quanming, 2020, INT C MACH LEARN
   Zhang Jing, 2020, P IEEECVF C COMPUTER, P12546, DOI DOI 10.1109/CVPR42600.2020.01256
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhou K, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103596
   Zhu J, 2010, ASIA S PACIF DES AUT, P220
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
NR 47
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6297
EP 6308
DI 10.1007/s00371-022-02730-9
EA DEC 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000903992800001
DA 2024-07-18
ER

PT J
AU Dyke, RM
   Hormann, K
AF Dyke, Roberto M.
   Hormann, Kai
TI Histogram equalization using a selective filter
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; Dequantization; Histogram equalization; Histogram
   matching
ID IMAGE-ENHANCEMENT; SPECIFICATION
AB Many popular modern image processing software packages implement a naive form of histogram equalization. This implementation is known to produce histograms that are not truly uniform. While exact histogram equalization techniques exist, these may produce undesirable artifacts in some scenarios. In this paper we consider the link between the established continuous theory for global histogram equalization and its discrete implementation, and we formulate a novel histogram equalization technique that builds upon and considerably improves the naive approach. We show that we can linearly interpolate the cumulative distribution of a low-bit image by approximately dequantizing its intensities using a selective box filter. This helps to distribute the intensities more evenly. The proposed algorithm is subsequently evaluated and compared with existing works in the literature. We find that the method is capable of producing an equalized histogram that has a high entropy, while distances between similar intensities are preserved. The described approach has implications on several related image processing problems, e.g., edge detection.
C1 [Dyke, Roberto M.; Hormann, Kai] Univ Svizzera Italiana, Fac Informat, Via Buffi 13, CH-6900 Lugano, Switzerland.
C3 Universita della Svizzera Italiana
RP Hormann, K (corresponding author), Univ Svizzera Italiana, Fac Informat, Via Buffi 13, CH-6900 Lugano, Switzerland.
EM kai.hormann@usi.ch
RI Dyke, Roberto/JPY-2995-2023
OI Dyke, Roberto/0000-0003-0361-422X; Hormann, Kai/0000-0001-6455-4246
FU Universita della Svizzera italiana
FX Open access funding provided by Universita della Svizzera italiana
CR ANDREWS HC, 1972, IEEE SPECTRUM, V9, P20, DOI 10.1109/MSPEC.1972.5218964
   Banterle F., 2006, Computer Graphics and Interactive Techniques in Australasia and Southeast Asia Association for Computing Machinery, P349, DOI 10.1145/1174429.1174489
   Bhagavathy S, 2009, IEEE T IMAGE PROCESS, V18, P1936, DOI 10.1109/TIP.2009.2022293
   Byun J, 2019, LECT NOTES COMPUT SC, V11362, P67, DOI 10.1007/978-3-030-20890-5_5
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen Q, 2015, PROC SPIE, V9404, DOI 10.1117/12.2080389
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Cheng CH, 2009, IEEE INT SYMP CIRC S, P944, DOI 10.1109/ISCAS.2009.5117913
   Choi HR, 2006, IEEE T CONSUM ELECTR, V52, P1099
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Coltuc D., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P150, DOI 10.1109/ICIP.1999.817089
   Coltuc D., 1998, 9 EUR SIGN PROC C EU, P1
   Daly S, 2004, PROC SPIE, V5292, P130, DOI 10.1117/12.526937
   Dhote K., 2015, INT J SCI RES IJSR, V4, P740
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Gonzalez R.C., 2018, Digital Image Processing
   HALL EL, 1974, IEEE T COMPUT, VC 23, P207, DOI 10.1109/T-C.1974.223892
   HALL EL, 1971, IEEE T COMPUT, VC 20, P1032, DOI 10.1109/T-C.1971.223399
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hou X., 2017, PREPRINT
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Javadi S, 2020, PROCEEDINGS OF THE 2020 3RD INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING (ICIGP 2020), P57, DOI 10.1145/3383812.3383830
   Jensen JohnR., 2004, INTRO DIGITAL IMAGE, V3rd
   Joy G, 1996, COMPUT GRAPH, V20, P231, DOI 10.1016/0097-8493(95)00098-4
   Kaur M, 2011, INT J ADV COMPUT SC, V2, P137
   Ketcham DJ, 1974, Image enhancement techniques for cockpit displays
   Kim SY, 1999, IEEE T CONSUM ELECTR, V45, P828, DOI 10.1109/30.793618
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Kite TD, 2000, IEEE T IMAGE PROCESS, V9, P1583, DOI 10.1109/83.862639
   Kong N.S.P., 2013, Int. J. Innov., Manag. Technol., V4, P386, DOI 10.7763/IJIMT.2013.V4.426
   Lee JW, 2006, IEEE T CONSUM ELECTR, V52, P179
   Liu C., 2018, PREPRINT
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Mese M, 2001, IEEE T IMAGE PROCESS, V10, P1566, DOI 10.1109/83.951541
   Mustafa WA, 2018, J PHYS CONF SER, V1019, DOI 10.1088/1742-6596/1019/1/012026
   Nikolova M, 2013, J MATH IMAGING VIS, V46, P309, DOI 10.1007/s10851-012-0401-8
   Nithyananda CR, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON DATA MINING AND ADVANCED COMPUTING (SAPIENCE), P150, DOI 10.1109/SAPIENCE.2016.7684156
   Ooi CH, 2009, IEEE T CONSUM ELECTR, V55, P2072, DOI 10.1109/TCE.2009.5373771
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Reinhard E., 2010, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting
   Robinson G.S., 1975, COMPUTER PROCESSING
   Son CH, 2014, IEEE T IMAGE PROCESS, V23, P2542, DOI 10.1109/TIP.2014.2319732
   Song Q, 2020, IEEE T CIRC SYST VID, V30, P2575, DOI 10.1109/TCSVT.2019.2928270
   Song Q, 2016, IEEE IMAGE PROC, P3299, DOI 10.1109/ICIP.2016.7532970
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tang JR, 2017, APPL SOFT COMPUT, V55, P31, DOI 10.1016/j.asoc.2017.01.053
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wan PF, 2016, IEEE T IMAGE PROCESS, V25, P2896, DOI 10.1109/TIP.2016.2553523
   Wang C, 2008, IET IMAGE PROCESS, V2, P249, DOI 10.1049/iet-ipr:20070198
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
NR 52
TC 4
Z9 4
U1 15
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6221
EP 6235
DI 10.1007/s00371-022-02723-8
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000890092700001
PM 37969935
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Ma, C
   He, T
   Gao, J
AF Ma, Chuang
   He, Tang
   Gao, Jun
TI Skin scar segmentation based on saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Skin scar segmentation; Saliency detection; Image segmentation; Gaussian
   pyramid
ID LESION SEGMENTATION; IMAGES; ATTENTION; MODEL
AB Effective segmentation of skin scars is an important part of judicial scar identification. The current evaluation procedure mainly focuses on visual examination, and the accuracy needs to be improved. This paper proposes an unsupervised scar segmentation method based on saliency detection, which can correctly extract scars in complex scar conditions. The image Gaussian pyramid feature map is extracted for feature extraction, and the clustering algorithm is used to fuse the contrast features and spatial features to establish the skin scar saliency map. Finally, the skin scar region is segmented through post-processing. The experimental results show that the precision of the proposed method is improved by 18.4% over the suboptional method compared with seven unsupervised methods on private datasets provided by a local forensic department, which proves that the proposed method supports the optimization and improvement of skin scar segmentation.
C1 [Ma, Chuang; He, Tang] Chongqing Univ Posts & Telecommun, Sch Software Engn, Chongqing 400065, Peoples R China.
   [Gao, Jun] Chongqing Publ Secur Bur, Nanan Dist Branch, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Gao, J (corresponding author), Chongqing Publ Secur Bur, Nanan Dist Branch, Chongqing 400065, Peoples R China.
EM gaojunerick12388@163.com
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ahn E, 2017, IEEE J BIOMED HEALTH, V21, P1685, DOI 10.1109/JBHI.2017.2653179
   Ahn E, 2015, IEEE ENG MED BIO, P3009, DOI 10.1109/EMBC.2015.7319025
   Bi L, 2016, I S BIOMED IMAGING, P1059, DOI 10.1109/ISBI.2016.7493448
   Celebi ME, 2008, SKIN RES TECHNOL, V14, P347, DOI 10.1111/j.1600-0846.2008.00301.x
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Dhane DM, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0554-x
   Fan HD, 2017, COMPUT BIOL MED, V85, P75, DOI 10.1016/j.compbiomed.2017.03.025
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JAIN AK, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P14, DOI [10.1109/ICSMC.1990.142050, 10.1016/0031-3203(91)90143-S]
   Jian MW, 2018, J VIS COMMUN IMAGE R, V57, P1, DOI 10.1016/j.jvcir.2018.10.008
   Jiang FL, 2021, COGN COMPUT, V13, P69, DOI 10.1007/s12559-020-09724-6
   Jones TD, 2000, IEEE T MED IMAGING, V19, P1202, DOI 10.1109/42.897812
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kaymak R, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113742
   Kumar M, 2020, MOBILE NETW APPL, V25, P1319, DOI 10.1007/s11036-020-01550-2
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Liu GH, 2019, IEEE T IMAGE PROCESS, V28, P6, DOI 10.1109/TIP.2018.2847422
   Liu GH, 2015, PATTERN RECOGN, V48, P2554, DOI 10.1016/j.patcog.2015.02.005
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Ma Z, 2016, IEEE J BIOMED HEALTH, V20, P615, DOI 10.1109/JBHI.2015.2390032
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mukherjee Subhayan, 2020, Smart Multimedia. Second International Conference, ICSM 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12015), P87, DOI 10.1007/978-3-030-54407-2_8
   Olugbara OO, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/1524286
   Patiño D, 2018, LECT NOTES COMPUT SC, V11073, P728, DOI 10.1007/978-3-030-00937-3_83
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Silveira M, 2009, IEEE J-STSP, V3, P35, DOI 10.1109/JSTSP.2008.2011119
   Tajbakhsh N, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101693
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wu HS, 2021, IEEE T MED IMAGING, V40, P357, DOI 10.1109/TMI.2020.3027341
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang KF, 2016, IEEE T IMAGE PROCESS, V25, P3475, DOI 10.1109/TIP.2016.2572600
   Yuan YC, 2018, IEEE T IMAGE PROCESS, V27, P1311, DOI 10.1109/TIP.2017.2762422
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zhou HY, 2013, COMPUT VIS IMAGE UND, V117, P1004, DOI 10.1016/j.cviu.2012.11.015
NR 48
TC 1
Z9 1
U1 6
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4887
EP 4899
DI 10.1007/s00371-022-02635-7
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000837917300001
DA 2024-07-18
ER

PT J
AU Li, ZQ
   Shen, XK
   Hu, Y
   Zhou, XY
AF Li, Zhiqiang
   Shen, Xukun
   Hu, Yong
   Zhou, Xueyang
TI High-resolution SVBRDF estimation based on deep inverse rendering from
   two-shot images
SO VISUAL COMPUTER
LA English
DT Article
DE Material capture; Appearance capture; SVBRDF; Deep learning
ID SURFACE; REPRESENTATION
AB The surface of a large number of objects in the real world shows the characteristics of stationary material. These appearances usually frequently exhibit the same or similar reflectance properties in different locations, while recent methods based on deep learning can estimate the surface reflectance properties from one or multiple photographs. For stationary materials, these lightweight methods do not use the potential reflectance information to recover plausible material properties from fewer images or produce better results from the same number of images. Moreover, directly using high-resolution images to optimize models greatly increases GPU consumption. In this paper, we present a pipeline capable of reconstructing high-resolution material properties from two images taken with the flash turned on or turned off. To reduce the number of captures to two, we utilize the stationary feature to generate multiple observations of the same small region. The new observations and original observation are used to estimate the higher-quality reflectance properties of this area. We then map the estimated reflectance properties to the high-resolution appearance parameters. Optimizing the model using only small area images reduces GPU consumption. Furthermore, we use a high-resolution flash image to estimate initial high-resolution reflectance parameter maps, crop the same location area from the initial results to initialize the auto-encoder. In addition, we leverage the jump rearrangement to reduce artifacts of high-resolution results. For reconstruction, two refinements that can reintroduce details are indispensable. We demonstrate and evaluate our method on the synthetic and real data.
C1 [Shen, Xukun; Hu, Yong] Beihang Univ, Yunnan Innovat Inst, Kunming 650233, Yunnan, Peoples R China.
   [Li, Zhiqiang; Shen, Xukun; Hu, Yong; Zhou, Xueyang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Shen, Xukun; Hu, Yong] Beihang Univ, Sch New Media Art & Design, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University; Beihang University
RP Shen, XK (corresponding author), Beihang Univ, Yunnan Innovat Inst, Kunming 650233, Yunnan, Peoples R China.; Shen, XK (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Shen, XK (corresponding author), Beihang Univ, Sch New Media Art & Design, Beijing 100191, Peoples R China.
EM lzqiang12@buaa.edu.cn; xkshen@buaa.edu.cn; huyong@buaa.edu.cn;
   zhouxueyang@buaa.edu.cn
FU Beihang University Yunnan Innovation Institute Yunding Technology Plan
   of Yunnan Provincial Key R D Program [202103AN080001-003]; National
   Natural Science Foundation of China [U19A2063]
FX This work was supported by Beihang University Yunnan Innovation
   Institute Yunding Technology Plan (2021) of Yunnan Provincial Key R &D
   Program (No. 202103AN080001-003) and the National Natural Science
   Foundation of China (No. U19A2063)
CR Aittala M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925917
   Aittala M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766967
   [Anonymous], 2017, ACM Transactions on Graphics (ToG), DOI DOI 10.1145/3072959.3073641
   Asselin LP, 2020, INT CONF 3D VISION, P1157, DOI 10.1109/3DV50981.2020.00126
   da Silva Nunes M., 2021, VISUAL COMPUT, V38, P1
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deschaintre V, 2020, COMPUT GRAPH FORUM, V39, P91, DOI 10.1111/cgf.14056
   Deschaintre V, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13765
   Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378
   Dong Y, 2019, VIS INFORM, V3, P59, DOI 10.1016/j.visinf.2019.07.003
   Dong Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661283
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Gatys L., 2015, NIPS
   Goldman DB, 2010, IEEE T PATTERN ANAL, V32, P1060, DOI 10.1109/TPAMI.2009.102
   Guo Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417779
   Karis B., 2013, Proc. Physically Based Shading Theory Practice, V4, P1
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Lawrence J, 2006, ACM T GRAPHIC, V25, P735, DOI 10.1145/1141911.1141949
   Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255
   Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5
   Li Zi-xin, 2018, Advanced Technology of Electrical Engineering and Energy, V37, P1, DOI 10.12067/ATEEE1712020
   Matusik W., 2003, PhD thesis
   Nicodemus F. E., 1978, Geometrical considerations and nomenclature for reflectance
   Palma G, 2012, COMPUT GRAPH FORUM, V31, P1491, DOI 10.1111/j.1467-8659.2012.03145.x
   Riviere J, 2016, COMPUT GRAPH FORUM, V35, P191, DOI 10.1111/cgf.12719
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Sole A, 2021, VISUAL COMPUT, V37, P2207, DOI 10.1007/s00371-020-01980-9
   TROWBRIDGE TS, 1975, J OPT SOC AM, V65, P531, DOI 10.1364/JOSA.65.000531
   Walter B., 2007, EUROGRAPHICS C RENDE
   Ye WJ, 2018, COMPUT GRAPH FORUM, V37, P201, DOI 10.1111/cgf.13560
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 32
TC 1
Z9 1
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4609
EP 4622
DI 10.1007/s00371-022-02612-0
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000834687900001
DA 2024-07-18
ER

PT J
AU Su, GD
   Chang, CC
AF Su, Guo-Dong
   Chang, Ching-Chun
TI Toward high-capacity crypto-domain reversible data hiding with
   huffman-based lossless image coding
SO VISUAL COMPUTER
LA English
DT Article
DE Reversible data hiding; JPEG-like compression; Huffman coding; Encrypted
   images
ID PREDICTION-ERROR EXPANSION; ENCRYPTED IMAGES; SCHEME; TRANSFORM
AB The rapid development of the Internet of Things has opened up commercial and economic opportunities, but there are also risks and threats to cybersecurity: a significant amount of private data is exposed in an untrusted cyberspace. In view of this issue, the research on reversible data hiding in encrypted image (RDHEI) aims to establish a secure and private communication channel between a sender and a recipient by embedding data into encrypted images. Therefore, this paper proposes a novel RDHEI scheme that employs a Huffman-based lossless image coding technique to achieve a high embedding capacity. Specifically, we compress the original image into a bitstream through the JPEG-like compression and then further encode the difference values between the reconstructed JPEG image and the original image via the Huffman coding. Due to the high compression rate and coding efficiency of the combined strategy, a large embedding space can be created. Our proposed scheme not only ensures a lossless recovery of the original image but also an error-free extraction of the secret messages. Several experiments are conducted to verify that our proposed scheme achieves a state-of-the-art performance compared with the prior art in terms of embedding capacity.
C1 [Su, Guo-Dong] Fujian Polytech Normal Univ, Sch Big Data & Artificial Intelligence, Fuzhou 350300, Peoples R China.
   [Su, Guo-Dong] Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung 40724, Taiwan.
   [Chang, Ching-Chun] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
C3 Fujian Polytechnic Normal University; Feng Chia University; University
   of Warwick
RP Chang, CC (corresponding author), Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
EM gdsu0206@gmail.com; ching-chun.chang@warwickgrad.net
FU Open Fund of Engineering Research Center for ICH Digitalization and
   Multi-source Information Fusion of Fujian Province [FJ-ICH201901];
   Natural Science Foundation of Fujian Province [2022J01974]
FX This work was supported in part by the Open Fund of Engineering Research
   Center for ICH Digitalization and Multi-source Information Fusion of
   Fujian Province under Grant No. FJ-ICH201901, and in part by the Natural
   Science Foundation of Fujian Province under Grant No. 2022J01974.
CR [Anonymous], 2020, BOSSBASE DATASET
   [Anonymous], 1992, 10918 DIS
   [Anonymous], 2020, BOWS 2 DATASET
   Chang CC, 2020, MULTIMED TOOLS APPL, V79, P24795, DOI 10.1007/s11042-020-09132-w
   Chang CC, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/5538720
   Chang CC, 2020, IEEE ACCESS, V8, P198425, DOI 10.1109/ACCESS.2020.3034936
   Chang CC, 2019, MATH BIOSCI ENG, V16, P3367, DOI 10.3934/mbe.2019168
   Chang CC, 2019, IEEE ACCESS, V7, P54117, DOI 10.1109/ACCESS.2019.2908924
   Chang CC, 2018, IEEE ACCESS, V6, P70720, DOI 10.1109/ACCESS.2018.2880904
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Grosse HJ, 2000, PATTERN RECOGN LETT, V21, P1061, DOI 10.1016/S0167-8655(00)00065-9
   Gui XL, 2014, SIGNAL PROCESS, V98, P370, DOI 10.1016/j.sigpro.2013.12.005
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Huang DL, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115632
   Huang LC, 2013, J SYST SOFTWARE, V86, P716, DOI 10.1016/j.jss.2012.11.024
   Jia YJ, 2019, SIGNAL PROCESS, V163, P238, DOI 10.1016/j.sigpro.2019.05.020
   Kim HJ, 2008, IEEE T INF FOREN SEC, V3, P456, DOI 10.1109/TIFS.2008.924600
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Li XL, 2011, IEEE T IMAGE PROCESS, V20, P3524, DOI 10.1109/TIP.2011.2150233
   Liu ZL, 2022, IEEE T DEPEND SECURE, V19, P1382, DOI 10.1109/TDSC.2020.3011838
   Liu ZL, 2018, INFORM SCIENCES, V433, P188, DOI 10.1016/j.ins.2017.12.044
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Mohammadi A, 2020, IEEE T CIRC SYST VID, V30, P2366, DOI 10.1109/TCSVT.2020.2990952
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qin C, 2019, INFORM SCIENCES, V487, P176, DOI 10.1016/j.ins.2019.03.008
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Shiu PF, 2019, SIGNAL PROCESS-IMAGE, V74, P64, DOI 10.1016/j.image.2019.01.003
   Su GD, 2020, IET IMAGE PROCESS, V14, P4633, DOI 10.1049/iet-ipr.2019.1694
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang P, 2020, IEEE ACCESS, V8, P28902, DOI 10.1109/ACCESS.2020.2972622
   Wang YM, 2021, IEEE T MULTIMEDIA, V23, P1466, DOI 10.1109/TMM.2020.2999187
   Wu Y., 2021, IEEE TRANSA INFORM F, V16, P2445, DOI [10.1109/TIFS.2021.3055630, DOI 10.1109/TIFS.2021.3055630]
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhenxing Qian, 2018, IEEE Transactions on Dependable and Secure Computing, V15, P1055, DOI 10.1109/TDSC.2016.2634161
NR 43
TC 3
Z9 3
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4623
EP 4638
DI 10.1007/s00371-022-02613-z
EA JUL 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000829815500001
DA 2024-07-18
ER

PT J
AU Zhang, T
   Cao, YH
   Zhang, L
   Li, X
AF Zhang, Tao
   Cao, Yahui
   Zhang, Le
   Li, Xuan
TI Efficient feature fusion network based on center and scale prediction
   for pedestrian detection
SO VISUAL COMPUTER
LA English
DT Article
DE Pedestrian detection; Convolutional neural network; Feature fusion;
   Center and scale prediction
AB Center and scale prediction (CSP) is an anchor-free pedestrian detector with good performance. However, there are lots of parameters in the detector, which seriously limits the speed. In this paper, a new network is designed for the improvement of the detector speed, which contains less parameters, named Feature Fusion: Center and Scale Prediction (F-CSP). F-CSP fuses multi-scale feature maps with two efficient feature fusion networks: Feature Pyramid Networks (FPN) and Balanced Feature Pyramid (BFP). Specifically, FPN is used to reduce the channel of feature maps, and BFP is used to fuse multiple feature maps into a single one. This way, the proposed detector achieves competitive accuracy and higher speed on the challenging pedestrian detection benchmark. The performance of F-CSP is demonstrated on the Caltech dataset. Compared with CSP, under the premise of ensuring accuracy, the speed is increased from 45.1 to 32.9 ms/img.
C1 [Zhang, Tao; Cao, Yahui; Zhang, Le; Li, Xuan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Li, X (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM zhangtao@tju.edu.cn; caoyahui@tju.edu.cn; Polatis963@163.com;
   lixuantju@tju.edu.cn
CR Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Ballit A, 2022, VISUAL COMPUT, V38, P919, DOI 10.1007/s00371-021-02059-9
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Fan XM, 2020, VISUAL COMPUT, V36, P1203, DOI 10.1007/s00371-019-01732-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He Z, 2020, VISUAL COMPUT, V36, P1157, DOI 10.1007/s00371-019-01724-4
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kaiwen Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P399, DOI 10.1007/978-3-030-58580-8_24
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YL, 2016, PATTERN RECOGN, V55, P58, DOI 10.1016/j.patcog.2016.01.030
   Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639
   Moya S, 2013, VISUAL COMPUT, V29, P795, DOI 10.1007/s00371-013-0831-3
   Musse SR, 2021, VISUAL COMPUT, V37, P3077, DOI 10.1007/s00371-021-02252-w
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rothe R, 2015, LECT NOTES COMPUT SC, V9003, P290, DOI 10.1007/978-3-319-16865-4_19
   Sherstyuk A, 2011, VISUAL COMPUT, V27, P173, DOI 10.1007/s00371-010-0516-0
   Shu Chang, 2011, Tsinghua Science and Technology, V16, P216, DOI 10.1016/S1007-0214(11)70032-3
   Silveira R, 2010, VISUAL COMPUT, V26, P1183, DOI 10.1007/s00371-009-0399-0
   Singh VK, 2020, VISUAL COMPUT, V36, P1423, DOI 10.1007/s00371-019-01750-2
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang B, 2020, VISUAL COMPUT, V36, P1897, DOI 10.1007/s00371-019-01779-3
   Wang S., 2018, DEEP REINFORCEMENT L
   Xu JW, 2022, VISUAL COMPUT, V38, P3627, DOI 10.1007/s00371-021-02192-5
   Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
NR 40
TC 5
Z9 5
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3865
EP 3872
DI 10.1007/s00371-022-02528-9
EA JUL 2022
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000828445600002
DA 2024-07-18
ER

PT J
AU Chang, Y
   Peng, T
   Yu, F
   He, RH
   Hu, XR
   Liu, JP
   Zhang, ZL
   Jiang, MH
AF Chang, Yuan
   Peng, Tao
   Yu, Feng
   He, Ruhan
   Hu, Xinrong
   Liu, Junping
   Zhang, Zili
   Jiang, Minghua
TI VTNCT: an image-based virtual try-on network by combining feature with
   pixel transformation
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual try-on; Image-based; Feature transformation; Occlusion handling
AB Image-based virtual try-on tasks with the goal of transferring a target clothing item onto the corresponding region of a person have attracted increasing research attention recently. However, most of the existing image-based virtual try-on methods have a shortcoming in detail generation and preservation. To resolve these issues, we propose a novel virtual try-on network to generate photo-realistic try-on image while preserving the details of clothes and non-target regions. We introduce two key innovations. One is the clothing warping module, which uses a warping strategy combining feature with pixel transformation to obtain the warped clothes with realistic texture and robust alignment. The other is the arm generation module, which is an original module and is highly effective for dealing with occlusion and generating the details of the arm region. In addition, we use a distillation strategy to solve the degeneration caused by the wrong parsing, which further proves the effectiveness of our components. Extensive experiments on a public fashion dataset demonstrate our system achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively. The code is available at https://github.com/changyuan96/VTNCT.
C1 [Chang, Yuan; Peng, Tao; Yu, Feng; He, Ruhan; Hu, Xinrong; Jiang, Minghua] Hubei Prov Engn Res Ctr Intelligent Text & Fash, Wuhan 430200, Peoples R China.
   [Chang, Yuan; Peng, Tao; Yu, Feng; Liu, Junping; Zhang, Zili] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.
   [Chang, Yuan; Peng, Tao; Yu, Feng; He, Ruhan; Hu, Xinrong; Liu, Junping; Zhang, Zili; Jiang, Minghua] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
C3 Wuhan Textile University
RP Peng, T (corresponding author), Hubei Prov Engn Res Ctr Intelligent Text & Fash, Wuhan 430200, Peoples R China.; Peng, T (corresponding author), Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.; Peng, T (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
EM pt@wtu.edu.cn
RI Hu, Xinrong/HGA-1351-2022; Yu, Feng/U-9998-2019
OI Yu, Feng/0000-0003-1913-2882; Chang, Yuan/0000-0001-6026-6191
FU Science Foundation of Hubei [2014CFB764]; Department of Education of the
   Hubei Province of China [Q20131608]; Engineering Research Center of
   Hubei Province for Clothing Information
FX This work is supported in part by the Science Foundation of Hubei under
   Grant No.2014CFB764 and Department of Education of the Hubei Province of
   China under Grant No.Q20131608, and Engineering Research Center of Hubei
   Province for Clothing Information.
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chang Y, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2295, DOI 10.1109/ICASSP39728.2021.9414874
   Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Guo JX, 2018, AAAI CONF ARTIF INTE, P5141
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Kingma D. P., 2014, arXiv
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Minar Matiur Rahman, 2020, C COMPUTER VISION PA, P11
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   PONSMOLL G, 2017, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/3072959.3073711
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reed S, 2016, PR MACH LEARN RES, V48
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Shin J., 2018, ARXIV PREPRINT ARXIV
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yu L, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI 10.14722/ndss.2017.23241
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhang YC, 2017, PR MACH LEARN RES, V70
   Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 4
Z9 4
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2583
EP 2596
DI 10.1007/s00371-022-02480-8
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000785702200003
DA 2024-07-18
ER

PT J
AU Nozaripour, A
   Soltanizadeh, H
AF Nozaripour, Ali
   Soltanizadeh, Hadi
TI Image classification via convolutional sparse coding
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional sparse coding; Sparse representation; Image
   classification; Filters; Feature map
ID FACE RECOGNITION; K-SVD; DISCRIMINATIVE DICTIONARY; REPRESENTATION;
   RECONSTRUCTION; ALGORITHM
AB The Convolutional Sparse Coding (CSC) model has recently attracted a lot of attention in the signal and image processing communities. Since, in traditional sparse coding methods, a significant assumption is that all input samples are independent, so it is not well for most dependent works. In such cases, CSC models are a good choice. In this paper, we proposed a novel CSC-based classification model which combines the local block coordinate descent (LoBCoD) algorithm with the classification strategy. For this, in the training phase, the convolutional dictionary atoms (filters) of each class are learned by all training samples of the same class. In the test phase, the label of the query sample can be determined based on the reconstruction error of the filters related to every subject. Experimental results on five benchmark databases at the different number of training samples clearly demonstrate the superiority of our method to many state-of-the-art classification methods. Besides, we have shown that our method is less dependent on the number of training samples and therefore it can better work than other methods in small databases with fewer samples. For instance, increases of 26.27%, 18.32%, 11.35%, 13.5%, and 19.3% in recognition rates are observed for our method when compared to conventional SRC for five used databases at the least number of training samples per class.
C1 [Nozaripour, Ali] Semnan Univ, Dept Elect Comp Engn, Semnan 3513119111, Iran.
   [Soltanizadeh, Hadi] Semnan Univ, Fac Elect Comp Engn, Semnan 3513119111, Iran.
C3 Semnan University; Semnan University
RP Soltanizadeh, H (corresponding author), Semnan Univ, Fac Elect Comp Engn, Semnan 3513119111, Iran.
EM A_Nozari@semnan.ac.ir; h_Soltanizadeh@semnan.ac.ir
RI Soltanizadeh, Hadi/AAH-1840-2019
OI Soltanizadeh, Hadi/0000-0002-2210-675X
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   An FP, 2020, SOFT COMPUT, V24, P16967, DOI 10.1007/s00500-020-04989-3
   Bai XF, 2013, IEICE T INF SYST, VE96D, P387, DOI 10.1587/transinf.E96.D.387
   Lahaw ZB, 2022, VISUAL COMPUT, V38, P2431, DOI 10.1007/s00371-021-02121-6
   Benrhouma O, 2016, MULTIMED TOOLS APPL, V75, P8695, DOI 10.1007/s11042-015-2786-z
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Chen BH, 2016, IEEE IMAGE PROC, P1918, DOI 10.1109/ICIP.2016.7532692
   CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Ding ZM, 2016, LECT NOTES COMPUT SC, V9910, P567, DOI 10.1007/978-3-319-46466-4_34
   Fan CD, 2019, VISUAL COMPUT, V35, P1797, DOI 10.1007/s00371-018-1603-x
   Foroughi H, 2018, IEEE T IMAGE PROCESS, V27, P806, DOI 10.1109/TIP.2017.2766446
   Gai S, 2019, VISUAL COMPUT, V35, P109, DOI 10.1007/s00371-017-1456-8
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goel N, 2005, PROC SPIE, V5779, P426, DOI 10.1117/12.605553
   Gou JP, 2020, NEURAL NETWORKS, V125, P104, DOI 10.1016/j.neunet.2020.01.020
   Grosse R., 2012, ARXIV PREPRINT ARXIV
   Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212
   He J, 2021, SLEEP BREATH, V25, P2213, DOI 10.1007/s11325-021-02350-y
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hu JL, 2018, PATTERN RECOGN, V75, P282, DOI 10.1016/j.patcog.2017.02.009
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Jin JW, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION, CYBERNETICS AND COMPUTATIONAL SOCIAL SYSTEMS (ICCSS), P137, DOI 10.1109/ICCSS.2017.8091400
   Li ZM, 2020, IEEE T NEUR NET LEAR, V31, P786, DOI 10.1109/TNNLS.2019.2910146
   Liao HW, 2018, IEEE SIGNAL PROC LET, V25, P1156, DOI 10.1109/LSP.2018.2847236
   Liu L, 2020, VISUAL COMPUT, V36, P1521, DOI 10.1007/s00371-019-01746-y
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5
   Martinez A., 1998, AR FACE DATABASE
   Mokhayeri F, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107129
   Nozaripour A., 2021, J AI DATA MIN
   Papyan V, 2017, IEEE I CONF COMP VIS, P5306, DOI 10.1109/ICCV.2017.566
   Parvasideh P, 2021, ADV DATA ANAL CLASSI, V15, P575, DOI 10.1007/s11634-020-00417-4
   Pour AN, 2015, INT J APPL PATTERN R, V2, P111, DOI 10.1504/IJAPR.2015.069540
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Shazeeda S, 2019, IET BIOMETRICS, V8, P49, DOI 10.1049/iet-bmt.2018.5130
   Vu TH, 2017, IEEE T IMAGE PROCESS, V26, P5160, DOI 10.1109/TIP.2017.2729885
   Wang B, 2017, VISUAL COMPUT, V33, P647, DOI 10.1007/s00371-016-1215-2
   Wang LM, 2021, MMPT '21: PROCEEDINGS OF THE 2021 WORKSHOP ON MULTI-MODAL PRE-TRAINING FOR MULTIMEDIA UNDERSTANDING, P1, DOI 10.1145/3463945.3468169
   Wang XY, 2019, PROCEEDINGS OF 2019 THE 3RD INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY (ICCSP 2019) WITH WORKSHOP 2019 THE 4TH INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2019), P179, DOI 10.1145/3309074.3309106
   Wohlberg B, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6854992
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu Y, 2017, IEEE ACCESS, V5, P8502, DOI 10.1109/ACCESS.2017.2695239
   Yang JZ, 2021, MEASUREMENT, V175, DOI 10.1016/j.measurement.2021.109104
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Yuksel A, 2011, IET COMPUT VIS, V5, P398, DOI 10.1049/iet-cvi.2010.0175
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zeng SN, 2022, IEEE T CYBERNETICS, V52, P4935, DOI 10.1109/TCYB.2020.3025757
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhao L, 2016, VISUAL COMPUT, V32, P1165, DOI 10.1007/s00371-015-1169-9
   Zhao Z, 2021, SOFT COMPUT, V25, P7627, DOI 10.1007/s00500-021-05723-3
   Zheng H, 2015, NEUROCOMPUTING, V162, P9, DOI 10.1016/j.neucom.2015.03.071
   Zhu YY, 2015, IEEE T PATTERN ANAL, V37, P529, DOI 10.1109/TPAMI.2013.2295311
   Zisselman E, 2019, PROC CVPR IEEE, P8200, DOI 10.1109/CVPR.2019.00840
NR 56
TC 3
Z9 3
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1731
EP 1744
DI 10.1007/s00371-022-02441-1
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781250600003
DA 2024-07-18
ER

PT J
AU Jia, ZQ
   Li, Y
   Tan, ZF
   Wang, WC
   Wang, ZG
   Yin, GQ
AF Jia, Zhaoqian
   Li, Ye
   Tan, Zhuofu
   Wang, Wenchao
   Wang, Zhiguo
   Yin, Guangqiang
TI Domain-invariant feature extraction and fusion for cross-domain person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person ReID; Cross-domain; Feature fusion; Domain-invariant feature
   extraction
ID ATTRIBUTE; MODEL
AB There are notable style differences among person re-identification (ReID) datasets, such as brightness, tone, resolution, background, and clothing style, that result in serious challenges for cross-domain person ReID. Two methods are usually used to solve these problems. One is to remove style differences between datasets by applying specific modules, such as instance normalization (IN). However, this method will filter out large amounts of valuable information for ReID. The other is to use person attributes as auxiliary information, but this method does not deeply explore the relationship between attribute features and global features, resulting in underutilized attribute information. We propose the domain-invariant feature extraction and fusion (DFEF), which consists of the attention and style normalization (ASN) and the attribute feature extraction and fusion (AFEF). The ASN module integrates spatial and channel attention modules on the basis of the IN layer to effectively remove the style differences between datasets and recovers the filtered-out information, which is useful for ReID. The AFEF module includes the attribute branch and the feature fusion module. For the attribute branch, we embed the convolutional block attention module (CBAM) into the attribute branch and adopt the multi-label focal loss (MLFL) to improve the accuracy of attribute recognition. For the feature fusion module, we propose the dispersion reweighting strategy to explore the correlation between attribute features and global features. The proposed DFEF method achieves 30.1% and 35.0% mAP on Market-1501 -> DukeMTMC-reID and DukeMTMC-reID -> Market-1501, respectively.
C1 [Jia, Zhaoqian; Li, Ye; Tan, Zhuofu; Wang, Wenchao; Wang, Zhiguo; Yin, Guangqiang] UESTC, Qingshuihe Campus 2006,Xiyuan Ave,West Hi tech Zo, Chengdu, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Yin, GQ (corresponding author), UESTC, Qingshuihe Campus 2006,Xiyuan Ave,West Hi tech Zo, Chengdu, Sichuan, Peoples R China.
EM yingq@uestc.edu.cn
RI jia, zq/IAN-9752-2023
CR Chang XB, 2019, AAAI CONF ARTIF INTE, P3288
   Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chen ZC, 2021, VISUAL COMPUT, V37, P685, DOI 10.1007/s00371-020-01880-y
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Fan X., 2020, VISUAL COMPUT, V38, P1
   Franco A, 2017, PATTERN RECOGN, V61, P593, DOI 10.1016/j.patcog.2016.07.013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jia J., 2019, ARXIV PREPRINT ARXIV
   Jia J., 2019, FRUSTRATINGLY EASY P
   Kumar Devinder, 2019, ARXIV190712016
   Layne R., 2014, P BRIT MACH VIS C
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Y, 2020, IEEE ACCESS, V8, P164570, DOI 10.1109/ACCESS.2020.3010435
   Liang W., 2018, ARXIV181103768
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu X, 2012, PATTERN RECOGN, V45, P4204, DOI 10.1016/j.patcog.2012.05.019
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Peng PX, 2016, LECT NOTES COMPUT SC, V9908, P336, DOI 10.1007/978-3-319-46493-0_21
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002
   Su C, 2018, PATTERN RECOGN, V75, P77, DOI 10.1016/j.patcog.2017.07.005
   Su C, 2017, PATTERN RECOGN, V66, P4, DOI 10.1016/j.patcog.2017.01.006
   Ulyanov Dmitry, 2016, arXiv
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yin Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1100
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 42
TC 10
Z9 10
U1 1
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1205
EP 1216
DI 10.1007/s00371-022-02398-1
EA MAR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000767903700003
DA 2024-07-18
ER

PT J
AU Pai, AK
   Chandrahasan, P
   Raghavendra, U
   Karunakar, AK
AF Pai, Abhilash K.
   Chandrahasan, Prahaladh
   Raghavendra, U.
   Karunakar, A. K.
TI Motion pattern-based crowd scene classification using histogram of
   angular deviations of trajectories
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd behaviour analysis; Crowd collectiveness; Crowd scene analysis;
   Machine learning; Video surveillance
ID BEHAVIOR
AB Automated crowd behaviour analysis and monitoring is a challenging task due to the unpredictable nature of the crowd within a particular scene and across different scenes. The prior knowledge of the type of scene under consideration is a crucial mid-level information, which could be utilized to develop robust crowd behaviour analysis systems. In this paper, we propose an approach to automatically detect the type of a crowded scene based on the global motion patterns of the objects within the scene. Three different types of scenes whose global motion pattern characteristics vary from uniform to non-uniform are considered in this work, namely structured, semi-structured, and unstructured scenes, respectively. To capture the global motion pattern characteristics of an input crowd scene, we first extract the motion information in the form of trajectories using a key-point tracker and then compute the average angular orientation feature of each trajectory. This paper utilizes these angular features to introduce a novel feature vector, termed as Histogram of Angular Deviations (HAD), which depicts the distribution of the pair-wise angular deviation values for each trajectory vector. Since angular deviation information is resistant to changes in scene perspectives, we consider it as a key feature for distinguishing the scene types. To evaluate the effectiveness of the proposed HAD-based feature vector in classifying the crowded scenes, we build a crowd scene classification model by training the classical machine learning algorithms on the publicly available Collective Motion Database. The experimental results demonstrate the superior crowd classification performance of the proposed approach as compared to the existing methods. In addition to this, we propose a technique based on quantizing the angular deviation values to reduce the feature dimension and subsequently introduce a novel crowd scene structuredness index to quantify the structuredness of an input crowded scene based on its HAD.
C1 [Pai, Abhilash K.; Karunakar, A. K.] Manipal Acad Higher Educ, Manipal Inst Technol, Dept Comp Applicat, Manipal 576104, Karnataka, India.
   [Chandrahasan, Prahaladh] Manipal Acad Higher Educ, Manipal Inst Technol, Dept Informat & Commun Technol, Manipal 576104, Karnataka, India.
   [Raghavendra, U.] Manipal Acad Higher Educ, Manipal Inst Technol, Dept Instrumentat & Control Engn, Manipal 576104, Karnataka, India.
C3 Manipal Academy of Higher Education (MAHE); Manipal Academy of Higher
   Education (MAHE); Manipal Academy of Higher Education (MAHE)
RP Karunakar, AK (corresponding author), Manipal Acad Higher Educ, Manipal Inst Technol, Dept Comp Applicat, Manipal 576104, Karnataka, India.
EM karunakar.ak@manipal.edu
RI Raghavendra, U/G-8634-2015
OI A K, Karunakar/0000-0002-2458-3891; K Pai, Abhilash/0000-0001-7218-0475
FU Manipal Academy of Higher Education, Manipal
FX Open access funding provided by ManipalAcademy ofHigher Education,
   Manipal.
CR Afiq AA, 2019, J VIS COMMUN IMAGE R, V58, P285, DOI 10.1016/j.jvcir.2018.11.035
   Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Cheng Y, 2019, IEEE IMAGE PROC, P2429, DOI [10.1109/icip.2019.8803324, 10.1109/ICIP.2019.8803324]
   DUDANI SA, 1976, IEEE T SYST MAN CYB, V6, P327
   Farooq MU, 2022, VISUAL COMPUT, V38, P1553, DOI 10.1007/s00371-021-02088-4
   Gao G., 2020, ARXIV
   Haghani M, 2018, TRANSPORT RES B-METH, V107, P253, DOI 10.1016/j.trb.2017.06.017
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Lamba S., 2019, VISUAL COMPUT, P1
   Le Bon Gustave, 1897, The Crowd: A Study of the Popular Mind
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li XL, 2020, IEEE T IMAGE PROCESS, V29, P5571, DOI 10.1109/TIP.2020.2985284
   Li XL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2854000
   Lu W, 2017, NEUROCOMPUTING, V247, P213, DOI 10.1016/j.neucom.2017.03.074
   Sánchez FL, 2020, INFORM FUSION, V64, P318, DOI 10.1016/j.inffus.2020.07.008
   Olson D.L., 2008, ADV DATA MINING TECH, DOI https://doi.org/10.1007/978-3-540-76917-0
   Pai ABK, 2020, IEEE ACCESS, V8, P145984, DOI 10.1109/ACCESS.2020.3015375
   Raafat RM, 2009, TRENDS COGN SCI, V13, P420, DOI 10.1016/j.tics.2009.08.002
   Ren WY., 2015, ARXIV PREPRINT ARXIV
   Rodriguez M, 2009, IEEE I CONF COMP VIS, P1389, DOI 10.1109/ICCV.2009.5459301
   Roy A, 2019, IEEE REGION 10 SYMP, P102, DOI [10.1109/TENSYMP46218.2019.8971252, 10.1109/tensymp46218.2019.8971252]
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Shao J, 2017, IEEE T CIRC SYST VID, V27, P1290, DOI 10.1109/TCSVT.2016.2539878
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Vicsek T, 2012, PHYS REP, V517, P71, DOI 10.1016/j.physrep.2012.03.004
   Wang Q, 2017, AAAI CONF ARTIF INTE, P4292
   Wu S, 2017, MULTIMED TOOLS APPL, V76, P20167, DOI 10.1007/s11042-017-4568-2
   Zhao WQ, 2018, PATTERN RECOGN, V75, P112, DOI 10.1016/j.patcog.2017.06.020
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
   Zitouni MS, 2016, NEUROCOMPUTING, V186, P139, DOI 10.1016/j.neucom.2015.12.070
NR 35
TC 3
Z9 3
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 557
EP 567
DI 10.1007/s00371-021-02356-3
EA JAN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600010
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, QY
   Chen, Y
AF Zhang, Qingyu
   Chen, Ying
TI Spatial and contextual aware network based on multi-resolution for human
   pose estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose estimation; Detail information; Global dependency; Contextual
   information
AB Aiming at capturing high-resolution spatial information and rich contextual information for accurate positioning and inference of keypoints in the task of human pose estimation, a Spatial and Contextual Aware Network (SCANet) based on multi-resolution is proposed. The network is based on HRNet and extends it with three effective modules, namely Spatial Self-Attention Module (SSAM), Information Supplement Module (ISM) and Detail Enhancement Module (DEM). The SSAM is used to provide global dependency for local features by establishing spatial correlation between locations in feature maps. ISM is proposed to further enrich spatial information and refine local representation by skip connection and dilated convolution. DEM is designed to generate high-resolution features and compensate detail information for more precise prediction. The proposed method is better than most of the state-of-the-art methods, and experiments on two keypoint benchmarks, MPII and COCO, validating the effectiveness of the model.
C1 [Zhang, Qingyu; Chen, Ying] Jiangnan Univ, Minist Educ, Key Lab Adv Proc Control Light Ind, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Chen, Y (corresponding author), Jiangnan Univ, Minist Educ, Key Lab Adv Proc Control Light Ind, Wuxi 214122, Jiangsu, Peoples R China.
EM 1030615206@vip.jiangnan.edu.cn; chenying@jiangnan.edu.cn
FU National Natural Science Foundation of China [61573168, 62173160]
FX This work is supported by the National Natural Science Foundation of
   China (Grant Nos. 61573168 and 62173160)
CR [Anonymous], 2019, ARXIV PREPRINT ARXIV
   Barmpoutis A, 2013, IEEE T CYBERNETICS, V43, P1347, DOI 10.1109/TCYB.2013.2276430
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang CL, 2004, EURASIP J APPL SIG P, V2004, P1648, DOI 10.1155/S1110865704401206
   Huo ZQ, 2020, IEEE ACCESS, V8, P224947, DOI 10.1109/ACCESS.2020.3044885
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang CR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2364, DOI 10.1145/3394171.3414041
   Liang S, 2022, VISUAL COMPUT, V38, P1369, DOI 10.1007/s00371-021-02282-4
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Lu Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P396, DOI 10.1007/978-3-030-58565-5_24
   Newell A., 2017, ADV NEUR IN, P2171
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tang W, 2019, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2019.00120
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang Z., 2020, ARXIV200712061
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu XX, 2009, PATTERN RECOGN LETT, V30, P1077, DOI 10.1016/j.patrec.2009.04.002
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu Xinyi, 2020, ARXIV PREPRINT ARXIV
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yanrui Bin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P606, DOI 10.1007/978-3-030-58529-7_36
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu F., 2015, ARXIV
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
NR 43
TC 4
Z9 4
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 651
EP 662
DI 10.1007/s00371-021-02364-3
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600003
DA 2024-07-18
ER

PT J
AU Li, N
   Ai, HJ
AF Li, Ning
   Ai, Haojun
TI EfiLoc: large-scale visual indoor localization with efficient
   correlation between sparse features and 3D points
SO VISUAL COMPUTER
LA English
DT Article
DE Visual localization; Large-scale data; Sparse features; 3D point clouds;
   Feature associate
ID FUSION
AB Important location information of a query image can be obtained directly through indoor 3D points. However, the 3D model-based indoor positioning is still an open issue to be addressed, especially in large-scale dynamic environments. We design and realize the positioning system for large indoor scenes called the EfiLoc. First, we develop a lightweight network model, which can quickly extract discriminative global deep features to improve the discrimination of similar scenes. Another property is that the generated sparser main global descriptors can greatly reduce the retrieval time of multi-dimensional features. Second, we innovatively implement the efficient association of 3D point with the 2D features generated by its projection regions. Preserving the associations of the pixels in some key areas of the image, the precise and quick large-scale indoor localization can be realized. The experimental results show that EfiLoc can achieve good positioning accuracy and is of better robustness to the environment of weak textures and similar scenes compared with current state-of-the-art vision-based solutions.
C1 [Li, Ning] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Ai, Haojun] Wuhan Univ, Sch Cyber Sci & Engn, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University; Wuhan University
RP Li, N (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM li_ning@whu.edu.cn; aihj@whu.edu.cn
OI Li, Ning/0000-0002-7548-9379
FU National Key Research and Development Program of China [2016YFB0502201];
   National Natural Science Foundation of China [61971316]
FX This work is partially supported by The National Key Research and
   Development Program of China (2016YFB0502201) and The National Natural
   Science Foundation of China (General Program), Grant No. 61971316.
CR [Anonymous], 2017, 2017 INT C SERVICE S
   Azzi C., 2016, P BRIT MACH VIS C BM
   Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489
   Cao S, 2013, PROC CVPR IEEE, P700, DOI 10.1109/CVPR.2013.96
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chen YJ, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18082692
   Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626
   Dong J, 2019, IEEE T MOBILE COMPUT, V18, P1461, DOI 10.1109/TMC.2018.2857772
   Dong J, 2015, SENSYS'15: PROCEEDINGS OF THE 13TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P85, DOI 10.1145/2809695.2809722
   Fu YP, 2020, VISUAL COMPUT, V36, P2215, DOI 10.1007/s00371-020-01899-1
   Ghofrani A., 2019, ARXIV PREPRINT ARXIV
   Gopalan R, 2015, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2015.7298857
   Gu F, 2017, IEEE ACM T NETWORK, V25, P2267, DOI 10.1109/TNET.2017.2680448
   Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8
   Guo B, 2017, IEEE COMMUN SURV TUT, V19, P2526, DOI 10.1109/COMST.2017.2726686
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Hu JS, 2019, IEEE INTERNET THINGS, V6, P891, DOI 10.1109/JIOT.2018.2864607
   Huitl R., 2012, TUMINDOOR DATASET
   Kanzow C, 2004, J COMPUT APPL MATH, V172, P375, DOI 10.1016/j.cam.2004.02.013
   Kasap Z, 2012, VISUAL COMPUT, V28, P87, DOI 10.1007/s00371-011-0630-7
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Laoudias C, 2018, IEEE COMMUN SURV TUT, V20, P3607, DOI 10.1109/COMST.2018.2855063
   Li X., 2018, P EUROPEAN C COMPUTE, P229
   Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151
   Lim H, 2012, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2012.6247782
   Liu ZG, 2017, IEEE T MULTIMEDIA, V19, P874, DOI 10.1109/TMM.2016.2636750
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu FX, 2018, VISUAL COMPUT, V34, P753, DOI 10.1007/s00371-018-1540-8
   Lu GY, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P465, DOI 10.1145/3206025.3206070
   Lu GY, 2016, NEUROCOMPUTING, V173, P83, DOI 10.1016/j.neucom.2015.07.106
   Micusik B, 2015, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2015.7298936
   Niu Q, 2019, ACM T SENSOR NETWORK, V15, DOI 10.1145/3284555
   Redzic MD, 2020, IEEE T MOBILE COMPUT, V19, P1109, DOI 10.1109/TMC.2019.2903044
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342
   Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662
   Sattler T, 2015, IEEE I CONF COMP VIS, P2102, DOI 10.1109/ICCV.2015.243
   Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302
   Saurer O, 2016, INT J COMPUT VISION, V116, P213, DOI 10.1007/s11263-015-0830-0
   Schlegel D, 2018, IEEE INT CONF ROBOT, P3833
   Shah R., 2015, ARXIV PREPRINT ARXIV
   Shahjalal M, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/7680780
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Toft C, 2018, LECT NOTES COMPUT SC, V11206, P391, DOI 10.1007/978-3-030-01216-8_24
   Van Opdenbosch D, 2014, IEEE IMAGE PROC, P2804, DOI 10.1109/ICIP.2014.7025567
   Vasudevan Smrithi, 2020, Advances in Communication and Computational Technology. Select Proceedings of ICACCT 2019. Lecture Notes in Electrical Engineering (LNEE 668), P257, DOI 10.1007/978-981-15-5341-7_21
   Wu C., 2018, WIRELESS INDOOR LOCA, DOI 10.1007/978-981-13-0356-2
   Wu CS, 2018, IEEE T MOBILE COMPUT, V17, P517, DOI 10.1109/TMC.2017.2737004
   Xiao J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2933232
   Xin Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6622, DOI 10.1109/CVPR.2019.00679
   Xompero A, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1519, DOI 10.23919/ICIF.2018.8455444
   Xu H, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P208, DOI 10.1145/2971648.2971668
   Xu H, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P963, DOI 10.1145/2750858.2807516
   Xu JG, 2018, IEEE INT CONF MOB, P281, DOI 10.1109/MASS.2018.00050
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yanga G., 2018, 2018 INT C MECH ELEC, P103
   Yin X, 2019, PEER PEER NETW APPL, V12, P310, DOI 10.1007/s12083-017-0615-z
   Yin ZW, 2017, IEEE J SEL AREA COMM, V35, P1141, DOI 10.1109/JSAC.2017.2680844
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang WX, 2019, PROC CVPR IEEE, P12428, DOI 10.1109/CVPR.2019.01272
   Zhao JY, 2019, IEEE IMAGE PROC, P2636, DOI [10.1109/icip.2019.8803016, 10.1109/ICIP.2019.8803016]
   Zheng YQ, 2017, IEEE ACM T NETWORK, V25, P2655, DOI 10.1109/TNET.2017.2707101
NR 66
TC 8
Z9 8
U1 2
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2091
EP 2106
DI 10.1007/s00371-021-02270-8
EA SEP 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000692712300001
DA 2024-07-18
ER

PT J
AU Li, GQ
   Wu, ZH
   Liu, YX
   Zhang, HQ
   Nie, YW
   Mao, AH
AF Li, Guiqing
   Wu, Zihui
   Liu, Yuxin
   Zhang, Huiqian
   Nie, Yongwei
   Mao, Aihua
TI 3D hand reconstruction from a single image based on biomechanical
   constraints
SO VISUAL COMPUTER
LA English
DT Article
DE 3D hand motion reconstruction; Biomechanical constraints; Block
   coordinate descent; MANO parameterization
ID TRACKING; POSE
AB This paper investigates the estimate of motion parameters from 3D hand joint positions. We formulate the issue as an inverse kinematics problem with biomechanical constraints and propose a fast and robust iterative approach to address the constrained optimization. It elaborately designs a coordinate descent algorithm to decompose the problem into a sequence of decisions on the transformation around each kinematic node (i.e., joint), while the decision for each node is equivalent to a point matching problem. Addressing the whole optimization then amounts to considering all nodes of the kinematic tree from its root to leaves one by one. This not only accelerates the process but also improves the accuracy of the solution of the inverse kinematic optimization. Experiments show that our approach is able to yield results comparable to and even better than those by the state-of-the-art methods.
C1 [Li, Guiqing; Wu, Zihui; Liu, Yuxin; Zhang, Huiqian; Nie, Yongwei; Mao, Aihua] South China Univ Technol, Guangzhou, Peoples R China.
C3 South China University of Technology
RP Mao, AH (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.
EM ligq@scut.edu.cn; 437898809@qq.com; 465367868@qq.com; 791771249@qq.com;
   nieyongwei@scut.edu.cn; ahmao@scut.edu.cn
FU NSFC [61972160, 62072191]; Guangdong Basic and Applied Basic Research
   Foundation [2021A1515012301, 2019A1515010833, 2019A1515010860];
   Fundamental Research Funds for the Central Universities [2020ZYGXZR089,
   D2190670]
FX This work was partially supported by NSFC (61972160, 62072191),
   Guangdong Basic and Applied Basic Research Foundation (2021A1515012301,
   2019A1515010833, 2019A1515010860), and the Fundamental Research Funds
   for the Central Universities (2020ZYGXZR089, D2190670).
CR Ahmad A, 2019, IMAGE VISION COMPUT, V89, P35, DOI 10.1016/j.imavis.2019.06.003
   Aristidou A, 2018, VISUAL COMPUT, V34, P213, DOI 10.1007/s00371-016-1327-8
   Athitsos V, 2003, PROC CVPR IEEE, P432
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Bray M., 2004, PROC INT C ARTIFICIA, P59
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Choi H., 2020, COMPUTER VISION ECCV, P769
   de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33
   Diebel J., 2006, Representing attitude: Euler angles, unit quaternions, and rotation vectors, V20, P35, DOI DOI 10.1093/JXB/ERM298
   ElKoura G., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P110
   Feng ZQ, 2010, VISUAL COMPUT, V26, P607, DOI 10.1007/s00371-010-0452-z
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Glauser O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322957
   GOWER J., 2004, Procrustes Problems
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   Heap T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P140, DOI 10.1109/AFGR.1996.557255
   Imai A, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P895, DOI 10.1109/AFGR.2004.1301647
   Imai A, 2007, LECT NOTES COMPUT SC, V4843, P596
   Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743
   KABSCH W, 1976, ACTA CRYSTALLOGR A, V32, P922, DOI 10.1107/S0567739476001873
   Kitagawa M., 2012, MoCap for Artists: Workflow and Techniques for Motion Capture
   Kolluri R., 2004, ser. SGP '04, P11, DOI DOI 10.1145/1057432.1057434
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Magnenat-Thalmann, 1988, Proceedings of Graphics Interface '88, P26
   Marquardt A., 2018, P 24 ACM S VIRTUAL R, P3
   Melax S., 2013, P ACM SIGGRAPH S INT, P63, DOI DOI 10.1145/2448196.2448232
   Miyamoto S, 2012, INT C PATT RECOG, P453
   Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958
   Neng Qian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P54, DOI 10.1007/978-3-030-58621-8_4
   Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P651, DOI 10.1109/VR.2018.8446173
   Peng H, 2020, VISUAL COMPUT, V36, P2227, DOI 10.1007/s00371-020-01908-3
   Polygerinos P, 2015, IEEE INT CONF ROBOT, P2913, DOI 10.1109/ICRA.2015.7139597
   Qian C., 2014, PROC CVPR IEEE, P1106, DOI DOI 10.1109/CVPR.2014.145
   Romero J., 2009, 2009 9 IEEE RAS INT, P87
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Rong Yu, 2020, ARXIV200808324
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Spurr A, 2018, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2018.00017
   Spurr Adrian, 2020, P EUR C COMP VIS, P211
   Stenger B, 2001, PROC CVPR IEEE, P310
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tuffield P, 2003, IND ROBOT, V30, P56, DOI 10.1108/01439910310457715
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wheatland N., 2013, P MOTION GAMES, P197, DOI DOI 10.1145/2522628.2522656
   Wheatland N, 2015, COMPUT GRAPH FORUM, V34, P735, DOI 10.1111/cgf.12595
   Wilding J., 2001, PHYS THER REHABIL J
   Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122
   Zhang JW, 2017, IEEE IMAGE PROC, P982, DOI 10.1109/ICIP.2017.8296428
   Zhang X, 2019, IEEE I CONF COMP VIS, P2354, DOI 10.1109/ICCV.2019.00244
   Zhao Wenping, 2012, Proceedings of the ACM SIGGRAPH/eurographics symposium on computer animation. Eurographics Association, P33, DOI [10.2312/SCA/SCA12/033-042, DOI 10.2312/SCA/SCA12/033-042]
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 57
TC 8
Z9 9
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2699
EP 2711
DI 10.1007/s00371-021-02250-y
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000678565200001
DA 2024-07-18
ER

PT J
AU Kamboj, A
   Rani, R
   Nigam, A
AF Kamboj, Aman
   Rani, Rajneesh
   Nigam, Aditya
TI A comprehensive survey and deep learning-based approach for human
   recognition using ear biometric
SO VISUAL COMPUTER
LA English
DT Article
DE Ear; Handcrafted; Deep learning; Biometric; Detection; Recognition;
   Unconstrained; Wild
ID FEATURES; IDENTIFICATION; EXTRACTION; NETWORKS
AB Human recognition systems based on biometrics are much in demand due to increasing concerns of security and privacy. The human ear is unique and useful for recognition. It offers numerous advantages over popular biometrics traits face, iris, and fingerprints. A lot of work has been attributed to ear biometric, and the existing methods have achieved remarkable success over constrained databases. However, in unconstrained environment, a significant level of difficulty is observed as the images experience various challenges. In this paper, we first have provided a comprehensive survey on ear biometric using a novel taxonomy. The survey includes in-depth details of databases, performance evaluation parameters, and existing approaches. We have introduced a new database, NITJEW, for evaluation of unconstrained ear detection and recognition. A modified deep learning models Faster-RCNN and VGG-19 are used for ear detection and ear recognition tasks, respectively. The benchmark comparative assessment of our database is performed with six existing popular databases. Lastly, we have provided insight into open-ended research problems worth examining in the near future. We hope that our work will be a stepping stone for new researchers in ear biometrics and helpful for further development.
C1 [Kamboj, Aman; Rani, Rajneesh] Natl Inst Technol Jalandhar, Jalandhar 144011, Punjab, India.
   [Nigam, Aditya] Indian Inst Technol Mandi, Mandi 175005, Himachal Prades, India.
C3 National Institute of Technology (NIT System); Dr B R Ambedkar National
   Institute of Technology Jalandhar; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Mandi
RP Kamboj, A (corresponding author), Natl Inst Technol Jalandhar, Jalandhar 144011, Punjab, India.
EM amank.cs.16@nitj.ac.in; ranir@nitj.ac.in; aditya@iitmandi.ac.in
RI Kamboj, Aman/AAL-1094-2021
OI Kamboj, Aman/0000-0002-2229-0454
CR Abaza A, 2013, IMAGE VISION COMPUT, V31, P640, DOI 10.1016/j.imavis.2013.06.001
   Priyadharshini RA, 2021, APPL INTELL, V51, P2161, DOI 10.1007/s10489-020-01995-8
   Alsaadi I.M., 2015, International Journal of Scientific & Technology Research, V4, P285
   Alshazly H, 2020, IEEE ACCESS, V8, P170295, DOI 10.1109/ACCESS.2020.3024116
   Alshazly H, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11121493
   Amirthalingam G, 2016, J KING SAUD UNIV-COM, V28, P381, DOI 10.1016/j.jksuci.2014.12.011
   Anand R, 2020, ADV INTELL SYST COMP, V1048, P261, DOI 10.1007/978-981-15-0035-0_20
   [Anonymous], 2010, P 4 IEEE INT C BIOM, P1, DOI [DOI 10.1109/BTAS.2010.5634486, 10.1109/BTAS.2010.5634486]
   Anwar AS, 2015, PROCEDIA COMPUT SCI, V65, P529, DOI 10.1016/j.procs.2015.09.126
   Arbab-Zavar B, 2011, COMPUT VIS IMAGE UND, V115, P487, DOI 10.1016/j.cviu.2010.11.014
   Athawale Uttara., 2018, International Journal of Computer Sciences and Engineering, V6, P1208, DOI DOI 10.26438/IJCSE/V6I6.12081211
   Awad AI, 2016, STUD COMPUT INTELL, V630, P1, DOI 10.1007/978-3-319-28854-3
   Bertillon Alphonse., 1890, PHOTOGRAPHIE JUDICIA
   Boodoo-Jahangeer NB, 2013, IEEE INT C BIOINF BI
   Bustard JD, 2010, IEEE T SYST MAN CY A, V40, P486, DOI 10.1109/TSMCA.2010.2041652
   Chan TS, 2012, PATTERN RECOGN LETT, V33, P1870, DOI 10.1016/j.patrec.2011.11.013
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   Chauhan N., 2011, INT C INTELLIGENT SY, P50
   Chauhan S, 2010, PROCEDIA COMPUT SCI, V2, P213, DOI 10.1016/j.procs.2010.11.027
   Chidananda P, 2015, MACH VISION APPL, V26, P185, DOI 10.1007/s00138-015-0669-y
   Cintas C, 2017, IET BIOMETRICS, V6, P211, DOI 10.1049/iet-bmt.2016.0002
   der Lugt, 2000, EAR PRINTS
   Emersic Z, 2018, IET BIOMETRICS, V7, P175, DOI 10.1049/iet-bmt.2017.0240
   Emersic Z, 2017, IEEE INT CONF AUTOMA, P987, DOI 10.1109/FG.2017.123
   Emersic Z, 2017, NEUROCOMPUTING, V255, P26, DOI 10.1016/j.neucom.2016.08.139
   Esther Gonzalez Luis Alvarez., 2008, Ami ear database
   Gadre V., 2019, 2019 IEEE BOMB SECT, P1
   Galbally J, 2014, ADV COMPUT VIS PATT, P35, DOI 10.1007/978-1-4471-6524-8_3
   Galbally J, 2014, IEEE ACCESS, V2, P1530, DOI 10.1109/ACCESS.2014.2381273
   Ganapathi II, 2020, CONCURR COMP-PRACT E, V32, DOI 10.1002/cpe.5197
   Ganesh MR, 2014, ENG APPL ARTIF INTEL, V27, P115, DOI 10.1016/j.engappai.2013.07.022
   Garnett, 2015, ADV NEURAL INFORM PR, V28, P91
   Ghoualmi L, 2015, LECT NOTES ARTIF INT, V8869, P102, DOI 10.1007/978-3-319-14899-1_10
   Halawani A., 2016, INT J SIGNAL PROCESS, V4, P258, DOI [10.18178/ijsps.4.3.258-262, DOI 10.18178/IJSPS.4.3.258-262]
   Hansley EE, 2018, IET BIOMETRICS, V7, P215, DOI 10.1049/iet-bmt.2017.0210
   Harkeerat Kaur, 2017, INT J ADV RES COMPUT, V7
   Hassaballah M, 2020, MULTIMED TOOLS APPL, V79, P31183, DOI 10.1007/s11042-020-09456-7
   Hassaballah M, 2019, EXPERT SYST APPL, V118, P182, DOI 10.1016/j.eswa.2018.10.007
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heard B., 2012, 2012 INT C ENG TECHN, P1
   Hezil N, 2017, IET BIOMETRICS, V6, P351, DOI 10.1049/iet-bmt.2016.0072
   Iannarelli A., 1989, FORENSIC IDENTIFICAT
   Ibrahim Mina, 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117584
   Jaswal G, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2938727
   Kamboj A, 2021, PATTERN ANAL APPL, V24, P779, DOI 10.1007/s10044-020-00914-4
   Kasprzak, 2015, FORENSIC OTOSCOPY NE
   Kumar A, 2012, PATTERN RECOGN, V45, P956, DOI 10.1016/j.patcog.2011.06.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2018, IEEE SIGNAL PROC LET, V25, P848, DOI 10.1109/LSP.2018.2823910
   Liu Y, 2018, COMPLEXITY, DOI 10.1155/2018/5345241
   Liu YH, 2020, APPL INTELL, V50, P397, DOI 10.1007/s10489-019-01530-4
   Marcel S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6524-8
   Michele D., 2010, 2010 IEEE COMP SOC C, P178
   Mustafa A. J., 2020, Int. J. Adv. Sci. Technol., V29, P7423
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Nigam A, 2015, LECT NOTES COMPUT SC, V9010, P617, DOI 10.1007/978-3-319-16634-6_45
   Nourmohammadi-Khiarak J., 2018, 2018 INT CARN C SEC, P1, DOI DOI 10.1109/CCST.2018.8585637
   Peer, 2015, IEEE EUROCON 2015 IN, P1
   Peer P., 2019, DEEP EAR RECOGNITION, P333, DOI DOI 10.1007/978-3-030-03000-1_14
   Pflug A, 2014, P INT JOINT C BIOM, P1, DOI [10.1109/BTAS.2014.6996239, DOI 10.1109/BTAS.2014.6996239]
   Pflug A, 2013, INT CONF BIOMETR
   Prakash S, 2012, IMAGE VISION COMPUT, V30, P38, DOI 10.1016/j.imavis.2011.11.005
   Prakash S, 2009, PROC SPIE, V7306, DOI 10.1117/12.818371
   Radhika K., 2020, Nature inspired computing for data science, P57
   Raposo Rui., 2011, 2011 IEEE workshop on computational intelligence in biometrics and identity management (CIBIM), P84
   Rathore R., 2013, 6th International Conference on Biometrics: Theory, Applications and Systems (BTAS), P1
   Raveane W, 2019, PROCESSES, V7, DOI 10.3390/pr7070457
   Resmi K., 2019, P 2019 INT C DAT SCI, P1
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sabhanayagam T., 2018, International Journal of Applied Engineering Research, V13, P2276, DOI [10.1016/j.matpr.2021.07.005, DOI 10.1016/J.MATPR.2021.07.005]
   Sai, 2019, 2019 IEEE INT C INT
   Sajadi S, 2020, EXPERT SYST APPL, V159, DOI 10.1016/j.eswa.2020.113639
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sepas-Moghaddam A, 2018, EUR SIGNAL PR CONF, P2355, DOI 10.23919/EUSIPCO.2018.8553302
   Sibai FN, 2013, NEURAL COMPUT APPL, V23, P1265, DOI 10.1007/s00521-012-1068-1
   Snelick R, 2005, IEEE T PATTERN ANAL, V27, P450, DOI 10.1109/TPAMI.2005.57
   Stepec Dejan, 2020, CONSTELLATION BASED, P161
   Sun ZN, 2014, ADV COMPUT VIS PATT, P103, DOI 10.1007/978-1-4471-6524-8_6
   Thakkar Sejal, 2020, 2020 International Conference on Industry 4.0 Technology (I4Tech), P167, DOI 10.1109/I4Tech48345.2020.9102681
   Tian L, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P437, DOI 10.1109/CISP-BMEI.2016.7852751
   Toprak I, 2020, SIGNAL IMAGE VIDEO P, V14, P417, DOI 10.1007/s11760-019-01570-w
   Trabelsi Selma, 2020, 2020 1st International Conference on Communications, Control Systems and Signal Processing (CCSSP), P163, DOI 10.1109/CCSSP49278.2020.9151531
   UMIST, 2014, FACE DATABASE
   USTB, 2004, EAR RECOGINITION LAB
   Hoang VT, 2019, DATA BRIEF, V27, DOI 10.1016/j.dib.2019.104630
   Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067
   Yuan Li, 2014, ScientificWorldJournal, V2014, P702076, DOI 10.1155/2014/702076
   Zhang Y, 2017, LECT NOTES COMPUT SC, V10667, P405, DOI 10.1007/978-3-319-71589-6_35
   Zhang Y, 2018, IET BIOMETRICS, V7, P185, DOI 10.1049/iet-bmt.2017.0176
   Zhang Y, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9040053
   Zhao SP, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107071
   Zhou YX, 2017, IEEE INT CONF AUTOMA, P626, DOI 10.1109/FG.2017.79
   Zibran M.F., 2012, Biometric Authentication: The Security Issues
NR 96
TC 26
Z9 26
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2383
EP 2416
DI 10.1007/s00371-021-02119-0
EA APR 2021
PG 34
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000642386800001
PM 33907343
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Parihar, AS
   Varshney, D
   Pandya, K
   Aggarwal, A
AF Parihar, Anil Singh
   Varshney, Disha
   Pandya, Kshitija
   Aggarwal, Ashray
TI A comprehensive survey on video frame interpolation techniques
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Video frame interpolation; Video processing; Survey
ID RATE UP-CONVERSION; BILATERAL MOTION ESTIMATION; OPTICAL-FLOW;
   ENHANCEMENT; PREDICTION
AB Video frame interpolation is an important area in the computer vision research activities for video post-processing, surveillance, and video restoration tasks. It aims toward increasing the frame rate of a video sequence by calculating intermittent frames between consecutive input frames. This ensures extra smooth, clear motion in order to make animation fluid enough and reduce display motion blur. Advanced deep learning algorithms have the potential to discover knowledge from large-scale diverse video data. These algorithms gain insights about intermediate motion and provide new opportunities to further improve video interpolation technologies. This survey demonstrates a comprehensive overview of about a good number of contributions over past decade pertinent to the latest developments in this domain. The survey paper highlights common challenges in the area of video frame interpolation based on three key aspects: high visual quality, low complexity, and high efficiency of interpolated output from regular videos with the standard frame rate. We scrutinize the architectures, workflows, performance, advantages, and disadvantages and generate a broad categorization along with an overview of experimental results of various state-of-the-art methods executed on benchmark datasets. This survey discusses applications of diverse interpolation frameworks. It provides a backbone reference that inspires future researchers to optimize current techniques on academic and industrial grounds.
C1 [Parihar, Anil Singh; Varshney, Disha; Pandya, Kshitija; Aggarwal, Ashray] Delhi Technol Univ, Machine Learning Res Lab, Delhi, India.
C3 Delhi Technological University
RP Parihar, AS (corresponding author), Delhi Technol Univ, Machine Learning Res Lab, Delhi, India.
EM parihar.anil@gmail.com; dishavarshney9@gmail.com;
   kshitijajain99@gmail.com; ashray14aggarwal@gmail.com
RI Parihar, Anil Singh/Z-4992-2019
OI Parihar, Anil Singh/0000-0001-5339-8671
CR Ahn HE, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050619
   Ahn HE, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11101251
   alvarez L., 2002, ECCV
   Ancuti C, 2008, VISUAL COMPUT, V24, P709, DOI 10.1007/s00371-008-0251-y
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bao WB, 2018, IEEE T IMAGE PROCESS, V27, P3813, DOI 10.1109/TIP.2018.2825100
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Bouguet J.Y, 2003, OPEN SOURCE COMPUTER
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chen Wei, 2016, Advances in Neural Information Processing Systems, P1659
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Cheng XH, 2020, IEEE T CIRC SYST VID, V30, P3968, DOI 10.1109/TCSVT.2019.2939143
   Chenguang Li, 2018, 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC). Proceedings, P553, DOI 10.1109/DSC.2018.00089
   Choi BD, 2007, IEEE T CIRC SYST VID, V17, P407, DOI 10.1109/TCSVT.2007.893835
   Choi C, 2018, VISUAL COMPUT, V34, P1479, DOI 10.1007/s00371-017-1414-5
   Choi D, 2016, IEEE T CIRC SYST VID, V26, P1789, DOI 10.1109/TCSVT.2015.2473275
   Churiwala S, 2011, PRINCIPLES OF VLSI RTL DESIGN: A PRATICAL GUIDE, P1, DOI 10.1007/978-1-4419-9296-3
   Didyk P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508376
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Fan YZ, 2016, IEEE IMAGE PROC, P569, DOI 10.1109/ICIP.2016.7532421
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fourure D., 2017, ARXIV170707958, P181
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gilliam C, 2015, INT CONF ACOUST SPEE, P1533, DOI 10.1109/ICASSP.2015.7178227
   Gu DH, 2019, IEEE INT CON MULTI, P1768, DOI 10.1109/ICME.2019.00304
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Herbst Evan., 2009, Occlusion Reasoning for Temporal Interpolation Using Optical Flow"
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu ZZ, 2018, ADV ENG SOFTW, V115, P1, DOI 10.1016/j.advengsoft.2017.08.007
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jacobson N, 2010, IEEE T IMAGE PROCESS, V19, P2924, DOI 10.1109/TIP.2010.2050928
   Janai J, 2017, PROC CVPR IEEE, P1406, DOI 10.1109/CVPR.2017.154
   Jayashankar T, 2019, IEEE IMAGE PROC, P4195, DOI [10.1109/icip.2019.8803484, 10.1109/ICIP.2019.8803484]
   Jeon BW, 2003, IEEE T CONSUM ELECTR, V49, P499, DOI 10.1109/TCE.2003.1233761
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jiang Y, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION, CYBERNETICS AND COMPUTATIONAL SOCIAL SYSTEMS (ICCSS), P77, DOI 10.1109/ICCSS.2017.8091388
   Kang SJ, 2007, IEEE T CONSUM ELECTR, V53, P1759, DOI 10.1109/TCE.2007.4429281
   Kang SJ, 2010, IEEE T CIRC SYST VID, V20, P1909, DOI 10.1109/TCSVT.2010.2087832
   Kang SJ, 2008, IEEE T CONSUM ELECTR, V54, P1830, DOI 10.1109/TCE.2008.4711242
   Kaviani HR, 2016, IEEE T CIRC SYST VID, V26, P1581, DOI 10.1109/TCSVT.2015.2469120
   Keller SH, 2010, SIGNALS COMMUN TECHN, P275, DOI 10.1007/978-3-642-12802-8_11
   Kim D, 2013, IEEE T CIRC SYST VID, V23, P445, DOI 10.1109/TCSVT.2012.2207271
   Kim S.Y., 2020, AAAI
   Kim US, 2014, IEEE T CIRC SYST VID, V24, P384, DOI 10.1109/TCSVT.2013.2278142
   Koren Mark, 2017, Frame interpolation using generative adversarial networks
   Krishnamurthy R, 1999, IEEE T CIRC SYST VID, V9, P713, DOI 10.1109/76.780361
   Krizhevsky A., 2017, CACM
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Lee WH, 2014, IEEE T IMAGE PROCESS, V23, P399, DOI 10.1109/TIP.2013.2288139
   Li J., 2004, MINI-MICRO SYST, V25
   Li R, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9010156
   Li SY, 2019, IEEE INT CONF COMP V, P3427, DOI 10.1109/ICCVW.2019.00425
   Li W., 2017, ARXIV160308124
   Li WB, 2013, PROC CVPR IEEE, P2435, DOI 10.1109/CVPR.2013.315
   Li YL, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11020026
   Lim H, 2015, IEEE T CIRC SYST VID, V25, P518, DOI 10.1109/TCSVT.2014.2352557
   Lim H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P325, DOI 10.1109/PCS.2012.6213358
   Lim H, 2011, IEEE T IMAGE PROCESS, V20, P3653, DOI 10.1109/TIP.2011.2159232
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu C, 2009, NANOTECHNOLOGY, V20, DOI 10.1088/0957-4484/20/6/065604
   Liu HB, 2012, IEEE T CIRC SYST VID, V22, P1188, DOI 10.1109/TCSVT.2012.2197081
   Liu Y., 2019, AAAI
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Lu QC, 2016, J DISP TECHNOL, V12, P45, DOI 10.1109/JDT.2015.2453252
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Mahajan D., 2009, SIGGRAPH 2009
   Mathieu M., 2016, ARXIV15110544 CORR
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Myungsub Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9441, DOI 10.1109/CVPR42600.2020.00946
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Park S, 2020, MINERALS-BASEL, V10, DOI 10.3390/min10080663
   Patraucean V., 2015, arXiv
   Peleg T, 2019, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2019.00250
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Raket L., 2012, ISVC
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ShashiKiran S, 2017, INT J INNOVAT RES EL, V5, P297
   Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Steinbrücker F, 2009, IEEE I CONF COMP VIS, P1609, DOI 10.1109/ICCV.2009.5459364
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang J., 2004, 2004 IEEE EL TECHN C, P143
   Wang P., 2020, AAAI
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang YK, 2014, VISUAL COMPUT, V30, P1157, DOI 10.1007/s00371-013-0896-z
   Wen SP, 2019, IEEE T CIRC SYST VID, V29, P2337, DOI 10.1109/TCSVT.2018.2867934
   Werlberger M., 2011, EMMCVPR
   Xiao J, 2020, IEEE ACCESS, V8, P94842, DOI 10.1109/ACCESS.2020.2995705
   Xiao Jiangjian., 2006, ECCV
   Van XH, 2018, IET IMAGE PROCESS, V12, P113, DOI 10.1049/iet-ipr.2016.0938
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Xu XY, 2018, LECT NOTES COMPUT SC, V11213, P36, DOI 10.1007/978-3-030-01240-3_3
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Xue Tianfan, 2016, Advances in Neural Information Processing Systems, P91
   Xue W, 2020, NEUROCOMPUTING, V380, P95, DOI 10.1016/j.neucom.2019.11.015
   Yan B, 2013, J VIS COMMUN IMAGE R, V24, P661, DOI 10.1016/j.jvcir.2011.12.002
   Yang XH, 2014, IEEE SIGNAL PROC LET, V21, P423, DOI 10.1109/LSP.2014.2304975
   Yu S, 2019, IEEE INT CONF COMP V, P3503, DOI 10.1109/ICCVW.2019.00434
   Yu ZF, 2013, IEEE T CIRC SYST VID, V23, P1235, DOI 10.1109/TCSVT.2013.2242631
   Yunlong Zhao, 2019, 2019 7th International Conference on Information, Communication and Networks (ICICN), P158, DOI 10.1109/ICICN.2019.8834946
   Zhai JF, 2005, IEEE INT SYMP CIRC S, P4927
   Zhang HX, 2019, IEEE ACCESS, V7, P130610, DOI 10.1109/ACCESS.2019.2940510
   Zhang T, 2018, ASIAPAC SIGN INFO PR, P1061, DOI 10.23919/APSIPA.2018.8659604
   Zhang YB, 2016, IEEE T CIRC SYST VID, V26, P1421, DOI 10.1109/TCSVT.2015.2441355
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou MC, 2010, PROC CVPR IEEE, P701, DOI 10.1109/CVPR.2010.5540146
   Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 124
TC 17
Z9 17
U1 4
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 295
EP 319
DI 10.1007/s00371-020-02016-y
EA JAN 2021
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000604815100001
DA 2024-07-18
ER

PT J
AU Zhang, ZH
   Lian, DZ
   Gao, SH
AF Zhang, Ziheng
   Lian, Dongze
   Gao, Shenghua
TI RGB-D-based gaze point estimation via multi-column CNNs and facial
   landmarks global optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Gaze tracking; Human&#8211; computer interaction; Multi-column CNNs
AB In this work, we utilize a multi-column CNNs framework to estimate the gaze point of a person sitting in front of a display from an RGB-D image of the person. Given that gaze points are determined by head poses, eyeball poses, and 3D eye positions, we propose to infer the three components separately and then integrate them for gaze point estimation. The captured depth images, however, usually contain noises and black holes which prevent us from acquiring reliable head pose and 3D eye position estimation. Therefore, we propose to refine the raw depth for 68 facial keypoints by first estimating their relative depths from RGB face images, which along with the captured raw depths are then used to solve the absolute depth for all facial keypoints through global optimization. The refined depths will provide us reliable estimation for both head pose and 3D eye position. Given that existing publicly available RGB-D gaze tracking datasets are small, we also build a new dataset for training and validating our method. To the best of our knowledge, it is the largest RGB-D gaze tracking dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the Eyediap dataset.
C1 [Zhang, Ziheng; Lian, Dongze; Gao, Shenghua] ShanghaiTech Univ, Shanghai, Peoples R China.
   [Zhang, Ziheng; Lian, Dongze; Gao, Shenghua] Univ Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai, Peoples R China.
C3 ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute
   of Microsystem & Information Technology, CAS; University of Chinese
   Academy of Sciences, CAS
RP Zhang, ZH (corresponding author), ShanghaiTech Univ, Shanghai, Peoples R China.; Zhang, ZH (corresponding author), Univ Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai, Peoples R China.
EM zhangzh@shanghaitech.edu.cn; liandz@shanghaitech.edu.cn;
   gaoshh@shanghaitech.edu.cn
RI Zhang, Ziheng/AAD-7050-2019
CR [Anonymous], 2015, ARXIV150801244
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Deng H, 2017, IEEE I CONF COMP VIS, P3162, DOI 10.1109/ICCV.2017.341
   Eigen D, 2014, ADV NEUR IN, V27
   Funes Mora KennethAlberto., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P25, DOI DOI 10.1109/CVPRW.2012.6239182
   Funes-Mora KA, 2016, INT J COMPUT VISION, V118, P194, DOI 10.1007/s11263-015-0863-4
   Ghiass R.S., 2016, P 25 INT JOINT C ART, P3368
   Ghiass RS, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124280
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kar A, 2017, IEEE ACCESS, V5, P16495, DOI 10.1109/ACCESS.2017.2735633
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuno Y, 1996, MF '96 - 1996 IEEE/SICE/RSJ INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, P389, DOI 10.1109/MFI.1996.572205
   Kuno Y., 1997, P 7 INT C HUM COMP I, V1
   Liu G., 2018, 29 BRIT MACH VIS C 2
   Liu G., 2019, ARXIV190409459
   Majaranta P., 2014, ADV PHYSL COMPUTING, P39, DOI DOI 10.1007/978-1-4471-6392-3_3
   Masko D, 2017, THESIS
   McMurrough C.D., 2012, Proceedings of the Symposium on Eye Tracking Research and Applications, P305, DOI DOI 10.1145/2168556.2168622
   Mora K. A. F., 2014, P S EYE TRACK RES AP, P255, DOI [10.1145/2578153.2578190, 10.1145/2578153]
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   Qiuhai He, 2015, Image Analysis. 19th Scandinavian Conference, SCIA 2015. Proceedings: LNCS 9127, P418, DOI 10.1007/978-3-319-19665-7_35
   Ranjan R, 2018, IEEE COMPUT SOC CONF, P2237, DOI 10.1109/CVPRW.2018.00290
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stellato B, 2020, MATH PROGRAM COMPUT, V12, P637, DOI 10.1007/s12532-020-00179-2
   Stiefelhagen R., 1997, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V6, P193, DOI 10.1142/S0218213097000116
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Suwajanakorn S, 2015, PROC CVPR IEEE, P3497, DOI 10.1109/CVPR.2015.7298972
   Wang KH, 2018, IEEE INFOCOM SER, P1907
   Wang YF, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061287
   Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51
   Xiong XH, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1113, DOI 10.1145/2638728.2641694
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang X., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI 10.1109/CVPR.2015.7299081
   Zhang X, 2017, IEEE COMPUT SOC CONF, P2299, DOI 10.1109/CVPRW.2017.284
   Zhang YD, 2018, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2018.00026
   Zhu ZW, 2006, INT C PATT RECOG, P1132
NR 42
TC 6
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1731
EP 1741
DI 10.1007/s00371-020-01934-1
EA OCT 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000585777900001
DA 2024-07-18
ER

PT J
AU Cai, CT
   Meng, HY
   Qiao, RJ
AF Cai, Chengtao
   Meng, Haiyang
   Qiao, Renjie
TI Adaptive cropping and deskewing of scanned documents based on high
   accuracy estimation of skew angle and cropping value
SO VISUAL COMPUTER
LA English
DT Article
DE Image edge salient; Image classification; Skew angle and cropping value
   estimation; Deskewing and cropping; Region growing
ID IMAGES
AB Scanned documents commonly suffer from skew and redundant edges when a paper document is scanned and saved as an image file. In this paper, we propose an improved algorithm that can automatically deskew and crop scanned documents. To improve the accuracy of edge detection, the image edge can be made salient based on different edge types. The estimation of the deskew and the cropping value can benefit from the salient image edge. This paper also adopts the improved region growing method to automatically obtain the cropping value to crop the scanned image. The proposed method mainly includes image preprocessing, image classification, skew angle estimation, deskewing and cropping; estimation of the cropping values is based on different image types. Compared with the previous algorithms, the proposed algorithm not only has good anti-interference ability, but can also accurately estimate the cropping value and skew angle. Since the scanned images in the database from DISEC'2013 do not have redundant edges, another experiment with redundant edges must be performed with our database. The experimental results illustrate that the proposed method performs better than other methods.
C1 [Cai, Chengtao; Meng, Haiyang; Qiao, Renjie] Harbin Engn Univ, Coll Automat, Harbin 150001, Peoples R China.
C3 Harbin Engineering University
RP Meng, HY (corresponding author), Harbin Engn Univ, Coll Automat, Harbin 150001, Peoples R China.
EM menghaiyang@hrbeu.edu.cn
RI Qiao, Renjie/JCO-9357-2023; cai, chengtao/HGB-8304-2022
OI Qiao, Renjie/0000-0001-7580-0553; 
FU National Natural Science Foundation of China [61673129, 51674109]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 61673129, 51674109).
CR Alaei A, 2011, PROC INT CONF DOC, P299, DOI 10.1109/ICDAR.2011.68
   [Anonymous], 2016, ABS160609002 CORR
   [Anonymous], 2016, P IEEE C COMP VIS PA
   Aradhya VNM, 2007, INT J ROBOT AUTOM, V22, P272, DOI 10.2316/Journal.206.2007.4.206-2992
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Brodic D, 2013, INT J COMPUT COMMUN, V8, P673, DOI 10.15837/ijccc.2013.5.377
   Brodic D, 2013, ELEKTRON ELEKTROTECH, V19, P61, DOI 10.5755/j01.eee.19.2.3471
   Chen YK, 2001, J CHIN INST ENG, V24, P761, DOI 10.1080/02533839.2001.9670672
   Diem M., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P292, DOI 10.1109/DAS.2012.81
   Diem M, 2016, MSIO MULTISPECTRAL D
   Ding JH, 2015, ACSR ADV COMPUT, V34, P105
   Dobai L., 2019, 27 EUR S ART NEUR NE, P547
   Epshtein B, 2011, PROC INT CONF DOC, P27, DOI 10.1109/ICDAR.2011.15
   Fabrizio J, 2014, PRECISE SKEW ESTIMAT
   Fan D.P., 2018, ECCV
   Fan KC, 2002, PATTERN RECOGN, V35, P2593, DOI 10.1016/S0031-3203(01)00205-9
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gao YF, 2008, PROC SPIE, V6625, DOI 10.1117/12.790784
   Han XW, 2015, ADV ENG RES, V1, P12
   Himeur Y, 2017, MULTIMED TOOLS APPL, V76, P2813, DOI 10.1007/s11042-015-3216-y
   Jiao X, 2016, IMAGING SCI J, V64, P34, DOI 10.1080/13682199.2015.1115193
   Kleber F., 2014, ROBUST SKEW ESTIMATI
   Li ST, 2007, PATTERN RECOGN LETT, V28, P555, DOI 10.1016/j.patrec.2006.10.002
   Ma J., 2018, ARXIV170301086
   Ouwayed N, 2009, TRAIT SIGNAL, V26, P307
   Papandreou A, 2014, INT J DOC ANAL RECOG, V17, P433, DOI 10.1007/s10032-014-0228-5
   Papandreou A, 2013, PROC INT CONF DOC, P1444, DOI 10.1109/ICDAR.2013.291
   Papandreou A, 2011, PROC INT CONF DOC, P384, DOI 10.1109/ICDAR.2011.85
   Postl W., 1986, Eighth International Conference on Pattern Recognition. Proceedings (Cat. No.86CH2342-4), P687
   Saba T, 2011, ARTIF INTELL REV, V35, P101, DOI 10.1007/s10462-010-9186-6
   Shivakumara P., 2006, INT C ADV COMP COMM
   Singh C, 2008, PATTERN RECOGN, V41, P3528, DOI 10.1016/j.patcog.2008.06.002
   Srihari S. N., 1989, Machine Vision and Applications, V2, P141, DOI 10.1007/BF01212455
   Stahlberg F, 2015, DOCUMENT SKEW DETECT
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Xiaoyi Jiang, 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P629, DOI 10.1109/ICDAR.1999.791866
   Yi R, 2013, DIGITAL COMPENSATION
   Yildirim B, 2014, J TEXT I, V105, P654, DOI 10.1080/00405000.2013.843850
   Zhang F, 2015, LECT NOTES ARTIF INT, V9426, P226, DOI 10.1007/978-3-319-26181-2_21
   Zhao DL, 2020, VISUAL COMPUT, V36, P175, DOI 10.1007/s00371-018-1598-3
   Zhao J.X., 2019, CVPR
   Zhao LJ, 2017, VISUAL COMPUT, V33, P1169, DOI 10.1007/s00371-016-1279-z
NR 43
TC 4
Z9 4
U1 2
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1917
EP 1930
DI 10.1007/s00371-020-01952-z
EA AUG 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000558137100003
DA 2024-07-18
ER

PT J
AU Liu, YC
   Liao, LC
   Wu, H
   Qin, J
   He, L
   Yang, G
   Zhang, H
   Zhang, J
AF Liu, Yicheng
   Liao, Luchuan
   Wu, Hao
   Qin, Jing
   He, Ling
   Yang, Gang
   Zhang, Han
   Zhang, Jing
TI Trajectory and image-based detection and identification of UAV
SO VISUAL COMPUTER
LA English
DT Article
DE UAV; Drone detection; Trajectory identification; Tracking; Object
   recognition; Deep learning
AB Much more attentions have been attracted to the inspection and prevention of unmanned aerial vehicle (UAV) in the wake of increasing high frequency of security accident. Many factors like the interferences and the small fuselage of UAV pose challenges to the timely detection of the UAV. In our work, we present a system that is capable of detecting, recognizing, and tracking an UAV using single camera automatically. For our method, a single pan-tilt-zoom (PTZ) camera detects flying objects and gets their trajectories; then, the trajectory identified as a UAV guides the camera and PTZ to capture the detailed region image of the target. Therefore, the images can be classified into the UAV and interference classes (such as birds) by the convolution neural network classifier trained with our image dataset. For the target recognized as a UAV with the double verification, the radio jammer emits the interferential radio to disturb its control radio and GPS. This system could be applied in some complex environment where many birds and UAV appear simultaneously.
C1 [Liu, Yicheng; Liao, Luchuan; He, Ling; Yang, Gang; Zhang, Han; Zhang, Jing] Sichuan Univ, Coll Elect Engn, Chengdu 610065, Peoples R China.
   [Wu, Hao] Sky Def Technol, Chengdu 610213, Peoples R China.
   [Qin, Jing] Hong Kong Polytech Univ, Sch Nursing, Ctr Smart Hlth, Hung Hom, Hong Kong, Peoples R China.
C3 Sichuan University; Hong Kong Polytechnic University
RP Zhang, J (corresponding author), Sichuan Univ, Coll Elect Engn, Chengdu 610065, Peoples R China.
EM jing_zhang@scu.edu.cn
RI He, Ling/AAI-6434-2021; Qin, Jing/JMC-1371-2023; Qin, Jing/J-9807-2016;
   wu, tong/ITV-6896-2023; Zhang, Han/I-8297-2015
OI Qin, Jing/0000-0002-7059-0929; zhang, jin/0000-0003-2663-053X; Zhang,
   Han/0000-0002-0166-1973
FU National Natural Science Foundation of China [61571314]; Sky Defence
   Technology Co., Ltd.
FX This research was partially supported by research grants from the
   National Natural Science Foundation of China (Grant No.: 61571314) and
   the Sky Defence Technology Co., Ltd.
CR Aker C, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2017.8078539
   Alvine C, 2018, DRONE CARRYING EXPLO
   Andrasi P, 2017, TRANSP RES PROC, V28, P183, DOI 10.1016/j.trpro.2017.12.184
   [Anonymous], 2017, ARXIV170904666
   Anwar MZ, 2019, IEEE T VEH TECHNOL, V68, P2526, DOI 10.1109/TVT.2019.2893615
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chen Y, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1133-2
   Coluccia A, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/AVSS.2019.8909876
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Craye C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909854
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kapoor A, 2017, VISUAL COMPUT, V33, P665, DOI 10.1007/s00371-016-1216-1
   Ketkar N, 2017, DEEP LEARNING PYTHON, P195, DOI [10.1007/978-1-4842-2766-4_12, DOI 10.1007/978-1-4842-2766-4_12]
   Kingma D.P., 2014, ARXIV14126980
   Li CCJ, 2017, IEEE ANTENN WIREL PR, V16, P649, DOI 10.1109/LAWP.2016.2594766
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Magoulianitis V, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909865
   McCallum A., 1998, AAAI 98 WORKSH LEARN, V752, P41, DOI DOI 10.1109/TSMC.1985.6313426
   Mohajerin N, 2014, IEEE RAD CONF, P674, DOI 10.1109/RADAR.2014.6875676
   Molchanov P, 2014, INT J MICROW WIREL T, V6, P435, DOI 10.1017/S1759078714000282
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Park J, 2017, INT C CONTR AUTOMAT, P696, DOI 10.23919/ICCAS.2017.8204318
   Nguyen P, 2016, DRONET'16: PROCEEDINGS OF THE 2ND WORKSHOP ON MICRO AERIAL VEHICLE NETWORKS, SYSTEMS, AND APPLICATIONS FOR CIVILIAN USE, P17, DOI 10.1145/2935620.2935632
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rozantsev A, 2017, IEEE T PATTERN ANAL, V39, P879, DOI 10.1109/TPAMI.2016.2564408
   Samaras S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19224837
   Schumann A, 2018, PROC SPIE, V10799, DOI 10.1117/12.2325735
   Shin DH, 2017, IEEE T INSTRUM MEAS, V66, P340, DOI 10.1109/TIM.2016.2626038
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Unlu E., 2018, Electron. Imaging, V2018, p128
   Unlu E, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P550, DOI 10.5220/0006680105500554
   Yue XJ, 2018, IEEE COMMUN MAG, V56, P90, DOI 10.1109/MCOM.2018.1700423
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
NR 37
TC 18
Z9 20
U1 7
U2 51
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1769
EP 1780
DI 10.1007/s00371-020-01937-y
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000553760200002
DA 2024-07-18
ER

PT J
AU Fan, XY
   Wu, WJ
   Zhang, L
   Yan, QG
   Fu, G
   Chen, ZP
   Long, CJ
   Xiao, CX
AF Fan, Xinyun
   Wu, Wenjun
   Zhang, Ling
   Yan, Qingan
   Fu, Gang
   Chen, Zipei
   Long, Chengjiang
   Xiao, Chunxia
TI Shading-aware shadow detection and removal from a single image
SO VISUAL COMPUTER
LA English
DT Article
DE Shadow removal; Complex shadow; Image processing
ID DECOMPOSITION
AB Shadow removal is a challenging problem due to its sensitivity to lighting and material conditions. In this paper, we propose a shading-aware shadow processing algorithm, which can automatically detect and remove complex shadows from a single color image. Our framework consists of two key steps. We firstly conduct a shadow-preserving filter upon the image which will effectively remove the image texture while preserving the shadow and shading information. Shadow regions are estimated by establishing a confidence map from the filtered image incorporating depth cue. We then develop a shading-aware optimization framework to remove shadows and recover shading in these regions. The extensive experimental results show that the proposed algorithm produces visually compelling results in a series of challenging images and it can handle complex shadows in both indoor and outdoor scenes. Quantitative and qualitative comparisons with current state-of-the-art methods strongly demonstrate the efficacy of our proposed approach.
C1 [Fan, Xinyun; Wu, Wenjun; Fu, Gang; Chen, Zipei; Xiao, Chunxia] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Zhang, Ling] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430081, Peoples R China.
   [Yan, Qingan] JD Com Amer Technol Corp, Mountain View, CA 94043 USA.
   [Long, Chengjiang] JD Digits, Mountain View, CA 94043 USA.
C3 Wuhan University; Wuhan University of Science & Technology
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
EM xinyunfan@whu.edu.cn; 815012933@qq.com; zhling@wust.edu.cn;
   qingan.yan@jd.com; xyzgfu@gmail.com; 2019202110105@whu.edu.cn;
   cjfykx@gmail.com; cxxiao@whu.edu.cn
RI wenjun, wu/ABI-4150-2020
FU Key Technological Innovation Projects of Hubei Province [2018AAA062];
   NSFC [61972298, 61672390, 61902286]; National Key Research and
   Development Program of China [2017Y FB1002600]; China Postdoctoral
   Science Found [2018M642933]; Wuhan University -Huawei GeoInformatics
   Innovation Lab
FX Funding was provided by the Key Technological Innovation Projects of
   Hubei Province (Grant No. 2018AAA062), NSFC (Grant Nos. 61972298,
   61672390, 61902286), National Key Research and Development Program of
   China (Grant No. 2017Y FB1002600), China Postdoctoral Science Found (No.
   2018M642933), and Wuhan University -Huawei GeoInformatics Innovation
   Lab.
CR Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298
   Cucchiara R., 2002, INTELLIGENT TRANSPOR, P334
   Cun Xiaodong, 2020, AAAI
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo Ruiqi, 2011, CVPR 2011, DOI DOI 10.1109/CVPR.2011.5995725
   Hachama M, 2015, IEEE I CONF COMP VIS, P810, DOI 10.1109/ICCV.2015.99
   Hu X., 2019, IEEE TPAMI, P7454
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Jeon J, 2014, LECT NOTES COMPUT SC, V8695, P218, DOI 10.1007/978-3-319-10584-0_15
   Jinjiang Wei, 2019, Computer Graphics Forum, V38, P381, DOI 10.1111/cgf.13845
   Karsch K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602146
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lalonde JF, 2010, LECT NOTES COMPUT SC, V6312, P322, DOI 10.1007/978-3-642-15552-9_24
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Le Hieu, 2018, ECCV, P662
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu F, 2008, LECT NOTES COMPUT SC, V5305, P437
   Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907
   Mikic I, 2000, INT C PATT RECOG, P321, DOI 10.1109/ICPR.2000.905341
   Mohan A, 2007, IEEE COMPUT GRAPH, V27, P23, DOI 10.1109/MCG.2007.30
   Okabe Takahiro, 2009, 2009 IEEE 12th International Conference on Computer Vision (ICCV), P1693, DOI 10.1109/ICCV.2009.5459381
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sidorov Oleksii, 2019, CVPR
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wu TP, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243982
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12198
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xiao Y, 2014, PROC CVPR IEEE, P3011, DOI 10.1109/CVPR.2014.385
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Zhang L., 2020, AAAI
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P4114, DOI 10.1109/TIP.2017.2712283
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zheng Quanlong, 2019, CVPR, P5167
NR 49
TC 12
Z9 14
U1 0
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2175
EP 2188
DI 10.1007/s00371-020-01916-3
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549695200002
DA 2024-07-18
ER

PT J
AU Zhang, XW
   Wang, J
   Lu, GD
   Zhang, XS
AF Zhang, Xinwei
   Wang, Jin
   Lu, Guodong
   Zhang, Xusheng
TI Pattern understanding and synthesis based on layout tree descriptor
SO VISUAL COMPUTER
LA English
DT Article
DE Patterns; Layouts; Synthesis; Design tools; Graphical models; Design
   space exploration
ID RELATIVE POSITIONS; DESIGN; REPRESENTATION; TEXTILE; SYSTEM
AB Synthesis from existing examples is a promising way to generate new patterns. However, pattern synthesis is challenging because it is difficult to understand and generate complex structures in patterns. In this paper, we propose an approach based on the layout tree descriptor (LTD) to understand and synthesize patterns from existing ones. The LTD is a binary tree that parametrically describes all primitives, layouts, their dependencies and hierarchies in a pattern. The LTD can be constructed automatically with proposed instance grouping, layout recognition, hyper-primitive matching and tree merging algorithms to realize pattern understanding. To meet specialists' requirements for detailed modification and recombination of patterns, we designed LTD operations including add, remove, replace and grafting operations to allow users to get new patterns by simply adjusting the LTDs. For stylized synthesis, we gave the computing method of LTD similarity. Therefore, the styles of results and input can be compared and users can control generated serialized results by setting the input pattern weights. To meet user's implicit preferences and provide novelty in creative design, we propose an evolutionary approach to creative synthesis. The system generates new patterns continuously based on LTD grafting, meanwhile user selection of preferred patterns will guide the direction of evolution. Experiments using the developed prototype system show that our approach can synthesize novel and complex patterns effectively, meeting different requirements in practice and providing plenty of digital textures for products.
C1 [Zhang, Xinwei; Wang, Jin; Lu, Guodong] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
   [Zhang, Xusheng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Wang, J (corresponding author), Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Hangzhou 310027, Peoples R China.
EM dwjcom@zju.edu.cn
RI chen, yijia/KGM-4378-2024; SUN, Ye/KBC-8159-2024
FU National Natural Science Foundation of China [51775492]; Robotics
   Institute of Zhejiang University [K18-508116-001]
FX This study is funded by the National Natural Science Foundation of China
   (51775492) and Robotics Institute of Zhejiang University
   (K18-508116-001).
CR Alhashim I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601102
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   [Anonymous], ACM T GRAPHIC
   [Anonymous], P ACM C HUM FACT COM
   [Anonymous], C COMP GRAPH INT TEC
   [Anonymous], VIS COMPUT
   [Anonymous], J URBAN DES
   [Anonymous], COMPUT AIDED DES
   [Anonymous], ACM T GRAPHICS TOG
   [Anonymous], PAPER ASIA
   [Anonymous], ACM T GRAPHICS
   [Anonymous], COMMUN COMPUT INF SC
   [Anonymous], P 10 ANN ACM S US IN
   [Anonymous], ADV SWARM COMPUTATIO
   Cui J, 2013, COMPUT AIDED DESIGN, V45, P591, DOI 10.1016/j.cad.2012.08.002
   Dino IG, 2016, AUTOMAT CONSTR, V69, P131, DOI 10.1016/j.autcon.2016.05.020
   Dong XY, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL II, P132
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XK, 2014, GRAPH MODELS, V76, P376, DOI 10.1016/j.gmod.2014.03.019
   Hua H, 2016, AUTOMAT CONSTR, V72, P388, DOI 10.1016/j.autcon.2016.09.009
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jaiswal Prakhar, 2016, Computer-Aided Design, V74, P45, DOI 10.1016/j.cad.2015.10.002
   Jesus D, 2016, VISUAL COMPUT, V32, P933, DOI 10.1007/s00371-016-1254-8
   Jia Q, 2016, PATTERN RECOGN, V52, P358, DOI 10.1016/j.patcog.2015.11.003
   KUSIAK A, 1987, EUR J OPER RES, V29, P229, DOI 10.1016/0377-2217(87)90238-4
   Lee AJT, 2003, PATTERN RECOGN LETT, V24, P3015, DOI 10.1016/S0167-8655(03)00162-4
   Lee JH, 2015, ARCHIT SCI REV, V58, P189, DOI 10.1080/00038628.2015.1015948
   Lu SF, 2014, ENG APPL ARTIF INTEL, V32, P124, DOI 10.1016/j.engappai.2014.02.015
   Matsakis P, 2010, INT J GEOGR INF SCI, V24, P1, DOI 10.1080/13658810802270587
   Matsakis P, 2001, IEEE T SYST MAN CY B, V31, P573, DOI 10.1109/3477.938261
   Nan LL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024219
   O'Donovan P, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1221, DOI 10.1145/2702123.2702149
   O'Donovan P, 2014, IEEE T VIS COMPUT GR, V20, P1200, DOI 10.1109/TVCG.2014.48
   Ruiz-Montiel M, 2014, COMPUT AIDED DESIGN, V56, P104, DOI 10.1016/j.cad.2014.06.012
   Shen W, 2016, PATTERN RECOGN LETT, V83, P321, DOI 10.1016/j.patrec.2016.02.002
   Talton JO, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618513
   Valor M, 2003, LECT NOTES COMPUT SC, V2669, P569
   Vitayasak S, 2017, INT J PROD ECON, V190, P146, DOI 10.1016/j.ijpe.2016.03.019
   Xu P., 2014, P 27 ANN ACM S US IN, P243, DOI DOI 10.1145/2642918.2647398
   Xu PF, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2787, DOI 10.1145/2702123.2702198
   Yao JC, 2017, VISUAL COMPUT, V33, P179, DOI 10.1007/s00371-015-1171-2
   Yeh YT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421639
   Yeh YT, 2009, COMPUT GRAPH FORUM, V28, P707, DOI 10.1111/j.1467-8659.2009.01411.x
   Zhou J, 2018, COMPUT GRAPH-UK, V70, P165, DOI 10.1016/j.cag.2017.07.033
NR 45
TC 1
Z9 1
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1141
EP 1155
DI 10.1007/s00371-019-01723-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400005
DA 2024-07-18
ER

PT J
AU Zarbakhsh, P
   Demirel, H
AF Zarbakhsh, Payam
   Demirel, Hasan
TI 4D facial expression recognition using multimodal time series analysis
   of geometric landmark-based deformations
SO VISUAL COMPUTER
LA English
DT Article
DE 4D facial expression recognition; Geometric deformation; Temporal
   analysis; Multimodal time series; Adaptive cost dynamic time warping
ID ACTION UNITS; REAL-TIME; SEQUENCES; CLASSIFICATION; FEATURES; IMAGES
AB One of the main challenges in dynamic facial expression recognition is how to capture temporal information in the system. In this study, a novel approach based on time series analysis is adapted for this problem. The proposed dynamic facial expression recognition system comprises four phases: head pose correction and normalization, feature extraction, feature selection and classification. Head pose detection and correction is the first phase to realign locations of the facial landmarks. A comprehensive set of geometric deformations including point, distance and angle deformations are extracted from the key points. The concept of facial action unit analysis is interlocked with this phase to identify related key points from the landmarks. A set of multimodal time series are then constructed from the extracted deformations by applying a sliding window to characterize the dynamics of mean deformations in a window. In the third phase, a feature selection method based on neighborhood component analysis is applied on the peak value of deformation features to select useful features and discard irrelevant ones. Finally, adaptive cost dynamic time warping is utilized to recognize six prototypic expressions from multimodal time series of selected features. Experimental results on BU-4DFE data set confirm that proposed algorithm is efficient in dynamic facial expression recognition compared with state of the art.
C1 [Zarbakhsh, Payam; Demirel, Hasan] Eastern Mediterranean Univ, Elect & Elect Engn Dept, Via Mersin 10, Famagusta, Northern Cyprus, Turkey.
   [Zarbakhsh, Payam] Cyprus Int Univ, Fac Engn, Via Mersin 10, Nicosia, Northern Cyprus, Turkey.
C3 Eastern Mediterranean University; Cyprus International University
RP Zarbakhsh, P (corresponding author), Eastern Mediterranean Univ, Elect & Elect Engn Dept, Via Mersin 10, Famagusta, Northern Cyprus, Turkey.; Zarbakhsh, P (corresponding author), Cyprus Int Univ, Fac Engn, Via Mersin 10, Nicosia, Northern Cyprus, Turkey.
EM payam.zarbakhsh@emu.edu.tr; hasan.demirel@emu.edu.tr
CR An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   [Anonymous], EMOTION RECOGNITION
   [Anonymous], P 2008 8 IEEE INT C, DOI DOI 10.1109/AFGR.2008.4813324
   Neto ENA, 2014, INTEGR COMPUT-AID E, V21, P281, DOI 10.3233/ICA-140462
   Ben Amor B, 2014, IEEE T CYBERNETICS, V44, P2443, DOI 10.1109/TCYB.2014.2308091
   Berndt D.J., 1996, Advances in Knowledge Discovery and Data Mining, P229
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Bolourchi P, 2017, SIGNAL IMAGE VIDEO P, V11, P1033, DOI 10.1007/s11760-017-1054-2
   Carcagnì P, 2015, SPRINGERPLUS, V4, DOI 10.1186/s40064-015-1427-3
   Choi JY, 2018, VISUAL COMPUT, V34, P1535, DOI 10.1007/s00371-017-1429-y
   Clien JK, 2015, IEEE IMAGE PROC, P4967, DOI 10.1109/ICIP.2015.7351752
   Derkach D, 2017, IEEE INT CONF AUTOMA, P820, DOI 10.1109/FG.2017.104
   Ekman P, 1978, FACIAL ACTION CODING
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Ghimire D, 2017, MULTIMED TOOLS APPL, V76, P7921, DOI 10.1007/s11042-016-3428-9
   Ghimire D, 2013, SENSORS-BASEL, V13, P7714, DOI 10.3390/s130607714
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goh KM, 2020, VISUAL COMPUT, V36, P445, DOI 10.1007/s00371-018-1607-6
   Goldberger J., 2004, P INT C NEUR INF PRO, V17, P513
   Górecki T, 2015, EXPERT SYST APPL, V42, P2305, DOI 10.1016/j.eswa.2014.11.007
   Guo YM, 2016, IEEE T IMAGE PROCESS, V25, P1977, DOI 10.1109/TIP.2016.2537215
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kalsum T, 2018, IET IMAGE PROCESS, V12, P1004, DOI 10.1049/iet-ipr.2017.0499
   Keogh E, 2003, DATA MIN KNOWL DISC, V7, P349, DOI 10.1023/A:1024988512476
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li WJ, 2018, IEEE INT CONF AUTOMA, P24, DOI 10.1109/FG.2018.00014
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Lien JJJ, 2000, ROBOT AUTON SYST, V31, P131, DOI 10.1016/S0921-8890(99)00103-7
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Rastegari M, 2011, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2011.6126556
   Reale M, 2013, IEEE INT CONF AUTOMA
   Sandbach G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Shao J, 2015, PATTERN RECOGN LETT, V65, P157, DOI 10.1016/j.patrec.2015.07.039
   Sun Y, 2010, IEEE T SYST MAN CY A, V40, P461, DOI 10.1109/TSMCA.2010.2041659
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Wan Y, 2017, J COMPUT APPL MATH, V319, P514, DOI 10.1016/j.cam.2017.01.004
   Wegrzyn M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177239
   Xue ML, 2015, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2015.34
   Yao YQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131345
   Yeganli SF, 2017, SIGNAL IMAGE VIDEO P, V11, P1477, DOI 10.1007/s11760-017-1110-y
   Yurtkan K, 2014, PATTERN RECOGN LETT, V38, P26, DOI 10.1016/j.patrec.2013.10.026
   Zarbakhsh P, 2018, SIGNAL IMAGE VIDEO P, V12, P1611, DOI 10.1007/s11760-018-1318-5
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhao JW, 2018, VISUAL COMPUT, V34, P1677, DOI 10.1007/s00371-017-1441-2
   Zhen QK, 2019, IEEE T AFFECT COMPUT, V10, P524, DOI 10.1109/TAFFC.2017.2747553
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhong C, 2012, J COMPUT, V7, P30, DOI 10.4304/jcp.7.1.30-41
NR 51
TC 7
Z9 8
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 951
EP 965
DI 10.1007/s00371-019-01705-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100007
DA 2024-07-18
ER

PT J
AU Hajizadeh, M
   Ebrahimnezhad, H
AF Hajizadeh, Mohammadali
   Ebrahimnezhad, Hossein
TI NLME: a nonlinear motion estimation-based compression method for
   animated mesh sequence
SO VISUAL COMPUTER
LA English
DT Article
DE Animated mesh sequence; Dynamic mesh compression; Rate-distortion;
   Nonlinear motion modeling; Motion segmentation
ID PREDICTIVE COMPRESSION; GEOMETRY
AB This paper proposes an efficient compression algorithm for animated three-dimensional meshes by introducing nonlinear transformations to model the motion field of deforming patches. First, a segmentation process is applied to separate the 3D model into different patches which have similar motion patterns through the sequence. Next, the motion of the resulting patches is accurately described by a nonlinear motion estimation model. The main idea is to exploit the temporal coherence of the geometry component by using a nonlinear predictor in order to get better approximation of vertex locations. Nonlinear motion transforms are computed at previous frame to match the subsequent ones. Moreover, an adaptive bit allocation algorithm is employed to determine the near-optimal number of bits for quantizing the prediction errors. The number of quantization bits for each segmented patch is determined by analyzing the geometry complexity of the patch and the statistical properties of the prediction errors. Finally, an extensive experimental study has been conducted to evaluate the coding efficiency of the proposed compression scheme. Simulation results demonstrate that the proposed method is very efficient in terms of rate-distortion performance, particularly for the animated models with non-rigid deformations, and outperforms the state-of-the-art methods.
C1 [Hajizadeh, Mohammadali; Ebrahimnezhad, Hossein] Sahand Univ Technol, Dept Elect Engn, Comp Vis Res Lab, Tabriz, Iran.
C3 Sahand University of Technology
RP Ebrahimnezhad, H (corresponding author), Sahand Univ Technol, Dept Elect Engn, Comp Vis Res Lab, Tabriz, Iran.
EM m_hajizadeh@sut.ac.ir; ebrahimnezhad@sut.ac.ir
RI ebrahimnezhad, hossein/ABC-3865-2021; Ebrahimnezhad,
   Hossein/ACP-2704-2022
OI ebrahimnezhad, hossein/0000-0003-4071-2750; 
CR Ahn JH, 2001, ELECTRON LETT, V37, P1445, DOI 10.1049/el:20010993
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   AMJOUN R, 2007, J WSCG, V15, P32
   Amjoun R, 2006, LECT NOTES COMPUT SC, V4035, P606
   [Anonymous], 3DTV C
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Bici MO, 2011, J VIS COMMUN IMAGE R, V22, P577, DOI 10.1016/j.jvcir.2011.07.006
   Briceno H. M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P136
   Chen J, 2017, VISUAL COMPUT, V33, P801, DOI 10.1007/s00371-017-1389-2
   Guo SH, 2015, VISUAL COMPUT, V31, P497, DOI 10.1007/s00371-014-0943-4
   Guskov I., 2004, Proc. 2004 ACM SIG- GRAPH/Eurographics Symp. Comput. Animation (SCA '04), P183
   Hachani M, 2016, SIGNAL IMAGE VIDEO P, V10, P1065, DOI 10.1007/s11760-015-0859-0
   Hajizadeh M, 2018, MULTIMED TOOLS APPL, V77, P19347, DOI 10.1007/s11042-017-5394-2
   Hajizadeh M, 2016, COMPUT ANIMAT VIRT W, V27, P556, DOI 10.1002/cav.1685
   Ibarria L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P126
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Lalos AS, 2017, VISUAL COMPUT, V33, P811, DOI 10.1007/s00371-017-1395-4
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Lee PF, 2007, IEICE T INF SYST, VE90D, P1073, DOI 10.1093/ietisy/e90-d.7.1073
   Lengyel J. E., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P89, DOI 10.1145/300523.300533
   Liu W, 2011, IMAGE VISION COMPUT, V29, P509, DOI 10.1016/j.imavis.2011.03.003
   Mamou K., 2006, WSEAS Transactions on Information Science and Applications, V3, P1947
   Mamou K, 2008, IEEE IMAGE PROC, P2676, DOI 10.1109/ICIP.2008.4712345
   Mamou K, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P711
   Mamou K, 2006, COMPUT ANIMAT VIRT W, V17, P337, DOI 10.1002/cav.137
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Müller K, 2006, SIGNAL PROCESS-IMAGE, V21, P812, DOI 10.1016/j.image.2006.07.002
   Muller K., 2005, IEEE INT C IM PROC, V1, p621 624
   Payan F., 2005, P IEEE ACIDCA ICMI
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   STEFANOSKI N, 2007, 3DTV C 2007, P1, DOI DOI 10.1109/3DTV.2007.4379461
   Stefanoski N, 2006, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2006.312961
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Vása L, 2014, COMPUT GRAPH FORUM, V33, P145, DOI 10.1111/cgf.12304
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang SF, 2015, VISUAL COMPUT, V31, P1163, DOI 10.1007/s00371-014-1000-z
   Yang JH, 2002, IEEE T CIRC SYST VID, V12, P1178, DOI 10.1109/TCSVT.2002.806814
   Zhang JH, 2004, IEEE DATA COMPR CONF, P508
   Zhang JH, 2007, COMPUT GRAPH-UK, V31, P463, DOI 10.1016/j.cag.2006.12.002
NR 41
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 649
EP 665
DI 10.1007/s00371-019-01645-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500015
OA Bronze
DA 2024-07-18
ER

PT J
AU Ernawan, F
   Kabir, MN
AF Ernawan, Ferda
   Kabir, Muhammad Nomani
TI A block-based RDWT-SVD image watermarking method using human visual
   system characteristics
SO VISUAL COMPUTER
LA English
DT Article
DE Image watermarking; Arnold transform; Human visual characteristics;
   Redundant wavelet transform; Singular value decomposition
ID DISCRETE WAVELET TRANSFORM; SINGULAR-VALUE DECOMPOSITION; ARNOLD
   TRANSFORM; PSYCHOVISUAL THRESHOLD; SCHEME; ROBUST; ENCRYPTION; DWT; DCT
AB With the rapid growth of internet technology, image watermarking method has become a popular copyright protection method for digital images. In this paper, we propose a watermarking method based on 4x4 image blocks using redundant wavelet transform with singular value decomposition considering human visual system (HVS) characteristics expressed by entropy values. The blocks which have the lower HVS entropies are selected for embedding the watermark. The watermark is embedded by examining U2,1 and U3,1 components of the orthogonal matrix obtained from singular value decomposition of the redundant wavelet transformed image block where an optimal threshold value based on the trade-off between robustness and imperceptibility is used. In order to provide additional security, a binary watermark is scrambled by Arnold transform before the watermark is embedded into the host image. The proposed scheme is tested under various image processing, compression and geometrical attacks. The test results are compared to other watermarking schemes that use SVD techniques. The experimental results demonstrate that our method can achieve higher imperceptibility and robustness under different types of attacks compared to existing schemes. Our method provides high robustness especially under image processing attacks, JPEG2000 and JPEG XR attacks. It has been observed that the proposed method achieves better performance over the recent existing watermarking schemes.
C1 [Ernawan, Ferda; Kabir, Muhammad Nomani] Univ Malaysia Pahang, Fac Comp Syst & Software Engn, Lebuhraya Tun Razak, Kuantan 26300, Pahang Darul Ma, Malaysia.
C3 Universiti Malaysia Pahang Al-Sultan Abdullah (UMPSA)
RP Ernawan, F (corresponding author), Univ Malaysia Pahang, Fac Comp Syst & Software Engn, Lebuhraya Tun Razak, Kuantan 26300, Pahang Darul Ma, Malaysia.
EM ferda@ump.edu.my
RI Kabir, Noman/M-4841-2016; Ernawan, Ferda/B-4214-2012
OI Kabir, Noman/0000-0003-2796-9324; Ernawan, Ferda/0000-0002-6779-1594
CR Abu Nur Azman, 2013, Information and Communication Technology. International Conference, ICT-EurAsia 2013. Proceedings: LNCS 7804, P519, DOI 10.1007/978-3-642-36818-9_60
   Abu N. A., 2014, APPL MATH SCI, V8, P6951
   Ansari IA, 2016, ENG APPL ARTIF INTEL, V49, P114, DOI 10.1016/j.engappai.2015.12.004
   Basso A, 2009, ALGORITHMS, V2, P46, DOI 10.3390/a2010046
   Cao LJ, 2013, VISUAL COMPUT, V29, P231, DOI 10.1007/s00371-012-0732-x
   Chang CC, 2005, PATTERN RECOGN LETT, V26, P1577, DOI 10.1016/j.patrec.2005.01.004
   Chen W, 2009, OPT COMMUN, V282, P3680, DOI 10.1016/j.optcom.2009.06.014
   Chung KL, 2007, APPL MATH COMPUT, V188, P54, DOI 10.1016/j.amc.2006.09.117
   Deng XP, 2011, OPT COMMUN, V284, P5623, DOI 10.1016/j.optcom.2011.08.071
   Dong, 2017, IEEE PHOTON J, V9, P1, DOI DOI 10.1109/JPHOT.2017.2731620
   Emawan F., 2016, INT INF I TOKYO, V9, P4177
   Ernawan Ferda, 2016, 2016 2nd International Conference on Science and Technology - Computer (ICST). Proceedings, P6, DOI 10.1109/ICSTC.2016.7877339
   Ernawan Ferda, 2013, Journal of Computer Science, V9, P716, DOI 10.3844/jcssp.2013.716.725
   Ernawan F, INT J ELECT COMPUT E
   Ernawan F., 2014, J THEORETICAL APPL I, V70, P566
   Ernawan F, 2017, Journal of Telecommunication, Electronic and Computer Engineering (JTEC), V9, P111
   Ernawan F, 2018, MULTIMED TOOLS APPL, V77, P13923, DOI 10.1007/s11042-017-4999-9
   Ernawan F, 2018, IEEE ACCESS, V6, P20464, DOI 10.1109/ACCESS.2018.2819424
   Ernawan F, 2016, J ICT RES APPL, V10, P228, DOI 10.5614/itbj.ict.res.appl.2016.10.3.3
   Ernawan F, 2017, OPTIK, V148, P106, DOI 10.1016/j.ijleo.2017.08.007
   Ernawan F, 2014, ADV SCI LETT, V20, P26, DOI 10.1166/asl.2014.5255
   Ernawan F, 2014, ADV SCI LETT, V20, P70, DOI 10.1166/asl.2014.5316
   Fan MQ, 2008, APPL MATH COMPUT, V203, P926, DOI 10.1016/j.amc.2008.05.003
   Fazli S, 2016, OPTIK, V127, P964, DOI 10.1016/j.ijleo.2015.09.205
   Gao L, 2015, INT J DIGIT CRIME FO, V7, P1, DOI [10.4018/IJDCF.2015100101, 10.4018/ijdcf.2015100101]
   Hien TD, 2006, ADV SOFT COMP, V34, P401, DOI 10.1007/3-540-31662-0_31
   Keshavarzian R, 2016, AEU-INT J ELECTRON C, V70, P278, DOI 10.1016/j.aeue.2015.12.003
   Lagzian S., 2011, 2011 International Symposium on Artificial Intelligence and Signal Processing (AISP), P48, DOI 10.1109/AISP.2011.5960985
   Lagzian S., 2011, International Journal of Intelligent Information Processing, V2, P22, DOI DOI 10.4156/IJIIP
   Lai CC, 2011, OPT COMMUN, V284, P938, DOI 10.1016/j.optcom.2010.10.047
   Lai CC, 2010, IEEE T INSTRUM MEAS, V59, P3060, DOI 10.1109/TIM.2010.2066770
   Lee H, 2011, VISUAL COMPUT, V27, P781, DOI 10.1007/s00371-011-0586-7
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Ling HC, 2013, AEU-INT J ELECTRON C, V67, P894, DOI 10.1016/j.aeue.2013.04.013
   Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   Liu ZJ, 2012, OPT LASER ENG, V50, P248, DOI 10.1016/j.optlaseng.2011.08.006
   Liu ZJ, 2011, OPT COMMUN, V284, P123, DOI 10.1016/j.optcom.2010.09.013
   Makbol NM, 2016, IET IMAGE PROCESS, V10, P34, DOI 10.1049/iet-ipr.2014.0965
   Makbol NM, 2013, AEU-INT J ELECTRON C, V67, P102, DOI 10.1016/j.aeue.2012.06.008
   Purohit N, 2017, LECT NOTES ELECTR EN, V395, P13, DOI 10.1007/978-81-322-3592-7_2
   Rassem TH, 2016, AIP CONF PROC, V1774, DOI 10.1063/1.4965108
   Rykaczewski R, 2007, IEEE T MULTIMEDIA, V9, P421, DOI 10.1109/TMM.2006.886297
   Su QT, 2015, SIGNAL IMAGE VIDEO P, V9, P991, DOI 10.1007/s11760-013-0534-2
   Wang X, 2013, INT J COMPUT GRAPH, V443, P566
   Xu JY, 2015, ADV EDUC SCI, V9, P3
   Yavuz E, 2013, DIGIT SIGNAL PROCESS, V23, P1335, DOI 10.1016/j.dsp.2013.02.009
   Zhang XP, 2005, IEEE T MULTIMEDIA, V7, P593, DOI 10.1109/TMM.2005.843357
NR 47
TC 61
Z9 63
U1 3
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 19
EP 37
DI 10.1007/s00371-018-1567-x
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800004
DA 2024-07-18
ER

PT J
AU Dai, SJ
   Wang, R
   Zhang, HB
AF Dai, Shijie
   Wang, Rui
   Zhang, Huibo
TI A gait skeleton model extraction method based on the fusion between
   vision and tactility
SO VISUAL COMPUTER
LA English
DT Article
DE 3D gait skeleton model; Spatial and temporal fusion; Shoulder and hip
   points; Equal proportion restructure
ID RECOGNITION
AB In the traditional gait skeleton model, the fixed length proportions among different bones cause the loss of model personality, and the shoulder and hip points that are blocked by the body are difficult to be extracted. For those problems, a method based on the spatial and temporal fusion between vision and tactility is proposed, by which a equal proportion 3D gait skeleton model can be restructured in the camera coordinate system accurately through a single-frame image. In the geometric analysis process, some logical assumptions are proposed according to human anatomy and the laws of human movement, and an effective method is proposed for the extraction of thigh slope. The experimental result shows that the consistent equal proportion models, in which single bone length error is eventually controlled within +/- 5mm, can be extracted in different position under the premise of rapidity with the aid of this method. The extracted shoulder and hip points also meet the real human body skeleton structure, which lays the foundation for the integrity and rationality of the entire skeletal model.
C1 [Dai, Shijie; Wang, Rui; Zhang, Huibo] Hebei Univ Technol, Sch Mech Engn, Tianjin, Peoples R China.
   [Dai, Shijie; Wang, Rui; Zhang, Huibo] Hebei Univ Technol, Hebei Key Lab Robot Percept & Human Robot Interac, Tianjin, Peoples R China.
C3 Hebei University of Technology; Hebei University of Technology
RP Zhang, HB (corresponding author), Hebei Univ Technol, Sch Mech Engn, Tianjin, Peoples R China.; Zhang, HB (corresponding author), Hebei Univ Technol, Hebei Key Lab Robot Percept & Human Robot Interac, Tianjin, Peoples R China.
EM zhanghb@hebut.edu.cn
RI dai, jianping/AAI-7046-2020
OI dai, jianping/0000-0003-4802-4485
FU Master Innovation Funding Project Foundation of Hebei Province, P. R.
   China [CXZZSS2018026]; Science and Technology on Space Intelligent
   Control Laboratory [ZDSYS-2017-08]; State Key Laboratory of Robotics and
   System (HIT) [SKLRS-2017-KF-15]; Hebei Natural Science Foundation [F
   2017202243]
FX This work is supported by Master Innovation Funding Project Foundation
   of Hebei Province, P. R. China (Grant No. CXZZSS2018026), Science and
   Technology on Space Intelligent Control Laboratory (Grant No:
   ZDSYS-2017-08), State Key Laboratory of Robotics and System (HIT) (Grant
   No: SKLRS-2017-KF-15) and Hebei Natural Science Foundation (Grant No: F
   2017202243).
CR [Anonymous], 2016, ALTERNAT MED
   [柴艳妹 Chai Yanmei], 2012, [计算机科学, Computer Science], V39, P10
   Cho CW, 2009, EXPERT SYST APPL, V36, P7033, DOI 10.1016/j.eswa.2008.08.076
   Hamzaçebi H, 2017, NONLINEAR DYNAM, V88, P1237, DOI 10.1007/s11071-016-3307-y
   Jahangiri E, 2017, IEEE INT CONF COMP V, P805, DOI 10.1109/ICCVW.2017.100
   Jan Nordin M. D., 2016, Research Journal of Applied Sciences, Engineering and Technology, V12, P756
   Kusakunniran W, 2012, IEEE T SYST MAN CY B, V42, P1654, DOI 10.1109/TSMCB.2012.2197823
   [刘玉栋 Liu Yudong], 2005, [计算机工程与应用, Computer Engineering and Application], V41, P88
   Lu H, 2007, EUR J ADV SIGNAL PRO, V2008, P261
   Poppe R, 2017, P CVPR WORKSH EH
   Portillo J, 2017, APPL INTELL, V2017, P1
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Sun J, 2018, INT J POLYM SCI, V2018, DOI [10.1155/2018/3286491, 10.1155/2018/8230965]
   Wang GF, 2015, VISUAL COMPUT, V31, P979, DOI 10.1007/s00371-015-1098-7
   Yu Y, 2015, VISUAL COMPUT, V31, P19, DOI 10.1007/s00371-013-0901-6
   Zhang F, 2012, COMPUT ELECTR ENG, V38, P882, DOI 10.1016/j.compeleceng.2012.03.007
   Zhang R, 2007, IMAGE VISION COMPUT, V25, P321, DOI 10.1016/j.imavis.2005.10.007
   Zhang X, 2017, IEEE T CIRC SYST VID, V27, P1540, DOI 10.1109/TCSVT.2016.2527218
   Zhang YZ, 2015, VISUAL COMPUT, V31, P1615, DOI 10.1007/s00371-014-1043-1
NR 19
TC 0
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1713
EP 1723
DI 10.1007/s00371-018-1601-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300003
DA 2024-07-18
ER

PT J
AU Serpa, YR
   Rodrigues, MAF
AF Serpa, Yvens Reboucas
   Formico Rodrigues, Maria Andreia
TI A draw call-oriented approach for visibility of static and dynamic
   scenes with large number of triangles
SO VISUAL COMPUTER
LA English
DT Article
DE Visibility culling; Spatial partitioning data structures; Heuristics;
   Draw calls; Interactive FPS; Dynamic scenes
AB Graphical applications require high levels of interactivity, usually measured in frames per second (FPS), which motivates the development of increasingly efficient and versatile solutions for the rendering process. Visibility culling algorithms are used to determine which surfaces (or sets of triangles) in a 3D scene are not visible from a certain viewpoint. However, the precision of this step in the graphics pipeline does not necessarily guarantee better FPS rates, since it is directly related to the number of draw calls made to render the set of triangles currently viewed. In addition, this process may be costly enough to compromise the level of interactivity that the application user perceives. In this work, we present RHView, a solution to the visibility problem that focuses on the trade-off between visibility culling precision and the number of draw calls to achieve high FPS rates in complex scenes with approximately 650 million triangles, with both static and dynamic objects. Our approach uses both the view frustum culling and occlusion culling algorithms adapted for greater synergy with RHOctree, a spatial partitioning structure we have designed from heuristics, temporal coherence and index replication to reduce the number of draw calls. The results show that it is possible to achieve FPS rates up to thirty times higher than with some traditional solutions.
C1 [Serpa, Yvens Reboucas; Formico Rodrigues, Maria Andreia] Univ Fortaleza UNIFOR, PPGIA, Av Washington Soares 1321,J30, BR-60811905 Fortaleza, Ceara, Brazil.
C3 Universidade Fortaleza
RP Rodrigues, MAF (corresponding author), Univ Fortaleza UNIFOR, PPGIA, Av Washington Soares 1321,J30, BR-60811905 Fortaleza, Ceara, Brazil.
EM yvensre@gmail.com; andreia.formico@gmail.com
RI Oliveira, Beatriz/KGL-8895-2024; Rodrigues, Maria/KFR-3007-2024
FU CAPES [88881.120921/2016-01]; FUNCAP-CE [PEP-0094-00005.01.09/2014]
FX Maria Andreia Formico Rodrigues and Yvens Reboucas Serpa would like to
   thank the Brazilian Agencies CAPES and FUNCAP-CE for their financial
   support, respectively, under grants 88881.120921/2016-01 and
   PEP-0094-00005.01.09/2014. The authors would like to also thank the
   referees for their valuable comments which helped to improve the
   manuscript.
CR Alderson T, 2015, VISUAL COMPUT, V31, P407, DOI 10.1007/s00371-014-0936-3
   [Anonymous], 2017, RHVIEW STATIC SCENES
   Batagelo HC, 2002, SIBGRAPI 2002: XV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P43, DOI 10.1109/SIBGRA.2002.1167122
   Bittner J, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P207, DOI 10.1109/CGI.1998.694268
   Chen J, 2017, VISUAL COMPUT, V33, P139, DOI 10.1007/s00371-015-1165-0
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   Corporation V, 2007, TEAM FORTRESS 2
   Foundation B, 2016, BLENDER 3D
   Funkhouser T. A., 1993, Computer Graphics Proceedings, P247, DOI 10.1145/166117.166149
   Guthe M., 2006, RENDERING TECHNIQUES, P207
   Hasselgren J., 2016, P HIGH PERF GRAPH, P23
   Hey H., 2001, P 2001 EUR, P43
   Hillesland K., 2002, TECH REP
   Hudson T., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P1, DOI 10.1145/262839.262847
   Mattausch O, 2008, COMPUT GRAPH FORUM, V27, P221, DOI 10.1111/j.1467-8659.2008.01119.x
   Muldoon M, 2015, BLEND SWAP
   Papaioannou G., 2006, P 14 INT C CENTR EUR
   SAMET H, 1988, IEEE COMPUT GRAPH, V8, P48, DOI 10.1109/38.513
   Schmidt MR, 2013, J GRAPH TOOLS, V17, P151, DOI [10.1080/2165347X.2014.909340, DOI 10.1080/2165347X.2014.909340]
   Sekulic  D., 2004, GPU GEMS, P487
   Serpa Y.R., 2016, P 15 SBGAMES SAO PAU, P36
   Stanford University, 2014, The Stanford 3D Scanning Repository
   Toledo L, 2014, VISUAL COMPUT, V30, P949, DOI 10.1007/s00371-014-0975-9
   University ofNorth Carolina, 2001, POW PLANT MOD
   Wimmer M., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P118
   Wloka Matthias, 2003, GAM DEV C
   Zhang H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P77, DOI 10.1145/258734.258781
NR 27
TC 4
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 549
EP 563
DI 10.1007/s00371-018-1484-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800007
DA 2024-07-18
ER

PT J
AU Park, JH
   Nadeem, S
   Kaufman, A
AF Park, Ji Hwan
   Nadeem, Saad
   Kaufman, Arie
TI GeoBrick: exploration of spatiotemporal data
SO VISUAL COMPUTER
LA English
DT Article
DE Visual analytics; Spatiotemporal visualization; Multivariate data;
   Interactive data analysis
ID VISUAL ANALYSIS; COLOR SCHEMES; MASS MOBILITY; VISUALIZATION; SPACE;
   TIME; PATTERNS; SCALE; MAPS
AB We present GeoBrick, an interactive technique for exploring spatiotemporal data. In GeoBrick, each region is comprised of multivariate data, which is encoded into simple shapes with colors. Additionally, users can adjust the resolution of data values to get an overview as well as details of the data. GeoBrick allows users to (1) juxtapose data and spatial profiles of discontiguous regions, (2) identify temporal patterns of user-defined classes of regions, and (3) comparatively evaluate across distinct configurations of regions. We demonstrate the effectiveness and efficacy of GeoBrick using two case studies.
C1 [Park, Ji Hwan; Nadeem, Saad; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Park, JH (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM jihwan@cs.stonybrook.edu; sanadeem@cs.stonybrook.edu;
   ari@cs.stonybrook.edu
RI Park, Ji Hwan/AAW-8991-2020
OI Park, Ji Hwan/0000-0002-7971-2419
FU National Science Foundation [IIP1069147, CNS1302246, IIS1527200,
   NRT1633299, CNS1650499]
FX This work has been partially supported by the National Science
   Foundation Grants IIP1069147, CNS1302246, IIS1527200, NRT1633299, and
   CNS1650499.
CR Andrienko G, 2010, COMPUT GRAPH FORUM, V29, P913, DOI 10.1111/j.1467-8659.2009.01664.x
   Andrienko G, 2017, IEEE T VIS COMPUT GR, V23, P2120, DOI 10.1109/TVCG.2016.2616404
   Andrienko G, 2010, INT J GEOGR INF SCI, V24, P1577, DOI 10.1080/13658816.2010.508043
   Andrienko GL, 1999, INT J GEOGR INF SCI, V13, P355, DOI 10.1080/136588199241247
   [Anonymous], COMPUTER GRAPHICS FO
   [Anonymous], 2012, INFORM VISUAL
   Buschmann S, 2016, VISUAL COMPUT, V32, P371, DOI 10.1007/s00371-015-1185-9
   Cibulski L, 2016, VISUAL COMPUT, V32, P847, DOI 10.1007/s00371-016-1255-7
   Claessen JHT, 2011, IEEE T VIS COMPUT GR, V17, P2310, DOI 10.1109/TVCG.2011.201
   Dorling D. F., 1996, Area Cartograms: Their Use and Creation
   Eppstein D, 2013, IEEE PAC VIS SYMP, P25, DOI 10.1109/PacificVis.2013.6596124
   Fuchs J, 2014, IEEE T VIS COMPUT GR, V20, P2251, DOI 10.1109/TVCG.2014.2346426
   Goodwin S, 2016, IEEE T VIS COMPUT GR, V22, P599, DOI 10.1109/TVCG.2015.2467199
   Gratzl S, 2014, IEEE T VIS COMPUT GR, V20, P2023, DOI 10.1109/TVCG.2014.2346260
   Gratzl S, 2013, IEEE T VIS COMPUT GR, V19, P2277, DOI 10.1109/TVCG.2013.173
   Guo DS, 2006, IEEE T VIS COMPUT GR, V12, P1461, DOI 10.1109/TVCG.2006.84
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Hoeber O, 2011, IEEE PAC VIS SYMP, P139, DOI 10.1109/PACIFICVIS.2011.5742383
   Im JF, 2013, IEEE T VIS COMPUT GR, V19, P2606, DOI 10.1109/TVCG.2013.160
   Jern M, 2006, INFORMATION VISUALIZATION-BOOK, P25
   Kehrer J, 2013, IEEE T VIS COMPUT GR, V19, P2287, DOI 10.1109/TVCG.2013.122
   Meulemans W, 2017, IEEE T VIS COMPUT GR, V23, P381, DOI 10.1109/TVCG.2016.2598542
   Papadopoulos C, 2015, IEEE COMPUT GRAPH, V35, P33, DOI 10.1109/MCG.2014.80
   RAO R, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P318, DOI 10.1145/191666.191776
   Roth RE, 2013, IEEE T VIS COMPUT GR, V19, P2356, DOI 10.1109/TVCG.2013.130
   Sadana R, 2014, IEEE T VIS COMPUT GR, V20, P1993, DOI 10.1109/TVCG.2014.2346249
   Slingsby A, 2011, IEEE T VIS COMPUT GR, V17, P2545, DOI 10.1109/TVCG.2011.197
   Speckmann B, 2010, IEEE T VIS COMPUT GR, V16, P881, DOI 10.1109/TVCG.2010.180
   Swedberg B., 2014, GEOVISUAL ANAL INTER
   Tennekes M, 2014, IEEE T VIS COMPUT GR, V20, P2072, DOI 10.1109/TVCG.2014.2346277
   Tominski C., 2012, P VIS MOD VIS WORK 2, P199
   von Landesberger T, 2016, IEEE T VIS COMPUT GR, V22, P11, DOI 10.1109/TVCG.2015.2468111
   Wickham H, 2011, IEEE T VIS COMPUT GR, V17, P2223, DOI 10.1109/TVCG.2011.227
   Witten I. H., 2005, DATA MINING PRACTICA
   Yuan XR, 2009, IEEE T VIS COMPUT GR, V15, P1001, DOI 10.1109/TVCG.2009.179
NR 35
TC 8
Z9 9
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 191
EP 204
DI 10.1007/s00371-017-1461-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600004
DA 2024-07-18
ER

PT J
AU dos Anjos, RK
   Ribeiro, CS
   Lopes, DS
   Pereira, JM
AF dos Anjos, Rafael Kuffner
   Ribeiro, Claudia Sofia
   Lopes, Daniel Simoes
   Pereira, Joao Madeiras
TI Stroke-based splatting: an efficient multi-resolution point cloud
   visualization technique
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud visualization; Non-photorealistic rendering; Splatting;
   Householder formula
ID RECONSTRUCTION; REPRESENTATION
AB Current state-of-the-art point cloud visualization techniques have shortcomings when dealing with sparse and less accurate data or close-up interactions. In this paper, we present a visualization technique called stroke-based splatting, which applies concepts of stroke-based rendering to surface-aligned splatting, allowing for better shape perception at lower resolutions and close-ups. We create a painterly depiction of the data with an impressionistic aesthetic, which is a metaphor the user is culturally trained to recognize, thus attributing higher quality to the visualization. This is achieved by shaping each object-aligned splat as a brush stroke, and orienting it according to globally coherent tangent vectors from the Householder formula, creating a painterly depiction of the scanned cloud. Each splat is sized according to a color-based clustering analysis of the data, ensuring the consistency of brush strokes within neighborhood areas. By controlling brush shape generation parameters and blending factors between neighboring splats, the user is able to simulate different painting styles in real time. We have tested our method with data sets captured by commodity laser scanners as well as publicly available high-resolution point clouds, both having highly interactive frame rates in all cases. In addition, a user study was conducted comparing our approach to state-of-the-art point cloud visualization techniques. Users considered stroke-based splatting a valuable technique as it provides a higher or similar visual quality to current approaches.
C1 [dos Anjos, Rafael Kuffner; Ribeiro, Claudia Sofia] Univ Nova Lisboa, FCSH, Av Berna 26 C, P-1069061 Lisbon, Portugal.
   [dos Anjos, Rafael Kuffner; Lopes, Daniel Simoes; Pereira, Joao Madeiras] INESC ID Lisboa, Rua Alves Redol 9, P-1000029 Lisbon, Portugal.
C3 Universidade Nova de Lisboa; Universidade de Lisboa; INESC-ID
RP dos Anjos, RK (corresponding author), Univ Nova Lisboa, FCSH, Av Berna 26 C, P-1069061 Lisbon, Portugal.; dos Anjos, RK (corresponding author), INESC ID Lisboa, Rua Alves Redol 9, P-1000029 Lisbon, Portugal.
EM rafael.kuffner@fcsh.unl.pt; claudia.ribeiro@fcsh.unl.pt;
   daniel.lopes@inesc-id.pt; jap@inesc-id.pt
RI Ribeiro, Claudia/JAN-6609-2023; Li, Mengqi/AAG-6804-2021; Pereira,
   João/IAP-6686-2023; Ribeiro, Claudia/EYQ-5320-2022; pereira,
   joao/GWN-1007-2022; Madeiras Pereira, Joao/M-9287-2013; Simoes Lopes,
   Daniel/M-2930-2015
OI Ribeiro, Claudia/0000-0002-1796-0636; Madeiras Pereira,
   Joao/0000-0002-8120-7649; Simoes Lopes, Daniel/0000-0003-0917-9396;
   Kuffner dos Anjos, Rafael/0000-0002-2616-7541
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   [Anonymous], 2011, IEEE INT C ROBOTICS
   [Anonymous], 2009, THESIS
   Awano N, 2010, WSCG 2010: COMMUNICATION PAPERS PROCEEDINGS, P193
   Botsch M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P53
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   Camplani M, 2013, IEEE T CYBERNETICS, V43, P1560, DOI 10.1109/TCYB.2013.2271112
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   Dos Anjos R.K., 2012, J WSCG
   Eberly D., 2016, COMPUTING ORTHONORMA
   Fabio Remondino, 2003, International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, V34, pW10
   Goesele M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778832
   Gooch B., 2002, 2 INT S NONPHOTOREAL, P83
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   Haeberli P., 1990, Computer Graphics, V24, P207, DOI 10.1145/97880.97902
   Healey CG, 2004, ACM T GRAPHIC, V23, P64, DOI 10.1145/966131.966135
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Katz S, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276407, 10.1145/1239451.1239475]
   Kawata H, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P293, DOI 10.1109/CW.2004.42
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Klasing K, 2009, IEEE INT CONF ROBOT, P2011
   Kolluri R., 2004, ser. SGP '04, P11, DOI DOI 10.1145/1057432.1057434
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Lopes DS, 2013, COMPUT AIDED DESIGN, V45, P683, DOI 10.1016/j.cad.2012.11.003
   Lopes DS, 2010, MULTIBODY SYST DYN, V24, P255, DOI 10.1007/s11044-010-9220-0
   Majumder A., 2002, INT S NONPHOTOREALIS, P59, DOI DOI 10.1145/508539.508541
   Meier B. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P477, DOI 10.1145/237170.237288
   Pajarola R, 2004, IEEE T VIS COMPUT GR, V10, P598, DOI 10.1109/TVCG.2004.19
   Preiner Reinhold, 2012, EGPGV, P139
   Ren L, 2002, COMPUT GRAPH FORUM, V21, P461, DOI 10.1111/1467-8659.00606
   Runions A, 2007, VISUAL COMPUT, V23, P945, DOI 10.1007/s00371-007-0153-4
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Sainz M, 2004, COMPUT GRAPH-UK, V28, P869, DOI 10.1016/j.cag.2004.08.014
   Schmid J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964923
   Shakarji CM, 1998, J RES NATL INST STAN, V103, P633, DOI 10.6028/jres.103.043
   SHIRAISHI M, 2000, P 1 INT S NONPH AN R, P53, DOI DOI 10.1145/340916.340923
   Westover L. A., 1991, THESIS
   XU H., 2004, Proceedings of the 3rd International Symposium on Non-Photorealistic Animation and Rendering (NPAR 2006), P25, DOI DOI 10.22633/RPGE.V25I3.15840
   Xu H., 2004, EUR WORKSH REND EUR, DOI [10.2312/EGWR/EGSR04/045-052, DOI 10.2312/EGWR/EGSR04/045-052]
   Yang CK, 2008, VISUAL COMPUT, V24, P303, DOI 10.1007/s00371-007-0183-y
   Zhan Q., 2009, Laser scanning, V38, P155
   Zwicker M., 2004, PERSPECTIVE ACCURATE
NR 43
TC 1
Z9 1
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1383
EP 1397
DI 10.1007/s00371-017-1420-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400009
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU El Hazzat, S
   Merras, M
   El Akkad, N
   Saaidi, A
   Satori, K
AF El Hazzat, Soulaiman
   Merras, Mostafa
   El Akkad, Nabil
   Saaidi, Abderrahim
   Satori, Khalid
TI 3D reconstruction system based on incremental structure from motion
   using a camera with varying parameters
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; Self-calibration; Incremental structure from motion;
   Bundle adjustment
ID SURFACE RECONSTRUCTION; IMAGE; STEREO
AB In this paper, we present a flexible and fast system for multi-scale objects/scenes 3D reconstruction from uncalibrated images/video taken by a moving camera characterized by variable parameters. The proposed system is based on incremental structure from motion and good exploitation of bundle adjustment. At first, from two selected images, our system allows to recover, in a well-chosen reference, coordinates of a set of 3D points. In this context, we have proposed a new method of self-calibration based on the use of two unknown scene points with their image projections. After that, new images are inserted progressively using 3D information already obtained. Local bundle adjustment is used to adjust the new estimated entities. At some time, we introduce a global bundle adjustment to adjust as best as possible all estimated entities and to have an initial 3D model of quality covering an interesting part of the object/scene. This model will be used as reference for the insertion of the rest of images. The proposed system allows to obtain satisfactory results within a reasonable time.
C1 [El Hazzat, Soulaiman; Merras, Mostafa; El Akkad, Nabil; Saaidi, Abderrahim; Satori, Khalid] Sidi Mohamed Ben Abdellah Univ, Dept Math & Informat, LIIAN, Fac Sci Dhar Mahraz, Fes, Morocco.
   [Saaidi, Abderrahim] Sidi Mohamed Ben Abdellah Univ, Polydisciplinary Fac Taza, Dept Math Phys & Informat, LSI, Taza, Morocco.
   [El Akkad, Nabil] Mohamed First Univ, Natl Sch Appl Sci ENSA Al Hoceima, Dept Math & Comp Sci, Oujda, Morocco.
C3 Sidi Mohamed Ben Abdellah University of Fez; Sidi Mohamed Ben Abdellah
   University of Fez; Mohammed First University of Oujda
RP El Hazzat, S (corresponding author), Sidi Mohamed Ben Abdellah Univ, Dept Math & Informat, LIIAN, Fac Sci Dhar Mahraz, Fes, Morocco.
EM soulaiman.elhazzat@yahoo.fr; merras.mostafa@gmail.com;
   nabil.elakkad@usmba.ac.ma; abderrahim.saaidi@usmba.ac.ma;
   khalidsatorim3i@yahoo.fr
RI Saaidi, Abderrahim/R-1916-2019; Merras, Mostafa/AAJ-4405-2020; El
   Hazzat, Soulaiman/AAI-6689-2020; AKKAD, Nabil EL/AAL-4049-2020; satori,
   khalid/GSE-3077-2022
OI Merras, Mostafa/0000-0002-3020-726X; AKKAD, Nabil
   EL/0000-0003-0277-8003; Saaidi, Abderrahim/0000-0003-1708-0468; El
   Hazzat, Soulaiman/0000-0002-6647-1767; SATORI,
   khalid/0000-0001-6055-4169
CR Amenta N., 1998, COMP GRAPH P ACM SIG, P415, DOI DOI 10.1145/280814.280947
   [Anonymous], 2016, IEEE C COMP VIS PATT
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Brown M, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P56, DOI 10.1109/3DIM.2005.81
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Ding L, 2014, VISUAL COMPUT, V30, P189, DOI 10.1007/s00371-013-0795-3
   El Akkad N, 2016, 3D RES, V7, DOI 10.1007/s13319-016-0082-y
   El Hazzat S, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0041-z
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fuhrmann S, 2015, COMPUT GRAPH-UK, V53, P44, DOI 10.1016/j.cag.2015.09.003
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44
   Lhuillier M, 2002, IEEE T PATTERN ANAL, V24, P1140, DOI 10.1109/TPAMI.2002.1023810
   Lim H, 2014, IEEE INT CONF ROBOT, P1532, DOI 10.1109/ICRA.2014.6907055
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Merras M, 2017, INT J AUTOM COMPUT, V14, P661, DOI 10.1007/s11633-016-0999-x
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Strecha C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1194
   Strecha C., 2008, 2008 IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2008.4587706
   Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8
   Tran S, 2006, LECT NOTES COMPUT SC, V3952, P219
   Wang GH, 2009, IEEE T CIRC SYST VID, V19, P1793, DOI 10.1109/TCSVT.2009.2031380
   Wong SS, 2010, PATTERN ANAL APPL, V13, P437, DOI 10.1007/s10044-009-0173-y
   Wu CC, 2014, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2014.11
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Wu J, 2013, MEAS SCI REV, V13, P122, DOI 10.2478/msr-2013-0021
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 36
TC 19
Z9 19
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1443
EP 1460
DI 10.1007/s00371-017-1451-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400013
DA 2024-07-18
ER

PT J
AU Chien, ST
   Hu, CH
   Huang, CY
   Tsai, YT
   Lin, WC
AF Chien, Shuo-Ting
   Hu, Chen-Hui
   Huang, Cheng-Yang
   Tsai, Yu-Ting
   Lin, Wen-Chieh
TI Deformation simulation based on model reduction with rigidity-guided
   sampling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Deformation; Model reduction; Rigidity fields; Finite element method;
   Harmonic fields
ID PROPER ORTHOGONAL DECOMPOSITION; GRAPHICS
AB The deformation results of previous model reduction methods with external forces applied show noticeable differences from full-scale finite element method (FEM) simulation. We found that data-driven approaches, specifically proper orthogonal decomposition, can be a solution to this nonlinear deformation simulation problem in the subspace. Nevertheless, off-line FEM simulation with an infinite number of possible input forces at different locations makes it infeasible if no prior information is given. We propose rigidity-guided sampling to efficiently select the points of application of forces (force sample points) to construct more effective and compact subspace bases, thereby improving the simulation accuracy of reduced deformable models with applied external forces and still retaining fast run-time performance. The key idea of our approach is that distinct deformations of an object at different force sample points can be estimated prior to FEM simulation. By selecting the force sample points with distinct deformations, the computational cost of off-line FEM simulation can be reduced significantly. Our run-time deformation results are much closer to the full-scale FEM simulation with external forces applied, compared to the results of using only the modal derivative bases while the speedup over full-scale simulation is still substantial.
C1 [Chien, Shuo-Ting; Hu, Chen-Hui; Huang, Cheng-Yang; Lin, Wen-Chieh] Natl Chiao Tung Univ, Dept Comp Sci, Coll Comp Sci, Hsinchu, Taiwan.
   [Tsai, Yu-Ting] Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan, Taiwan.
C3 National Yang Ming Chiao Tung University; Yuan Ze University
RP Hu, CH (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Coll Comp Sci, Hsinchu, Taiwan.
EM oppenheimhu@gmail.com
RI Chu, Chen/J-6527-2019; Hu, Chen-Hui/AAE-7912-2021
OI Chu, Chen/0000-0001-8084-0867; Hu, Chen-Hui/0000-0003-4821-1133
FU Ministry of Science and Technology of Taiwan [104-2628-E-009-001-MY3,
   105-2221-E-009-095-MY3]
FX This study was funded in part by the Ministry of Science and Technology
   of Taiwan under Grant Nos. 104-2628-E-009-001-MY3 and
   105-2221-E-009-095-MY3.
CR An SS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409118
   [Anonymous], 12 AIAA ISSMO MULT A
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Au O. K. C., 2007, ACM SIGGRAPH
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Barbic J., 2011, ACM SIGGRAPH
   Barbic J., 2009, COMPUTER GRAPHICS RE
   Bickel B., 2009, ACM T GRAPHIC
   Chatterjee A, 2000, CURR SCI INDIA, V78, P808
   Farhat C, 2015, INT J NUMER METH ENG, V102, P1077, DOI 10.1002/nme.4820
   IDELSOHN SR, 1985, COMPUT METHOD APPL M, V49, P253, DOI 10.1016/0045-7825(85)90125-2
   James DL, 2003, ACM T GRAPHIC, V22, P47, DOI 10.1145/588272.588278
   James DL, 2002, ACM T GRAPHIC, V21, P582, DOI 10.1145/566570.566621
   Kerschen G, 2005, NONLINEAR DYNAM, V41, P147, DOI 10.1007/s11071-005-2803-2
   Koyama Y., 2012, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P19, DOI DOI 10.2312/SCA/SCA12/019-024
   Krysl P, 2001, INT J NUMER METH ENG, V51, P479, DOI 10.1002/nme.167
   Martin S., 2011, ACM SIGGRAPH
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   MURTAGH F, 1983, COMPUT J, V26, P354, DOI 10.1093/comjnl/26.4.354
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Pentland A., 1989, Computer Graphics, V23, P215, DOI 10.1145/74334.74355
   Dinh Q, 2016, IEEE T MAGN, V52, DOI 10.1109/TMAG.2015.2477602
   Schilders WHA, 2008, EUR CONSORT MATH IND, P1, DOI 10.1007/978-3-540-78841-6
   Shabana A. A., 1991, THEORY OF VIBRATION, V2
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, DOI [10.1145/2343483.2343501, DOI 10.1145/2343483.2343501]
   Sin FS, 2013, COMPUT GRAPH FORUM, V32, P36, DOI 10.1111/j.1467-8659.2012.03230.x
   von Tycowicz C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2729972
   Yu DS, 2017, VISUAL COMPUT, V33, P779, DOI 10.1007/s00371-017-1376-7
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
NR 31
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 937
EP 947
DI 10.1007/s00371-018-1533-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400017
DA 2024-07-18
ER

PT J
AU Zheng, ML
   Yuan, ZY
   Tong, QQ
   Zhang, GA
   Zhu, WX
AF Zheng, Mianlun
   Yuan, Zhiyong
   Tong, Qianqian
   Zhang, Guian
   Zhu, Weixu
TI A novel unconditionally stable explicit integration method for finite
   element method
SO VISUAL COMPUTER
LA English
DT Article
DE Finite element method; Implicit integration; Explicit integration;
   Unconditionally stable
ID STRUCTURAL DYNAMICS; SIMULATION; ALGORITHMS; IMPLICIT; FAMILY; CLOTH;
   MODEL
AB Physics-based deformation simulation demands much time in integration process for solving motion equations. To ameliorate, in this paper we resort to structural mechanics and mathematical analysis to develop a novel unconditionally stable explicit integration method for both linear and nonlinear FEM. First we advocate an explicit integration formula with three adjustable parameters. Then we analyze the spectral radius of both linear and nonlinear dynamic transfer function's amplification matrix to obtain limitations for these three parameters to meet unconditional stability conditions. Finally, we theoretically analyze the accuracy property of the proposed method so as to optimize the computational errors. The experimental results indicate that our method is unconditionally stable for both linear and nonlinear systems and its accuracy property is superior to both common and recent explicit and implicit methods. In addition, the proposed method can efficiently solve the problem of huge computation cost in integration procedure for FEM.
C1 [Zheng, Mianlun; Yuan, Zhiyong; Tong, Qianqian; Zhang, Guian; Zhu, Weixu] Wuhan Univ, Sch Comp, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University
RP Yuan, ZY (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Hubei, Peoples R China.
EM zhiyongyuan@whu.edu.cn
OI Yuan, Zhiyong/0000-0001-9608-6037
FU Science and Technology Program of Wuhan, China [2016010101010022];
   National Natural Science Foundation of China [61373107]
FX This work is supported by the Science and Technology Program of Wuhan,
   China, under Grant No. 2016010101010022; National Natural Science
   Foundation of China under Grant No. 61373107.
CR Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Barbic J, 2007, THESIS
   Bathe K., 1973, Earthquake Engineering and Structural Dynamics, V1, P283
   Bathe K J, 1976, NUMERICAL METHODS FI
   Belytschko T., 1983, Computational methods for transient analysis, P1
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Chang SY, 2007, J ENG MECH, V133, P748, DOI 10.1061/(ASCE)0733-9399(2007)133:7(748)
   Chang SY, 2011, EARTHQ ENG ENG VIB, V10, P51, DOI 10.1007/s11803-011-0046-4
   Chang SY, 2009, INT J NUMER METH ENG, V77, P1100, DOI 10.1002/nme.2452
   Chang SY, 2002, J ENG MECH-ASCE, V128, P935, DOI 10.1061/(ASCE)0733-9399(2002)128:9(935)
   Chen C, 2008, J ENG MECH, V134, P676, DOI 10.1061/(ASCE)0733-9399(2008)134:8(676)
   Choi MG, 2005, IEEE T VIS COMPUT GR, V11, P91
   Dick C, 2011, SIMUL MODEL PRACT TH, V19, P801, DOI 10.1016/j.simpat.2010.11.005
   Ding Z, 2016, COMPUT STRUCT, V171, P31, DOI 10.1016/j.compstruc.2016.04.002
   Erleben K., 2005, POINT BASED GRAPH, V30, P340
   Gui Y, 2014, NONLINEAR DYNAM, V77, P1157, DOI 10.1007/s11071-014-1368-3
   Hauth M, 2003, VISUAL COMPUT, V19, P581, DOI 10.1007/s00371-003-0206-2
   Hirota G, 2003, VISUAL COMPUT, V19, P291, DOI 10.1007/s00371-002-0188-5
   Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890
   Hughes T. J., 2000, FINITE ELEMENT METHO
   Hussein B, 2008, NONLINEAR DYNAM, V54, P283, DOI 10.1007/s11071-007-9328-9
   Kang YM, 2001, VISUAL COMPUT, V17, P147, DOI 10.1007/s003710100103
   Krysl P, 2001, INT J NUMER METH ENG, V51, P479, DOI 10.1002/nme.167
   Liu T., ARXIV160407378
   Miller K, 2007, COMMUN NUMER METH EN, V23, P121, DOI 10.1002/cnm.887
   Newmark NM, 1959, J.Eng. Mech. Div., V85, P67, DOI [10.1061/JMCEA3.0000098, DOI 10.1061/JMCEA3.0000098]
   Oh S, 2006, VISUAL COMPUT, V22, P70, DOI 10.1007/s00371-006-0367-x
   PENTLAND A., 1989, COMPUT GRAPHICS-US, V23, P207
   Rezaiee-Pajand M, 2011, INT J NUMER METH ENG, V88, P880, DOI 10.1002/nme.3204
   Sha D, 2003, COMPUT METHOD APPL M, V192, P291, DOI 10.1016/S0045-7825(02)00516-9
   Tamma KK, 2001, INT J NUMER METH ENG, V50, P1619, DOI 10.1002/nme.89
   Taylor ZA, 2008, IEEE T MED IMAGING, V27, P650, DOI 10.1109/TMI.2007.913112
   Wang HM, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980236
   Wilson E.L., 1968, 681 U CAL
   Wriggers P., 2002, COMPUTATIONAL CONTAC, V2
   Yang C, 2016, COMPUT AIDED GEOM D, V43, P53, DOI 10.1016/j.cagd.2016.02.014
   Yang Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818089
   Yang Y, 2013, IEEE T VIS COMPUT GR, V19, P1633, DOI 10.1109/TVCG.2013.12
NR 38
TC 2
Z9 2
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 721
EP 733
DI 10.1007/s00371-017-1410-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100010
DA 2024-07-18
ER

PT J
AU Wang, C
   Chan, SC
   Zhu, ZY
   Zhang, L
   Shum, HY
AF Wang, Chong
   Chan, Shing-Chow
   Zhu, Zhen-Yu
   Zhang, Li
   Shum, Heung-Yeung
TI Superpixel-based color-depth restoration and dynamic environment
   modeling for Kinect-assisted image-based rendering systems
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based rendering; Superpixel; Kinect; Background modeling; Local
   polynomial regression
ID REAL-TIME TRACKING; GESTURE RECOGNITION; FILTER; RECONSTRUCTION;
   REGRESSION; CAMERA
AB Depth information is an important ingredient in many multiview applications including image-based rendering (IBR). With the advent of electronics, low-cost and high-speed depth cameras, such as the Microsoft Kinect, are getting increasingly popular. In this paper, we propose a superpixel-based joint color-depth restoration approach for Kinect depth camera and study its application to view synthesis in IBR systems. Thus, an edge-based matching method is proposed to reduce the color-depth registration errors. Then the Kinect depth map is restored based on probabilistic color-depth superpixels, probabilistic local polynomial regression and joint color-depth matting. The proposed restoration algorithm does not only inpaint the missing data, but also correct and refine the depth map to provide better color-depth consistency. Last but not the least, a dynamic background modeling scheme is proposed to address the disocclusion problem in the view synthesis for dynamic environment. The experimental results show the effectiveness of the proposed algorithm and system.
C1 [Wang, Chong] Ningbo Univ, Ningbo, Zhejiang, Peoples R China.
   [Chan, Shing-Chow; Zhang, Li] Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Zhu, Zhen-Yu] DJI Corp, Shenzhen, Peoples R China.
   [Shum, Heung-Yeung] Microsoft Corp, Redmond, WA 98052 USA.
C3 Ningbo University; University of Hong Kong; Microsoft
RP Wang, C (corresponding author), Ningbo Univ, Ningbo, Zhejiang, Peoples R China.
EM wangchong@nbu.edu.cn
RI Wang, Chong/IRZ-7328-2023
OI Wang, Chong/0000-0001-6016-6545
FU K.C. Wong Magna Fund in Ningbo University; National Natural Science
   Foundation of China [61603202]; Zhejiang Open Foundation from
   Information and Communication Engineering of the Most Important
   Subjects, China [xkxl1512, xkxl1526]; Open Project Program of the State
   Key Lab of CAD&CG in Zhejiang University [A1606]; Research Foundation of
   Education Department of Zhejiang Province, China [Y201533827]; Zhejiang
   Provincial Natural Science Foundation, China [LQ16F030001]; Ningbo
   Natural Science Foundation, China [2016A610070]; General Research Fund
   (GRF) of Hong Kong Research Grant Council (RGC)
FX This work was supported in part by K.C. Wong Magna Fund in Ningbo
   University; National Natural Science Foundation of China (61603202);
   Zhejiang Open Foundation from Information and Communication Engineering
   of the Most Important Subjects, China (xkxl1512, xkxl1526); the Open
   Project Program of the State Key Lab of CAD&CG in Zhejiang University
   (A1606); the Research Foundation of Education Department of Zhejiang
   Province, China (Y201533827); Zhejiang Provincial Natural Science
   Foundation, China (LQ16F030001); Ningbo Natural Science Foundation,
   China (2016A610070) and the General Research Fund (GRF) of Hong Kong
   Research Grant Council (RGC).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2008, Image-Based Rendering
   BERMAN A, 2000, Patent No. 6134346
   Burrus N., 2014, KINECT CALIBRATION
   Butler D.A., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '12, (New York, NY, USA), P1933, DOI [DOI 10.1145/2208276.2208335, DOI 10.1145/2207676.2208335]
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen XW, 2013, PROC CVPR IEEE, P1902, DOI 10.1109/CVPR.2013.248
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Han JG, 2012, IEEE T CONSUM ELECTR, V58, P255, DOI 10.1109/TCE.2012.6227420
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   Janoch A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1168, DOI 10.1109/ICCVW.2011.6130382
   Katkovnik V, 2005, IEEE T IMAGE PROCESS, V14, P1469, DOI 10.1109/TIP.2005.851705
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kong XN, 2013, IEEE T KNOWL DATA EN, V25, P704, DOI 10.1109/TKDE.2011.141
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Liu W, 2016, VISUAL COMPUT, V32, P579, DOI 10.1007/s00371-015-1074-2
   Matyunin S., 2011, 3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video, P1
   Munshi A, 2014, OPENCL SPECIFICATION
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Vázquez C, 2006, PROC SPIE, V6392, DOI 10.1117/12.685047
   Wang C, 2013, IEEE INT SYMP CIRC S, P1388, DOI 10.1109/ISCAS.2013.6572114
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang C, 2015, J SIGNAL PROCESS SYS, V79, P1, DOI [10.1007/s11265-013-0819-2, 10.1155/2013/390534]
   Wang YK, 2014, VISUAL COMPUT, V30, P1157, DOI 10.1007/s00371-013-0896-z
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zhang CY, 2011, PROF GEOGR, V63, P262, DOI 10.1080/00330124.2010.547792
   Zhang ZG, 2011, J SIGNAL PROCESS SYS, V64, P361, DOI 10.1007/s11265-010-0495-4
   Zhang ZQ, 2012, IEEE T INSTRUM MEAS, V61, P2817, DOI 10.1109/TIM.2012.2196397
   Zhu ZY, 2012, IEEE T CIRC SYST VID, V22, P1405, DOI 10.1109/TCSVT.2012.2198133
   Zitnick CL, 2007, INT J COMPUT VISION, V75, P49, DOI 10.1007/s11263-006-0018-8
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 35
TC 8
Z9 8
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 67
EP 81
DI 10.1007/s00371-016-1312-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200008
DA 2024-07-18
ER

PT J
AU Wang, YM
   Che, WJ
   Xu, B
AF Wang, Yumeng
   Che, Wujun
   Xu, Bo
TI Encoder-decoder recurrent network model for interactive character
   animation generation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Human-character interaction; Long short-term memory; Encoder-decoder;
   Character animation; Recurrent neural network; Motion capture data
AB In this paper, we propose a generative recurrent model for human-character interaction. Our model is an encoder-recurrent-decoder network. The recurrent network is composed by multiple layers of long short-term memory (LSTM) and is incorporated with an encoder network and a decoder network before and after the recurrent network. With the proposed model, the virtual character's animation is generated on the fly while it interacts with the human player. The coming animation of the character is automatically generated based on the history motion data of both itself and its opponent. We evaluated our model based on both public motion capture databases and our own recorded motion data. Experimental results demonstrate that the LSTM layers can help the character learn a long history of human dynamics to animate itself. In addition, the encoder-decoder networks can significantly improve the stability of the generated animation. This method can automatically animate a virtual character responding to a human player.
C1 [Wang, Yumeng; Che, Wujun; Xu, Bo] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
   [Wang, Yumeng] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Che, WJ (corresponding author), Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
EM yumeng.wang@ia.ac.cn; wujun.che@ia.ac.cn; xubo@ia.ac.cn
FU National Natural Science Foundation of China [61471359]; National Key
   Technology R&D Program of China [2015BAH53F01]
FX This work was supported by National Natural Science Foundation of China
   under No. 61471359, by National Key Technology R&D Program of China
   under No. 2015BAH53F01.
CR [Anonymous], 2015, ARXIV151106653
   [Anonymous], 2001, A Field Guide to Dynamical Recurrent Networks
   [Anonymous], ARXIV160403692
   [Anonymous], 2014, P 20 ACM S VIRT REAL
   [Anonymous], 2016, ARXIV160200991
   [Anonymous], 2016, P 9 INT C MOTION GAM
   [Anonymous], 2016, ARXIV160307772
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bin Peng X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925881
   Deng LQ, 2011, COMPUT ANIMAT VIRT W, V22, P229, DOI 10.1002/cav.397
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Feng AndrewW., 2012, I3D, P95, DOI [DOI 10.1145/2159616.2159632, 10.1145/2159616.2159632]
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Graves A., 2013, Generating sequences with recurrent neural networks
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2012, STUD COMPUT INTELL, V385, P5
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Ho ESL, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487274
   Ho ESL, 2011, COMPUT ANIMAT VIRT W, V22, P435, DOI 10.1002/cav.376
   Ho ESL, 2009, COMPUT GRAPH FORUM, V28, P299, DOI 10.1111/j.1467-8659.2009.01369.x
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Kim J, 2013, COMPUT ANIMAT VIRT W, V24, P565, DOI 10.1002/cav.1557
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Multon F, 2008, LECT NOTES COMPUT SC, V5277, P72
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Sak H, 2014, INTERSPEECH, P338
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutskever Ilya, 2011, P 28 INT C MACH LEAR, P1017
   Tan J, 2011, IEEE COMPUT GRAPH, V31, P34, DOI 10.1109/MCG.2011.30
   van Welbergen H, 2010, COMPUT GRAPH FORUM, V29, P2530, DOI 10.1111/j.1467-8659.2010.01822.x
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vogt D, 2014, LECT NOTES ARTIF INT, V8637, P463, DOI 10.1007/978-3-319-09767-1_57
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wang YM, 2016, PROC INT C TOOLS ART, P291, DOI [10.1109/ICTAI.2016.49, 10.1109/ICTAI.2016.0052]
   Wang YM, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P780, DOI 10.1109/ICALIP.2014.7009901
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Ye YT, 2010, COMPUT GRAPH FORUM, V29, P555, DOI 10.1111/j.1467-8659.2009.01625.x
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
NR 46
TC 9
Z9 9
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 971
EP 980
DI 10.1007/s00371-017-1378-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800027
DA 2024-07-18
ER

PT J
AU Wang, R
   Dong, H
   Han, TX
   Mei, L
AF Wang, Rui
   Dong, Hao
   Han, Tony X.
   Mei, Lei
TI Robust tracking via monocular active vision for an intelligent teaching
   system
SO VISUAL COMPUTER
LA English
DT Article
DE Long-term tracking; Informative random fern; Monocular active vision;
   Intelligent tracking teaching system
ID VISUAL TRACKING
AB The research of this paper investigates a practical intelligent tracking teaching system, addressing the problem of teacher detection and tracking via monocular active vision in real time. The split lines and position-based visual servo rules are created to realize the robust and stable tracking, which is designed to keep the tracked teacher in the middle of image with a fixed size by automatically controlling a pan/tilt/zoom monocular camera in either rostrum region or other regions in the classroom. Face tracking in rostrum region is initiated by a face detector based on Adaboost followed by a novel long-term tracking algorithm named as informative random fern-tracking-learning-detection (IRF-TLD), which has advantages for its high accuracy and low memory requirement using real-valued feature and Gaussian random projection. Moreover, Gaussian mixture model can be automatically started to detect the teacher's movement when face tracking fails or stand-up students are detected. Experimental results on many benchmark sequences, which include various challenges for tracking, such as occlusion, illumination and pose variations, and scaling, have demonstrated the superior performance of the proposed IRF-TLD method when compared with several state-of-the-art tracking algorithms. Extensive experiments in a series of challenging real classroom scenarios also demonstrate the effectiveness of the complete system.
C1 [Wang, Rui; Dong, Hao] Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [Han, Tony X.] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Mei, Lei] China Elect Technol Grp Corp, Software Syst Res Dept, Res Inst 38, 199 Xiangzhang Ave, Hefei 230088, Anhui, Peoples R China.
C3 Beihang University; University of Missouri System; University of
   Missouri Columbia; China Electronics Technology Group
RP Dong, H (corresponding author), Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Lab Precis Optomechatron Technol, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
EM wangr@buaa.edu.cn; qianxin_dh@163.com
FU National Natural Science Foundation of China [60974108]
FX The authors thank the anonymous reviewers for helping to review this
   paper. This work was partially supported by National Natural Science
   Foundation of China (60974108).
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Al Haj Murad, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1690, DOI 10.1109/ICPR.2010.418
   [Anonymous], 2015, TRANSFERRING RICH FE
   [Anonymous], 2015, ARXIV150104505
   [Anonymous], 2014, PERIPHERAL NEUROPATH
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   Chen HK, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P2860, DOI 10.1109/WCICA.2014.7053182
   Grabner H., 2006, BMVC, P47
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalal Z, 2010, IEEE IMAGE PROC, P3789, DOI 10.1109/ICIP.2010.5653525
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Ozuysal M., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383123
   Pan Feng, 2004, Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788), P3846, DOI 10.1109/WCICA.2004.1342210
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Ruiguo Y., 2009, 2 INT C IM SIGN PROC, P1
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Stiefelhagen R., 2007, CVPR, P1, DOI 10.1109/CVPR.2007.383502
   Tsuruoka S, 2001, 10TH IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOLS 1-3, P940, DOI 10.1109/FUZZ.2001.1009111
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang R, 2013, IEEE CONF IMAGING SY, P431, DOI 10.1109/IST.2013.6729736
   [王睿 WANG Rui], 2008, [红外与激光工程, Infrared and Laser engineering], V37, P616
   Wang ZH, 2015, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-662-44365-1
   Wang ZJ, 2014, VISUAL COMPUT, V30, P173, DOI 10.1007/s00371-013-0793-5
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   Wulff B., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P549, DOI 10.1109/ISM.2011.97
   Zafeiriou S, 2015, COMPUT VIS IMAGE UND, V138, P1, DOI 10.1016/j.cviu.2015.03.015
   Zhang J, 2014, SIGNAL PROCESS-IMAGE, V29, P987, DOI 10.1016/j.image.2014.06.009
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 39
TC 16
Z9 19
U1 0
U2 33
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1379
EP 1394
DI 10.1007/s00371-015-1206-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000003
DA 2024-07-18
ER

PT J
AU Luo, ZX
   Wang, Q
   Fan, X
   Gao, YQ
   Shui, PP
AF Luo, Zhongxuan
   Wang, Qian
   Fan, Xin
   Gao, Yaqi
   Shui, Panpan
TI Generalized rational B,zier curves for the rigid body motion design
SO VISUAL COMPUTER
LA English
DT Article
DE Generalized rational Bezier curves; Generalized rational de Casteljau
   algorithm; Hermite interpolation; Geometrical continuity; Rigid body
   motion
ID SPHERICAL SPLINES; INTERPOLATION; CONSTRUCTION
AB In this paper, we present a new method for the smooth interpolation of the orientations of a rigid body motion. The method is based on the geometrical Hermite interpolation in a hypersphere. However, the non-Euclidean structure of a sphere brings a great challenge to the interpolation problem. For this consideration and the requirements for practical application, we construct the spherical analogue of classical rational B,zier curves, called generalized rational B,zier curves. The new spherical curves are obtained using the generalized rational de Casteljau algorithm, which is a generalization of the classical rational de Casteljau algorithm to a hypersphere. Then, Hermite interpolation problem in hypersphere is solved analytically using the generalized rational B,zier curve of degree 5. The new method offers residual free parameters including shape parameters and weights, which guarantee the existence of the interpolant to arbitrary motion data and offer great flexibility for the shape design of the motion. Numerical examples show that our method is far better behaved according to the energy functional which is regarded as a measure of the motion shape.
C1 [Luo, Zhongxuan; Fan, Xin; Gao, Yaqi; Shui, Panpan] Dalian Univ Technol, Sch Software, Dalian 116620, Peoples R China.
   [Luo, Zhongxuan; Wang, Qian] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Luo, Zhongxuan; Fan, Xin] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116620, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Luo, ZX (corresponding author), Dalian Univ Technol, Sch Software, Dalian 116620, Peoples R China.; Luo, ZX (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.; Luo, ZX (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116620, Peoples R China.
EM zxluo@dlut.edu.cn; wangqian603603@sina.com; xin.fan@ieee.org
FU National Natural Science Foundation of China [11171052, 61033012,
   61432003, 61272371, 61003177, 61328206]; program for New Century
   Excellent Talents [NCET-11-0048]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. 11171052, 61033012, 61432003, 61272371, 61003177 and
   61328206), and the program for New Century Excellent Talents
   (NCET-11-0048).
CR [Anonymous], 1996, CURVES SURFACES COMP
   [Anonymous], 1985, ACM SIGGRAPH COMPUTE
   BARSKY BA, 1989, IEEE COMPUT GRAPH, V9, P60, DOI 10.1109/38.41470
   Bezier P., 1986, MATH BASIC UNISURF C
   Bottema O., 1990, THEORETICAL KINEMATI
   Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124
   Curtis Michael, 1979, Matrix Groups
   DE CASTELJAU P., 1959, OUTILLAGES METHODES
   DIETZ R, 1993, COMPUT AIDED GEOM D, V10, P211, DOI 10.1016/0167-8396(93)90037-4
   FARIN G, 1983, COMPUT AIDED DESIGN, V15, P73, DOI 10.1016/0010-4485(83)90171-9
   Farouki RT, 2013, COMPUT AIDED GEOM D, V30, P653, DOI 10.1016/j.cagd.2013.03.001
   GE QJ, 1994, J MECH DESIGN, V116, P756, DOI 10.1115/1.2919447
   GE QJ, 1994, J MECH DESIGN, V116, P749, DOI 10.1115/1.2919446
   Gfrerrer A, 1999, COMPUT AIDED GEOM D, V16, P21, DOI 10.1016/S0167-8396(98)00027-2
   Hartmann E, 1996, VISUAL COMPUT, V12, P181
   Horsch T, 1998, COMPUT AIDED DESIGN, V30, P217, DOI 10.1016/S0010-4485(97)00061-4
   Jaklic G, 2013, J COMPUT APPL MATH, V240, P20, DOI 10.1016/j.cam.2012.08.021
   Jüttler B, 2003, COMPUT AIDED GEOM D, V20, P621, DOI 10.1016/j.cagd.2003.07.003
   Krajnc M, 2014, COMPUT AIDED GEOM D, V31, P427, DOI 10.1016/j.cagd.2014.06.001
   Krajnc M, 2014, J COMPUT APPL MATH, V256, P92, DOI 10.1016/j.cam.2013.07.014
   Luo Zhongxuan, 2012, [Journal of Mathematical Research with Applications, 数学研究及应用], V32, P379
   Myoung-Jun Kim, 1995, Computer Graphics Proceedings. SIGGRAPH 95, P369
   Nielson GM, 2004, IEEE T VIS COMPUT GR, V10, P224, DOI 10.1109/TVCG.2004.1260774
   Noakes L, 2006, COMP IMAG VIS, P77
   PARKER RL, 1979, GEOPHYS J ROY ASTR S, V58, P685, DOI 10.1111/j.1365-246X.1979.tb04802.x
   POBEGAILO AP, 1994, VISUAL COMPUT, V11, P63, DOI 10.1007/BF01900700
   Pockaj K, 2014, NUMER ALGORITHMS, V66, P721, DOI 10.1007/s11075-013-9756-1
   Popiel T, 2006, COMPUT AIDED GEOM D, V23, P261, DOI 10.1016/j.cagd.2005.11.003
   Pottmann H, 2005, COMPUT AIDED GEOM D, V22, P693, DOI 10.1016/j.cagd.2005.06.006
   Wang WP, 2000, VISUAL COMPUT, V16, P187, DOI 10.1007/s003710050207
   Wang XP, 2010, VISUAL COMPUT, V26, P813, DOI 10.1007/s00371-010-0462-x
NR 31
TC 3
Z9 5
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1071
EP 1084
DI 10.1007/s00371-015-1173-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400002
DA 2024-07-18
ER

PT J
AU Kleiman, Y
   Goldberg, G
   Amsterdamer, Y
   Cohen-Or, D
AF Kleiman, Yanir
   Goldberg, George
   Amsterdamer, Yael
   Cohen-Or, Daniel
TI Toward semantic image similarity from crowdsourced clustering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Crowdsourcing; Image similarity; Image distance metric
AB Determining the similarity between images is a fundamental step in many applications, such as image categorization, image labeling and image retrieval. Automatic methods for similarity estimation often fall short when semantic context is required for the task, raising the need for human judgment. Such judgments can be collected via crowdsourcing techniques, based on tasks posed to web users. However, to allow the estimation of image similarities in reasonable time and cost, the generation of tasks to the crowd must be done in a careful manner. We observe that distances within local neighborhoods provide valuable information that allows a quick and accurate construction of the global similarity metric. This key observation leads to a solution based on clustering tasks, comparing relatively similar images. In each query, crowd members cluster a small set of images into bins. The results yield many relative similarities between images, which are used to construct a global image similarity metric. This metric is progressively refined, and serves to generate finer, more local queries in subsequent iterations. We demonstrate the effectiveness of our method on datasets where ground truth is available, and on a collection of images where semantic similarities cannot be quantified. In particular, we show that our method outperforms alternative baseline approaches, and prove the usefulness of clustering queries, and of our progressive refinement process.
C1 [Kleiman, Yanir; Goldberg, George; Cohen-Or, Daniel] Tel Aviv Univ, Tel Aviv, Israel.
   [Amsterdamer, Yael] Bar Ilan Univ, Ramat Gan, Israel.
C3 Tel Aviv University; Bar Ilan University
RP Amsterdamer, Y (corresponding author), Bar Ilan Univ, Ramat Gan, Israel.
EM yael.amsterdamer@biu.ac.il
OI Amsterdamer, Yael/0000-0001-8032-9962
CR [Anonymous], 2002, ADV NEURAL INF PROCE
   [Anonymous], P ADV NEURAL INFORM
   [Anonymous], IEEE T COMPUTERS
   [Anonymous], 2013, Proceedings of the 16th International Conference on Database Theory, DOI DOI 10.1145/2448496.2448524
   [Anonymous], C HUM COMP CROWDS
   Bar-Hillel AB, 2005, J MACH LEARN RES, V6, P937
   Biswas A, 2014, INT J COMPUT VISION, V108, P133, DOI 10.1007/s11263-013-0680-6
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Frome A., 2007, INT C COMPUTER VISIO, P1
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Gomes R. G., 2011, NIPS, P558
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lun ZL, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766929
   Marcus A, 2011, PROC VLDB ENDOW, V5, P13
   O'Donovan P, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601110
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Saleh B., 2015, Proceedings of the 41st Graphics Interface Conference, P59
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tamuz O., 2011, P 28 INT C MACH LEAR, P673
   Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263
   Yi J., 2012, Advances in Neural Information Processing Systems, P1772
   Zha Zheng-Jun, 2008, CVPR, P1
NR 23
TC 4
Z9 5
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1045
EP 1055
DI 10.1007/s00371-016-1266-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600035
DA 2024-07-18
ER

PT J
AU Nguyen, HM
   Wünsche, B
   Delmas, P
   Lutteroth, C
   Zhang, EGN
AF Hoang Minh Nguyen
   Wuensche, Burkhard
   Delmas, Patrice
   Lutteroth, Christof
   Zhang, Eugene
TI A robust hybrid image-based modeling system
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape recovery; Texture acquisition; Surface reconstruction; Surface
   parameterization; Texture inpainting
ID SHAPE-FROM-SILHOUETTE; TIME PART
AB This paper presents a new robust image-based modeling system for creating high-quality 3D models of complex objects from a sequence of unconstrained photographs. The images can be acquired by a video camera or hand-held digital camera without the need of camera calibration. In contrast to previous methods, we integrate correspondence-based and silhouette-based approaches, which significantly enhances the reconstruction of objects with few visual features (e.g., uni-colored objects) and improves surface smoothness. Our solution uses a mesh segmentation and charting approach in order to create a low-distortion mesh parameterization suitable for objects of arbitrary genus. A high-quality texture is produced by first parameterizing the reconstructed objects using a segmentation and charting approach, projecting suitable sections of input images onto the model, and combining them using a graph-cut technique. Holes in the texture due to surface patches without projecting input images are filled using a novel exemplar-based inpainting method which exploits appearance space attributes to improve patch search, and blends patches using Poisson-guided interpolation. We analyzed the effect of different algorithm parameters, and compared our system with a laser scanning-based reconstruction and existing commercial systems. Our results indicate that our system is robust, superior to other image-based modeling techniques, and can achieve a reconstruction quality visually not discernible from that of a laser scanner.
C1 [Hoang Minh Nguyen; Wuensche, Burkhard; Delmas, Patrice; Lutteroth, Christof] Univ Auckland, Auckland 1, New Zealand.
   [Zhang, Eugene] Oregon State Univ, Corvallis, OR 97331 USA.
C3 University of Auckland; Oregon State University
RP Nguyen, HM (corresponding author), Univ Auckland, Auckland 1, New Zealand.
EM hngu039@aucklanduni.ac.nz; burkhard@cs.auckland.ac.nz;
   p.delmas@auckland.ac.nz; lutteroth@cs.auckland.ac.nz;
   zhange@eecs.oregonstate.edu
RI Lutteroth, Christof/A-3003-2010
OI Lutteroth, Christof/0000-0003-0634-7569; Wuensche,
   Burkhard/0000-0002-8013-4118
CR Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   [Anonymous], THESIS STANFORD U
   [Anonymous], P GRAPP 2011 ALG POR
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Brown M, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P56, DOI 10.1109/3DIM.2005.81
   Cheung KM, 2005, INT J COMPUT VISION, V63, P225, DOI 10.1007/s11263-005-6879-4
   Cheung KM, 2005, INT J COMPUT VISION, V62, P221, DOI 10.1007/s11263-005-4881-5
   Clark X B, 2012, P 27 C IM VIS COMP N, P480
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Franco J.S., 2003, P 14 BRIT MACHINE VI, P329, DOI [10.5244/C.17.32, DOI 10.5244/C.17.32]
   Franco JS, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P397
   Garai G, 1999, IMAGE VISION COMPUT, V17, P75, DOI 10.1016/S0262-8856(98)00089-4
   Grauman K, 2003, PROC CVPR IEEE, P187
   Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190
   Hernández C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Nguyen HM, 2012, WSCG'2012, CONFERENCE PROCEEDINGS, PTS I & II, P249
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manke F, 2009, GRAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P5
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Nguyen H.M., 2012, THESIS U AUCKLAND NZ
   Nguyen HM, 2013, INT CONF IMAG VIS, P226, DOI 10.1109/IVCNZ.2013.6727020
   Nguyen M.H., 2013, P 36 AUSTR COMP SCI
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Quan L, 2006, ACM T GRAPHIC, V25, P599, DOI 10.1145/1141911.1141929
   Rakhmanov EA, 1994, MATH RES LETT, V1, P647
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Sander PedroV., 2002, EGRW 02, P87
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Zambanini S, 2013, LECT NOTES COMPUT SC, V7944, P11
   Zhang E, 2005, ACM T GRAPHIC, V24, P1, DOI 10.1145/1037957.1037958
NR 43
TC 6
Z9 6
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 625
EP 640
DI 10.1007/s00371-015-1078-y
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800008
DA 2024-07-18
ER

PT J
AU Hong, JY
   Way, DL
   Shih, ZC
   Tai, WK
   Chang, CC
AF Hong, Jhen-Yao
   Way, Der-Lor
   Shih, Zen-Chung
   Tai, Wen-Kai
   Chang, Chin-Chen
TI Inner engraving for the creation of a balanced LEGO sculpture
SO VISUAL COMPUTER
LA English
DT Article
DE LEGO; Brick layout; Voxel; Center of mass; Legolization
ID 3D; MODELS
AB LEGO is a globally popular toy composed of colorful interlocking plastic bricks that can be assembled in many ways; however, this special feature makes designing a LEGO sculpture particularly challenging. Building a stable sculpture is not easy for a beginner; even an experienced user requires a good deal of time to build one. This paper provides a novel approach to creating a balanced LEGO sculpture for a 3D model in any pose, using centroid adjustment and inner engraving. First, the input 3D model is transformed into a voxel data structure. Next, the model's centroid is adjusted to an appropriate position using inner engraving to ensure that the model stands stably. A model can stand stably without any struts when the center of mass is moved to the ideal position. Third, voxels are merged into layer-by-layer brick layout assembly instructions. Finally, users will be able to build a LEGO sculpture by following these instructions. The proposed method is demonstrated with a number of LEGO sculptures and the results of the physical experiments are presented.
C1 [Way, Der-Lor] Taipei Natl Univ Arts, Dept NewMedia Art, 1 Hsueh Yuan Rd, Taipei 112, Taiwan.
   [Hong, Jhen-Yao; Shih, Zen-Chung] Natl Chiao Tung Univ, Inst Multimedia Engn, 1001 Univ Rd, Hsinchu 300, Taiwan.
   [Tai, Wen-Kai] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Hualien 97401, Taiwan.
   [Chang, Chin-Chen] Natl United Univ, Dept Comp Sci & Informat Engn, Miaoli 36003, Taiwan.
C3 National Yang Ming Chiao Tung University; National Dong Hwa University;
   National United University
RP Way, DL (corresponding author), Taipei Natl Univ Arts, Dept NewMedia Art, 1 Hsueh Yuan Rd, Taipei 112, Taiwan.
EM adler@newmedia.tnua.edu.tw
RI Chang, Ching-Chun/JAN-6210-2023; WAY, Der-Lor/HLG-1230-2023
CR Akenine-Moller T., 2005, P SIGGRAPH COURS, V8
   ANDREW AM, 1979, INFORM PROCESS LETT, V9, P216, DOI 10.1016/0020-0190(79)90072-3
   Gower R.A. H., 1998, 32 EUROPEAN STUDY GR, P81
   Holroyd M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024221
   Iizuka S, 2011, VISUAL COMPUT, V27, P605, DOI 10.1007/s00371-011-0564-0
   Kilian M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360674
   Kim J.W., 2014, Proc. WSCG, P89
   Lambrecht B., 2008, VOXELIZATION BOUNDAR
   Li XY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778848
   Li XY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964993
   Lo KY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618503
   Luo LJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366148
   Sá AME, 2014, VISUAL COMPUT, V30, P1321, DOI 10.1007/s00371-013-0883-4
   ONO S, 2013, ITE T MEDIA TECHNOL, V1, P354
   Petrovic P., 2001, TECHNICAL REPORT
   Peysakhov M, 2003, AI EDAM, V17, P155, DOI 10.1017/S0890060403172046
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Rivers A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185584
   Rivers A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366176
   Shatz I, 2006, VISUAL COMPUT, V22, P825, DOI 10.1007/s00371-006-0067-6
   Smal E., 2008, THESIS U STELLENBOSC
   Song P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366147
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Tachi T, 2010, IEEE T VIS COMPUT GR, V16, P298, DOI 10.1109/TVCG.2009.67
   Testuz Romain., 2013, EUROGRAPHICS SHORT P, P81
   Van Zijl L., 2008, P AUT THEOR APPL CEL, P425
   Way DL, 2013, J INF SCI ENG, V29, P1195
   Weyrich T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239483
   Winkler D., 2005, AUTOMATED BRICK LAYO
   Xin SQ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964992
NR 30
TC 7
Z9 7
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 569
EP 578
DI 10.1007/s00371-015-1072-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800003
DA 2024-07-18
ER

PT J
AU Hasan, M
   Samavati, FF
   Jacob, C
AF Hasan, Mahmudul
   Samavati, Faramarz F.
   Jacob, Christian
TI Interactive multilevel focus plus context visualization framework
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Focus plus context visualization; Contextual close-up; Multilevel
   hierarchy; Balanced decomposition; Perfect reconstruction; Balanced
   wavelet transform
ID MULTIRESOLUTION
AB In this article, we present the construction of an interactive multilevel focus+context visualization framework for the navigation and exploration of large-scale 2D and 3D images. The presented framework utilizes a balanced multiresolution technique supported by a balanced wavelet transform (BWT). It extends the mode of focus+context visualization, where spatially separate magnification of regions of interest (ROIs) is performed, as opposed to in-place magnification. Each resulting visualization scenario resembles a tree structure, where the root constitutes the main context, each non-root internal node plays the dual roles of both focus and context, and each leaf solely represents a focus. Our developed prototype supports interactive manipulation of the visualization hierarchy, such as addition and deletion of ROIs and desired changes in their resolutions at any level of the hierarchy on the fly. We describe the underlying data structure efficiently support such operations. Changes in the spatial locations of query windows defining the ROIs trigger on-demand reconstruction queries. We explain in detail how to efficiently process such reconstruction queries within the hierarchy of details (wavelet coefficients) contained in the BWT in order to ensure real-time feedback. As the BWT need only be constructed once in a preprocessing phase on the server-side and robust on-demand reconstruction queries require minimal data communication overhead, our presented framework is a suitable candidate for efficient web-based visualization of complex large-scale imagery. We also discuss the performance characteristics of our proposed framework from various aspects, such as time and space complexities and achieved frame rates.
C1 [Hasan, Mahmudul; Samavati, Faramarz F.; Jacob, Christian] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Jacob, Christian] Univ Calgary, Dept Biochem & Mol Biol, Calgary, AB, Canada.
C3 University of Calgary; University of Calgary
RP Hasan, M (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM mhasan@ucalgary.ca
CR [Anonymous], 2006, P 8 EUR IEEE VGTC S, DOI [DOI 10.2312/VISSYM/EUROVIS06/347-354, 10.2312/ VisSym/EuroVis06/347-354]
   Bartels RH, 2006, BIT, V46, P455, DOI 10.1007/s10543-006-0075-y
   Bartels R, 2011, COMPUT GRAPH-UK, V35, P185, DOI 10.1016/j.cag.2010.12.001
   Card S.K., 2002, Proceedings of the 2002 Working Conference on Advanced Visual Interfaces, P231, DOI [DOI 10.1145/1556262.1556300, 10.1145/1556262.1556300]
   Chaikin GM., 1974, COMPUT GRAPHICS IMAG, V3, P346, DOI DOI 10.1016/0146-664X(74)90028-8
   Cohen M, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P32, DOI 10.1109/TPCG.2004.1314450
   Cossalter M., 2013, P SPIE C VIS DAT AN, V8654, P1, DOI [10.1117/12.2005096.865403, DOI 10.1117/12.2005096.865403]
   Hasan M, 2015, GRAPH MODELS, V78, P36, DOI 10.1016/j.gmod.2015.01.001
   Hasan M, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P145, DOI 10.1109/CW.2014.28
   Hauser H., 2006, SCI VISUALIZATION VI, P305, DOI [DOI 10.1007/3-540-30790-7_18, 10.1007/354030790718]
   Hodges ElaineR S., 2003, GUILD HDB SCI ILLUST, VSecond
   Hsu WH, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024165
   Kalkofen D., 2007, P INT S MIXED AUGMEN, P191, DOI [10.1109/ISMAR.2007.4538846, DOI 10.1109/ISMAR.2007.4538846]
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   Mendez Erick, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P209, DOI 10.1109/ISMAR.2006.297816
   Packer J.F., 2013, THESIS U CALGARY CAL
   Plate J., 2002, VISSYM 02, P53, DOI DOI 10.2312/VISSYM/VISSYM02/053-060
   Ropinski Timo., 2009, Proceedings of EG UK Theory and Practice of Computer Graphics, P17, DOI 10. 2312/LocalChapterEvents/TPCG/TPCG09/017-024 20
   Samavati FF, 2007, SER MACH PERCEPT ART, V67, P65
   Samavati FF, 1999, COMPUT GRAPH FORUM, V18, P97, DOI 10.1111/1467-8659.00361
   Suter SK, 2011, IEEE T VIS COMPUT GR, V17, P2135, DOI 10.1109/TVCG.2011.214
   Tu Y, 2008, IEEE T VIS COMPUT GR, V14, P1157, DOI 10.1109/TVCG.2008.114
   Wang CL, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P259
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P171, DOI 10.1109/TVCG.2010.34
   Wong PC, 2012, IEEE COMPUT GRAPH, V32, P63, DOI 10.1109/MCG.2012.87
NR 26
TC 4
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 323
EP 334
DI 10.1007/s00371-015-1180-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200006
DA 2024-07-18
ER

PT J
AU Lan, ZR
   Sourina, O
   Wang, LP
   Liu, YS
AF Lan, Zirui
   Sourina, Olga
   Wang, Lipo
   Liu, Yisi
TI Real-time EEG-based emotion monitoring using stable features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE EEG; Emotion recognition; Fractal dimension (FD); Stability; Intra-class
   correlation coefficient (ICC)
ID TEST-RETEST RELIABILITY; RECOGNITION; ASYMMETRY; STABILITY
AB In human-computer interaction (HCI), electroencephalogram (EEG) signals can be added as an additional input to computer. An integration of real-time EEG-based human emotion recognition algorithms in human-computer interfaces can make the users experience more complete, more engaging, less emotionally stressful or more stressful depending on the target of the applications. Currently, the most accurate EEG-based emotion recognition algorithms are subject-dependent, and a training session is needed for the user each time right before running the application. In this paper, we propose a novel real-time subject-dependent algorithm with the most stable features that gives a better accuracy than other available algorithms when it is crucial to have only one training session for the user and no re-training is allowed subsequently. The proposed algorithm is tested on an affective EEG database that contains five subjects. For each subject, four emotions (pleasant, happy, frightened and angry) are induced, and the affective EEG is recorded for two sessions per day in eight consecutive days. Testing results show that the novel algorithm can be used in real-time emotion recognition applications without re-training with the adequate accuracy. The proposed algorithm is integrated with real-time applications "Emotional Avatar" and "Twin Girls" to monitor the users emotions in real time.
C1 [Lan, Zirui; Sourina, Olga; Liu, Yisi] Nanyang Technol Univ, Fraunhofer IDM NTU, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Wang, Lipo] Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Lan, ZR (corresponding author), Nanyang Technol Univ, Fraunhofer IDM NTU, 50 Nanyang Ave, Singapore 639798, Singapore.
EM LanZ0001@e.ntu.edu.sg; EOSourina@ntu.edu.sg; ELPWang@ntu.edu.sg;
   LiuYS@ntu.edu.sg
RI Lan, Zirui/AAQ-8624-2020; Wang, Lipo/A-5154-2011
CR Allen JJB, 2004, PSYCHOPHYSIOLOGY, V41, P269, DOI 10.1111/j.1469-8986.2003.00149.x
   [Anonymous], EEG BASED EMOTION RE
   Bouton ME, 2007, Learning and Behavior: A Contemporary Synthesis
   Bradley M. M., 2007, Bradley, M. M. , Lang, P. J. (2007). The International Affective Digitized Sounds (IADS-2): Affective ratings of sounds and instruction manual (Tech. Rep. B-3). Gainesville, FL: University of Florida.
   Brown L, 2011, IEEE ENG MED BIO, P2188, DOI 10.1109/IEMBS.2011.6090412
   Chanel G, 2009, INT J HUM-COMPUT ST, V67, P607, DOI 10.1016/j.ijhcs.2009.03.005
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Frantzidis CA, 2010, IEEE T INF TECHNOL B, V14, P589, DOI 10.1109/TITB.2010.2041553
   GASSER T, 1985, ELECTROEN CLIN NEURO, V60, P312, DOI 10.1016/0013-4694(85)90005-7
   GASSER T, 1987, ELECTROEN CLIN NEURO, V67, P151, DOI 10.1016/0013-4694(87)90038-1
   Gudmundsson S, 2007, CLIN NEUROPHYSIOL, V118, P2162, DOI 10.1016/j.clinph.2007.06.018
   HIGUCHI T, 1988, PHYSICA D, V31, P277, DOI 10.1016/0167-2789(88)90081-4
   Hosseini Seyyed Abed, 2010, Proceedings of the 2010 2nd International Conference on Information Technology and Computer Science (ITCS 2010), P60, DOI 10.1109/ITCS.2010.21
   Ishino K, 2003, IEEE SYS MAN CYBERN, P4204
   Kedem B., 1994, TIME SERIES ANAL HIG
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kondacs A, 1999, CLIN NEUROPHYSIOL, V110, P1708, DOI 10.1016/S1388-2457(99)00122-4
   Lan ZR, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P137, DOI 10.1109/CW.2014.27
   Lang PJ, 2008, A8 U FLOR
   Li Mu, 2009, Annu Int Conf IEEE Eng Med Biol Soc, V2009, P1323, DOI 10.1109/IEMBS.2009.5334139
   Lin YP, 2010, IEEE T BIO-MED ENG, V57, P1798, DOI 10.1109/TBME.2010.2048568
   Lin YP, 2009, INT CONF ACOUST SPEE, P489, DOI 10.1109/ICASSP.2009.4959627
   Liu Y., 2013, T COMPUTATIONAL SCI, P101
   Liu YS, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P302, DOI 10.1109/CW.2013.52
   Liu YS, 2011, LECT NOTES COMPUT SC, V6670, P256, DOI 10.1007/978-3-642-22336-5_13
   McGraw KO, 1996, PSYCHOL METHODS, V1, P30, DOI 10.1037/1082-989X.1.1.30
   Murugappan M, 2011, J MED BIOL ENG, V31, P45, DOI 10.5405/jmbe.710
   Petrantonakis PC, 2012, IEEE T SIGNAL PROCES, V60, P2604, DOI 10.1109/TSP.2012.2187647
   Petrantonakis PC, 2011, IEEE T INF TECHNOL B, V15, P737, DOI 10.1109/TITB.2011.2157933
   Petrantonakis PC, 2010, IEEE T INF TECHNOL B, V14, P186, DOI 10.1109/TITB.2009.2034649
   Picard RW, 2001, IEEE T PATTERN ANAL, V23, P1175, DOI 10.1109/34.954607
   SALINSKY MC, 1991, ELECTROEN CLIN NEURO, V79, P382, DOI 10.1016/0013-4694(91)90203-G
   Sanei S, 2013, EEG Signal Processing
   Schaaff Kristina, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P792, DOI 10.1109/ROMAN.2009.5326306
   Sohaib Ahmad Tauseef, 2013, Foundations of Augmented Cognition. 7th International Conference, AC 2013. Held as Part of HCI International 2013. Proceedings, P492, DOI 10.1007/978-3-642-39454-6_53
   Sourina O, 2012, J MULTIMODAL USER IN, V5, P27, DOI 10.1007/s12193-011-0080-6
   TOMARKEN AJ, 1992, PSYCHOPHYSIOLOGY, V29, P576, DOI 10.1111/j.1469-8986.1992.tb02034.x
   Wang XW, 2011, LECT NOTES COMPUT SC, V7062, P734, DOI 10.1007/978-3-642-24955-6_87
   WILLIAMS CE, 1972, J ACOUST SOC AM, V52, P1238, DOI 10.1121/1.1913238
   Yisi Liu, 2014, Transactions on Computational Science XXIII. Special Issue on Cyberworlds: LNCS 8490, P199, DOI 10.1007/978-3-662-43790-2_11
NR 40
TC 135
Z9 141
U1 1
U2 55
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 347
EP 358
DI 10.1007/s00371-015-1183-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200008
DA 2024-07-18
ER

PT J
AU Etemad, SA
   Arya, A
AF Etemad, S. Ali
   Arya, Ali
TI Correlation-optimized time warping for motion
SO VISUAL COMPUTER
LA English
DT Article
DE Motion analysis; Time warping; Temporal alignment; Correlation;
   Optimization
ID RETRIEVAL; ALIGNMENT; PERCEPTION; ALGORITHMS
AB Retrieval and comparative editing/modeling of motion data require temporal alignment. In other words, for such processes to perform accurately, critical features of motion sequences need to occur simultaneously. In this paper, we propose correlation-optimized time warping (CoTW) for aligning motion data. CoTW utilizes a correlation-based objective function for characterizing alignment. The method solves an optimization problem to determine the optimum warping degree for different segments of the sequence. Using segment-wise interpolated warping, smooth motion trajectories are achieved that can be readily used for animation. Our method allows for manual tuning of the parameters, resulting in high customizability with respect to the number of actions in a single sequence as well as spatial regions of interest within the character model. Moreover, measures are taken to reduce distortion caused by over-warping. The framework also allows for automatic selection of an optimum reference when multiple sequences are available. Experimental results demonstrate the very accurate performance of CoTW compared to other techniques such as dynamic time warping, derivative dynamic time warping and canonical time warping. The mentioned customization capabilities are also illustrated.
C1 [Etemad, S. Ali; Arya, Ali] Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
C3 Carleton University
RP Etemad, SA (corresponding author), Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
EM ali.etemad@carleton.ca; arya@carleton.ca
FU Natural Sciences and Engineering Council of Canada (NSERC); Ontario
   Centers of Excellence (OCE)
FX This work was supported in part by the Natural Sciences and Engineering
   Council of Canada (NSERC) and Ontario Centers of Excellence (OCE).
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Amaya K, 1996, PROC GRAPH INTERF, P222
   [Anonymous], 2011, COMPUTER ANIMATION A
   [Anonymous], GESTALT GROUPING PRI
   Bouillaguet C, 2010, LECT NOTES COMPUT SC, V6225, P203, DOI 10.1007/978-3-642-15031-9_14
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Caspi Y, 2002, INT J COMPUT VISION, V48, P39, DOI 10.1023/A:1014803327923
   Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148
   Cimen G, 2013, COMPUT ANIMAT VIRT W, V24, P355, DOI 10.1002/cav.1509
   Etemad SA, 2014, BIOL INSPIR COGN ARC, V7, P15, DOI 10.1016/j.bica.2013.10.001
   Etemad SA, 2014, NEUROSCI LETT, V558, P132, DOI 10.1016/j.neulet.2013.11.010
   Etemad SA, 2014, NEUROCOMPUTING, V129, P585, DOI 10.1016/j.neucom.2013.09.001
   Etemad SA, 2013, IEEE INT C SEMANT CO, P387, DOI 10.1109/ICSC.2013.72
   Etemad SA, 2010, GRAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P307
   Fu AWC, 2008, VLDB J, V17, P899, DOI 10.1007/s00778-006-0040-z
   Gleicher M., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P33, DOI 10.1145/280814.280820
   Gray Peter., 2006, PSYCHOLOGY
   Heloir A, 2006, COMPUT ANIMAT VIRT W, V17, P347, DOI 10.1002/cav.138
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   HSU E, 2007, P ACM SIGGRAPH EUR S, P45
   Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68
   Junqiu Wang, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P204
   Keogh E.J., 2001, 1 SIAM INT C DAT MIN
   Kim M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531385
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kovar L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P214
   Lemire D, 2009, PATTERN RECOGN, V42, P2169, DOI 10.1016/j.patcog.2008.11.030
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Listgarten J., 2005, Advances in Neural Information Processing Systems, V17, P817
   Liu GD, 2008, COMPUT ANIMAT VIRT W, V19, P199, DOI 10.1002/cav.254
   Lu C, 2013, IEEE T MULTIMEDIA, V15, P70, DOI 10.1109/TMM.2012.2225036
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   MYERS C, 1980, IEEE T ACOUST SPEECH, V28, P623, DOI 10.1109/TASSP.1980.1163491
   Nielsen NPV, 1998, J CHROMATOGR A, V805, P17, DOI 10.1016/S0021-9673(98)00021-1
   Pádua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301
   Pravdova V, 2002, ANAL CHIM ACTA, V456, P77, DOI 10.1016/S0003-2670(02)00008-9
   Prazˇak M., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '11, P287
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Shapiro A, 2006, PROC GRAPH INTERF, P33
   Skov T, 2006, J CHEMOMETR, V20, P484, DOI 10.1002/cem.1031
   Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Tomasi G, 2004, J CHEMOMETR, V18, P231, DOI 10.1002/cem.859
   TOMASI G, 2006, THESIS ROYAL VET AGR
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wang SJ, 2009, MED PHYS, V36, P5595, DOI 10.1118/1.3259727
   Witkin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P105, DOI 10.1145/218380.218422
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Yu T, 2005, COMPUT ANIMAT VIRT W, V16, P273, DOI 10.1002/cav.89
   Zhou F., 2009, ADV NEURAL INFORM PR, V22, P1
   Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812
NR 56
TC 10
Z9 10
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1569
EP 1586
DI 10.1007/s00371-014-1034-2
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900001
DA 2024-07-18
ER

PT J
AU Hartmann, S
   Trunz, E
   Krüger, B
   Klein, R
   Hullin, MB
AF Hartmann, Stefan
   Trunz, Elena
   Krueger, Bjoern
   Klein, Reinhard
   Hullin, Matthias B.
TI Efficient multi-constrained optimization for example-based synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Example-based synthesis; Data-driven animation; Motion synthesis;
   Building layouts
ID SHORTEST-PATH
AB Digital media content comes in a wide variety of modalities and representations. Although they have obvious semantic and structural difference, many of them can be unwrapped into a one-dimensional parameter domain, e.g., time, one spatial dimension. Novel content can then be generated in this parameter domain by computing sequences of elements that are optimal according to an objective to be minimized and in addition satisfy a number of user-defined constraints. Examples for this type of content generation task are audio synthesis, human motion synthesis or architectural texture synthesis. In that work, we present a generalized algorithm for this type of content generation task. We demonstrate the potential of our technique on a selection of content creation tasks, namely the generation of extended animation sequences from motion capture libraries and the example-based synthesis of architectural geometry such as buildings and street blocks.
C1 [Hartmann, Stefan; Trunz, Elena; Krueger, Bjoern; Klein, Reinhard; Hullin, Matthias B.] Univ Bonn, Inst Comp Sci 2, D-53113 Bonn, Germany.
C3 University of Bonn
RP Hartmann, S (corresponding author), Univ Bonn, Inst Comp Sci 2, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.
EM hartmans@cs.uni-bonn.de; trunz@cs.uni-bonn.de; kruegerb@cs.uni-bonn.de;
   rk@cs.uni-bonn.de; hullin@cs.uni-bonn.de
RI Krüger, Björn/AAA-6944-2022
OI Kruger, Bjorn/0000-0002-1596-6487
FU AIF Projekt GmbH through the AtEgoSim project
FX We thank AIF Projekt GmbH for their support through the AtEgoSim
   project, and Max Hermann for valuable discussions and his illustration
   of Figs. 2 and 3.
CR [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   Bokeloh Martin, 2010, ACM SIGGRAPH 2010 SI
   EPPSTEIN D, 1994, AN S FDN CO, P154
   Garcia R., 2009, THESIS GEORGIA I TEC
   Horswill IanD., 2012, AIIDE
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Lan RY, 2015, VISUAL COMPUT, V31, P35, DOI 10.1007/s00371-013-0902-5
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Lefebvre S., 2010, ACM SIGGRAPH 2010 SI, P84
   Lo WY, 2010, COMPUT GRAPH FORUM, V29, P563, DOI 10.1111/j.1467-8659.2009.01626.x
   Merrell P., 2009, 2009 SIAMACM JOINT C, P101
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Muller M., 2007, Tech. Rep. CG-2007-2
   RIBEIRO CC, 1985, DISCRETE APPL MATH, V10, P125, DOI 10.1016/0166-218X(85)90007-1
   Safonova A., 2007, ACM SIGGRAPH 2007 SI
   Smith Gillian., 2009, P 4 INT C FDN DIGITA, P175
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Turner L., 2011, Algorithmic Oper Res, V6, P91
   Vogele A., 2014, ACM SCA JUL
   Wenner S, 2013, COMPUT GRAPH FORUM, V32, P345, DOI 10.1111/cgf.12054
   Yeh Y., 2012, ACM T GRAPHIC, V32, P614
   Zhou F., 2008, IEEE C AUT FAC GEST
   Zhou SZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661238
   Zhou SZ, 2013, COMPUT GRAPH FORUM, V32, P355, DOI 10.1111/cgf.12055
   Zhu XY, 2012, COMPUT OPER RES, V39, P164, DOI 10.1016/j.cor.2011.03.008
   Ziegelmann M., 2004, THESIS SAARLAND U
NR 28
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 893
EP 904
DI 10.1007/s00371-015-1114-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500015
DA 2024-07-18
ER

PT J
AU Kang, H
   Jang, H
   Cho, CS
   Han, J
AF Kang, HyeongYeop
   Jang, Hanyoung
   Cho, Chang-Sik
   Han, JungHyun
TI Multi-resolution terrain rendering with GPU tessellation
SO VISUAL COMPUTER
LA English
DT Article
DE Terrain rendering; GPU tessellation; Height map; Geometry image
ID REPRESENTATION; VISUALIZATION
AB GPU tessellation is very efficient and is reshaping the terrain-rendering paradigm. We present a novel terrain-rendering algorithm based on GPU tessellation. The planar domain of the terrain is partitioned into a set of tiles, and a coarse-grained quadtree is constructed for each tile using a screen-space error metric. Then, each node of the quadtree is input to the GPU pipeline together with its own tessellation factors. The nodes are tessellated and the vertices of the tessellated mesh are displaced by filtering the displacement maps. The multi-resolution scheme is designed to optimize the use of GPU tessellation. Further, it accepts not only height maps but also geometry images, which displace more vertices toward the higher curvature feature parts of the terrain surface such that the surface detail can be well reconstructed with a small number of vertices. The efficiency of the proposed method is proven through experiments on large terrain models. When the screen-space error threshold is set to a pixel, a terrain surface tessellated into 8.5 M triangles is rendered at 110 fps on commodity PCs.
C1 [Kang, HyeongYeop; Han, JungHyun] Korea Univ, Comp Sci & Engn, Seoul, South Korea.
   [Jang, Hanyoung] NCsoft, Seongnam, South Korea.
   [Cho, Chang-Sik] Elect & Telecommun Res Inst, Digital Content Ctr, Taejon 305606, South Korea.
C3 Korea University; Electronics & Telecommunications Research Institute -
   Korea (ETRI)
RP Han, J (corresponding author), Korea Univ, Comp Sci & Engn, Seoul, South Korea.
EM jhan@korea.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU National Research Foundation of Korea (NRF) - Korea government (MEST)
   [NRF-2012R1A2A2A06047007]; Ministry of Culture, Sports and Tourism
   (MCST); Korea Creative Content Agency (KOCCA) in the Culture Technology
   (CT) Research and Development Program
FX This research is supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MEST)
   (NRF-2012R1A2A2A06047007). It is also supported by Ministry of Culture,
   Sports and Tourism (MCST) and Korea Creative Content Agency (KOCCA) in
   the Culture Technology (CT) Research and Development Program 2013.
CR Berry J., 2009, GEOWORLD, V22, P14
   Cantlay I., 2011, DIRECTX 11 TERRAIN T, V8
   Cignoni P, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P147, DOI 10.1109/VISUAL.2003.1250366
   Cignoni P, 2003, COMPUT GRAPH FORUM, V22, P505, DOI 10.1111/1467-8659.00698
   Cignoni P, 1997, VISUAL COMPUT, V13, P199, DOI 10.1007/s003710050099
   Dachsbacher Carsten., 2004, RENDERING TECHNIQUES, P103
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   El-Sana J, 1999, COMPUT GRAPH FORUM, V18, pC83, DOI 10.1111/1467-8659.00330
   Evans W, 2001, ALGORITHMICA, V30, P264, DOI 10.1007/s00453-001-0006-x
   Feng WW, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731049
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hoppe H, 1998, VISUALIZATION '98, PROCEEDINGS, P35, DOI 10.1109/VISUAL.1998.745282
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Hwa LM, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P219, DOI 10.1109/VISUAL.2004.4
   Jang H, 2012, COMPUT GRAPH FORUM, V31, P1880, DOI 10.1111/j.1467-8659.2012.03068.x
   Lindstrom P, 2002, IEEE T VIS COMPUT GR, V8, P239, DOI 10.1109/TVCG.2002.1021577
   Lindstrom P, 2001, IEEE VISUAL, P363, DOI 10.1109/VISUAL.2001.964533
   Lindstrom P., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P109, DOI 10.1145/237170.237217
   Livny Y, 2008, VISUAL COMPUT, V24, P139, DOI 10.1007/s00371-007-0180-1
   Livny Y, 2009, VISUAL COMPUT, V25, P197, DOI 10.1007/s00371-008-0214-3
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   Pajarola R, 2007, VISUAL COMPUT, V23, P583, DOI [10.1007/s00371-007-0163-2, 10.1007/S00371-007-0163-2]
   Puppo E., 1996, Canadian Conference on Computational Geometry. CCCG '96. Proceedings of the 8th Canadian Conference on Computational Geometry, P202
   Ripolles O, 2012, COMPUT GEOSCI-UK, V41, P147, DOI 10.1016/j.cageo.2011.08.025
   Samet H., 1990, The Design and Analysis of Spatial Data Structures
   Schneider J, 2006, JOURNAL WSCG, V14, P49
   Sullivan J. M., 2005, ACM SIGGRAPH 2005 CO, DOI [10.1145/1198555.1198662, DOI 10.1145/1198555.1198662]
   Tatarchuk N., 2009, SHADERX 7 ADV RENDER
   Valdetaro A., 2010, Proceedings 2010 Brazilian Symposium on Games and Digital Entertainment (SBGAMES 2010), P182, DOI 10.1109/SBGAMES.2010.30
   Wagner D., 2003, SHADERX2 SHADER PROG, P18
   Xia JC, 1997, IEEE T VIS COMPUT GR, V3, P171, DOI 10.1109/2945.597799
   Yoon S., 2008, Real-Time Massive Model Rendering
   Yoshizawa S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P200
   Yusov E., 2011, WSCG, V19, P85
NR 35
TC 19
Z9 24
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 455
EP 469
DI 10.1007/s00371-014-0941-6
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600007
DA 2024-07-18
ER

PT J
AU Li, BB
   Jiang, G
   Shao, WJ
AF Li, Beibei
   Jiang, Guang
   Shao, Wenjian
TI Color correction based on point clouds alignment in the logarithmic RGB
   space
SO VISUAL COMPUTER
LA English
DT Article
DE Color correction; Lambertian reflection model; Color space; Point clouds
ID ESTIMATING ILLUMINATION CHROMATICITY; TRANSFERRING COLOR; IMAGE;
   CONSTANCY; NORMALIZATION; SEGMENTATION
AB Color distortion which is caused by different illuminations and different camera settings in imaging process, incurs many problems. To overcome these problems, color correction is required. In this paper, a novel method for point cloud alignment-based color correction is proposed to eliminate unwanted color variation between images of the same or similar scenes automatically. The method is based on the Lambertian reflection model. For the advantages inherent in the logarithmic representation of an image, we first project the RGB color space into the logarithmic color space; then, in the logarithmic color space, we prove that the point clouds of the same or similar objects in different images can be related by a 3D translation vector. Next, we seek to find an accurate translation vector to align corresponding point clouds; and finally, when the source image is transformed by the translation vector, it can exhibit a similar appearance to the reference image. Various experimental results verify the validity of the proposed algorithm.
C1 [Li, Beibei; Jiang, Guang; Shao, Wenjian] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian, Shaanxi, Peoples R China.
C3 Xidian University
RP Li, BB (corresponding author), Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian, Shaanxi, Peoples R China.
EM bblixidian@gmail.com; gjiang@mail.xidian.edu.cn; wenjianshao@live.cn
RI Li, Bei-Bei/HCH-1650-2022; Shao, Wenjian/C-1764-2009
FU 111 Project [B08038]; Fundamental Research Funds for the Central
   Universities [K5051204006]
FX The authors would like to thank the editor and the anonymous reviewers
   for their help in improving the paper. The work is supported by the 111
   Project (B08038), the Fundamental Research Funds for the Central
   Universities (K5051204006).
CR Agarwal V, 2006, IEEE IMAGE PROC, P981, DOI 10.1109/ICIP.2006.312652
   An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   [Anonymous], 1760, Photometria sive de mensura et gradibus luminis, colorum et umbrae
   Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P985, DOI 10.1109/TIP.2002.802529
   Barnard K, 2002, COLOR RES APPL, V27, P152, DOI 10.1002/col.10050
   Barnard K, 2002, COLOR RES APPL, V27, P147, DOI 10.1002/col.10049
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Chang YH, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P176, DOI 10.1109/CGI.2003.1214463
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Cheng H, 2009, NEURAL COMPUT APPL, V18, P237, DOI 10.1007/s00521-008-0176-4
   Drew MS, 2003, J OPT SOC AM A, V20, P1181, DOI 10.1364/JOSAA.20.001181
   Drew RS, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P533, DOI 10.1109/ICCV.1998.710768
   FAUGERAS OD, 1979, IEEE T ACOUST SPEECH, V27, P380, DOI 10.1109/TASSP.1979.1163262
   Finlayson G, 2003, PATTERN RECOGN LETT, V24, P1679, DOI 10.1016/S0167-8655(02)00324-0
   Finlayson G. D., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P475, DOI 10.1007/BFb0055685
   Finlayson GD, 2006, INT J COMPUT VISION, V67, P93, DOI 10.1007/s11263-006-4100-z
   Finlayson GD, 1996, IEEE T PATTERN ANAL, V18, P1034, DOI 10.1109/34.541413
   FINLAYSON GD, 1994, J OPT SOC AM A, V11, P3011, DOI 10.1364/JOSAA.11.003011
   Forsyth G. D., 1991, INT J COMPUT VISION, V5, P5
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gershon R., 1987, Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, Italy, V2, P755
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   HORN BKP, 1979, APPL OPTICS, V18, P1770, DOI 10.1364/AO.18.001770
   Jeong K, 2008, MACH VISION APPL, V19, P443, DOI 10.1007/s00138-007-0079-x
   JOURLIN M, 1988, J MICROSC-OXFORD, V149, P21, DOI 10.1111/j.1365-2818.1988.tb04559.x
   Klinker G.J., 1990, INT J COMPUT VISION, V2, P7
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li B, 2010, COLOR RES APPL, V35, P304, DOI 10.1002/col.20574
   Omer I, 2004, PROC CVPR IEEE, P946
   Parraga C. A., 2010, 5 EUR C COL GRAPH IM
   Pokorny J., 1979, CONGENITAL ACQUIRED, P243
   Przemyslaw M., 2012, VISUAL COMPUT, P1
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rosenberg C., 2004, ADV NEURAL INFORM PR
   Shen HL, 2005, APPL OPTICS, V44, P1969, DOI 10.1364/AO.44.001969
   STOCKHAM TG, 1972, PR INST ELECTR ELECT, V60, P828, DOI 10.1109/PROC.1972.8782
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Tai YW, 2005, PROC CVPR IEEE, P747
   Tian GY, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P483, DOI 10.1109/IV.2002.1028817
   Torres-Méndez LA, 2005, LECT NOTES COMPUT SC, V3757, P60, DOI 10.1007/11585978_5
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Vazquez-Corral J, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.031105
   Wang N, 2009, IEICE T INF SYST, VE92D, P2279, DOI 10.1587/transinf.E92.D.2279
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wen CL, 2008, COMPUT GRAPH FORUM, V27, P1765, DOI 10.1111/j.1467-8659.2008.01321.x
   Xiong W., 2008, PROC 16 COLOR IMAGIN, P210
   Xiong WH, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P143
   Xiong WH, 2006, J IMAGING SCI TECHN, V50, P341, DOI 10.2352/J.ImagingSci.Technol.(2006)50:4(341)
NR 51
TC 5
Z9 5
U1 1
U2 27
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 257
EP 270
DI 10.1007/s00371-013-0916-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900002
DA 2024-07-18
ER

PT J
AU Sá, AME
   Echavarria, KR
   Arnold, D
AF Medeiros e Sa, Asla
   Echavarria, Karina Rodriguez
   Arnold, David
TI Dual joints for 3D-structures Producing skins for skeletons by exploring
   duality
SO VISUAL COMPUTER
LA English
DT Article
DE Cell complexes; Additive manufacturing; Low density structures
ID SURFACES; DESIGN; SPACE
AB The increasing popularity of 3D printing is drawing the interest of the research community to the possibilities and challenges of this manufacturing method. Amongst its many uncertainties, we are concerned here with one of its certainties-that reduction of the material required in 3D printing is critical for efficiency and affordability. In this paper, we propose a solution to the computer graphics problem of, given a volume boundary, automatically defining the mesh of a low density internal structure that is 3D-printable. The proposed solution involves two steps. The first step is to define a cell complex partition for the internal space of a volume defined by its boundaries. The second step, is to apply the Skin4Skeleton algorithm, which uses the cell complex dual to produce a 3D-printable cell-complex mesh with a parametrised thickness.
C1 [Medeiros e Sa, Asla] Escola Matemat Aplicada FGV EMAp, Rio De Janeiro, Brazil.
   [Echavarria, Karina Rodriguez; Arnold, David] Univ Brighton, Cultural Informat Res Grp, Brighton, E Sussex, England.
C3 Getulio Vargas Foundation; University of Brighton
RP Sá, AME (corresponding author), Escola Matemat Aplicada FGV EMAp, Rio De Janeiro, Brazil.
EM asla.sa@fgv.br; K.Rodriguez@brighton.ac.uk; D.Arnold@brighton.ac.uk
RI Echavarria, Karina Rodriguez/AAF-5093-2020; Medeiros e Sa,
   Asla/P-5129-2014
OI Echavarria, Karina Rodriguez/0000-0002-8679-1602; Medeiros e Sa,
   Asla/0000-0002-3705-9095
FU CAPES-Brazil
FX A.M.S. supported by CAPES-Brazil.
CR [Anonymous], SOLIDIFYING WIREFRAM
   [Anonymous], ASME C P
   [Anonymous], LECT SER KAREMAN I F
   [Anonymous], ADD MAN C P
   [Anonymous], MESH GENERATORS
   [Anonymous], COMPUT AIDED DES
   [Anonymous], IMR
   [Anonymous], 2012, 13 INT S VIRT REAL A
   Bitzer T., 1997, Honeycomb Technology, DOI [10.1007/978-94-011-5856-5, DOI 10.1007/978-94-011-5856-5]
   Chen Yong., 2006, ASME 2006 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, P269
   Christensen RM, 2000, INT J SOLIDS STRUCT, V37, P93, DOI 10.1016/S0020-7683(99)00080-3
   Conway JH, 2011, P NATL ACAD SCI USA, V108, P11009, DOI 10.1073/pnas.1105594108
   Deshpande VS, 2001, J MECH PHYS SOLIDS, V49, P1747, DOI 10.1016/S0022-5096(01)00010-2
   Friedrichs OD, 1999, DISCRETE COMPUT GEOM, V21, P299, DOI 10.1007/PL00009423
   Gibson I, 2010, ADDITIVE MANUFACTURING TECHNOLOGIES: RAPID PROTOTYPING TO DIRECT DIGITAL MANUFACTURING, P1, DOI 10.1007/978-1-4419-1120-9_1
   Greiner G, 2000, VISUAL COMPUT, V16, P357, DOI 10.1007/PL00007214
   Hutmacher DW, 2004, TRENDS BIOTECHNOL, V22, P354, DOI 10.1016/j.tibtech.2004.05.005
   Kang H, 2010, STRUCT MULTIDISCIP O, V42, P633, DOI 10.1007/s00158-010-0508-8
   Kowalski N, 2012, ENG COMPUT-GERMANY, V28, P241, DOI 10.1007/s00366-010-0207-5
   Ledoux H, 2007, COMPUT ENVIRON URBAN, V31, P393, DOI 10.1016/j.compenvurbsys.2006.03.003
   Murdoch P, 1997, FINITE ELEM ANAL DES, V28, P137, DOI 10.1016/S0168-874X(97)81956-7
   Nieser M, 2011, COMPUT GRAPH FORUM, V30, P1397, DOI 10.1111/j.1467-8659.2011.02014.x
   Patrikalakis N.M., 2010, Shape Interrogation for Computer Aided Design and Manufacturing
   Pavic D, 2008, COMPUT GRAPH FORUM, V27, P165, DOI 10.1111/j.1467-8659.2008.01113.x
   PHAM B, 1992, COMPUT AIDED DESIGN, V24, P223, DOI 10.1016/0010-4485(92)90059-J
   Rosen D.W., 2007, COMPUTER AIDED DESIG, V4, P585, DOI [10.1080/16864360.2007.10738493, DOI 10.1080/16864360.2007.10738493]
   Si H., 2009, Tetgen: A quality tetrahedral mesh generator and three-dimensional delaunay triangulator
   Wadley HNG, 2006, PHILOS T R SOC A, V364, P31, DOI 10.1098/rsta.2005.1697
   Yoo DJ, 2011, BIOMATERIALS, V32, P7741, DOI 10.1016/j.biomaterials.2011.07.019
   Yoo DJ, 2009, INT J PRECIS ENG MAN, V10, P103, DOI [10.1007/s12541-009-0016-1, 10.1007/S12541-009-0016-1]
NR 30
TC 7
Z9 7
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1321
EP 1331
DI 10.1007/s00371-013-0883-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400002
DA 2024-07-18
ER

PT J
AU Hu, W
   Huang, YY
   Zhang, F
   Yuan, GD
   Li, W
AF Hu, Wei
   Huang, Yangyu
   Zhang, Fan
   Yuan, Guodong
   Li, Wei
TI Ray tracing via GPU rasterization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Ray tracing; Global illumination; Rasterization; GPUs
ID ILLUMINATION
AB Ray tracing is a dominant method for generating a wide variety of global illumination effects, such as reflections/refractions, shadows, etc. In this paper, we propose an efficient technique to perform nearly accurate ray tracing using the programmable graphics processor units (GPUs). With the aid of the linked-list A-buffer and the uniform voxel grid to represent scene geometry, the ray-scene intersection can be efficiently computed via the built-in rasterization on GPUs. Based on this novel ray-scene intersection technique, a new ray-tracing framework which supports various light transport algorithms is introduced, including Ray Casting, Whitted Ray tracing, Ambient Occlusion, Path Tracing, and so on. The experimental results demonstrate the accuracy and efficiency of our approach.
C1 [Hu, Wei; Huang, Yangyu; Zhang, Fan; Li, Wei] Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China.
   [Yuan, Guodong] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100010, Peoples R China.
C3 Beijing University of Chemical Technology; Beijing Information Science &
   Technology University
RP Hu, W (corresponding author), Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China.
EM huwei@mail.buct.edu.cn
RI Zhang, Fan/W-3340-2019; LI, WEI/ABD-5001-2021
OI Zhang, Fan/0000-0002-2058-2373; 
FU Nation Nature Science Foundation of China [61003132]; 973 Program of
   China [2011CB706900]
FX We would like to thank the anonymous reviewers for their constructive
   comments. We thank Zhao Dong for his valuable suggestions. This work was
   supported jointly by Nation Nature Science Foundation of China (No.
   61003132) and 973 Program of China (No. 2011CB706900).
CR Aila T., 2010, P C HIGH PERF GRAPH, P112
   Aila T., 2009, P C HIGH PERF GRAPH, P113
   Amanatides John, 1987, P EUROGRAPHICS, P3
   Burger K., 2007, P INT WORKSH VIS MOD, P51
   Carpenter L., 1984, Computers & Graphics, V18, P103
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Crassin C, 2012, OPENGL INSIGHTS, P303
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Krivánek J, 2008, COMPUT GRAPH FORUM, V27, P1147, DOI 10.1111/j.1467-8659.2008.01252.x
   Laine S., 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P55, DOI DOI 10.1145/1730804.1730814
   Niessner M, 2010, VISUAL COMPUT, V26, P679, DOI 10.1007/s00371-010-0486-2
   Novák J, 2012, COMPUT GRAPH FORUM, V31, P403, DOI 10.1111/j.1467-8659.2012.03019.x
   Papaioannou G, 2010, IEEE T VIS COMPUT GR, V16, P752, DOI 10.1109/TVCG.2010.18
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Policarpo F., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, P155, DOI DOI 10.1145/1053427.1053453
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Rosen P, 2011, IEEE COMPUT GRAPH, V31, P68, DOI 10.1109/MCG.2011.32
   Szirmay-Kalos L, 2009, COMPUT GRAPH FORUM, V28, P1586, DOI 10.1111/j.1467-8659.2009.01350.x
   Thiedemann Sinje., 2011, Symposium on Interactive 3D Graphics and Games, I3D '11, P103
   Tokuyoshi Y., 2012, P ACM SIGGRAPH S INT, P183, DOI [10.1145/2159616.2159647, DOI 10.1145/2159616.2159647]
   Umenhoffer T., 2010, GPU GEMS, V3, P387
   Wald I., 1999, ACM T GRAPHIC, V25, P485
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Wyman Chris., 2005, Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia (GRAPHITE '05), P205
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
   Yao CH, 2010, COMPUT GRAPH FORUM, V29, P1315, DOI 10.1111/j.1467-8659.2010.01727.x
   Zhang C., 2008, P IADIS INT C COMP G
   Zhou K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409079
   ZHUKOV S, 1998, P EUR WORKSH REND TE, P45
NR 32
TC 12
Z9 16
U1 2
U2 34
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 697
EP 706
DI 10.1007/s00371-014-0968-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700013
DA 2024-07-18
ER

PT J
AU Koa, MD
   Johan, H
AF Koa, Ming Di
   Johan, Henry
TI ESLPV: enhanced subsurface light propagation volumes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Subsurface scattering; Real-time rendering; Translucency; Refraction
AB In this paper, we present an enhanced subsurface light propagation volumes (ESLPV) method for real-time rendering of translucent materials. Our method is an extension of the subsurface light propagation volumes (SSLPV) (Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics, HPG '11, pp 7-14 ACM 2011) technique. We improve the SSLPV by incorporating a single-scattering framework that uses the same spherical harmonics (SH) storage structure as the SSLPV. The new single-scattering technique deposits radiance as SH coefficients during a ray marching procedure. The final result is rendered using a ray tracer with importance sampling along the camera ray. This framework also enables the ESLPV to render refractive objects. In addition, we also propose a distance transform optimization that can remove the unnecessary computations during the propagation cycle of LPV (Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D '10, pp 99-107 ACM 2010) based methods. A hierarchical propagation process is also proposed to render highly translucent materials. Similar to the SSLPV, our ESLPV method contains no precomputations, and has low storage requirements that are independent of the mesh size.
C1 [Koa, Ming Di] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Johan, Henry] Fraunhofer IDM NTU, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Koa, MD (corresponding author), Nanyang Technol Univ, Singapore 639798, Singapore.
EM mdkoa1@e.ntu.edu.sg; henryjohan@ntu.edu.sg
FU Fraunhofer IDM@ NTU - National Research Foundation (NRF)
FX We thank all reviewers for their comments and Stanford Computer Graphics
   Laboratory for their 3D models. Henry Johan is supported by Fraunhofer
   IDM@ NTU, which is funded by the National Research Foundation (NRF) and
   managed through the multi-agency Interactive and Digital Media Programme
   Office (IDMPO) hosted by Media Development Authority of Singapore (MDA).
CR Bernabei D, 2012, IEEE COMPUT GRAPH, V32, P34, DOI 10.1109/MCG.2011.106
   Billeter M., 2012, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D 2012, P119
   Borlum J., 2011, Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics, P7
   CARR N.A., 2003, HWWS 03, P51
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   dEon E., 2007, P EUR S REND TECHN, P147
   Eisemann Elmar., 2008, Proceedings of Graphics Interface, P73
   Geist R., 2004, Proceedings of the 15th Eurographics Conference on Rendering Techniques (EGSR'04), Norkooping, Sweden, June 21-23, P355, DOI [10.2312/EGWR/EGSR04/355-362, DOI 10.2312/EGWR/EGSR04/355-362]
   Gkioulekas I., 2013, ACM T GRAPHIC, V32
   Habel R, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/cgf.12148
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Ihrke L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451,1239510
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jimenez J, 2010, IEEE COMPUT GRAPH, V30, P32, DOI 10.1109/MCG.2010.39
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Li DP, 2013, IEEE T VIS COMPUT GR, V19, P484, DOI 10.1109/TVCG.2012.127
   Mertens T, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P51, DOI 10.1109/PCCGA.2003.1238246
   Munoz A, 2011, COMPUT GRAPH FORUM, V30, P2279, DOI 10.1111/j.1467-8659.2011.02034.x
   Olovsson J.D., 2013, P SIGRAD, V2013, P27
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Schwarz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866201
   Sheng Y., 2013, P ACM SIGGRAPH S INT, P63
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Stam J, 1995, SPRING COMP SCI, P41
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
   Wang R, 2008, VISUAL COMPUT, V24, P565, DOI 10.1007/s00371-008-0237-9
   Xu K, 2007, COMPUT GRAPH FORUM, V26, P545, DOI 10.1111/j.1467-8659.2007.01077.x
NR 32
TC 9
Z9 9
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 821
EP 831
DI 10.1007/s00371-014-0952-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yoon, W
   Lee, N
   Um, K
   Han, J
AF Yoon, Wonbae
   Lee, Namil
   Um, Kiwon
   Han, JungHyun
TI Computer-generated iron filing art
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Digital artistic tool; Iron filings; Magnetic fields and forces
ID DIFFUSION
AB This paper proposes a digital tool which enables users to create the iron filing effects. It is a 2D interactive system, where the user provides a silhouette of a 2D target shape and interactively specifies various parameters of magnets. Then, the patterns of the iron filings are automatically generated through the principle of magnetism. The system also provides a real-time feedback while the user updates the magnet parameters, i.e., the magnet lines are interactively displayed such that the user can speculate the final patterns of iron filings. The experimental results show that a variety of artwork can be created by the proposed tool.
C1 [Yoon, Wonbae; Lee, Namil; Um, Kiwon; Han, JungHyun] Korea Univ, Seoul, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Seoul, South Korea.
EM ywbgkz@naver.com; 2namil@naver.com; kiwon_um@korea.ac.kr;
   jhan@korea.ac.kr
RI Um, Kiwon/AAO-6776-2020
OI /0000-0002-4139-9308
FU National Research Foundation of Korea (NRF) - Korea government (MEST)
   [NRF-2012R1A2A2A06047007]
FX This research was supported by the National Research Foundation of Korea
   (NRF) Grant funded by the Korea government (MEST)
   (NRF-2012R1A2A2A06047007).
CR Abbott B., 1958, MAGNETISM KEY
   [Anonymous], 2004, P VMV
   Baxter W, 2004, COMPUT ANIMAT VIRT W, V15, P433, DOI 10.1002/cav.47
   BAXTER W, 2004, ACM SIGGRAPH 2004 SK, P3, DOI DOI 10.1145/1186223.1186227
   Bridson R., 2008, Fluid Simulation
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   Jackson John David, 1999, CLASSICAL ELECTRODYN
   Kim T, 2007, COMPUT ANIMAT VIRT W, V18, P329, DOI 10.1002/cav.187
   Le-Hair D., 2011, MAGNET IRON FILING T
   Lee S., 2006, Proc. NPAR '06, P97, DOI DOI 10.1145/1124728.1124745
   Lei SIE, 2013, COMPUT GRAPH FORUM, V32, P147, DOI 10.1111/cgf.12222
   Ling M., MAGNETISM SERIES
   Mater A., 2012, MAGNETISM
   Qinglian Guo, 1991, Modeling in Computer Graphics. Proceedings of the IFIP WG 5.10 Working Conference, P329
   Thomaszewski B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409115
   Wang CM, 2007, IEEE T VIS COMPUT GR, V13, P235, DOI 10.1109/TVCG.2007.41
   Yang IS., 2012, PROC 11 ACM SIGGRAPH, DOI [10.1145/2407516.2407564, DOI 10.1145/2407516.2407564]
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 19
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 889
EP 895
DI 10.1007/s00371-014-0953-2
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700030
DA 2024-07-18
ER

PT J
AU Sipiran, I
   Bustos, B
AF Sipiran, Ivan
   Bustos, Benjamin
TI Key-components: detection of salient regions on 3D meshes
SO VISUAL COMPUTER
LA English
DT Article
DE 3D features; Mesh decomposition
ID GEOMETRIC FEATURES; POINT
AB In this paper, we present a method to detect stable components on 3D meshes. A component is a salient region on the mesh which contains discriminative local features. Our goal is to represent a 3D mesh with a set of regions, which we called key-components, that characterize the represented object and therefore, they could be used for effective matching and recognition. As key-components are features in coarse scales, they are less sensitive to mesh deformations such as noise. In addition, the number of key-components is low compared to other local representations such as keypoints, allowing us to use them in efficient subsequent tasks. A desirable characteristic of a decomposition is that the components should be repeatable regardless shape transformations. We show in the experiments that the key-components are repeatable and robust under several transformations using the SHREC'2010 feature detection benchmark. In addition, we discover the connection between the theory of saliency of visual parts from the cognitive science and the results obtained with our technique.
C1 [Sipiran, Ivan; Bustos, Benjamin] Univ Chile, Dept Comp Sci, PRISMA Res Grp, Santiago, Chile.
C3 Universidad de Chile
RP Sipiran, I (corresponding author), Univ Chile, Dept Comp Sci, PRISMA Res Grp, Santiago, Chile.
EM isipiran@dcc.uchile.cl; bebustos@dcc.uchile.cl
RI Sipiran, Ivan/GRR-8629-2022; Sipiran, Ivan/AAL-7603-2020; Bustos,
   Benjamin/G-1170-2010
OI Bustos, Benjamin/0000-0002-3955-361X; Sipiran, Ivan/0000-0002-8766-3581
FU CONICYT (Chile); FONDECYT (Chile) [1110111]
FX This project has been partially funded by CONICYT (Chile) through the
   Doctoral Scholarship, and FONDECYT (Chile) Project 1110111. We would
   like to thank Roee Litman for gently providing us the implementation of
   MSER components for our evaluation. Also, we thank Michael Bronstein for
   his extremely useful help with the SHREC' 2010 feature detection and
   description benchmark.
CR Agathos A, 2010, VISUAL COMPUT, V26, P63, DOI 10.1007/s00371-009-0383-8
   [Anonymous], CVPR WORKSH NONR SHA
   Aubry Mathieu, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P122, DOI 10.1007/978-3-642-23123-0_13
   Borg I., 2005, Modern Multidimensional Scaling: Theory and Applications
   Boyer E., 2011, Proceedings of the 4th Eurographics Conference on 3D Object Retrieval, P71
   Bronstein A. M., 2010, P WORKSH 3D OBJ RETR
   Digne J., 2010, P 5 INT S 3D DAT PRO
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Huang QX, 2009, COMPUT GRAPH FORUM, V28, P407, DOI 10.1111/j.1467-8659.2009.01380.x
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Leow WK, 2004, COMPUT VIS IMAGE UND, V94, P67, DOI 10.1016/j.cviu.2003.10.010
   Litman R, 2011, COMPUT GRAPH-UK, V35, P549, DOI 10.1016/j.cag.2011.03.011
   Matousek J., 1992, Proceedings of the Eighth Annual Symposium on Computational Geometry, P1, DOI 10.1145/142675.142678
   Mortara M, 2004, ALGORITHMICA, V38, P227, DOI 10.1007/s00453-003-1051-4
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Sipiran I., 2012, Proceedings of the 5th Eurographics Conference on 3D Object Retrieval, P25
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sun MT, 2012, PROC CVPR IEEE, P630, DOI 10.1109/CVPR.2012.6247730
   Toldo R., 2009, Proceedings of the 2nd Eurographics Conference on 3D Object Retrieval, P21
   Yi Fang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2145, DOI 10.1109/CVPR.2011.5995695
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
NR 33
TC 10
Z9 10
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1319
EP 1332
DI 10.1007/s00371-013-0870-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Brunnett, G
   Wang, J
AF Liu, Shengjun
   Brunnett, Guido
   Wang, Jun
TI Multi-level hermite variational interpolation and quasi-interpolation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Interpolation; Quasi-interpolation; Hermite; Multi-level
ID SURFACE RECONSTRUCTION; IMPLICIT SURFACES; SCATTERED DATA;
   APPROXIMATION; PARTITION
AB Based on the Hermite variational implicit surface reconstruction presented in Pan et al. (Science in China Series F: Information Sciences 52(2):308-315, 2009), we propose a multi-level interpolation method to overcome the problems resulted from using compactly supported radial basis functions (CSRBFs). In addition, we present a multi-level quasi-interpolation method which directly uses normal vectors to construct non-zero constraints and avoids solving any linear system, a common step of variational surface reconstruction, and leads to a fast and stable surface reconstruction from scattered points. With adaptive support size, our approach is robust and can successfully reconstruct surfaces on non-uniform and noisy point sets. Moreover, as the computation of quasi-interpolation is independent for each point, it can be easily parallelized on multi-core CPUs.
C1 [Liu, Shengjun] Cent S Univ, Sch Math & Stat, Changsha 410083, Peoples R China.
   [Brunnett, Guido] Tech Univ Chemnitz, Dept Comp Sci, D-09107 Chemnitz, Germany.
   [Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Peoples R China.
C3 Central South University; Technische Universitat Chemnitz; Nanjing
   University of Aeronautics & Astronautics
RP Liu, SJ (corresponding author), Cent S Univ, Sch Math & Stat, Changsha 410083, Peoples R China.
EM shjliu.cg@csu.edu.cn; Guido.Brunnett@informatik.tu-chemnitz.de;
   davis.wjun@gmail.com
RI Wang, Jun/AAM-6868-2021; Liu, Shengjun/AAI-8456-2020
OI Wang, Jun/0000-0001-9223-2615; 
FU Natural Science Foundation of China (NSFC) [61173119, 60970097];
   Alexander von Humboldt Foundation
FX This research is supported by the Natural Science Foundation of China
   (NSFC) grants (61173119 and 60970097), and the Research Fellowship
   provided by Alexander von Humboldt Foundation.
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 1968, P 1968 ACM NAT C
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   Calakli F, 2011, COMPUT GRAPH FORUM, V30, P1993, DOI 10.1111/j.1467-8659.2011.02058.x
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chen D., 1993, Approximation Theory and its Applications, V9, P17, DOI [10.1007/BF02836480, DOI 10.1007/BF02836480]
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Desbrun, 2007, P 5 EUROGRAPHICS S G, V7, P39
   Dey T.K., 2007, CURVE SURFACE RECONS, P173
   Dey TK, 2006, COMP GEOM-THEOR APPL, V35, P124, DOI 10.1016/j.comgeo.2005.10.006
   Dey TK, 2011, COMPUT GRAPH-UK, V35, P483, DOI 10.1016/j.cag.2011.03.014
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Han XL, 2008, LECT NOTES COMPUT SC, V4975, P541
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kolluri R., 2004, ser. SGP '04, P11, DOI DOI 10.1145/1057432.1057434
   Liu SJ, 2012, COMPUT AIDED GEOM D, V29, P435, DOI 10.1016/j.cagd.2012.03.011
   Liu SJ, 2012, IEEE COMPUT GRAPH, V32, P70, DOI 10.1109/MCG.2011.14
   Llanas B, 2006, J COMPUT APPL MATH, V188, P283, DOI 10.1016/j.cam.2005.04.019
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   MHASKAR HN, 1992, ADV APPL MATH, V13, P350, DOI 10.1016/0196-8858(92)90016-P
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Ohtake Y, 2005, GRAPH MODELS, V67, P150, DOI 10.1016/j.gmod.2004.06.003
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pan RJ, 2009, SCI CHINA SER F, V52, P308, DOI 10.1007/s11432-009-0032-x
   Schall O., 2005, Proceedings of the International Workshop on Semantic Virtual Environments, P138
   Tobor I, 2006, GRAPH MODELS, V68, P25, DOI 10.1016/j.gmod.2005.09.003
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Walder C, 2006, COMPUT GRAPH FORUM, V25, P635, DOI 10.1111/j.1467-8659.2006.00983.x
   Wang J, 2013, COMPUT GRAPH FORUM, V32, P164, DOI 10.1111/cgf.12006
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Zhang WX, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P85
NR 34
TC 7
Z9 9
U1 3
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 627
EP 637
DI 10.1007/s00371-013-0801-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400017
DA 2024-07-18
ER

PT J
AU Song, R
   Liu, YH
   Martin, RR
   Rosin, PL
AF Song, Ran
   Liu, Yonghuai
   Martin, Ralph R.
   Rosin, Paul L.
TI 3D point of interest detection via spectral irregularity diffusion
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Mesh saliency; Points of interest; Laplacian; Eigendecomposition
ID SALIENCY
AB This paper presents a method for detecting points of interest on 3D meshes. It comprises two major stages. In the first, we capture saliency in the spectral domain by detecting spectral irregularities of a mesh. Such saliency corresponds to the interesting portions of surface in the spatial domain. In the second stage, to transfer saliency information from the spectral domain to the spatial domain, we rely on spectral irregularity diffusion (SID) based on heat diffusion. SID captures not only the information about neighbourhoods of a given point in a multiscale manner, but also cues related to the global structure of a shape. It thus preserves information about both local and global saliency. We finally extract points of interest by looking for global and local maxima of the saliency map. We demonstrate the advantages of our proposed method using both visual and quantitative comparisons based on a publicly available benchmark.
C1 [Song, Ran; Liu, Yonghuai] Aberystwyth Univ, Dept Comp Sci, Aberystwyth, Dyfed, Wales.
   [Martin, Ralph R.; Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AX, S Glam, Wales.
C3 Aberystwyth University; Cardiff University
RP Song, R (corresponding author), Aberystwyth Univ, Dept Comp Sci, Aberystwyth, Dyfed, Wales.
EM res@aber.ac.uk
RI Liu, Yonghuai/ABF-3794-2020; Martin, Ralph R/D-2366-2010
OI Rosin, Paul/0000-0002-4965-3884; Martin, Ralph/0000-0002-8495-8536
FU HEFCW/WAG
FX We gratefully acknowledge funding by HEFCW/WAG on the RIVIC project.
CR [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P SMI
   [Anonymous], SIGGRAPH COURSES
   [Anonymous], P SIGGRAPH
   [Anonymous], P SIGGRAPH
   [Anonymous], IEEE INT S BIOM IM
   [Anonymous], P SPIE
   [Anonymous], COMP VIS PATT REC 20
   [Anonymous], 2007 IEEE C COMPUTER
   [Anonymous], 2011, VISUAL COMPUT, DOI DOI 10.1007/s00371-011-0610-y
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Gebal K, 2009, COMPUT GRAPH FORUM, V28, P1405, DOI 10.1111/j.1467-8659.2009.01517.x
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Koch C, 1999, NAT NEUROSCI, V2, P9, DOI 10.1038/4511
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Novatnack J., 2007, P ICCV, P1
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Seo S, 2010, LECT NOTES COMPUT SC, V6363, P505
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
NR 29
TC 15
Z9 17
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 695
EP 705
DI 10.1007/s00371-013-0806-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400023
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Yang, XS
   Chang, J
   Southern, R
   Zhang, JJ
AF Yang, Xiaosong
   Chang, Jian
   Southern, Richard
   Zhang, Jian J.
TI Automatic cage construction for retargeted muscle fitting
SO VISUAL COMPUTER
LA English
DT Article
DE Muscle modelling; Character animation
AB The animation of realistic characters necessitates the construction of complicated anatomical structures such as muscles, which allow subtle shape variation of the character's outer surface to be displayed believably. Unfortunately, despite numerous efforts, the modelling of muscle structures is still left for an animator who has to painstakingly build up piece by piece, making it a very tedious process. What is even more frustrating is the animator has to build the same muscle structure for every new character. We propose a muscle retargeting technique to help an animator to automatically construct a muscle structure by reusing an already built and tested model (the template model). Our method defines a spatial transfer between the template model and a new model based on the skin surface and the rigging structure. To ensure that the retargeted muscle is tightly packed inside a new character, we define a novel spatial optimization based on spherical parameterization. Our method requires no manual input, meaning that an animator does not require anatomical knowledge to create realistic accurate musculature models.
C1 [Yang, Xiaosong] Bournemouth Univ, Natl Ctr Comp Animat, Media Sch, Bournemouth, Dorset, England.
   [Chang, Jian; Southern, Richard; Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
C3 Bournemouth University; Bournemouth University
RP Yang, XS (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Media Sch, Bournemouth, Dorset, England.
EM xyang@bmth.ac.uk; jchang@bmth.ac.uk; rsouthern@bmth.ac.uk;
   jzhang@bmth.ac.uk
OI Zhang, Jian/0000-0002-7069-5771; Yang, Xiaosong/0000-0003-3815-0584;
   Chang, Jian/0000-0003-4118-147X; Southern, Richard/0000-0002-1933-3951
FU EPSRC [EP/F030355/1] Funding Source: UKRI
CR Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340
   Ben-Chen Mirela, 2009, P 2009 ACM SIGGRAPH
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Chen L, 2010, COMPUT GRAPH-UK, V34, P107, DOI 10.1016/j.cag.2010.01.003
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Gotsman C, 2003, ACM T GRAPHIC, V22, P358, DOI 10.1145/882262.882276
   Hong M., 2005, ACM SIGGRAPH 2005 SK
   Huang J, 2009, COMPUT AIDED GEOM D, V26, P617, DOI 10.1016/j.cagd.2008.12.002
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Ju T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409075
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Kavan Ladislav, 2007, P 2007 S INT 3D GRAP
   Kobayashi K. G., 2003, P 8 ACM S SOL MOD AP
   Kojekine N., 2002, P EUR
   Lemos R., 2001, P 14 BRAZ S COMP GRA
   Lipman Y., 2007, P 5 EUR S GEOM PROC, P123
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   MacCracken R., 1996, P 23 ANN C COMP GRAP
   Park S, 2008, ACTA OCEANOL SIN, V27, P1, DOI 10.1145/1360612.1360695
   Pratscher M., 2005, P 2005 ACM SIGGRAPH
   Saba S, 2011, BARYCENTRIC SPHERICA
   Scheepers F., 1997, P 24 ANN C COMP GRAP
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Seo J, 2010, COMPUT ANIMAT VIRT W, V21, P375, DOI 10.1002/cav.358
   Shi XH, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360628
   Xian C., 2009, IEEE INT C SHAP MOD
   Yang XS, 2007, COMPUT SCI ENG, V9, P39, DOI 10.1109/MCSE.2007.95
   Yang X, 2006, COMPUT ANIMAT VIRT W, V17, P293, DOI 10.1002/cav.133
NR 34
TC 1
Z9 3
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 369
EP 380
DI 10.1007/s00371-012-0739-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200005
DA 2024-07-18
ER

PT J
AU Goswami, P
   Erol, F
   Mukhi, R
   Pajarola, R
   Gobbetti, E
AF Goswami, Prashant
   Erol, Fatih
   Mukhi, Rahul
   Pajarola, Renato
   Gobbetti, Enrico
TI An efficient multi-resolution framework for high quality interactive
   rendering of massive point clouds using multi-way kd-trees
SO VISUAL COMPUTER
LA English
DT Article
DE Point-based rendering; Level-of-detail; Multi-way kd-tree; Entropy-based
   reduction; k-clustering; Parallel rendering; Geo-morphing
AB We present an efficient technique for out-of-core multi-resolution construction and high quality interactive visualization of massive point clouds. Our approach introduces a novel hierarchical level of detail (LOD) organization based on multi-way kd-trees, which simplifies memory management and allows control over the LOD-tree height. The LOD tree, constructed bottom up using a fast high-quality point simplification method, is fully balanced and contains all uniformly sized nodes. To this end, we introduce and analyze three efficient point simplification approaches that yield a desired number of high-quality output points. For constant rendering performance, we propose an efficient rendering-on-a-budget method with asynchronous data loading, which delivers fully continuous high quality rendering through LOD geo-morphing and deferred blending. Our algorithm is incorporated in a full end-to-end rendering system, which supports both local rendering and cluster-parallel distributed rendering. The method is evaluated on complex models made of hundreds of millions of point samples.
C1 [Goswami, Prashant; Erol, Fatih; Pajarola, Renato] Univ Zurich, Visualizat & MultiMedia Lab, Zurich, Switzerland.
   [Gobbetti, Enrico] CRS4, Pula, CA, Italy.
   [Mukhi, Rahul] Univ Zurich, Dept Informat, Zurich, Switzerland.
C3 University of Zurich; University of Zurich
RP Goswami, P (corresponding author), Univ Zurich, Visualizat & MultiMedia Lab, Zurich, Switzerland.
EM goswami@ifi.uzh.ch; erol@ifi.uzh.ch; rahul.mukhi@uzh.ch;
   pajarola@acm.org; gobbetti@crs4.it
RI Gobbetti, Enrico/O-2188-2015
OI Gobbetti, Enrico/0000-0003-0831-2458; Pajarola,
   Renato/0000-0002-6724-526X
FU Swiss Commission for Technology and Innovation (KTI/CTI) [9394.2
   PFES-ES]
FX We would like to thank and acknowledge the Stanford 3D Scanning
   Repository and Digital Michelangelo projects as well as Roberto
   Scopigno, for the Pisa Cathedral model, for providing the 3D geometric
   test datasets used in this paper. This work was supported in parts by
   the Swiss Commission for Technology and Innovation (KTI/CTI) under Grant
   9394.2 PFES-ES.
CR Bettio F., 2009, P 10 INT S VIRT REAL, P25
   Bierbaum A, 2001, P IEEE VIRT REAL ANN, P89, DOI 10.1109/VR.2001.913774
   Corrêa WT, 2002, SIBGRAPI 2002: XV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P59, DOI 10.1109/SIBGRA.2002.1167124
   Dachsbacher C., 2003, ACM T GRAPH, V22
   Dasgupta S., 2008, CS20080916 DEP COMP
   Eilemann S, 2009, IEEE T VIS COMPUT GR, V15, P436, DOI 10.1109/TVCG.2008.104
   GOBBETTI E., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P113
   Goswami P., 2010, 2010 Pacific Graphics (PG). Proceedings 18th Pacific Conference on Computer Graphics and Applications, P93, DOI 10.1109/PacificGraphics.2010.20
   Goswami P., 2010, P EUR S PAR GRAPH VI, P63
   Gross M., 2007, POINT BASED GRAPHICS
   Gross M, 2006, IEEE COMPUT GRAPH, V26, P96, DOI 10.1109/MCG.2006.106
   Grossman J. P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P181
   Grottel S, 2010, COMPUT GRAPH FORUM, V29, P953, DOI 10.1111/j.1467-8659.2009.01698.x
   Hubo E., 2005, P INT C COMP GRAPH V, P1
   Humphreys G., 2002, ACM T GRAPH, V21
   Klosowski J.T., 2002, 4 EUR WORKSH PAR GRA, P63
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Levoy Marc, 1985, The use of points as a display primitive
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   MOLNAR S, 1994, IEEE COMPUT GRAPH, V14, P23, DOI 10.1109/38.291528
   Pajarola R, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P628
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pfister H, 2004, IEEE COMPUT GRAPH, V24, P22, DOI 10.1109/MCG.2004.15
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Sainz M, 2004, COMPUT GRAPH-UK, V28, P869, DOI 10.1016/j.cag.2004.08.014
   WAND M., 2007, Proceedings of Eurographics Symposium on Point-Based Graphics 2007, P37
   WIMMER M., 2006, Proceedings of Eurographics Symposium on Point-Based Graphics 2006, P129, DOI DOI 10.2312/SPBG/SPBG06/129-136
   Zhang YC, 2007, COMPUT GRAPH-UK, V31, P175, DOI 10.1016/j.cag.2006.11.012
NR 28
TC 44
Z9 54
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2013
VL 29
IS 1
BP 69
EP 83
DI 10.1007/s00371-012-0675-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 064HB
UT WOS:000313063400006
DA 2024-07-18
ER

PT J
AU Emilien, A
   Bernhardt, A
   Peytavie, A
   Cani, MP
   Galin, E
AF Emilien, Arnaud
   Bernhardt, Adrien
   Peytavie, Adrien
   Cani, Marie-Paule
   Galin, Eric
TI Procedural generation of villages on arbitrary terrains
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Procedural generation; Open shape grammars; Villages; 3D modelling
AB Although procedural modeling of cities has attracted a lot of attention for the past decade, populating arbitrary landscapes with non-urban settlements remains an open problem. In this work, we focus on the modeling of small, European villages that took benefit of terrain features to settle in safe, sunny or simply convenient places. We introduce a three step procedural generation method. First, an iterative process based on interest maps is used to progressively generate settlement seeds and the roads that connect them. The fact that a new road attracts settlers while a new house often leads to some extension of the road network is taken into account. Then, an anisotropic conquest method is introduced to segment the land into parcels around settlement seeds. Finally, we introduce open shape grammar to generate 3D geometry that adapts to the local slope. We demonstrate the effectiveness of our method by generating different kinds of village on arbitrary terrains, from a mountain hamlet to a fisherman village, and validate through comparison with real data.
C1 [Emilien, Arnaud; Bernhardt, Adrien; Cani, Marie-Paule] LJK U Grenoble, CNRS, Grenoble, France.
   [Emilien, Arnaud; Bernhardt, Adrien; Cani, Marie-Paule] Inria, Grenoble, France.
   [Peytavie, Adrien] Univ Lyon 1, LIRIS, UMR5205, F-69622 Lyon, France.
   [Galin, Eric] Univ Lyon 2, LIRIS, UMR5205, F-69622 Lyon, France.
C3 Centre National de la Recherche Scientifique (CNRS); Inria; Institut
   National des Sciences Appliquees de Lyon - INSA Lyon; Universite Claude
   Bernard Lyon 1; Universite Lyon 2; Institut National des Sciences
   Appliquees de Lyon - INSA Lyon
RP Emilien, A (corresponding author), LJK U Grenoble, CNRS, Grenoble, France.
EM arnaud.emilien@inria.fr; adrien.bernhardt@inria.fr;
   adrien.peytavie@liris.cnrs.fr; marie-paule.cani@inria.fr;
   eric.galin@liris.cnrs.fr
RI Galin, Eric/X-1938-2019; Peytavie, Adrien/X-2253-2019
OI Galin, Eric/0000-0002-5946-4112; Bernhardt, Adrien/0009-0004-8308-9457;
   Peytavie, Adrien/0000-0002-6994-9164
CR Aliaga Daniel G, 2008, SIGGRAPH ASIA
   Barry T., 1999, STORY SETTLEMENT IRE
   Bruneton Eric, 2008, COMPUTER GRAPHICS FO
   Chen GN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360702
   Desbenoit B, 2004, COMPUT GRAPH FORUM, V23, P341, DOI 10.1111/j.1467-8659.2004.00765.x
   Galin E, 2010, COMPUT GRAPH FORUM, V29, P429, DOI 10.1111/j.1467-8659.2009.01612.x
   Galin E, 2011, COMPUT GRAPH FORUM, V30, P2021, DOI 10.1111/j.1467-8659.2011.02055.x
   Glass K.R., 2006, P AFRIGRAPH
   Kelly G., 2006, GAM DES TECHN WORKSH
   Kelly T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944854
   Lipp M, 2011, COMPUT GRAPH FORUM, V30, P345, DOI 10.1111/j.1467-8659.2011.01865.x
   McCrae J., 2009, Proceedings - Graphics Interface, P95
   Mech R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P397, DOI 10.1145/237170.237279
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Vanegas CA, 2010, COMPUT GRAPH FORUM, V29, P25, DOI 10.1111/j.1467-8659.2009.01535.x
   Vanegas CA, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618457
   Weber B, 2009, COMPUT GRAPH FORUM, V28, P481, DOI 10.1111/j.1467-8659.2009.01387.x
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
NR 19
TC 25
Z9 29
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 809
EP 818
DI 10.1007/s00371-012-0699-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500028
DA 2024-07-18
ER

PT J
AU Shen, JB
   Zhao, Y
   He, Y
AF Shen, Jianbing
   Zhao, Ying
   He, Ying
TI Detail-preserving exposure fusion using subband architecture
SO VISUAL COMPUTER
LA English
DT Article
DE Exposure fusion; Subband architecture; QMF pyramid; Tone mapping
ID IMAGE FUSION; DESIGN
AB In this paper, we present a novel detail-preserving fusion approach from multiple exposure images using subband architecture. Given a sequence of different exposures, the Quadrature Mirror Filter (QMF) based subband architecture is first employed to decompose the original sequence into different frequency subbands. After that, we compute the importance weight maps according to the image appearance measurements, such as exposure, contrast, and saturation. In order to preserve the details of the subband signals, we compute the gain control maps and improve these subbands. Finally, the coefficients of subbands are blended into a high-quality detail-preserving fusion image. Experimental results demonstrate that the proposed approach successfully creates a visually pleasing exposure fusion image.
C1 [Shen, Jianbing; Zhao, Ying] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
C3 Beijing Institute of Technology; Nanyang Technological University
RP Shen, JB (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM shenjianbing@bit.edu.cn; bhyzhao@bit.edu.cn; yhe@ntu.edu.sg
RI He, Ying/A-3708-2011; Shen, Jianbing/U-8796-2019
OI He, Ying/0000-0002-6749-4485; Shen, Jianbing/0000-0002-4109-8353; Zhao,
   Ying/0000-0002-4113-8423
FU National Natural Science Foundation of China [60903068]; NSFC-Guangdong
   Union Foundation [U1035004]; Beijing Institute of Technology
   [2009Y0707]; SRF for ROCS, SEM;  [NRF2008IDM-IDM004-006]
FX This work was supported by the National Natural Science Foundation of
   China (No. 60903068), the Key Program of NSFC-Guangdong Union Foundation
   (No. U1035004), and Excellent Young Teacher Research Fund of Beijing
   Institute of Technology (2009Y0707). The Project-sponsored by SRF for
   ROCS, SEM. Ying He was supported by the NRF2008IDM-IDM004-006 grant.
CR [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 2010, P 7 IND C COMP VIS G
   [Anonymous], IEEE INT C COMP PHOT
   [Anonymous], 2010, AS C COMP VIS
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Chan SC, 2004, IEEE T SIGNAL PROCES, V52, P2135, DOI 10.1109/TSP.2004.828918
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fattal R., 2002, ACM Transactions on Graphics, V21, P249, DOI 10.1145/566570.566573
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P427, DOI 10.1017/S095252380001124X
   Jacobs K, 2008, IEEE COMPUT GRAPH, V28, P84, DOI 10.1109/MCG.2008.23
   Johnston J. D., 1980, ICASSP 80 Proceedings. IEEE International Conference on Acoustics, Speech and Signal Processing, P291
   Kao WC, 2006, IEEE T CONSUM ELECTR, V52, P735
   Khan EA, 2006, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2006.312892
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   MALLAT S, 1992, IEEE T PATTERN ANAL, V14, P710, DOI 10.1109/34.142909
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Raman S., 2009, P 26 INT C MACHINE L, P1
   Raman S., 2009, P SIGGRAPH ASIA 2009
   Raskar R., 2004, NONPHOTOREALISTIC AN, P85, DOI DOI 10.1145/987657.987671
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Simoncelli E.P., 1990, SUBBAND CODING, P143
   Smith MI, 2005, PROC SPIE, V5782, P29, DOI 10.1117/12.597618
   SWAMINATHAN K, 1986, IEEE T CIRCUITS SYST, V33, P1170, DOI 10.1109/TCS.1986.1085876
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Ward G., 2003, Journal of Graphics Tools, V8, P17, DOI 10.1080/10867651.2003.10487583
   WOODS JW, 1986, IEEE T ACOUST SPEECH, V34, P1278, DOI 10.1109/TASSP.1986.1164962
   Xia T, 1999, IEEE T SIGNAL PROCES, V47, P1878, DOI 10.1109/78.771037
   Yee YH, 2003, VISUAL COMPUT, V19, P457, DOI 10.1007/s00371-003-0211-5
   Zhang W, 2010, PROC CVPR IEEE, P530, DOI 10.1109/CVPR.2010.5540168
NR 36
TC 13
Z9 15
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2012
VL 28
IS 5
BP 463
EP 473
DI 10.1007/s00371-011-0642-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EJ
UT WOS:000302813800004
DA 2024-07-18
ER

PT J
AU Lin, WC
   Huang, YJ
AF Lin, Wen-Chieh
   Huang, Yi-Jheng
TI Animating rising up from various lying postures and environments
SO VISUAL COMPUTER
LA English
DT Article
DE Rising motion; Motion planning; Character animation
ID MOVEMENT PATTERNS; SUPINE POSITION; AGE-DIFFERENCES; HUMAN MOTION; RISE
AB Generating rising up motions is an important problem but has less been addressed in computer animation. This problem is challenging as rising motions involve complex motor skills and exhibit wide varieties due to various lying postures and environments. In this paper, we present an approach that utilizes motion planning and dynamics filtering to produce physically plausible rising motions. Our motion planning algorithm connects a given posture to a closest posture in a database of 14 rising motions. Then the dynamics filtering generates a physically plausible motion from a planned motion path. Our experiments show that a variety of motions of rising from various lying postures and different environments with obstacles can be generated easily by our approach.
C1 [Lin, Wen-Chieh; Huang, Yi-Jheng] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Lin, Wen-Chieh] Natl Chiao Tung Univ, Inst Multimedia Engn, Hsinchu, Taiwan.
   [Huang, Yi-Jheng] Natl Chiao Tung Univ, Inst Network Engn, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Yang Ming Chiao Tung University
RP Lin, WC (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM wclin@cs.nctu.edu.tw; sayhello.cs96g@g2.nctu.edu.tw
FU National Science Council [NSC-99-2628-E-009-178-]; UST-UCSD
   International Center of Excellence in Advanced Bioengineering; Taiwan
   National Science Council I-RiCE [NSC-99-2911-I-009-101]
FX This work was supported in part by National Science Council under Grant
   NSC-99-2628-E-009-178- and the UST-UCSD International Center of
   Excellence in Advanced Bioengineering sponsored by the Taiwan National
   Science Council I-RiCE Program under Grant No. NSC-99-2911-I-009-101.
CR Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   David H. A., 1988, The Method of Paired Comparisons
   Ericson C., 2005, Real-time collision detection
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   FORDSMITH CD, 1993, PHYS THER, V73, P300, DOI 10.1093/ptj/73.5.300
   Fujiwara K., 2003, Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453), P1920
   Hirukawa H, 2005, INT J ROBOT RES, V24, P755, DOI 10.1177/0278364905057217
   Janssen WGM, 2002, PHYS THER, V82, P866, DOI 10.1093/ptj/82.9.866
   Kalisiak M, 2006, IEEE INT CONF ROBOT, P1237, DOI 10.1109/ROBOT.2006.1641878
   Kanehiro F, 2007, IEEE INT CONF ROBOT, P2540, DOI 10.1109/ROBOT.2007.363847
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kuffner J. J.  Jr., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P995, DOI 10.1109/ROBOT.2000.844730
   Kuffner JamesJ., 2003, Robotics Research, The Eleventh International Symposium, ISRR, October 19-22, 2003, Siena, Italy, P365
   LaValle SM, 2001, ALGORITHMIC AND COMPUTATIONAL ROBOTICS: NEW DIRECTIONS, P293
   LaValle SM, 2001, INT J ROBOT RES, V20, P378, DOI 10.1177/02783640122067453
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Liu CK, 2002, ACM T GRAPHIC, V21, P408, DOI 10.1145/566570.566596
   Liu L., 2010, ACM SIGGRAPH Papers, P1
   Macchietto A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531386
   MCCOY JO, 1993, PHYS THER, V73, P182, DOI 10.1093/ptj/73.3.182
   Morimoto J, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P1721, DOI 10.1109/IROS.1998.724846
   Pratt J, 1996, IROS 96 - PROCEEDINGS OF THE 1996 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - ROBOTIC INTELLIGENCE INTERACTING WITH DYNAMIC WORLDS, VOLS 1-3, P1219, DOI 10.1109/IROS.1996.568974
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Shiratori T., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P37
   Tsai YY, 2010, IEEE T VIS COMPUT GR, V16, P325, DOI 10.1109/TVCG.2009.76
   Ulbrich J, 2000, J AM GERIATR SOC, V48, P1626, DOI 10.1111/j.1532-5415.2000.tb03874.x
   VANSANT AF, 1988, PHYS THER, V68, P1330, DOI 10.1093/ptj/68.9.1330
   VANSANT AF, 1988, PHYS THER, V68, P185, DOI 10.1093/ptj/68.2.185
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   Yamane K, 2004, ACM T GRAPHIC, V23, P532, DOI 10.1145/1015706.1015756
   Yamane K, 2003, IEEE T ROBOTIC AUTOM, V19, P421, DOI 10.1109/TRA.2003.810579
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
NR 32
TC 3
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 413
EP 424
DI 10.1007/s00371-011-0648-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Teramoto, O
   Park, IK
   Igarashi, T
AF Teramoto, Okihide
   Park, In Kyu
   Igarashi, Takeo
TI Interactive motion photography from a single image
SO VISUAL COMPUTER
LA English
DT Article
DE Motion photograph; Informative; Evocative; User interface; Photoshop
AB In this paper, we present an interactive and computational method of creating a motion photograph from a single image. Inspired by cartoon arts, a motion photograph is a still image in which moving objects are depicted as informative and motion-evocative. With a still image that contains moving objects, the user can add and edit various motion effects by a simple but efficient user interface. As a result, the edited object conveys an effective motion effect without blurring the object. The proposed system is so user-friendly that novices can create motion photographs without any special skills. Furthermore, the extensible nature of the system means that new effects can be developed and plugged in. The experimental results and user study show that the proposed motion photography system can produce a variety of interesting motion photographs. Compared with general image editing tools (such as Photoshop), the proposed system can create high-quality motion photographs in a significantly shorter time.
C1 [Park, In Kyu] Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
   [Teramoto, Okihide; Igarashi, Takeo] Univ Tokyo, Tokyo 1130033, Japan.
C3 Inha University; University of Tokyo
RP Park, IK (corresponding author), Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
EM tera1984@gmail.com; pik@inha.ac.kr; takeo@acm.org
RI Igarashi, Takeo/ITT-5921-2023; Park, In Kyu/B-5967-2013
FU Korean Government [KRF-2008-331-D00512]
FX This work was supported by the Korea Research Foundation Grant funded by
   the Korean Government (MOEHRD, Basic Research Promotion Fund)
   (KRF-2008-331-D00512).
CR *AD SYST INC, AD PHOT
   Bennett EP, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276505, 10.1145/1239451.1239553]
   Brostow GJ, 2001, COMP GRAPH, P561, DOI 10.1145/383259.383325
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Collomosse JP, 2005, GRAPH MODELS, V67, P549, DOI 10.1016/j.gmod.2004.12.002
   Cutting JE, 2002, PERCEPTION, V31, P1165, DOI 10.1068/p3318
   FREEMAN WT, 1991, COMP GRAPH, V25, P27, DOI 10.1145/127719.122721
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   HALLER M, 2004, ACM COMPUT ENTERTAIN, V2, P11
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kawagishi Y, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P276, DOI 10.1109/CGI.2003.1214482
   Kim B, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P32
   MASUCH M, 1999, ACM SIGGRAPH ABSTRAC, P227
   OBAYASHI S, 2005, ACM SIGGRAPH SKETCHE, P93
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   [No title captured]
NR 17
TC 7
Z9 9
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1339
EP 1348
DI 10.1007/s00371-009-0405-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Courbet, C
   Isenburg, M
AF Courbet, Clement
   Isenburg, Martin
TI Streaming compression of hexahedral meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Large meshes; Streaming compression; Hexahedral meshes; Cell data
   compression
AB We describe a method for streaming compression of hexahedral meshes. Given an interleaved stream of vertices and hexahedra our coder incrementally compresses the mesh in the presented order. Our coder is extremely memory efficient when the input stream also documents when vertices are referenced for the last time (i.e. when it contains topological finalization tags). Our coder then continuously releases and reuses data structures that no longer contribute to compressing the remainder of the stream. This means in practice that our coder has only a small fraction of the whole mesh in memory at any time. We can therefore compress very large meshes-even meshes that do not fit in memory.
   Compared to traditional, non-streaming approaches that load the entire mesh and globally reorder it during compression, our algorithm trades a less compact compressed representation for significant gains in speed, memory, and I/O efficiency. For example, on the 456k hexahedra "blade" mesh, our coder is twice as fast and uses 88 times less memory (only 3.1 MB) with the compressed file increasing about 3% in size. We also present the first scheme for predictive compression of properties associated with hexahedral cells.
C1 [Courbet, Clement] Ecole Cent Paris, Paris, France.
   [Isenburg, Martin] Lawrence Livermore Natl Lab, Lawrence, CA USA.
C3 Universite Paris Saclay; United States Department of Energy (DOE);
   Lawrence Livermore National Laboratory
RP Courbet, C (corresponding author), Ecole Cent Paris, Paris, France.
EM clement.courbet@ecp.fr; isenburg@llnl.gov
FU French National Research Agency (ANR) [ANR-08-COSI-003]; U.S. Department
   of Energy [DE-AC52-07NA27344]; Office of Advanced Scientific Computing
   Research, Office of Science, of the U.S. Department of Energy
   [DE-AC02-05CH11231]
FX This work has in part been supported by French National Research Agency
   (ANR) through COSINUS program (project COLLAVIZ ANR-08-COSI-003) and by
   the collaboration SACO with the CEA DAM/DIF. This work was in part
   performed under the auspices of the U.S. Department of Energy by
   Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344
   and was supported by the Director, Office of Advanced Scientific
   Computing Research, Office of Science, of the U.S. Department of Energy
   under Contract No. DE-AC02-05CH11231. The "cedre" dataset comes from a
   combustion simulation at the French aerospace laboratory ONERA using
   their CEDRE software.
CR BENZLEY, 1995, MESHING ROUNDTABLE
   BLACKER, 1996, MESHING ROUNDTABLE
   GUTHE, 1999, VISUALIZATION
   HO, 2001, VISUALIZATION
   Ibarria L, 2007, J COMPUT, V2, P53, DOI 10.4304/jcp.2.8.53-63
   ISENBURG, 2006, GRAPHICS INTERFACE
   ISENBURG, 2002, GRAPHICS INTERFACE
   ISENBURG, 2005, VISUALIZATION
   ISENBURG, 2003, SIGGRAPH
   ISENBURG, 2005, EUR S GEOM PROC
   ISENBURG, 2005, COMPUTER AIDED DESIG
   ISENBURG, 2005, SPRING C COMP GRAPH
   ISENBURG, 2002, GRAPHICAL MODELS
   Krivograd S, 2008, COMPUT AIDED DESIGN, V40, P1105, DOI 10.1016/j.cad.2008.10.013
   LINDSTROM, 2008, IEEE DAT COMPR C
   MULLERHANNEMANN, 2001, J GRAPH ALGORITHMS A, V5, P59
   Prat S, 2005, VISUAL COMPUT, V21, P876, DOI 10.1007/s00371-005-0325-z
   STATEN, 2005, INT MESHING ROUNDTAB
   TOUMA, 1998, GRAPHICS INTERFACE
NR 19
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1113
EP 1122
DI 10.1007/s00371-010-0481-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800070
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, BQ
   Clapworthy, GJ
   Dong, F
AF Liu, Baoquan
   Clapworthy, Gordon J.
   Dong, Feng
TI Wavefront raycasting using larger filter kernels for on-the-fly GPU
   gradient reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE GPU; Raycasting; CUDA; Wavefront tracing
AB The quality of images generated by volume rendering strongly depends on the accuracy of gradient estimation. However, the most commonly used techniques for on-the-fly gradient reconstruction are still very simple, such as central differences; they generally gather only limited neighbourhood information and thus ultimately produce rather poor quality images. While there are many higher-order reconstruction methods, such as 3x3x3 or 5x5x5 filters, which can improve the quality, their excessive sampling costs have meant that they are generally used only for pre-computed gradients, which are then quantized and stored for later runtime re-interpolation. This may introduce further errors and, significantly, may consume valuable texture memory.
   In this paper, we address these issues by proposing a CUDA-based rendering framework that uses larger filter kernels for on-the-fly gradient computation in real-time raycasting applications. By using adaptive wavefront tracing, our approach can dramatically reduce the memory bandwidth requirements related to larger neighbour samples. To further ensure that samples are consumed wisely, we have devised a novel adaptive sampling scheme and a customized 3D mipmapping technique in the CUDA environment to sample at a proper level of detail as the ray recedes into the distance. We compared our technique with two previous state-of-the-art GPU raycasting algorithms and found that it achieves higher quality imaging and faster rendering performance across a variety of data sets than the previous methods.
C1 [Liu, Baoquan] Univ Bedfordshire, Dept Comp Sci & Technol, Res Ctr Comp Graph & Visualisat, Luton, Beds, England.
   [Clapworthy, Gordon J.] Univ Bedfordshire, Ctr Comp Graph & Visualizat, Luton, Beds, England.
C3 University of Bedfordshire; University of Bedfordshire
RP Liu, BQ (corresponding author), Univ Bedfordshire, Dept Comp Sci & Technol, Res Ctr Comp Graph & Visualisat, Luton, Beds, England.
EM Baoquan.Liu@beds.ac.uk; Gordon.Clapworthy@beds.ac.uk;
   Feng.Dong@beds.ac.uk
FU European Commission [FP7-PEOPLE-IIF-2008-236120]
FX We would like to thank the anonymous reviewers for their constructive
   comments and suggestions. This work was partially supported by the
   European Commission within the Marie Curie project GAMVolVis
   (FP7-PEOPLE-IIF-2008-236120).
CR AGUS M, 2009, FIN WORKSH IN PRESS
   AGUS M, 2008, EUR IT CHAPT C C HEL
   Amanatides J., 1984, Computers & Graphics, V18, P129
   [Anonymous], 2007, Optimizing parallel reductions in CUDA
   DONG F, 2001, IEEE VISUALIZATION 2, P387
   Engel K, 2006, Real-Time Volume Graphics
   Hadwiger M, 2005, COMPUT GRAPH FORUM, V24, P303, DOI 10.1111/j.1467-8659.2005.00855.x
   Ihrke L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451,1239510
   KIM J, 2008, THESIS U MARYLAND CO
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Liu B, 2009, COMPUT GRAPH FORUM, V28, P839, DOI 10.1111/j.1467-8659.2009.01466.x
   Ljung P, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P25
   Ljung P., 2006, P VOL GRAPH, P39
   Marsalek L., 2008, IEEE S INT RAY TRAC
   Neumann L, 2000, COMPUT GRAPH FORUM, V19, pC351, DOI 10.1111/1467-8659.00427
   *NVIDIA, 2009, NVIDIA CUDA SDK 2 3
   Weinlich A., 2008, Proceedings of the First International Workshop on New Frontiers in Highperformance and Hardware-aware Computing (HipHaC'08), V1, P25
NR 17
TC 1
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1079
EP 1089
DI 10.1007/s00371-010-0493-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800067
DA 2024-07-18
ER

PT J
AU Martens, G
   Poppe, C
   Lambert, P
   Van de Walle, R
AF Martens, Gaetan
   Poppe, Chris
   Lambert, Peter
   Van de Walle, Rik
TI Noise- and compression-robust biological features for texture
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Texture classification; Grating cell; Gabor; Noise; Image compression
ID SPATIAL-FREQUENCY; MODEL
AB Texture classification is an important aspect of many digital image processing applications such as surface inspection, content-based image retrieval, and biomedical image analysis. However, noise and compression artifacts in images cause problems for most texture analysis methods. This paper proposes the use of features based on the human visual system for texture classification using a semisupervised, hierarchical approach. The texture feature consists of responses of cells which are found in the visual cortex of higher primates. Classification experiments on different texture libraries indicate that the proposed features obtain a very high classification near 97%. In contrast to other well-established texture analysis methods, the experiments indicate that the proposed features are more robust to various levels of speckle and Gaussian noise. Furthermore, we show that the classification rate of the textures using the presented biologically inspired features is hardly affected by image compression techniques.
C1 [Martens, Gaetan; Poppe, Chris; Lambert, Peter; Van de Walle, Rik] Ghent Univ IBBT, Multimedia Lab, Dept Elect & Informat Syst, B-9050 Ghent, Belgium.
C3 Ghent University
RP Martens, G (corresponding author), Ghent Univ IBBT, Multimedia Lab, Dept Elect & Informat Syst, Gaston Crommenlaan 8,Bus 201, B-9050 Ghent, Belgium.
EM gaetan.martens@ugent.be; chris.poppe@ugent.be; peter.lambert@ugent.be;
   rik.vandewalle@ugent.be
RI Lambert, Peter/D-7776-2016
OI Lambert, Peter/0000-0001-5313-4158
FU Ghent University; Interdisciplinary Institute for Broadband Technology
   (IBBT); Institute for the Promotion of Innovation by Science and
   Technology in Flanders (IWT); Fund for Scientific Research-Flanders
   (FWO-Flanders); European Union
FX The research activities as described in this paper were funded by Ghent
   University, the Interdisciplinary Institute for Broadband Technology
   (IBBT), the Institute for the Promotion of Innovation by Science and
   Technology in Flanders (IWT), the Fund for Scientific Research-Flanders
   (FWO-Flanders), and the European Union.
CR [Anonymous], MATLAB IMAGE PROCESS
   [Anonymous], MEASTEX IMAGE TEXTUR
   [Anonymous], SELF ORGANIZING MAPS
   Aschkenasy SV, 2005, ULTRASOUND MED BIOL, V31, P361, DOI 10.1016/j.ultrasmedbio.2004.11.009
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   Brodatz P., 1966, TEXTURES PHOTOGRAPHI
   CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P959, DOI 10.1109/TASSP.1985.1164641
   CLARK M, 1987, PATTERN RECOGN LETT, V6, P261, DOI 10.1016/0167-8655(87)90086-9
   Clausi DA, 2000, PATTERN RECOGN, V33, P1835, DOI 10.1016/S0031-3203(99)00181-8
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   FOUNTAIN SR, 1997, P IEEE INT C IM PROC, V3, P197
   HALL CF, 1977, IEEE T SYST MAN CYB, V7, P161, DOI 10.1109/TSMC.1977.4309680
   Iakovidis DK, 2008, LECT NOTES COMPUT SC, V5112, P750, DOI 10.1007/978-3-540-69812-8_74
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   KACHOUIE NN, 2005, J APPL SIGNAL PROCES, V12, P1834
   KRUIZINGA P, 1999, P NSIP 99 IEEE WORKS, V2, P881
   Lourens T, 2005, BIOL CYBERN, V92, P61, DOI 10.1007/s00422-004-0522-2
   Ma HF, 2005, P SOC PHOTO-OPT INS, V5676, P148, DOI 10.1117/12.586345
   MARTENS G, 2008, P 2008 IEEE 10 WORKS, P159
   MARTENS G, 2007, PERF EV COMP VIS 31, P9
   Murino V, 1998, PATTERN RECOGN, V31, P383, DOI 10.1016/S0031-3203(97)00055-1
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   PICARD RW, 1995, MULTIMEDIA SYST, V3, P3, DOI 10.1007/BF01236575
   POLLEN DA, 1983, IEEE T SYST MAN CYB, V13, P907, DOI 10.1109/TSMC.1983.6313086
   Vesanto J., 2000, Proceedings of the Matlab DSP Conference, P35
   VONDERHEYDT R, 1992, J NEUROSCI, V12, P1416
   Zhang G, 2008, LECT NOTES COMPUT SC, V4987, P125
NR 28
TC 6
Z9 6
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 915
EP 922
DI 10.1007/s00371-010-0455-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800051
DA 2024-07-18
ER

PT J
AU Wu, JZ
   Zheng, CW
   Hu, XH
   Wang, Y
   Zhang, LQ
AF Wu, Jiaze
   Zheng, Changwen
   Hu, Xiaohui
   Wang, Yang
   Zhang, Liqiang
TI Realistic rendering of bokeh effect based on optical aberrations
SO VISUAL COMPUTER
LA English
DT Article
DE Bokeh effect; Optical aberrations; Realistic rendering; General
   sequential ray tracing
ID DEPTH
AB Bokeh effect is an important characteristic for realistic image synthesis. However, existing bokeh rendering methods are incapable of simulating realistic bokeh effects due to not taking into account optical characteristics of real lenses, especially optical aberrations. In this paper, a novel realistic bokeh rendering method, based on an accurate camera lens model and distributed ray tracing, is presented. An optical analysis of the relationship between bokeh and optical aberrations, including spherical aberration, coma, astigmatism and field curvature, is firstly introduced. Based on this analysis, a physically-based camera lens model, which takes detailed lens prescription as input, is then introduced for accurately modeling the aberrations. The position and diameter of the entrance and exit pupils are calculated by tracing rays inside the lens for achieving efficient ray sampling, and a general sequential ray tracing algorithm is proposed to better combine with bidirectional ray tracing. Furthermore, correct integration between the lens model and bidirectional ray tracing is also analyzed. The rendering results demonstrate a variety of realistic bokeh effects caused by the aberrations.
C1 [Wu, Jiaze; Zheng, Changwen; Hu, Xiaohui; Wang, Yang; Zhang, Liqiang] Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
   [Wu, Jiaze; Wang, Yang; Zhang, Liqiang] Chinese Acad Sci, Grad Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Wu, JZ (corresponding author), Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
EM wujiaze05@gmail.com
RI Zhang, Xiaoling/AAT-4795-2020
OI Zhang, Xiaoling/0000-0002-6369-9424
FU National High-Tech Research and Development Plan of China [2009AA01Z303]
FX We would like to very thank Yuxuan Zhang, Jie Zhang and Chao Li for
   their valuable comments. We also thank the LuxRender community and the
   chess model creators. This work was partly supported by the National
   High-Tech Research and Development Plan of China (Grant No.
   2009AA01Z303).
CR Ang T., 2002, DICT PHOTOGRAPHY DIG
   [Anonymous], EUR 2002 SEP
   Born M., 2013, PRINCIPLES OPTICS
   BUHLER J, 2002, COMPUTER GRAPHICS P, P142
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Evans GF, 1999, PROC GRAPH INTERF, P42
   Fischer RE, 2008, P SOC PHOTO-OPT INS, V7068, DOI 10.1117/12.796840
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   HAEBERLI P, 1990, COMPUTER GRAPHICS, P309
   Kass Michael, 2006, Interactive depth of field using simulated diffusion on a GPU
   KODAMA KMOH, 2006, COMPUTER GRAPHICS P, P77
   Kolb C., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P317, DOI 10.1145/218380.218463
   Kosloff T.J., 2009, GI 09, P39
   Lanman D., 2008, P INT S COMP AESTH G, P102
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P453, DOI 10.1109/TVCG.2008.106
   Lee S, 2008, COMPUT GRAPH FORUM, V27, P1955, DOI 10.1111/j.1467-8659.2008.01344.x
   Lee Sungkil, 2009, ACM T GRAPHIC, V28, P1
   Merklinger H. M., 1997, PHOTO TECHN, V18, P37
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Pharr Matt., 2004, PHYSICALLY BASED REN
   Potmesil M., 1981, Computer Graphics, V15, P297, DOI 10.1145/965161.806818
   RIGUER G, 2003, SHADERX2 SHADER PROG, P529
   ROKITA P, 1993, COMPUT GRAPH, V17, P593, DOI 10.1016/0097-8493(93)90010-7
   Smith W., 1992, MODERN LENS DESIGN
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Sun YL, 2001, VISUAL COMPUT, V17, P429, DOI 10.1007/s003710100116
   VANWALREE P, ASTIGMATISM FIELD CU
   Veach E., 1998, ROBUST MONTE CARLO M
   Zhou TS, 2007, COMPUT GRAPH FORUM, V26, P15, DOI 10.1111/j.1467-8659.2007.00935.x
NR 29
TC 22
Z9 26
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 555
EP 563
DI 10.1007/s00371-010-0459-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800016
DA 2024-07-18
ER

PT J
AU Zhang, X
   Wang, ZY
   Wang, R
   Yang, ZL
   Chen, W
   Peng, QS
AF Zhang, Xin
   Wang, Zhangye
   Wang, Ran
   Yang, Zhonglei
   Chen, Wei
   Peng, Qunsheng
TI Real-time foveation filtering using nonlinear Mipmap interpolation
SO VISUAL COMPUTER
LA English
DT Article
DE Foveation filtering; Nonlinear Mipmap interpolation; Human Vision
   System; The Cornsweet illusion
ID COMPRESSION; VISION; SYSTEM
AB In recent years, several techniques have been proposed to simulate the gaze effect of the Human Visual System (HVS). It is believed that this effect is due to the foveation filtering. Current techniques to simulate the foveation filtering in computer graphics are either slow or suffer from artifacts and limitations. In this paper, we present a new approach of foveation filtering based on the Mipmap Pyramid of the current view by considering the relationship between the Gaussian kernel and Mipmap level. Due to the nonlinear Mipmap interpolation under the Bilateral Filtering scheme, we are able to simulate the foveation filtering more naturally and efficiently than in previous work. Moreover, a detail enhancement method based on the Cornsweet illusion is proposed to augment the gazing effect. We demonstrate our new approach with a variety of examples and provide comparisons with recent approaches.
C1 [Zhang, Xin; Wang, Zhangye; Wang, Ran; Yang, Zhonglei; Chen, Wei; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Peng, QS (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM peng@cad.zju.edu.cn
RI Zhou, Hong/JKJ-1067-2023
FU National Grand Foundation Research 973 Program of China [2009CB320802];
   National 863 High Technology Plan Foundation of China [2007AA01Z316];
   National Natural Science Foundation of China [60970020, 60873123,
   60970075, 60833007]
FX This research is supported by National Grand Foundation Research 973
   Program of China under Grant No. 2009CB320802, National 863 High
   Technology Plan Foundation of China under Grant No. 2007AA01Z316, Key
   Project of National Natural Science Foundation of China under Grant No.
   60833007, National Natural Science Foundation of China Under Grant No.
   60970020, No. 60873123 and No. 60970075. We would like to thank Miss.
   Jing Fang for her help in video production.
CR [Anonymous], ACM T GRAPH
   [Anonymous], 1995, HDB OPTICS
   Badamchizadeh M. A., 2004, Proceedings. Third International Conference on Image and Graphics, P27
   BANKS MS, 1991, J OPT SOC AM A, V8, P1775, DOI 10.1364/JOSAA.8.001775
   BJORKE K, 2004, TIPS TRICKS REAL TIM, P391
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cornsweet T.N., 1970, Visual Perception
   Dhavale N, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P229, DOI 10.1109/ISSPA.2003.1224682
   Dikici Ç, 2004, LECT NOTES COMPUT SC, V3211, P285
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Duchowski AT, 2000, IEEE T IMAGE PROCESS, V9, P1437, DOI 10.1109/83.855439
   Etienne-Cummings R, 2000, IEEE T CIRCUITS-II, V47, P504, DOI 10.1109/82.847066
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   GARCIA DD, 2000, CWHATUC SOFTWARE TOO
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Kinser J. M., 1999, Proceedings 1999 International Conference on Information Intelligence and Systems (Cat. No.PR00446), P86, DOI 10.1109/ICIIS.1999.810228
   Klarquist WN, 1998, IEEE T ROBOTIC AUTOM, V14, P755, DOI 10.1109/70.720351
   KOZ A, 2002, INT C IM PROC, V3, P661
   Krawczyk G, 2007, COMPUT GRAPH FORUM, V26, P581, DOI 10.1111/j.1467-8659.2007.01081.x
   Lee S, 2003, IEEE T CIRC SYST VID, V13, P149, DOI 10.1109/TCSVT.2002.808441
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Murphy HA, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462053
   Perry JS, 2002, P SOC PHOTO-OPT INS, V4662, P57, DOI 10.1117/12.469554
   Popkin T, 2010, IEEE T IMAGE PROCESS, V19, P1362, DOI 10.1109/TIP.2010.2041400
   Ritschel T., 2008, ACM T GRAPHIC, V27, P1
   ROBSON JG, 1981, VISION RES, V21, P409, DOI 10.1016/0042-6989(81)90169-3
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Stiles WS, 1933, P R SOC LOND B-CONTA, V112, P428, DOI 10.1098/rspb.1933.0020
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsumura N, 1996, P SOC PHOTO-OPT INS, V2657, P361, DOI 10.1117/12.238733
   WALLACE RS, 1994, INT J COMPUT VISION, V13, P71, DOI 10.1007/BF01420796
   Wandell B. A, 1995, Foundations of vision
   Wang Z, 2006, SIGN PROC COMMUN SER, V28, P431
   WARD G, 1997, IEEE T VIS COMPUT GR, V4, P291
   Yitzhaky Y, 2005, OPT ENG, V44, DOI 10.1117/1.2084667
NR 38
TC 3
Z9 6
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 923
EP 932
DI 10.1007/s00371-010-0432-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800052
DA 2024-07-18
ER

PT J
AU Fan, Z
   Kuo, YC
   Zhao, Y
   Qiu, F
   Kaufman, A
   Arcieri, W
AF Fan, Zhe
   Kuo, Yu-Chuan
   Zhao, Ye
   Qiu, Feng
   Kaufman, Arie
   Arcieri, William
TI Visual simulation of thermal fluid dynamics in a pressurized water
   reactor
SO VISUAL COMPUTER
LA English
DT Article
DE Power plant; Pressurized thermal shock; Thermal fluid dynamics; Lattice
   Boltzmann method
ID LATTICE; COMPUTATION; GPU
AB We present a simulation and visualization system for a critical application - analysis of the thermal fluid dynamics inside a pressurized water reactor of a nuclear power plant when cold water is injected into the reactor vessel. We employ a hybrid thermal lattice Boltzmann method (HTLBM), which has the advantages of ease of parallelization and ease of handling complex simulation boundaries. For efficient computation and storage of the irregular-shaped simulation domain, we classify the domain into nonempty and empty cells and apply a novel packing technique to organize the nonempty cells. This method is implemented on a GPU cluster for acceleration. We demonstrate the formation of cold-water plumes in the reactor vessel. A set of interactive visualization tools, such as side-view slices, 3D volume rendering, thermal layers rendering, and panorama rendering, are provided to collectively visualize the structure and dynamics of the temperature field in the vessel. To the best of our knowledge, this is the first system that combines 3D simulation and visualization for analyzing thermal shock risk in a pressurized water reactor.
C1 [Fan, Zhe; Kuo, Yu-Chuan; Qiu, Feng; Kaufman, Arie] SUNY Stony Brook, Ctr Visual Comp, Stony Brook, NY 11794 USA.
   [Fan, Zhe; Kuo, Yu-Chuan; Qiu, Feng; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Zhao, Ye] Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
   [Arcieri, William] ISL Inc, Rockville, MD 20852 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook; University System of Ohio;
   Kent State University; Kent State University Kent; Kent State University
   Salem
RP Fan, Z (corresponding author), SUNY Stony Brook, Ctr Visual Comp, Stony Brook, NY 11794 USA.
EM fzhe@cs.sunysb.edu; yukuo@cs.sunysb.edu; zhao@cs.kent.edu;
   qfeng@cs.sunysb.edu; ari@cs.sunysb.edu; billa@islinc.com
FU NSF [CCF0702699]; U. S. Nuclear Regulatory Commission (NRC) through
   Information Systems Laboratories, Inc (ISL)
FX The work has been supported by NSF CCF0702699 and funding from the U. S.
   Nuclear Regulatory Commission (NRC) through Information Systems
   Laboratories, Inc (ISL). Figures 1 and 2 were adapted from [20]. We
   would also like to thank Autodesk, Inc. for donating Maya 2008 to the
   Center for Visual Computing. Maya 2008 has been used to create the PWR
   geometry for the simulation in this research project.
CR Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Chen S, 1998, ANNU REV FLUID MECH, V30, P329, DOI 10.1146/annurev.fluid.30.1.329
   Chu NSH, 2005, ACM T GRAPHIC, V24, P504, DOI 10.1145/1073204.1073221
   CRANE K, 2003, GPU GEMS, V3, P633
   d'Humières D, 2002, PHILOS T ROY SOC A, V360, P437, DOI 10.1098/rsta.2001.0955
   DYKEN C, 2007, HISTOPYRAMIDS ISOSUR
   Engel Klaus, 2001, P ACM SIGGRAPH EUROG, P9, DOI [DOI 10.1145/383507.383515, 10.1145/383507.383515]
   EricksonKirk M., 2006, NUREG 1806, P1
   Everitt C, 2001, INTERACTIVE ORDER IN
   Fan Z., 2004, SC 04, P47, DOI [DOI 10.1109/SC.2004.26, 10.1109/SC.2004.26]
   Fan Z, 2008, COMPUT GRAPH FORUM, V27, P341, DOI 10.1111/j.1467-8659.2008.01131.x
   GOODNIGHT N, 2003, ACM SIGGRAPH EUROGRA, P102
   HARRIS M, 2003, ACM SIGGRAPH EUR WOR, P92
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Krüger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   Lallemand P, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.036706
   Li F, 2005, NUCL ENG DES, V235, P1, DOI 10.1016/j.nucengdes.2004.09.004
   Li W, 2003, VISUAL COMPUT, V19, P444, DOI 10.1007/s00371-003-0210-6
   Li W., 2005, GPU GEMS 2 CHAPTER 4, P747
   Lobner P., 1990, NUREGCR5640
   Martin A, 2003, J PRESS VESS-T ASME, V125, P418, DOI 10.1115/1.1616583
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   SUCCI S, 2001, L BOLTZMANN EQUATION
   Thuerey N., 2004, VISION MODELING VISU, P199
   Van Treeck C, 2006, COMPUT FLUIDS, V35, P863, DOI 10.1016/j.compfluid.2005.03.006
   Wei XM, 2004, IEEE T VIS COMPUT GR, V10, P719, DOI 10.1109/TVCG.2004.48
   Wei XM, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P227, DOI 10.1109/VISUAL.2002.1183779
   Zhao Y, 2008, VISUAL COMPUT, V24, P323, DOI 10.1007/s00371-007-0191-y
   Zhao Y, 2007, IEEE T VIS COMPUT GR, V13, P179, DOI 10.1109/TVCG.2007.24
   Zhu HB, 2006, COMPUT ANIMAT VIRT W, V17, P403, DOI 10.1002/cav.143
NR 30
TC 6
Z9 6
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 985
EP 996
DI 10.1007/s00371-008-0309-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700002
DA 2024-07-18
ER

PT J
AU Morera, DM
   Carvalho, PC
   Velho, L
AF Morera, Dimas Martinez
   Carvalho, Paulo Cezar
   Velho, Luiz
TI Modeling on triangulations with geodesic curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Geodesic Bezier curve; Discrete geodesic; de Casteljau algorithm; Spline
   curves; Free-form design; Convex sets on surfaces
ID SUBDIVISION SCHEMES; MANIFOLDS; PATHS
AB This paper discusses the problem of modeling on triangulated surfaces with geodesic curves. In the first part of the paper we define a new class of curves, called geodesic Bezier curves, that are suitable for modeling on manifold triangulations. As a natural generalization of Bezier curves, the new curves are as smooth as possible. In the second part we discuss the construction of C-0 and C-1 piecewise Bezier splines. We also describe how to perform editing operations, such as trimming, using these curves. Special care is taken to achieve interactive rates for modeling tasks. The third part is devoted to the definition and study of convex sets on triangulated surfaces. We derive the convex hull property of geodesic Bezier curves.
C1 [Morera, Dimas Martinez; Carvalho, Paulo Cezar; Velho, Luiz] Inst Nacl Matemat Pura & Aplicada IMPA, BR-22460320 Rio De Janeiro, Brazil.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA)
RP Morera, DM (corresponding author), Inst Nacl Matemat Pura & Aplicada IMPA, Estrada Dona Castorina 110, BR-22460320 Rio De Janeiro, Brazil.
EM dimasmm@impa.br; pcezar@impa.br; lvelho@impa.br
CR Aleksandrov A.D., 1967, TRANSLATION MATH MON, V15
   ALEKSANDROV AD, 2006, INTRINSIC GEOMETRY C
   Biermann H, 2002, GRAPH MODELS, V64, P61, DOI 10.1006/gmod.2002.0570
   CHEN JD, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P360, DOI 10.1145/98524.98601
   Coelho LCG, 2000, INT J NUMER METH ENG, V47, P777
   Cohen E., 2001, Geometric modeling with splines: an introduction, V1st
   Crouch P., 1999, Journal of Dynamical and Control Systems, V5, P397, DOI 10.1023/A:1021770717822
   DECASTELJAU P, 1959, OUTILLAGE METHODES C
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   Kapoor S., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P770, DOI 10.1145/301250.301449
   Kasap E, 2005, APPL MATH COMPUT, V171, P1206, DOI 10.1016/j.amc.2005.01.109
   KIMMEL R, 1995, COMPUT MATH APPL, V29, P49, DOI 10.1016/0898-1221(94)00228-D
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Klassen E, 2004, IEEE T PATTERN ANAL, V26, P372, DOI 10.1109/TPAMI.2004.1262333
   Krishnan S, 1997, ACM T GRAPHIC, V16, P74, DOI 10.1145/237748.237751
   LAGES E, 1999, PROJETO EUCLIDES, P5
   Litke N, 2001, COMPUT AIDED GEOM D, V18, P463, DOI 10.1016/S0167-8396(01)00042-5
   Martínez D, 2005, COMPUT GRAPH-UK, V29, P667, DOI 10.1016/j.cag.2005.08.003
   MARTINEZ D, 2007, P SIBGRAPI 2007 20 B, P71
   Mémoli F, 2005, SIAM J APPL MATH, V65, P1227, DOI 10.1137/S003613990342877X
   MITCHELL JSB, 1987, SIAM J COMPUT, V16, P647, DOI 10.1137/0216045
   PARK FC, 1995, J MECH DESIGN, V117, P36, DOI 10.1115/1.2826114
   Polthier Konrad., 1998, Mathematical visualization (Berlin, 1997), P135, DOI [10.1007/978-3-662-03567-2_11, DOI 10.1007/978-3-662-03567-2_11]
   Pottmann H, 2005, COMPUT AIDED GEOM D, V22, P693, DOI 10.1016/j.cagd.2005.06.006
   Rodrigues R.C., 2005, LMS J. Comput. Math., V8, P251
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Wallner J, 2005, COMPUT AIDED GEOM D, V22, P593, DOI 10.1016/j.cagd.2005.06.003
   Wallner J, 2006, CONSTR APPROX, V24, P289, DOI 10.1007/s00365-006-0638-3
   Wallner J, 2006, ACM T GRAPHIC, V25, P356, DOI 10.1145/1138450.1138459
NR 31
TC 10
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1025
EP 1037
DI 10.1007/s00371-008-0298-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500004
DA 2024-07-18
ER

PT J
AU Tsai, YT
   Chang, CC
   Jiang, QZ
   Weng, SC
AF Tsai, Yu-Ting
   Chang, Chin-Chen
   Jiang, Qing-Zhen
   Weng, Shr-Ching
TI Importance sampling of products from illumination and BRDF using
   spherical radial basis functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE illumination; environment map; bidirectional reflectance distribution
   function; importance sampling; spherical radial basis function
ID WAVELETS
AB In this paper, a new approach for the importance sampling of products from a complex high dynamic range (HDR) environment map and measured bidirectional reflectance distribution function (BRDF) data using spherical radial basis functions (SRBFs) is presented. In the pre-process, a complex HDR environment map and measured BRDF data are transformed into a scattered SRBF representation by using a non-uniform and non-negative SRBF fitting algorithm. An initial guess is determined for the fitting operation. In the run-time rendering process, after the product of the two SRBFs is evaluated, this is used to guide the number of samples. The sampling is done by mixing samples from the various "product" SRBFs using multiple importance sampling. Hence, the proposed approach efficiently renders images with multiple HDR environment maps and measured BRDFs.
C1 [Tsai, Yu-Ting] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Jiang, Qing-Zhen] Natl Chiao Tung Univ, Inst Multimedia Engn, Hsinchu, Taiwan.
   [Weng, Shr-Ching] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Yang Ming Chiao Tung University
EM ccchang@nuu.edu.tw
RI Chang, Ching-Chun/JAN-6210-2023
CR Burke David., 2005, PROC EGSR 2005, P147
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Claustres L, 2003, COMPUT GRAPH FORUM, V22, P701, DOI 10.1111/j.1467-8659..00718.x
   Cline D., 2006, EUROGRAPHICS WORKSHO, P103
   Cohen Jonathan., 2001, LIGHTGEN HDRSHOP PLU
   Freeden W., 1998, Constructive Approximation on the Sphere: with Applications to Geomathematics
   GHOSH A., 2006, Eurographics Symposium on Rendering, P115
   Ghosh A, 2006, VISUAL COMPUT, V22, P693, DOI 10.1007/s00371-006-0055-x
   Kajiya J.T., 1986, P 13 ANN C COMP GRAP, P143, DOI 10.1145/15922.15902
   KOLLIG T, 2003, P 14 EUR WORKSH REND, P45
   LAFORTUNE EP, 1994, RPCW197 DEP COMP SCI
   LAFORTUNE EPF, 1997, P SIGGRAPH, P117
   LALONDE P, 1997, THESIS U BRIT COLUMB
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   MCCOOL MD, 1997, P GRAPH INT KEL BRIT, V97, P37
   Narcowich FJ, 1996, APPL COMPUT HARMON A, V3, P324, DOI 10.1006/acha.1996.0025
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611
   RUSINKIEWICZ SM, 1998, P EUR WORKSH REND, P11
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   Shirley P., 1991, Ph.D. Thesis
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   Subr K, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P133, DOI 10.1109/RT.2007.4342601
   TALBOT JF, 2005, P 16 EUR C REND TECH, P139
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI [DOI 10.1145/218380.218498, 10.1145/218380.218498]
   Veach E., 1998, ROBUST MONTE CARLO M
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 32
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 817
EP 826
DI 10.1007/s00371-008-0263-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800037
DA 2024-07-18
ER

PT J
AU Wang, CB
   Wang, ZY
   Zhang, X
   Huang, L
   Yang, ZL
   Peng, QS
AF Wang, Changbo
   Wang, Zhangye
   Zhang, Xin
   Huang, Lei
   Yang, Zhiliang
   Peng, Qunsheng
TI Real-time modeling and rendering of raining scenes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE raining scene; real-time rendering; rain streak; atmosphere scattering
ID SCATTERING
AB Real-time modeling and rendering of a realistic raining scene is a challenging task. This is because the visual effects of raining involve complex physical mechanisms, reflecting the physical, optical and statistical characteristics of raindrops, etc. In this paper, we propose a set of new methods to model the raining scene according to these physical mechanisms. Firstly, by adhering to the physical characteristic of raindrops, we model the shapes, movements and intensity of raindrops in different situations. Then, based on the principle of human vision persistence, we develop a new model to calculate the shapes and appearances of rain streaks. To render the foggy effect in a raining scene, we present a statistically based multi-particles scattering model exploiting the particle distribution coherence along each viewing ray. By decomposing the conventional equations of single scattering of non-isotropic light into two parts with the physical parameter independent part precalculated, we are able to render the respective scattering effect in real time. We also realize diffraction of lamps, wet ground, the ripples on puddles in the raining scene, as well as the beautiful rainbow. By incorporating GPU acceleration, our approach permits real-time walkthrough of various raining scenes with average 20 fps rendering speed and the results are quite satisfactory.
C1 [Wang, Changbo] E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
   [Wang, Changbo; Wang, Zhangye; Zhang, Xin; Huang, Lei; Yang, Zhiliang; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 East China Normal University; Zhejiang University
RP Wang, CB (corresponding author), E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
EM cbwang@cad.zju.edu.cn
RI Zhou, Hong/JKJ-1067-2023; Huang, Li/IUQ-0909-2023; li,
   mengyang/JWO-9551-2024; huang, lei/GQP-8739-2022; HUANG,
   LING/HTR-1819-2023
CR BEST AC, 1950, Q J ROY METEOR SOC, V76, P16, DOI 10.1002/qj.49707632704
   Garg K, 2004, PROC CVPR IEEE, P528
   Garg K, 2006, ACM T GRAPHIC, V25, P996, DOI 10.1145/1141911.1141985
   Jackel D, 1997, COMPUT GRAPH FORUM, V16, P201, DOI 10.1111/1467-8659.00180
   Kaneda K, 1999, J VISUAL COMP ANIMAT, V10, P15, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<15::AID-VIS192>3.0.CO;2-P
   KANEDA K, 1991, VISUAL COMPUT, V7, P245
   LUO J, 2004, J IMAGE GRAPH, V9, P495
   Manning R.M., 1993, STOCHASTIC ELECTROMA
   Nakamae E., 1990, Computer Graphics, V24, P395, DOI 10.1145/97880.97922
   NARASIMHAN S, 2003, ICCV WORKSH COL PHOT, P395
   Nayar S. K., 2003, Photometric model of a rain drop
   Nishita T., 1986, Computer Graphics, V20, P125, DOI 10.1145/15886.15900
   Nishita Tomoyuki., 1987, COMPUTER GRAPHICS P, V21, P303, DOI [10.1145/37401.37437, DOI 10.1145/37401.37437]
   REEVES W, 1983, COMPUT GRAPH, V17, P251
   Rousseau P, 2006, COMPUT GRAPH-UK, V30, P507, DOI 10.1016/j.cag.2006.03.013
   Starik S., 2003, P TEXTURE 3 INT WORK, P95
   Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309
   TATARCHUK NIJ, 2006, P C SIGGRAPH 2006 CO, P29
   WANG LF, 2006, P SIGGR 2006 SKETCH, P156
   WANG N, 2004, SIGGRAPH 2004 SKETCH, P186
NR 20
TC 9
Z9 12
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 605
EP 616
DI 10.1007/s00371-008-0241-0
PG 12
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 322OI
UT WOS:000257384800016
DA 2024-07-18
ER

PT J
AU Rustamov, RM
AF Rustamov, Raif M.
TI Augmented planar reflective symmetry transform
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 9th International Conference on Shape Modeling and Applications
CY JUN 13-15, 2007
CL Lyon, FRANCE
SP ACM SIGRAPT, CNRS, Groupement Rech Informat Mathemat, Reg Rhone Alpes, Univ Claude Bernard Lyon 1
DE symmetry transform; 3D shape retrieval; spherical harmonic descriptors;
   Zernike descriptors
ID LINEAR-TIME ALGORITHM; 3D
AB Symmetry has been playing an increasing role in 3D shape processing. Recently introduced planar reflective symmetry transform (PRST) has been found useful for canonical coordinate frame determination, shape matching, retrieval, and segmentation. Guided by the intuition that every imperfect symmetry is imperfect in its own way, we investigate the possibility of incorporating more information into symmetry transforms like PRST. As a step in this direction, the concept of augmented symmetry transform is introduced; we obtain a family of symmetry transforms indexed by a parameter. While the original PRST measures how much the symmetry is broken, the augmented PRST also gives some information about how it is broken. Several approaches to calculating the augmented transform are described. We demonstrate that the augmented transform is beneficial for shape retrieval.
C1 Purdue Univ, Dept Math, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Rustamov, RM (corresponding author), Purdue Univ, Dept Math, 150 N Univ St, W Lafayette, IN 47907 USA.
EM rustamov@math.purdue.edu
CR [Anonymous], P INT C COMP VIS ICC
   ATALLAH MJ, 1985, IEEE T COMPUT, V34, P663, DOI 10.1109/TC.1985.1676605
   CANTERAKIS N, 1999, SCAND C IM AN DANSK
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Hardle W., 2004, NONPARAMETRIC SEMIPA, DOI DOI 10.1007/978-3-642-17146-8
   HIGHNAM PT, 1986, INFORM PROCESS LETT, V22, P219, DOI 10.1016/0020-0190(86)90097-9
   Hirata T, 1996, INFORM PROCESS LETT, V58, P129, DOI 10.1016/0020-0190(96)00049-X
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   JIANG X, 1991, WORKSH COMP GEOM, P113
   Kazhdan M, 2004, ALGORITHMICA, V38, P201, DOI 10.1007/s00453-003-1050-5
   KAZHDAN M, 2004, S GEOM PROC EUR ASS
   KAZHDAN M, 2003, S GEOM PROC EUR ASS
   Laga H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P75
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   MEYER M, 2002, P VIS MATH
   MINOVIC P, 1993, IEEE T PATTERN ANAL, V15, P507, DOI 10.1109/34.211472
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Mousa M., 2006, Journal of Graphics Tools, V11, P17
   Novoselov VV, 2003, GENE EXPR PATTERNS, V3, P225, DOI 10.1016/S1567-133X(02)00077-7
   Novotni M, 2004, COMPUT AIDED DESIGN, V36, P1047, DOI 10.1016/j.cad.2004.01.005
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   RUSINKIEWICZ S, 2008, TRIMESH2
   Scott D.W., 2015, Multivariate Density Estimation: Theory, Practice and Visualization, DOI 10.1002/9780470316849
   Shah MI, 2006, SIAM J MATRIX ANAL A, V28, P749, DOI 10.1137/050646676
   SHILANE P, 2004, P INT C SHAP MOD APP
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   SNEEUW N, 1994, GEOPHYS J INT, V118, P707, DOI 10.1111/j.1365-246X.1994.tb03995.x
   Sun CM, 1997, IEEE T PATTERN ANAL, V19, P164, DOI 10.1109/34.574800
   Wolter J., 1985, The Visual Computer, V1, P37, DOI DOI 10.1007/BF01901268
   Xu GL, 2006, INT J COMPUT GEOM AP, V16, P75, DOI 10.1142/S0218195906001938
   YEO BTT, COMPUTING SPHERICAL
   ZABRODSKY H, 1995, IEEE T PATTERN ANAL, V17, P1154, DOI 10.1109/34.476508
   ZABRODSKY H, ICPR C, P9
NR 34
TC 5
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2008
VL 24
IS 6
BP 423
EP 433
DI 10.1007/s00371-008-0225-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 299FP
UT WOS:000255741800005
DA 2024-07-18
ER

PT J
AU Busking, S
   Vilanova, A
   van Wijk, JJ
AF Busking, Stef
   Vilanova, Anna
   van Wijk, Jarke J.
TI Particle-based non-photorealistic volume visualization
SO VISUAL COMPUTER
LA English
DT Article
DE visualization; non-photorealistic rendering; volume rendering; particle
   systems
ID SHAPE
AB Non-photorealistic techniques are usually applied to produce stylistic renderings. In visualization, these techniques are often able to simplify data, producing clearer images than traditional visualization methods. We investigate the use of particle systems for visualizing volume datasets using non-photorealistic techniques. In our VolumeFlies framework, user-selectable rules affect particles to produce a variety of illustrative styles in a unified way. The techniques presented do not require the generation of explicit intermediary surfaces.
C1 [Busking, Stef] Delft Univ Technol, Fac Elect Engn Math & Comp Sci, Dept Mediamat, NL-2600 GA Delft, Netherlands.
   [Vilanova, Anna] Tech Univ Eindhoven, Dept Biomed Engn, Biomed Image Anal Grp, NL-5600 MB Eindhoven, Netherlands.
   [van Wijk, Jarke J.] Tech Univ Eindhoven, Dept Math, NL-5600 MB Eindhoven, Netherlands.
C3 Delft University of Technology; Eindhoven University of Technology;
   Eindhoven University of Technology
RP Busking, S (corresponding author), Delft Univ Technol, Fac Elect Engn Math & Comp Sci, Dept Mediamat, POB 5031, NL-2600 GA Delft, Netherlands.
EM S.Busking@tudelft.nl; A.Vilanova@tue.nl; J.J.v.Wijk@tue.nl
RI Vilanova, Anna/G-1752-2010
OI Vilanova, Anna/0000-0001-7273-4597
CR [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   BAER A, 2007, EUROVIS P EUR ASS AI, P235
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Bruckner S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P671
   BURNS M, 2005, SIGGRAPH 05 ACM SIGG, P512
   BUSKING S, 2006, THESIS TU EINDHOVEN
   Co CS, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2003.1238274
   CORNISH D, 2001, GRAPHICS INTERFACE 2, P151
   Deussen O, 2000, COMPUT GRAPH FORUM, V19, pC41, DOI 10.1111/1467-8659.00396
   Dong F, 2003, IEEE COMPUT GRAPH, V23, P44, DOI 10.1109/MCG.2003.1210864
   Ebert D, 2000, IEEE VISUAL, P195, DOI 10.1109/VISUAL.2000.885694
   Haeberli P., 1990, SIG GRAPH, P207
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Interrante V, 1997, IEEE T VIS COMPUT GR, V3, P98, DOI 10.1109/2945.597794
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   Lu AD, 2003, IEEE T VIS COMPUT GR, V9, P127, DOI 10.1109/TVCG.2003.1196001
   Meyer MD, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/SMI.2005.41
   Nagy Z, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P497
   PANG A, 1993, IEEE VIS 1993 C P, P283
   Pastor OM, 2004, PROCEEDINGS OF THE FIFTH MEXICAN INTERNATIONAL CONFERENCE IN COMPUTER SCIENCE (ENC 2004), P141, DOI 10.1109/ENC.2004.1342599
   PRAUN E, 2001, P SIGGRAPH 2001, P579
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   STRUIK DJ, 1988, LECTURES CLASSICAL D
   Su WY, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P114, DOI 10.1109/SMI.2005.2
   Treavett SMF, 2000, IEEE VISUAL, P203, DOI 10.1109/VISUAL.2000.885696
   Witkin A. P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P269, DOI 10.1145/192161.192227
   Yuan Xiaoru., 2004, Proceedings of Joint IEEE/EG Symposium on Visualization, P9
NR 29
TC 6
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 335
EP 346
DI 10.1007/s00371-007-0192-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200003
DA 2024-07-18
ER

PT J
AU Zhao, W
   Gao, SM
   Lin, HW
AF Zhao, Wei
   Gao, Shuming
   Lin, Hongwei
TI A robust hole-filling algorithm for triangular mesh
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE hole-filling; poisson equation; harmonic function; triangular mesh
AB This paper presents a novel hole-filling algorithm that can fill arbitrary holes in triangular mesh models. First, the advancing front mesh technique is used to cover the hole with newly created triangles. Next, the desirable normals of the new triangles are approximated using our desirable normal computing schemes. Finally, the three coordinates of every new vertex are re-positioned by solving the Poisson equation based on the desirable normals and the boundary vertices of the hole. Many experimental results and error evaluations are given to show the robustness and efficiency of the algorithm.
C1 Zhejiang Univ, State Key Lab CAD&CG, Han Zhou, Peoples R China.
C3 Zhejiang University
RP Gao, SM (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Han Zhou, Peoples R China.
EM zhaowei@cad.zju.edu.cn; smgao@cad.zju.edu.cn; hwlin@cad.zju.edu.cn
CR [Anonymous], P 28 ANN C COMP GRAP, DOI DOI 10.1145/383259.383266
   Chen CC, 2005, I S INTELL SIG PROC, P613
   Chen CY, 2005, COMPUT AIDED GEOM D, V22, P376, DOI 10.1016/j.cagd.2005.04.003
   CHEN CY, 2004, 12 INT C CENTR EUR C, P175
   Chui CK, 2000, COMPUT AIDED GEOM D, V17, P297, DOI 10.1016/S0167-8396(00)00005-4
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Dey T. K., 1992, Computer-Aided Geometric Design, V9, P457, DOI 10.1016/0167-8396(92)90044-P
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   GEORGE PL, 1994, INT J NUMER METH ENG, V37, P3605, DOI 10.1002/nme.1620372103
   Girod B., 2000, PRINCIPLES 3D IMAGE
   JOSHUA P, 2005, P EUR S GEOM PROC DU, P33
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Jun Y, 2005, COMPUT AIDED DESIGN, V37, P263, DOI 10.1016/j.cad.2004.06.012
   LEVIN A, FILLING N SIDE HOLE
   LIEPA P, 2003, P EUR S GEOM PROC GR
   O'Rourke J., 1999, Computational Geometry in C
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pfeifle R, 1996, PROC GRAPH INTERF, P186
   PINKALL U, 1993, COMPUT AIDED DESIGN, V25, P225
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
NR 22
TC 132
Z9 177
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 987
EP 997
DI 10.1007/s00371-007-0167-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400004
DA 2024-07-18
ER

PT J
AU Imagire, T
   Johan, H
   Tamura, N
   Nishita, T
AF Imagire, Takashi
   Johan, Henry
   Tamura, Naoki
   Nishita, Tomoyuki
TI Anti-aliased and real-time rendering of scenes with light scattering
   effects
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE volumetric object; real-time rendering; anti-aliasing; GPU
AB Recently, for real-time applications such as games, the rendering of scenes with light scattering effects in the presence of volumetric objects such as smoke, mist, etc., has gained much attention. Slice-based methods are well-known techniques for achieving fast rendering of these effects. However, for real-time applications, it is necessary to reduce the number of slice planes that are used. As a result, aliasing (striped patterns) can appear in the rendered images. In this paper, we propose a real-time rendering method for scenes containing volumetric objects that does not generate aliasing in the rendered images. When a scene consists of volumetric and polygonal objects, the proposed method also does not generate aliasing at the boundaries between the polygonal and the volumetric objects. Moreover, we are able to reduce aliasing at shadows inside a volumetric object that are cast by polygonal objects by interpolating the occlusion rates of light at several locations. The proposed method can be efficiently implemented on a GPU.
C1 Univ Tokyo, Dept Complex Sci & Engn, Kashiwa, Chiba 2778561, Japan.
   Namco Bandai Games Inc, Tokyo, Japan.
   Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
C3 University of Tokyo; Nanyang Technological University
RP Nishita, T (corresponding author), Univ Tokyo, Dept Complex Sci & Engn, 5-1-5 Kashiwa No Ha, Kashiwa, Chiba 2778561, Japan.
EM henryjohan@ntu.edu.sg; nis@is.s.u-tokyo.ac.jp
RI Johan, Henry/A-3707-2011
CR Brady M, 1997, VISUALIZATION '97 - PROCEEDINGS, P183, DOI 10.1109/VISUAL.1997.663878
   CULLIP TJ, 1994, TR93027 U N CAR
   DOBASHI Y., 2002, GRAPHICS HARDWARE, P99
   ENGEL K, 2001, EUR SIGGRAPH WORKSH, P9
   GUTHE S, 2002, SIGGRAPH EUR WORKSH, P119
   KAJIHARA Y, 2003, P COMP GRAPH INT 200, P930
   Keller A, 2001, SPRING EUROGRAP, P269
   KNISS J, 2002, IEEE VISUALIZATION, P168
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   LORENSEN WE, 1978, COMPUTER GRAPHICS P, P163
   Montrym J, 2005, IEEE MICRO, V25, P41, DOI 10.1109/MM.2005.37
   REZK-SALAMA C., 2000, EGSIGGRAPH WORKSHOP, P109, DOI DOI 10.1145/346876.348238
   TUY HK, 1984, IEEE COMPUT GRAPH, V4, P29, DOI 10.1109/MCG.1984.6429333
   Umenhoffer T, 2006, PROC GRAPH INTERF, P57
   WESTERMANN R, 1998, COMPUTER GRAPHICS, P291
   WESTOVER L, 1991, THESIS U N CAROLINA
   William D., 2006, P 2006 S INT 3D GRAP, P161, DOI DOI 10.1145/1111411.1111440
NR 18
TC 8
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 935
EP 944
DI 10.1007/s00371-007-0140-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600034
DA 2024-07-18
ER

PT J
AU Specht, M
   Lebrun, R
   Zollikofer, CPE
AF Specht, Matthias
   Lebrun, Renaud
   Zollikofer, Christoph P. E.
TI Visualizing shape transformation between chimpanzee and human braincases
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE scientific visualization; surface parameterization; morphing; brain;
   geometric morphometrics
ID PARAMETERIZATION
AB The quantitative comparison of the form of the braincase is a central issue in paleo anthropology (i.e., the study of human evolution based on fossil evidence). The major difficulty is that there are only few locations defining biological correspondence between individual braincases. In this paper, we use mesh parameterization techniques to tackle this problem. We propose a method to conformally parameterize the genus-0 surface of the braincase on the sphere and to calibrate the parameterization to match biological constraints. The resulting consistent parameterization gives detailed information about shape differences between the braincase of human and chimp. This opens up new perspectives for the quantitative comparison of "featureless" biological structures.
C1 Univ Zurich, Inst Anthropol, CH-8057 Zurich, Switzerland.
C3 University of Zurich
RP Specht, M (corresponding author), Univ Zurich, Inst Anthropol, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
EM specht@corebounce.org; renaud_lebrun@yahoo.fr; zolli@aim.unizh.ch
RI Rohlf, F J/A-8710-2008
OI Lebrun, Renaud/0000-0002-5819-2653
CR Alexa M, 2000, VISUAL COMPUT, V16, P26, DOI 10.1007/PL00007211
   Andresen PR, 2001, MED IMAGE ANAL, V5, P81, DOI 10.1016/S1361-8415(00)00036-0
   Asirvatham A, 2005, LECT NOTES COMPUT SC, V3515, P265
   ATTENE M, 2004, 062004 IMATIGECNR
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Bookstein FL., 1997, MED IMAGE ANAL, V1, P225, DOI 10.1016/S1361-8415(97)85012-8
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Glaunés J, 2004, J MATH IMAGING VIS, V20, P179, DOI 10.1023/B:JMIV.0000011326.88682.e5
   Gotsman C, 2003, ACM T GRAPHIC, V22, P358, DOI 10.1145/882262.882276
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   GU X, 2003, S GEOM PROC, P127
   Gu X., 2002, Commun. Inf. Syst., V2, P121, DOI DOI 10.4310/CIS.2002.V2.N2.A2
   Gu XF, 2004, IEEE T MED IMAGING, V23, P949, DOI 10.1109/TMI.2004.831226
   Gunz P, 2005, DEV PRIMATOL-PROG PR, P73, DOI 10.1007/0-387-27614-9_3
   Haker S, 2000, IEEE T VIS COMPUT GR, V6, P181, DOI 10.1109/2945.856998
   Jin M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P267, DOI 10.1109/VISUAL.2004.75
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Saba S, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P256, DOI 10.1109/SMI.2005.32
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Thompson DW., 1917, GROWTH FORM
   Wang YL, 2005, LECT NOTES COMPUT SC, V3750, P675, DOI 10.1007/11566489_83
   Zollikofer CPE, 2002, P ROY SOC B-BIOL SCI, V269, P801, DOI 10.1098/rspb.2002.1960
   ZOU G, 2006, INT C IM PROC ICIP 2
NR 28
TC 43
Z9 51
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 743
EP 751
DI 10.1007/s00371-007-0156-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600014
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Dornbusch, T
   Wernecke, P
   Diepenbrock, W
AF Dornbusch, Tino
   Wernecke, Peter
   Diepenbrock, Wulf
TI Description and visualization of graminaceous plants with an organ-based
   3D architectural model, exemplified for spring barley (<i>Hordeum
   vulgare</i> L.)
SO VISUAL COMPUTER
LA English
DT Article
DE plant architecture; virtual plants; 3D model; Matlab
ID COTTON PLANT; CANOPY; LIGHT; MORPHOLOGY; SYSTEM; LEAVES; WHEAT;
   PARAMETERIZATION; VARIETIES; PROFILES
AB Within the framework of functional-structural plant models (FSPMs) this paper presents a structural (architectural) plant model that describes the morphology of spring barley (Hordeum vulgare L.) plants. A set of equations is introduced and implemented in self-written Matlab(R) computer programs to calculate the surface shape of vegetative and generative plant organs. Organ surfaces are approximated as geometrical primitives, visualized as triangulated surface meshes. The output of the model is a set of triangles that can be associated with geometrical (e.g. area), topological (e.g. main stem) and physiological (e.g. chlorophyll content) attributes, depending on the measurement techniques applied. This information is a prerequisite for functional (process) models to compute, e.g., the radiation field or gas exchange of the respective canopy.
C1 Univ Halle Wittenberg, Inst Agr & Nutr Sci, D-06108 Halle, Germany.
C3 Martin Luther University Halle Wittenberg
RP Dornbusch, T (corresponding author), Univ Halle Wittenberg, Inst Agr & Nutr Sci, Ludwig Wucherer Str 2, D-06108 Halle, Germany.
EM tino.dornbusch@landw.uni-halle.de
RI Diepenbrock, Wulf/C-6920-2016
CR Bloomenthal J., 1990, GRAPHICS GEMS CHAPTE, P567
   BLUM A, 1985, J EXP BOT, V36, P432, DOI 10.1093/jxb/36.3.432
   BONHOMME R, 1978, PHOTOSYNTHETICA, V12, P193
   BORT J, 1994, AGRONOMIE, V2, P133
   Buck-Sorlin GH, 2000, AGRONOMIE, V20, P691, DOI 10.1051/agro:2000161
   Chelle M, 1998, ECOL MODEL, V111, P75, DOI 10.1016/S0304-3800(98)00100-8
   de Reffye P, 1997, CURR SCI INDIA, V73, P984
   de Reffye P., 1988, Computer Graphics, V22, P151, DOI 10.1145/378456.378505
   Dornbusch T, 2007, ECOL MODEL, V200, P119, DOI 10.1016/j.ecolmodel.2006.07.028
   Drouet JL, 2003, FIELD CROP RES, V83, P215, DOI 10.1016/S0378-4290(03)00070-4
   Elings A, 1999, PHYTOPATHOLOGY, V89, P789, DOI 10.1094/PHYTO.1999.89.9.789
   España M, 1998, AGRONOMIE, V18, P609, DOI 10.1051/agro:19981001
   Evers JB, 2005, NEW PHYTOL, V166, P801, DOI 10.1111/j.1469-8137.2005.01337.x
   Evers JB, 2007, ECOL MODEL, V200, P308, DOI 10.1016/j.ecolmodel.2006.07.042
   GALLAGHER JN, 1979, J EXP BOT, V30, P625, DOI 10.1093/jxb/30.4.625
   Godin C, 2005, NEW PHYTOL, V166, P705, DOI 10.1111/j.1469-8137.2005.01445.x
   Godin C., 1997, PLANTS ECOSYSTEMS AD, P63
   Hanan J, 1997, ENVIRON MODELL SOFTW, V12, P35, DOI 10.1016/S1364-8152(96)00040-0
   Harlan HV, 1920, J AGRIC RES, V19, P0431
   KLEPPER B, 1982, AGRON J, V74, P789, DOI 10.2134/agronj1982.00021962007400050005x
   LANG ARG, 1973, AGR METEOROL, V11, P37, DOI 10.1016/0002-1571(73)90049-6
   Lewis P, 1999, AGRONOMIE, V19, P185, DOI 10.1051/agro:19990302
   Lintermann B, 1999, IEEE COMPUT GRAPH, V19, P56, DOI 10.1109/38.736469
   MARSHALL B, 1980, J AGR RES, V31, P857
   McClelland CK, 1916, SCIENCE, V44, P578, DOI 10.1126/science.44.1138.578
   McMaster GS, 2005, J AGR SCI-CAMBRIDGE, V143, P137, DOI 10.1017/S0021859605005083
   Meier U., 1997, GROWTH STAGES MONO D
   MOORE KJ, 1995, CROP SCI, V35, P37, DOI 10.2135/cropsci1995.0011183X003500010007x
   MOULIA B, 1993, CROP STRUCTURE AND LIGHT MICROCLIMATE, P183
   Niklas K.J., 1992, ENG APPROACH PLANT F
   NIKLAS KJ, 1988, EVOLUTION, V42, P1, DOI 10.1111/j.1558-5646.1988.tb04103.x
   Pararajasingham S, 1996, CAN J PLANT SCI, V76, P43, DOI 10.4141/cjps96-008
   PREVOT L, 1991, AGRONOMIE, V11, P491, DOI 10.1051/agro:19910606
   Prusinkiewicz P, 2000, LECT NOTES COMPUT SC, V1779, P457
   Prusinkiewicz P, 1999, AGRONOMIE, V19, P211, DOI 10.1051/agro:19990303
   RAAB FE, 1979, IEEE T AERO ELEC SYS, V5, P709
   ROSS J, 1981, RADIATION REGIME ARC
   ROSS JK, 1988, REMOTE SENS ENVIRON, V24, P213, DOI 10.1016/0034-4257(88)90026-0
   SANDERSON JB, 1981, CAN J PLANT SCI, V61, P1009, DOI 10.4141/cjps81-151
   SINOQUET H, 1991, AGR FOREST METEOROL, V55, P233, DOI 10.1016/0168-1923(91)90064-W
   Sinoquet H, 1998, ANN BOT-LONDON, V82, P203, DOI 10.1006/anbo.1998.0665
   SKINNER RH, 1992, ANN BOT-LONDON, V70, P493, DOI 10.1093/oxfordjournals.aob.a088509
   SMITH GS, 1992, ANN BOT-LONDON, V70, P265, DOI 10.1093/oxfordjournals.aob.a088468
   Soler C, 2003, ACM T GRAPHIC, V22, P204, DOI 10.1145/636886.636890
   Stuppy WH, 2003, TRENDS PLANT SCI, V8, P2, DOI 10.1016/S1360-1385(02)00004-3
   Takenaka A, 1998, FUNCT ECOL, V12, P159, DOI 10.1046/j.1365-2435.1998.00171.x
   Tanaka T, 1998, AGR FOREST METEOROL, V91, P149, DOI 10.1016/S0168-1923(98)00081-1
   Watanabe T, 2005, ANN BOT-LONDON, V95, P1131, DOI 10.1093/aob/mci136
NR 48
TC 21
Z9 30
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 569
EP 581
DI 10.1007/s00371-007-0119-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200004
DA 2024-07-18
ER

PT J
AU McDonnell, KT
   Qin, H
AF McDonnell, Kevin T.
   Qin, Hong
TI A novel framework for physically based sculpting and animation of
   free-form solids
SO VISUAL COMPUTER
LA English
DT Article
DE physically based modeling; subdivision algorithms; virtual sculpting;
   animation
ID SUBDIVISION; DEFORMATIONS; MUSCLE
AB This paper presents a new, physically based model for performing finite element simulation of deformable objects in which all quantities - strain, stress, displacement, etc. - are computed entirely in local frames of reference. In our framework, subdivision solids with non-homogeneous material properties, such as mass and deformation distributions, can be defined throughout continuous, volumetric domains. This capability enables an animator or virtual sculptor to exert fine-level control over deforming objects and to define a wide variety of physical behaviors. Furthermore, since all quantities pertinent to physical simulation are computed locally, our model facilitates both large-scale and small-scale deformations, as well as rigid or near-rigid transformations. We demonstrate applications of our framework in animation and interactive sculpting and show that interactive simulation of non-trivial, volumetric shapes is possible with our methodologies.
C1 Dowling Coll, Dept Math & Comp Sci, Oakdale, NY 11769 USA.
   SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP McDonnell, KT (corresponding author), Dowling Coll, Dept Math & Comp Sci, Idle Hour Blvd, Oakdale, NY 11769 USA.
EM mcdonnek@dowling.edu; qin@cs.sunysb.edu
CR [Anonymous], 2001, P 2001 S INT 3D GRAP
   Bajaj C, 2002, VISUAL COMPUT, V18, P343, DOI 10.1007/s003710100150
   Bathe K.J., 1996, Finite Element Procedures
   Bertram M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P449, DOI 10.1109/VISUAL.2004.127
   Bertram M., 2002, Proceedings of the seventh ACM symposium on Solid modeling and applications, SMA '02, P72
   Capell S., 2002, P 2002 ACM SIGGRAPHE, P41
   Chang Yu-Sung, 2004, P 9 ACM S SOL MOD AP, P123
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   Faloutsos P, 1997, IEEE T VIS COMPUT GR, V3, P201, DOI 10.1109/2945.620488
   HAUTH M, 2003, P EUR SIGGR S COMP A, P17
   Irving G, 2006, GRAPH MODELS, V68, P66, DOI 10.1016/j.gmod.2005.03.007
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   Linsen L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P346, DOI 10.1109/PCCGA.2002.1167878
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   McDonnell KT, 2004, VISUAL COMPUT, V20, P418, DOI 10.1007/s00371-004-0246-2
   MOLINO N, 2003, P 12 INT MESH ROUNDT, P103
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   Ng-Thow-Hing V, 2002, PROC GRAPH INTERF, P107
   Pascucci V., 2000, PROC VOLUME VISUALIZ, P33
   Press W.H., 2003, NUMERICAL RECIPES C
   Qin H, 1998, IEEE T VIS COMPUT GR, V4, P215, DOI 10.1109/2945.722296
   Qin H, 1996, IEEE T VIS COMPUT GR, V2, P85, DOI 10.1109/2945.489389
   Teran J, 2005, ENG COMPUT-GERMANY, V21, P2, DOI 10.1007/s00366-005-0308-8
   Teran J, 2005, IEEE T VIS COMPUT GR, V11, P317, DOI 10.1109/TVCG.2005.42
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   WEILER K, 1986, THESIS RENSSELAER PO
NR 26
TC 6
Z9 7
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2007
VL 23
IS 4
BP 285
EP 296
DI 10.1007/s00371-007-0096-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 172ZW
UT WOS:000246844300007
DA 2024-07-18
ER

PT J
AU Chang, J
   Zhang, JJ
   You, LH
AF Chang, Jian
   Zhang, Jian J.
   You, L. H.
TI Physically-based deformations: copy and paste
SO VISUAL COMPUTER
LA English
DT Article
DE deformation; computer animation; physically-based modeling
AB In contrast with purely geometric approaches, physically-based deformation techniques usually afford greater realism in the animation of soft objects and characters, due to the consideration of the inherent physical properties of the materials. In this paper, we present a novel physically-based technique allowing versatile deformation effects to be created using simple copy and paste operations. The copy process involves the extraction of the deformation behaviour from a source object, and the paste operation applies it to a different object (target), resulting in the target object being deformed in a physically plausible manner. Our technique defines the deformation of an object as a linear combination of a set of carefully chosen fundamental solutions from classic mechanics, which separates the deformation data from the geometry. The deformation of the source object can be computed using any existing physically-based technique. To copy the deformation from an object is to formulate the weighting coefficients to the fundamental solutions. To paste the deformation to another object is to apply our deformation model to the target object with the derived weighting coefficients. Repeated copy and paste operations allow various local and global deformation effects to be created. In addition to visual realism, the main advantages of our technique compared with other existing physically-based methods are its capability to reuse the deformation data, computational efficiency and friendliness to animators.
C1 Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
   Bournemouth Univ, Bournemouth Med Sch, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University; Bournemouth University
RP Zhang, JJ (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
EM jchang@bmth.ac.uk; jzhang@bmth.ac.uk; lyou@bmth.ac.uk
OI Chang, Jian/0000-0003-4118-147X
CR Barr A. H., 1984, Computers & Graphics, V18, P21
   Bathe K.J., 1996, Finite Element Procedures
   Chang J, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P204, DOI 10.1109/TPCG.2004.1314472
   Chang J, 2004, COMPUT ANIMAT VIRT W, V15, P211, DOI 10.1002/cav.23
   CHEN DT, 1992, COMP GRAPH, V26, P89, DOI 10.1145/142920.134016
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   HIROTA G, 2002, THESIS U N CAROLINA
   KOCH R.M., 1996, P SIGGRAPH, P421
   Liu G.R., 2003, Mesh Free Methods: Moving beyond The Finite Element Method
   Miller G. S. P., 1988, Computer Graphics, V22, P169, DOI 10.1145/378456.378508
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Pauly M, 2005, ACM T GRAPHIC, V24, P957, DOI 10.1145/1073204.1073296
   Rappoport A., 1995, Proceedings. Third Symposium on Solid Modeling and Applications, P361, DOI 10.1145/218013.218086
   Saada A.S., 1993, ELASTICITY THEORY AP
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   TU X., 1994, P ACM SIGGRAPH 94, P43, DOI DOI 10.1145/192161.192170
NR 20
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2007
VL 23
IS 1
BP 71
EP 82
DI 10.1007/s00371-006-0086-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 113RB
UT WOS:000242613900007
DA 2024-07-18
ER

PT J
AU Nijholt, A
   Rienks, R
   Zwiers, J
   Reidsma, D
AF Nijholt, Anton
   Rienks, Rutger
   Zwiers, Job
   Reidsma, Dennis
TI Online and off-line visualization of meeting information and meeting
   support
SO VISUAL COMPUTER
LA English
DT Article
DE smart meeting room; multi-modal interaction; virtual meeting room;
   multi-modal corpus; corpus annotation; 3D reconstruction
AB In current meeting research we see modest attempts to visualize the information that has been obtained by either capturing and, probably more importantly, by interpreting the activities that take place during a meeting. The meetings being considered take place in smart meeting rooms. Cameras, microphones and other sensors capture meeting activities. Captured information can be stored and retrieved. Captured information can also be manipulated and in turn displayed on different media. We survey our research in this area, look at issues that deal with turn-taking and gaze behavior of meeting participants, influence and talkativeness, and virtual embodied representations of meeting participants. We stress that this information is interesting not only for real-time meeting support, but also for remote participants and off-line consultation of meeting information.
C1 Univ Twente, Human Media Interact Lab, NL-7500 AE Enschede, Netherlands.
C3 University of Twente
RP Nijholt, A (corresponding author), Univ Twente, Human Media Interact Lab, POB 217, NL-7500 AE Enschede, Netherlands.
EM aniholt@ewi.utwente.nl; rienks@ewi.utwente.nl; zwiers@ewi.utwente.nl;
   denissr@ewi.utwente.nl
OI Nijholt, Anton/0000-0002-5669-9290; Reidsma, Dennis/0000-0002-7503-573X
CR [Anonymous], 1995, ACM Transactions on Computer-Human Interaction (TOCHI), DOI DOI 10.1145/210079.210088
   Bales R.F., 2001, SOCIAL INTERACTION S
   Barthelmess P, 2005, J INTELL INF SYST, V25, P207, DOI 10.1007/s10844-005-0862-y
   Benford S., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P242
   Carletta J, 2003, BEHAV RES METH INS C, V35, P353, DOI 10.3758/BF03195511
   Deutscher M, 2004, LECT NOTES COMPUT SC, V3295, P267
   DIMICCO J, 2004, DOCT CONS P C HUM FA, P1041
   DiMicco J.M., 2006, C HUMAN FACTORS COMP, P706
   Frecon E., 1998, P ACM S VIRT REAL SO, P105, DOI [10.1145/293701.293715, DOI 10.1145/293701.293715]
   GREENHALGH C, 1995, P 4 EUR C COMP SUPP
   MAATMAN R, 2005, LNCS, V3661
   MASOODIAN M, 1996, THESIS U WAIKATO
   MCCOWAN I, 2003, LNCS, V2875
   McGrath J.E., 1984, GROUPS INTERACTION P
   Nijholt A, 2005, MULTIMODAL MULTIPART, P93
   NIJHOLT A, 2006, J HUMAN CTR SYST, V20, P202
   NIJHOLT A, 2005, DIGITAL APPL CULTURA, P285
   REIDSMA D, 2005, MEASURING BEHAV
   REIDSMA D, IN PRESS J HUMAN CTR
   REINKS R, 2005, LNCS, V3869
   Rienks R., 2005, Multimodal Multiparty Meeting Processing, Work-shop at the 7th International Conference on Multimodal Interfaces, P85
   Rienks RJ, 2006, P SOC INT DES SID200, P213
   TUROFF M, 1977, IEEE SPECTRUM, V14, P58, DOI 10.1109/MSPEC.1977.6367610
   VANWELBERGEN H, 2005, P INT, P203
   VERTEGAAL R, 1998, THESIS U TWENTE
   VOIDA S, 2002, UBICOMP 2002 WORKSH, P105
   WAIBEL A, 2004, P NIST ICASSP M REC
   ZHANG D, 2004, 2 IEEE WORKSH EV MIN
NR 28
TC 17
Z9 22
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2006
VL 22
IS 12
BP 965
EP 976
DI 10.1007/s00371-006-0041-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 120HE
UT WOS:000243074300003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cheng, YM
   Wang, CM
AF Cheng, Yu-Ming
   Wang, Chung-Ming
TI A high-capacity steganographic approach for 3D polygonal meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE steganography; spatial domain; representation domain
AB We present a high-capacity steganographic approach for three-dimensional (3D) polygonal meshes. We first use the representation information of a 3D model to embed messages. Our approach successfully combines both the spatial domain and the representation domain for steganography. In the spatial domain, every vertex of a 3D polygonal mesh can be represented by at least three bits using a modified multi-level embed procedure (MMLEP). In the representation domain, the representation order of vertices and polygons and even the topology information of polygons can be represented with an average of six bits per vertex using the proposed representation rearrangement procedure (RRP). Experimental results show that the proposed technique is efficient and secure, has high capacity and low distortion, and is robust against affine transformations. Our technique is a feasible alternative to other steganographic approaches.
C1 Natl Chung Hsing Univ, Inst Comp Sci, Taichung 40227, Taiwan.
C3 National Chung Hsing University
RP Wang, CM (corresponding author), Natl Chung Hsing Univ, Inst Comp Sci, Taichung 40227, Taiwan.
EM ssmtt.cym@msa.hinet.net; cmwang@cs.nchu.edu.tw
CR [Anonymous], 2003, Techniques and Applications of Digital Watermarking and Content Protection
   Arkin EM, 1996, VISUAL COMPUT, V12, P429
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   ASPERT N, 2002, SPIE 47 ANN M, P705
   Cachin C, 1998, LECT NOTES COMPUT SC, V1525, P306
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chow MM, 1997, VISUALIZATION '97 - PROCEEDINGS, P347, DOI 10.1109/VISUAL.1997.663902
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cotting D, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P233, DOI 10.1109/SMI.2004.1314510
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Evans F, 1996, IEEE VISUAL, P319, DOI 10.1109/VISUAL.1996.568125
   Johnson NF, 1998, COMPUTER, V31, P26, DOI 10.1109/MC.1998.4655281
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   KATZENBEISSER S, 2000, INFORMATION HIDING T
   Maret Y., 2004, P 2004 WORKSH MULT S, P68
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohbuchi R., 2001, Graphics Interface, P9
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   Rencher A., 2002, METHODS MULTIVARIATE
   WAGNER MG, 2000, GMP 00, P201
   Wang CM, 2005, COMPUT GRAPH FORUM, V24, P591, DOI 10.1111/j.1467-8659.2005.00884.x
   Xiang X., 1999, ACM Symposium on Interactive 3D Graphics, P71
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
   Zollner J., 1998, INT HID WORKSH P, P345
NR 29
TC 53
Z9 54
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 845
EP 855
DI 10.1007/s00371-006-0069-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000027
DA 2024-07-18
ER

PT J
AU Matsuyama, K
   Fujimoto, T
   Chiba, N
AF Matsuyama, Katsutsugu
   Fujimoto, Tadahiro
   Chiba, Norishige
TI Real-time animation of spark discharge
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE natural phenomena; real-time graphics; physically-based animation; spark
   discharge; lightning
ID DIGITAL CERAUNOSCOPE; SYNTHETIC THUNDER
AB We have developed a CG technique that creates real-time animations of spark discharges. The technique proposed in this paper has factors related to the generation of a discharge, such as electric charge distribution and boundary conditions in a virtual space as the input, and generates spark discharge shape patterns based on the electric field defined by the input information. An electric field is expressed by the Laplace equation. Our method efficiently obtains the numerical solution of the equation using the calculation technique of the conjugate gradient method implemented on GPU, and can cope with dynamic changes in input. It also produces discharge patterns in both two and three dimensions. In addition, an efficient pseudo-dimensional expansion technique is proposed in this paper, which uses multiple two-dimensional electric fields to generate three-dimensional discharge patterns.
C1 Future Univ Hakodate, Hakodate, Hokkaido 0418655, Japan.
   Iwate Univ, Morioka, Iwate 0208551, Japan.
C3 Future University Hakodate; Iwate University
RP Matsuyama, K (corresponding author), Future Univ Hakodate, 116-2 Kamedanakano Cho, Hakodate, Hokkaido 0418655, Japan.
EM kmatsu@fun.ac.jp; fujimoto@cis.iwate-u.ac.jp; nchiba@cis.iwate-u.ac.jp
CR [Anonymous], 2004, GPU GEMS
   Dobashi Y, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P390, DOI 10.1109/PCCGA.2001.962896
   Glassner A, 2000, IEEE COMPUT GRAPH, V20, P92, DOI 10.1109/38.844376
   Glassner A, 2000, IEEE COMPUT GRAPH, V20, P89, DOI 10.1109/38.824552
   HAYAMI T, 1996, FULL MYSTERIES SCI L
   Kim T, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P267, DOI 10.1109/PCCGA.2004.1348357
   KITAGAWA S, 2001, SCI LIGHTNING THUNDE
   KRUGER J, 2005, GPU GEMS, V2, P703
   Kruszewski P, 1999, COMPUT GRAPH-UK, V23, P287, DOI 10.1016/S0097-8493(99)00038-2
   Narasimhan SG, 2003, PROC CVPR IEEE, P665
   NIEMEYER L, 1984, PHYS REV LETT, V52, P1033, DOI 10.1103/PhysRevLett.52.1033
   Reed T., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques, P359
   Sosorbaram B, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P89, DOI 10.1109/CGI.2001.934662
   TAKAYASU H, 1985, PHYS REV LETT, V54, P1099, DOI 10.1103/PhysRevLett.54.1099
NR 14
TC 3
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 761
EP 771
DI 10.1007/s00371-006-0061-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000019
DA 2024-07-18
ER

PT J
AU Price, B
   Barrett, W
AF Price, Brian
   Barrett, William
TI Object-based vectorization for interactive image editing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE image-based rendering and modeling; interaction techniques; computer
   vision for graphics; non-photorealistic rendering
AB We present a technique for creating an editable vector graphic from an object in a raster image. Object selection is performed interactively in subsecond time by calling graph cut with each mouse movement. A renderable mesh is then computed automatically for the selected object and each of its subobjects by (1) generating a coarse object mesh; (2) performing recursive graph cut segmentation and hierarchical ordering of subobjects; (3) applying error-driven mesh refinement to each (sub)object. The fully layered object hierarchy compares favorably with current approaches and is computed in a few 10s of seconds, facilitating object-level editing without leaving holes.
C1 Brigham Young Univ, Provo, UT 84602 USA.
C3 Brigham Young University
RP Price, B (corresponding author), Brigham Young Univ, 3361 TMCB POB 26576, Provo, UT 84602 USA.
EM bprice@rivit.cs.byu.edu; barrett@cs.byu.edu
CR Ablameyko S, 2002, INT C PATT RECOG, P69, DOI 10.1109/ICPR.2002.1047797
   *AD SYST INC, 2005, CREAT SUIT 2 STREAML
   [Anonymous], P SIGGRAPH
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   BERTALMIO M, 2002, P ACM SIGGRAPH 2002, P882
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Mortensen E. N., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P452, DOI 10.1109/CVPR.1999.784720
   REESE LJ, 2002, P EUROGRAPHICS, V21, P714
   Rother C., 2004, ACM Transactions on Graphics (SIGGRAPH), V23, P309
   Yang L, 2004, INT C PATT RECOG, P303, DOI 10.1109/ICPR.2004.1334180
   Yu XH, 2001, IEEE COMPUT GRAPH, V21, P62, DOI 10.1109/38.920628
   ZOU J, 2000, P INT C PATT REC
NR 14
TC 30
Z9 42
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 661
EP 670
DI 10.1007/s00371-006-0051-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000009
DA 2024-07-18
ER

PT J
AU Xu, D
   Chen, W
   Zhang, HX
   Bao, HJ
AF Xu, Dong
   Chen, Wei
   Zhang, Hongxin
   Bao, Hujun
TI Multi-level differential surface representation based on local
   transformations
SO VISUAL COMPUTER
LA English
DT Article
DE multi-level editing; detail representation; surface reconstruction;
   large deformation; local transformations
AB One crucial issue of multi-resolution surface representations is how to effectively record and reconstruct geometric details among surface levels. Standard multi-resolution techniques encode details directly as local displacements in the vertices, and may produce unplausible results when the base level endures large deformations. In this paper we propose an alternative detail representation and reconstruction scheme, based on local transformations on a per-triangle basis. While more storage is required, recording details as local transformations favors global coupling of geometric details and allows for large-scale surface manipulations. By modeling the scale components of the surface modifications as a set of deforming factors, we achieved detail-preserving reconstruction results naturally under very large deformations. Comprehensive experimental results verify the efficiency and robustness of our approach.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Bao, HJ (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM xudong@cad.zju.edu.cn; chenwei@cad.zju.edu.cn; zhx@cad.zju.edu.cn;
   bao@cad.zju.edu.cn
RI Chen, Wei/AAR-9817-2020; Zhang, Hongxin/T-3714-2019
CR *AD CORP, 2004, AD MAN AD PHOT 7 0
   Alexa M, 2003, VISUAL COMPUT, V19, P105, DOI 10.1007/s00371-002-0180-0
   ALLIEZ P, 2002, P SIGGRAPH 02, P347
   [Anonymous], 1981, An Introduction to Continuum Mechanics
   Biermann H, 2002, ACM T GRAPHIC, V21, P312, DOI 10.1145/566570.566583
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Botsch M, 2003, COMPUT GRAPH FORUM, V22, P483, DOI 10.1111/1467-8659.00696
   BOTSCH M, 2004, P EUR S GEOM PROC, P189
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   FORSEY D, 1995, P ACM SIGGRAPH 1995, P134
   FORSEY DR, 1988, P ACM SIGGRAPH 1988
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kobbelt L, 1999, COMP GEOM-THEOR APPL, V14, P5, DOI 10.1016/S0925-7721(99)00032-2
   KOBBELT L, 1997, P 7 IMA C MATH SURF, P101
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Kraevoy V, 2003, ACM T GRAPHIC, V22, P326, DOI 10.1145/882262.882271
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lee AWF, 1999, COMP GRAPH, P343, DOI 10.1145/311535.311586
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Lipman Y, 2005, ACM T GRAPHIC, V24, P479, DOI 10.1145/1073204.1073217
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Schruder P., 1995, Proc. 22nd Ann. Conf. Comput. Graphics Interactive Techniques (SIGGRAPH'95), P161
   SHEFFER A, 2004, 2 INT S 3D DAT PROC
   SORKINE O, 2005, P EUR 2005 EUR ASS
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Toledo Sivan., 2003, Taucs: A library of sparse linear solvers
   WELCH W, 1994, P SIGGRAPH 94, P247
   XU D, 2005, P ACM SOL PHYS MOD 2
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   ZORIN D, 1997, P SIGGRAPH 97, P259
NR 45
TC 1
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 493
EP 505
DI 10.1007/s00371-006-0024-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, HS
   Joslin, C
   Di Giacomo, T
   Garchery, S
   Magnenat-Thalmann, N
AF Kim, HS
   Joslin, C
   Di Giacomo, T
   Garchery, S
   Magnenat-Thalmann, N
TI Device-based decision-making for adaptation of three-dimensional content
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2004)
CY JUN 16-19, 2004
CL Crete, GREECE
SP SCI
DE multi-resolution model; virtual human animation; content adaption;
   benchmarking; MPEG-21
AB The goal of this research was the creation of an adaptation mechanism for the delivery of three-dimensional content. The adaptation of content, for various network and terminal capabilities - as well as for different user preferences, is a key feature that needs to be investigated. Current state-of-the art research of the adaptation shows promising results for specific tasks and limited types of content, but is still not well-suited for massive heterogeneous environments. In this research, we present a method for transmitting adapted three-dimensional content to multiple target devices. This paper presents some theoretical and practical methods for adapting three-dimensional content, which includes shapes and animation. We also discuss practical details of the integration of our methods into MPEG-21 and MPEG-4 architectures.
C1 Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
C3 University of Geneva
RP Konkuk Univ, Seoul, South Korea.
EM hyung.kim@acm.org; chris_joslin@carleton.ca; Giacomo@miralab.unige.ch;
   Garchery@miralab.unige.ch; Thalmann@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960; Kim, HyungSeok/0000-0003-4816-2992
CR ADELSON EH, 1991, OPTICS PHOTONICS NEW, V2, P24
   AGGARWAL A, 2001, P 110 AUD ENG SOC CO
   AHN J, 2004, P COMP AN SOC AG CAS, P129
   AMIELH M, 2002, P 11 INT WWW C
   Amielh M, 2001, P 8 INT C MULT MOD A, P127
   [Anonymous], P ACM SIGGRAPH C AUG
   BERKA R, 1997, SPRING C COMP GRAPH, P69
   Carlson DA, 1997, PROC GRAPH INTERF, P1
   CHEN BY, 2002, P 7 INT C 3D WEB TEC, P35
   Cohen J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P119, DOI 10.1145/237170.237220
   Cohen J.D., 1998, P 25 ANN C COMP GRAP, P115
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   DEHAEMER MJ, 1991, COMPUT GRAPH, V15, P175, DOI 10.1016/0097-8493(91)90071-O
   DIGIACOMO T, 2004, COMPUT GRAPHICS, V28, P65
   DIGIACOMO T, 2001, P EUR WORKSH COMP AN, P65
   Distler HK, 2000, PERCEPTION, V29, P1423, DOI 10.1068/p3115
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   FOGEL E, 2001, P 6 INT C 3D WEB TEC, P35
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   GIANG T, 2000, P EUR, P71
   Granieri J. P., 1995, ACM Transactions on Modeling and Computer Simulation, V5, P222, DOI 10.1145/217853.217856
   HECKBERT P, 1997, ACM SIGGRAPH, V25
   HECKBERT PS, 1994, GRAPH INTER, P43
   Hoeting JA, 1999, STAT SCI, V14, P382, DOI 10.1214/ss/1009212519
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, P SIGGRAPH 97, P189, DOI DOI 10.1145/258734.258843
   Hutchinson D., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P31
   JOSLIN C, 2004, IEEE C MULT EXP ICME
   Kourtzi Z, 1999, PERCEPTION, V28, P49, DOI 10.1068/p2870
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Ponder M, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P96
   Soucy M, 1996, COMPUT VIS IMAGE UND, V63, P1, DOI 10.1006/cviu.1996.0001
   SPECVIEWPERF 7 1 1
NR 35
TC 9
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 332
EP 345
DI 10.1007/s00371-006-0009-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500006
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Miao, YW
   Liu, S
   Peng, QS
AF Xiao, CX
   Miao, YW
   Liu, S
   Peng, QS
TI A dynamic balanced flow for filtering point-sampled geometry
SO VISUAL COMPUTER
LA English
DT Article
DE covariance analysis; point-sampled geometry; filtering; anisotropic
   diffusion flow; dynamic balanced flow
AB 3D point data acquisition has become a practical approach for generating complex 3D shapes. Subsequent smoothing or denoising operations on these raw data sets are required before performing sophisticated modeling operations. Based on covariance analysis and constructed directional curvature, a new approach of anisotropic curvature flow is developed for filtering the point data set. By introducing a forcing term, a balanced flow equation is constructed, which allows the anisotropic diffusion flow to be restricted in the flow diffusion band of the original surface. Thus, the common problem of shape shrinkage that puzzles most current denoising approaches for point-sampled geometry is avoided. Applying dynamic balance techniques, the equation converges to the solution quickly with appealing physical interpretations. The algorithms operate directly on the discrete sample points, requiring no vertex connectivity information. They are shown to be computationally efficient, robust and simple to implement.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM cxxiao@cad.zju.edu.cn; miaoyw@cad.zju.edu.cn; liushu@cad.zju.edu.cn;
   peng@cad.zju.edu.cn
RI Miao, Yongwei/ABH-1238-2021
OI Miao, Yongwei/0000-0002-5479-9060
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Barcelos CAZ, 2003, IEEE T IMAGE PROCESS, V12, P751, DOI 10.1109/TIP.2003.814242
   Clarenz U, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P600, DOI 10.1109/CGI.2004.1309272
   CLARENZ U, 2004, EUR S POINT BAS GRAP
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Jones TR, 2004, IEEE COMPUT GRAPH, V24, P53, DOI 10.1109/MCG.2004.14
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   LANGE C, 2005, 0516 ZIB
   Ohtake Y, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P153
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   PAULY M, 2003, P EUR ACM SIGGRAPH S, P281
   Pauly Mark., 2002, Multiresolution modeling of pointsampled geometry
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   SCHALL O, 2005, EUR S POINT BAS GRAP, P71, DOI DOI 10.2312/SPBG/SPBG05/071-077
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   TAUBIN G, 2001, RC2213 IBM
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Weyrich T., 2004, SPBG 04 S POINT BASE, DOI [10.2312/SPBG/SPBG04/085-094, DOI 10.2312/SPBG/SPBG04/085-094]
   Xiao CX, 2004, COMPUT ANIMAT VIRT W, V15, P201, DOI 10.1002/cav.22
NR 26
TC 20
Z9 27
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2006
VL 22
IS 3
BP 210
EP 219
DI 10.1007/s00371-006-0377-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 028UP
UT WOS:000236514600006
DA 2024-07-18
ER

PT J
AU Pronost, N
   Dumont, G
   Berillon, G
   Nicolas, G
AF Pronost, N
   Dumont, G
   Berillon, G
   Nicolas, G
TI Morphological and stance interpolations in database for simulating
   bipedalism of virtual humans
SO VISUAL COMPUTER
LA English
DT Article
DE motion retargeting; biomechanics; bipedalism; morphological and
   multidimensional interpolations; virtual human
ID AUSTRALOPITHECUS-AFARENSIS; LOCOMOTION; WALKING; MOTION; POSTURE; ERECT
AB We present a computer tool for testing walk hypotheses for human beings. This tool aims to generate plausible walking movements according to anatomical knowledge. To this end, we introduce an interpolation method based, on one hand, on morphological data and, on the other hand, on stance hypotheses and on footprint hypotheses. We want to test these hypotheses for application to the reconstruction of early hominid walking. We interpolate from a specific representation of the movement-a characteristic relative displacement. First, we use a motion capture system to acquire real movements of a walk cycle, and we propose to represent them by using a generic parametric model. Thus, we create a database of movements. The interpolation process produces, thanks to this database, a retargeted motion adapted to the morphology of the considered targeted skeleton. The interpolation is done according to three main hypotheses. The first concerns the reference stance, the second the lateral spacing between the feet, and the third the length of the step. In the introduction, we refer to related work. Then we propose the two following points of our method: the 3D representation of our motion representation and the multidimensional interpolation method applied to this representation. The interpolation method addresses morphological adaptation, and the use of an inverse kinematics solver addresses the computation of skeleton movements. The self-coherent validation process aims to test the coherence of the proposed method. The results propose an application to a virtual skeleton of Lucy (Australopithecus afarensis A.L. 288-1) reconstructed from real data. Finally, the relevance of the method for anthropological investigations and for animation purposes is discussed and future work is discussed with respect to the limitations of the proposed method.
C1 Univ Rennes, IRISA, UMR 6074, F-35042 Rennes, France.
   Ecole Normale Super, IRISA, UMR 6074, F-35042 Rennes, France.
   CNRS, UPR 2147, F-75014 Paris, France.
   Univ Rennes, Lab Physiol & Biomecan Exercice Musculaire, F-35044 Rennes, France.
C3 Universite de Rennes; Ecole Normale Superieure de Rennes (ENS Rennes);
   Universite de Rennes; Centre National de la Recherche Scientifique
   (CNRS); CNRS - Institute of Ecology & Environment (INEE); Universite de
   Rennes
RP Univ Rennes, IRISA, UMR 6074, F-35042 Rennes, France.
EM Nicolas.Pronost@irisa.fr; Georges.Dumont@irisa.fr;
   berillon@ivry.cnrs.fr; guillaume.nicolas@uhb.fr
RI DUMONT, Georges/K-8173-2013
OI DUMONT, Georges/0000-0002-0709-0921; PRONOST,
   NICOLAS/0000-0003-4499-509X
CR Boulic R, 1996, COMPUT GRAPH, V20, P693, DOI 10.1016/S0097-8493(96)00043-X
   Boulic R., 1992, Computer Graphics Forum, V11, P189, DOI 10.1111/1467-8659.1140189
   Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b
   Bruderlin A., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P97, DOI 10.1145/218380.218421
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   COONS S, 1967, TR41
   Crompton RH, 1998, J HUM EVOL, V35, P55, DOI 10.1006/jhev.1998.0222
   DELOISON Y, 1992, CR ACAD SCI II, V315, P103
   GLEICHER M, 1998, SIGGRAPH98
   Grasso R, 2000, J NEUROPHYSIOL, V83, P288, DOI 10.1152/jn.2000.83.1.288
   GROCHOV K, 2004, P SIGGRAPH
   HODGINS J, 1996, IEEE INT C ROB AUT
   Komura T, 2000, VISUAL COMPUT, V16, P254, DOI 10.1007/s003719900065
   Kovar L., 2002, ACM SIGGR S COMP AN
   KOVAT L, 2004, ACM T GRAPHICS ACM S, V23
   Lee J, 1999, COMP GRAPH, P39
   MARCHAL F, 2001, 14 C IUISSP LIEG
   Ménardais S, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P156, DOI 10.1109/CGI.2004.1309206
   MENARDAIS S, 2004, ACM SIGGRAPH EUROGRA, P325
   MONZANI JS, 2000, EUROGRAPHICS 2000
   Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2
   MULTON F, 1998, THESIS U RENNES, V1
   NICOLAS G, 2004, CASA 2004 C, P103
   POPOVIC Z, 1999, SIGGRAPH 99
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   SAVENKO A, 2002, USING MOTION ANAL TE
   Ward CV, 2002, YEARB PHYS ANTHROPOL, V45, P185, DOI 10.1002/ajpa.10185
   Whittle MW., 1991, GAIT ANAL INTRO
   WIKKIN A, 1995, P SIGGRAPH 95, P105
NR 29
TC 6
Z9 6
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2006
VL 22
IS 1
BP 4
EP 13
DI 10.1007/s00371-005-0350-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 991NT
UT WOS:000233821400002
DA 2024-07-18
ER

PT J
AU Wang, CCL
   Tang, K
AF Wang, CCL
   Tang, K
TI Achieving developability of a polygonal surface by minimum deformation:
   a study of global and local optimization approaches
SO VISUAL COMPUTER
LA English
DT Article
DE developable surface; polygonal mesh; assembled patches; deformation;
   optimization
ID PLANAR DEVELOPMENTS; APPROXIMATION
AB Surface developability is required in a variety of applications in product design, such as clothing, ship hulls, automobile parts, etc. However, most current geometric modeling systems using polygonal surfaces ignore this important intrinsic geometric property. This paper investigates the problem of how to minimally deform a polygonal surface to attain developability, or the so-called developability-by-deformation problem. In our study, this problem is first formulated as a global constrained optimization problem and a penalty-function-based numerical solution is proposed for solving this global optimization problem. Next, as an alternative to the global optimization approach, which usually requires lengthy computing time, we present an iterative solution based on a local optimization criterion that achieves near real-time computing speed.
C1 Chinese Univ Hong Kong, Dept Automat & Comp Aided Engn, Hong Kong, Hong Kong, Peoples R China.
   Hong Kong Univ Sci & Technol, Dept Mech Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Hong Kong University of Science &
   Technology
RP Chinese Univ Hong Kong, Dept Automat & Comp Aided Engn, Hong Kong, Hong Kong, Peoples R China.
EM cwang@acae.cuhk.edu.hk; mektang@ust.hk
RI Tang, Kai/ABA-9642-2021; Wang, Charlie C. L./B-3730-2010
OI Tang, Kai/0000-0002-5184-2086; Wang, Charlie C. L./0000-0003-4406-8480
CR [Anonymous], 1993, GM RES PUBLICATION
   Aono M, 1996, IEEE COMPUT GRAPH, V16, P60, DOI 10.1109/38.536276
   Aono M, 2001, COMPUT AIDED DESIGN, V33, P989, DOI 10.1016/S0010-4485(00)00135-4
   Aumann G., 1991, Computer-Aided Geometric Design, V8, P409, DOI 10.1016/0167-8396(91)90014-3
   Azariadis PN, 2000, COMPUT GRAPH-UK, V24, P539, DOI 10.1016/S0097-8493(00)00057-1
   Azariadis PN, 2002, COMPUT IND, V47, P357, DOI 10.1016/S0166-3615(01)00155-5
   Azariadis PN, 2001, COMPUT AIDED DESIGN, V33, P581, DOI 10.1016/S0010-4485(00)00102-0
   Belegundu A., 1999, Optimization Concepts and Applications in Engineering
   BLOOMENTHAL J, 1990, 1990 S INT 3D GRAPH, P109
   CALLADINE CR, 1986, P MATH SURF OXF UK C, P179
   Chen HY, 1999, GRAPH MODEL IM PROC, V61, P110, DOI 10.1006/gmip.1999.0487
   Chu CH, 2002, COMPUT AIDED DESIGN, V34, P511, DOI 10.1016/S0010-4485(01)00122-1
   DESBRUN M, 1999, SIGGRAPH 99 P NEW YO, P409
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   GANAPATHY S, 1982, COMPUT GRAPHICS ACM, V16, P69, DOI DOI 10.1145/965145.801264
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoschek J, 1995, MATHEMATICAL METHODS FOR CURVES AND SURFACES, P255
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   KHODAKOVSKY A., 1999, Proceedings of the 1999 ACM Symposium on Solid and Physical Modeling, P203
   Kobbelt L., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P105, DOI 10.1145/280814.280831
   Maekawa T, 1998, J MECH DESIGN, V120, P453, DOI 10.1115/1.2829173
   Markosian L, 1999, COMP GRAPH, P393, DOI 10.1145/311535.311595
   McCartney J, 1999, COMPUT AIDED DESIGN, V31, P249, DOI 10.1016/S0010-4485(99)00025-1
   MORETON HP, 1992, COMP GRAPH, V26, P167, DOI 10.1145/142920.134035
   PARIDA L, 1993, COMPUT AIDED DESIGN, V25, P225, DOI 10.1016/0010-4485(93)90053-Q
   PARK FC, 2002, ASME T, V124, P602
   Pottmann H, 1999, COMPUT AIDED GEOM D, V16, P539, DOI 10.1016/S0167-8396(99)00012-6
   Randrup T, 1998, COMPUT AIDED DESIGN, V30, P807, DOI 10.1016/S0010-4485(98)00038-4
   REDONT P, 1989, COMPUT AIDED DESIGN, V21, P13, DOI 10.1016/0010-4485(89)90111-5
   Rossl C., 2000, P ACM SIGGRAPH COURS
   Schneider R, 2001, COMPUT AIDED GEOM D, V18, P359, DOI 10.1016/S0167-8396(01)00036-X
   Schneider R., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P251, DOI 10.1109/GMAP.2000.838257
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Sheffer A, 2002, ACM T GRAPHIC, V21, P874, DOI 10.1145/571647.571651
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   SHEFFER A, 2002, SMI 2002 INT C SHAP, P261
   SHIMADA T, 1991, COMPUT AIDED DESIGN, V23, P153, DOI 10.1016/0010-4485(91)90006-I
   Singh K., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P405, DOI 10.1145/280814.280946
   Suzuki H, 2000, VISUAL COMPUT, V16, P159, DOI 10.1007/s003710050205
   TAUBIN G, 1995, SIGGRAPH 95 C P, P351, DOI [DOI 10.1145/218380.218473, 10.1145/218380.218473]
   Wang CCL, 2002, COMPUT AIDED DESIGN, V34, P823, DOI 10.1016/S0010-4485(01)00150-6
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
   Yu GX, 2000, COMPUT AIDED GEOM D, V17, P545, DOI 10.1016/S0167-8396(00)00017-0
   Zeleznik R. C., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P163, DOI 10.1145/237170.237238
   Zorin D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P259, DOI 10.1145/258734.258863
NR 46
TC 38
Z9 45
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 521
EP 539
DI 10.1007/s00371-004-0256-0
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600002
DA 2024-07-18
ER

PT J
AU Vergeest, JSM
   Spanjaard, S
   Song, Y
AF Vergeest, JSM
   Spanjaard, S
   Song, Y
TI Directed mean Hausdorff distance of parameterized freeform shapes in 3D:
   a case study
SO VISUAL COMPUTER
LA English
DT Article
DE 3D pattern matching; reverse engineering of shape; distance measure for
   3D shapes
ID RECOGNITION
AB Shape matching in 3D is a process underlying several applications, including engineering, medicine, and animation. Searching, recognition, and matching of shape patterns directly in 3D model-space is far from a mature technique. In this paper we report an investigation of the computation of shape similarity based on Hausdorff-like measures. We have assessed a number of properties of the directed Hausdorff distance between 3D shapes, namely accuracy, robustness, and computational complexity of the algorithms. The driving application here was the reuse of digitized shapes in geometric modeling. The shapes involved in the analysis were from sampled physical object surfaces, or they were synthetic shapes. To support the application it was necessary to extend the search space beyond rigid and affine transformation spaces; a number of deformation parameters (intrinsic shape parameters) needed to be introduced. The sensitivity of the Hausdorff distance to pose parameters and to intrinsic shape parameters was investigated. Shape parameter estimation turned out to be feasible when an appropriate fitting strategy was selected. Besides a short straight ridge, ridges developing along a 3D spine can also be successfully registered. Finally, it is demonstrated how the performance of the algorithms is improved by a 3D binning technique.
C1 Delft Univ Technol, NL-2628 CE Delft, Netherlands.
C3 Delft University of Technology
RP Delft Univ Technol, Landbergstr 15, NL-2628 CE Delft, Netherlands.
EM j.s.m.vergeest@io.tudelft.nl; s.spanjaard@its.tudelft.nl;
   y.song@io.tudelft.nl
RI Song, Yu/M-6102-2017
OI Song, Yu/0000-0002-9542-1312
CR Barequet G, 1997, IEEE T PATTERN ANAL, V19, P929, DOI 10.1109/34.615444
   BROERE A, 2000, FEATURE RECOGNITION
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Kwon OK, 2001, PATTERN RECOGN, V34, P2005, DOI 10.1016/S0031-3203(00)00132-1
   Li CL, 2000, COMPUT GRAPH-UK, V24, P569, DOI 10.1016/S0097-8493(00)00059-5
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   MEIRITZ B, 1999, C REV ENG 3D SCANN S
   Piegl LA, 2000, VISUAL COMPUT, V16, P386, DOI 10.1007/PL00013393
   PROKOP RJ, 1992, CVGIP-GRAPH MODEL IM, V54, P438, DOI 10.1016/1049-9652(92)90027-U
   SCLAROFF S, 1995, IEEE T PATTERN ANAL, V17, P545, DOI 10.1109/34.387502
   SPANJAARD S, 2000, DOCUMENTATION RIDGE
   SPANJAARD S, 2001, P 2001 COMP INF ENG
   TANGELDER JWH, 1999, INT ARCH PHOTOG 5W11, V32, P23
   Thompson WB, 1999, IEEE T ROBOTIC AUTOM, V15, P57, DOI 10.1109/70.744602
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Vergeest JSM, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P20, DOI 10.1109/SMA.2001.923371
   VERGEEST JSM, 2001, P 2001 DES THEOR MET
   VERGEEST JSM, 2000, P ASME DES ENG TECHN
NR 19
TC 11
Z9 11
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 480
EP 492
DI 10.1007/s00371-003-0213-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600005
DA 2024-07-18
ER

PT J
AU Hirota, G
   Fisher, S
   State, A
AF Hirota, G
   Fisher, S
   State, A
TI An improved finite-element contact model for anatomical simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Animation Conference (CA2001)
CY NOV, 2001
CL SEOUL NATL UNIV, SEOUL, SOUTH KOREA
HO SEOUL NATL UNIV
DE contact problem; physically based modeling; deformation; finite-element
   method; human-body simulation
ID ALGORITHM; IMPACT
AB This work focuses on the simulation of mechanical contact between nonlinearly elastic objects, such as the components of the human body. In traditional methods, contact forces are often defined as discontinuous functions of deformations, which leads to poor convergence characteristics and high-frequency noises. We introduce a novel penalty method for finite-element simulation based on the concept of material depth, which is the distance between a particle inside an object and the object's boundary. By linearly interpolating precomputed material depths at node points, contact forces can be analytically integrated over contact surfaces without raising the computational cost. The continuity achieved by this formulation reduces oscillation and artificial acceleration, resulting in a more reliable simulation algorithm.
C1 Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Hirota, G (corresponding author), Univ N Carolina, Dept Comp Sci, Campus Box 3175,Sitterson Hall, Chapel Hill, NC 27599 USA.
CR ANDERSON P, 1999, USING MAYA
   [Anonymous], SAND922141UC705
   [Anonymous], P 25 ANN C COMP GRAP
   BARAFF D, 1992, P 19 ANN C COMP GRAP
   BELYTSCHKO T, 1991, INT J NUMER METH ENG, V31, P547, DOI 10.1002/nme.1620310309
   BELYTSCHKO T, 1992, COMPUTING METHODS AP
   BENSON DJ, 1990, COMPUT METHOD APPL M, V78, P141, DOI 10.1016/0045-7825(90)90098-7
   Bonet J., 1997, Nonlinear continuum mechanics for finite element analysis
   Bridson R., 2002, P 29 ANN C COMP GRAP
   Brown K, 2000, COMPUT METHOD APPL M, V184, P375, DOI 10.1016/S0045-7825(99)00235-2
   Carstensen C, 1999, SIAM J SCI COMPUT, V20, P1605, DOI 10.1137/S1064827595295350
   GOURRET JP, 1989, P 16 ANN C COMP GRAP
   HEINSTEIN MW, 1993, CONTACT MECH COMPUTA
   HIROTA G, 2001, SIGGRAPH 2001 C ABST
   HIROTA G, 2001, COMP AN 2001 SEOUL K
   HIROTA G, 2002, THESIS U N CAROLINA
   LIN JI, 1998, DYNA3D NONLINEAR EXP
   MALCOLM LL, 1976, THESIS U CALIFORNIA
   MARCUM DL, 1995, AIAA J, V33, P1619, DOI 10.2514/3.12701
   Miller K, 2000, J BIOMECH, V33, P1369, DOI 10.1016/S0021-9290(00)00120-2
   Moro-oka T, 1999, J BIOMECH, V32, P1131, DOI 10.1016/S0021-9290(99)00084-6
   OBRIEN JF, 1999, P 26 ANN C COMP0 GRA
   PADMANABHAN V, 1998, SERIES STABILITY V B, V14
   PAPADOPOULOS P, 1993, COMPUT STRUCT, V46, P1107, DOI 10.1016/0045-7949(93)90096-V
   PUSO MA, 2001, 3D CONTACT SMOOTHING
   Richtmyer Robert D, 1994, Difference methods for initial-value problems
   Spitzer V, 1996, J AM MED INFORM ASSN, V3, P118, DOI 10.1136/jamia.1996.96236280
   TERZOPOULOS D, 1987, ELASTICALLY DEFORMA
   Zhuang Y., 2000, THESIS U CALIFORNIA
NR 29
TC 21
Z9 22
U1 1
U2 4
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2003
VL 19
IS 5
BP 291
EP 309
DI 10.1007/s00371-002-0188-5
PG 19
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 726KP
UT WOS:000185598500003
DA 2024-07-18
ER

PT J
AU Wang, JH
   Chua, TS
AF Wang, JH
   Chua, TS
TI A cinematic-based framework for scene boundary detection in video
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Multimedia Modeling Conference (MMM2001)
CY NOV, 2001
CL DUTCH NATL CTR MATH & COMP SCI, AMSTERDAM, NETHERLANDS
HO DUTCH NATL CTR MATH & COMP SCI
DE cinematic model; scene detection; video retrieval
ID RETRIEVAL
AB Most current video retrieval systems use shots as the basis for information organization and access. In cinematography, scene is the basic story unit that the directors use to compose and convey their ideas. This paper proposes a framework based on the concept of continuity to analyze video contents and extract scene boundaries. Starting from a set of shots, the framework successively applies the concept of visual, position, camera focal distance, motion, audio and semantic continuity to group shots that exhibit some form of continuity into scenes. The framework helps to explain the principles and the heuristics behind most cinematic rules. The idea is tested using the first three levels of continuity to extract the scenes defined using the most common cinematic rules. The method has been found to be effective.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
C3 National University of Singapore
RP Wang, JH (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
CR [Anonymous], P INT C MULT COMP SY
   [Anonymous], GRAMMAR SHOT
   [Anonymous], 1991, Grammar of the film language
   AOKI H, 1996, P 4 ACM INT C MULT B
   ARMAN F, 1994, P 2 ACM INT C MULT S
   Beaver Frank E., 1994, Dictionary of Film Terms
   CHEN LP, 2001, P 2001 IEEE INT C MU
   Chua TS, 2002, VISUAL COMPUT, V18, P121, DOI 10.1007/s003710100137
   CHUA TS, 1995, ACM T INFORM SYST, V13, P373, DOI 10.1145/211430.211431
   CHUA TS, 2000, P 3 INT WORKKSH ADV
   DAVENPORT G, 1991, IEEE COMPUT GRAPH, V11, P67, DOI 10.1109/38.126883
   Eisenstein S., 1968, FILM SENSE
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HARI S, 2000, P ACM MULT 2000 LOS
   HEARST M, 1993, P 16 ANN INT ACM SIG
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   KENDER JR, 1998, IEEE COMP SOC C COMP
   KOH CK, 2000, DETECTION SEGMENTATI
   MERCER J, 1971, INTRO CINEMATOGRAPHY
   NNOMA T, 1992, CREATING ANIMATING V
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   RUI Y, 1998, IEEE INT C MULT COMP
   SHAN MK, 1998, INT WORKSH MULT DAT
   WANG J, 2001, P INT C MULT MOD AMS
   YEUNG M, 1995, 2 INT C IM PROC WASH
   YOSHITAKA A, 1997, P IEEE S VIS LANG LO
   ZHONG D, 1996, STORAGE RETRIEVAL ST, V4
NR 27
TC 4
Z9 5
U1 0
U2 0
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2003
VL 19
IS 5
BP 329
EP 341
DI 10.1007/s00371-002-0184-9
PG 13
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 726KP
UT WOS:000185598500006
DA 2024-07-18
ER

PT J
AU Hornus, S
   Angelidis, A
   Cani, MP
AF Hornus, S
   Angelidis, A
   Cani, MP
TI Implicit modeling using subdivision curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on Shape Modeling and Applications (SMI
   2001)
CY MAY 07-11, 2001
CL GENOA, ITALY
SP Consiglio Nazl Ricerche
DE computational geometry; object modeling; convolution surfaces; levels of
   detail; implicit surfaces
ID SURFACES
AB To remain an attractive model, skeleton-based implicit surfaces have to allow the design and display of shapes at interactive rates. This paper focuses on surfaces whose skeletons are graphs of interconnected curves. We present subdivision-curve primitives that rely on convolution for generating bulge-free and crease-free implicit surfaces. These surfaces are efficiently yet correctly displayed using local meshes around each curve that locally overlap in blending regions. Subdivision-curve primitives offer a practical solution to the unwanted-blending problem that ensures C-1 continuity everywhere. Moreover, they can be used to generate representations at different levels of detail, enabling the interactive display of at least a coarse version of the objects, whatever the performance of the workstation. We also present a practical solution to the unwanted-blending problem, used to avoid blending between parts of the surface that do not correspond to neighboring skeletal elements.
C1 INRIA Rhone Alpes, iMAGIS GRAVIR, F-38330 Montbonnot St Martin, France.
RP INRIA Rhone Alpes, iMAGIS GRAVIR, 655 Ave Europe, F-38330 Montbonnot St Martin, France.
EM Samuel.Hornus@imag.fr; Alexis.Angelidi@imag.fr; Marie-Paule.Cani@imag.fr
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   Attali D, 1997, COMPUT VIS IMAGE UND, V67, P261, DOI 10.1006/cviu.1997.0536
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bloomenthal J., 1990, Computer Graphics, V24, P109, DOI 10.1145/91394.91427
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   BLOOMENTHAL J, 1995, THESIS U CALGARY CAL
   CANI MP, 2001, SHAPE MODELLING INT
   CaniGascuel MP, 1997, IEEE T VIS COMPUT GR, V3, P39, DOI 10.1109/2945.582343
   CANIGASCUEL MP, 1998, GRAPH INT GI 98 P VA
   Desbrun M, 1996, COMPUT GRAPH FORUM, V15, P319, DOI 10.1111/1467-8659.1550319
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   Ferley E, 1997, COMPUT GRAPH FORUM, V16, P283, DOI 10.1111/1467-8659.00195
   FRISKEN S, 2000, SIGGRAPH 2000 C P
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   GRISONI L, 1998, IMPL SURF 98 P SEATT, P1
   Nishimura H., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P718
   OPALACH A, 1993, 4 EUR WORKSH AN SIM, P233
   Perry RN, 2001, COMP GRAPH, P47, DOI 10.1145/383259.383264
   Shen J., 1995, PROC EUROGRAPHICS IM, P187
   Sherstyuk A, 1999, VISUAL COMPUT, V15, P171, DOI 10.1007/s003710050170
   Sherstyuk A, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P56, DOI 10.1109/SMA.1999.749324
   STOLLNITZ E.J., 1996, WAVELETS COMPUTER GR
   Velho L, 1996, COMPUT GRAPH FORUM, V15, P327, DOI 10.1111/1467-8659.1550327
   Verroust A, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P194, DOI 10.1109/SMA.1999.749340
   Witkin A.P., 1994, P 21 ANN C COMPUTER, P269, DOI [10.1145/1198555.1198656, DOI 10.1145/1198555.1198656, DOI 10.1145/192161.192227]
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 26
TC 20
Z9 23
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 94
EP 104
DI 10.1007/s00371-002-0179-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 691AU
UT WOS:000183583300003
DA 2024-07-18
ER

PT J
AU Dehshibi, MM
   Ashtari-Majlan, M
   Adhane, G
   Masip, D
AF Dehshibi, Mohammad Mahdi
   Ashtari-Majlan, Mona
   Adhane, Gereziher
   Masip, David
TI ADVISE: ADaptive feature relevance and VISual Explanations for
   convolutional neural networks
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Convolutional neural network; Deep learning; EXplainable AI
ID CLASSIFICATION
AB To equip convolutional neural networks (CNNs) with explainability, it is essential to interpret how opaque models make specific decisions, understand what causes the errors, improve the architecture design, and identify unethical biases in the classifiers. This paper introduces ADVISE, a new explainability method that quantifies and leverages the relevance of each unit of the feature map to provide better visual explanations. To this end, we propose using adaptive bandwidth kernel density estimation to assign a relevance score to each unit of the feature map with respect to the predicted class. We also propose an evaluation protocol to quantitatively assess the visual explainability of CNN models. Our extensive evaluation of ADVISE in image classification tasks using pretrained AlexNet, VGG16, ResNet50, and Xception models on ImageNet shows that our method outperforms other visual explainable methods in quantifying feature-relevance and visual explainability while maintaining competitive time complexity. Our experiments further show that ADVISE fulfils the sensitivity and implementation independence axioms while passing the sanity checks. The implementation is accessible for reproducibility purposes on https://github.com/dehshibi/ADVISE.
C1 [Dehshibi, Mohammad Mahdi] Univ Carlos III Madrid, Comp Sci & Engn Dept, Ave Univ 30 Leganes, Madrid 28911, Spain.
   [Dehshibi, Mohammad Mahdi] Univ West England, Unconvent Comp Lab, Coldharbour Lane, Bristol BS16 1QY, England.
   [Ashtari-Majlan, Mona; Adhane, Gereziher; Masip, David] Univ Oberta Catalunya, Dept Comp Sci, Rambla Poblenou 156, Barcelona 08018, Spain.
C3 Universidad Carlos III de Madrid; UOC Universitat Oberta de Catalunya
RP Dehshibi, MM (corresponding author), Univ Carlos III Madrid, Comp Sci & Engn Dept, Ave Univ 30 Leganes, Madrid 28911, Spain.; Dehshibi, MM (corresponding author), Univ West England, Unconvent Comp Lab, Coldharbour Lane, Bristol BS16 1QY, England.
EM mohammad.dehshibi@yahoo.com; ashtari.mona@gmail.com; gadhane@uoc.edu;
   dmasipr@uoc.edu
RI Dehshibi, Mohammad Mahdi/S-9946-2017
OI Dehshibi, Mohammad Mahdi/0000-0001-8112-5419; Ashtari-Majlan,
   Mona/0000-0003-3207-0129; Masip Rodo, David/0000-0001-7898-1847
FU European Research Council (ERC) [101002711]; Spanish Ministry of
   Science, Innovation (FEDER funds) [PID2022-138721NB-I00]; European
   Research Council (ERC) [101002711] Funding Source: European Research
   Council (ERC)
FX This work is partially supported by funding from the European Research
   Council (ERC) under the European Union's Horizon2020 research and
   innovation programme, with grant agreement No.101002711, and by
   PID2022-138721NB-I00 grant from the Spanish Ministry of Science,
   Innovation (FEDER funds).
CR Adebayo J. etal, 2018, P ADV NEUR INF PROC, P1
   Adhane G, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON OMNI-LAYER INTELLIGENT SYSTEMS (IEEE COINS 2022), P413, DOI 10.1109/COINS54846.2022.9854971
   Adhane G, 2022, IEEE J-STSP, V16, P224, DOI 10.1109/JSTSP.2021.3122886
   Adhane G, 2021, IEEE ACCESS, V9, P72681, DOI 10.1109/ACCESS.2021.3079700
   Alsagheer Dana, 2022, 2022 International Conference on Intelligent Data Science Technologies and Applications (IDSTA), P75, DOI 10.1109/IDSTA55301.2022.9923132
   Ashtari-Majlan M, 2022, IEEE J BIOMED HEALTH, V26, P3918, DOI 10.1109/JBHI.2022.3155705
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354
   Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8
   Borji A, 2016, IEEE T NEUR NET LEAR, V27, P1214, DOI 10.1109/TNNLS.2015.2480683
   BOWMAN AW, 1984, BIOMETRIKA, V71, P353
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen WT, 2022, IEEE J-STARS, V15, P1591, DOI 10.1109/JSTARS.2022.3144339
   Chen WT, 2022, IEEE J-STARS, V15, P1150, DOI 10.1109/JSTARS.2022.3141826
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dehshibi MM, 2023, IEEE J-STSP, V17, P677, DOI 10.1109/JSTSP.2023.3262358
   Dehshibi MM, 2023, IEEE T AFFECT COMPUT, V14, P944, DOI 10.1109/TAFFC.2021.3090809
   Dehshibi MM, 2019, VISUAL COMPUT, V35, P23, DOI 10.1007/s00371-017-1442-1
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0
   Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009
   Gupta Rajiv, 2015, P 19 INT C EV ASS SO, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hooker S., 2019, Adv. Neural Inf. Process. Syst, V32, P1
   Janzing D, 2020, PR MACH LEARN RES, V108, P2907
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Kasanishi T, 2021, IEEE INT SYM MULTIM, P249, DOI 10.1109/ISM52913.2021.00049
   Kim S, 2022, APPL INTELL, V52, P471, DOI 10.1007/s10489-021-02451-x
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XH, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3200, DOI 10.1145/3447548.3467148
   Li Y., 2016, FORTH INT C LEARNING, P196
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lipton ZC., 2018, QUEUE, V16, P31, DOI 10.1145/3236386.3241340
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Nadaraya E.A., 1964, Theory of Probability and Its Applications, V61, P405, DOI [10.1137/1109020, DOI 10.1137/1109020]
   Nguyen A., 2016, ADV NEURAL INFORM PR, P3387, DOI DOI 10.5555/3157382.3157477
   Olah C., 2017, Distill, V2, P7, DOI [DOI 10.23915/DISTILL.00007, 10.23915/distill.00007]
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samek W., 2017, ARXIV PREPRINT ARXIV
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Seo S, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P297, DOI 10.1145/3109859.3109890
   Shimazaki H, 2010, J COMPUT NEUROSCI, V29, P171, DOI 10.1007/s10827-009-0180-4
   Shrikumar A, 2017, PR MACH LEARN RES, V70
   Simonyan K., 2014, CORR
   Sixt L., 2018, P 37 INT C MACH LEAR, P9046
   Sturmfels P, 2020, DISTILL, V5, pe22, DOI 10.23915/distill.00022.
   Sundararajan M., 2020, INT C MACHINE LEARNI, P9269
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Taha Ahmed, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P734, DOI 10.1007/978-3-030-58520-4_43
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolf C, 2021, J VISION, V21, DOI 10.1167/jov.21.8.23
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang QS, 2019, PROC CVPR IEEE, P6254, DOI 10.1109/CVPR.2019.00642
   Zheng Q, 2022, LECT NOTES COMPUT SC, V13672, P459, DOI 10.1007/978-3-031-19775-8_27
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
NR 66
TC 0
Z9 0
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 10
PY 2023
DI 10.1007/s00371-023-03112-5
EA OCT 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T9LD5
UT WOS:001081117300001
OA Green Submitted
DA 2024-07-18
ER

EF