FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Peng, ZY
   Wu, L
   Xiao, B
AF Peng, Zhiyong
   Wu, Lei
   Xiao, Bo
TI High-speed dense matching algorithm for high-resolution aerial image
   based on CPU-FPGA
SO VISUAL COMPUTER
LA English
DT Article
DE Dense matching; Local feature description; Edge enhancement; Dynamic
   programming
ID STEREO; ACCURATE
AB The paper proposes a new initial disparity estimation algorithm based on sparse local feature matching and one-dimensional dynamic programming. The new algorithm solves the problems of long calculation time, high memory consumption and unknown search range of disparity in the classical pyramid SGM dense matching algorithm. Based on the initial disparity, the final accurate disparity can be got by using SGM algorithm within fixed little search range. The new algorithm is more accurate than the traditional SGM algorithm (by testing on a public data set, the accuracy of new algorithm is 1.57% higher than traditional SGM algorithm), it does not need to artificially estimate the search range of disparity, and it is suitable for implementation on FPGA. Based on the new algorithm, the CPU-FPGA collaborative high-speed dense matching for high-resolution aerial images is realized. The average speed of dense matching on CPU-FPGA is 31.81times faster than the original SGM algorithm and 6.46 times faster than pyramid SGM algorithm. The speed of accurate matching by FPGA is 4.75 times faster than running on GPU.
C1 [Peng, Zhiyong; Wu, Lei; Xiao, Bo] Guilin Univ Elect Technol, Sch Optoelect Engn, Guilin 541004, Peoples R China.
   [Peng, Zhiyong] Guangxi Key Lab Optoelect Informat Proc, Guilin 541004, Peoples R China.
C3 Guilin University of Electronic Technology
RP Peng, ZY (corresponding author), Guilin Univ Elect Technol, Sch Optoelect Engn, Guilin 541004, Peoples R China.; Peng, ZY (corresponding author), Guangxi Key Lab Optoelect Informat Proc, Guilin 541004, Peoples R China.
EM pzy@guet.edu.cn
RI Wu, Lei/JDC-4172-2023
FU Natural Science Foundation of Guangxi Province [2020GXNSFAA159091];
   Guangxi Key laboratory for optoelectronics information Processing
   [GD18108]; Innovation Project of Guangxi Graduate Education
   [JGY2022131]; Graduate education innovation program of Guilin University
   of Electronic technology [2022YCXS157]
FX This study was funded by Natural Science Foundation of Guangxi Province
   (2020GXNSFAA159091), Guangxi Key laboratory for optoelectronics
   information Processing (GD18108), Innovation Project of Guangxi Graduate
   Education (JGY2022131) and Graduate education innovation program of
   Guilin University of Electronic technology (2022YCXS157).
CR Banz Christian, 2010, Proceedings of the 2010 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (IC-SAMOS 2010), P93, DOI 10.1109/ICSAMOS.2010.5642077
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cambuim LFS, 2017, 2017 30TH SYMPOSIUM ON INTEGRATED CIRCUITS AND SYSTEMS DESIGN (SBCCI 2017): CHOP ON SANDS, P53, DOI 10.1145/3109984.3109992
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Gehrig SK, 2009, LECT NOTES COMPUT SC, V5815, P134, DOI 10.1007/978-3-642-04667-4_14
   Gehrke S., 2010, 2010 ISPRS C
   HANNAH MJ, 1974, THESIS STANFORD U
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hermann S, 2011, LECT NOTES COMPUT SC, V7087, P395
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   [黄超 Huang Chao], 2019, [中国图象图形学报, Journal of Image and Graphics], V24, P1381
   Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668
   Lee CA, 2011, IEEE J-STARS, V4, P508, DOI 10.1109/JSTARS.2011.2162643
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Liu Y, 2020, VISUAL COMPUT, V36, P827, DOI 10.1007/s00371-019-01656-z
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Nurvitadhi E, 2016, 2016 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), P77, DOI 10.1109/FPT.2016.7929192
   Passalis N., 2021, SIGNAL PROCESS-IMAGE, V93, P116
   Puglia L, 2017, IEEE T CIRCUITS-II, V64, P1307, DOI 10.1109/TCSII.2017.2691675
   Qamar A, 2017, IEEE ACCESS, V5, P8419, DOI 10.1109/ACCESS.2016.2635378
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Rothermei M., 2012, P LC3D WORKSHOP
   Rothermel M., 2017, DEV SGM BASED MULTIV
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Shahbazi M, 2018, ISPRS J PHOTOGRAMM, V146, P373, DOI 10.1016/j.isprsjprs.2018.10.005
   Shi JL, 2021, VISUAL COMPUT, V37, P815, DOI 10.1007/s00371-020-01832-6
   Spangenberg Robert, 2014, 2014 IEEE Intelligent Vehicles Symposium Proceedings, P195, DOI 10.1109/IVS.2014.6856419
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Ttofis C, 2016, IEEE T COMPUT, V65, P2678, DOI 10.1109/TC.2015.2506567
   Ttofis C, 2015, ACM T EMBED COMPUT S, V14, DOI 10.1145/2629699
   Yao P, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01211-8
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhang K, 2008, IEEE IMAGE PROC, P313, DOI 10.1109/ICIP.2008.4711754
   [张永军 Zhang Yongjun], 2021, [测绘学报, Acta Geodetica et Cartographica Sinica], V50, P1
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 37
TC 1
Z9 1
U1 3
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5263
EP 5278
DI 10.1007/s00371-022-02658-0
EA SEP 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000854726000003
DA 2024-07-18
ER

PT J
AU Wang, R
   Wu, WY
   Wang, XY
AF Wang, Rui
   Wu, Wanyu
   Wang, Xiangyang
TI Enhancing multi-scale information exchange and feature fusion for human
   pose estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose estimation; Multi-scale feature fusion; Spatial attention and
   channel attention; Long-range dependency
AB Multi-scale feature fusion is an important part of modern network architectures to extract more comprehensive information for most computer vision tasks, such as semantic segmentation and keypoint estimation. However, most existing multi-scale methods add fusion connections between layers or branches directly, which inevitably ignores the semantic information discrepancy between feature maps with different resolutions and depths. Moreover, inappropriate fusion connections may lead to the loss of channel-wise and spatial information. In this paper, we propose a method to enhance and refine multi-scale feature fusion for human pose estimation by employing two attention mechanisms. Specifically, we present a novel multi-head spatial attention (MHSA), which is employed to model context information of the intermediate feature maps and reinforce important local features. Meanwhile, we utilize the position channel attention (PCA) to capture long-range dependencies while retaining the important position information in the attention maps. Combining with the modules of MHSA and PCA, we design an enhanced multi-scale feature fusion network (EMF-HRNet) based on the high-resolution network (HRNet). Our proposed EMF-HRNet yields better results with repeated multi-scale information exchange and feature fusion units. Extensive experiments on two common benchmarks, COCO Keypoint dataset and MPII Human Pose dataset, show that our method significantly improves the performance of state-of-the-art pose estimation methods.
C1 [Wang, Rui; Wu, Wanyu; Wang, Xiangyang] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
C3 Shanghai University
RP Wang, XY (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
EM wangxiangyang@shu.edu.cn
OI Wang, Xiangyang/0000-0003-1394-6068
FU National Natural Science Foundation of China (NSFC) [61771299]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61771299.
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Artacho B., ARXIV
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Everingham M., 2010, BMVC, V2, P5
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Geng ZG, 2021, PROC CVPR IEEE, P14671, DOI 10.1109/CVPR46437.2021.01444
   Gong X., ARXIV
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Li JF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11005, DOI 10.1109/ICCV48922.2021.01084
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H., arXiv
   Liu Z, 2015, J VIS COMMUN IMAGE R, V32, P10, DOI 10.1016/j.jvcir.2015.06.013
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Luvizon DC, 2021, IEEE T PATTERN ANAL, V43, P2752, DOI 10.1109/TPAMI.2020.2976014
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Manchen Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11085, DOI 10.1109/CVPR42600.2020.01110
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Park J-H, ARXIV
   Ren P, ARXIV
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Wang JH, 2021, PROC CVPR IEEE, P11850, DOI 10.1109/CVPR46437.2021.01168
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Z., 2022, CVPR, P13096
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xue N., 2022, CVPR, P13065
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Yuan Y., ARXIV
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang J., 2022, IEEECVF C COMPUT VIS, P13232
   Zhao X., 2021, VISUAL COMPUT, P1
   Zhu JG, 2019, NEUROCOMPUTING, V370, P109, DOI 10.1016/j.neucom.2019.08.043
NR 58
TC 1
Z9 1
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4751
EP 4765
DI 10.1007/s00371-022-02623-x
EA AUG 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000841698500001
DA 2024-07-18
ER

PT J
AU Guan, JQ
   Meng, M
   Liang, TY
   Liu, JG
   Wu, JG
AF Guan, Jiaqi
   Meng, Min
   Liang, Tianyou
   Liu, Jigang
   Wu, Jigang
TI Dual-level contrastive learning network for generalized zero-shot
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Generalized zero-shot learning; Contrastive learning; Generative
   adversarial networks
AB Generalized zero-shot learning (GZSL) aims to utilize semantic information to recognize the seen and unseen samples, where unseen classes are unavailable during training. Though recent advances have been made by incorporating contrastive learning into GZSL, existing approaches still suffer from two limitations: (1) without considering fine-grained cluster structures, these models cannot guarantee the discriminability and semantic awareness of synthetic features; (2) classifiers tend to overfit the seen classes, as they only concentrate on the seen domain. To address these challenges, we propose a Dual-level Contrastive Learning Network (DCLN), in which intra-domain and cross-domain contrastive learning are seamlessly integrated into a unified learning model. Specifically, the former performs center-prototype contrasting to fully explore the discriminative structure knowledge, while the latter is proposed to effectively alleviate the overfitting problem by utilizing the semantic relationships between the seen and unseen domain. Finally, the experimental results on four benchmark datasets demonstrate the superiority of our DCLN over the state-of-the-art methods.
C1 [Guan, Jiaqi; Meng, Min; Liang, Tianyou; Wu, Jigang] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou, Peoples R China.
   [Liu, Jigang] Ping An Life Insurance China, Shenzhen, Peoples R China.
C3 Guangdong University of Technology
RP Meng, M (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou, Peoples R China.; Liu, JG (corresponding author), Ping An Life Insurance China, Shenzhen, Peoples R China.
EM mengmin1985@gmail.com; liujigang82@gmail.com
RI wu, ji/IAR-8520-2023
OI Meng, Min/0000-0002-5107-5585
FU National Natural Science Foundation of China [62172109, 62072118];
   Natural Science Foundation of Guangdong Province [2020A1515011361,
   2022A1515010322]; High-Level Talents Programme of Guangdong Province
   [2017GC010556]; Guangdong Basic and Applied Basic Research Foundation
   [2021B1515120010]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172109 and Grant 62072118, in part by
   the Natural Science Foundation of Guangdong Province under Grant
   2020A1515011361 and Grant 2022A1515010322, in part by the High-Level
   Talents Programme of Guangdong Province under Grant 2017GC010556, and in
   part by the Guangdong Basic and Applied Basic Research Foundation under
   Grant 2021B1515120010.
CR Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen T., 2020, ARXIV
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Goodfellow I.J., 2014, P ANN C NEUR INF PRO
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huynh D. T., 2020, NEURIPS
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Krizhevsky Alex., NIPS 2012
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li J, 2019, PROC CVPR IEEE, P5458, DOI 10.1109/CVPR.2019.00561
   Liu Yang, 2021, IEEE CVF C COMP VIS
   Papadomanolaki M, 2021, IEEE T GEOSCI REMOTE, V59, P7651, DOI 10.1109/TGRS.2021.3055584
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Tian Y., 2020, NeurIPS, V33, P6827
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang J., 2021, P IEEECVF INT C COMP, P885
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xingyu Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P572, DOI 10.1007/978-3-030-58586-0_34
   Yue ZQ, 2021, PROC CVPR IEEE, P15399, DOI 10.1109/CVPR46437.2021.01515
   Zhang Fei, 2019, PR MACH LEARN RES, P7434
   Zongyan Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12862, DOI 10.1109/CVPR42600.2020.01288
NR 35
TC 4
Z9 4
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3087
EP 3095
DI 10.1007/s00371-022-02539-6
EA JUN 2022
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000809328000001
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, M
   Du, HQ
   Mei, WB
   Wang, S
   Yuan, D
AF Wang, Man
   Du, Huiqian
   Mei, Wenbo
   Wang, Shaui
   Yuan, Dasen
TI Material-aware Cross-channel Interaction Attention (MCIA) for occluded
   prohibited item detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Prohibited items; X-ray images; Occlusion
AB For security inspection, detecting prohibited items in X-ray images is challenging since they are usually occluded by non-prohibited items. In X-ray images, different materials present different colors and textures. On this basis, we exploit the material characteristics to detect occluded prohibited items. Moreover, the occlusion mainly exists between prohibited items and non-prohibited ones, belonging to inter-class occlusion. We propose a Material-aware Cross-channel Interaction Attention (MCIA) module which can use the material information of X-ray images to deal with the inter-class occlusion. Specifically, MCIA is composed of Material Perception (MP) and Cross-channel Interaction (CI). MP captures distinctive material information of X-ray images and CI gets the local cross-channel interaction to convert material information into channel-wise weights. By combining MP and CI, MCIA effectively helps the network to highlight the core features of prohibited items while suppressing non-prohibited items. Meanwhile, we design the MCIA-Net and MCIA-FPN by placing our MCIA module behind each stage in ResNet. Our MCIA-Net and MCIA-FPN can be used as backbones to detect occluded prohibited items. Note that MCIA-FPN also takes into account the prohibited items of various sizes. Our MCIA-Net and MCIA-FPN have been comprehensively validated on the SIXray dataset and OPIXray dataset. The experimental results prove the superiority of our method. Furthermore, our proposed MCIA module outperforms several widely used attention mechanisms and effectively improves the performance of Faster R-CNN and Cascade R-CNN in detecting occluded prohibited items.
C1 [Wang, Man; Du, Huiqian; Mei, Wenbo; Wang, Shaui] Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
   [Yuan, Dasen] Inner Mongolia Autonomous Reg Publ Secur Bur, Hohhot 010051, Peoples R China.
C3 Beijing Institute of Technology
RP Du, HQ (corresponding author), Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
EM 3120190807@bit.edu.cn; duhuiqian@bit.edu.cn
OI Wang, Man/0000-0002-1201-7518
CR Akcay S., 2017, 2017 IEEE INT C IMAG
   Akcay S., 2020, ARXIV PREPRINT ARXIV
   Akçay S, 2016, IEEE IMAGE PROC, P1057, DOI 10.1109/ICIP.2016.7532519
   Bastan M, 2011, LECT NOTES COMPUT SC, V6854, P360, DOI 10.1007/978-3-642-23672-3_44
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bodla N., 2017, P IEEE INT C COMPUTE
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao Y., 2019, P IEEE INT C COMPUTE
   Cui YQ, 2019, PROC SPIE, V10999, DOI 10.1117/12.2517817
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hassan T., 2019, ARXIV PREPRINT ARXIV
   Hassan T, 2020, IEEE IMAGE PROC, P2016, DOI 10.1109/ICIP40778.2020.9190711
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X., 2017, P IEEE INT C COMPUTE
   Jaccard N., 2014, 2014 11 IEEE INT C A
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kundegorski M. E., 2016, USING FEATURE DESCRI
   Lee H., 2019, P IEEE INT C COMPUTE
   Li Z., 2017, CORR
   Liang K.J., 2019, ARXIV PREPRINT ARXIV
   Liang KJ, 2018, PROC SPIE, V10632, DOI 10.1117/12.2309484
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JY, 2019, PROC INT C TOOLS ART, P1757, DOI 10.1109/ICTAI.2019.00262
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2020, VISUAL COMPUT, V2020, P1
   Liu ZQ, 2018, INT CONF SYST INFORM, P278, DOI 10.1109/ICSAI.2018.8599420
   Mery D, 2016, LECT NOTES COMPUT SC, V9431, P709, DOI 10.1007/978-3-319-29451-3_56
   Miao C., 2019, P IEEE C COMPUTER VI
   Nam H, 2018, ADV NEUR IN, V31
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Riffo V, 2016, IEEE T SYST MAN CY-S, V46, P472, DOI 10.1109/TSMC.2015.2439233
   Shajini M, 2021, VISUAL COMPUT, V37, P1517, DOI 10.1007/s00371-020-01885-7
   Shi W., 2020, VISUAL COMPUT, P112
   Steitz Jan-Martin O., 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P153, DOI 10.1007/978-3-030-12939-2_12
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wei Y., 2020, ARXIV PREPRINT ARXIV
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zhang TW, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3119875
   Zhang YT, 2020, OPTOELECTRON LETT, V16, P313, DOI 10.1007/s11801-020-9118-x
NR 46
TC 9
Z9 10
U1 4
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2865
EP 2877
DI 10.1007/s00371-022-02498-y
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000792533800003
DA 2024-07-18
ER

PT J
AU Li, SP
   Xian, Y
   Wu, W
   Zhang, T
   Li, BJ
AF Li, Shaopeng
   Xian, Yong
   Wu, Wei
   Zhang, Tao
   Li, Bangjie
TI Parameter-adaptive multi-frame joint pose optimization method
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric vision; Pose optimization; Image process; Neural network;
   Machine learning
AB Camera pose optimization is the basis of geometric vision works, such as 3D reconstruction, structure from motion, and visual odometry. We designed a multi-frame pose optimization method based on the inverse compositional algorithm. The neural networks are added into the optimization model to improve the problems of hyperparameter selection and loss function design. The multi-frame joint is used to fully utilize the constraints between the sequence images. A multi-layer stepwise method is used, which incorporates scale factors on the loss of each layer to enhance the convergence of the network. The simulation verifies that the proposed method achieves higher precision of pose estimation compared with the state-of-the-art.
C1 [Li, Shaopeng; Xian, Yong; Wu, Wei; Li, Bangjie] High Tech Inst Xian, Xian 710025, Peoples R China.
   [Li, Shaopeng; Zhang, Tao] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Rocket Force University of Engineering; Tsinghua University
RP Li, SP (corresponding author), High Tech Inst Xian, Xian 710025, Peoples R China.; Li, SP (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM sp-li16@mails.tsinghua.edu.cn; taozhang@tsinghua.edu.cn
OI Li, Shao-peng/0000-0001-7560-9951
FU National Natural Science Foundation of China [62103432]
FX This study was funded by the National Natural Science Foundation of
   China (grant number: 62103432).
CR ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717
   DeTone D., 2018, P IEEE C COMPUTER VI
   En S., 2018, RPNET END TO END NET
   Engel J., 2014, EUR C COMP VIS
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Han X., 2015, P IEEE C COMPUTER VI
   Indra Gandhi M.P., 2015, 2015 INT C COMM SIGN
   Kendall A., 2015, P IEEE INT C COMPUTE
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Li R., 2018, 2018 IEEE INT C ROBO
   Li S., 2019, IEEE T AUTOM SCI ENG, V16, P1575
   Li Y., 2018, P EUROPEAN C COMPUTE
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv Z., 2019, P IEEE C COMPUTER VI
   Mahjourian R., 2018, P IEEE C COMPUTER VI
   Melekhov I., 2017, INT C ADV CONCEPTS I
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Rublee E, 2011, 2011 INT C COMPUTER
   Schönberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721
   Schonberger J.L., 2016, P IEEE C COMPUTER VI
   Shen T., 2019, 2019 INT C ROBOTICS
   Sturm J., P INT C INTELLIGENT
   Tang Chengzhou, 2018, ARXIV180604807
   Tang J., 2019, ARXIV191203426
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Vijayanarasimhan Sudheendra, 2017, ARXIV170407804
   Wang S., 2017, 2017 IEEE INT C ROB
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Wu J., 2017, 2017 IEEE INT C ROB
   Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50
   Zhan H., 2019, ARXIV190909803
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 37
TC 0
Z9 0
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2529
EP 2541
DI 10.1007/s00371-022-02476-4
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000791065400001
DA 2024-07-18
ER

PT J
AU Liu, DF
   Chen, LF
AF Liu, Defeng
   Chen, Lifang
TI SECPNet-secondary encoding network for estimating camera parameters
SO VISUAL COMPUTER
LA English
DT Article
DE Camera calibration; Camera pose estimation; Deep learning; Focal length
   estimation; Multi-view image
ID CALIBRATION METHOD; POSE
AB Camera parameter estimation can be used in visual odometry, robot vision, SLAM, 3D reconstruction and other directions. It is also the main research content of computer vision. Based on the deep learning strategy, we propose a secondary encoding network for camera parameters (SECPNet), which can predict the camera parameters and recover the camera pose according to a single RGB image. Based on the three-dimensional dataset ShapeNet40 (Chang et al. in An information-rich 3D model repository, 2015. arXiv:1512.03012), we build a varifocal multi-viewpoint image dataset for camera parameter estimation. Experimental results show that our method has state-of-the-art performance in camera parameter estimation.
C1 [Liu, Defeng; Chen, Lifang] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Chen, LF (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM defengjnu@stu.jiangnan.edu.cn; may7366@163.com
FU Postgraduate Research & Practice Innovation Program of Jiangsu Province
   [SJCX20 _ 0775]
FX This study was funded by Postgraduate Research & Practice Innovation
   Program of Jiangsu Province Grant Number SJCX20 _ 0775.
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   [Anonymous], 2018, ECCV
   Ardakani HK, 2020, VISUAL COMPUT, V36, P413, DOI 10.1007/s00371-019-01632-7
   Bock B., 2010, US Patent App., Patent No. [29/350,912, 29350912]
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Cai BL, 2019, OPT LASER ENG, V114, P44, DOI 10.1016/j.optlaseng.2018.10.011
   Chang A. X., 2015, ARXIV
   Charco JL, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS), P224, DOI 10.1109/SITIS.2018.00041
   Chen B, 2020, OPT LASER ENG, V126, DOI 10.1016/j.optlaseng.2019.105919
   DeMa S, 1996, IEEE T ROBOTIC AUTOM, V12, P114, DOI 10.1109/70.481755
   Do T.T., 2018, ARXIV180210367
   Fang Q, 2018, IEEE ANN INT CONF CY, P61, DOI 10.1109/CYBER.2018.8688359
   Frosio I, 2016, VISUAL COMPUT, V32, P663, DOI 10.1007/s00371-015-1089-8
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Grabner A, 2019, IEEE I CONF COMP VIS, P2222, DOI 10.1109/ICCV.2019.00231
   Guan W, 2017, CHIN CONT DECIS CONF, P5217, DOI 10.1109/CCDC.2017.7979423
   Guo F, 2017, IEEE GLOB CONF SIG, P408, DOI 10.1109/GlobalSIP.2017.8308674
   Guo Yan Xu, 2011, 2011 International Conference on Image Analysis and Signal Processing (IASP 2011), P133, DOI 10.1109/IASP.2011.6109013
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Kehl W, 2017, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.2017.57
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kim T.-K., 2019, INSTANCEAND CATEGORY, P243
   Li SG, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P168, DOI 10.1109/ACPR.2017.79
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   Lu L., 2010, 2010 2 INT C SIGN PR, V3, pV3
   Matei BC, 2006, IEEE T PATTERN ANAL, V28, P1537, DOI 10.1109/TPAMI.2006.205
   Maybank S.J, 1998, LECT NOTES COMPUTER, V588
   Mottaghi R, 2015, PROC CVPR IEEE, P418, DOI 10.1109/CVPR.2015.7298639
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Duong ND, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P258, DOI 10.1109/ISMAR-Adjunct.2018.00080
   Naseer T, 2017, IEEE INT C INT ROBOT, P1525, DOI 10.1109/IROS.2017.8205957
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Sattler T, 2014, LECT NOTES COMPUT SC, V8692, P828, DOI 10.1007/978-3-319-10593-2_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P86, DOI 10.1109/WCICA.2014.7052692
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388
   Wang C, 2019, ARXIV191010750
   Workman S, 2015, IEEE IMAGE PROC, P1369, DOI 10.1109/ICIP.2015.7351024
   Wu CC, 2015, PROC CVPR IEEE, P2440, DOI 10.1109/CVPR.2015.7298858
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Zhang Z., 1999, FLEXIBLE CAMERA CALI, DOI DOI 10.1109/ICCV.1999.791289
   Zhang Z, 2018, MEASUREMENT, V130, P298, DOI 10.1016/j.measurement.2018.07.085
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zheng YQ, 2016, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2016.198
   Zheng YQ, 2014, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2014.62
   Zhu ZM, 2019, OPT LASER ENG, V112, P128, DOI 10.1016/j.optlaseng.2018.09.009
NR 52
TC 2
Z9 2
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1689
EP 1702
DI 10.1007/s00371-021-02098-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000782465400013
DA 2024-07-18
ER

PT J
AU Wang, Y
   Zhao, LN
AF Wang, Yong
   Zhao, Lina
TI Point cloud sampling method based on offset-attention and mutual
   supervision
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Point cloud sampling; Geometric features; Offset-attention mechanism;
   Mutual supervision
AB In applications based on a three-dimensional point cloud, massive point cloud data often brings processing difficulties. To deal with the problem, many point cloud sampling methods were proposed. But there are still some issues in these methods: (i) lack the consideration of geometric features and (ii) how to train the distribution of projected points by the observed coefficient effectively. This paper introduces a fine-tuned pointnet module, which extracts the geometric features of points and applies the offset-attention mechanism to enhance the feature expression ability. Furthermore, it corrects the positions of simplified points by a mutual supervision loss. The experimental results show our method can improve the effectiveness and robustness of the point cloud sampling.
C1 [Wang, Yong; Zhao, Lina] Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 401135, Peoples R China.
C3 Chongqing University of Technology
RP Zhao, LN (corresponding author), Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 401135, Peoples R China.
EM zhaolina956@gmail.com
RI wang, yong/ISB-5675-2023
OI zhao, lina/0000-0002-3312-0565
FU Nature Science Foundation of China [61502065, 61976158]
FX This work is supported by the Nature Science Foundation of China under
   grants 61502065 and 61976158. The authors declare that they have no
   conflict of interest.
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Bruna J., 2014, P INT C LEARN REPR
   Chen YC, 2020, IEEE T NETW SCI ENG, V7, P3279, DOI 10.1109/TNSE.2020.3024723
   Deng CH, 2020, CAAI T INTELL TECHNO, V5, P276, DOI 10.1049/trit.2020.0055
   Dovrat O, 2019, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR.2019.00287
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Guo MH, 2020, ARXIV PREPRINT ARXIV
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang C.R, 1988, SIMULATED ANNEALING
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang R, 2020, CAAI T INTELL TECHNO, V5, P165, DOI 10.1049/trit.2019.0107
   Lang Itai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7575, DOI 10.1109/CVPR42600.2020.00760
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Muis F.J., 2020, ARXIV PREPRINT ARXIV
   Nezhadarya Ehsan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12953, DOI 10.1109/CVPR42600.2020.01297
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Sarode Vinit, 2019, ARXIV190807906
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Jihong, 2020, Semi-supervised active learning for instance segmentation via scoring predictions
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xing YL, 2021, CAAI T INTELL TECHNO, V6, P80, DOI 10.1049/cit2.12014
   Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yao XX, 2019, IEEE ACCESS, V7, P37121, DOI 10.1109/ACCESS.2019.2905546
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zaheer M, 2017, ADV NEUR IN, V30
NR 44
TC 1
Z9 1
U1 4
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2337
EP 2345
DI 10.1007/s00371-022-02440-2
EA APR 2022
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000785702100001
DA 2024-07-18
ER

PT J
AU Murali, P
   Niranjana, G
   Paul, AJ
   Muthu, JS
AF Murali, P.
   Niranjana, G.
   Paul, Aditya Jyoti
   Muthu, Joan S.
TI Domain-flexible selective image encryption based on genetic operations
   and chaotic maps
SO VISUAL COMPUTER
LA English
DT Article
DE Chaos; Edge detection; Genetic operators; Orthogonal polynomials; Region
   of Interest (ROI); Selective encryption
ID ALGORITHM; SYSTEM; RETRIEVAL; SCHEME; MODEL
AB Image encryption research has seen massive advances in recent times, but many avenues of improvement still remain nascent. This paper takes head on various research challenges, giving the user fine grained control over their encryption requirements, by proposing a domain-flexible and selective image encryption scheme based on genetic algorithm, chaotic map, square-wave diffusion and orthogonal polynomials transformation. Initially, the proposed cryptosystem separates the image into important and unimportant regions making use of edges in the image with the orthogonal polynomials transformation. Important blocks, termed as Regions of Interest (ROI), are encrypted based on genetic operators and fitness score with chaos and unimportant blocks are encrypted with shuffling operations in the orthogonal polynomial domain. Then, square-wave diffusion is carried on the entire image to obtain the final encrypted image. The novel feature of the proposed encryption scheme is the unique design of the fitness function, wherein the fitness value can be varied between 1 for maximum speed and 10 for maximum security, to suit the user's requirements and can operate in frequency or spatial or hybrid domain suitable for a vast range of real-time applications. Extensive experiments and analyses have been conducted to demonstrate the efficiency of the proposed work.
C1 [Murali, P.; Niranjana, G.; Paul, Aditya Jyoti; Muthu, Joan S.] SRM Inst Sci & Technol, Dept Comp Sci & Engn, Kattankulathur 603203, Tamil Nadu, India.
   [Paul, Aditya Jyoti] Cognit Applicat Res Lab, Chennai, Tamil Nadu, India.
C3 SRM Institute of Science & Technology Chennai
RP Murali, P (corresponding author), SRM Inst Sci & Technol, Dept Comp Sci & Engn, Kattankulathur 603203, Tamil Nadu, India.
EM muralip@srmist.edu.in; niranjag@srmist.edu.in;
   aditya_jyoti@srmuniv.edu.in; joans@srmist.edu.in
RI Paul, Aditya Jyoti/AAX-5365-2021; P, Murali/IUN-8987-2023; Govindan,
   Niranjana/ABE-8887-2021
OI Paul, Aditya Jyoti/0000-0002-4351-2108; Govindan,
   Niranjana/0000-0001-7709-2239; S Muthu, Joan/0000-0001-6527-0683; P.,
   Murali/0000-0003-4988-3345
CR Afarin R, 2013, IRAN CONF MACH, P441, DOI 10.1109/IranianMVIP.2013.6780026
   [Anonymous], 2016, MULTIMEDIA TOOLS APP
   Babaei M, 2013, NAT COMPUT, V12, P101, DOI 10.1007/s11047-012-9334-9
   Bahrami S, 2013, OPTIK, V124, P3693, DOI 10.1016/j.ijleo.2012.11.028
   Bhatnagar G, 2012, DIGIT SIGNAL PROCESS, V22, P648, DOI 10.1016/j.dsp.2012.02.005
   Bigdeli N, 2012, ENG APPL ARTIF INTEL, V25, P753, DOI 10.1016/j.engappai.2012.01.007
   Bigdeli N, 2012, COMPUT ELECTR ENG, V38, P356, DOI 10.1016/j.compeleceng.2011.11.019
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Enayatifar R, 2014, OPT LASER ENG, V56, P83, DOI 10.1016/j.optlaseng.2013.12.003
   Ganesan L, 1997, IEEE T SYST MAN CY B, V27, P823, DOI 10.1109/3477.623235
   Guesmi R, 2016, MULTIMED TOOLS APPL, V75, P4753, DOI 10.1007/s11042-015-2501-0
   Gupta A, 2020, J AMB INTEL HUM COMP, V11, P1309, DOI 10.1007/s12652-019-01493-x
   Han CY, 2019, OPTIK, V181, P779, DOI 10.1016/j.ijleo.2018.12.178
   Hoang TM, 2014, EUR PHYS J-SPEC TOP, V223, P1635, DOI 10.1140/epjst/e2014-02121-3
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Huang XL, 2014, COMMUN NONLINEAR SCI, V19, P4094, DOI 10.1016/j.cnsns.2014.04.012
   Huang XL, 2012, NONLINEAR DYNAM, V67, P2411, DOI 10.1007/s11071-011-0155-7
   Kadir A, 2014, OPTIK, V125, P1671, DOI 10.1016/j.ijleo.2013.09.040
   Kalpana J, 2016, MULTIMED TOOLS APPL, V75, P49, DOI 10.1007/s11042-014-2262-1
   Kalpana J, 2015, OPTIK, V126, P5703, DOI 10.1016/j.ijleo.2015.09.091
   Kalpana J, 2012, SIGNAL PROCESS, V92, P3062, DOI 10.1016/j.sigpro.2012.05.014
   Khan M, 2019, MULTIMED TOOLS APPL, V78, P26203, DOI 10.1007/s11042-019-07818-4
   Khashan OA, 2014, J ZHEJIANG U-SCI C, V15, P435, DOI 10.1631/jzus.C1300262
   Krishnamoorthi R, 2007, PATTERN RECOGN LETT, V28, P771, DOI 10.1016/j.patrec.2006.10.009
   Krishnamoorthi R, 2008, INT J COMPUT SCI NET, V8, P195
   Krishnamoorthi R, 2012, J VIS COMMUN IMAGE R, V23, P18, DOI 10.1016/j.jvcir.2011.07.011
   Krishnamoorthi R, 2009, IMAGE VISION COMPUT, V27, P999, DOI 10.1016/j.imavis.2008.08.006
   Krishnamoorthy, 2015, MULTIMED TOOLS APPL
   Krishnamoorthy R, 2014, MULTIDIM SYST SIGN P, V25, P637, DOI 10.1007/s11045-013-0222-y
   Kumar J., 2012, ADV COMPUTER SCI ENG, P783, DOI [10.1007/978-3-642-30111-7_75, DOI 10.1007/978-3-642-30111-7_75]
   Li SS, 2013, MULTIMED TOOLS APPL, V66, P573, DOI 10.1007/s11042-012-1281-z
   Liu Y, 2016, MULTIMED TOOLS APPL, V75, P4363, DOI 10.1007/s11042-015-2479-7
   Luo, PARALLEL IMAGE ENCRY
   Mandal MK, 2014, SECUR COMMUN NETW, V7, P2145, DOI 10.1002/sec.927
   Murali P, 2019, MULTIMED TOOLS APPL, V78, P2135, DOI 10.1007/s11042-018-6234-8
   Musanna F, 2019, MULTIMED TOOLS APPL, V78, P14867, DOI 10.1007/s11042-018-6827-2
   Muthu Joan S., 2020, 2020 IEEE Recent Advances in Intelligent Computational Systems (RAICS), P125, DOI 10.1109/RAICS51191.2020.9332470
   Muthu JS, 2021, OPTIK, V242, DOI 10.1016/j.ijleo.2021.167300
   Muthu JS, 2020, OPTIK, V207, DOI 10.1016/j.ijleo.2019.163843
   Muthu JS, 2021, SN COMPUT SCI, V2, DOI [10.1007/s42979-021-00778-3, DOI 10.1007/S42979-021-00778-3]
   Niu Y, 2020, MULTIMED TOOLS APPL, V79, P25613, DOI 10.1007/s11042-020-09237-2
   Pareek NK, 2016, SOFT COMPUT, V20, P763, DOI 10.1007/s00500-014-1539-7
   Paul Aditya Jyoti, 2020, 2020 IEEE Recent Advances in Intelligent Computational Systems (RAICS), P201, DOI 10.1109/RAICS51191.2020.9332513
   Praveenkumar P, 2015, SECUR COMMUN NETW, V8, P3335, DOI 10.1002/sec.1257
   Premkumar R, 2019, MULTIMED TOOLS APPL, V78, P9577, DOI 10.1007/s11042-018-6534-z
   Rehman AU, 2015, MULTIMED TOOLS APPL, V74, P4655, DOI 10.1007/s11042-013-1828-7
   Shafique A, 2018, EUR PHYS J PLUS, V133, DOI 10.1140/epjp/i2018-12138-3
   Souici I, 2011, ANALOG INTEGR CIRC S, V69, P49, DOI 10.1007/s10470-011-9627-4
   Sui LS, 2019, OPT LASER ENG, V122, P113, DOI 10.1016/j.optlaseng.2019.06.005
   Wang L, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 3, PROCEEDINGS, P22, DOI 10.1109/CISP.2008.129
   Wang XY, 2016, NONLINEAR DYNAM, V83, P333, DOI 10.1007/s11071-015-2330-8
   Wang XY, 2014, NONLINEAR DYNAM, V78, P2975, DOI 10.1007/s11071-014-1639-z
   Xiang T, 2007, CHAOS, V17, DOI 10.1063/1.2728112
   Xue HW, 2018, OPT LASER TECHNOL, V106, P506, DOI 10.1016/j.optlastec.2018.04.030
   Ye GD, 2012, NONLINEAR DYNAM, V69, P2079, DOI 10.1007/s11071-012-0409-z
   Zeng L, 2016, MULTIMED TOOLS APPL, V75, P5439, DOI 10.1007/s11042-015-2511-y
   Zhang YQ, 2014, INFORM SCIENCES, V273, P329, DOI 10.1016/j.ins.2014.02.156
NR 57
TC 19
Z9 19
U1 2
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1057
EP 1079
DI 10.1007/s00371-021-02384-z
EA FEB 2022
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000757164600001
DA 2024-07-18
ER

PT J
AU Abbas, F
   Babahenini, MC
AF Abbas, Faycal
   Babahenini, Mohamed Chaouki
TI Forest fog rendering using generative adversarial networks
SO VISUAL COMPUTER
LA English
DT Article
DE Forest rendering; Generative adversarial network; Natural scene
   lighting; Deep learning
AB Producing a physically realistic rendering of forests with fog involves the simulation of light diffusion in participating media. This volumetric phenomenon increases the realism of natural scenes in several fields, such as video games and flight simulators. However, this effect in a natural scene is characterized by complexity at all scales, representing a challenge for the computer graphics community. In this paper, we propose a novel approach based on a generative adversarial network to estimate fog in forest scenes. Our approach operates in two steps. The first step consists of training a generative adversarial network on a large dataset of synthetic images. Our network takes four images as input (normal map, depth map, albedo map and RGB map without fog), and it generates an estimation of the rendering forest with fog for output. A reference image conditions the input images to produce a better approximation. The second step consists of the production of realistic images with fog. Our technique can be generalized to high-frequency lighting effects (specularity and shadow), so it does not require any calculation in 3D space.
C1 [Abbas, Faycal] Univ Abbes Laghrour Khenchela, Dept Comp Sci, BP 1252, Khenchela 40004, Algeria.
   [Babahenini, Mohamed Chaouki] Univ Mohamed Khider, Dept Comp Sci, BP 145, Biskra 07000, Algeria.
C3 Universite Abbes Laghrour Khenchela; Universite Mohamed Khider Biskra
RP Abbas, F (corresponding author), Univ Abbes Laghrour Khenchela, Dept Comp Sci, BP 1252, Khenchela 40004, Algeria.
EM abbas_faycal@univ-khenchela.dz; chaouki.babahenini@gmail.com
RI BABAHENINI, Mohamed Chaouki/F-1427-2017; ABBAS, Fayçal/ABZ-7850-2022
OI BABAHENINI, Mohamed Chaouki/0000-0001-7972-8026; ABBAS,
   Fayçal/0000-0002-0251-9901
CR Abbas F, 2018, 3D RES, V9, DOI 10.1007/s13319-018-0171-1
   [Anonymous], Unity: Unity technologies
   Bitterli B, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073698
   Blasi P., 1993, Computer Graphics Forum, V12, pC201, DOI 10.1111/1467-8659.1230201
   Boulanger K., 2008, THESIS U RENNES I FR
   Bruneton E, 2008, COMPUT GRAPH FORUM, V27, P1079, DOI 10.1111/j.1467-8659.2008.01245.x
   Chen J., 2011, Symposium on Interactive 3D Graphics and Games, I3D 2011, P39, DOI 10.1145/1944745.1944752
   Chen Z, 2020, PROC CVPR IEEE, P5598, DOI 10.1109/CVPR42600.2020.00564
   Dahm K., 2016, INT C MONT CARL QUAS, P181, DOI [10.1007/978-3-319-91436-7_9, DOI 10.1007/978-3-319-91436-7_9]
   Deng X, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323041
   Georgoulis S, 2018, IEEE T PATTERN ANAL, V40, P1932, DOI 10.1109/TPAMI.2017.2742999
   Girod Bernd, 1993, P207
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hachisuka Toshiya., 2017, P EGSR EXPT IDEAS IM, DOI DOI 10.2312/SRE.20171195
   Hoffman N., 2002, Proceedings of Game Developer Conference, V2002, P337
   Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jakob Wenzel., 2010, Mitsuba physically based renderer
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kallweit S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130880
   Keller A, 2019, MATH COMPUT SIMULAT, V161, P2, DOI 10.1016/j.matcom.2019.01.010
   Kim S, 2020, VISUAL COMPUT, V36, P911, DOI 10.1007/s00371-019-01701-x
   LeCun Y., 1990, ADV NEURAL INFORM PR, P396
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lei Si, 2018, Multi-disciplinary Trends in Artificial Intelligence. 12th International Conference, MIWAI 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 11248), P226, DOI 10.1007/978-3-030-03014-8_20
   Lunz Sebastian, 2020, arXiv preprint arXiv:2002.12674
   Miller B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323025
   Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P65, DOI 10.1111/cgf.13225
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   Pegoraro V, 2009, COMPUT GRAPH FORUM, V28, P329, DOI 10.1111/j.1467-8659.2009.01372.x
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schafhitzel T, 2007, JOURNAL WSCG, V15, P91
   Simon F, 2017, COMPUT GRAPH FORUM, V36, P101, DOI 10.1111/cgf.13228
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Song SR, 2019, PROC CVPR IEEE, P6911, DOI 10.1109/CVPR.2019.00708
   STAM J, 1994, GRAPH INTER, P51
   Subileau T., 2016, THESIS U TOULOUSE 3
   Taniai T, 2018, PR MACH LEARN RES, V80
   Thomas M.M., 2017, DEEP ILLUMINATION AP
   von Bernuth A, 2019, IEEE INT C INTELL TR, P41, DOI 10.1109/ITSC.2019.8917367
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wronski B., 2018, GPU PRO 360 GUIDE LI, P321
   Wu JJ, 2016, ADV NEUR IN, V29
   Xu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201313
   Yusov E., 2018, GPU PRO 360 GUIDE LI, P211
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
NR 47
TC 4
Z9 4
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 943
EP 952
DI 10.1007/s00371-021-02376-z
EA JAN 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000749149000001
DA 2024-07-18
ER

PT J
AU Li, YY
   Wang, ZY
   Yin, L
   Zhu, ZQ
   Qi, GQ
   Liu, Y
AF Li, Yuanyuan
   Wang, Ziyu
   Yin, Li
   Zhu, Zhiqin
   Qi, Guanqiu
   Liu, Yu
TI X-Net: a dual encoding-decoding method in medical image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Medical image segmentation; Transformer; Variational auto-encoder; Dual
   encoding-decoding
AB Medical image segmentation has the priori guiding significance for clinical diagnosis and treatment. In the past ten years, a large number of experimental facts have proved the great success of deep convolutional neural networks in various medical image segmentation tasks. However, the convolutional networks seem to focus too much on the local image details, while ignoring the long-range dependence. The Transformer structure can encode long-range dependencies in image and learn high-dimensional image information through the self-attention mechanism. But this structure currently depends on the database scale to give full play to its excellent performance, which limits its application in medical images with limited database size. In this paper, the characteristics of CNNs and Transformer are integrated to propose a dual encoding-decoding structure of the X-shaped network (X-Net). It can serve as a good alternative to the traditional pure convolutional medical image segmentation network. In the encoding phase, the local and global features are simultaneously extracted by two types of encoders, convolutional downsampling, and Transformer and then merged through jump connection. In the decoding phase, a variational auto-encoder branch is added to reconstruct the input image itself in order to weaken the impact of insufficient data. Comparative experiments on three medical image datasets show that X-Net can realize the organic combination of Transformer and CNNs.
C1 [Li, Yuanyuan; Wang, Ziyu; Zhu, Zhiqin] Chongqing Univ Posts & Telecommun, Coll Automat, Chongqing 400065, Peoples R China.
   [Yin, Li] Chongqing Univ Canc Hosp, Chongqing 400044, Peoples R China.
   [Qi, Guanqiu] Buffalo State Coll, Comp Informat Syst Dept, Buffalo, NY 14222 USA.
   [Liu, Yu] Hefei Univ Technol Hefei, Dept Biomed Engn, Hefei 230009, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; State University of
   New York (SUNY) System; Buffalo State College; Hefei University of
   Technology
RP Zhu, ZQ (corresponding author), Chongqing Univ Posts & Telecommun, Coll Automat, Chongqing 400065, Peoples R China.
EM liyy@cqupt.edu.cn; s200332016@stu.cqupt.edu.cn; lyin01@outlook.com;
   zhuzq@cqupt.edu.cn; qig@buffalostate.edu; yuliu@hfut.edu.cn
RI wang, zhiwen/JDV-9990-2023; wen, liang/JNR-7720-2023; li,
   yuanyuan/GZA-4435-2022; WANG, JIAXUAN/JMP-8599-2023; XIE,
   WANYING/JNR-9259-2023; Liu, Yu/HKF-4917-2023; Zhang, Jun/JPK-7723-2023;
   Zhu, Zhiqin/KMA-6385-2024; Qi, Guanqiu/M-8332-2017
OI Liu, Yu/0000-0003-2211-3535; Zhu, Zhiqin/0000-0002-3883-2529
FU National Natural Science Foundation of China [61803061, 61906026,
   6217021768]; Innovation research group of universities in Chongqing;
   Innovative project of shapingba district, Chongqing [Jcd202135];
   Chongqing Kewei Joint Medical Research Project [2020GDRC019,
   2021MSXM337]; Chongqing Natural Science Foundation
   [cstc2020jcyjmsxmX0577, cstc2020jcyj-msxmX0634]; "Chengdu-Chongqing
   Economic Circle" innovation funding of Chongqing Municipal Education
   Commission [KJCXZD2020028]; Special key project of Chongqing technology
   innovation and application development [cstc2019jscx-zdztzx0068]; China
   Postdoctoral Science Foundation [2020M670111ZX]; Special Fund for Young
   and Middle-aged Medical Top Talents of Chongqing [ZQNYXGDRCGZS2019005]
FX This work is jointly funded by the National Natural Science Foundation
   of China under Grant 61803061, 61906026, 6217021768; Innovation research
   group of universities in Chongqing; Innovative project of shapingba
   district, Chongqing, Jcd202135; Funding: Chongqing Kewei Joint Medical
   Research Project, 2020GDRC019, 2021MSXM337; the Chongqing Natural
   Science Foundation under Grant cstc2020jcyjmsxmX0577,
   cstc2020jcyj-msxmX0634; "Chengdu-Chongqing Economic Circle" innovation
   funding of Chongqing Municipal Education Commission KJCXZD2020028;
   Special key project of Chongqing technology innovation and application
   development cstc2019jscx-zdztzx0068; the China Postdoctoral Science
   Foundation 2020M670111ZX; and Special Fund for Young andMiddle-aged
   Medical Top Talents of Chongqing ZQNYXGDRCGZS2019005.
CR Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   Caicedo JC, 2019, NAT METHODS, V16, P1247, DOI 10.1038/s41592-019-0612-7
   Chen J., ARXIV210204306
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   Gibson E, 2018, IEEE T MED IMAGING, V37, P1822, DOI 10.1109/TMI.2018.2806309
   Guerrout El-Hachemi, 2013, International Journal of New Computer Architectures and their Applications, V3, P35
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jin QG, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.605132
   Kingma D. P., 2013, ARXIV13126114
   Li B, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-323
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Liu S, 2021, IEEE T FUZZY SYST, V29, P90, DOI 10.1109/TFUZZ.2020.3006520
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Muthukrishnan R., 2011, International Journal of Computer Science & Information Technology, V3, P259, DOI 10.5121/ijcsit.2011.3620
   Naylor P, 2019, IEEE T MED IMAGING, V38, P448, DOI 10.1109/TMI.2018.2865709
   Naylor P, 2017, I S BIOMED IMAGING, P933, DOI 10.1109/ISBI.2017.7950669
   Ott M., 2018, P 3 C MACH TRANSL WM, P1, DOI DOI 10.18653/V1/W18-6301
   Pappagari R, 2019, 2019 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU 2019), P838, DOI [10.1109/asru46091.2019.9003958, 10.1109/ASRU46091.2019.9003958]
   Patil DD., 2013, IJCSMC, V2, P22
   Saha PK, 2000, COMPUT VIS IMAGE UND, V77, P145, DOI 10.1006/cviu.1999.0813
   Shao TH, 2019, IEEE ACCESS, V7, P26146, DOI 10.1109/ACCESS.2019.2900753
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Valanarasu J.M.J., ARXIV210210662
   Wang YB, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9456015
   Wenchao Cui, 2013, 2013 Seventh International Conference on Image and Graphics (ICIG), P205, DOI 10.1109/ICIG.2013.47
   Xiao X, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P327, DOI 10.1109/ITME.2018.00080
   Zhao YQ, 2005, P ANN INT IEEE EMBS, P6492, DOI 10.1109/IEMBS.2005.1615986
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu ZQ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13163104
   Zhu ZQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024335
NR 39
TC 73
Z9 73
U1 14
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2223
EP 2233
DI 10.1007/s00371-021-02328-7
EA NOV 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000714523700001
DA 2024-07-18
ER

PT J
AU Li, BW
   He, FZ
   Zeng, XT
AF Li, Bowen
   He, Fazhi
   Zeng, Xiantao
TI A novel privacy-preserving outsourcing computation scheme for Canny edge
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Privacy-preserving computation; Canny edge detection; Image processing;
   Data privacy; Cloud computing
ID FEATURE-EXTRACTION; IMAGE; EFFICIENT; ALGORITHM
AB With the advancement of cloud computing technology, cloud servers are utilized to process large-scale data, especially multimedia data. However, concerns about leakage of private information prevent cloud computing from being further popularized. Thus, privacy-preserving computation for multimedia data is becoming increasingly important as a research hotspot. Edge detection plays an important role in image processing and computer vision. Different from previous researches on privacy-preserving computation, privacy-preserving edge detection faces new problems such as how to encrypt and represent edges. In this paper, we propose a privacy-preserving computation scheme for Canny edge detection. We first give an overview of our scheme, which involves one client and three cloud servers. Then, three key building blocks in the proposed scheme are put forward: pixel permutation; secure comparison and multiplication protocols; secure edge representation. Based on these building blocks, our scheme is carefully designed and constructed step by step. Furthermore, we analyze the correctness and the security of our scheme in detail. Finally, comparative experiments show that our scheme can maintain the quality of edge detection while meeting security requirements.
C1 [Li, Bowen; He, Fazhi] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Zeng, Xiantao] Wuhan Univ, Ctr Evidence Based & Translat Med, Zhongnan Hosp, Wuhan 430071, Peoples R China.
C3 Wuhan University; Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018
OI Zeng, Xian-Tao/0000-0003-1262-725X
FU National Natural Science Foundation of China [62072348]; Science and
   Technology Major Project of Hubei Province (Next-Generation AI
   Technologies) [2019AEA170]; Translational Medicine and Interdisciplinary
   Research Joint Fund of Zhongnan Hospital of Wuhan University
   [ZNJC201917]
FX This work is supported by the National Natural Science Foundation of
   China under Grant No 62072348, the Science and Technology Major Project
   of Hubei Province (Next-Generation AI Technologies) under Grant No
   2019AEA170 and Translational Medicine and Interdisciplinary Research
   Joint Fund of Zhongnan Hospital of Wuhan University under Grant No
   ZNJC201917).
CR [Anonymous], 2013, BSDS500
   [Anonymous], 2006, CALTECH 256
   Bai Y, 2014, IEEE INT CON MULTI
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bos Joppe W., 2013, Cryptography and Coding. 14th IMA International Conference, IMACC 2013. Proceedings: LNCS 8308, P45, DOI 10.1007/978-3-642-45239-0_4
   Brakerski Z, 2011, LECT NOTES COMPUT SC, V6841, P505, DOI 10.1007/978-3-642-22792-9_29
   Cai EJ, 2020, INTEGR COMPUT-AID E, V27, P173, DOI 10.3233/ICA-190614
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen YL, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106335
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Hamreras S, 2020, INTEGR COMPUT-AID E, V27, P317, DOI 10.3233/ICA-200625
   Hoffstein J., 1998, Algorithmic Number Theory. Third International Symposium, ANTS-III. Proceedings, P267, DOI 10.1007/BFb0054868
   Hsu CY, 2012, IEEE T IMAGE PROCESS, V21, P4593, DOI 10.1109/TIP.2012.2204272
   Hu SS, 2016, IEEE T IMAGE PROCESS, V25, P3411, DOI 10.1109/TIP.2016.2568460
   Ibn Ziad MT, 2016, IEEE CONF COMM NETW, P570, DOI 10.1109/CNS.2016.7860550
   Jiang LZ, 2020, IEEE T DEPEND SECURE, V17, P179, DOI 10.1109/TDSC.2017.2751476
   Kitayama M, 2019, IEEE ICCE, P80, DOI 10.1109/icce-asia46551.2019.8942217
   Kiva H., 2019, P INT S INT SIGN PRO, P1
   Li HR, 2021, MEMET COMPUT, V13, P1, DOI 10.1007/s12293-021-00328-7
   Liang, 2020, INT J BIOINSPIR COMP, DOI 10.1504/IJBIC.2020.10036562
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Lindell Y, 2009, J CRYPTOL, V22, P161, DOI 10.1007/s00145-008-9036-8
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JK, 2020, INTELL DATA ANAL, V24, P581, DOI 10.3233/IDA-194641
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Pan YT, 2020, WORLD WIDE WEB, V23, P2259, DOI 10.1007/s11280-020-00793-z
   Pan YT, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8123-3
   Qin Z, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P497, DOI 10.1145/2647868.2654941
   Quan Q, 2021, VISUAL COMPUT, V37, P245, DOI 10.1007/s00371-020-01796-7
   Rajput A.S., 2017, IEEE INT C AC SPEECH, P1
   Senel HG, 2009, IEEE T IMAGE PROCESS, V18, P867, DOI 10.1109/TIP.2008.2011758
   Shamir A., 1979, Communications of the ACM, V22, P612, DOI 10.1145/359168.359176
   Sirichotedumrong W, 2019, ASIAPAC SIGN INFO PR, P1756, DOI 10.1109/APSIPAASC47483.2019.9023091
   Song Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4434, DOI 10.1109/ICASSP.2018.8462600
   Wang Q, 2016, ASIA CCS'16: PROCEEDINGS OF THE 11TH ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P257, DOI 10.1145/2897845.2897861
   Wang Q, 2016, INT CON DISTR COMP S, P700, DOI 10.1109/ICDCS.2016.84
   Wang QF, 2017, INT J BIOMETEOROL, V61, P685, DOI 10.1007/s00484-016-1246-4
   Wang SM, 2017, J COMPUT INF SCI ENG, V17, DOI 10.1115/1.4036615
   Xia ZH, 2018, IEEE ACCESS, V6, P30392, DOI 10.1109/ACCESS.2018.2845456
   Zhang HR, 2019, IEEE INFOCOM SER, P1432, DOI [10.1109/INFOCOM.2019.8737484, 10.1109/infocom.2019.8737484]
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zheng PJ, 2018, IEEE T IMAGE PROCESS, V27, P2541, DOI 10.1109/TIP.2018.2802199
NR 43
TC 4
Z9 4
U1 2
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4437
EP 4455
DI 10.1007/s00371-021-02307-y
EA OCT 2021
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000712932700002
DA 2024-07-18
ER

PT J
AU Tian, Y
   Zhang, LG
   Sun, JG
   Yin, GS
   Dong, YX
AF Tian, Ye
   Zhang, Liguo
   Sun, Jianguo
   Yin, Guisheng
   Dong, Yuxin
TI Consistency regularization teacher-student semi-supervised learning
   method for target recognition in SAR images
SO VISUAL COMPUTER
LA English
DT Article
DE SAR automatic target recognition; Semi-supervised learning; Consistency
   regularization; Teacher-student structure
ID CLASSIFICATION
AB Synthetic aperture radar automatic target recognition (SAR-ATR) is a hotspot in the field of remote sensing, which has been widely used in disaster monitoring, environmental monitoring, resource exploration, and crop yield estimates. In recent years, deep convolutional neural networks (DCNNs) have achieved promising performance among a variety of supervised classification methods under the condition of sufficiently labeled samples. However, it is expensive and time-consuming to collect large amounts of labeled samples suitable for DCNNs in SAR domains. To reduce the dependence of SAR-ATR on labeled samples, in this work, a consistency regularization teacher-student semi-supervised (CRTS) method for SAR-ATR is proposed, in which consistency regularization is applied to analyze and divide unlabeled samples and the teacher-student structure is introduced to generate pseudo-labels for the divided unlabeled samples. Firstly, by using consistent pseudo-label prediction, unlabeled samples are divided into consistent unlabeled samples and confident unlabeled samples. Then, in order to improve the quality of pseudo-labeled labels, the student model is used to generate a pseudo-label for consistent unlabeled samples, and the teacher model which is ensembled by the other two student models labels the confident unlabeled samples. Finally, these pseudo-label unlabeled samples are mixed with the labeled samples and trained together to improve recognition performance. Experiments are conducted on the MSTAR dataset, and the results demonstrate the effectiveness of the proposed method. As compared with several state-of-the-art methods, the recognition accuracy shows the superiority of the proposed method, especially when the training dataset is limited.
C1 [Tian, Ye; Zhang, Liguo; Sun, Jianguo; Yin, Guisheng; Dong, Yuxin] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Harbin Engineering University
RP Dong, YX (corresponding author), Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
EM dongyuxin@hrbeu.edu.cn
RI LIGUO, ZHANG/AAC-8765-2021
FU Natural Science Foundation of Heilongjiang Province; China Postdoctoral
   Science Foundation [2019M661319]; Fundamental Research Fund for the
   Central Universities [3072021CF0609]
FX Our research fund is funded by Natural Science Foundation of
   Heilongjiang Province No.F2018006, China Postdoctoral Science Foundation
   No. 2019M661319, Fundamental Research Fund for the Central Universities
   (3072021CF0609).
CR Berthelot D, 2019, ADV NEUR IN, V32
   Berthelot David, 2019, ARXIV190502249, P2
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Donahue J, 2019, Advances in Neural Information Processing Systems
   El-Darymli K, 2013, J APPL REMOTE SENS, V7, DOI 10.1117/1.JRS.7.071598
   Gao F, 2019, IEEE ACCESS, V7, P108617, DOI 10.1109/ACCESS.2019.2933459
   Gao F, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060846
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Guo D., 2017, SAR IMAGE TARGET REC, P1
   Han D, 2015, J APPL REMOTE SENS, V9, DOI 10.1117/1.JRS.9.097294
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Ke ZH, 2019, IEEE I CONF COMP VIS, P6727, DOI 10.1109/ICCV.2019.00683
   Laine S., 2016, ARXIV161002242
   Lan RY, 2015, VISUAL COMPUT, V31, P35, DOI 10.1007/s00371-013-0902-5
   Lang HT, 2018, IEEE GEOSCI REMOTE S, V15, P439, DOI 10.1109/LGRS.2018.2792683
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Nair V., 2019, ARXIV191208766
   Qiao S., 2018, DEEP COTRAINING SEMI
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Sohn Kihyuk, 2020, Advances in Neural Information Processing Systems, P596, DOI DOI 10.48550/ARXIV.2001.07685
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tian ZZ, 2018, INT J REMOTE SENS, V39, P9249, DOI 10.1080/01431161.2018.1531317
   Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635
   Wagner S, 2015, 2015 3RD INTERNATIONAL WORKSHOP ON COMPRESSED SENSING THEORY AND ITS APPLICATION TO RADAR, SONAR, AND REMOTE SENSING (COSERA), P46, DOI 10.1109/CoSeRa.2015.7330261
   Wu YX, 2019, INT GEOSCI REMOTE SE, P2619, DOI [10.1109/igarss.2019.8897831, 10.1109/IGARSS.2019.8897831]
   Xie Q., 2020, ADV NEURAL INFORM PR, V33, P6256, DOI DOI 10.48550/ARXIV.1904.12848
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang JH, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010128
   Zhang J, 2019, VISUAL COMPUT, V35, P1181, DOI 10.1007/s00371-019-01667-w
NR 31
TC 6
Z9 6
U1 0
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4179
EP 4192
DI 10.1007/s00371-021-02287-z
EA AUG 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000690347400001
DA 2024-07-18
ER

PT J
AU Ohmori, K
AF Ohmori, Kenji
TI Formalization of Kublai Khan's globalization using Kunii's incrementally
   modular abstraction hierarchy
SO VISUAL COMPUTER
LA English
DT Article
DE Incrementally modular abstraction hierarchy; Homotopy; Attaching
   function; Category theory; Globalization; Kublai Khan
ID ARCHITECTURE; CYBERWORLDS
AB Using the design method proposed by Kunii, this study mathematically explains the achievement of Kublai of merging two different societies to build a globalized nation after conquering China. The considered design method, called incrementally modular abstraction hierarchy, consists of seven levels, from the most abstract, homotopy, to the most concrete, computer graphics. Moreover, it allows a flexible design while moving up and down its hierarchy. Using this method, we attempt to restore the concepts of Kublai, aiming to build a globalized state by combining nomadic and agricultural cultures. Specifically, we examine the use of two capitals to resolve the problem of different lifestyles of the Mongolian Khan, who moved between the summer and winter camps of the nomads, and the Chinese Emperor, who lived in a fixed palace of the agricultural people. By incorporating the category theory into Kunii's method, we discuss the adjunction of the Mongolian 1000-household social and military system based on decimal numbers with the Chinese bureaucratic system. The inclusion of the category theory in his original method allows visualization by programming, making the design more versatile and flexible.
C1 [Ohmori, Kenji] Hosei Univ, Fac Comp & Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.
C3 Hosei University
RP Ohmori, K (corresponding author), Hosei Univ, Fac Comp & Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.
EM ohmori@hosei.ac.jp
OI Ohmori, Kenji/0000-0003-2160-974X
CR [Anonymous], 1962, SAVAGE MIND
   [Anonymous], DISTINCTION SOCIAL C
   Foucault M., 2008, The Archaeology of Knowledge
   Furumatsu T, 2020, CONQUEST STEPPE GREA
   Kalemis D, TREES MONADS
   Kunii TL, 2005, 2005 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P3, DOI 10.1109/CW.2005.35
   Kunii TL, 2005, IEICE T INF SYST, VE88D, P790, DOI 10.1093/ietisy/e88-d.5.790
   Kunii TL, 2006, VISUAL COMPUT, V22, P949, DOI 10.1007/s00371-006-0040-4
   Kunii TL, 2006, COMPUT ANIMAT VIRT W, V17, P145, DOI 10.1002/cav.118
   Milewski B., 2019, Category Theory for Programmers
   Ohmori Kenji, 2012, Transactions on Computational Science XVI, P95, DOI 10.1007/978-3-642-32663-9_6
   Ohmori K, 2013, T COMPUTER SCI T COMPUTER SCI, P41
   Ohmori K, 2013, INT C CYBERWORLDS INT C CYBERWORLDS, V2013, P267
   Ohmori K., 2011, 2011 INT C FRONT ED 2011 INT C FRONT ED, P181
   Ohmori K, 2007, ICEIS 2007: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS, P437
   Ohmori K, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P100, DOI 10.1109/CW.2007.19
   Ohmori K, 2006, 2006 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P216, DOI 10.1109/CW.2006.14
   Ohmori K, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P198, DOI 10.1109/CW.2014.35
   Ohmori K, 2011, 2011 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P203, DOI 10.1109/CW.2011.14
   Ohmori K, 2010, VISUAL COMPUT, V26, P297, DOI 10.1007/s00371-010-0420-7
   Ohmori K, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P80, DOI 10.1109/CW.2009.20
   Ohmori K, 2008, 2008 FIRST IEEE INTERNATIONAL CONFERENCE ON UBI-MEDIA COMPUTING AND WORKSHOPS, PROCEEDINGS, P69, DOI 10.1109/UMEDIA.2008.4570868
   Ohmori K, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P363, DOI 10.1109/CW.2008.74
   Okada H, 2001, RISE FALL MONGOL EMP
   Okamoto T, 2020, READ CHINESE HIST CU
   Richter B., 2020, CATEGORIES HOMOTOPY, DOI [10.1017/9781108855891, DOI 10.1017/9781108855891]
   Shimada M, 2014, KHITAN DYNASTY NOMAD
   Shiraishi N, 2006, GENGHIS KHAN
   Sieradski A.J., 1992, INTRO TOPOLOGY HOMOT
   Spanier EH., 1994, Algebraic Topology, DOI 10.1007/978-1-4684-9322-1
   Spivak DI, 2014, CATEGORY THEORY FOR THE SCIENCES, P1
   Sugiyama M, 2021, KUBLAI KHANS CHALLEN
   Sugiyama M, 2014, WORLD GREAT MONGOL H
   Sugiyama M, 2021, RIDING STEPPE CONQUE
   Sugiyama M, 2016, MONGOL EMPIRE LONG A
   Todd E., 1999, The diversity of the world: family and modernity (La diversite du monde: famille et modernite)
   Todd E., 2011, ORIGIN FAMILY SYSTEM, V1
   Wittfogel KA, 2012, APS, V36, P907
NR 38
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2989
EP 2997
DI 10.1007/s00371-021-02234-y
EA AUG 2021
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000680796000001
DA 2024-07-18
ER

PT J
AU Li, XM
   Wu, GH
   Zhou, SB
   Lin, XR
   Li, X
AF Li, Xueming
   Wu, Guohao
   Zhou, Shangbo
   Lin, Xiaoran
   Li, Xu
TI Active instance segmentation with fractional-order network and
   reinforcement learning
SO VISUAL COMPUTER
LA English
DT Article
DE Reinforcement learning; Fractional-order network; Instance segmentation;
   Guiding strategy; Pixels-action policy
ID CHAOTIC SYNCHRONIZATION
AB In this paper, a novel model is proposed to segment image instance based on fractional-order chaotic synchronization network and reinforcement learning method. In the proposed model, fractional-order network is used for the preliminary image segmentation, which can obtain fine-grained information to provide a guiding strategy for the exploration of reinforcement learning; afterward, reinforcement learning method is committed to generate high-quality bounding contour curves for the object instances, which can combine the pixel features with local information in the image to improve the overall accuracy. Compared with other fractional-order models, the experimental results show that our proposed model achieves higher accuracy on the datasets of Pascal VOC2007 and Pascal VOC2012.
C1 [Li, Xueming; Wu, Guohao; Zhou, Shangbo; Li, Xu] Chongqing Univ, Sch Comp, Chongqing 400000, Peoples R China.
   [Lin, Xiaoran] Hebei Univ Econ & Business, Sch Informat & Technol, Hebei 050000, Peoples R China.
C3 Chongqing University; Hebei University of Economics & Business
RP Li, XM (corresponding author), Chongqing Univ, Sch Comp, Chongqing 400000, Peoples R China.
EM lixuemin@cqu.edu.cn
RI Li, Xue/HPE-4649-2023
CR Abbeel P., 2004, INT C MACH LEARN ICM
   Bai J, 2007, IEEE T IMAGE PROCESS, V16, P2492, DOI 10.1109/TIP.2007.904971
   Benicasa AX, 2016, NEUROCOMPUTING, V180, P35, DOI 10.1016/j.neucom.2015.10.111
   Breve FA, 2009, NEURAL NETWORKS, V22, P728, DOI 10.1016/j.neunet.2009.06.027
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chen K, 2002, NEURAL NETWORKS, V15, P423, DOI 10.1016/S0893-6080(02)00028-X
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Gondy LeroyA., 1993, Advances in Neural Information Processing Systems, V79, P937
   Han JW, 2018, PROC CVPR IEEE, P9080, DOI 10.1109/CVPR.2018.00946
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   HUNGENAHALLY S, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOLS 1-5, P4626, DOI 10.1109/ICSMC.1995.538525
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Krull A, 2017, PROC CVPR IEEE, P2566, DOI 10.1109/CVPR.2017.275
   Larochelle H., ADV NEURAL INFORM PR, P1243
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Lillicrap Timothy P, 2015, ARXIV150902971
   Lin XR, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20040251
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luciano L, 2019, VISUAL COMPUT, V35, P1171, DOI 10.1007/s00371-019-01668-9
   Mnih V, 2013, ARXIV
   Qiao YH, 2016, J INTEGR NEUROSCI, V15, P321, DOI 10.1142/S0219635216500205
   Quiles MG, 2011, NEURAL NETWORKS, V24, P54, DOI 10.1016/j.neunet.2010.09.002
   Saleem AB, 2017, NEURON, V93, P315, DOI 10.1016/j.neuron.2016.12.028
   WANG DL, 1995, IEEE T NEURAL NETWOR, V6, P283, DOI 10.1109/72.363423
   Zhao L, 2008, NEUROCOMPUTING, V71, P3360, DOI 10.1016/j.neucom.2008.02.024
   Zhao L, 2008, NEUROCOMPUTING, V71, P2761, DOI 10.1016/j.neucom.2007.09.011
NR 27
TC 1
Z9 1
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 3027
EP 3040
DI 10.1007/s00371-021-02174-7
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000679625200001
DA 2024-07-18
ER

PT J
AU Radwan, M
   Ohrhallinger, S
   Wimmer, M
AF Radwan, Mohamed
   Ohrhallinger, Stefan
   Wimmer, Michael
TI Fast occlusion-based point cloud exploration
SO VISUAL COMPUTER
LA English
DT Article
DE 3D navigation; Real-time processing; Occlusion
AB Large-scale unstructured point cloud scenes can be quickly visualized without prior reconstruction by utilizing levels-of-detail structures to load an appropriate subset from out-of-core storage for rendering the current view. However, as soon as we need structures within the point cloud, e.g., for interactions between objects, the construction of state-of-the-art data structures requires O(NlogN) time for N points, which is not feasible in real time for millions of points that are possibly updated in each frame. Therefore, we propose to use a surface representation structure which trades off the (here negligible) disadvantage of single-frame use for both output-dominated and near-linear construction time in practice, exploiting the inherent 2D property of sampled surfaces in 3D. This structure tightly encompasses the assumed surface of unstructured points in a set of bounding depth intervals for each cell of a discrete 2D grid. The sorted depth samples in the structure permit fast surface queries, and on top of that an occlusion graph for the scene comes almost for free. This graph enables novel real-time user operations such as revealing partially occluded objects, or scrolling through layers of occluding objects, e.g., walls in a building. As an example application we showcase a 3D scene exploration framework that enables fast, more sophisticated interactions with point clouds rendered in real time.
C1 [Radwan, Mohamed; Ohrhallinger, Stefan] TU Wien, Inst Visual Comp & Human Ctr Technol, Vienna, Austria.
   [Radwan, Mohamed; Ohrhallinger, Stefan] TU Wien, Vienna, Austria.
   [Wimmer, Michael] TU Wien, Inst Visual Comp & Human Ctr Technol, Rendering & Modeling Grp, Vienna, Austria.
   [Wimmer, Michael] TU Wien, Inst Visual Comp & Human Ctr Technol, Ctr Geometry & Computat Design, Vienna, Austria.
C3 Technische Universitat Wien; Technische Universitat Wien; Technische
   Universitat Wien; Technische Universitat Wien
RP Radwan, M (corresponding author), TU Wien, Inst Visual Comp & Human Ctr Technol, Vienna, Austria.; Radwan, M (corresponding author), TU Wien, Vienna, Austria.
EM mradwan@cg.tuwien.ac.at; ohrhallinger@cg.tuwien.ac.at;
   wimmer@cg.tuwien.ac.at
FU Austrian Science Fund (FWF) [P32418-N31]; Wiener Wissenschafts-,
   Forschungs- und Technologiefonds (WWTF) [ICT19-009]; Austrian Science
   Fund (FWF) [P32418] Funding Source: Austrian Science Fund (FWF)
FX This work has been partially funded by the Austrian Science Fund (FWF)
   project no. P32418-N31 and by the Wiener Wissenschafts-, Forschungs- und
   Technologiefonds (WWTF) project ICT19-009
CR [Anonymous], 2010, Thrust: A parallel template library
   Asafi S, 2013, COMPUT GRAPH FORUM, V32, P23, DOI 10.1111/cgf.12169
   Benger W., 2018, PROC COMPUT GRAPH IN, P267, DOI [10.1145/3208159.3208194, DOI 10.1145/3208159.3208194]
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1077, DOI 10.1109/TVCG.2006.140
   Calderon S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073714
   Carnecky R, 2013, IEEE T VIS COMPUT GR, V19, P838, DOI 10.1109/TVCG.2012.159
   Cashion J, 2012, IEEE T VIS COMPUT GR, V18, P634, DOI 10.1109/TVCG.2012.40
   Correa CD, 2007, IEEE T VIS COMPUT GR, V13, P1320, DOI 10.1109/TVCG.2007.70565
   Dachsbacher C, 2003, ACM T GRAPHIC, V22, P657, DOI 10.1145/882262.882321
   Deng ZJ, 2011, J COMPUT SCI TECH-CH, V26, P538, DOI 10.1007/s11390-011-1153-4
   Diepstraten J, 2002, COMPUT GRAPH FORUM, V21, P317, DOI 10.1111/1467-8659.t01-1-00591
   Eisemann E, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531389
   Elmqvist N, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P51
   Elvins T. T., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P21, DOI 10.1145/263407.263504
   Everitt C, 2001, INTERACTIVE ORDER IN
   Faure F., 2008, EG ASS
   Gobbetti E, 2004, COMPUT GRAPH-UK, V28, P815, DOI 10.1016/j.cag.2004.08.010
   Heidelberger B, 2003, VISION, MODELING, AND VISUALIZATION 2003, P461
   Heidelberger B., 2004, PROC WSCG, P145
   Hofmann N, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105781
   Hu Q., 2020, P IEEE C CVPR
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Kopper R., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P67, DOI 10.1109/3DUI.2011.5759219
   Mossel A, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P201, DOI 10.1145/2993369.2993384
   Ponto K, 2017, 2017 INTERNATIONAL CONFERENCE ON VIRTUAL REHABILITATION (ICVR)
   Qi CR, 2017, ADV NEUR IN, V30
   Qi Charles Ruizhongtai, 2016, PROC CVPR IEEE
   Radwan M., 2014, P 2014 GRAPH INT C, P25
   Rohlig M., 2016, P 9 INT S VICI VINCI, P51, DOI DOI 10.1145/2968220.2968230
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Sacht L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818093
   Sander PV, 2000, COMP GRAPH, P327, DOI 10.1145/344779.344935
   Schuetz M., 2016, Potree: Rendering Large Point Clouds in Web Browsers
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Stoakley R., 1995, P SIGCHI C HUM FACT, P265, DOI [10.1145/223904.223938, DOI 10.1145/223904.223938]
   Tredinnick R, 2016, P IEEE VIRT REAL ANN, P301, DOI 10.1109/VR.2016.7504773
   Vanacken L, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P115
   WIMMER M., 2006, Proceedings of Eurographics Symposium on Point-Based Graphics 2006, P129, DOI DOI 10.2312/SPBG/SPBG06/129-136
   Wu ML, 2016, IEEE T VIS COMPUT GR, V22, P1555, DOI 10.1109/TVCG.2015.2443804
   Xian CH, 2012, VISUAL COMPUT, V28, P21, DOI 10.1007/s00371-011-0595-6
   Xian CH, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P21, DOI 10.1109/SMI.2009.5170159
NR 41
TC 0
Z9 0
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2769
EP 2781
DI 10.1007/s00371-021-02243-x
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000678496800001
PM 34720293
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Li, X
   Fan, YY
   Lv, GY
   Ma, HY
AF Li, Xing
   Fan, Yangyu
   Lv, Guoyun
   Ma, Haoyue
TI Area-based correlation and non-local attention network for stereo
   matching
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo matching; Area-based correlation; Non-local attention module;
   Stacked hourglass network
ID DEPTH
AB Stereo matching plays an essential role in various computer vision applications. Cost volume is the crucial part in disparity estimation for measuring the similarity between the left-right feature locations. However, most previous cost volume construction based on concatenation or pixel-wise correlation lack of local similarity, leads to an unsatisfactory performance on the large textureless regions. We propose a simple but efficient method for stereo matching to tackle the problem, called area-based correlation and non-local attention network (Abc-Net). First, we exploit the area-based correlation to capture more local similarity in cost volume. The left-right features are sliced into various size patches along the channel dimension. Correlation maps are calculated between the left feature patches and corresponding traversed right patches and then pack them into a 4D area-based cost volume. Second, based on the hourglass module, we combined it with the non-local attention module as the 3D feature matching module, which exploits various spatial relationships and global information. The experiments show that (1) the area-based correlation can capture local similarity to increase accuracy on the large textureless region, (2) the improved 3D feature matching module can exploit global context information to further improve performance, (3) our method achieves competitive results on the SceneFlow, KITTI 2012, and KITTI 2015 datasets.
C1 [Li, Xing; Fan, Yangyu; Lv, Guoyun; Ma, Haoyue] Northwestern Polytech Univ, Xian 710129, Peoples R China.
C3 Northwestern Polytechnical University
RP Li, X; Lv, GY (corresponding author), Northwestern Polytech Univ, Xian 710129, Peoples R China.
EM lixinger@mail.nwpu.edu.cn; lvguoyun101@nwpu.edu.cn
OI Xing, Li/0000-0003-2193-0437
FU theKey Program of Research and Development Plan of Shaanxi Province
   [2020ZDLGY0409]; Department of Science and Technology of Shaanxi
   Province
FX Thisworkwas supported by theKey Program of Research and Development Plan
   of Shaanxi Province under Grant 2020ZDLGY0409 funded by the Department
   of Science and Technology of Shaanxi Province.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2015, PROC CVPR IEEE
   Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen XZ, 2015, ADV NEUR IN, V28
   Chen X, 2020, ANN OPER RES, DOI 10.1007/s10479-020-03621-9
   Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI 10.1109/TPAMI.2019.2947374
   Du X., 2019, 190409099 ARXIV
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Fan R, 2018, IEEE T IMAGE PROCESS, V27, P3025, DOI 10.1109/TIP.2018.2808770
   Geiger A., 2012, CVPR
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Hamzah RA, 2010, INT CONF COMP SCI, P652, DOI 10.1109/ICCSIT.2010.5565062
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Lee S, 2014, VISUAL COMPUT, V30, P455, DOI 10.1007/s00371-013-0868-3
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Li YJ, 2019, VISUAL COMPUT, V35, P257, DOI 10.1007/s00371-018-1491-0
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   Liu R., 2020, P IEEE CVF C COMP VI, P12757
   LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   MENZE M, 2015, PROC CVPR IEEE, P3061, DOI DOI 10.1109/CVPR.2015.7298925
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Nie G., 2019, 2019 IEEE INT C COMP, P3283
   Rao Z., 2020, IEEE T GEOSCI REMOTE, P1
   Rao ZB, 2022, VISUAL COMPUT, V38, P77, DOI 10.1007/s00371-020-02001-5
   Rao ZB, 2019, ASIAPAC SIGN INFO PR, P578, DOI 10.1109/APSIPAASC47483.2019.9023237
   Ren H., 2020, IEEE C COMPUTER VISI, P750
   Ren HY, 2020, IEEE IMAGE PROC, P2760, DOI 10.1109/ICIP40778.2020.9191126
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Song X, 2020, INT J COMPUT VISION, V128, P910, DOI 10.1007/s11263-019-01287-w
   Tang YY, 2019, LECT NOTES COMPUT SC, V11132, P219, DOI 10.1007/978-3-030-11018-5_20
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Wang J., 2020, 200811098 ARXIV
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Woodford O, 2009, IEEE T PATTERN ANAL, V31, P2115, DOI 10.1109/TPAMI.2009.131
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang CL, 2019, VISUAL COMPUT, V35, P473, DOI 10.1007/s00371-018-1475-0
   Yang GS, 2020, PROC CVPR IEEE, P1331, DOI 10.1109/CVPR42600.2020.00141
   Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   Zhang C, 2015, IEEE I CONF COMP VIS, P2057, DOI 10.1109/ICCV.2015.238
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926
   Zhao H.-H., 2019, VISUAL COMPUT, V36, P1
   Zinner C, 2008, LECT NOTES COMPUT SC, V5358, P216, DOI 10.1007/978-3-540-89639-5_21
NR 57
TC 13
Z9 13
U1 2
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3881
EP 3895
DI 10.1007/s00371-021-02228-w
EA JUL 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000674546600003
DA 2024-07-18
ER

PT J
AU Zhao, HS
   Yang, DD
   Yu, JK
AF Zhao, Hanshuo
   Yang, Dedong
   Yu, Jiankang
TI 3D target detection using dual domain attention and SIFT operator in
   indoor scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; 3D point cloud; SIFT; Attention mechanism
AB In a large number of real-life scenes and practical applications, 3D object detection is playing an increasingly important role. We need to estimate the position and direction of the 3D object in the real scene to complete the 3D object detection task. In this paper, we propose a new network architecture based on VoteNet to detect 3D point cloud targets. On the one hand, we use channel and spatial dual-domain attention module to enhance the features of the object to be detected while suppressing other useless features. On the other hand, the SIFT operator has scale invariance and the ability to resist occlusion and background interference. The PointSIFT module we use can capture information in different directions of point cloud in space, and is robust to shapes of different proportions, so as to better detect objects that are partially occluded. Our method is evaluated on the SUN-RGBD and ScanNet datasets of indoor scenes. The experimental results show that our method has better performance than VoteNet.
C1 [Zhao, Hanshuo; Yang, Dedong; Yu, Jiankang] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
C3 Hebei University of Technology
RP Yang, DD (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
EM 1637995529@qq.com; dedongyang@hebut.edu.cn
OI Yang, Dedong/0000-0001-7950-6810
CR Chen X., 2017, PROC CVPR IEEE, V1, P3, DOI DOI 10.1109/CVPR.2017.691
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dyer C., 2016, ARXIV160207776, DOI DOI 10.18653/V1/N16-1024
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang M., 2018, SIFT LIKE NETWORK MO
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   REN Z, 2016, PROC CVPR IEEE, P1525, DOI DOI 10.1109/CVPR.2016.169
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Scannet, 2017, 2017 IEEE C COMP VIS
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Simon M., 2018, ARXIV180306199
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Zhang, 2019, P 27 ACM INT C MULT
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 28
TC 6
Z9 7
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3765
EP 3774
DI 10.1007/s00371-021-02217-z
EA JUN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000667600400002
DA 2024-07-18
ER

PT J
AU Bouguezzi, S
   Ben Fredj, H
   Faiedh, H
   Souani, C
AF Bouguezzi, Safa
   Ben Fredj, Hana
   Faiedh, Hassene
   Souani, Chokri
TI Improved architecture for traffic sign recognition using a
   self-regularized activation function: SigmaH
SO VISUAL COMPUTER
LA English
DT Article
DE Advanced driver assistance systems; Optimization; Convolutional neural
   network; Traffic sign recognition (TSR); Benchmark (GTSRB); Activation
   function
AB Traffic sign recognition (TSR) is a crucial intelligent transport system. Nowadays, the convolutional neural network has become a vital tool in the conception of a TSR model. In this work, we propose an improved TSR algorithm for the transportation system inspired by the classical model LeNet-5. In this model, firstly, we replace the hyperbolic tangent activation function with a self-regularized non-monotonic activation function called SigmaH (SigmaH(x) = x tanh(root sigma(x))). SigmaH is experimentally validated on various popular benchmarks against the most suitable combinations of architectures and activation functions. Secondly, we use a convolutional block attention module, which is beneficial for extracting the most valuable features using the attention method. Finally, we combine the triplet-center loss with the Softmax activation function as a loss function to maximize the correct recognition rate. The TSR experiments are carried out based on the German Traffic Sign Recognition benchmark. The experimental results demonstrate that the improved LeNet-5 has an identification accuracy rate of 98.25%, and the average processing time per frame is 8 ms. In the meantime, the number of parameters is reduced by more than 51% compared with the classic LeNet-5 model. Our proposed model has remarkable accuracy and high training efficiency compared with other algorithms.
C1 [Bouguezzi, Safa; Ben Fredj, Hana] Univ Monastir, Fac Sci Monastir, Monastir 5000, Tunisia.
   [Faiedh, Hassene; Souani, Chokri] Univ Sousse, Higher Inst Appl Sci & Technol, Sousse 4003, Tunisia.
C3 Universite de Monastir; Universite de Sousse
RP Bouguezzi, S (corresponding author), Univ Monastir, Fac Sci Monastir, Monastir 5000, Tunisia.
EM safa_bouguezzi@outlook.com
RI souani, chokri/B-1853-2015
OI souani, chokri/0000-0002-8987-3582; bouguezzi, safa/0000-0001-5107-7508
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Alcaide E, 2018, ARXIV180107145
   Andreev S, 2019, IEEE COMMUN MAG, V57, P34, DOI 10.1109/MCOM.2019.1800226
   Bouti A, 2020, SOFT COMPUT, V24, P6721, DOI 10.1007/s00500-019-04307-6
   Chollet Francois, 2018, Keras: the Python Deep Learning Library
   Deepika, 2018, 2018 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2206, DOI 10.1109/ICACCI.2018.8554624
   Gonzalez-Reyna SE, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/364305
   Fan XM, 2020, VISUAL COMPUT, V36, P1203, DOI 10.1007/s00371-019-01732-4
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li H, 2018, ADV NEUR IN, V31
   Li WL, 2019, ICVIP 2019: PROCEEDINGS OF 2019 3RD INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING, P13, DOI 10.1145/3376067.3376102
   Liu XY, 2021, IET COMPUT VIS, V15, P136, DOI 10.1049/cvi2.12020
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Misra D., 2019, ARXIV190808681
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Olaverri-Monreal C., 2017, SMART CITIES GREEN T, P318
   Pan Y., 2020, COGNITIVE INFORMATIC, P319
   Paszke A, 2019, ADV NEUR IN, V32
   Peng Xishuai., 2017, Proceedings of the 2017 IEEE Symposium Series on Computational Intelligence (SSCI), P1
   Radu MD, 2020, INT C ELECT COMPUT, DOI 10.1109/ecai50035.2020.9223186
   Ramachandran P., 2017, Searching for activation functions
   Scheidegger F, 2021, VISUAL COMPUT, V37, P1593, DOI 10.1007/s00371-020-01922-5
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Sun W, 2019, CMC-COMPUT MATER CON, V60, P147, DOI 10.32604/cmc.2019.03581
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan M., 2020, INT C MACH LEARN, DOI DOI 10.48550/ARXIV.1905.11946
   Wang GY, 2014, VISUAL COMPUT, V30, P539, DOI 10.1007/s00371-013-0879-0
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang Y, 2016, IEEE T INTELL TRANSP, V17, P2022, DOI 10.1109/TITS.2015.2482461
   Yi Luo, 2018, CICTP 2017. Transportation Reform and Change-Equity, Inclusiveness, Sharing, and Innovation. Proceedings of the 17th Cota International Conference of Transportation Professionals, P4197
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zheng QM, 2019, IEEE ACCESS, V7, P151359, DOI 10.1109/ACCESS.2019.2948112
NR 44
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3747
EP 3764
DI 10.1007/s00371-021-02211-5
EA JUN 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000667441600002
DA 2024-07-18
ER

PT J
AU Lu, YF
   Fu, SY
   Zhang, XH
   Xie, N
AF Lu, Yifan
   Fu, Siyuan
   Zhang, Xiao Hua
   Xie, Ning
TI Denoising Monte Carlo renderings via a multi-scale featured
   dual-residual GAN
SO VISUAL COMPUTER
LA English
DT Article
DE Denoising Monte Carlo renderings; Generative adversarial networks;
   Multi-scale auxiliary features; Dual residual connections
AB Monte Carlo (MC) path tracing causes a lot of noise on the rendered image at a low samples per pixel. Recently, with the help of inexpensive auxiliary buffers and the generative adversarial network (GAN), deep learning-based denoising MC rendering methods have been able to generate noise-free images with high perceptual quality in seconds. In this paper, we propose a novel GAN structure for denoising Monte Carlo renderings, called dual residual connection GAN. Our key insight is that the dual residual connections can improve the chance of the optimal feature selection and implicitly increase the number of potential interactions between modules. We also propose a multi-scale auxiliary features extraction method, aiming to make full use of the rich geometry and texture information of auxiliary buffers. Moreover, we adopt a spatial-adaptive block with the deformable convolution to help the network adapt to the variance in spatial texture and edge features. Compared with the state-of-the-art methods, our network has fewer parameters and less inference time, and the results surpass the previous in terms of visual effects and quantitative metrics.
C1 [Lu, Yifan; Fu, Siyuan; Xie, Ning] Univ Elect Sci & Technol China UESTC, Ctr Future Media, Dept Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhang, Xiao Hua] Hiroshima Inst Technol, Hiroshima, Japan.
C3 University of Electronic Science & Technology of China; Hiroshima
   Institute of Technology
RP Xie, N (corresponding author), Univ Elect Sci & Technol China UESTC, Ctr Future Media, Dept Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM luyifan0821@gmail.com; AlexHuku@hotmail.com; zhxh@cc.it-hiroshima.ac.jp;
   seanxiening@gmail.com
OI Lu, Yifan/0000-0001-6184-6200
FU National Nature Science Foundation of China [61602088]; Sichuan
   Provincial NSFC [2018JY0528]; Fundamental Research Funds for the Central
   Universities [Y03019023601008011]; interactive Technology Research Fund
   of the Research Center for Interactive Technology Industry, School of
   Economics and Management, Tsinghua University [RCITI2021T006]; TiMi L1
   Studio of Tencent corporation
FX This work is part of the research supported by the National Nature
   Science Foundation of China under Grant No. 61602088, No. This work is
   part of the research supported by the Sichuan Provincial NSFC (No.
   2018JY0528), the Fundamental Research Funds for the Central Universities
   No. Y03019023601008011, the interactive Technology Research Fund of the
   Research Center for Interactive Technology Industry, School of Economics
   and Management, Tsinghua University (No. RCITI2021T006) and sponsored by
   TiMi L1 Studio of Tencent corporation.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Bitterli B., 2016, RENDERING RESOURCES
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Chaitanya CRA, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073601
   Chang M., 2020, EUR C COMP VIS, P171
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gharbi M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322954
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356538
   Hasselgren J, 2020, COMPUT GRAPH FORUM, V39, P147, DOI 10.1111/cgf.13919
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Keller Alexander, 2015, ACM SIGGRAPH 2015 CO, P1
   Kettunen M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323038
   Kingma D, 2014, ICLR P, V2014, P1
   Kuznetsov A, 2018, COMPUT GRAPH FORUM, V37, P35, DOI 10.1111/cgf.13473
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Lu Y., 2020, SIGGRAPH ASIA 2020 T, P1, DOI DOI 10.1145/3410700
   Meng Xiaoxu, 2020, Real-time Monte Carlo Denoising with the Neural Bilateral Grid
   Munkberg J, 2020, COMPUT GRAPH FORUM, V39, P1, DOI 10.1111/cgf.14049
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vicini D, 2019, COMPUT GRAPH FORUM, V38, P316, DOI 10.1111/cgf.13533
   Vogels T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201388
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wong KM, 2019, COMPUT VIS MEDIA, V5, P239, DOI 10.1007/s41095-019-0142-3
   Xu B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356547
   Yang X, 2019, J COMPUT SCI TECH-CH, V34, P1123, DOI 10.1007/s11390-019-1964-2
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 38
TC 6
Z9 6
U1 3
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2513
EP 2525
DI 10.1007/s00371-021-02204-4
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000663489600001
DA 2024-07-18
ER

PT J
AU Hepperle, D
   Purps, CF
   Deuchler, J
   Woelfel, M
AF Hepperle, Daniel
   Purps, Christian Felix
   Deuchler, Jonas
   Woelfel, Matthias
TI Aspects of visual avatar appearance: self-representation, display type,
   and uncanny valley
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Avatar customization; Nonverbal communication;
   Head-mounted displays; Social networks
ID VIRTUAL-REALITY; CHARACTERS; BEHAVIOR
AB The visual representation of human-like entities in virtual worlds is becoming a very important aspect as virtual reality becomes more and more "social". The visual representation of a character's resemblance to a real person and the emotional response to it, as well as the expectations raised, have been a topic of discussion for several decades and have been debated by scientists from different disciplines. But as with any new technology, the findings may need to be reevaluated and adapted to new modalities. In this context, we make two contributions which may have implications for how avatars should be represented in social virtual reality applications. First, we determine how default and customized characters of current social virtual reality platforms appear in terms of human likeness, eeriness, and likability, and whether there is a clear resemblance to a given person. It can be concluded that the investigated platforms vary strongly in their representation of avatars. Common to all is that a clear resemblance does not exist. Second, we show that the uncanny valley effect is also present in head-mounted displays, but-compared to 2D monitors-even more pronounced.
C1 [Hepperle, Daniel; Purps, Christian Felix; Deuchler, Jonas; Woelfel, Matthias] Karlsruhe Univ Appl Sci, Fac Comp Sci & Business Informat Syst, Karlsruhe, Germany.
   [Hepperle, Daniel; Woelfel, Matthias] Univ Hohenheim, Fac Business Econ & Social Sci, Stuttgart, Germany.
C3 Karlsruhe University of Applied Sciences; University Hohenheim
RP Hepperle, D (corresponding author), Karlsruhe Univ Appl Sci, Fac Comp Sci & Business Informat Syst, Karlsruhe, Germany.; Hepperle, D (corresponding author), Univ Hohenheim, Fac Business Econ & Social Sci, Stuttgart, Germany.
EM daniel.hepperle@h-ka.de; christian_felix.purps@h-ka.de;
   jonasdeuchler@gmail.com; matthias.woelfel@h-ka.de
OI Hepperle, Daniel/0000-0002-3556-5867
FU Carl Zeiss Foundation
FX The Unity program has been developed with the help of Benjamin
   Herzberger. Hannah odell has contributed to the paper on which this
   extended version is based on. This work was partly funded by the Carl
   Zeiss Foundation.
CR ARGYLE M, 1965, SOCIOMETRY, V28, P289, DOI 10.2307/2786027
   Argyle Michael, 2013, Bodily communication
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2004, PRESENCE-VIRTUAL AUG, V13, P428, DOI 10.1162/1054746041944803
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Bekkering E, 2006, COMMUN ACM, V49, P103, DOI 10.1145/1139922.1139925
   Bente G, 2008, HUM COMMUN RES, V34, P287, DOI 10.1111/j.1468-2958.2008.00322.x
   Chattopadhyay D, 2016, J VISION, V16, DOI 10.1167/16.11.7
   Choi KS, 2003, ITRE2003: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: RESEARCH AND EDUCATION, P207, DOI 10.1109/ITRE.2003.1270604
   CLARK HH, 1991, PERSPECTIVES ON SOCIALLY SHARED COGNITION, P127, DOI 10.1037/10096-006
   Dill Vanderson, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P511, DOI 10.1007/978-3-642-33197-8_62
   Edward, 2007, DIGRA P 2007 DIGRA I
   Garau M., 2003, IMPACT AVATAR FIDELI
   Greenhalgh C, 1997, BT TECHNOL J, V15, P101, DOI 10.1023/A:1018635613702
   HEPPERLE D, 2020, 2020 INT C CYBERWORL
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   Kätsyri J, 2017, INT J HUM-COMPUT ST, V97, P149, DOI 10.1016/j.ijhcs.2016.09.010
   Kim K, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P143, DOI 10.1109/VR.2012.6180922
   Kolesnichenko A, 2019, PROCEEDINGS OF THE 2019 ACM DESIGNING INTERACTIVE SYSTEMS CONFERENCE (DIS 2019), P241, DOI 10.1145/3322276.3322352
   LATOSCHIK ME, 2017, VRST17 P 23 ACM S
   Lugrin J.-L., 2015, ICAT EGVE 2015 INT C, P1, DOI DOI 10.2312/EGVE.20151303
   MacDorman K.F., 2006, SUBJECTIVE RATINGS R
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   Maloney Divine, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3415246
   Manaf A. A. A., 2019, INT J APPL CREAT ART, V2, P7, DOI [10.33736/ijaca.1575.2019, DOI 10.33736/IJACA.1575.2019]
   Mansour S., 2006, WSEAS Transactions on Communications, V5, P1501
   Mathur MB, 2016, COGNITION, V146, P22, DOI 10.1016/j.cognition.2015.09.008
   McMahan Ryan P., 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P59, DOI 10.1007/978-3-319-39907-2_6
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Riva G, 2007, CYBERPSYCHOL BEHAV, V10, P45, DOI 10.1089/cpb.2006.9993
   Ruddle RA, 2004, INT J HUM-COMPUT ST, V60, P299, DOI 10.1016/j.ijhcs.2003.10.001
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Schwind V, 2018, INT J HUM-COMPUT ST, V111, P49, DOI 10.1016/j.ijhcs.2017.11.003
   Schwind V, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1577, DOI 10.1145/3025453.3025602
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Sousa Santos B, 2009, MULTIMED TOOLS APPL, V41, P161, DOI 10.1007/s11042-008-0223-2
   Susindar Sahinya, 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P252, DOI 10.1177/1071181319631509
   Tanenbaum TJ, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376606
   Tinwell A, 2010, J GAMING VIRTUAL WOR, V2, P3, DOI 10.1386/jgvw.2.1.3_1
   Wainfan L., 2004, CHALLENGES VIRTUAL C
   Weidner F, 2017, P IEEE VIRT REAL ANN, P281, DOI 10.1109/VR.2017.7892286
   Yee N, 2007, HUM COMMUN RES, V33, P271, DOI 10.1111/j.1468-2958.2007.00299.x
NR 43
TC 26
Z9 28
U1 4
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1227
EP 1244
DI 10.1007/s00371-021-02151-0
EA JUN 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0D9DX
UT WOS:000662812100002
PM 34177022
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Damiand, G
   Coeurjolly, D
   Bourquat, P
AF Damiand, Guillaume
   Coeurjolly, David
   Bourquat, Pierre
TI Stripped halfedge data structure for parallel computation of
   arrangements of segments
SO VISUAL COMPUTER
LA English
DT Article
DE Arrangement of segments; Parallel algorithm; Out-of-core construction;
   Halfedge data structure
ID INTERSECTING LINE SEGMENTS; ALGORITHMS
AB Computing an arrangement of segments with some geometrical and topological guarantees is a critical step in many geometry processing applications. In this paper, we propose a method to efficiently compute arrangements of segments using a strip-based data structure. Thanks to this new data structure, the arrangement computation algorithm can easily be parallelized as the per strip computations are independent. Another interest of our approach is that we can propose an out-of-core and streamed construction for large datasets, while keeping a low memory footprint. We prove the correctness of our structure and provide a complete comparative evaluation with respect to state-of-the-art demonstrating the interest of our construction for the computation of an exact arrangement.
C1 [Damiand, Guillaume; Coeurjolly, David; Bourquat, Pierre] Univ Lyon, UCBL, LIRIS, INSALyon,CNRS,UMR5205, F-69622 Lyon, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Claude
   Bernard Lyon 1; Institut National des Sciences Appliquees de Lyon - INSA
   Lyon
RP Damiand, G (corresponding author), Univ Lyon, UCBL, LIRIS, INSALyon,CNRS,UMR5205, F-69622 Lyon, France.
EM guillaume.damiand@liris.cnrs.fr
RI Damiand, Guillaume/IAM-8662-2023
OI Damiand, Guillaume/0000-0003-1580-5517
CR Agarwal PK, 2000, HANDBOOK OF COMPUTATIONAL GEOMETRY, P49, DOI 10.1016/B978-044482537-7/50003-6
   AGGARWAL A, 1988, ALGORITHMICA, V3, P293, DOI 10.1007/BF01762120
   Anderson R, 1996, ALGORITHMICA, V15, P104, DOI 10.1007/BF01941684
   [Anonymous], 2016, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences
   [Anonymous], 1998, Algorithmic geometry
   Atallah M.J., 1986, P 2 ANN S COMP GEOM, P216
   Balaban I. J., 1995, Proceedings of the Eleventh Annual Symposium on Computational Geometry, P211, DOI 10.1145/220279.220302
   BAUMGART B., 1975, NATL COMPUTER C AFIP, P589, DOI DOI 10.1145/1499949.1500071
   BENTLEY JL, 1979, IEEE T COMPUT, V28, P643, DOI 10.1109/TC.1979.1675432
   Berg M., 2008, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-3-540-77974-2
   Boissonnat J.D., 2006, Effective computational geometry for curves and surfaces
   Botsch M., 2010, Polygon Mesh Processing
   Bronnimann H., 2020, CGAL User and Reference Manual, V5.2
   CHAZELLE B, 1992, J ACM, V39, P1, DOI 10.1145/147508.147511
   Damiand G, 2014, Combinatorial maps: efficient data structures for computer graphics and image processing
   Devillers O, 2002, COMP GEOM-THEOR APPL, V22, P119, DOI 10.1016/S0925-7721(01)00050-5
   Edelsbrunner H., 1987, Algorithms in Combinatorial Geometry
   Fogel E., 2012, GEOMETRY COMPUTING, V7
   Goodrich MT, 1996, ALGORITHMICA, V15, P126, DOI 10.1007/BF01941685
   GOODRICH MT, 1991, SIAM J COMPUT, V20, P737, DOI 10.1137/0220047
   Grunbaum B., 1967, CONVEX POLYTOPES
   Gryaditskaya Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356533
   Hershberger J, 2011, COMPUTATIONAL GEOMETRY (SCG 11), P197
   Hoffmann C., 1989, Geometric and Solid Modeling: An Introduction
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   McKenney M., 2009, Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems GIS, V09, P392, DOI [10.1145/1653771.1653827, DOI 10.1145/1653771.1653827]
   McKenney M, 2017, GEOINFORMATICA, V21, P151, DOI 10.1007/s10707-016-0277-7
   Muller D. E., 1978, Theoretical Computer Science, V7, P217, DOI 10.1016/0304-3975(78)90051-8
   Pion S, 2011, SCI COMPUT PROGRAM, V76, P307, DOI 10.1016/j.scico.2010.09.003
   Rossignac J, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P278
   Shewchuk J. R., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P141, DOI 10.1145/237218.237337
   Sieger Daniel., 2012, Proceedings of the 20th International Meshing Roundtable, P533
   The CGAL Project, 2020, CGAL User and Reference Manual
   WEILER K, 1985, IEEE COMPUT GRAPH, V5, P21, DOI 10.1109/MCG.1985.276271
   Wein Ron, 2020, CGAL USER REFERENCE
   Wolff, 2017, HEAPTRACK HEAP MEMOR
NR 36
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2461
EP 2472
DI 10.1007/s00371-021-02185-4
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000662131200001
DA 2024-07-18
ER

PT J
AU Li, DL
   Jahan, H
   Huang, XY
   Feng, ZL
AF Li, Donglu
   Jahan, Hosney
   Huang, Xiaoyi
   Feng, Ziliang
TI Human action recognition method based on historical point cloud
   trajectory characteristics
SO VISUAL COMPUTER
LA English
DT Article
DE Human behavior recognition; Depth map; Time pyramid; Three-dimensional
   point cloud; Skeletal information
AB Human behavior recognition is a research hot spot in the field of computer vision. However, due to the interference of a complex environment and the diversity of behavior itself, it is difficult to extract suitable behavior features for behavior recognition, thus increasing the difficulty of recognition. In this paper, we propose a new behavior feature called the historical point cloud track feature which includes depth information and skeleton information obtained by Kinect, to solve the problem that the existing methods lose time series and spatial information. We reconstruct a three-dimensional point cloud by using depth information and then keep the 3D point cloud near limbs overlapping with skeleton information; then, we obtain the corresponding features. This feature set can well represent the spatial distribution of the point cloud of the main moving parts of the body. In addition, we use the time pyramid to segment the time series at different scales and splice the characteristics of each time period to strengthen the order relationship for the associated behavior. Finally, a support vector machine is used for training and recognition. Several groups of comparative experiments on the UTD-MHAD dataset show that the recognition method of human behavior based on historical point cloud trajectory features is very effective, and the recognition rate reaches 90.23%, which is higher than that of similar comparison algorithms.
C1 [Li, Donglu; Jahan, Hosney; Huang, Xiaoyi; Feng, Ziliang] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
C3 Sichuan University
RP Li, DL (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
EM 3227908281@qq.com; hosney.hstu08@gmail.com; 1157149161@qq.com;
   fengziliang@scu.edu.cn
RI huang, xiaoyi/ITU-8606-2023
FU National Natural Science Foundation of China [U1833115]; Civil Aviation
   Administration of China [U1833115]
FX This work is partially supported by the Joint Funds of National Natural
   Science Foundation of China and Civil Aviation Administration of China
   (U1833115).
CR Bulbul MF, 2015, INT J MULTIMED DATA, V6, P23, DOI 10.4018/IJMDEM.2015100102
   Chen C, 2015, LECT NOTES COMPUT SC, V9474, P613, DOI 10.1007/978-3-319-27857-5_55
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen C, 2015, IEEE WINT CONF APPL, P1092, DOI 10.1109/WACV.2015.150
   Chen C, 2014, IEEE ENG MED BIO, P4983, DOI 10.1109/EMBC.2014.6944743
   Chen C, 2016, J REAL-TIME IMAGE PR, V12, P155, DOI 10.1007/s11554-013-0370-1
   Collins RT, 2000, IEEE T PATTERN ANAL, V22, P745, DOI 10.1109/TPAMI.2000.868676
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ji YQ., 2009, RES HUMAN BEHAV UNDE
   Kirkman B. L., 2017, 3D TEAM LEADERSHIP N
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Li Yuanxiang, 2020, Computer Engineering and Applications, V56, P194, DOI 10.3778/j.issn.1002-8331.1810-0300
   Liu Junjie, 2019, CVPR WORKSH
   Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339
   Shao ZP, 2018, IEEE INT CONF ROBOT, P1978
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Stehman SV, 1997, REMOTE SENS ENVIRON, V62, P77, DOI 10.1016/S0034-4257(97)00083-7
   Tao, 2014, COMPUT KNOWL TECHNOL, V6, P1287
   Nguyen TP, 2013, IEEE IMAGE PROC, P4354, DOI 10.1109/ICIP.2013.6738897
   Vapnik V., 2013, The nature of statistical learning theory
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   [王素玉 WANG Suyu], 2007, [中国图象图形学报, Journal of Image and Graphics], V12, P1505
   Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005
   Wang Y, 2020, NEUROCOMPUTING, V385, P340, DOI 10.1016/j.neucom.2019.10.068
   Wang Y, 2019, J INF SECUR APPL, V49, DOI 10.1016/j.jisa.2019.102399
   Yang WD., 2016, RES HUMAN BEHAV RECO, P1
   Yang X., 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yin Jian-qin, 2011, Journal of Sichuan University, V43, P101
   Yonghong Hou, 2018, IEEE Transactions on Circuits and Systems for Video Technology, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Zan BF, 2018, LASER OPTOELECTRON P, V55, DOI 10.3788/LOP55.011010
   Zhang BC, 2017, IEEE T IMAGE PROCESS, V26, P4648, DOI 10.1109/TIP.2017.2718189
   Zhang JH, 2019, INFORM SCIENCES, V486, P88, DOI 10.1016/j.ins.2019.02.024
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
NR 40
TC 4
Z9 4
U1 3
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2971
EP 2979
DI 10.1007/s00371-021-02167-6
EA JUN 2021
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000657199700001
DA 2024-07-18
ER

PT J
AU Zhao, T
   Pan, SG
   Gao, W
   Sheng, C
   Sun, YC
   Wei, JS
AF Zhao, Tao
   Pan, Shuguo
   Gao, Wang
   Sheng, Chao
   Sun, Yingchun
   Wei, Jiansheng
TI Attention Unet plus plus for lightweight depth estimation from sparse
   depth samples and a single RGB image
SO VISUAL COMPUTER
LA English
DT Article
DE Deeply supervised; Depth estimation; Lightweight; Self-attention; Skip
   connection; Sparse samples
ID PREDICTION; FUSION
AB Depth estimation from a single RGB image with sparse depth measurements has already been proved to be an effective way of predicting dense and high-precision depth maps. However, most of its networks are based on comparatively complex and fixed architectures that are too slow and inflexible to pursue the maximum task performance for various conditions. Addressing this problem, we proposed a flexible and lightweight network architecture that can be split into a series of sub-networks with different accuracy and parameter size in use. We verified our proposed method's effectiveness on NYUv2 and KITTI Odometry datasets. The results show that it is possible to achieve approximate accuracy as prior works but at the number of parameters that are an order of magnitude smaller. Our methodology's most efficient sub-network performs the best for balancing the computation and accuracy with only no more than 1 M parameters.
C1 [Zhao, Tao; Pan, Shuguo; Gao, Wang; Sheng, Chao; Sun, Yingchun; Wei, Jiansheng] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
C3 Southeast University - China
RP Zhao, T; Pan, SG (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM zhaotao@seu.edu.cn; psg@seu.edu.cn
RI chen, yijia/KGM-4378-2024; SUN, Ye/KBC-8159-2024; Yu, ZH/KBC-6889-2024
FU National Natural Science Foundation of China [41774027]; Fundamental
   Research Funds for the Central Universities [2242020R40135]; Foundation
   of Key Laboratory of Micro-Inertial Instrument and Advanced Navigation
   Technology, Ministry of Education, China [SEU-MIAN-201801]
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant No. 41774027), the Fundamental Research Funds
   for the Central Universities (Grant No. 2242020R40135) and the
   Foundation of Key Laboratory of Micro-Inertial Instrument and Advanced
   Navigation Technology, Ministry of Education, China (Grant No.
   SEU-MIAN-201801).
CR [Anonymous], 2017, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2017.683
   Bui G, 2018, VISUAL COMPUT, V34, P829, DOI 10.1007/s00371-018-1550-6
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen Z, 2018, LECT NOTES COMPUT SC, V11208, P176, DOI 10.1007/978-3-030-01225-0_11
   Cheng XJ, 2018, LECT NOTES COMPUT SC, V11220, P108, DOI 10.1007/978-3-030-01270-0_7
   Cheng XJ, 2020, AAAI CONF ARTIF INTE, V34, P10615
   DROZDZAL M, 2016, ARXIV160804117CS
   Fu C, 2019, IEEE INT C INTELL TR, P273, DOI [10.1109/ITSC.2019.8917201, 10.1109/itsc.2019.8917201]
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guerrero P, 2018, VISUAL COMPUT, V34, P1165, DOI 10.1007/s00371-018-1551-5
   Hambarde P, 2020, IEEE T COMPUT IMAG, V6, P806, DOI 10.1109/TCI.2020.2981761
   Hawe S, 2011, IEEE I CONF COMP VIS, P2126, DOI 10.1109/ICCV.2011.6126488
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/ICASSP40776.2020.9053405, 10.1109/icassp40776.2020.9053405]
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Liu LK, 2015, IEEE T IMAGE PROCESS, V24, P1983, DOI 10.1109/TIP.2015.2409551
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma FC, 2019, IEEE INT CONF ROBOT, P3288, DOI [10.1109/ICRA.2019.8793637, 10.1109/icra.2019.8793637]
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Park J., 2018, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1807.06514
   Qiu JX, 2019, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR.2019.00343
   Rao ZB, 2022, VISUAL COMPUT, V38, P77, DOI 10.1007/s00371-020-02001-5
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi JL, 2021, VISUAL COMPUT, V37, P815, DOI 10.1007/s00371-020-01832-6
   Shivakumar SS, 2019, IEEE INT C INTELL TR, P13, DOI [10.1109/ITSC.2019.8917294, 10.1109/itsc.2019.8917294]
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan, 2019, ARXIV190801238CS
   Tu Z., 2014, DEEPLY SUPERVISED NE
   Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012
   Wang X, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225062
   Wolk D, 2019, IEEE INT CONF ROBOT, P6101, DOI [10.1109/icra.2019.8794182, 10.1109/ICRA.2019.8794182]
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yiyi Liao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5059, DOI 10.1109/ICRA.2017.7989590
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang H., 2019, INT C MACHINE LEARNI, P12744, DOI DOI 10.48550/ARXIV.1805.08318
   Zhang ZH, 2021, VISUAL COMPUT, V37, P1731, DOI 10.1007/s00371-020-01934-1
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 50
TC 12
Z9 12
U1 2
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1619
EP 1630
DI 10.1007/s00371-021-02092-8
EA MAY 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000654080300001
DA 2024-07-18
ER

PT J
AU Verma, P
   Srivastava, R
AF Verma, Pratishtha
   Srivastava, Rajeev
TI Two-stage multi-view deep network for 3D human pose reconstruction using
   images and its 2D joint heatmaps through enhanced stack-hourglass
   approach
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose reconstruction (HPR); Deep convolutional neural network
   (DCNN); Enhanced stack-hourglass approach (ESHA); Human activity
   recognition
AB Human beings easily reconstruct the 3D pose of a human from a 2D image, but 3D human pose reconstruction (HPR) continues to exist as a challenging task for machines. Traditional methods can reconstruct the 3D pose from the image directly or from the 2D joint locations that have been used for 3D HPR. Such traditional strategies have their merits and demerits. In this paper, we have tried to combine the merits of such traditional techniques with that of a deep architecture model. By this strategy, the model delivers both of the merits concurrently in a multi-view scenario and also fuses this knowledge on the upcoming step with early and late fusion strategies. We also introduce an enhanced stack-hourglass network for the prediction of 2D keypoint heatmaps. The predicted 2D keypoint heatmaps and the image have been utilized with simple CNN neural architecture along with both of the fusion strategies for 3D pose reconstruction. Experimental results show that the proposed method achieves comparable performance to the state-of-the-art methods on MPII, and Human3.6M datasets.
C1 [Verma, Pratishtha; Srivastava, Rajeev] IIT BHU, Varanasi 221005, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology BHU Varanasi (IIT BHU Varanasi)
RP Verma, P (corresponding author), IIT BHU, Varanasi 221005, Uttar Pradesh, India.
EM pratishthaver.rs.cse17@iitbhu.ac.in
RI verma, Pratishtha/ABE-4921-2022; Verma, Pratishtha/AGV-5388-2022;
   Srivastava, Rajeev/C-7906-2016
OI Srivastava, Rajeev/0000-0002-0165-1556; Verma,
   Pratishtha/0000-0003-1571-392X
CR Agarwal A, 2004, PROC CVPR IEEE, P882
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64
   Bo LF, 2008, PROC CVPR IEEE, P1833
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Núñez JC, 2019, NEUROCOMPUTING, V323, P335, DOI 10.1016/j.neucom.2018.10.009
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen Yun-Chun., 2019, IEEE Transactions on Pattern Analysis and Machine Intelligence in press
   Chou CJ, 2018, ASIAPAC SIGN INFO PR, P17, DOI 10.23919/APSIPA.2018.8659538
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Ershadi-Nasab S, 2018, MULTIMED TOOLS APPL, V77, P15573, DOI 10.1007/s11042-017-5133-8
   Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44
   Habibie I, 2019, PROC CVPR IEEE, P10897, DOI 10.1109/CVPR.2019.01116
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hong CQ, 2016, SIGNAL PROCESS, V124, P132, DOI 10.1016/j.sigpro.2015.10.004
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Hong CQ, 2015, IEEE T IND ELECTRON, V62, P3742, DOI 10.1109/TIE.2014.2378735
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Katircioglu I, 2018, INT J COMPUT VISION, V126, P1326, DOI 10.1007/s11263-018-1066-6
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mori G, 2006, IEEE T PATTERN ANAL, V28, P1052, DOI 10.1109/TPAMI.2006.149
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nibali A, 2019, IEEE WINT CONF APPL, P1477, DOI 10.1109/WACV.2019.00162
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Popa AI, 2017, PROC CVPR IEEE, P4714, DOI 10.1109/CVPR.2017.501
   Rafi U., 2016, BMVC, V1, P2
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425
   Tekin Bugra, 2016, P BRIT MACH VIS C 20
   Tripathy SK, 2020, MULTIMEDIA SYST, V26, P585, DOI 10.1007/s00530-020-00667-4
   Trumble M, 2017, BRIT MACHINE VISION
   Verma P, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102866
   Verma P, 2020, MULTIMEDIA SYST, V26, P671, DOI 10.1007/s00530-020-00677-2
   Wang Keze., 2019, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Zhang XY, 2019, PATTERN RECOGN LETT, V125, P404, DOI 10.1016/j.patrec.2019.05.020
NR 50
TC 8
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2417
EP 2430
DI 10.1007/s00371-021-02120-7
EA MAY 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000653632500003
DA 2024-07-18
ER

PT J
AU Yang, YZ
   Cheng, ZH
   Yu, HT
   Zhang, YQ
   Cheng, X
   Zhang, Z
   Xie, GJ
AF Yang, Yizhong
   Cheng, Zhihang
   Yu, Haotian
   Zhang, Yongqiang
   Cheng, Xin
   Zhang, Zhang
   Xie, Guangjun
TI MSE-Net: generative image inpainting with multi-scale encoder
SO VISUAL COMPUTER
LA English
DT Article
DE Image inpainting; Generative adversarial network; Encoder-decoder;
   Contextual Attention
AB Image inpainting methods based on deep convolutional neural networks (DCNN), especially generative adversarial networks (GAN), have made tremendous progress, due to their forceful representation capabilities. These methods can generate visually reasonable contents and textures; however, the existing deep models based on a single receptive field type usually not only cause image artifacts and content mismatches but also ignore the correlation between the hole region and long-distance spatial locations in the image. To address the above problems, in this paper, we propose a new generative model based on GAN, which is composed of a two-stage encoder-decoder with a Multi-Scale Encoder Network (MSE-Net) and a new Contextual Attention Model based on the Absolute Value (CAM-AV). The former utilizes different-size convolution kernels to encode features, which improves the ability of abstract feature characterization. The latter uses a new search algorithm to enhance the matching of features in the network. Our network is a fully convolutional network that can complete holes of arbitrary size, number, and spatial location in the image. Experiments with regular and irregular inpainting on different datasets including CelebA and Places2 demonstrate that the proposed method achieves higher quality inpainting results with reasonable contents than the most existing state-of-the-art methods.
C1 [Yang, Yizhong; Cheng, Zhihang; Yu, Haotian; Zhang, Yongqiang; Cheng, Xin; Zhang, Zhang; Xie, Guangjun] Hefei Univ Technol, Sch Elect Sci & Appl Phys, Hefei 230009, Peoples R China.
C3 Hefei University of Technology
RP Yang, YZ; Xie, GJ (corresponding author), Hefei Univ Technol, Sch Elect Sci & Appl Phys, Hefei 230009, Peoples R China.
EM yangyizhong@hfut.edu.cn; gjxie8005@hfut.edu.cn
RI Zhang, Zhang/JAX-2097-2023; zhang, zhang/GQZ-6804-2022; zhang,
   zheng/HCH-9684-2022
FU National Natural Science Foundation of China [61674049, U19A2053];
   Fundamental Research Funds for the Central Universities of China
   [JZ2019HGTB0092, JZ2020YYPY0089, JZ2020HGTA0085]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61674049 and U19A2053 and the Fundamental Research
   Funds for the Central Universities of China under Grant JZ2019HGTB0092,
   JZ2020YYPY0089 and JZ2020HGTA0085.
CR Alec Radford, 2015, ARXIV151106434
   Alilou VK, 2017, MULTIMED TOOLS APPL, V76, P7213, DOI 10.1007/s11042-016-3366-6
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Daniel Pak-Kong, 2020, P EUR C COMP VIS, P656
   Esedoglu S, 2002, EUR J APPL MATH, V13, P353, DOI 10.1017/S0956792501004904
   Ghorai M, 2019, IEEE T IMAGE PROCESS, V28, P5495, DOI 10.1109/TIP.2019.2920528
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Haouchine N, 2020, VISUAL COMPUT, V36, P211, DOI 10.1007/s00371-018-1600-0
   HONGYU L, 2019, P IEEE INT C COMP VI, P4170
   Hui Zheng, 2020, Proceedings of ACM/IEEE Design Automation Conference (DAC)
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jay, 2018, P EUR C COMP VIS, P665
   Jay K., 2018, BRIT MACH VIS C, P1521
   Jin X, 2018, IEEE ACCESS, V6, P49967, DOI 10.1109/ACCESS.2018.2866089
   Junyuan X., 2012, ADV NEURAL INF PROCE, V25, P2341
   Kamyar N., 2019, ARXIV190100212V3
   Leon, 2017, ARXIV170107875V3
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P3050, DOI 10.1109/TIFS.2017.2730822
   Li KS, 2016, SOFT COMPUT, V20, P885, DOI 10.1007/s00500-014-1547-7
   Liu BW, 2019, VISUAL COMPUT, V35, P85, DOI 10.1007/s00371-017-1454-x
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu YQ, 2013, IEEE T IMAGE PROCESS, V22, P1699, DOI 10.1109/TIP.2012.2218828
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Martin H., 2018, ADV NEURAL INFORM PR, P1341
   Mehdi M., 2014, ARXIV14111784V1
   Miyato T, 2018, INT C LEARN REPR
   Mo JC, 2019, CLUSTER COMPUT, V22, pS7593, DOI 10.1007/s10586-018-2323-8
   Pandey G, 2020, VISUAL COMPUT, V36, P1291, DOI 10.1007/s00371-019-01729-z
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qingguo X., ARXIV181201458V1
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shiguang, 2018, P EUR C COMP VIS, P1153
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Thanh-Tung H., 2019, P INT C LEARN REPR
   Wang C, 2018, VISUAL COMPUT, V34, P67, DOI 10.1007/s00371-016-1312-2
   Wei Xiong, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5833, DOI 10.1109/CVPR.2019.00599
   Xie C., 2019, P IEEE INT C COMP VI, P2510
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi Z., 2020, CVPR, P7508
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuqing M., 28 INT JOINT C ARTF
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 53
TC 10
Z9 10
U1 4
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2647
EP 2659
DI 10.1007/s00371-021-02143-0
EA MAY 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000651332800001
DA 2024-07-18
ER

PT J
AU Ben Salah, K
   Othmani, M
   Kherallah, M
AF Ben Salah, Khawla
   Othmani, Mohamed
   Kherallah, Monji
TI A novel approach for human skin detection using convolutional neural
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Skin detection; Color models; Deep learning; Convolutional neural
   networks
AB Human skin detection, which is one of the important pre-processing phases, has a wide range of applications such as face tracking, skin diseases, video surveillance, web content filtering, and so on. Skin detection is a challenging problem because skin color can vary dramatically in its appearance due to many factors such as illumination conditions, pose variations, race, aging, and complex background. Several methods dealing with skin detection assume that skin pixels can be extracted from background colors according to some thresholding rules related to a specific color model. Nevertheless, it is a complex task to recognize skin pixels under the challenging factors aforementioned. In the recent era, the success of deep convolutional neural network (CNN) has strongly influenced the field of computer vision. However, we could find only a few researches that apply deep learning methods to deal with the skin detection problem. This paper presents a novel approach based on CNN for skin detection. Extensive experiments show that the proposed approach exceeds the best result for other state-of-the-art methods.
C1 [Ben Salah, Khawla] Univ Sfax, Natl Engn Sch Sfax, BP 1173, Sfax, Tunisia.
   [Ben Salah, Khawla] Univ Gafsa, Fac Sci Gafsa, Res Lab Math Earth Sci Modeling & Intelligent Sys, Gafsa, Tunisia.
   [Othmani, Mohamed] Univ Gafsa, Fac Sci Gafsa, BP 2100, Gafsa, Tunisia.
   [Kherallah, Monji] Univ Sfax, Fac Sci Sfax, BP 1173, Sfax, Tunisia.
C3 Universite de Sfax; Ecole Nationale dIngenieurs de Sfax (ENIS);
   Universite de Gafsa; Universite de Gafsa; Universite de Sfax; Faculty of
   Sciences Sfax
RP Ben Salah, K (corresponding author), Univ Sfax, Natl Engn Sch Sfax, BP 1173, Sfax, Tunisia.
EM khawlabensalah8@gmail.com; mohamed.othmani@yahoo.fr;
   monji.kherallah@fss.usf.tn
RI ben salah, khaoula/AAA-4718-2022
OI KHERALLAH, Monji/0000-0002-4549-1005; Ben Salah,
   Khawla/0000-0002-4227-9623
CR Ahmad J, 2018, FUTURE GENER COMP SY, V81, P314, DOI 10.1016/j.future.2017.11.002
   Al-Tairi ZH, 2014, J INF PROCESS SYST, V10, P283, DOI 10.3745/JIPS.02.0002
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   Bin Abdul Rahman NA., 2006, P MMU INT S INF COMM, P90
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   Chen W, 2016, MULTIMED TOOLS APPL, V75, P839, DOI 10.1007/s11042-014-2328-0
   Chitra S., 2012, Appl. Math. Sci, V6, P4229
   Devi Mandalapu Sarada, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P649, DOI 10.1109/ICETET.2008.17
   Erdem CE, 2011, INT CONF ACOUST SPEE, P1497, DOI 10.1109/ICASSP.2011.5946777
   Fang RG, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2932707
   Ganesan P, 2013, 2013 INTERNATIONAL CONFERENCE ON GREEN COMPUTING, COMMUNICATION AND CONSERVATION OF ENERGY (ICGCE), P77, DOI 10.1109/ICGCE.2013.6823403
   Hajiarbabi M., 2014, Journal of Automation, Mobile Robotics and Intelligent Systems, V8, P41
   Hwang I., 2017, IEEE ICASSP, DOI [10.1109/ICASSP.2017.7952361, DOI 10.1109/ICASSP.2017.7952361]
   Hwang I, 2013, IEEE IMAGE PROC, P2622, DOI 10.1109/ICIP.2013.6738540
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   Kawulok M, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553733
   Kawulok M, 2014, PATTERN RECOGN LETT, V41, P3, DOI 10.1016/j.patrec.2013.08.028
   Kim Y, 2017, IEEE IMAGE PROC, P3919, DOI 10.1109/ICIP.2017.8297017
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Ma C., 2018, IEEE GLOB CONF CONSU, DOI [10.1109/GCCE.2018.8574747, DOI 10.1109/GCCE.2018.8574747]
   Nikolskaia K., 2018, CEUR WORKSH P, P1323
   Rahman M.A., 2015, P 2014 IEEE INT C IN, P58
   Rajini V.:, 2014, ADV ELECT ENG ICAEE, P9
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Raza M, 2018, FUTURE GENER COMP SY, V88, P28, DOI 10.1016/j.future.2018.05.002
   Rodrigues E.L.L.:, 2013, 9 WORKSH VIS COMP RE
   Sack H., 2009, DIGITALE KOMMUNIKATI, DOI [10.1007/978-3-540-92923-9, DOI 10.1007/978-3-540-92923-9]
   Schaefer G., 2006, IEEE EMBS INT C, V1, P965, DOI DOI 10.1109/IEMBS.2006.259275
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tan JH, 2018, FUTURE GENER COMP SY, V87, P127, DOI 10.1016/j.future.2018.05.001
   Tan WR, 2012, IEEE T IND INFORM, V8, P138, DOI 10.1109/TII.2011.2172451
   Yogarajah P, 2010, IEEE IMAGE PROC, P2225, DOI 10.1109/ICIP.2010.5652798
   Zafarifar B, 2013, IEEE ICCE, P88, DOI 10.1109/ICCE.2013.6486807
   Zhang Z, 2009, IEEE IMAGE PROC, P1137, DOI 10.1109/ICIP.2009.5413535
   Zhu Q, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P37
NR 36
TC 12
Z9 12
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1833
EP 1843
DI 10.1007/s00371-021-02108-3
EA APR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000636429600001
DA 2024-07-18
ER

PT J
AU Li, M
   Lei, L
   Sun, H
   Li, X
   Kuang, GY
AF Li, Ming
   Lei, Lin
   Sun, Hao
   Li, Xiao
   Kuang, Gangyao
TI Fine-grained visual classification via multilayer bilinear pooling with
   object localization
SO VISUAL COMPUTER
LA English
DT Article
DE Fine-grained visual classification; Multilayer bilinear pooling (MLBP);
   Object localization; Convolutional neural networks (CNNs)
AB Fine-grained visual classification is a challenging task in the computer vision field. How to explore discriminative features is vital for classification. As one crucial step, exactly object localization is able to eliminate the background noises and highlight interesting objects at the same time. However, some current methods usually use bounding boxes to locate objects, that are not suitable when the poses of objects change. Furthermore, it has been demonstrated that deep features have strong feature representation capability, especially the bilinear pooling features, which achieved superior performance in fine-grained visual classification tasks. However, the bilinear features, which captured only from the last convolutional layer, have limited discriminability, especially when dealing with small-scale objects. In this paper, we propose a multilayer bilinear pooling model combined with object localization. First, a flexible and scalable object localization module is utilized to locate the interesting object in an image instead of using bounding boxes. Then the refined features are obtained by highlighting object region and suppressing background noises. While the multilayer bilinear pooling, which exploits the complementarity between different layers, is used for further extracting more discriminative features. Experiment results on three public datasets show that our proposed method can achieve competitive performance compared with several state-of-the-art methods.
C1 [Li, Ming; Lei, Lin; Sun, Hao; Li, Xiao; Kuang, Gangyao] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Lei, L (corresponding author), Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha 410073, Peoples R China.
EM liming17@nudt.edu.cn; alaleilin@163.com; clhaosun@gmail.com;
   lxcherishm@126.com; Kuangyeats@hotmail.com
RI sun, hao/HMD-2991-2023; Sun, Haoyang/KHD-3534-2024; sun,
   hao/GRS-7732-2022
FU National Natural Science Foundation of China (NSFC) [61971426]
FX This work is supported by National Natural Science Foundation of China
   (NSFC) under Grant 61971426.
CR [Anonymous], 2013, Tech. rep.
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Cui Q., 2019, DEEP LEARNING FINE G
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gupta Rajiv, 2015, P 19 INT C EV ASS SO, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kar P, 2012, ARTIFICIAL INTELLIGE, V22, P583
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, USING OLD SOLUTIONS TO NEW PROBLEMS - NATURAL DRUG DISCOVERY IN THE 21ST CENTURY, P3, DOI 10.5772/56424
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liao QY, 2019, IEEE INT CONF COMP V, P728, DOI 10.1109/ICCVW.2019.00093
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma Y, 2016, IEEE INT CONF COMMUN
   Meer, 2006, LECT NOTES COMPUTER
   Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun QL, 2018, NEUROCOMPUTING, V282, P174, DOI 10.1016/j.neucom.2017.12.020
   Sun XX, 2019, AAAI CONF ARTIF INTE, P273
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Xiao LX, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511399
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   You, 2018, LECT NOTES COMPUTER
   Zhang N., 2014, LECT NOTES COMPUTER
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 35
TC 8
Z9 9
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 811
EP 820
DI 10.1007/s00371-020-02052-8
EA JAN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000606397000001
DA 2024-07-18
ER

PT J
AU Berlin, SJ
   John, M
AF Berlin, S. Jeba
   John, Mala
TI Spiking neural network based on joint entropy of optical flow features
   for human action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Human action recognition; PWCNet; BindsNet; Spiking
   neural network
ID SPATIOTEMPORAL FEATURES; MODEL
AB In the recent past, human action recognition is inviting increased attention in the automated video surveillance systems. An efficient human action classification technique in an unconstrained environment is proposed in this paper. A novel descriptor relying on joint entropy of difference in magnitude and orientation of the optical flow feature is developed in order to model human actions. Initially, flow feature is computed using Pyramid-Warping-Cost volume Network (PWCNet), considering every two consecutive frames. Then, the feature descriptor is formed based on the joint entropy of difference in flow magnitude and orientation collected from the regular grid of each frame in the action sequence. Finally, in order to incorporate long-term temporal dependency, a spiking neural network is embedded to aggregate the information across the frames. Different optimization techniques and different types of hidden nodes are utilized in the spiking neural network to analyze the performance of the proposed work. Extensive experiments on the benchmark datasets for human action recognition show the efficacy of the proposed method.
C1 [Berlin, S. Jeba; John, Mala] Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.
C3 Anna University; Madras Institute of Technology; Anna University Chennai
RP Berlin, SJ (corresponding author), Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.
EM jebaberlin@mitindia.edu
RI JOHN, MALA/AAI-7508-2021
OI JOHN, MALA/0000-0001-5034-3405
FU DST INSPIRE Fellowship; DST, India
FX The first author is a recipient of DST INSPIRE Fellowship and wishes to
   thank DST, India, for the same.
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Berlin SJ, 2020, J INTELL FUZZY SYST, V39, P961, DOI 10.3233/JIFS-191914
   Berlin SJ, 2016, INT CARN CONF SECU, P143
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen EQ, 2019, IEEE ACCESS, V7, P57267, DOI 10.1109/ACCESS.2019.2910604
   Chen M, 2011, INT CONF CLOUD COMPU, P316, DOI 10.1109/CCIS.2011.6045082
   Chun S, 2016, IET COMPUT VIS, V10, P250, DOI 10.1049/iet-cvi.2015.0233
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Dhoble K., 2012, The 2012 International Joint Conference on Neural Networks (IJCNN), P1
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   El-Ghaish HE, 2018, IEEE ACCESS, V6, P49040, DOI 10.1109/ACCESS.2018.2868319
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Gao RH, 2018, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR.2018.00622
   Gao YB, 2018, IEEE ACCESS, V6, P52277, DOI 10.1109/ACCESS.2018.2869790
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Holte MB, 2012, IEEE J-STSP, V6, P553, DOI 10.1109/JSTSP.2012.2193556
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YK, 2020, IEEE ACCESS, V8, P45753, DOI 10.1109/ACCESS.2020.2978223
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Kingma D. P., 2014, arXiv
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Ladjailia A, 2020, NEURAL COMPUT APPL, V32, P16387, DOI 10.1007/s00521-018-3951-x
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li WH, 2018, IEEE ACCESS, V6, P44211, DOI 10.1109/ACCESS.2018.2863943
   Li X, 2007, ELECTRON LETT, V43, P560, DOI 10.1049/el:20070027
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Liu HH, 2018, IEEE T NEUR NET LEAR, V29, P1427, DOI 10.1109/TNNLS.2017.2669522
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Nikouei SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING (IEEE EDGE), P125, DOI 10.1109/EDGE.2018.00025
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sahoo SP, 2019, IET IMAGE PROCESS, V13, P983, DOI 10.1049/iet-ipr.2018.6045
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tong M, 2020, NEURAL COMPUT APPL, V32, P5285, DOI 10.1007/s00521-019-04030-1
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Ullah A, 2019, IEEE T IND ELECTRON, V66, P9692, DOI 10.1109/TIE.2018.2881943
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Wan YQ, 2020, IEEE ACCESS, V8, P85284, DOI 10.1109/ACCESS.2020.2993227
   Wang H., 2013, ICCV workshop on action recognition with a large number of classes, P1
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang HS, 2018, IEEE T CIRC SYST VID, V28, P2908, DOI 10.1109/TCSVT.2017.2746092
   Wang L., 2007, Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2007.383298
   Wang L, 2018, IEEE ACCESS, V6, P17913, DOI 10.1109/ACCESS.2018.2817253
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang YY, 2019, IEEE SIGNAL PROC LET, V26, P1556, DOI 10.1109/LSP.2019.2940111
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yao GL, 2019, APPL INTELL, V49, P2017, DOI 10.1007/s10489-018-1347-3
   Yi Y, 2018, VISUAL COMPUT, V34, P391, DOI 10.1007/s00371-016-1345-6
   Yu J, 2020, VISUAL COMPUT, V36, P1457, DOI 10.1007/s00371-019-01751-1
   Yu S, 2020, IEEE ACCESS, V8, P1840, DOI 10.1109/ACCESS.2019.2962284
   Zhang H, 2018, MACH VISION APPL, V29, P1127, DOI 10.1007/s00138-018-0956-5
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 66
TC 11
Z9 11
U1 4
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 223
EP 237
DI 10.1007/s00371-020-02012-2
EA DEC 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000600824200002
DA 2024-07-18
ER

PT J
AU Amara, Y
   Amamra, A
   Khemis, S
AF Amara, Yacine
   Amamra, Abdenour
   Khemis, Salim
TI Raw GIS to 3D road modeling for real-time traffic simulation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D road traffic simulation; Road modeling; 3D traffic rendering; Vehicle
   virtual navigation
ID SYSTEM
AB In this work, we propose a new approach to road modeling and 3D traffic simulation. Based on the raw geographic information system (GIS) data laid out as sparse polylines with attributes, we compute a more adequate functional description for real-time simulation of on-road vehicle animation. The proposed approach begins with a filtering/subdivision module where the raw polylines are transformed into a graph of functional road segments as arcs and the nodes as intersections. Then, the vehicle speed profile is computed based on its dynamics, its neighborhood and the curvature profile of the road. Afterward, a multi-agent system is proposed in order to handle a large number of simulated vehicle/driver couples. Finally, we deploy a 3D rendering engine to display the computed 3D simulation on screen. The resulting model satisfies most of the real road features for traffic simulation including road interchanges, roundabouts, intersections, lanes, etc. More importantly, the simulated driving qualitatively mimics the real behavior of the drivers/vehicles on the road as can be seen in the accompanying video (RTSP video). We also validate our findings with a technical assessment based on macroscopic and microscopic traffic simulation metrics in several road traffic scenarios.
C1 [Amara, Yacine; Amamra, Abdenour; Khemis, Salim] Ecole Militaire Polytech, BP 17, Algiers, Algeria.
C3 Ecole Military Polytechnic
RP Amamra, A (corresponding author), Ecole Militaire Polytech, BP 17, Algiers, Algeria.
EM amamra.abdenour@gmail.com
CR Ahmed K. I., 1999, Ph.D. dissertation
   Amara Y, 2019, FED CONF COMPUT SCI, P385, DOI 10.15439/2019F223
   Asaithambi G, 2018, TRANSP LETT, V10, P92, DOI 10.1080/19427867.2016.1190887
   Barrington-Leigh C, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180698
   Chen MY, 2019, 2019 INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION, BIG DATA & SMART CITY (ICITBS), P32, DOI 10.1109/ICITBS.2019.00017
   Ganguli S., 2019, ANAL CONTROL NOVEL T
   Heikoop D, 2017, DRIVER PSYCHOL AUTOM
   Hu Y., 2018, GIS BASED SIMULATION
   Kita H., 1993, Proceedings of the 12th International Symposium on the Theory of Traffic Flow and Transportation, P37
   Lambert ED, 2019, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON VEHICLE TECHNOLOGY AND INTELLIGENT TRANSPORT SYSTEMS (VEHITS 2019), P609, DOI 10.5220/0007801806090615
   Li WZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130847
   Min K, 2019, INT J PRECIS ENG MAN, V20, P1939, DOI 10.1007/s12541-019-00210-0
   Nguyen HH, 2014, COMPUT GRAPH FORUM, V33, P259, DOI 10.1111/cgf.12494
   Pfeifle M., 2019, US Patent, Patent No. [10,289,636, 10289636]
   Rababah A., 2020, INT J ELECT COMPUTER, V10, P2088, DOI [10.11591/ijece.v10i2.pp1648-1654, DOI 10.11591/IJECE.V10I2.PP1648-1654]
   Saidallah M, 2016, MATEC WEB CONF, V81, DOI 10.1051/matecconf/20168105002
   Schiefelbein J, 2019, BUILD ENVIRON, V149, P630, DOI 10.1016/j.buildenv.2018.12.025
   Schmitt PS, 2019, IEEE INT CONF ROBOT, P176, DOI [10.1109/ICRA.2019.8793824, 10.1109/icra.2019.8793824]
   Sewall J, 2010, COMPUT GRAPH FORUM, V29, P439, DOI 10.1111/j.1467-8659.2009.01613.x
   Shome R., 2019, ARXIV190301006
   Wang YB, 2013, VISUAL COMPUT, V29, P323, DOI 10.1007/s00371-012-0735-7
   Zhang H, 2018, CAN CON EL COMP EN
NR 22
TC 3
Z9 4
U1 4
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 239
EP 256
DI 10.1007/s00371-020-02013-1
EA NOV 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000591128700002
DA 2024-07-18
ER

PT J
AU Zhang, HX
   Hu, ZY
   Hao, RX
AF Zhang, Hexiang
   Hu, Ziyu
   Hao, Ruoxin
TI Joint information fusion and multi-scale network model for pedestrian
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE YOLOv3; Pedestrian detection; Information fusion; Multi-scale network
AB The existing pedestrian detection suffers the low accuracy when the environment changes dramatically. In order to solve the problem, a pedestrian detection model combining information fusion and multi-scale detection is proposed. The model is composed of a retinex algorithm and an improved YOLOv3 algorithm. Retinex algorithm is selected as the preprocessing algorithm to improve the brightness and contrast of pedestrians. The model improves the YOLOv3 algorithm by adding multiple scale detections. The K-means is used to determine the number of optimal anchors and the aspect ratio. By testing on the standard data set, the mean average precision (mAP) of the joint detection model increases from the original 80.69-91.07%, and the recall increases from 65.22 to 87.48%. The comparative experiments show that the improved model performs good robustness and generalization ability on the problem of low pedestrian detection accuracy in complex environments.
C1 [Zhang, Hexiang; Hao, Ruoxin] Yanshan Univ, Inst Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
   [Hu, Ziyu] Yanshan Univ, Engn Res Ctr, Minist Educ Intelligent Control Syst & Intelligen, Qinhuangdao, Hebei, Peoples R China.
C3 Yanshan University; Yanshan University
RP Hu, ZY (corresponding author), Yanshan Univ, Engn Res Ctr, Minist Educ Intelligent Control Syst & Intelligen, Qinhuangdao, Hebei, Peoples R China.
EM zhxysu@163.com; hzy@ysu.edu.cn
RI Hu, Ziyu/AAH-2396-2020; Hu, Ziyu/M-4082-2015
OI Zhang, Hexiang/0009-0004-7977-9666; Hu, Ziyu/0000-0001-6982-3265
FU National Natural Science Foundation of China [62003296]; Natural Science
   Foundation of Hebei [F2020203031]; Science and Technology Project of
   Hebei Education Department [QN2020225]
FX This work was supported by Project supported by National Natural Science
   Foundation of China (No. 62003296), the Natural Science Foundation of
   Hebei (No. F2020203031), Science and Technology Project of Hebei
   Education Department (No. QN2020225)
CR Asad M, 2021, VISUAL COMPUT, V37, P1415, DOI 10.1007/s00371-020-01878-6
   Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Bengler K, 2014, IEEE INTEL TRANSP SY, V6, P6, DOI 10.1109/MITS.2014.2336271
   Byeon YH, 2017, 2017 6TH IIAI INTERNATIONAL CONGRESS ON ADVANCED APPLIED INFORMATICS (IIAI-AAI), P858, DOI 10.1109/IIAI-AAI.2017.196
   Chong Li, 2020, Recent Trends in Intelligent Computing, Communication and Devices. Proceedings of ICCD 2018. Advances in Intelligent Systems and Computing (AISC 1031), P277, DOI 10.1007/978-981-13-9406-5_34
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   JADHAV A, 2020, 2020 NAT C COMM NCC, P1
   Lin Feng Y.L., 2019, ASSEMBLY AUTOMATION
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Meyer D, 2003, NEUROCOMPUTING, V55, P169, DOI 10.1016/S0925-2312(03)00431-4
   Nair VG, 2020, LECT NOTES ELECTR EN, V581, P59, DOI 10.1007/978-981-13-9419-5_5
   Pang YW, 2011, SIGNAL PROCESS, V91, P773, DOI 10.1016/j.sigpro.2010.08.010
   Qin YC, 2020, J MED IMAG HEALTH IN, V10, P152, DOI 10.1166/jmihi.2020.2859
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012
   Wong A, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P95, DOI 10.1109/CRV.2018.00023
   Zeng CB, 2010, IEEE IMAGE PROC, P3845, DOI 10.1109/ICIP.2010.5654100
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao ZQ, 2017, LECT NOTES COMPUT SC, V10361, P735, DOI 10.1007/978-3-319-63309-1_65
   ZIETKIEWICZ E, 1994, GENOMICS, V20, P176, DOI 10.1006/geno.1994.1151
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 28
TC 13
Z9 13
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2433
EP 2442
DI 10.1007/s00371-020-01997-0
EA NOV 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000588234000001
DA 2024-07-18
ER

PT J
AU Shuai, CY
   Wang, X
   He, M
   Ouyang, X
   Yang, J
AF Shuai, Chunyan
   Wang, Xu
   He, Min
   Ouyang, Xin
   Yang, Jun
TI A presentation and retrieval hash scheme of images based on principal
   component analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Eigenvector; High-dimensional vectors; K-nearest neighbors; Locality
   sensitive hash; Multi-dividing; Principal component analysis hashing
AB Image representation and approximate query is always a research challenge and is affected greatly by the dimension and size of images. Since hash-based methods and binary encodings in combination with other techniques, such as kernel tricks, a longer binary code and mapping vectors rotation, can maintain a linear query time and query accuracy, they have been used in this area broadly. This paper develops principal component analysis hashing (PCAH) and unequal length of binary coding to divide images into more categories, denoted as PCA-MD, to improve accuracy of the representation and lookup of images. This paper firstly proves that the eigenvector mapping is locality sensitive, which is the basis for more classes division. For the anisotropy of the eigenvectors, PCA-MD utilizes an unequal length of binary coding and fewer eigenvectors, rather than an equal code, to divide the images mapped on every eigenvector to more categories. Moreover, L1-norm distance is applied to measure the distances of images to avoid the enormous computation of Euclidean distance. Theoretical analysis and extensive experimental results demonstrate that the PCA-MD has a higher query performance and a slight longer run time than the state-of-the-art approaches based on the Hamming distance. This in turn verifies that PCAH is a locality sensitive hash and that partitioning into more categories rather than only two categories is reasonable.
C1 [Shuai, Chunyan; Wang, Xu; He, Min; Yang, Jun] Kunming Univ Sci & Technol, Fac Transportat Engn, Kunming, Yunnan, Peoples R China.
   [Ouyang, Xin] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming, Yunnan, Peoples R China.
C3 Kunming University of Science & Technology; Kunming University of
   Science & Technology
RP Ouyang, X (corresponding author), Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming, Yunnan, Peoples R China.
EM earth0806@sina.com; bbqwww@qq.com; hmebox@163.com; kmoyx@hotmail.com;
   2328360149@qq.com
RI xin, ouyang/JBI-6762-2023
FU National Key R&D Program of China [2017YFB0306405]; National Nature
   Science Foundation of China [61562056, 61364008]
FX This work was supported in part by the National Key R&D Program of China
   (No. 2017YFB0306405) and the National Nature Science Foundation of China
   (Nos. 61562056 and 61364008).
CR Amsaleg L., 2020, DATASETS APPROXIMATE
   Andoni A, 2015, ADV NEUR IN, V28
   [Anonymous], 2014, INT J RES APPL SCI E
   [Anonymous], 2012, NIPS
   [Anonymous], 2009, NEURIPS
   Cao Y, 2018, IEEE ACCESS, V6, P2039, DOI 10.1109/ACCESS.2017.2781360
   Cunningham JP, 2015, J MACH LEARN RES, V16, P2859
   Fearn T., 2014, NIR NEWS, V25, P23, DOI [https://doi.org/10.1255/nirn.1439, DOI 10.1255/nirn.1439]
   Feng H, 2018, IEEE T COMPUT, V67, P252, DOI 10.1109/TC.2017.2748131
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gratzer G., 1968, Universal Algebra
   Griffin G., 2020, CALTECH 256 OBJECT C
   Heo JP, 2019, IEEE T PATTERN ANAL, V41, P2084, DOI 10.1109/TPAMI.2018.2853161
   Heo JP, 2015, IEEE T PATTERN ANAL, V37, P2304, DOI 10.1109/TPAMI.2015.2408363
   Jiang K, 2015, PROC CVPR IEEE, P4933, DOI 10.1109/CVPR.2015.7299127
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Krizhevsky A., 2009, Tech. Rep.
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114
   Li W, 2020, IEEE T KNOWL DATA EN, V32, P1475, DOI 10.1109/TKDE.2019.2909204
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Matsushita Y, 2009, LECT NOTES COMPUT SC, V5414, P374, DOI 10.1007/978-3-540-92957-4_33
   Park Y, 2015, PROC VLDB ENDOW, V9, P144
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tang J, 2016, IEEE T CYBERNETICS, V46, P410, DOI 10.1109/TCYB.2015.2402751
   Vinay A, 2015, PROCEDIA COMPUT SCI, V57, P650, DOI 10.1016/j.procs.2015.07.434
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Yu FX, 2014, PR MACH LEARN RES, V32, P946
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
NR 31
TC 5
Z9 6
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2113
EP 2126
DI 10.1007/s00371-020-01973-8
EA OCT 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000580750200001
DA 2024-07-18
ER

PT J
AU Liu, HJ
   Tang, SR
   Lei, D
   Zhu, Q
   Sui, HG
   Zhang, GJ
   Li, C
AF Liu, Huajun
   Tang, Shiran
   Lei, Dian
   Zhu, Qing
   Sui, Haigang
   Zhang, Gaojian
   Li, Chao
TI Accurate estimation of feature points based on individual projective
   plane in video sequence
SO VISUAL COMPUTER
LA English
DT Article
DE Feature point prediction; Homography; Projective plane; Video sequence
ID IMAGE REGISTRATION; GRAPHS
AB The stability and quantity of feature matching in video sequence is one of the key issues for feature tracking and some relevant applications. The existing matching methods are based on feature detection, which is usually affected by illumination conditions, noise or occlusions, and this will directly influence matching results. In this paper, we propose an accurate prediction method for interest point estimation in video sequence by extracting the stable mapping for each undetected point in its suitable projective plane, which is based on coplanar feature points that have already been detected in adjacent frames. The proposed prediction method breaks the limitation of the previous approaches that largely rely on feature detection. Our experiments show that our method not only predicts features accurately, but also enriches the correspondences, which prolongs the track length of features.
C1 [Liu, Huajun; Tang, Shiran; Lei, Dian; Zhang, Gaojian; Li, Chao] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Zhu, Qing] Southwest Jiaotong Univ, Fac Geosci & Environm Engn, Chengdu, Peoples R China.
   [Sui, Haigang] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan, Peoples R China.
C3 Wuhan University; Southwest Jiaotong University; Wuhan University
RP Liu, HJ; Tang, SR; Lei, D (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
EM huajunliu@whu.edu.cn; Lightang@whu.edu.cn; dian_lei@whu.edu.cn;
   zhuq66@263.net; haigang_sui@263.net; 2019282110172@whu.edu.cn;
   ldl1118@whu.edu.cn
FU National Natural Science Foundation of China (NSFC) [41771427, 41631174]
FX This work was funded by the National Natural Science Foundation of China
   (NSFC) (41771427 and 41631174).
CR Ackermann Hanno, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2890, DOI 10.1109/CVPRW.2009.5206664
   Barath D, 2017, PATTERN RECOGN LETT, V94, P7, DOI 10.1016/j.patrec.2017.04.020
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Berg AC, 2005, PROC CVPR IEEE, P26
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Cho M, 2012, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2012.6247701
   Cho M, 2009, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2009.5459322
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geneva P, 2019, PROC CVPR IEEE, P12097, DOI 10.1109/CVPR.2019.01238
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hu YT, 2016, PROC CVPR IEEE, P346, DOI 10.1109/CVPR.2016.44
   Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38
   Jacobs DW, 2001, COMPUT VIS IMAGE UND, V82, P57, DOI 10.1006/cviu.2001.0906
   Jin YX, 2008, IEEE IMAGE PROC, P1572, DOI 10.1109/ICIP.2008.4712069
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li JY, 2017, PHOTOGRAMM ENG REM S, V83, P813, DOI 10.14358/PERS.83.12.813
   Li JY, 2017, ISPRS J PHOTOGRAMM, V132, P61, DOI 10.1016/j.isprsjprs.2017.08.009
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nowruzi FE, 2017, IEEE INT CONF COMP V, P904, DOI 10.1109/ICCVW.2017.111
   Ono Y, 2018, ADV NEUR IN, V31
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang C, 2014, LECT NOTES COMPUT SC, V8690, P788, DOI 10.1007/978-3-319-10605-2_51
   Weinmann M, 2011, ISPRS J PHOTOGRAMM, V66, pS62, DOI 10.1016/j.isprsjprs.2011.09.010
   Wu C., 2011, VisualSFM: A visual structure from motion system
   Yan QA, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12998
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P5957, DOI 10.1109/TIP.2016.2607425
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 37
TC 2
Z9 2
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2091
EP 2103
DI 10.1007/s00371-020-01928-z
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000557738600001
DA 2024-07-18
ER

PT J
AU Liu, HJ
   Li, C
   Lei, D
   Zhu, Q
AF Liu, Huajun
   Li, Chao
   Lei, Dian
   Zhu, Qing
TI Unsupervised video-to-video translation with preservation of frame
   modification tendency
SO VISUAL COMPUTER
LA English
DT Article
DE Video translation; Generative adversarial networks; Unsupervised;
   Spatial-temporal information
AB Tremendous advances have been achieved in image translation with the employment of generative adversarial networks (GANs). With respect to video-to-video translation, similar idea has been leveraged by various researches, which may focus on the associations among relevant frames. However, the existing video-synthesis methods based on GANs do not make full exploitation of the spatial-temporal information in videos, especially in the continuous frames. In this paper, we propose an efficient method to conduct video translation that can preserve the frame modification trends in sequential frames of the original video and smooth the variations between the generated frames. To constrain the consistency of the mentioned tendency between the generated video and the original one, we propose a tendency-invariant loss to impel further exploitation of spatial-temporal information. Experiments show that our method is able to learn more abundant information of adjacent frames and generate more desirable videos than the baselines, i.e., Recycle-GAN and CycleGAN.
C1 [Liu, Huajun; Li, Chao; Lei, Dian] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Zhu, Qing] Southwest Jiaotong Univ, Fac Geosci & Environm Engn, Chengdu, Peoples R China.
C3 Wuhan University; Southwest Jiaotong University
RP Liu, HJ; Lei, D (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
EM huajunliu@whu.edu.cn; ld11118@whu.edu.cn; dian_lei@whu.edu.cn;
   zhuq66@263.net
FU National Natural Science Foundation of China (NSFC) [41771427, 41631174]
FX This work was funded by the National Natural Science Foundation of China
   (NSFC) (41771427 and 41631174).
CR [Anonymous], 2017, Advances in neural information processing systems
   [Anonymous], 2017, P ASM 36 INT C OC
   [Anonymous], 2018, ARXIV180603698
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8
   Benaim S., 2018, PROC NEURAL INF PROC, P2104
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Brock A., 2019, INT C LEARN REPR
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Gafni Oran, 2019, Vid2game: Controllable characters extracted from real-world videos
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kang K, 2017, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2017.101
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D.P., 2013, ARXIV13126114
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Odena A, 2017, PR MACH LEARN RES, V70
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Reed S, 2016, PR MACH LEARN RES, V48
   Richter SR, 2017, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2017.243
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Taigman Y., 2016, INT C LEARN REPR
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Wang T.C., 2018, ARXIV180806601
   Wang T.C., 2019, ARXIV191012713
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Xiao FY, 2018, LECT NOTES COMPUT SC, V11212, P494, DOI 10.1007/978-3-030-01237-3_30
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yuan Q, 2020, VISUAL COMPUT, V36, P1591, DOI 10.1007/s00371-019-01762-y
   Zhang H., 2018, ARXIV180508318
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou YP, 2019, IEEE INT CONF COMP V, P1208, DOI 10.1109/ICCVW.2019.00153
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 3
Z9 3
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2105
EP 2116
DI 10.1007/s00371-020-01913-6
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000551368400003
DA 2024-07-18
ER

PT J
AU Qian, Q
   Wu, XJ
   Kittler, J
   Xu, TY
AF Qian, Qiang
   Wu, Xiao-Jun
   Kittler, Josef
   Xu, Tian-Yang
TI Correlation tracking with implicitly extending search region
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Correlation filter; Boundary effect
ID OBJECT TRACKING
AB Recently, the correlation filters have been successfully applied to visual tracking, but the boundary effect severely limits their tracking performance. In this paper, to overcome this problem, we propose a correlation tracking framework with the capacity implicitly to extend the search region (TESR), while inhibiting the undesirable impact of the background noise. The proposed tracking method is a two-stage detection framework. The search region of the correlation tracker is extended by considering other four search centers, in addition to the target location in the previous frame. Thus our TESR will generate five potential object loactions. Then, an SVM classifier is used to determine the correct target position. We also introduce and apply the salient object detection score to regularize the output of the SVM classifier to improve its performance. The experimental results demonstrate that TESR exhibits superior performance in comparison with the state-of-the-art trackers.
C1 [Qian, Qiang; Wu, Xiao-Jun; Xu, Tian-Yang] Jiangnan Univ, Sch IoT Engn, Wuxi, Jiangsu, Peoples R China.
   [Qian, Qiang] Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Zhenjiang, Jiangsu, Peoples R China.
   [Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford, Surrey, England.
C3 Jiangnan University; Jiangsu University of Science & Technology;
   University of Surrey
RP Wu, XJ (corresponding author), Jiangnan Univ, Sch IoT Engn, Wuxi, Jiangsu, Peoples R China.
EM qianqiang_just@163.com; wu_xiaojun@jiangnan.edu.cn;
   j.kittler@surrey.ac.uk; tianyang_xu@163.com
RI Xu, Tianyang/AAE-1982-2019
OI Xu, Tianyang/0000-0002-9015-3128
FU 111 Project of Chinese Ministry of Education [B12018]; National Natural
   Science Foundation of China [61373055, 61672265]
FX This study was funded by the 111 Project of Chinese Ministry of
   Education under Grant B12018 and the National Natural Science Foundation
   of China under Grant 61373055; 61672265.
CR [Anonymous], 2016, ARXIV160106032
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Atkinson R. C., 1968, Psychology of learning and motivation, V2, P89, DOI [10.1016/S0079-7421(08)60422-3, DOI 10.1016/S0079-7421(08)60422-3, DOI 10.1017/CBO9781316422250.025]
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Bolme DS, 2009, PROC CVPR IEEE, P2105, DOI 10.1109/CVPRW.2009.5206701
   Chen Z., 2015, Comput. Sci., V53, P68
   Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Kristan M., 2014, EUR C COMP VIS
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mbelwa JT, 2019, VISUAL COMPUT, V35, P371, DOI 10.1007/s00371-018-1470-5
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Sui Y, 2016, LECT NOTES COMPUT SC, V9912, P662, DOI 10.1007/978-3-319-46484-8_40
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang Q., 2018, 27 INT JOINT C ART I
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang Qiang, 2018, ARXIV181205050
   Wang X., 2019, ARXIV190307593
   Wang Y, 2019, VISUAL COMPUT, V35, P1641, DOI 10.1007/s00371-018-1563-1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yuan QM, 2019, INT C COMP SUPP COOP, P188, DOI [10.1109/CSCWD.2019.8791880, 10.1109/cscwd.2019.8791880]
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang TZ, 2016, PROC CVPR IEEE, P3880, DOI 10.1109/CVPR.2016.421
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zhuang Wang, 2010, Statistical Analysis and Data Mining, V3, P149, DOI 10.1002/sam.10075
NR 50
TC 2
Z9 2
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1029
EP 1043
DI 10.1007/s00371-020-01850-4
EA MAY 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000535381400001
DA 2024-07-18
ER

PT J
AU Karr, BA
   Debattista, K
   Chalmers, AG
AF Karr, Brian A.
   Debattista, Kurt
   Chalmers, Alan G.
TI Optical effects on HDR calibration via a multiple exposure noise-based
   workflow
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range; Veiling glare; Lens flare; vignette; Multiple
   exposure recombination; Radiometric calibration; Photon transfer curve;
   Flat field
ID VEILING GLARE
AB High dynamic range (HDR) technology allows more of the lighting in a specific scene to be captured at a set point in time, and thus is capable of delivering an overall view of the scene that more closely correlates with our visual experience in the real world, compared to standard, or low dynamic range (LDR) technology. Although HDR capabilities of single exposure capture systems are improving, the traditional method for creating HDR images still includes combing a number of different exposures, captured with an LDR system, into a single HDR image. Several use cases requiring absolute calibration of the resulting HDR luminance map have been undertaken, but none of these have provided a detailed analysis of the optical effects of glare on the results. We develop a calibrated HDR radiance map, including methodical linearization of captured image data, while characterizing the limitations due to the effects of optical glare. A purposely designed controlled test scene is used to challenge the calibrated reconstruction efforts, including low luminance levels, spatial inclusion of lens vignette over the full imaged area, and optical glare. Results demonstrate that even with careful processing and recombination of the LDR data, radiometric accuracy is limited as a result of glare. The proposed approach performs better than calibration methods in commercially available HDR recombination software.
C1 [Karr, Brian A.; Chalmers, Alan G.] Univ Warwick, Warwick Mfg Grp, Coventry, W Midlands, England.
   [Debattista, Kurt] Univ Warwick, Warwick Mfg Grp, Visualizat Grp, Coventry, W Midlands, England.
   [Karr, Brian A.] Rockledge Design Grp Inc, Rockledge, FL 32955 USA.
C3 University of Warwick; University of Warwick
RP Karr, BA (corresponding author), Univ Warwick, Warwick Mfg Grp, Coventry, W Midlands, England.; Karr, BA (corresponding author), Rockledge Design Grp Inc, Rockledge, FL 32955 USA.
EM b.karr@warwick.ac.uk
CR Akyüz AO, 2007, J VIS COMMUN IMAGE R, V18, P366, DOI 10.1016/j.jvcir.2007.04.001
   [Anonymous], 1999, IEEE COMP SOC C COMP
   [Anonymous], 2010, EMVA Standard 1288
   Arens EE, 2014, INT J HYDROGEN ENERG, V39, P9545, DOI 10.1016/j.ijhydene.2014.04.043
   Ashok S., 1995, 10 COMP AER C COMP A, P29
   Coffin D., 2017, DECODING RAW DIGITAL
   Coutelier B, 2003, LUMINANCE CALIBRATIO, V152, pD3
   Cozzi F, 2018, J IMAGING, V4, DOI 10.3390/jimaging4080100
   Debevec P.E., 2008, P 24 ANN C COM GRAPH, P1
   DiCarlo JM, 2000, PROC SPIE, V3965, P392, DOI 10.1117/12.385456
   Granados M, 2010, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2010.5540208
   Griffiths DJ, 2016, APPL OPTICS, V55, pC9, DOI 10.1364/AO.55.0000C9
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Heidrich Wolfgang, ERIK REINHARD
   Inanici MN, 2006, LIGHTING RES TECHNOL, V38, P123, DOI [10.1191/1365782806li164oa, 10.1191/13657828061i164oa]
   ISO, 1994, 93581994 ISO
   ISO, 2013, 157392013 BSI
   Janesick J.R., 2007, Photon Transfer DN
   Janesick J. R., 2001, SCI CHARGE COUPLED D
   Karr B, 2017, HIGH DYNAMIC RANGE VIDEO: CONCEPTS, TECHNOLOGIES, AND APPLICATIONS, P87, DOI 10.1016/B978-0-12-809477-8.00004-2
   Karr BA, 2016, HIGH DYNAMIC RANGE A
   Kirk K., 2006, BMVC, P1129
   Larson G.W, 2005, TECHNICAL REPORT
   Larson G.W., 1998, Rendering with radiance: the art and science of lighting visualization
   Larson GregW., 1992, GRAPHICS GEMS 2, P80, DOI [10.1016/B978-0-08-050754-5.50025-6, DOI 10.1016/B978-0-08-050754-5.50025-6]
   Madzsar G.C., 1992, TECHNICAL REPORT NAT
   Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485
   Mann S., 1995, P IS T 48 ANN C SOC
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   McCann JJ, 2007, J SOC INF DISPLAY, V15, P721, DOI 10.1889/1.2785205
   McCann JJ, 2018, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02079
   Olansen J. B., 2014, AIAA SPACE 2014 C EX, P4314
   Popa IC, 2016, 2016 INT C APPL THEO, P1, DOI [10.1109/ICATE.2016.7754608, DOI 10.1109/ICATE.2016.7754608]
   Raskar R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360655
   Reinhart CF, 2000, ENERG BUILDINGS, V32, P167, DOI 10.1016/S0378-7788(00)00042-6
   Robertson M. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P159, DOI 10.1109/ICIP.1999.817091
   Rouf M, 2011, PROC CVPR IEEE, P288
   Safranek S. F, COMPARISON TECHNIQUE
   Talvala EV, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239488
   Tomic I., 2014, J GRAPH ENG DES, V5, P23
   Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555
   Ward G, 2010, PHOTOSPHERE SOFTWARE
NR 43
TC 3
Z9 3
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 895
EP 910
DI 10.1007/s00371-020-01841-5
EA APR 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000527521600002
OA hybrid
DA 2024-07-18
ER

PT J
AU Hu, L
   Xiao, J
   Wang, Y
AF Hu, Liang
   Xiao, Jun
   Wang, Ying
TI An automatic 3D registration method for rock mass point clouds based on
   plane detection and polygon matching
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic registration; Rock mass; Plane detection; Polygon matching
ID LIDAR
AB Point cloud registration is an essential step in the process of 3D reconstruction. Considering that the surface of rock mass is complex and mainly composed of planes, in this paper, we introduce a novel and automatic 3D registration method for rock mass point clouds based on plane detection and polygon matching. Firstly, planes are detected from rock mass point clouds by an efficient tripe-region growing method, and then, the corresponding polygons are calculated by concave hull method. Secondly, PCA-based polygon matching procedure is used for coarse registration. Finally, ICP method is applied to fine registration. The performance of this method was tested in different rock mass point clouds. Compared with the existing methods, the proposed method demonstrates a reliable and stable solution for accurately registering in rock mass scenes.
C1 [Hu, Liang; Xiao, Jun; Wang, Ying] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xiao, J (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM xiaojun@ucas.ac.cn
OI Xiao, Jun/0000-0002-1799-3948
FU National Natural Science Foundation of China [61471338]; Youth
   Innovation Promotion Association CAS [2015361]; Key Research Program of
   Frontier Sciences CAS [QYZDY-SSW-SYS004]; Beijing Nova Program
   [Z171100001117048]; Beijing Science and Technology Project
   [Z181100003818019]; President Fund of UCAS
FX This work is supported by the National Natural Science Foundation of
   China (No. 61471338), Youth Innovation Promotion Association CAS
   (2015361), Key Research Program of Frontier Sciences CAS
   (QYZDY-SSW-SYS004), Beijing Nova Program (Z171100001117048), Beijing
   Science and Technology Project (Z181100003818019) and President Fund of
   UCAS.
CR Abellán A, 2014, EARTH SURF PROC LAND, V39, P80, DOI 10.1002/esp.3493
   [Anonymous], COMPUT GRAPH FORUM
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Borrmann D, 2011, 3D RES, V2, DOI 10.1007/3DRes.02(2011)3
   Díez Y, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2692160
   Elbaz G, 2017, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2017.265
   Gigli G, 2009, NAT HAZARD EARTH SYS, V9, P1759, DOI 10.5194/nhess-9-1759-2009
   Gomes RK, 2016, COMPUT GEOSCI-UK, V90, P170, DOI 10.1016/j.cageo.2016.02.011
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2013, INT C COMM SIG PROC
   Holz D, 2015, IEEE ROBOT AUTOM MAG, V22, P110, DOI 10.1109/MRA.2015.2432331
   Holz D, 2013, ADV INTELL SYST, V194, P61
   Junhao Xiao, 2011, Proceedings of the 2011 IEEE International Conference on Mechatronics and Automation (ICMA 2011), P1768, DOI 10.1109/ICMA.2011.5986247
   Lato M, 2013, COMPUT GEOSCI-UK, V50, P106, DOI 10.1016/j.cageo.2012.06.014
   Lato MJ, 2012, INT J ROCK MECH MIN, V54, P150, DOI 10.1016/j.ijrmms.2012.06.003
   Leng XX, 2016, PHOTOGRAMM REC, V31, P166, DOI 10.1111/phor.12145
   Limberger FA, 2015, PATTERN RECOGN, V48, P2043, DOI 10.1016/j.patcog.2014.12.020
   Lin H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461969
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mohamad M, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P598, DOI 10.1109/3DV.2015.74
   Montuori A, 2014, INT GEOSCI REMOTE SE, P4086, DOI 10.1109/IGARSS.2014.6947384
   Oesau S, 2016, COMPUT GRAPH FORUM, V35, P203, DOI 10.1111/cgf.12720
   Pomerleau F, 2011, IEEE INT C INT ROBOT, P3824, DOI 10.1109/IROS.2011.6048545
   Poppinga J, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3378, DOI 10.1109/IROS.2008.4650729
   Priest S.D., 2012, DISCONTINUITY ANAL R
   Restrepo MI, 2014, ISPRS J PHOTOGRAMM, V98, P1, DOI 10.1016/j.isprsjprs.2014.09.010
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Vöge M, 2013, ENG GEOL, V164, P155, DOI 10.1016/j.enggeo.2013.07.008
   Wang JF, 2015, PROCEEDINGS OF THE 2015 6TH INTERNATIONAL CONFERENCE ON AUTOMATION, ROBOTICS AND APPLICATIONS (ICARA), P456, DOI 10.1109/ICARA.2015.7081191
   Xian YR, 2019, FRONT COMPUT SCI-CHI, V13, P170, DOI 10.1007/s11704-016-6191-1
   Xiao JH, 2013, ROBOT AUTON SYST, V61, P1641, DOI 10.1016/j.robot.2013.07.001
   Yan WY, 2015, REMOTE SENS ENVIRON, V158, P295, DOI 10.1016/j.rse.2014.11.001
   Yang BS, 2014, ISPRS J PHOTOGRAMM, V95, P109, DOI 10.1016/j.isprsjprs.2014.05.012
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Yang MY, 2011, ISPRS J PHOTOGRAMM, V66, pS52, DOI 10.1016/j.isprsjprs.2011.09.004
NR 37
TC 20
Z9 22
U1 7
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 669
EP 681
DI 10.1007/s00371-019-01648-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800002
DA 2024-07-18
ER

PT J
AU Li, CH
   Lin, SK
   Qiao, JZ
   An, S
AF Li, Chuanhao
   Lin, Shukuan
   Qiao, Jianzhong
   An, Shan
TI Partial tracking method based on siamese network
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time object tracking; Siamese network; Part-based tracking method
ID OBJECT TRACKING
AB Robust object tracking is still a challenging task in the field of computer vision and has application value in many fields such as automatic driving, human-computer interaction and robot visual navigation. More and more researchers are devoted to researching more accurate object tracking methods. How to better deal with occlusion and deformation has always been the difficult challenges in the object tracking field, and the existing methods cannot solve these problems well. In this regard, we propose a novel, effective and portable module called part-based tracking and assembly (PTA), which is added to the fully convolutional siamese networks to divide the exemplar feature map into several parts. Each part is separately tracked, and then the tracking results of all parts are assembled to obtain the final tracking results. And the experiments on several popular tracking benchmarks show our variant trackers with the PTA module that operate at almost the same tracking speed with the original trackers and achieve superior tracking performance. Moreover, the tracking accuracy is significantly improved on the data with occlusion, deformation and background clutter. Compared with some real-time tracking methods, our variant trackers with the PTA module can achieve the state-of-the-art performance.
C1 [Li, Chuanhao; Lin, Shukuan; Qiao, Jianzhong; An, Shan] Northeastern Univ, Sch Comp Sci & Engn, Shenyang, Peoples R China.
C3 Northeastern University - China
RP Lin, SK (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang, Peoples R China.
EM 13359853236@163.com; linshukuan@cse.neu.edu.cn;
   qiaojianzhong@cse.neu.edu.cn; An943314718@163.com
OI Li, Chuanhao/0000-0002-5769-3739
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   [Anonymous], 2018, SIEM REAP
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dalal N., CVPR, P886, DOI [DOI 10.1109/CVPR.2005.177, 10.1109/CVPR.2005.177]
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong XW, 2018, IEEE CONF COMPUT
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu HW, 2018, IEEE T NEUR NET LEAR, V29, P1786, DOI 10.1109/TNNLS.2017.2688448
   Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Ma B, 2016, IEEE T CYBERNETICS, V46, P2411, DOI 10.1109/TCYB.2015.2477879
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Nam H., 2016, CORR
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Shen JB, 2018, IEEE T IMAGE PROCESS, V27, P2688, DOI 10.1109/TIP.2018.2795740
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang YH, 2018, LECT NOTES COMPUT SC, V11213, P355, DOI 10.1007/978-3-030-01240-3_22
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 49
TC 4
Z9 4
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 587
EP 601
DI 10.1007/s00371-020-01825-5
EA MAR 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000561626400001
DA 2024-07-18
ER

PT J
AU Dong, EN
   Deng, MT
   Wang, ZH
AF Dong, Enzeng
   Deng, Mengtao
   Wang, Zenghui
TI A robust tracking algorithm with on online detector and high-confidence
   updating strategy
SO VISUAL COMPUTER
LA English
DT Article
DE Correlation filter; PSR; Confidence degree; Consensus voting; Keypoints
   matching
ID VISUAL TRACKING; OBJECT TRACKING
AB The discriminative correlation filter-based tracking algorithms cannot correctly track the target if the target is occluded or out of view and reappears in the field of vision, and they cannot ensure the tracking model is updated correctly if the tracking information is not correct. In this paper, a robust correlation tracking algorithm is proposed. Here, a failure detection strategy, which is based on the maximal confidence score and peak-to-sidelobe ratio to detect or measure the reliability of the tracking result, is integrated into the tracker. Moreover, the redetection module based on the keypoints matching method for consensus voting is introduced into the proposed tracking algorithm to redetect objects in case of tracking failure. In addition, an adaptive high-confidence updating method is proposed to avoid error model information introduced into the tracker caused by occlusions, out-of-view or illumination changes, where the learning rate is determined by the change rate of the confidence map. The OTB-2015 dataset and VOT-2016 dataset are used to evaluate the performance of the proposed tracking algorithm. The experimental results show that the proposed tracking algorithm performs better than most of the state-of-the-art trackers, and it has higher accuracy and robustness than the DSST tracker.
C1 [Dong, Enzeng; Deng, Mengtao] Tianjin Univ Technol, Tianjin Key Lab Control Theory & Applicat Complic, Tianjin 300384, Peoples R China.
   [Wang, Zenghui] Univ South Africa, Dept Elect & Min Engn, ZA-1710 Florida, South Africa.
C3 Tianjin University of Technology; University of South Africa
RP Dong, EN (corresponding author), Tianjin Univ Technol, Tianjin Key Lab Control Theory & Applicat Complic, Tianjin 300384, Peoples R China.
EM dongenzeng@163.com
RI Wang, Zenghui/B-8280-2015
FU Natural Science Foundation of China [61603274]; Natural Science
   Foundation of Tianjin [18JCYBJC87700]; South African National Research
   Foundation [112108, 112142, 114911]; Eskom Tertiary Education Support
   Programme Grant
FX This work was partially supported by the Natural Science Foundation of
   China (No. 61603274), the Natural Science Foundation of Tianjin (No.
   18JCYBJC87700), South African National Research Foundation Grants (Nos.
   112108 and 112142), and South African National Research Foundation
   Incentive Grant (No. 114911), and Eskom Tertiary Education Support
   Programme Grant.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Danelljan M, 2016, 2016 IEEE C COMP VIS
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, BRIT MACH VIS C
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li CL, 2019, IEEE T PATTERN ANAL, V41, P2770, DOI 10.1109/TPAMI.2018.2864965
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu D., 2015, 7 INT S COMP INT DES
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wu K, 2015, International Conference on Mechanics, Building Material and Civil Engineering (MBMCE 2015), P864
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Zhang DJ, 2020, VISUAL COMPUT, V36, P509, DOI 10.1007/s00371-019-01634-5
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
NR 32
TC 2
Z9 2
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 567
EP 585
DI 10.1007/s00371-020-01824-6
EA FEB 2020
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000517006400001
DA 2024-07-18
ER

PT J
AU Fan, CD
   Hu, CH
   Liu, B
AF Fan, Changde
   Hu, Chunhai
   Liu, Bin
TI Linearized kernel dictionary learning with group sparse priors for
   action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Dictionary learning; Action recognition; Linearized kernel method; Group
   sparse
ID K-SVD; ALGORITHM
AB Classification-driven dictionary has been successfully used in pattern recognition and computer vision in recent years. In this paper, a discriminative dictionary is constructed by concatenating all class specific sub-dictionaries and one sub-dictionary containing the common patterns. To further enhance the discriminative power, we also propose to use group sparse priors in the coding stage of the dictionary learning process. A kernel dictionary is learned to solve the same direction distribution problem existing in the traditional dictionary learning framework. Actually, the kernel dictionary is learned in a linearized manner by using virtual features. We evaluate our method on three public action datasets including facial expression, Hand Gesture and UCF Sports. Experimental results demonstrate that our method can achieve the better or at least competitive performance when compared with other action recognition methods.
C1 [Fan, Changde; Hu, Chunhai; Liu, Bin] Yanshan Univ, Sch Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University
RP Hu, CH (corresponding author), Yanshan Univ, Sch Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
EM changdefan@stumail.ysu.edu.cn; fred-hu@ysu.edu.cn; dqlb@ysu.edu.cn
RI liu, bb/GXA-2527-2022
OI changde, fan/0000-0003-4965-0488
FU Hebei Province Science and Technology Support Program [15220324]
FX This study was funded by the Hebei Province Science and Technology
   Support Program (No. 15220324).
CR Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2014, NEURAL INFORM PROCES
   [Anonymous], 2016, IEEE C COMP VIS PATT
   Barnachon M, 2014, PATTERN RECOGN, V47, P238, DOI 10.1016/j.patcog.2013.06.020
   Bian ZP, 2015, IEEE J BIOMED HEALTH, V19, P430, DOI 10.1109/JBHI.2014.2319372
   Chi Y.T., 2013, IEEE C COMP VIS PATT
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Fernández-Caballero A, 2012, EXPERT SYST APPL, V39, P6982, DOI 10.1016/j.eswa.2012.01.050
   Gao S., 2010, EUR C COMP VIS ECCV
   Golts A, 2016, IEEE J-STSP, V10, P726, DOI 10.1109/JSTSP.2016.2555241
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Kong Y, 2014, IEEE T PATTERN ANAL, V36, P1775, DOI 10.1109/TPAMI.2014.2303090
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Liu BD, 2016, NEUROCOMPUTING, V204, P198, DOI 10.1016/j.neucom.2015.08.128
   Mairal J., 2008, Neural Information Processing Systems (NIPS)
   Meng Yang, 2011, INT C COMP VIS ICCV
   Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nguyen H.V., 2012, IEEE INT C AC SPEECH
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Suo Y., 2014, 2014 IEEE INT C IM P
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan Y, 2015, IEEE T IMAGE PROCESS, V24, P1867, DOI 10.1109/TIP.2015.2413294
   Yin J, 2012, NEUROCOMPUTING, V77, P120, DOI 10.1016/j.neucom.2011.08.018
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhu Y., 2010, ASIAN C COMP VIS ACC
NR 38
TC 6
Z9 6
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1797
EP 1807
DI 10.1007/s00371-018-1603-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300009
DA 2024-07-18
ER

PT J
AU Cai, Z
   Zhang, K
   Hu, DN
AF Cai, Zhuang
   Zhang, Kang
   Hu, Dong-Ni
TI Visualizing large graphs by layering and bundling graph edges
SO VISUAL COMPUTER
LA English
DT Article
DE Graph visualization; Visual clutter; Edge bundling; Edge routing;
   Layered approach; Evaluation
ID GEOMETRY
AB Edge bundling has been widely used to reduce visual clutter and reveal high-level edge patterns for large graphs. Due to strong edge attraction, bundled results often show unnecessary curvature and tangling at bundle intersections. Inappropriate bundling may fail to reveal true data patterns and even mislead users. This paper presents a parameterizable 6-step edge bundling approach called LEB that reveals the patterns of the input graph, with distinguishable and traceable bundles. The bundling results by LEB are also adjustable by tuning a small number of parameters. We have conducted a user experiment to test and compare LEB with previous approaches. The experiment on three datasets (including two common ones) demonstrates LEB's superiority over previous approaches in visualizing data patterns. Our implementation with reusable computation also delivers an execution speed fast enough for real-time interaction and animation.
C1 [Cai, Zhuang; Hu, Dong-Ni] Tianjin Univ, Sch Comp Software, Tianjin, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.
   [Zhang, Kang] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.
C3 Tianjin University; University of Texas System; University of Texas
   Dallas; Macau University of Science & Technology
RP Zhang, K (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.; Zhang, K (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.
EM caizhuang@tju.edu.cn; kzhang@mdallas.edu; dongnihu@tju.edu.cn
OI Zhang, Kang/0000-0003-3802-7535
CR [Anonymous], 2015, EXCEL 2007 HISTOGRAM
   [Anonymous], 1994, ACM Transactions on Computer-Human Interaction, DOI [10.1145/180171.180173, DOI 10.1145/180171.180173]
   Cui WW, 2008, IEEE T VIS COMPUT GR, V14, P1277, DOI 10.1109/TVCG.2008.135
   DECHTER R, 1936, ACM, V32, P505
   DOANE DP, 1976, AM STAT, V30, P181, DOI 10.2307/2683757
   Ellis G, 2007, IEEE T VIS COMPUT GR, V13, P1216, DOI 10.1109/TVCG.2007.70535
   Ersoy O, 2011, IEEE T VIS COMPUT GR, V17, P2364, DOI 10.1109/TVCG.2011.233
   FREEDMAN D, 1949, PROBAB THEORY REL, V57, P453
   Gansner ER, 2011, IEEE PAC VIS SYMP, P187, DOI 10.1109/PACIFICVIS.2011.5742389
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   HARTUV E, 1981, INFORM PROCESS LETT, V76, P175
   Healey CG, 1996, IEEE VISUAL, P263, DOI 10.1109/VISUAL.1996.568118
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   Holten D, 2009, COMPUT GRAPH FORUM, V28, P983, DOI 10.1111/j.1467-8659.2009.01450.x
   Hurter C, 2012, COMPUT GRAPH FORUM, V31, P865, DOI 10.1111/j.1467-8659.2012.03079.x
   Koenig S, 2004, AI MAG, V25, P99
   Lambert A, 2010, COMPUT GRAPH FORUM, V29, P853, DOI 10.1111/j.1467-8659.2009.01700.x
   Lee B., 2006, P AVI WORKSH TIM ERR, P1, DOI DOI 10.1145/1168149.1168168
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Luo SJ, 2012, IEEE T VIS COMPUT GR, V18, P810, DOI 10.1109/TVCG.2011.104
   McDonnell KT, 2008, COMPUT GRAPH FORUM, V27, P1031, DOI 10.1111/j.1467-8659.2008.01239.x
   Peng DC, 2012, IEEE PAC VIS SYMP, P65, DOI 10.1109/PacificVis.2012.6183575
   Peng W, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P89
   Phan D, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P219, DOI 10.1109/INFVIS.2005.1532150
   Pupyrev S, 2012, LECT NOTES COMPUT SC, V7034, P136
   Qu HM, 2007, LECT NOTES COMPUT SC, V4372, P399
   Nguyen Q, 2012, LECT NOTES COMPUT SC, V7034, P123
   Selassie D, 2011, IEEE T VIS COMPUT GR, V17, P2354, DOI 10.1109/TVCG.2011.190
   Shimazaki H, 2007, NEURAL COMPUT, V19, P1503, DOI 10.1162/neco.2007.19.6.1503
   Sturges HA, 1926, J AM STAT ASSOC, V21, P65, DOI 10.1080/01621459.1926.10502161
   Tarjan R., 1972, SIAM Journal on Computing, V1, P146, DOI 10.1137/0201010
   Telea A, 2010, COMPUT GRAPH FORUM, V29, P843, DOI 10.1111/j.1467-8659.2009.01680.x
   Tennekes M, 2014, IEEE T VIS COMPUT GR, V20, P2072, DOI 10.1109/TVCG.2014.2346277
   Verbeek K, 2011, IEEE T VIS COMPUT GR, V17, P2536, DOI 10.1109/TVCG.2011.202
   Wong N, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P51, DOI 10.1109/INFVIS.2003.1249008
   Ying-Huey Fua, 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P43, DOI 10.1109/VISUAL.1999.809866
   Zhou H, 2008, COMPUT GRAPH FORUM, V27, P1047, DOI 10.1111/j.1467-8659.2008.01241.x
   Zhou H, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P55
   Zhou H, 2013, TSINGHUA SCI TECHNOL, V18, P145, DOI 10.1109/TST.2013.6509098
   Zinsmaier M, 2012, IEEE T VIS COMPUT GR, V18, P2486, DOI 10.1109/TVCG.2012.238
NR 40
TC 3
Z9 3
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 739
EP 751
DI 10.1007/s00371-018-1509-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900010
DA 2024-07-18
ER

PT J
AU Gan, JB
   Wilbert, A
   Thormählen, T
   Drescher, P
   Hagens, R
AF Gan, Jiangbin
   Wilbert, Alwin
   Thormaehlen, Thorsten
   Drescher, Philip
   Hagens, Ralf
TI Multi-view photometric stereo using surface deformation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D Reconstruction; Surface deformation; Photometric stereo; Multi-view
   stereo
ID REFLECTANCE
AB This paper presents a hybrid approach for 3D reconstruction by fusing photometric stereo and multi-view stereo. The 3D surface is obtained by capturing a set of images taken from different viewpoints under time-varying illuminations. Key factors in the reconstruction process are surface normals that are obtained from photometric stereo. The surface is initialized by integrating the normals and then refined by performing iterative deformations on the initial surface and thereby optimizing image and normal consistency in multiple views. Benefiting from the employment of the deformation approach, we are able to perform image and normal consistency optimization without using matching windows. Instead, always the complete surface is back-projected. This makes the proposed approach much simpler and more robust compared to window-based approaches, which typically require global optimization with constraints on neighboring windows. Experiments on real-world data and ground-truth data show that for diffuse midsized objects without large depth discontinuities our approach improves the accuracy of the reconstructions compared to exiting approaches.
C1 [Gan, Jiangbin; Wilbert, Alwin; Thormaehlen, Thorsten] Univ Marburg, Marburg, Germany.
   [Drescher, Philip; Hagens, Ralf] Beiersdorf AG, Hamburg, Germany.
C3 Philipps University Marburg; Beiersdorf AG
RP Gan, JB (corresponding author), Univ Marburg, Marburg, Germany.
EM ganjiangbin@mathematik.uni-marburg.de
CR Aittala M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461978
   Alldrin N., 2008, P IEEE COMP SOC C CO
   Anderson R, 2011, IEEE I CONF COMP VIS, P2182, DOI 10.1109/ICCV.2011.6126495
   Basri R., 2001, P COMP VIS PATT REC, pII
   Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269
   Chandraker Manmohan, 2007, P COMP VIS PATT REC
   Chandraker MK, 2005, PROC CVPR IEEE, P788
   Favaro P, 2012, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2012.6247754
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Goldman D. B., 2005, IEEE INT C COMP VIS
   Grochulla M., 2015, 12 EUR C VIS MED PRO
   Grochulla M, 2011, LECT NOTES COMPUT SC, V6930, P110
   Hernández C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Horovitz I., 2000, TECHNICAL REPORT
   Ma W.-C., 2007, Proceedings of the 18th Eurographics Conference on Rendering Techniques, EGSR'07, V2007, P183, DOI 10.2312/EGWR/EGSR07/183-194
   Mei X., 2011, GPUCV
   Nayar S. K., 1989, MIV-89 Proceedings of the International Workshop on Industrial Applications of Machine Intelligence and Vision (Seiken Symposium) (Cat. No.89TH0250-1), P169, DOI 10.1109/MIV.1989.40544
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Nehab D, 2006, MESH OPT COMBINING P
   Okabe T, 2009, IEEE I CONF COMP VIS, P1693
   Park J., 2013, P INT C COMP VIS ICC
   Shi B., 2014, INT C 3D VIS 3DV
   Shi B., 2012, P IEEE C COMP VIS PA
   Shi BX, 2012, LECT NOTES COMPUT SC, V7574, P455, DOI 10.1007/978-3-642-33712-3_33
   Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19
   Vlasic D., 2009, ACM SIGGRAPH ASIA 20, P1, DOI [10.1145/1661412.1618520, DOI 10.1145/1661412.1618520]
   Wilson CA, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731055
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Wu C., 2011, VisualSFM: A visual structure from motion system
   Wu C., 2011, CVPR
   Wu CL, 2011, IEEE T VIS COMPUT GR, V17, P1082, DOI [10.1109/TVCG.2010.224, 10.1109/TPDS.2010.224]
   Xu S, 2008, PATTERN RECOGN LETT, V29, P1639, DOI 10.1016/j.patrec.2008.04.007
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zhang Q, 2012, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2012.6247962
   Zheng E., 2014, P CVPR
NR 36
TC 3
Z9 3
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1551
EP 1561
DI 10.1007/s00371-017-1430-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700007
DA 2024-07-18
ER

PT J
AU Bhunre, PK
   Bhowmick, P
AF Bhunre, Piyush Kanti
   Bhowmick, Partha
TI Carve in, carve out: a bimodal carving through voxelization and
   functional partitioning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Carving; Digital geometry; Pattern mapping; Voxelization; Voxel topology
AB We propose in this paper a novel technique for pattern-guided carving on an orientable 2-manifold surface. Its novelty lies in processing the surface in voxel space using certain theories and deductions of digital geometry. The carving pipeline designed by us is bimodal in nature, as it can generate both 'negative' and 'positive' carvings by carve in and carve out alongside the specified pattern. The 2D pattern is easily mapped to the 3D surface, as we consider the thinnest voxelized model. We perform functional partition of the voxelized surface and use a local optimization with these components in order to achieve a realistic carving. Necessary theoretical foundations, implementation details, and experimental results have been furnished to adjudge the merit of the proposed technique.
C1 [Bhunre, Piyush Kanti] Indian Inst Technol, ATDC, Kharagpur, W Bengal, India.
   [Bhowmick, Partha] Indian Inst Technol, CSE Dept, Kharagpur, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kharagpur
RP Bhowmick, P (corresponding author), Indian Inst Technol, CSE Dept, Kharagpur, W Bengal, India.
EM kbpiyush@gmail.com; pb@cse.iitkgp.ac.in
RI Bhunre, Piyush/AAA-1373-2021
CR Alexa M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2999536
   [Anonymous], 2015, ACM T GRAPHICS
   [Anonymous], 2011, P ACM SIGGRAPH S HIG
   Barneva RP, 2000, THEOR COMPUT SCI, V246, P73, DOI 10.1016/S0304-3975(98)00346-6
   Barton W., 2007, COMPLETE GUIDE CHIP
   Bhunre PK, 2019, INFORM SCIENCES, V499, P102, DOI 10.1016/j.ins.2018.03.006
   Brimkov V, 2007, DISCRETE APPL MATH, V155, P468, DOI 10.1016/j.dam.2006.08.004
   Brimkov VE, 2005, DISCRETE APPL MATH, V147, P169, DOI 10.1016/j.dam.2004.09.010
   Brunton A., 2015, ACM T GRAPHIC, V35
   Chen YT, 2008, COMPUT AIDED DESIGN, V40, P123, DOI 10.1016/j.cad.2007.06.013
   COHENOR D, 1995, GRAPH MODEL IM PROC, V57, P453, DOI 10.1006/gmip.1995.1039
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dachille F, 2000, PROC GRAPH INTERF, P205
   Dionne Olivier., 2013, P 12 ACM SIGGRAPH EU, P173, DOI DOI 10.1145/2485895.2485919
   DUMAS J, 2015, ACM T GRAPHIC, V34
   Fei Y., 2012, Proceedings of Graphics Interface 2012, GI'12, P9
   Gershenfeld N.A., 1999, The Nature of Mathematical Modeling
   Huang J, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P119, DOI 10.1109/SVV.1998.729593
   Kämpe V, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462024
   Karabassi E.-A., 1999, Journal of Graphics Tools, V4, P5, DOI 10.1080/10867651.1999.10487510
   Klette R., 2004, DIGITAL GEOMETRY GEO
   Koa MD, 2014, VISUAL COMPUT, V30, P821, DOI 10.1007/s00371-014-0952-3
   Laine S., 2010, Proceedings of the Symposium on Interactive 3D Graphics and Games, P55, DOI DOI 10.1145/1730804.1730814
   Laine S., 2016, US Patent, Patent No. [9,245,363, 9245363]
   Laine S, 2013, COMPUT GRAPH FORUM, V32, P77, DOI 10.1111/cgf.12153
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Lozano-Durán A, 2016, ACM T MATH SOFTWARE, V42, DOI [10.1145/28450760, 10.1145/2845076]
   NieBner M., 2013, ACM T GRAPHIC, V32
   Pasko A, 2001, COMPUT AIDED DESIGN, V33, P379, DOI 10.1016/S0010-4485(00)00129-9
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Schwarz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866201
   Sintorn E., 2014, ACM T GRAPHIC, V33
   Wu J, 2016, IEEE T VIS COMPUT GR, V22, P1195, DOI 10.1109/TVCG.2015.2502588
   Zhang J, 2012, PROCEEDINGS OF THE ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON GEOSTREAMING (IWGS) 2012, P101
   Zhang L, 2007, VISUAL COMPUT, V23, P783, DOI 10.1007/s00371-007-0149-0
   Zhao S., 2013, ACM T GRAPHIC, V32
NR 36
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1009
EP 1019
DI 10.1007/s00371-018-1527-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400023
DA 2024-07-18
ER

PT J
AU Sharma, O
   Pandey, J
   Akhtar, H
   Rathee, G
AF Sharma, Ojaswa
   Pandey, Jalaj
   Akhtar, Hammad
   Rathee, Gaurav
TI Navigation in AR based on digital replicas
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Geometric computation; Space identification; 3D navigation; Indoor
   positioning; Augmented reality
ID MODELS
AB In this paper, we address the two main topics of 3D navigation and space identification within the context of mixed reality. Creating navigable digital replicas from real-life buildings is a cumbersome task. We present a mostly automated pipeline to process 3D geometry created from architectural blueprints. We discuss a coherent procedural approach to build the topological information required for navigation and a semiautomatic generation of hierarchical tags for identification of spaces. The geometric and topological information along with tags is stored in a spatial database. We address challenges in automating the entire process such that manual effort is reduced to minimal. Our approach to asset creation enables navigation and identification in both indoor and outdoor spaces. Such a digital infrastructure is central to any VR and AR system that utilizes these assets for further computations.
C1 [Sharma, Ojaswa; Pandey, Jalaj; Akhtar, Hammad; Rathee, Gaurav] Indraprastha Inst Informat Technol Delhi, New Delhi, India.
C3 Indraprastha Institute of Information Technology Delhi
RP Sharma, O (corresponding author), Indraprastha Inst Informat Technol Delhi, New Delhi, India.
EM ojaswa@gmail.com
OI Sharma, Ojaswa/0000-0002-9902-1367
FU Science and Engineering Research Board (SERB) of Department of Science
   and Technology (DST) of India [ECR/2015/000006]
FX This research was supported by Science and Engineering Research Board
   (SERB) of Department of Science and Technology (DST) of India (Grant No.
   ECR/2015/000006).
CR Alnabhan A, 2014, P 6 ACM SIGSPATIAL I, P36
   [Anonymous], 2012, ROBOT AUTON SYST
   [Anonymous], 1999, P C HUM FACT COMP SY
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bartie P.J., 2006, Transactions in GIS, V10, P63
   Beomju Shin, 2012, Proceedings of the 2012 8th International Conference on Computing Technology and Information Management (NCM and ICNIT), P574
   Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940
   Craig AlanB., 2013, UNDERSTANDING AUGMEN
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   El-Hakim SF, 2004, IEEE COMPUT GRAPH, V24, P21, DOI 10.1109/MCG.2004.1297007
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Feiner S., 2016, PERS TECHNOL, V1, P208
   Grinberg M., 2014, Flask Web Development: Developing Web Applications With Python
   Heckbert P.S., 1990, Graphics gems, V275, P721
   Hollerer T., 1999, Digest of Papers. Third International Symposium on Wearable Computers, P79, DOI 10.1109/ISWC.1999.806664
   Kalkusch M., 2002, First IEEE International Augmented Reality Toolkit Workshop. Proceedings (Cat. No.02EX632), DOI 10.1109/ART.2002.1107018
   Langlotz T, 2011, COMPUT GRAPH-UK, V35, P831, DOI 10.1016/j.cag.2011.04.004
   Lewis R, 1998, COMPUT AIDED DESIGN, V30, P765, DOI 10.1016/S0010-4485(98)00031-1
   Low C.Y., 2014, INT C FRONTIERS COMM, P1
   Meyer F., 1990, Journal of Visual Communication and Image Representation, V1, P21, DOI 10.1016/1047-3203(90)90014-M
   Min P., 2014, Binvox 3D Mesh Voxelizer
   Mulloni Alessandro, 2011, P 13 INT C HUM COMP, P211
   Musialski P., 2016, COMPUT GRAPH FORUM, V32, P146
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Pandey J., 2016, 24 C COMP GRAPH VIS, V24, P335
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Soille P., 2013, MORPHOLOGICAL IMAGE
   Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Vlahakis V, 2001, VIRTUAL REALITY ARCH, V9
   Wagner D, 2010, P IEEE VIRT REAL ANN, P211, DOI 10.1109/VR.2010.5444786
   Yin XT, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.9
   Zhu J., 2014, Comput. Des. Appl, V11, P704, DOI [10.1080/16864360.2014.914388, DOI 10.1080/16864360.2014.914388]
NR 34
TC 5
Z9 5
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 925
EP 936
DI 10.1007/s00371-018-1530-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400016
DA 2024-07-18
ER

PT J
AU Zheng, CX
   Wang, JH
   Chen, WH
   Wu, XM
AF Zheng, Chuanxia
   Wang, Jianhua
   Chen, Weihai
   Wu, Xingming
TI Multi-class indoor semantic segmentation with deep structured model
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Scene classification; Convolutional neural
   network; Graph-RNN; Conditional random field
ID FEATURES
AB Indoor semantic segmentation plays a critical role in many applications, such as intelligent robots. However, multi-class recognition is still challenging, especially for pixel-level indoor semantic labeling. In this paper, a novel deep structured model that combines the strengths of the widely used convolutional neural networks (CNNs) and recurrent neural networks (RNNs) is proposed. We first present a multi-information fusion model that utilizes the scene category information to fine-tune the fully convolutional network. Then, to refine the coarse outputs of CNN, the RNN is applied to the final CNN layer so that we can build an end-to-end trainable system. This Graph-RNN is transformed from a conditional random field based on superpixel segmentation graphical modeling that can utilize flexible contextual information of different neighboring regions. The experimental results on the recent large SUN RGB-D dataset demonstrate that the proposed model outperforms existing state-of-the-art methods on the challenging 40 dominant classes task ( mean IU accuracy and pixel accuracy). We also evaluate our model on the public NYU depth V2 dataset and achieve remarkable performance.
C1 [Zheng, Chuanxia; Wang, Jianhua; Chen, Weihai; Wu, Xingming] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Chen, WH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM whchen@buaa.edu.cn
RI Zheng, Chuanxia/GQI-0645-2022; Chen, Wei/GZK-7348-2022
OI /0009-0006-1337-808X
FU National Science Foundation of China [61573048, 61620106012];
   International Scientific and Technological Cooperation Projects of China
   [2015DFG12650]; Key Laboratory of Robotics and Intelligent Manufacturing
   Equipment Technology of Zhejiang Province
FX The work described in this paper was supported by National Science
   Foundation of China under the research Project Grant Nos. 61573048,
   61620106012, the International Scientific and Technological Cooperation
   Projects of China under Grant No. 2015DFG12650, and the Key Laboratory
   of Robotics and Intelligent Manufacturing Equipment Technology of
   Zhejiang Province.
CR Anand A, 2013, INT J ROBOT RES, V32, P19, DOI 10.1177/0278364912461538
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, IEEE T PATTERN ANAL
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], VIS COMPUT
   [Anonymous], 2017, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.243
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, P INT C LEARN REPR
   [Anonymous], 2016, ARXIV160106759
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], EUR C COMP VISION
   [Anonymous], 2014, 2014 INT C LEARNING
   [Anonymous], 2011, P ADV NEURAL INFORM
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970
   Bo L., 2013, EXPT ROBOTICS VOLUME, P387, DOI DOI 10.1007/978-3-319-00065-7
   Chen WH, 2014, OPT LASER ENG, V55, P69, DOI 10.1016/j.optlaseng.2013.10.025
   Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628
   Couprie C., 2013, P 1 INT C LEARN REPR, P1
   Deng Z, 2015, IEEE I CONF COMP VIS, P1733, DOI 10.1109/ICCV.2015.202
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hayat M, 2016, IEEE T IMAGE PROCESS, V25, P4829, DOI 10.1109/TIP.2016.2599292
   Hermans A, 2014, IEEE INT CONF ROBOT, P2631, DOI 10.1109/ICRA.2014.6907236
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Khan SH, 2016, INT J COMPUT VISION, V117, P1, DOI 10.1007/s11263-015-0843-8
   Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Nathan Silberman Derek Hoiem P.K., 2012, ECCV
   Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou BL, 2014, ADV NEUR IN, V27
NR 45
TC 10
Z9 10
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 735
EP 747
DI 10.1007/s00371-017-1411-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100011
DA 2024-07-18
ER

PT J
AU Salam, H
   Séguier, R
AF Salam, Hanan
   Seguier, Renaud
TI A survey on face modeling: building a bridge between face analysis and
   synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Face modeling; Face analysis; Analysis-by-Synthesis; Facial animation
   and synthesis
ID ACTIVE SHAPE MODELS; MORPHABLE MODEL; HEAD TRACKING; TEXTURE;
   SEGMENTATION; RECOGNITION; OPTIMIZATION; ANIMATION; MIXTURES; FEATURES
AB Face modeling refers to modeling the shape and appearance of human faces which lays the basis for model-based facial analysis, synthesis and animation. This paper summarizes the existing state-of-the-art work on face modeling and animation in the Computer Graphics and the Computer Vision areas. While some models or techniques are exclusively used for facial analysis or for facial animation and synthesis, other models combine analysis and synthesis in an analysis-by-synthesis loop. This paper introduces a taxonomy of face modeling methods in function of the area of application (synthesis and analysis) and builds a link between the two by reviewing analysis-by-synthesis face modeling methods. The interest of such a taxonomy is to introduce new face models that combine ideas from the analysis and synthesis domains. We also provide an overview of the extensions of the seminal works presented in this paper. Within each category, we discuss the advantages and disadvantages of each method with respect to the others.
C1 [Salam, Hanan] Pierre & Marie Curie Univ, ISIR Lab, 4 Pl Jussieu, F-75252 Paris 05, France.
   [Seguier, Renaud] Supelec, FAST, Ave Boulaie, F-35576 Cesson Sevigne, France.
C3 Sorbonne Universite; Universite Paris Saclay
RP Salam, H (corresponding author), Pierre & Marie Curie Univ, ISIR Lab, 4 Pl Jussieu, F-75252 Paris 05, France.
EM hanane.salame@gmail.com; renaud.seguier@supelec.fr
RI Salam, Hanan/AAJ-3283-2020
OI Salam, Hanan/0000-0001-6971-5264; Seguier, Renaud/0000-0001-7199-7563
CR Abboud B, 2005, IEE P-VIS IMAGE SIGN, V152, P327, DOI 10.1049/ip-vis:20045060
   Ahlberg J, 2002, EURASIP J APPL SIG P, V2002, P566, DOI 10.1155/S1110865702203078
   Ahlberg J., 2001, CANDIDE-3 -- an updated parameterized face
   Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   [Anonymous], BRIT MACH VIS C BMCV
   [Anonymous], 3D FACE PROCESSING M
   [Anonymous], IEEE 8 INT C SIGN PR
   [Anonymous], TECH REP
   [Anonymous], 2011, Handbook of face recognition
   [Anonymous], CONSTRAINED LOCAL MO
   [Anonymous], 10 IEEE INT C WORKSH
   [Anonymous], EURASIP J IMAGE VIDE
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], TECH REP
   [Anonymous], ACTIVE APPEARANCE MO
   [Anonymous], P ACM SIGGRAPH 2006
   [Anonymous], C MECHATRON ROBOT
   [Anonymous], 4D FACE REAL TIME 3D
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], 1971, THESIS
   [Anonymous], Environmental Psychology & Nonverbal Behavior
   [Anonymous], P ACM S VIRT REAL SO
   [Anonymous], IEEE T COMPUT
   [Anonymous], P 2 IEE EUR C VIS ME
   [Anonymous], INT J MULTIMED APPL
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], WORKSH CONJ IEEE CVP
   [Anonymous], 2016, P 11 INT JOINT C COM
   [Anonymous], THESIS
   [Anonymous], 2008, Computer Facial Animation
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], EOS LIGHTWEIGHT HEAD
   [Anonymous], 2008 IEEE INT S INF
   [Anonymous], COMPUTER VISION PATT
   [Anonymous], MPEG4 SNHC
   [Anonymous], THESIS
   [Anonymous], 3D MORPHABLE MODEL S
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], COOTES 2D 3D ACTIVE
   [Anonymous], 3D FACE PROCESSING M
   [Anonymous], INT C COMP AN IM PRO
   [Anonymous], 1997, TECH REP
   [Anonymous], P INT C PATT REC ICP
   [Anonymous], 2012, AS C COMP VIS
   [Anonymous], INT C COMP VIS SYST
   [Anonymous], BMVC 07
   [Anonymous], AAM API C IMPLEMENTA
   Ayala-Raggi SE, 2011, COMPUT VIS IMAGE UND, V115, P194, DOI 10.1016/j.cviu.2010.11.005
   Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Basso C, 2003, FIRST IEEE INTERNATIONAL WORKSHOP ON HIGHER-LEVEL KNOWLEDGE IN 3D MODELING AND MOTION ANALYSIS, PROCEEDINGS, P3, DOI 10.1109/HLK.2003.1240853
   Batur AU, 2005, IEEE T IMAGE PROCESS, V14, P1707, DOI 10.1109/TIP.2005.854473
   Blake A., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P66, DOI 10.1109/ICCV.1993.378234
   Blake A., 1998, ACTIVE CONTOURS APPL
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Caunce A, 2012, LECT NOTES COMPUT SC, V7432, P398, DOI 10.1007/978-3-642-33191-6_39
   Caunce A, 2010, LECT NOTES COMPUT SC, V6453, P132
   Çeliktutan O, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-13
   Cesar R, 2002, INT C PATT RECOG, P465, DOI 10.1109/ICPR.2002.1048339
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Cootes T. F., 1998, BMVC 98. Proceedings of the Ninth British Machine Vision Conference, P680
   Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484, DOI 10.1007/BFb0054760
   Cootes T.F., 2006, BRIT MACHINE VISION, P919
   COOTES TF, 1992, IMAGE VISION COMPUT, V10, P289, DOI 10.1016/0262-8856(92)90044-4
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 1999, IMAGE VISION COMPUT, V17, P567, DOI 10.1016/S0262-8856(98)00175-9
   Cordea MD, 2008, IEEE T INSTRUM MEAS, V57, P1578, DOI 10.1109/TIM.2008.923784
   Cristinacce D, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P429
   Cristinacce D, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P375, DOI 10.1109/AFGR.2004.1301561
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Cristinacce D., 2007, British Mach. Vision Conf, P880
   Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024
   Cuiping Zhang, 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P206
   Ding CX, 2016, IEEE T PATTERN ANAL, V38, P518, DOI 10.1109/TPAMI.2015.2462338
   Donner R, 2006, IEEE T PATTERN ANAL, V28, P1690, DOI 10.1109/TPAMI.2006.206
   Dornaika F., 2004, PROC C COMPUTER VISI, V10, P153
   Dornaika F, 2006, IEEE T CIRC SYST VID, V16, P1107, DOI 10.1109/TCSVT.2006.881200
   Ersotelos N, 2008, VISUAL COMPUT, V24, P13, DOI 10.1007/s00371-007-0175-y
   Essa I., 1996, Proceedings. Computer Animation '96, P68, DOI 10.1109/CA.1996.540489
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Feng WW, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360690
   Gao XB, 2010, IEEE T SYST MAN CY C, V40, P145, DOI 10.1109/TSMCC.2009.2035631
   Gao XB, 2009, NEUROCOMPUTING, V72, P3174, DOI 10.1016/j.neucom.2009.03.003
   Ge YX, 2013, J VIS COMMUN IMAGE R, V24, P627, DOI 10.1016/j.jvcir.2013.04.011
   Glasbey CA, 1998, J APPL STAT, V25, P155, DOI 10.1080/02664769823151
   Goldenberg R, 2001, IEEE T IMAGE PROCESS, V10, P1467, DOI 10.1109/83.951533
   Gonzalez- Mora J., 2007, IEEE 11 INT C COMPUT, P1
   Graciano ABV, 2003, LECT NOTES COMPUT SC, V2905, P71
   Histace A., 2013, Annals of British Machine Vision Association, V2013, P1
   Hodge AC, 2006, COMPUT METH PROG BIO, V84, P99, DOI 10.1016/j.cmpb.2006.07.001
   Hou YS, 2011, SIGNAL PROCESS-IMAGE, V26, P550, DOI 10.1016/j.image.2011.05.003
   Hu Guosheng., 2014, IEEE INT JOINT C BIO, P1
   Huber P, 2015, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2015.7350989
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Joshi P., 2005, ACM SIGGRAPH 2005 Courses, SIGGRAPH'05, P8, DOI [DOI 10.1145/1198555.1198588, 10.1145/1198555.1198588]
   Kahraman F, 2006, 2006 7TH NORDIC SIGNAL PROCESSING SYMPOSIUM, P102
   Kalra P., 1992, Computer Graphics Forum, V11, pC59, DOI 10.1111/1467-8659.1130059
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kazemi V, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.27
   La Cascia M, 1998, PROC CVPR IEEE, P508, DOI 10.1109/CVPR.1998.698653
   Leo M. J., 2011, 2011 3rd International Conference on Trendz in Information Sciences & Computing (TISC), P40, DOI 10.1109/TISC.2011.6169081
   Li YZ, 2005, IEEE I CONF COMP VIS, P251
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Magnenat-Thalmann N., 2005, Handbook of Virtual Humans
   Malciu M, 2000, PROC SPIE, V4121, P51, DOI 10.1117/12.402450
   Martins P, 2016, IEEE T PATTERN ANAL, V38, P704, DOI 10.1109/TPAMI.2015.2462343
   Martins P, 2014, IEEE IMAGE PROC, P303, DOI 10.1109/ICIP.2014.7025060
   Martins P, 2014, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR.2014.232
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Maurer T, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P176, DOI 10.1109/AFGR.1996.557261
   Mayer C, 2011, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER-HUMAN INTERACTIONS (ACHI 2011), P106
   MENET S, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P194, DOI 10.1109/ICSMC.1990.142091
   Meyer M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276469
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Noh J., 1998, U OFSOUTHERN CALIFOR, P99
   Pandzic Igor S., 2003, MPEG-4 facial animation: the standard, implementation and applications
   Paquet U, 2009, PROC CVPR IEEE, P1193, DOI 10.1109/CVPRW.2009.5206751
   Pardo XM, 2001, IMAGE VISION COMPUT, V19, P461, DOI 10.1016/S0262-8856(00)00092-5
   PARKE FI, 1982, IEEE COMPUT GRAPH, V2, P61
   Parke Frederic Ira, 1974, THESIS
   Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522
   Patel N, 2013, SIGNAL IMAGE VIDEO P, V7, P889, DOI 10.1007/s11760-011-0278-9
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   Ravyse I, 2008, INT CONF ACOUST SPEE, P1089, DOI 10.1109/ICASSP.2008.4517803
   Roberts MG., 2003, 14 BRIT MACHINE VISI, P349
   Rogers M, 2002, LECT NOTES COMPUT SC, V2353, P517
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Romdhani S, 2002, LECT NOTES COMPUT SC, V2353, P3
   Rydfalk M., 1987, LITHISYI866 LINK U D
   Salam H, 2012, IEEE IMAGE PROC, P1833, DOI 10.1109/ICIP.2012.6467239
   Saragih J, 2007, IEEE I CONF COMP VIS, P2173
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Sattar A., 2010, 2010 Fifth International Conference on Digital Information Management (ICDIM 2010), P152, DOI 10.1109/ICDIM.2010.5664688
   Sattar A, 2008, IEEE IMAGE PROC, P3220, DOI 10.1109/ICIP.2008.4712481
   Sclaroff S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1146, DOI 10.1109/ICCV.1998.710860
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Senechal T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P860, DOI 10.1109/FG.2011.5771363
   Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567
   Shin HJ, 2009, COMPUT GRAPH FORUM, V28, P1829, DOI 10.1111/j.1467-8659.2009.01560.x
   Soladié C, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P493
   Storer Markus, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P192, DOI 10.1109/ICCVW.2009.5457701
   Stricker R, 2009, LECT NOTES COMPUT SC, V5815, P364, DOI 10.1007/978-3-642-04667-4_37
   Su Y, 2008, IEEE SYS MAN CYBERN, P2235
   Sung JW, 2007, INT J COMPUT VISION, V75, P297, DOI 10.1007/s11263-006-0034-8
   Sung J, 2007, PATTERN RECOGN, V40, P108, DOI 10.1016/j.patcog.2006.06.017
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Tresadern P.A., 2009, Proceedings of British Machine Vision Conference, P1, DOI [DOI 10.5244/C.23.95, 10.5244/C.23.95]
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Wang X, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO 2009), VOLS 1-4, P2314, DOI 10.1109/ROBIO.2009.5420738
   Wang YH, 2008, IEEE IMTC P, P1, DOI 10.4018/jcini.2008040101
   Waters K., 1987, ACM SIGGRAPH Comput. Graph., V21, P17
   Weissenfeld A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P225, DOI 10.1109/ICME.2006.262423
   Whitmarsh T., 2006, Proceedings of the 1st International Workshop on Shape and Semantics, P71
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wu YW, 2004, IEEE SYS MAN CYBERN, P604
   Xiao J, 2004, PROC CVPR IEEE, P535
   Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50
   Yan PK, 2010, IEEE T BIO-MED ENG, V57, P1158, DOI 10.1109/TBME.2009.2037491
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Yu M, 2010, WSCG 2010: FULL PAPERS PROCEEDINGS, P181
   YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169
   Zalewski L, 2005, PROC CVPR IEEE, P217
   Zhou Y, 2003, PROC CVPR IEEE, P109
   Zhu X., Face Detection, Pose Estimation, and Landmark Localization in the Wild
NR 174
TC 8
Z9 8
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 289
EP 319
DI 10.1007/s00371-016-1332-y
PG 31
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800011
DA 2024-07-18
ER

PT J
AU Liu, MM
   Guo, YW
   Wang, J
AF Liu, Mingming
   Guo, Yanwen
   Wang, Jun
TI Indoor scene modeling from a single image using normal inference and
   edge features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 28-30, 2016
CL Chongqing Univ Technol, Chongqing, PEOPLES R CHINA
SP ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, China Comp Federat, China Soc Image & Graph, Zhongxing Telecommunicat Equipment Corp, Kingdee Software Corp, ACM
HO Chongqing Univ Technol
DE Indoor image modeling; Orientation estimation; Edge; Calibration
ID RECONSTRUCTION; SEGMENTATION
AB We present in this paper an interactive approach for semantically modeling the indoor environment given only a single indoor image as input, without requiring access to the scene or using any additional measurements like RGBD cameras. Our key insight is that, although depth estimation from a single image is notoriously difficult, we can conveniently obtain a relatively accurate normal map, which essentially conveys a great deal of scene geometry. This enables us to model each object in a data-driven manner by representing the object as a normal-based graph and retrieving a similar model from the database by graph matching. Moreover, edge information is integrated to further improve the searching result. We hypothesize a set of sparse surface orientations for the image and further refine them in an intuitive and straightforward manner. With a small amount of simple user interaction, our approach is able to generate a plausible model of the scene. To verify the effectiveness of our proposed method, we show the modeling results on a variety of indoor images.
C1 [Liu, Mingming; Guo, Yanwen] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing University; Nanjing University of Aeronautics & Astronautics
RP Guo, YW (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
EM ieliuming@163.com; ywguo@nju.edu.cn; wjun@nuaa.edu.cn
RI Wang, Jun/AAM-6868-2021
OI Wang, Jun/0000-0001-9223-2615
FU National Natural Science Foundation of China [61373059, 61672279,
   61321491]; Natural Science Foundation of Jiangsu Province [BK20150016]
FX The authors would like to thank the reviewers for their constructive
   comments which helped improve this paper greatly. This work was
   supported in part by the National Natural Science Foundation of China
   under Grants 61373059, 61672279, and 61321491 and the Natural Science
   Foundation of Jiangsu Province under Grants BK20150016.
CR [Anonymous], 1989, Shape from shading
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Delage E, 2007, SPRINGER TRAC ADV RO, V28, P305
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Guo YW, 2014, COMPUT GRAPH-UK, V38, P174, DOI 10.1016/j.cag.2013.10.038
   Gupta A., 2010, ADV NEURAL INFORM PR, P1288
   Henry P., 2010, International Symposium on Experimental Robotics, V20, P22
   Nguyen HM, 2016, VISUAL COMPUT, V32, P625, DOI 10.1007/s00371-015-1078-y
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Hou F, 2016, VISUAL COMPUT, V32, P151, DOI 10.1007/s00371-015-1061-7
   KARSCH K, 2014, ACM T GRAPHIC, V33
   Kim YM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366157
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Liu M., 2016, INT C CYB
   Nan LL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366156
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Oh BM, 2001, COMP GRAPH, P433
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Su H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601159
   Tardif Jean-Philippe, 2009, 2009 IEEE 12th International Conference on Computer Vision (ICCV), P1250, DOI 10.1109/ICCV.2009.5459328
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   XIAO J, 2012, ADV NEURAL INFORM PR, P746
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 38
TC 6
Z9 6
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1227
EP 1240
DI 10.1007/s00371-016-1348-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FF8VN
UT WOS:000409296000002
DA 2024-07-18
ER

PT J
AU Yang, WZ
   Luo, XQ
   Wu, XL
   Zhou, Q
   Pan, ZG
   Wu, YC
AF Yang Wenzhen
   Luo Xianquan
   Wu Xinli
   Zhou Qiang
   Pan Zhigeng
   Wu Yuecheng
TI Intersection workspace visualization of multi-finger hands
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 28-30, 2016
CL Chongqing Univ Technol, Chongqing, PEOPLES R CHINA
SP ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, China Comp Federat, China Soc Image & Graph, Zhongxing Telecommunicat Equipment Corp, Kingdee Software Corp, ACM
HO Chongqing Univ Technol
DE Intersection workspace; Workspace volume analysis; Multi-finger hand;
   Visualization
ID DESIGN
AB The intersection workspace of multi-finger hands means the overlapping workspace in 3D Euclidean space where several fingertips can reach together, and is very valuable for grasp space planning, motion control and mechanical design for multi-finger hands. But, the intersection workspaces usually have sophisticated forms, which are really hard for robotic experts to image them. We present a general approach to visualize and analyze the intersection workspace of multi-finger hands. We firstly deduce the displacement equation of the fingertip, then generate 3D workspace of the fingertip and lastly obtain the intersection workspace of the multi-finger hands. This approach can visualize intersection workspaces with integrated forms and contours, and analyze the volume of the intersection workspaces. The vivid form and volume analysis of intersection workspaces are useful for robotic experts to accurately and intuitionally understand the fingertip motion ranges, plan the motion routes of fingers and avoid the motion collision among fingers.
C1 [Yang Wenzhen; Luo Xianquan; Wu Xinli; Zhou Qiang; Wu Yuecheng] Zhejiang Sci Tech Univ, Virtual Real Lab, Hangzhou 310018, Zhejiang, Peoples R China.
   [Pan Zhigeng] Hangzhou Normal Univ, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Zhejiang Sci-Tech University; Hangzhou Normal University
RP Yang, WZ (corresponding author), Zhejiang Sci Tech Univ, Virtual Real Lab, Hangzhou 310018, Zhejiang, Peoples R China.
EM ywz@zstu.edu.cn; zgpan@hznu.edu.cn
RI Yang, Wenzhen/HHY-7607-2022
OI Yang, Wenzhen/0000-0002-0068-1497
FU National Natural Science Foundation of China [61332017]; Zhejiang
   Province Public Welfare Project of China [2016C33174]; Zhejiang
   Provincial Natural Science Foundation of China [LY13E050025]; National
   High-Tech Research and Development Program of China [2013AA013703]
FX This research was supported by Key Project of the National Natural
   Science Foundation of China (No. 61332017), Zhejiang Province Public
   Welfare Project of China (No. 2016C33174), Zhejiang Provincial Natural
   Science Foundation of China (No. LY13E050025) and National High-Tech
   Research and Development Program of China (No. 2013AA013703). The
   authors appreciate the anonymous reviewers for valuable comments and
   suggestions.
CR Borst C, 2003, IEEE INT CONF ROBOT, P702
   Cipriani C, 2010, ROBOTICA, V28, P919, DOI 10.1017/S0263574709990750
   Cui L, 2009, RECONFIGURABLE MECHANISMS AND ROBOTS, P619
   Dai JS, 2007, J MECH DESIGN, V129, P1191, DOI 10.1115/1.2771576
   Dalley SA, 2009, IEEE-ASME T MECH, V14, P699, DOI 10.1109/TMECH.2009.2033113
   Dragulescu D, 2007, BIOMED ENG ONLINE, V6, DOI 10.1186/1475-925X-6-15
   Guan Y., 2008, IEEE RSJ INT C INT R, DOI [10.1109/IROS.2008.4650691, DOI 10.1109/IROS.2008.4650691]
   Guan YS, 2011, ADV ROBOTICS, V25, P2293, DOI 10.1163/016918611X603837
   Guan YS, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3705
   Hong L., 2008, IEEE RSJ INT C INT R, P3692, DOI DOI 10.1109/IROS.2008.4650624
   Jacobsen S., 1986, IEEE INT C ROB AUT, P1520, DOI [10.1109/ROBOT.1986.1087395, DOI 10.1109/ROBOT.1986.1087395]
   Kawasaki H, 2001, ADV ROBOTICS, V15, P357, DOI 10.1163/156855301300235913
   KERR J, 1986, INT J ROBOT RES, V4, P3, DOI 10.1177/027836498600400401
   Michelman P, 1998, IEEE T ROBOTIC AUTOM, V14, P105, DOI 10.1109/70.660851
   Palli G, 2014, INT J ROBOT RES, V33, P799, DOI 10.1177/0278364913519897
   Roa MA, 2015, AUTON ROBOT, V38, P65, DOI 10.1007/s10514-014-9402-3
   Salisbury J.K., 1982, Int. J. Robot. Res, V1, P4, DOI [10.1177/027836498200100102, DOI 10.1177/027836498200100102]
   Song P, 2018, VISUAL COMPUT, V34, P257, DOI 10.1007/s00371-016-1333-x
   Yang W., 2012, INT J ADV ROBOT SYST, V9, P1
   Yang W., 2016, INT J ADV ROBOT SYST, V13, P1
   Yang WZ, 2015, INT J HUM ROBOT, V12, DOI 10.1142/S0219843615500061
   Zhe X., 2016, IEEE INT C ROB AUT, DOI [10.1109/ICRA.2016.7487528, DOI 10.1109/ICRA.2016.7487528]
NR 22
TC 0
Z9 0
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1253
EP 1263
DI 10.1007/s00371-017-1421-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FF8VN
UT WOS:000409296000004
DA 2024-07-18
ER

PT J
AU Li, R
   Liu, L
   Sheng, Y
   Zhang, GX
AF Li, Rui
   Liu, Lei
   Sheng, Yun
   Zhang, Guixu
TI A heuristic convexity measure for 3D meshes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Shape analysis; Convexity measure; PCA; 3D mesh retrieval
ID SHAPE; DECOMPOSITION
AB In this paper we propose a heuristic convexity measure for 3D meshes. Built upon a state-of-the-art convexity measure that employs a time-consuming genetic algorithm for optimization, our new measure projects only once a given 3D mesh onto the orthogonal 2D planes along its principal directions for an initial estimation of mesh convexity, followed by a correction calculation based on mesh slicing. Our measure experimentally shows several advantages over the state-of-the-art one: first, it accelerates the overall computation by approximately an order of magnitude; second, it properly handles those bony meshes usually overestimated by the state-of-the-art measure; third, it improves the accuracy of the state-of-the-art measure in 3D mesh retrieval.
C1 [Li, Rui; Liu, Lei; Sheng, Yun; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
C3 East China Normal University
RP Sheng, Y (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
EM ysheng@cs.ecnu.edu.cn
FU National Natural Science Foundation of China [61202291]
FX The authors would like to thank Dr. Lian for his supports and patience
   in answering our questions. The authors would also like to thank the
   reviewers for their precious time and constructive comments. This work
   has been supported by the National Natural Science Foundation of China
   (61202291).
CR Asafi S, 2013, COMPUT GRAPH FORUM, V32, P23, DOI 10.1111/cgf.12169
   Chalmeta R, 2013, COMPUT AIDED DESIGN, V45, P93, DOI 10.1016/j.cad.2012.07.012
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Ghosh M, 2013, COMPUT AIDED DESIGN, V45, P494, DOI 10.1016/j.cad.2012.10.032
   LIAN ZH, 2012, COMPUTER VISION PATT, P119
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Liu HR, 2010, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2010.5540225
   Pao H.-K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P948, DOI 10.1109/ICCV.1999.790350
   Rahtu E, 2006, IEEE T PATTERN ANAL, V28, P1501, DOI 10.1109/TPAMI.2006.175
   Ren Z, 2013, IEEE T PATTERN ANAL, V35, P2546, DOI 10.1109/TPAMI.2013.67
   Rosin PL, 2008, COMPUT VIS IMAGE UND, V109, P176, DOI 10.1016/j.cviu.2007.09.010
   Rosin PL, 2006, COMPUT VIS IMAGE UND, V103, P101, DOI 10.1016/j.cviu.2006.04.002
   Rosin PL, 2009, PATTERN RECOGN LETT, V30, P570, DOI 10.1016/j.patrec.2008.12.001
   Rosin PL, 2003, MACH VISION APPL, V14, P172, DOI 10.1007/s00138-002-0118-6
   Sarfraz M, 2007, I C COMP SYST APPLIC, P730, DOI 10.1109/AICCSA.2007.370714
   Shilane P., 2004, SHAPE MODELING INT, V105
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Song YZ, 2013, IEEE T VIS COMPUT GR, V19, P1252, DOI 10.1109/TVCG.2013.13
   Zhouhui Lian, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P116, DOI 10.1109/3DIMPVT.2011.22
   Zimmer H, 2013, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2013.280
   Zunic J, 2004, IEEE T PATTERN ANAL, V26, P923, DOI 10.1109/TPAMI.2004.19
   Zunic J, 2003, IEEE T PATTERN ANAL, V25, P1193, DOI 10.1109/TPAMI.2003.1227997
NR 22
TC 3
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 903
EP 912
DI 10.1007/s00371-017-1385-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800021
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wu, SW
   Sun, XY
   Wang, QL
   Chen, J
AF Wu, Saiwen
   Sun, Xiaoying
   Wang, Qinglong
   Chen, Jian
TI Tactile modeling and rendering image-textures based on electrovibration
SO VISUAL COMPUTER
LA English
DT Article
DE Image-textures; Electrovibration; Tactile model; Rendering; Stimuli
   frequency
AB Image-textures contain most surface features of real objects that are largely missing from virtual tactile interactions. This paper presents a tactile model for rendering image-textures based on electrovibration, which is achieved by varying stimuli signals to modulate friction between the finger and the touchscreen. We do research on the relationships between human tactile sensation and stimuli signals through experiments. According to the relationships, we establish a mapping model based on gradients of image-textures which are gained by the Roberts filter. We use the mapping model to synthetize the frequency and amplitude of stimuli signals for rendering image-textures as a user interacts with our tactile prototype. Specifically, stimuli frequency mainly reflects hardness and granularity of image-textures, stimuli amplitude mainly reflects heights of image-textures. We compare the proposed model with the model based on stimuli amplitude through experiments on the prototype. Results show that the proposed model can effectively enhance tactile reality of image-textures.
C1 [Wu, Saiwen; Sun, Xiaoying; Wang, Qinglong; Chen, Jian] Jilin Univ, Coll Commun Engn, 5372 Nanhu Rd, Changchun 130012, Peoples R China.
C3 Jilin University
RP Sun, XY (corresponding author), Jilin Univ, Coll Commun Engn, 5372 Nanhu Rd, Changchun 130012, Peoples R China.
EM wusw13@mails.jlu.edu.cn; sunxy@jlu.edu.cn; wesleypk@126.com;
   chenjian@jlu.edu.cn
FU National High-tech RD Program [2013AA013704]
FX The authors would like to thank the anonymous reviewers for their
   constructive and the volunteers who participated in the experiments as
   subjects. This project was supported by the National High-tech RD
   Program (2013AA013704).
CR [Anonymous], 1994, P ASME WINT ANN M S
   [Anonymous], 2020, Feature Extraction and Image Processing
   Bau O., 2010, P 23 ANN ACM S US IN, P283, DOI DOI 10.1145/1866029.1866074
   Chouvardas VG, 2008, DISPLAYS, V29, P185, DOI 10.1016/j.displa.2007.07.003
   Culbertson H, 2014, IEEE T HAPTICS, V7, P381, DOI 10.1109/TOH.2014.2316797
   GRIMNES S, 1983, ACTA PHYSIOL SCAND, V118, P19, DOI 10.1111/j.1748-1716.1983.tb07235.x
   Ikei Y, 1997, P IEEE VIRT REAL ANN, P199, DOI 10.1109/VRAIS.1997.583071
   Israr A., 2012, P 2012 ACM ANN C EXT, P1571, DOI DOI 10.1145/2212776.2223674
   Kaczmarek KA, 2006, IEEE T BIO-MED ENG, V53, P2047, DOI 10.1109/TBME.2006.881804
   Kim S.-C., 2013, P 26 ANN ACM S US IN, P531, DOI DOI 10.1145/2501988.2502020
   Kim SH, 2007, INT SYM MICRO-NANOM, P544, DOI 10.1109/MHS.2007.4420914
   Martínez J, 2013, VISUAL COMPUT, V29, P111, DOI 10.1007/s00371-012-0716-x
   Minsky M., 1990, Computer Graphics, V24, P235, DOI 10.1145/91394.91451
   Saga S., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P15, DOI 10.1109/HAPTIC.2012.6183764
   Strong R.M, 1970, THESIS
   Vasudevan H, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P357
   Wijekoon Dinesh, 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P613, DOI 10.1007/978-3-642-31401-8_54
   Wu J, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P1315, DOI 10.1109/ROBIO.2007.4522354
   Xia PJ, 2013, VISUAL COMPUT, V29, P433, DOI 10.1007/s00371-012-0748-2
   Xu C., 2011, P 2011 ANN C EXTENDE, P317, DOI DOI 10.1145/1979742.1979705
   Yu G, 2015, IEEE T HAPTICS, V8, P67, DOI 10.1109/TOH.2014.2377745
NR 21
TC 25
Z9 27
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 637
EP 646
DI 10.1007/s00371-016-1214-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300008
DA 2024-07-18
ER

PT J
AU Chalás, I
   Urbanová, P
   Jurík, V
   Ferková, Z
   Jandová, M
   Sochor, J
   Kozlíková, B
AF Chalas, Igor
   Urbanova, Petra
   Jurik, Vojtech
   Ferkova, Zuzana
   Jandova, Marie
   Sochor, Jiri
   Kozlikova, Barbora
TI Generating various composite human faces from real 3D facial images
SO VISUAL COMPUTER
LA English
DT Article
DE Facial composite; 3D face model; Face perception; Crowd simulation;
   Morphological modification
ID VIRTUAL HUMANS; MODEL; LEVEL
AB Generating large human crowds of distinguishable individuals is one of the challenges in the gaming industry. When the scene contains many characters, it becomes impracticable to create all the individual characters manually. However, the requirement for the different appearances of individuals in a crowd, namely their faces, is now in greater demand. Therefore, this paper describes our solution to the automatic generation of human faces that are created as a composite of facial parts of 3D scans of real human faces. However, the user has the possibility to further adjust the composite by designing replacements, leading to a desired appearance. The final composite can be exported and attached to a given avatar. To evaluate the usability of our solution, we performed two case studies. The conducted perception study performed with 104 participants aimed to confirm the decreasing human ability to recognize morphologically modified faces. The morphological study focused on the quantification of the extent of facial modifications. Both studies were performed by domain experts from psychology and anthropology.
C1 [Chalas, Igor; Ferkova, Zuzana; Sochor, Jiri; Kozlikova, Barbora] Masaryk Univ, Dept Comp Graph & Design, Bot 68a, Brno 60200, Czech Republic.
   [Urbanova, Petra] Masaryk Univ, Dept Anthropol, Fac Sci, Biol Anthropol Unit, Brno, Czech Republic.
   [Urbanova, Petra] Masaryk Univ, Dept Anthropol, Fac Sci, Lab Forens Anthropol & Morphol, Brno, Czech Republic.
   [Jandova, Marie] Masaryk Univ, Dept Anthropol, Fac Sci, Brno, Czech Republic.
   [Jurik, Vojtech] Masaryk Univ, Dept Psychol, Fac Arts, Brno, Czech Republic.
C3 Masaryk University Brno; Masaryk University Brno; Masaryk University
   Brno; Masaryk University Brno; Masaryk University Brno
RP Chalás, I (corresponding author), Masaryk Univ, Dept Comp Graph & Design, Bot 68a, Brno 60200, Czech Republic.
EM chalas@fi.muni.cz; urbanova@mail.muni.cz; 372092@mail.muni.cz;
   xferkova@fi.muni; jandovam@mail.muni.cz; sochor@fi.muni.cz;
   kozlikova@fi.muni.cz
RI Juřík, Vojtěch/KVY-1534-2024; Juřík, Vojtěch/KVY-1548-2024; Urbanova,
   Petra/AFA-6848-2022; Juřík, Vojtěch/AAW-4983-2020; Sochor,
   Jiri/D-7067-2013; Kozlikova, Barbora/G-3890-2014
OI Juřík, Vojtěch/0000-0001-8779-1666; Juřík, Vojtěch/0000-0001-8779-1666;
   Urbanova, Petra/0000-0001-9321-3360; Juřík, Vojtěch/0000-0001-8779-1666;
   Kozlikova, Barbora/0000-0003-0045-0872
FU Masaryk University [MUNI/A/1213/2014, MUNI/33/08/2015,
   MUNI/FR/1843/2014, MUNI/A/1281/2014]
FX This work was supported by the Masaryk University projects
   MUNI/A/1213/2014, MUNI/33/08/2015, MUNI/FR/1843/2014, and
   MUNI/A/1281/2014.
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   [Anonymous], SOMATOSKOPICKE ZNAKY
   [Anonymous], STAT DAT AN SOFTW SY
   [Anonymous], 2014, P 20 WORLD M INT ASS
   [Anonymous], 2005, P EUR S GEOM PROC
   [Anonymous], 1967, ANTROPOLOGIE
   Aubel A, 2000, IEEE T CIRC SYST VID, V10, P207, DOI 10.1109/76.825720
   Bastioni M., 2008, P BANGALORE COMPUTE, P1
   Beacco A, 2016, COMPUT GRAPH FORUM, V35, P32, DOI 10.1111/cgf.12774
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bruce V., 2012, Face perception
   Dobbyn S, 2006, EUROGR TECH REP SER, P103
   Dobbyn S., 2005, ACM SIGGRAPH 2005 S, P95, DOI DOI 10.1145/1053427.1053443
   Galvanek M., 2015, P 31 SPRING C COMP G
   GOSSELIN D, 2005, SHADERX3, P505
   Hammer Oyvind, 2001, Palaeontologia Electronica, V4, pUnpaginated
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Hilton A, 2000, VISUAL COMPUT, V16, P411, DOI 10.1007/PL00013395
   Ip HHS, 1996, VISUAL COMPUT, V12, P254, DOI 10.1007/s003710050063
   Kahler K., 2002, Eurographics Symp. on Comp. Animation, P55, DOI DOI 10.1145/545261.545271
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kotulanova Z., 2014, DOLNI VESTONICE STUD, P177
   Loménie N, 2008, PATTERN RECOGN LETT, V29, P1571, DOI 10.1016/j.patrec.2008.03.019
   Maejima A., 2009, ACM SIGGRAPH ASIA 20
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Maïm J, 2009, IEEE COMPUT GRAPH, V29, P44, DOI 10.1109/MCG.2009.76
   McDonnell R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360625
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Nedel LP, 2000, VISUAL COMPUT, V16, P306, DOI 10.1007/PL00007212
   Oh S, 2005, VISUAL COMPUT, V21, P522, DOI 10.1007/s00371-005-0339-6
   Pighin F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P75, DOI 10.1145/280814.280825
   Tecchia F, 2002, IEEE COMPUT GRAPH, V22, P36, DOI 10.1109/38.988745
   Thalmann D., 2013, INT SERIES VIDEO COM, V11, P123
   Thalmann D., 2007, EUROGRAPHICS TUTORIA, V1, P21
   Toledo L, 2014, VISUAL COMPUT, V30, P949, DOI 10.1007/s00371-014-0975-9
   Ulicny B, 2002, COMPUT GRAPH FORUM, V21, P767, DOI 10.1111/1467-8659.00634
   Ulicny Branislav, 2004, SCA'04'- Proc. of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P243, DOI [DOI 10.2312/SCA/SCA04/243-252, 10.1145/1028523.1028555, DOI 10.1145/1028523.1028555]
   Visual Computing Lab, 2013, MESHLAB VERS 1 3 2
   ZACK GW, 1977, J HISTOCHEM CYTOCHEM, V25, P741, DOI 10.1177/25.7.70454
   Zhengjie Deng, 2012, 2012 4th International Conference on Digital Home (ICDH 2012), P278, DOI 10.1109/ICDH.2012.31
   Zinsser T., 2005, INT C PATT REC IM PR, P116
NR 43
TC 8
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 443
EP 458
DI 10.1007/s00371-016-1277-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4JZ
UT WOS:000398767100005
DA 2024-07-18
ER

PT J
AU Tandianus, B
   Johan, H
   Seah, HS
   Lin, F
AF Tandianus, Budianto
   Johan, Henry
   Seah, Hock Soon
   Lin, Feng
TI Spectral caustic rendering of a homogeneous caustic object based on
   wavelength clustering and eye sensitivity
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Caustics; Spectral; Progressive photon mapping
ID REPRESENTATION
AB In the real world, the index of refraction of a refractive object (caustic object) varies across the wavelengths. Therefore, in physically based caustic rendering, we need to take into account spectral information. However, this may lead to prohibitive running time. In response, we propose a two-step acceleration scheme for spectral caustic rendering. Our acceleration scheme takes into account information across visible wavelengths of the scene, that is, the index of refraction (IOR) (caustic object), light power (light), and material reflectance (surface). To process visible wavelengths effectively, firstly we cluster the wavelengths which have similar first refraction (air to caustic object) directions. In this way, all the wavelengths in a cluster can be represented by one light ray during rendering. Secondly, by considering the surrounding objects (their material reflectance from and visible surface area of the caustic objects) and light power, we compute the refinement amount of each wavelength cluster. Our accelerated algorithm can produce photorealistic rendering results close to their reference images (which are generated by rendering every 1 nm of visible wavelengths) with a significant acceleration magnitude. Computational experiment results and comparative analyses are reported in the paper.
C1 [Tandianus, Budianto] Nanyang Technol Univ, Sch Comp Engn Org SCE, GameLAB, Singapore 639798, Singapore.
   [Johan, Henry] Fraunhofer IDM NTU, Singapore 639798, Singapore.
   [Seah, Hock Soon; Lin, Feng] Nanyang Technol Univ, Sch Comp Engn Org SCE, Singapore 639798, Singapore.
   [Lin, Feng] Nanyang Technol Univ, Sch Comp Engn Org SCE, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University; Nanyang Technological University
RP Tandianus, B (corresponding author), Nanyang Technol Univ, Sch Comp Engn Org SCE, GameLAB, Block NS4,N4-B1b-13,36 Nanyang Ave, Singapore 639798, Singapore.
EM btandianus@ntu.edu.sg; henryjohan@ntu.edu.sg; ashsseah@ntu.edu.sg;
   asflin@ntu.edu.sg
RI Seah, Hock Soon/AAK-9900-2020; Tandianus, Budianto/ABR-4163-2022
OI Seah, Hock Soon/0000-0003-2699-7147; Tandianus,
   Budianto/0000-0002-0526-3497
FU Ministry of Education, Singapore [MOE2011-T2-2-037]; Fraunhofer IDM@NTU
   - National Research Foundation (NRF)
FX This work is partially supported by a research grant MOE2011-T2-2-037
   from Ministry of Education, Singapore. Henry Johan is supported by
   Fraunhofer IDM@NTU, which is funded by the National Research Foundation
   (NRF) and managed through the multiagency Interactive & Digital Media
   Programme Office (IDMPO) hosted by the Media Development Authority of
   Singapore (MDA).
CR Agu E.O., 2001, THESIS U MASSACHUSET
   [Anonymous], EUR 2002 SEP
   Bergner S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P101, DOI 10.1109/VISUAL.2002.1183763
   Chern JR, 2005, REAL-TIME IMAGING, V11, P117, DOI 10.1016/j.rti.2005.01.004
   DEVILLE PM, 1994, COMPUT GRAPH FORUM, V13, pC97, DOI 10.1111/1467-8659.1330097
   Dong WM, 2006, LECT NOTES COMPUT SC, V4035, P719
   Durikovic R., 2005, P 21 SPRING C COMP G, P233
   Durikovic R, 2006, INFORMATION VISUALIZATION-BOOK, P751
   Elek O., 2014, COMPUTER FORUM P EGS, V33
   Evans GF, 1999, PROC GRAPH INTERF, P42
   Frisvad JR, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276452
   Gondek J. S., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P213, DOI 10.1145/192161.192202
   Gutierrez D, 2008, COMPUT GRAPH FORUM, V27, P547, DOI 10.1111/j.1467-8659.2008.01152.x
   Guy S, 2004, ACM T GRAPHIC, V23, P231, DOI 10.1145/1015706.1015708
   Hachisuka T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618487
   Hirayama H., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P128, DOI 10.1109/PCCGA.1999.803356
   Iehl JC, 2003, COMPUT GRAPH-UK, V27, P747, DOI 10.1016/S0097-8493(03)00148-1
   Iehl JC, 2000, COMPUT GRAPH FORUM, V19, pC291, DOI 10.1111/1467-8659.00421
   Imura M., 2003, TECHN APPL P 7 DIG I
   Iwasaki K, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P344, DOI 10.1109/CGI.2004.1309231
   Jensen H.W., 1996, GLOBAL ILLUMINATION, P21
   Johnson GM, 1999, IEEE COMPUT GRAPH, V19, P47, DOI 10.1109/38.773963
   Lai G, 2007, WSCG 2007, SHORT COMMUNICATIONS PROCEEDINGS I AND II, P95
   MARIMONT DH, 1992, J OPT SOC AM A, V9, P1905, DOI 10.1364/JOSAA.9.001905
   MEYER GW, 1988, COMPUT VISION GRAPH, V41, P57, DOI 10.1016/0734-189X(88)90117-X
   Morley RK, 2006, PROC GRAPH INTERF, P179
   Musgrave F. K., 1989, Proceedings. Graphics Interface'89, P227
   Radziszewski Michal, 2009, Journal of WSCG, V17, P9
   Rougeron G., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P127
   Sadeghi I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077344
   Schjoth L, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P179
   Shi JJ, 2012, VISUAL COMPUT, V28, P647, DOI 10.1007/s00371-012-0685-0
   Smits B., 1999, Journal of Graphics Tools, V4, P11, DOI 10.1080/10867651.1999.10487511
   Stam J, 1999, COMP GRAPH, P101, DOI 10.1145/311535.311546
   Strengert M, 2006, VISUAL COMPUT, V22, P550, DOI 10.1007/s00371-006-0028-0
   Sun YL, 2008, COMPUT GRAPH FORUM, V27, P1607, DOI 10.1111/j.1467-8659.2007.01110.x
   Sun YL, 1999, IEEE COMPUT GRAPH, V19, P61, DOI 10.1109/38.773965
   Sun YL, 2006, ACM T GRAPHIC, V25, P100, DOI 10.1145/1122501.1122506
   Sun YL, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P102
   Sun YL, 2001, VISUAL COMPUT, V17, P429, DOI 10.1007/s003710100116
   WALLIS R, 1975, J OPT SOC AM, V65, P91, DOI 10.1364/JOSA.65.000091
   Weidlich A, 2009, COMPUT GRAPH FORUM, V28, P1065, DOI 10.1111/j.1467-8659.2009.01483.x
   Wright W.D., 1929, T OPTICAL SOC, V30, P141, DOI DOI 10.1088/1475-4878/30/4/301
   Wu JZ, 2013, VISUAL COMPUT, V29, P41, DOI 10.1007/s00371-012-0673-4
   Wyman Chris., 2006, I3D 06, P153
   Xing XX, 2010, LECT NOTES COMPUT SC, V6249, P509, DOI 10.1007/978-3-642-14533-9_52
   Xu HY, 2006, COMPUT GRAPH FORUM, V25, P759, DOI 10.1111/j.1467-8659.2006.00997.x
   Zeghers E, 1997, VISUAL COMPUT, V13, P424, DOI 10.1007/s003710050115
NR 48
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1601
EP 1614
DI 10.1007/s00371-014-1037-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, SG
   Wang, HB
   Wang, J
   Cho, SH
   Pan, CH
AF Liu, Shaoguo
   Wang, Haibo
   Wang, Jue
   Cho, Sunghyun
   Pan, Chunhong
TI Automatic blur-kernel-size estimation for motion deblurring
SO VISUAL COMPUTER
LA English
DT Article
DE Image deblurring; Blur-kernel-size; Motion blur prior; Autocorrelation;
   Directional attenuation
AB Existing image deblurring approaches often take the blur-kernel-size as an important manual parameter. When set improperly, this parameter can lead to significant errors in the estimated blur kernels. However, manually specifying a proper kernel size for an input image is usually a tedious trial-and-error process. In this paper, we propose a new approach for automatically estimating the underlying blur-kernel-size value that can lead to good kernel estimation. Our approach takes advantage of the autocorrelation map (automap) of image gradients that is known to reflect the motion blur information. We show that the standard automap suffers from structural edges in the image and cannot be directly used for kernel size estimation. To alleviate this problem, we develop a modified automap method that contains a directional attenuation component, which can effectively reduce the influence of structural edges, leading to more accurate and reliable kernel size estimation. Experimental results suggest that the proposed approach can help state-of-the-art deblurring algorithms achieve accurate kernel estimation without relying on manual parameter tweaking.
C1 [Liu, Shaoguo; Pan, Chunhong] Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China.
   [Wang, Haibo] Shandong Univ, Sch Control Sci & Engn, Jinan 250100, Peoples R China.
   [Wang, Jue; Cho, Sunghyun] Adobe Res, Seattle, WA USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Shandong
   University; Adobe Systems Inc.
RP Liu, SG (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China.
EM sgliu2013@gmail.com; hbwang1427@gmail.com; juewang@adobe.com;
   sodomau@gmail.com; chpan@nlpr.ia.ac.cn
RI Wang, Jue/GVU-0480-2022; Cho, Sunghyun/X-5508-2019
OI Cho, Sunghyun/0000-0001-7627-3513; Wang, Jue/0000-0002-3641-3136
FU National Natural Science Foundation of China [61370039, 61175025,
   61203277, 61375024, 61203279]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61370039, 61175025, 61203277, 61375024 and 61203279.
CR [Anonymous], 2007, P IEEE C COMP VIS PA
   BURNS JB, 1986, IEEE T PATTERN ANAL, V8, P425, DOI 10.1109/TPAMI.1986.4767808
   Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chen J, 2008, LECT NOTES COMPUT SC, V5018, P1, DOI 10.1007/978-3-540-79723-4_1
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Cho TS, 2012, IEEE T PATTERN ANAL, V34, P683, DOI 10.1109/TPAMI.2011.166
   Cho TS, 2011, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2011.5995479
   Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   Gioi R., 2012, J IMAGE PROCESS LINE
   Gioi R., 2008, J MATH IMAGING VIS, V32, P317
   Goldstein A, 2012, LECT NOTES COMPUT SC, V7576, P622, DOI 10.1007/978-3-642-33715-4_45
   Hu W, 2012, IEEE T IMAGE PROCESS, V21, P386, DOI 10.1109/TIP.2011.2160073
   Joshi N, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587834
   Joshi N, 2009, PROC CVPR IEEE, P1550, DOI 10.1109/CVPRW.2009.5206802
   KAHN P, 1990, IEEE T PATTERN ANAL, V12, P1098, DOI 10.1109/34.61710
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   LAGENDIJK RL, 1990, OPT ENG, V29, P422, DOI 10.1117/12.55611
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Matas J, 2000, COMPUT VIS IMAGE UND, V78, P119, DOI 10.1006/cviu.1999.0831
   OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Simoncelli EP, 1998, CONF REC ASILOMAR C, P673, DOI 10.1109/ACSSC.1997.680530
   Sun L, 2012, INT C COMP PHOT, P1, DOI DOI 10.1109/ICCPHOT.2012.6215221
   Topal Cihan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2424, DOI 10.1109/ICPR.2010.593
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu L., 2006, ACM T GRAPHIC, V31, P787
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yitzhaky Y, 1998, J OPT SOC AM A, V15, P1512, DOI 10.1364/JOSAA.15.001512
   You YL, 1996, IEEE T IMAGE PROCESS, V5, P416, DOI 10.1109/83.491316
NR 37
TC 23
Z9 24
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 733
EP 746
DI 10.1007/s00371-014-0998-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400017
DA 2024-07-18
ER

PT J
AU Ding, L
   Ding, XQ
   Fang, C
AF Ding, Liu
   Ding, Xiaoqing
   Fang, Chi
TI 3D face sparse reconstruction based on local linear fitting
SO VISUAL COMPUTER
LA English
DT Article
DE Shape reconstruction; 3D morphable model; Local linear fitting; Pose
   estimation
ID RECOGNITION; SHAPE; POSE
AB 3D face shape provides a pose and illumination invariant description of human faces. In this paper, we propose a novel component based method to recover the full 3D face shape from a set of sparse feature points. We use a local linear fitting (LLF) scheme so that reconstruction of each subregion depends on both its own vertices and adjacent subregions. This method results in a separate set of shape coefficients each emphasizing the quality of one subregion and improves the model expressiveness. Experiments show that the LLF strategy significantly reduces the model residual error, and thus reduces the sparse reconstruction error under pose variations. Moreover, the problem of estimating pose parameters is revisited, and we use a joint optimization method to improve the reconstruction quality under unknown pose. We evaluate the sensitivity of our method to the selection of feature points. Simulation results show that our method is more robust than prevailing methods.
C1 [Ding, Liu; Ding, Xiaoqing; Fang, Chi] Tsinghua Univ, Dept Elect Engn, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Ding, L (corresponding author), Tsinghua Univ, Dept Elect Engn, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM dingliu@ocrserv.ee.tsinghua.edu.cn; dxq@ocrserv.ee.tsinghua.edu.cn;
   fangchi@ocrserv.ee.tsinghua.edu.cn
RI ding, xiao/KAM-4458-2024
FU National Natural Science Foundation of China [60972094]
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 60972094. The authors would like to thank Dr.
   Thomas Vetter for providing the BFM Database and Oswald Aldrian for
   detailed discussions on his paper. The authors also would like to thank
   the anonymous reviewers for their helpful suggestions for improving the
   quality of this paper.
CR Aldrian O, 2010, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2010.5653015
   Amberg B., 2007, PROC 11 IEEE INT C C, P1
   [Anonymous], P BRIT MACH VIS C 20
   [Anonymous], SELECTIVE VS GLOBAL
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212
   Blanz V., 2002, IT+TI Informationstechnik und Technische Informatik, V44, P295, DOI 10.1524/itit.2002.44.6.295
   Ding L, 2012, IEEE SIGNAL PROC LET, V19, P721, DOI 10.1109/LSP.2012.2215586
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Hamilton W. R., 1844, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, V25, P10, DOI [DOI 10.1080/14786444408644923, 10.1080/14786444408644923]
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Hwang BW, 2000, INT C PATT RECOG, P838, DOI 10.1109/ICPR.2000.906205
   Jiang DL, 2005, PATTERN RECOGN, V38, P787, DOI 10.1016/j.patcog.2004.11.004
   Knothe R, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P637
   Levine MD, 2009, PATTERN RECOGN LETT, V30, P908, DOI 10.1016/j.patrec.2009.03.011
   LOGIE RH, 1987, APPL COGNITIVE PSYCH, V1, P53, DOI 10.1002/acp.2350010108
   Moghaddam B, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P20
   Patel A, 2012, PATTERN RECOGN, V45, P1993, DOI 10.1016/j.patcog.2011.11.013
   Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Zhao WY, 2001, INT J COMPUT VISION, V45, P55, DOI 10.1023/A:1012369907247
NR 24
TC 8
Z9 8
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 189
EP 200
DI 10.1007/s00371-013-0795-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900005
DA 2024-07-18
ER

PT J
AU Hou, XY
   Sourina, O
AF Hou, Xiyuan
   Sourina, Olga
TI Stable adaptive algorithm for Six Degrees-of-Freedom haptic rendering in
   a dynamic environment
SO VISUAL COMPUTER
LA English
DT Article
DE Haptic rendering; Stability; Virtual coupling; Physical property;
   Dynamic simulation
ID DISPLAY
AB Recently, physically-based simulations with haptics interaction attracted many researchers. In this paper, we propose an adaptive Six Degrees-of-Freedom (6-DOF) haptic rendering algorithm based on virtual coupling, which can automatically adjust virtual coupling parameters according to mass values of the simulated virtual tools. The algorithm can overcome the virtual tool displacement problem caused by the large mass values of the virtual tool and can provide stable force/torque display. The force/torque magnitude is saturated to the maximum force/torque values of the haptic device automatically. The implemented algorithm is tested on the simple and complex standard benchmarks. The experimental results confirm that the proposed adaptive 6-DOF haptic rendering algorithm displays good stability and accuracy for haptic rendering of dynamic virtual objects with mass values.
C1 [Hou, Xiyuan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Sourina, Olga] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Hou, XY (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM houx0003@e.ntu.edu.sg; EOSourina@ntu.edu.sg
FU Ministry of Education of Singapore [MOE2011-T2-1-006]; Russian
   Foundation for Basic Research [12-07-00678-a]
FX This project is supported by the Ministry of Education of Singapore
   Grant MOE2011-T2-1-006 "Collaborative Haptic Modeling for Orthopaedic
   Surgery Training in Cyberspace" and by Russian Foundation for Basic
   Research 12-07-00678-a "Research and development of force interaction of
   virtual objects in the tasks of biomolecular simulation."
CR Adams RJ, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P1254, DOI 10.1109/IROS.1998.727471
   Andrews S., 2006, P FUTUREPLAY 2006, P1
   Barbic J, 2008, IEEE T HAPTICS, V1, P39, DOI [10.1109/TOH.2008.1, 10.1109/ToH.2008.1]
   Basdogan C, 2001, IEEE-ASME T MECH, V6, P269, DOI 10.1109/3516.951365
   Baxter B, 2001, COMP GRAPH, P461, DOI 10.1145/383259.383313
   Böttcher G, 2010, VISUAL COMPUT, V26, P903, DOI 10.1007/s00371-010-0450-1
   COLGATE JE, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P140, DOI 10.1109/IROS.1995.525875
   De Paolis Lucio T., 2008, 2008 First International Conference on Advances in Computer Human Interaction - ACHI '08, P26
   Gregory A, 2000, IEEE VISUAL, P139, DOI 10.1109/VISUAL.2000.885687
   Hamza-Lup FG, 2010, INTERNET HIGH EDUC, V13, P78, DOI 10.1016/j.iheduc.2009.12.004
   HOGAN N, 1985, J DYN SYST-T ASME, V107, P1, DOI 10.1115/1.3140702
   Hou XY, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P157, DOI 10.1109/CW.2012.29
   Kim YJ, 2003, PRESENCE-VIRTUAL AUG, V12, P277, DOI 10.1162/105474603765879530
   Kim YJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P921, DOI 10.1109/ROBOT.2002.1013474
   McNeely W., 2006, HAPTICS E, V3, P7
   McNeely WA, 1999, COMP GRAPH, P401, DOI 10.1145/311535.311600
   Nelson D. D., 1999, P ASME DYN SYST CONT, V67, P101
   Ortega M, 2006, P IEEE VIRT REAL ANN, P191, DOI 10.1109/VR.2006.18
   Otaduy M. A., 2006, SYNTHESIS LECT COMPU, V1, P1
   Otaduy MA, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P247
   Otaduy MA, 2003, ACM T GRAPHIC, V22, P543, DOI 10.1145/882262.882305
   Otaduy MA, 2006, IEEE T ROBOT, V22, P751, DOI 10.1109/tro.2006.876897
   Otaduy Tristan M. A., 2004, THESIS
   Petersik A, 2003, LECT NOTES COMPUT SC, V2673, P194
   Ruspini D. C., 1997, ANN C COMP GRAPH INT, P345, DOI DOI 10.1145/258734.258878
   Salisbury K., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P123, DOI 10.1145/199404.199426
   Salisbury K, 2004, IEEE COMPUT GRAPH, V24, P24, DOI 10.1109/MCG.2004.1274058
   Wan M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P257
   Zilles C. B., 1995, HUMAN ROBOT INTERACT, V3, P146
   Zonta N, 2009, J MOL MODEL, V15, P193, DOI 10.1007/s00894-008-0387-8
NR 30
TC 8
Z9 8
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1063
EP 1075
DI 10.1007/s00371-013-0838-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400008
DA 2024-07-18
ER

PT J
AU Liu, H
   Yang, YL
   AlHalawani, S
   Mitra, NJ
AF Liu, Han
   Yang, Yong-Liang
   AlHalawani, Sawsan
   Mitra, Niloy J.
TI Constraint-aware interior layout exploration for pre-cast concrete-based
   buildings
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Interior layout; Constraint-aware form exploration; Pre-cast concrete;
   Functional layouts; Computational design
AB Creating desirable layouts of building interiors is a complex task as designers have to manually adhere to various local and global considerations arising from competing practical and design considerations. In this work, we present an interactive design tool to create desirable floorplans by computationally conforming to such design constraints. Specifically, we support three types of constraints: (i) functional constraints such as number of rooms, connectivity among the rooms, target room areas, etc., (ii) design considerations such as user modifications and preferences, and (iii) fabrication constraints such as cost and convenience of manufacturing. Based on user specifications, our system automatically generates multiple floor layouts with associated 3D geometry that all satisfy the design specifications and constraints, thus exposing only the desirable family of interior layouts to the user. In this work, we focus on pre-cast concrete-based constructions, which lead to interesting discrete and continuous optimization possibilities. We test our framework on a range of complex real-world specifications and demonstrate the control and expressiveness of the exposed design space relieving the users of the task of manually adhering to non-local functional and fabrication constraints.
C1 [Liu, Han; Yang, Yong-Liang; AlHalawani, Sawsan] King Abdullah Univ Sci & Technol, Geometr Modeling & Sci Visualizat Ctr, Thuwal, Saudi Arabia.
   [Mitra, Niloy J.] UCL, Dept Comp Sci, London, England.
C3 King Abdullah University of Science & Technology; University of London;
   University College London
RP Liu, H (corresponding author), King Abdullah Univ Sci & Technol, Geometr Modeling & Sci Visualizat Ctr, Thuwal, Saudi Arabia.
EM han.liu@kaust.edu.sa; n.mitra@cs.ucl.ac.uk
RI Alhalawani, Sawsan/GPX-1181-2022
OI Alhalawani, Sawsan/0000-0002-6328-5328; Liu, Han/0000-0003-0647-6627;
   Yang, Yongliang/0000-0002-8071-5756
CR Arvin SA, 2002, AUTOMAT CONSTR, V11, P213, DOI 10.1016/S0926-5805(00)00099-6
   Dasgupta P, 2001, ACM T DES AUTOMAT EL, V6, P447, DOI 10.1145/502175.502176
   Elezkurtaj T., 2002, UMBAU, V2, P16
   GALLE P, 1981, COMMUN ACM, V24, P813, DOI 10.1145/358800.358804
   Gary M. R., 1979, COMPUTERS INTRACTABI
   Harada M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P199, DOI 10.1145/218380.218443
   Knecht K., 2010, P 13 GEN ART C, P238
   Korf R. E., 2003, Proceedings, Thirteenth International Conference on Automated Planning and Scheduling, P287
   Li Jiang, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P191
   Marson F, 2010, INT J COMPUT GAMES T, V2010, DOI 10.1155/2010/624817
   Medjdoub B, 2001, ARTIF INTELL ENG, V15, P47, DOI 10.1016/S0954-1810(00)00027-3
   Merrell P., 2010, COMPUTER GENERATED R, P181
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Michalek JJ, 2002, ENG OPTIMIZ, V34, P461, DOI 10.1080/03052150214016
   Otten R. H. J. M., 1982, ACM IEEE Nineteenth Design Automation Conference Proceedings, P261
   PAN PC, 1995, IEEE T COMPUT AID D, V14, P123, DOI 10.1109/43.363119
   Pei-Ning Guo, 1999, Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361), P268, DOI 10.1109/DAC.1999.781324
   Preas B. T., 1979, 16th design automation conference proceedings, P474, DOI 10.1109/DAC.1979.1600152
   Schneider S., 2011, Design Computing and Cognition '10, P367, DOI DOI 10.1007/978-94-007-0510-4_20
   Umetani N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185582
   Wong D. F., 1986, 23rd ACM/IEEE Design Automation Conference. Proceedings 1986 (Cat. No.86CH2288-9), P101, DOI 10.1145/318013.318030
   Zhou H, 2004, PR IEEE COMP DESIGN, P572
NR 22
TC 29
Z9 30
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 663
EP 673
DI 10.1007/s00371-013-0825-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400020
DA 2024-07-18
ER

PT J
AU Wu, J
   Dick, C
   Westermann, R
AF Wu, Jun
   Dick, Christian
   Westermann, Ruediger
TI Efficient collision detection for composite finite element simulation of
   cuts in deformable bodies
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Cutting; Deformable bodies; Collision detection; Composite finite
   elements
ID DISTANCE FIELDS
AB Composite finite elements (CFEs) based on a hexahedral discretization of the simulation domain have recently shown their effectiveness in physically based simulation of deformable bodies with changing topology. In this paper we present an efficient collision detection method for CFE simulation of cuts. Our method exploits the specific characteristics of CFEs, i.e., the fact that the number of simulation degrees of freedom is significantly reduced. We show that this feature not only leads to a faster deformation simulation, but also enables a faster collision detection. To address the non-conforming properties of geometric composition and hexahedral discretization, we propose a topology-aware interpolation approach for the computation of penetration depth. We show that this approach leads to accurate collision detection on complex boundaries. Our results demonstrate that by using our method cutting on high-resolution deformable bodies including collision detection and response can be performed at interactive rates.
C1 [Wu, Jun; Dick, Christian] Tech Univ Munich, Comp Graph & Visualizat Grp, D-80290 Munich, Germany.
   [Westermann, Ruediger] Tech Univ Munich, D-80290 Munich, Germany.
C3 Technical University of Munich; Technical University of Munich
RP Wu, J (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, D-80290 Munich, Germany.
EM jun.wu@tum.de
RI Wu, Jun/L-2487-2017
OI Wu, Jun/0000-0003-4237-1806
FU Erasmus Mundus TANDEM, an European Commission
FX The first author, Jun Wu, is supported by the Erasmus Mundus TANDEM, an
   European Commission funded program.
CR [Anonymous], 2009, P 2009 ACM SIGGRAPH, DOI DOI 10.1145/1599470.1599480
   [Anonymous], P 2010 ACM SIGGRAPH
   [Anonymous], P ACM SIGGRAPH EUR S
   Barbic J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778818
   Barbic J, 2008, IEEE T HAPTICS, V1, P39, DOI [10.1109/TOH.2008.1, 10.1109/ToH.2008.1]
   Cuisenaire O., 1997, Image Analysis and Processing. 9th International Conference, ICIAP '97 Proceedings, P263
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P1663, DOI 10.1109/TVCG.2010.268
   Faure F., 2008, P 2008 ACM SIGGRAPHE, P155
   Fisher S, 2001, SPRING EUROGRAP, P99
   Hackbusch W, 1997, NUMER MATH, V75, P447, DOI 10.1007/s002110050248
   Heidelberger B., 2004, PROC WSCG, P145
   Hirota G, 2001, COMP ANIM CONF PROC, P136, DOI 10.1109/CA.2001.982387
   Hoff KennethE., 2001, I3D 01, P145
   James DL, 2004, ACM T GRAPHIC, V23, P393, DOI 10.1145/1015706.1015735
   Jerábková L, 2010, PROG BIOPHYS MOL BIO, V103, P217, DOI 10.1016/j.pbiomolbio.2010.09.012
   Jones MW, 2006, IEEE T VIS COMPUT GR, V12, P581, DOI 10.1109/TVCG.2006.56
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Larsen E., 1999, TR99018 U N CAR DEP
   Liehr F, 2009, COMPUT VIS SCI, V12, P171, DOI 10.1007/s00791-008-0093-1
   McAdams A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964932
   Nesme M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531358
   Otaduy MA, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P83
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Preusser T., 2007, Proceedings of the 14th Workshop on the Finite Element Method in Biomedical Engineering, Biomechanics and Related Fields, P52
   Satherley R, 2001, SPRING EUROGRAP, P195
   Sauter S, 2006, COMPUTING, V77, P29, DOI 10.1007/s00607-005-0150-2
   Seiler M, 2011, VISUAL COMPUT, V27, P519, DOI 10.1007/s00371-011-0561-3
   Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Todd D, B4WIND USERS GUIDE T
   Wu J., 2011, VRIPHYS, P29
   Wu J, 2010, J COMPUT INF SCI ENG, V10, DOI 10.1115/1.3402759
NR 32
TC 21
Z9 27
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 739
EP 749
DI 10.1007/s00371-013-0810-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400027
DA 2024-07-18
ER

PT J
AU Schwenk, K
   Behr, J
   Fellner, DW
AF Schwenk, Karsten
   Behr, Johannes
   Fellner, Dieter W.
TI Filtering noise in progressive stochastic ray tracing
SO VISUAL COMPUTER
LA English
DT Article
DE Noise reduction; Progressive rendering; Illumination filtering;
   Antialiasing recovery
AB We present an improved version of a state-of-the-art noise reduction technique for progressive stochastic rendering. Our additions make the method significantly faster at the cost of an acceptable loss in quality. Additionally, we improve the robustness of the method in the presence of difficult features like glossy reflection, caustics, and antialiased edges. We show with visual and numerical comparisons that our extensions improve the overall performance of the original approach and make it more broadly applicable.
C1 [Schwenk, Karsten; Fellner, Dieter W.] Fraunhofer IGD, Darmstadt, Germany.
   [Behr, Johannes] Fraunhofer IGD, Dept Visual Comp Syst Technol, Darmstadt, Germany.
   [Fellner, Dieter W.] Tech Univ Darmstadt, Interact Graph Syst Grp, Darmstadt, Germany.
   [Fellner, Dieter W.] Graz Univ Technol, Inst Comp Graph & Knowledge Visualizat, Graz, Austria.
C3 Technical University of Darmstadt; Graz University of Technology
RP Schwenk, K (corresponding author), Fraunhofer IGD, Darmstadt, Germany.
EM karsten.schwenk@igd.fraunhofer.de; johannes.behr@igd.fraunhofer.de;
   dieter.fellner@igd.fraunhofer.de
OI Fellner, Dieter W./0000-0001-7756-0901
CR Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327
   [Anonymous], P IEEE INT C MULT EX
   Bauszat P, 2011, COMPUT GRAPH FORUM, V30, P1361, DOI 10.1111/j.1467-8659.2011.01996.x
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chen YC, 2012, COMPUT GRAPH FORUM, V31, P189, DOI 10.1111/j.1467-8659.2012.02094.x
   Dammertz H., 2010, P C HIGH PERF GRAPH, P67, DOI DOI 10.5555/1921479.1921491
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Pajot Anthony., 2011, P GRAPHICS INTERFACE, P159
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Rushmeier H. E., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P131, DOI 10.1145/192161.192189
   Schwenk K, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2012.30
   Suykens F., 2000, WSCG 00 P
   Yang L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966401
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
NR 15
TC 0
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 359
EP 368
DI 10.1007/s00371-012-0738-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200004
DA 2024-07-18
ER

PT J
AU Dutagaci, H
   Cheung, CP
   Godil, A
AF Dutagaci, Helin
   Cheung, Chun Pan
   Godil, Afzal
TI Evaluation of 3D interest point detection techniques via human-generated
   ground truth
SO VISUAL COMPUTER
LA English
DT Article
DE 3D interest points; 3D salient points; 3D shape analysis
ID SALIENT; FEATURES
AB In this paper, we present an evaluation strategy based on human-generated ground truth to measure the performance of 3D interest point detection techniques. We provide quantitative evaluation measures that relate automatically detected interest points to human-marked points, which were collected through a web-based application. We give visual demonstrations and a discussion on the results of the subjective experiments. We use a voting-based method to construct ground truth for 3D models and propose three evaluation measures, namely False Positive and False Negative Errors, and Weighted Miss Error to compare interest point detection algorithms.
C1 [Dutagaci, Helin] Eskisehir Osmangazi Univ, Elect Elect Engn Dept, Eskisehir, Turkey.
   [Godil, Afzal] NIST, Informat Technol Lab, Gaithersburg, MD 20899 USA.
C3 Eskisehir Osmangazi University; National Institute of Standards &
   Technology (NIST) - USA
RP Dutagaci, H (corresponding author), Eskisehir Osmangazi Univ, Elect Elect Engn Dept, Eskisehir, Turkey.
EM helindutagaci@gmail.com; godil@nist.gov
RI Dutagaci, Helin/KFR-3162-2024
CR [Anonymous], 3D IMAGE PROCESSING
   [Anonymous], INS J 2005 MICCAI OP
   [Anonymous], 2010, P EUR WORKSH 3D OBJ
   [Anonymous], 2011, VISUAL COMPUT, DOI DOI 10.1007/s00371-011-0610-y
   Boyer E., 2011, Proceedings of the 4th Eurographics Conference on 3D Object Retrieval, P71
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chung MK, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 AND 2, P432
   Dutagaci H., 2011, Proceedings of the 4th Eurographics Conference on 3D Object Retrieval, 3DOR'11, P57
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Harris C., 1988, ALVEY VISION C, P147151
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Kim Y, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670676
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Litman R, 2011, COMPUT GRAPH-UK, V35, P549, DOI 10.1016/j.cag.2011.03.011
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maes C., 2010, BIOMETRICS THEORY AP, DOI [DOI 10.1109/BTAS.2010.5634543, 10.1109/BTAS.2010.5634543]
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Novatnack J., 2007, ICCV, P1
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Sebe N, 2003, PATTERN RECOGN LETT, V24, P89, DOI 10.1016/S0167-8655(02)00192-7
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Walter N, 2008, IEEE IMAGE PROC, P1512, DOI 10.1109/ICIP.2008.4712054
   WESSEL R., 2006, proceedings of Vision, Modeling, and Visualization, P365
   Zaharescu A., 2009, CVPR
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
NR 26
TC 84
Z9 100
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2012
VL 28
IS 9
BP 901
EP 917
DI 10.1007/s00371-012-0746-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 985QX
UT WOS:000307284700002
DA 2024-07-18
ER

PT J
AU Heider, P
   Pierre-Pierre, A
   Li, R
   Mueller, R
   Grimm, C
AF Heider, Paul
   Pierre-Pierre, Alain
   Li, Ruosi
   Mueller, Rolf
   Grimm, Cindy
TI Comparing local shape descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE Spin images; Shape descriptors; Mean curvature; Gaussian curvature;
   Feature detection
ID SURFACE SHAPE; REPRESENTATION; SIGNATURES
AB Local shape descriptors can be used for a variety of tasks, from registration to comparison to shape analysis and retrieval. There have been a variety of local shape descriptors developed for these tasks, which have been evaluated in isolation or in pairs, but not against each other. We provide a survey of existing descriptors and a framework for comparing them. We perform a detailed evaluation of the descriptors using real data sets from a variety of sources. We first evaluate how stable these metrics are under changes in mesh resolution, noise, and smoothing. We then analyze the discriminatory ability of the descriptors for the task of shape matching. Finally, we compare the descriptors on a shape classification task. Our conclusion is that sampling the normal distribution and the mean curvature, using 25 samples, and reducing this data to 5-10 samples via Principal Components Analysis, provides robustness to noise and the best shape discrimination results. For shape classification, mean curvature sampled at the vertex or averaged, and the more global Shape Diameter Function, performed the best.
C1 [Grimm, Cindy] Washington Univ, St Louis, MO 63130 USA.
   [Mueller, Rolf] Virginia Tech, Dept Mech Engn, Blacksburg, VA USA.
   [Li, Ruosi] Facebook, Palo Alto, CA USA.
C3 Washington University (WUSTL); Virginia Polytechnic Institute & State
   University; Facebook Inc
RP Grimm, C (corresponding author), Washington Univ, St Louis, MO 63130 USA.
EM rolf.mueller@vt.edu; cmg@wustl.edu
OI Grimm, Cindy/0000-0002-1711-7112
FU National Science Foundation [CCF 0702662, DBI 1053171, DMS 0540701]; NIH
   [T90 DA022871]; Shandong Taishan Fund; NNSF of China; Ministry of
   Education, People's Republic of China [985, 211]; Shandong University;
   EU CILIA Project; Div Of Biological Infrastructure; Direct For
   Biological Sciences [1313810] Funding Source: National Science
   Foundation
FX Funded in part by National Science Foundation grants CCF 0702662, DBI
   1053171, DMS 0540701, NIH T90 DA022871, Shandong Taishan Fund, NNSF of
   China, Ministry of Education, People's Republic of China (985 & 211),
   Shandong University, and the EU CILIA Project. Thanks to Dr. Bayly for
   the ferret brains, Dr. Daniel Low for the mandibles, and Dr. Crisco for
   the radius and ulna bones.
CR [Anonymous], 2011, Proceedings of the 4th Eurographics conference on 3D Object Retrieval, EG 3DOR'11, DOI DOI 10.2312/3DOR/3DOR11/049-056
   [Anonymous], 2003, 7 CENTR EUR SEM COMP
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   Cipriano G, 2009, IEEE T VIS COMPUT GR, V15, P1201, DOI 10.1109/TVCG.2009.168
   Clarenz U, 2004, VISUAL COMPUT, V20, P329, DOI 10.1007/s00371-004-0245-3
   Clarenz U, 2004, IEEE T VIS COMPUT GR, V10, P516, DOI 10.1109/TVCG.2004.34
   CONNOLLY ML, 1986, J MOL GRAPHICS, V4, P3
   de Goes F, 2008, COMPUT GRAPH FORUM, V27, P1349, DOI 10.1111/j.1467-8659.2008.01274.x
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Fehr J, 2009, LECT NOTES COMPUT SC, V5875, P34
   Fischl B, 1999, NEUROIMAGE, V9, P195, DOI 10.1006/nimg.1998.0396
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Gatzke T, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/SMI.2005.13
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Grimm C., 2011, 2011 IEEE 1st International Conference on Computational Advances in Bio and Medical Sciences (ICCABS), DOI 10.1109/ICCABS.2011.5729898
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Li X., 2005, SGP 2005
   Li XY, 2009, MODELLING SIMULATION, P437, DOI 10.1109/ICIP.2009.5414415
   Mortara M, 2004, ALGORITHMICA, V38, P227, DOI 10.1007/s00453-003-1051-4
   Ong J.L., 2010, IEEE T IMAGE PROCESS
   Pottmann H, 2009, COMPUT AIDED GEOM D, V26, P37, DOI 10.1016/j.cagd.2008.01.002
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Schmidt R, 2006, ACM T GRAPHIC, V25, P605, DOI 10.1145/1141911.1141930
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Yamany S. M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1098, DOI 10.1109/ICCV.1999.790402
   Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806
   Zelinka S., 2004, SGP 04 PROC 2004 EUR, P204, DOI DOI 10.1145/1057432.1057460
NR 35
TC 5
Z9 5
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2012
VL 28
IS 9
BP 919
EP 929
DI 10.1007/s00371-012-0725-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 985QX
UT WOS:000307284700003
DA 2024-07-18
ER

PT J
AU Cho, Y
   Kim, M
   Park, KS
AF Cho, Yongjoo
   Kim, Minyoung
   Park, Kyoung Shin
TI LOTUS: composing a multi-user interactive tiled display virtual
   environment
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Multi-user interaction; Tiled display; Mobile interface
ID ARCHITECTURE; PERFORMANCE
AB Recent development of advanced visualization technologies enables bringing virtual reality applications to a more cost-effective, scalable, high-resolution tiled display. This paper introduces a novel interactive tiled display framework called LOTUS, which allows designers to easily compose a multi-user interactive virtual environment using XML scripts for the cluster-based tiled display system. LOTUS supports more flexible multi-user interaction within multiple virtual environments on a tiled display through the use of a mobile interface coupled with the tiled display application controller. This paper describes the design and implementation of the LOTUS framework architecture, and its use in constructing various LOTUS applications and user interaction scenarios. It then concludes with suggestions for some improvements.
C1 [Park, Kyoung Shin] Dankook Univ, Dept Multimedia Engn, Cheonan, South Korea.
   [Cho, Yongjoo] Sangmyung Univ, Div Digital Media, Seoul, South Korea.
   [Kim, Minyoung] Sangmyung Univ, Dept Comp Sci, Seoul, South Korea.
   [Cho, Yongjoo] Sangmyung Univ, Interact Comp & Entertainment Lab, Seoul, South Korea.
C3 Dankook University; Sangmyung University; Sangmyung University;
   Sangmyung University
RP Park, KS (corresponding author), Dankook Univ, Dept Multimedia Engn, Cheonan, South Korea.
EM kpark@dankook.ac.kr
FU Dankook University
FX This present research was conducted by the research fund of Dankook
   University in 2011.
CR Ball R., 2005, CHI'05 extended abstracts on Human factors in computing systems, P1196, DOI 10.1145/1056808.1056875
   BOSE S, 2010, P INT C ADV COMP ENG, P316
   Cho HS, 2006, LECT NOTES COMPUT SC, V4161, P362
   CHO Y, 2011, P COMP GRAPH INT
   Cho YJ, 2005, LECT NOTES COMPUT SC, V3675, P194
   CHOE B, 2009, P HCI KOR C, P378
   CHOE G, 2007, P 7 IEEE INT C COMP
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P125
   Davis J, 2002, DISPLAYS, V23, P205, DOI 10.1016/S0141-9382(02)00039-2
   Eilemann S, 2009, IEEE T VIS COMPUT GR, V15, P436, DOI 10.1109/TVCG.2008.104
   Gaitatzes A., 2001, Proceedings of the 2001 conference on virtual reality, archaeology, and cultural heritage, P103, DOI [10.1145/584993.585011, DOI 10.1145/584993.585011]
   HARISH PN, 2007, IEEE T VIS COMPUT GR, V13, P377
   He E, 2003, FUTURE GENER COMP SY, V19, P919, DOI 10.1016/S0167-739X(03)00071-2
   Hirose M., 2006, Int. J. Virtual Real., V5, P31, DOI 10.20870/IJVR.2006.5.2.2686
   Jeong B., 2006, SC 2006 Conference, Proceedings of the ACM/IEEE, P24, DOI 10.1109/SC.2006.35
   Kim S, 2009, GRAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P367
   LEIGH J, 2007, P 1 INT S UN COMM
   Morillo P, 2008, J PARALLEL DISTR COM, V68, P221, DOI 10.1016/j.jpdc.2007.08.009
   Ni T, 2006, P IEEE VIRT REAL ANN, P223
   NOVAK J, 2008, P C HUM FACT COMP SY, P2877
   PAPE D, 2001, P INET
   Park C, 2003, COMPUT GRAPH-UK, V27, P223, DOI 10.1016/S0097-8493(02)00279-0
   Park C, 2003, PRESENCE-VIRTUAL AUG, V12, P125, DOI 10.1162/105474603321640905
   Peck SM, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P31
   RENAMBOT L, 2002, ENABLING TILED DISPL
   RICHARDS J, 2006, P 36 ANN FRONT ED C, P27
   Robertson G, 2005, IEEE COMPUT GRAPH, V25, P44, DOI 10.1109/MCG.2005.88
   ROUSSOU M, 2002, P VIRT REAL ARCH CUL, P93
   Speakman T., 2001, 3208 RFC
NR 29
TC 7
Z9 7
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 99
EP 109
DI 10.1007/s00371-011-0631-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000009
DA 2024-07-18
ER

PT J
AU Singh, VK
   Nevatia, R
AF Singh, Vivek Kumar
   Nevatia, Ram
TI Simultaneous tracking and action recognition for single actor human
   actions
SO VISUAL COMPUTER
LA English
DT Article
DE Human action recognition; Dynamic Bayesian network; Pictorial structure
AB This paper presents an approach to simultaneously tracking the pose and recognizing human actions in a video. This is achieved by combining a Dynamic Bayesian Action Network (DBAN) with 2D body part models. Existing DBAN implementation relies on fairly weak observation features, which affects the recognition accuracy. In this work, we use a 2D body part model for accurate pose alignment, which in turn improves both pose estimate and action recognition accuracy. To compensate for the additional time required for alignment, we use an action entropy-based scheme to determine the minimum number of states to be maintained in each frame while avoiding sample impoverishment. In addition, we also present an approach to automation of the keypose selection task for learning 3D action models from a few annotations. We demonstrate our approach on a hand gesture dataset with 500 action sequences, and we show that compared to DBAN our algorithm achieves 6% improvement in accuracy.
C1 [Singh, Vivek Kumar; Nevatia, Ram] Univ So Calif, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Singh, VK (corresponding author), Univ So Calif, Los Angeles, CA 90089 USA.
EM vivekks@gmail.com; nevatia@usc.edu
OI Singh, Vivek/0000-0001-6721-0211
CR [Anonymous], 2010, P 7 INDIAN C COMPUTE
   [Anonymous], 2004, levmar: Levenberg-Marquardt nonlinear least squares algorithms in C/C ++
   Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1
   Fathi A., 2008, COMPUTER VISION PATT, P1
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   GUPTA A, 2008, COMPUTER VISION PATT
   Ikizler N., 2007, Computer Vision and Pattern Recognition
   Ke Y., 2007, INT C COMP VIS ICCV
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lee MW, 2006, LECT NOTES COMPUT SC, V3953, P368
   Lv F. J., 2007, COMPUTER VISION PATT
   Morency L.-P., 2007, COMPUTER VISION PATT
   Natarajan Pradeep., 2010, CVPR
   Natarajan Pradeep., 2008, CVPR
   Shet Vinay., 2004, ICVGIP
   Sigal L., 2006, CVPR, P2041
   Sminchisescu C, 2005, IEEE I CONF COMP VIS, P1808
   Taylor CJ, 2000, COMPUT VIS IMAGE UND, V80, P349, DOI 10.1006/cviu.2000.0878
   Urtasun R., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P238, DOI DOI 10.1109/CVPR.2006.15
   Weinland D., 2006, COMPUTER VISION PATT, V2, P1639
   Wu B, 2005, IEEE I CONF COMP VIS, P90
NR 21
TC 10
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1115
EP 1123
DI 10.1007/s00371-011-0656-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500007
DA 2024-07-18
ER

PT J
AU Li, JT
   Lu, GD
   Ye, JT
AF Li, Jituo
   Lu, Guodong
   Ye, Juntao
TI Automatic skinning and animation of skeletal models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Skeleton; Cage; Deformation
ID MESH DEFORMATION
AB In this paper, we present an efficient yet easy-to-implement technique which performs automatic skinning and animation of skeletal models. At a pre-processing stage, a character model is firstly decomposed into a number of segments per bone basis, and each segment is then subdivided into several chunks. A convex cage is automatically created for each chunk. The skinning and animation of skeletal models is achieved via two steps. At the first step, by minimizing a sum of several energy terms, chunk cages are implicitly skinned to the skeleton and animated. These energies are carefully designed to prevent unnatural volume change and guarantee smooth deformation transition between adjacent cages. At the second step, the model mesh vertices, represented as the mean-value coordinates with reference to proper cage vertices, are updated via cage-based deformation technique. Our approach avoids the labor-intensive process of vertex weighting and cage generation. Given the motion of a skeleton, the character model can be animated automatically.
C1 [Li, Jituo; Lu, Guodong] Zhejiang Univ, Dept Mech Engn, Engn & Comp Graph Inst, Hangzhou 310027, Peoples R China.
   [Li, Jituo] Donghua Univ, Minist Educ, Engn Res Ctr Digitized Text & Fash Technol, Shanghai, Peoples R China.
   [Ye, Juntao] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Zhejiang University; Donghua University; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Li, JT (corresponding author), Zhejiang Univ, Dept Mech Engn, Engn & Comp Graph Inst, Hangzhou 310027, Peoples R China.
EM jituo_li@zju.edu.cn
CR [Anonymous], 2007, S GEOMETRY PROCESSIN
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Avis D, 1997, COMP GEOM-THEOR APPL, V7, P265, DOI 10.1016/S0925-7721(96)00023-5
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Ben-Chen M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531340
   BENCHEN M, 2009, EUR ACM SIGGRAPH S C, P67
   BLOOMENTHAL J, 2002, ACM SIGGRAPH EUR S C, P147
   CAPELL S, 2002, SIGGRAPH 02, P586
   Chen L, 2010, COMPUT GRAPH-UK, V34, P107, DOI 10.1016/j.cag.2010.01.003
   CUNO A, 2009, VISUAL COMPUT, V25, P911
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Forstmann S, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P141
   Guo Z, 2005, COMPUT GRAPH FORUM, V24, P373, DOI 10.1111/j.1467-8659.2005.00862.x
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Hyun DE, 2005, VISUAL COMPUT, V21, P542, DOI 10.1007/s00371-005-0343-x
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Ju T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409075
   KAVAN L, 2005, 2005 ACM SIGGRAPH S, P9
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li JT, 2010, COMPUT GRAPH-UK, V34, P742, DOI 10.1016/j.cag.2010.07.008
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   *MAYA, 3DSTUDIO MAX
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   MILLER C, 2010, SI3D 2010, P31
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Muico U, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531387
   Sorkine O., 2005, Eurographics 2005-State of the Art Reports, V4, P53, DOI [10.2312/egst.20051044, DOI 10.2312/EGST.20051044]
   Sorkine O, 2006, COMPUT GRAPH FORUM, V25, P789, DOI 10.1111/j.1467-8659.2006.00999.x
   Teran J, 2005, IEEE T VIS COMPUT GR, V11, P317, DOI 10.1109/TVCG.2005.42
   Weber O, 2007, COMPUT GRAPH FORUM, V26, P265, DOI 10.1111/j.1467-8659.2007.01048.x
   XIAN C, 2009, IEEE INT C SHAP MOD, P21
   Xu D, 2006, GRAPH MODELS, V68, P268, DOI 10.1016/j.gmod.2006.03.001
   Xu WW, 2009, J COMPUT SCI TECH-CH, V24, P6, DOI 10.1007/s11390-009-9209-4
   Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28
   Yang XS, 2006, COMPUT ANIMAT VIRT W, V17, P281, DOI 10.1002/cav.132
   Yuksel Can, 2007, Symposium on Geometry Processing, P153, DOI DOI 10.2312/SGP/SGP07/153-162
   Zhang ST, 2011, GRAPH MODELS, V73, P10, DOI 10.1016/j.gmod.2010.10.003
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 42
TC 4
Z9 6
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 585
EP 594
DI 10.1007/s00371-011-0585-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600017
DA 2024-07-18
ER

PT J
AU Liu, HJ
   He, FZ
   Cai, XT
   Chen, X
   Chen, Z
AF Liu, Huajun
   He, Fazhi
   Cai, Xiantao
   Chen, Xiao
   Chen, Zhao
TI Performance-based control interfaces using mixture of factor analyzers
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Performance animation; Data-driven animation; Motion capture; Inertial
   sensors
AB This paper introduces an approach to performance animation that employs a small number of inertial measurement sensors to create an easy-to-use system for an interactive control of a full-body human character. Our key idea is to construct a global model from a prerecorded motion database and utilize them to construct full-body human motion in a maximum a posteriori framework (MAP). We have demonstrated the effectiveness of our system by controlling a variety of human actions, such as boxing, golf swinging, and table tennis, in real time. One unique property of our system is its ability to learn priors from a large and heterogeneous motion capture database and use them to generate a wide range of natural poses, a capacity that has not been demonstrated in previous data-driven character posing systems.
C1 [Liu, Huajun; He, Fazhi; Cai, Xiantao; Chen, Xiao; Chen, Zhao] Wuhan Univ, Sch Comp, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Hubei, Peoples R China.
EM graphics@whu.edu.cn; fzhe@whu.edu.cn
RI Cai, xiantao/AAE-1499-2019; He, Fazhi/Q-3691-2018
CR Badler N.I., 1993, PRESENCE-VIRTUAL AUG, V2, P82, DOI 10.1162/pres.1993.2.1.82
   Bazaraa M.S., 2013, NONLINEAR PROGRAMMIN
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Ghahramani Z., 1997, Technical Repoort CRG-TR-96-1)
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Ishigaki S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531367
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lau M, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640446
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Park SI., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. SCA2, P105, DOI DOI 10.1145/545261.545279
   Rose C, 1998, IEEE COMPUT GRAPH, V18, P32, DOI 10.1109/38.708559
   Semwal SK, 1998, PRESENCE-TELEOP VIRT, V7, P1, DOI 10.1162/105474698565497
   SLYPER R, 2008, 2008 ACM SIGGRAPH EU
   Yin KangKang, 2003, 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P329
NR 18
TC 10
Z9 10
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 595
EP 603
DI 10.1007/s00371-011-0563-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600018
DA 2024-07-18
ER

PT J
AU Meng, Y
   Zhang, H
AF Meng, Yu
   Zhang, Hui
TI Registration of point clouds using sample-sphere and adaptive distance
   restriction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Point cloud; Pairwise rigid registration; Range image alignment;
   Iterative closest point; Random sample consensus
ID ICP; IMPLEMENTATION; ALGORITHM
AB Registration of point clouds is a fundamental problem in shape acquisition and shape modeling. In this paper, a novel technique, the sample-sphere method, is proposed to register a pair of point clouds in arbitrary initial positions. This method roughly aligns point clouds by matching pairs of triplets of points, which are approximately congruent under rigid transformation. For a given triplet of points, this method can find all its approximately congruent triplets in O(knlog n) time, where n is the number of points in the point cloud, and k is a constant depending only on a given tolerance to the rotation error. By employing the techniques of wide bases and largest common point set (LCP), our method is resilient to noise and outliers. Another contribution of this paper is proposing an adaptive distance restriction to improve ICP (iterative closest point) algorithm, which is a classical method to refine rough alignments. With this restriction, the improved ICP is able to reject unreasonable corresponding point pairs during each iteration, so it can precisely align the point clouds which have large non-overlapping regions.
C1 [Meng, Yu; Zhang, Hui] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Meng, Yu; Zhang, Hui] Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Meng, Yu; Zhang, Hui] Minist Educ, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Meng, Y (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM meng-y08@mails.tsinghua.edu.cn; huizhang@mail.tsinghua.edu.cn
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Almhdie A, 2007, PATTERN RECOGN LETT, V28, P1523, DOI 10.1016/j.patrec.2007.03.005
   [Anonymous], STRAITS TIMES 0917
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BOGDAN RR, 2009, ICRA 09, P1848
   Brown BJ, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276404, 10.1145/1239451.1239472]
   CAGATAY OB, 2008, VISUAL COMPUT, V24, P679
   Dalley G, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P246, DOI 10.1109/IM.2001.924446
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Goodrich M. T., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P103, DOI 10.1145/177424.177572
   Greenspan M, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P442, DOI 10.1109/IM.2003.1240280
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Irani S., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P68, DOI 10.1145/237218.237240
   Li N, 2005, EXP MECH, V45, P71, DOI 10.1177/0014485105050909
   Liu YH, 2004, PATTERN RECOGN, V37, P211, DOI 10.1016/S0031-3203(03)00239-5
   Liu YH, 2007, PATTERN RECOGN, V40, P2418, DOI 10.1016/j.patcog.2006.11.025
   Liu YH, 2006, IMAGE VISION COMPUT, V24, P762, DOI 10.1016/j.imavis.2006.01.009
   Liu YH, 2006, ROBOT AUTON SYST, V54, P428, DOI 10.1016/j.robot.2006.02.008
   Liu YH, 2009, INT J COMPUT VISION, V83, P30, DOI 10.1007/s11263-009-0210-8
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   RUSU RB, 2008, P 10 INT C INT AUT S
   Sharp GC, 2002, IEEE T PATTERN ANAL, V24, P90, DOI 10.1109/34.982886
   Silva L, 2005, IEEE T PATTERN ANAL, V27, P762, DOI 10.1109/TPAMI.2005.108
NR 26
TC 13
Z9 23
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 543
EP 553
DI 10.1007/s00371-011-0580-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600013
DA 2024-07-18
ER

PT J
AU Weber, D
   Kalbe, T
   Stork, A
   Fellner, D
   Goesele, M
AF Weber, Daniel
   Kalbe, Thomas
   Stork, Andre
   Fellner, Dieter
   Goesele, Michael
TI Interactive deformable models with quadratic bases in
   Bernstein-B,zier-form
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Deformation; Quadratic finite elements; Interactive simulation;
   Bernstein-Bezier form
AB We present a physically based interactive simulation technique for de formable objects. Our method models the geometry as well as the displacements using quadratic basis functions in Bernstein-B,zier form on a tetrahedral finite element mesh. The Bernstein-B,zier formulation yields significant advantages compared to approaches using the monomial form. The implementation is simplified, as spatial derivatives and integrals of the displacement field are obtained analytically avoiding the need for numerical evaluations of the elements' stiffness matrices. We introduce a novel traversal accounting for adjacency in order to accelerate the reconstruction of the global matrices. We show that our proposed method can compensate the additional effort introduced by the co-rotational formulation to a large extent. We validate our approach on several models and demonstrate new levels of accuracy and performance in comparison to current state-of-the-art.
C1 [Weber, Daniel; Stork, Andre; Fellner, Dieter] Fraunhofer IGD, Darmstadt, Germany.
   [Kalbe, Thomas; Stork, Andre; Fellner, Dieter; Goesele, Michael] Tech Univ Darmstadt, GRIS, Darmstadt, Germany.
   [Kalbe, Thomas] Tech Univ Darmstadt, Dept GRIS, Darmstadt, Germany.
C3 Technical University of Darmstadt; Technical University of Darmstadt
RP Weber, D (corresponding author), Fraunhofer IGD, Darmstadt, Germany.
EM daniel.weber@igd.fraunhofer.de
OI Weber, Daniel/0000-0001-6184-0570; Fellner, Dieter
   W./0000-0001-7756-0901; Goesele, Michael/0000-0002-0944-0980
CR [Anonymous], 1982, FINITE ELEMENT PROCE
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   ETZMUSS O, 2003, PG
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Georgii Joachim., 2008, VRIPHYS, V8, P11
   Hauth M, 2003, VISUAL COMPUT, V19, P581, DOI 10.1007/s00371-003-0206-2
   Hauth M., 2004, PROC WSCG, V12, P137
   IRVING G, 2004, SCA
   MEZGER J, 2008, SPM
   MEZGER J, 2006, AMDO
   Müller M, 2004, PROC GRAPH INTERF, P239
   MULLER M, 2002, SCA
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   PARKER EG, 2009, SCA 09
   ROTH S, 2002, THESIS ETHZ
   Roth SHM, 1998, COMPUT GRAPH FORUM, V17, pC285, DOI 10.1111/1467-8659.00275
   Schumaker L., 2007, Spline Functions on Triangulation
   SERNA SP, 2009, VRIPHYS
   TERZOPOULOS D, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.20317
   Zienckiewicz OC, 2000, FINITE ELEMENT METHO
NR 21
TC 13
Z9 13
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 473
EP 483
DI 10.1007/s00371-011-0579-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600007
DA 2024-07-18
ER

PT J
AU Ruiz, O
   Vanegas, C
   Cadavid, C
AF Ruiz, O.
   Vanegas, C.
   Cadavid, C.
TI Ellipse-based principal component analysis for self-intersecting curve
   reconstruction from noisy point sets
SO VISUAL COMPUTER
LA English
DT Article
DE Self-intersecting curve reconstruction; Elliptic support region;
   Principal component analysis; Noisy samples
AB Surface reconstruction from cross cuts usually requires curve reconstruction from planar noisy point samples. The output curves must form a possibly disconnected 1-manifold for the surface reconstruction to proceed. This article describes an implemented algorithm for the reconstruction of planar curves (1-manifolds) out of noisy point samples of a self-intersecting or nearly self-intersecting planar curve C. C:[a,b]aS,R -> R (2) is self-intersecting if C(u)=C(v), u not equal v, u,va(a,b) (C(u) is the self-intersection point). We consider only transversal self-intersections, i.e. those for which the tangents of the intersecting branches at the intersection point do not coincide (C'(u)not equal C'(v)). In the presence of noise, curves which self-intersect cannot be distinguished from curves which nearly self-intersect. Existing algorithms for curve reconstruction out of either noisy point samples or pixel data, do not produce a (possibly disconnected) Piecewise Linear 1-manifold approaching the whole point sample. The algorithm implemented in this work uses Principal Component Analysis (PCA) with elliptic support regions near the self-intersections. The algorithm was successful in recovering contours out of noisy slice samples of a surface, for the Hand, Pelvis and Skull data sets. As a test for the correctness of the obtained curves in the slice levels, they were input into an algorithm of surface reconstruction, leading to a reconstructed surface which reproduces the topological and geometrical properties of the original object. The algorithm robustly reacts not only to statistical non-correlation at the self-intersections (non-manifold neighborhoods) but also to occasional high noise at the non-self-intersecting (1-manifold) neighborhoods.
C1 [Vanegas, C.] EAFIT Univ, Lab CAD CAM CAE, Medellin, Colombia.
   [Ruiz, O.; Cadavid, C.] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 Universidad EAFIT; Purdue University System; Purdue University
RP Vanegas, C (corresponding author), EAFIT Univ, Lab CAD CAM CAE, Cra 49 7 sur 50, Medellin, Colombia.
EM oruiz@eafit.edu.co; cvanegas@cs.purdue.edu; ccadavid@eafit.edu.co
RI Ruiz, Oscar/HPE-8049-2023; RUIZ-SALGUERO, OSCAR/C-2255-2013
OI Cadavid, Carlos/0000-0002-7421-7531; RUIZ, OSCAR/0000-0002-9674-8974
CR Arora S, 2003, J COMPUT SYST SCI, V67, P325, DOI 10.1016/S0022-0000(03)00012-6
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   BLOOMENTHAL J, 1995, SIGGRAPH 95, P309
   Cheng SW, 2005, COMP GEOM-THEOR APPL, V31, P63, DOI 10.1016/j.comgeo.2004.07.004
   Dey T.K., 1999, P SODA BALT MD US 17, VVolume 99, P893
   GEIGER B, 1993, RR2105
   Kégl B, 2002, IEEE T PATTERN ANAL, V24, P59, DOI 10.1109/34.982884
   Klein J, 2004, COMPUT GRAPH-UK, V28, P839, DOI 10.1016/j.cag.2004.08.012
   KTGL B, 1999, THESIS CONCORDIA U M
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Liu Y, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P4
   LU D, 2005, 3 INT WORKSH VAR GEO, P283
   Mukhopadhyay A, 2007, I C COMP GRAPH IM VI, P177, DOI 10.1109/CGIV.2007.32
   Niyogi P, 2008, DISCRETE COMPUT GEOM, V39, P419, DOI 10.1007/s00454-008-9053-2
   Nyquist H, 2002, P IEEE, V90, P280, DOI 10.1109/5.989875
   NYQUIST H, 1928, BELL SYST TECH J, V47
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   Osher S., 2000, Level set methods: An overview and some recent results
   Pauly Mark, 2004, Eurographics Symposium on Point-Based Graphics, P77
   Ruiz O, 2007, J ENG DESIGN, V18, P437, DOI 10.1080/09544820701403771
   Ruiz OE, 2005, COMPUT GRAPH-UK, V29, P81, DOI 10.1016/j.cag.2004.11.009
   Shannon CE, 1998, P IEEE, V86, P447, DOI 10.1109/JPROC.1998.659497
   SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969
   Tagliasacchi A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531377
   UNNIKRISHNAN R, 2006, 3DPVT, P1026
   URIBE D, 2008, 2D CURVE RECONSTRUCT
   Verbeek JJ, 2001, LECT NOTES COMPUT SC, V2130, P450
   Wang WP, 2006, ACM T GRAPHIC, V25, P214, DOI 10.1145/1138450.1138453
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
   Zhao HK, 2000, COMPUT VIS IMAGE UND, V80, P295, DOI 10.1006/cviu.2000.0875
NR 31
TC 8
Z9 8
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 211
EP 226
DI 10.1007/s00371-010-0527-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000004
OA Green Published
DA 2024-07-18
ER

PT J
AU Chang, JA
   Yang, XS
   Pan, JJ
   Li, WX
   Zhang, JJ
AF Chang, Jian
   Yang, Xiaosong
   Pan, Jun J.
   Li, Wenxi
   Zhang, Jian J.
TI A fast hybrid computation model for rectum deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Colorectal cancer; Virtual surgery; Deformation; Simulation
ID SURGERY; SIMULATION; FRAMEWORK; TISSUES
AB It is a challenging task to realistically reproduce the complex deformation of soft bio-tissues in a surgical operation, especially when large deformations and movements exist. A hybrid model which we call the beads-on-string model is presented to handle the deformation and collision of the rectum in a virtual surgery simulation system. Specially tailored for this purpose, our model takes multiple layers to capture the dynamics of the rectum in an efficient manner. Inspired by the shape similarity between a rectum with regular bulges and a string of beads, we use a Cosserat rod model, coinciding with the centreline of the rectum, to calculate its deformation subject to external forces. We introduce rigid spheres, analogy to beads, moving along with the rod to approximate the shape of the rectum in handling collision. In addition, the beads (rigid spheres) provide a natural interlayer to map the deformation of the centreline to the associated mesh which presents detailed geometry of the rectum. Our approach is carefully crafted to achieve high computational efficiency and its multi-layer structure is designed to reproduce the physics of the deformation of the rectum.
C1 [Chang, Jian; Yang, Xiaosong; Pan, Jun J.; Li, Wenxi; Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Zhang, JJ (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
EM jchang@bournemouth.ac.uk; xyang@bournemouth.ac.uk;
   pjunjun@bournemouth.ac.uk; wli@bournemouth.ac.uk;
   jzhang@bournemouth.ac.uk
RI Pan, Junjun/A-1316-2013
OI Chang, Jian/0000-0003-4118-147X; Zhang, Jian/0000-0002-7069-5771; Yang,
   Xiaosong/0000-0003-3815-0584
CR Abate AF, 2010, IEEE T INF TECHNOL B, V14, P326, DOI 10.1109/TITB.2010.2043678
   AGUIAR E, 2010, ACM T GRAPHIC, V29, P1
   [Anonymous], 1996, Nonlinear problems in elasticity
   BARAN I, 2007, ACM SIGGRAPH 2007 PA, DOI DOI 10.1145/1276377.1276467
   Bathe K.J., 1996, Finite Element Procedures
   BERGOU M, 2008, SIGGRAPH 2008
   Bergou M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778853
   Bertails F, 2006, ACM T GRAPHIC, V25, P1180, DOI 10.1145/1141911.1142012
   Bro-Nielsen M, 1998, P IEEE, V86, P490, DOI 10.1109/5.662874
   Chang J, 2004, COMPUT ANIMAT VIRT W, V15, P211, DOI 10.1002/cav.23
   CHANG J, 2010, 3DAHSS
   Chang J, 2007, COMPUT ANIMAT VIRT W, V18, P429, DOI 10.1002/cav.197
   Chang J, 2009, INT J IMAGE GRAPH, V9, P591, DOI 10.1142/S0219467809003599
   Chang J, 2009, LECT NOTES COMPUT SC, V5903, P51, DOI 10.1007/978-3-642-10470-1_5
   Comas O, 2008, LECT NOTES COMPUT SC, V5104, P28, DOI 10.1007/978-3-540-70521-5_4
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Delingette H, 2005, COMMUN ACM, V48, P31, DOI 10.1145/1042091.1042116
   Drumwright E, 2008, IEEE T VIS COMPUT GR, V14, P231, DOI 10.1109/TVCG.2007.70416
   France L, 2005, MED IMAGE ANAL, V9, P123, DOI 10.1016/j.media.2004.11.006
   Hoeg H. D., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1599, DOI 10.1109/ROBOT.2000.844825
   Hong M, 2006, IEEE COMPUT GRAPH, V26, P83, DOI 10.1109/MCG.2006.104
   Huang PF, 2006, LECT NOTES COMPUT SC, V4072, P67
   James DL, 2004, ACM T GRAPHIC, V23, P393, DOI 10.1145/1015706.1015735
   James DL, 2003, ACM T GRAPHIC, V22, P47, DOI 10.1145/588272.588278
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Ju T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409075
   Kim J, 2007, INT J MED ROBOT COMP, V3, P149, DOI 10.1002/rcs.140
   Kim Y, 2008, COMPUT ANIMAT VIRT W, V19, P515, DOI 10.1002/cav.237
   LI H, 2010, COMPUT GRAPH FORUM, DOI DOI 10.1111/J.14678659.2010.01647.X
   LIPMAN Y, 2007, ACM INT C P SERIES, V257, P117
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Marchal M, 2008, LECT NOTES COMPUT SC, V5104, P176, DOI 10.1007/978-3-540-70521-5_19
   Molinas CR, 2008, GYNECOL SURG, V5, P281, DOI 10.1007/s10397-008-0391-0
   Mollemans W, 2004, LECT NOTES COMPUT SC, V3217, P371
   MULLER M, 2004, SCA 04, P141
   Raghupathi L, 2004, IEEE T VIS COMPUT GR, V10, P708, DOI 10.1109/TVCG.2004.36
   Rosen J, 2008, J BIOMECH ENG-T ASME, V130, DOI 10.1115/1.2898712
   Spillmann J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P63
   Teran J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P68
   Van Den Bergen G., 2003, Collision Detection in Interactive 3D Environments
   Wu W, 2004, COMPUT ANIMAT VIRT W, V15, P219, DOI 10.1002/cav.24
   2009, LATEST UK CANC INCID
NR 43
TC 5
Z9 5
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 97
EP 107
DI 10.1007/s00371-010-0533-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, BL
   Lambrou, T
   Offiah, A
   Fry, M
   Todd-Pokropek, A
AF Chen, Bailiang
   Lambrou, Tryphon
   Offiah, Amaka
   Fry, Martin
   Todd-Pokropek, Andrew
TI Combined MR imaging towards subject-specific knee contact analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3DAH Conference
CY MAY 23-24, 2010
CL Chania, GREECE
DE Combined MR imaging; Dynamic MRI; Knee; Contact analysis
ID LIVING KNEE; PRESSURE DISTRIBUTION; WEIGHT-BEARING; MOVEMENT;
   KINEMATICS; FLEXION; FEASIBILITY; FEMUR; AREA
AB A combined magnetic resonance (MR) imaging method has been proposed to investigate individual's knee functionality quantitatively under weight-bearing condition. High resolution MR data were acquired first to reconstruct the subject-specific anatomical model. A dynamic MR acquisition was obtained afterwards to record the motion of knee joint. A tri-rigid registration was applied to retrieve the knee joint motion, leading to a 12 degree-of-freedom (DoF) knee functional model. Using this model, the tibiofemoral contact mechanism was studied and analysed in both 2D and 3D. A mathematical definition of contact points of cartilage surfaces is given by modelling these surfaces as manifolds. It is believed that such subject-specific motion of contact points on cartilage surfaces of femur and two tibia plateaus can provide valuable insights for clinical applications such as knee replacement surgery.
C1 [Chen, Bailiang; Lambrou, Tryphon; Fry, Martin; Todd-Pokropek, Andrew] UCL, Dept Med Phys & Bioengn, London WC1E 6BT, England.
   [Lambrou, Tryphon] UCL, Inst Nucl Med, London WC1E 6BT, England.
   [Offiah, Amaka] Sheffield Childrens NHS Fdn Trust, Acad Unit Child Hlth, Western Bank, Sheffield S10 2TH, S Yorkshire, England.
C3 University of London; University College London; University of London;
   University College London; Sheffield Children's NHS Foundation Trust
RP Chen, BL (corresponding author), UCL, Dept Med Phys & Bioengn, Gower St, London WC1E 6BT, England.
EM bchen@medphys.ucl.ac.uk
RI CHEN, Bailiang/U-9572-2019; Offiah, Amaka/G-2911-2010; Lambrou,
   Tryphon/AAH-7255-2021; Lambrou, Tryphon/AAH-7260-2021
OI CHEN, Bailiang/0000-0003-1038-7846; Offiah, Amaka/0000-0001-8991-5036;
   Lambrou, Tryphon/0000-0003-2899-5815; 
CR AHMED AM, 1983, J BIOMECH ENG-T ASME, V105, P216, DOI 10.1115/1.3138409
   Arndt A, 2004, FOOT ANKLE INT, V25, P357, DOI 10.1177/107110070402500514
   Besier TF, 2005, J ORTHOP RES, V23, P345, DOI 10.1016/j.orthres.2004.08.003
   Cohen ZA, 1999, OSTEOARTHR CARTILAGE, V7, P95, DOI 10.1053/joca.1998.0165
   DeFrate LE, 2004, J BIOMECH, V37, P1499, DOI 10.1016/j.jbiomech.2004.01.012
   Draper CE, 2008, J MAGN RESON IMAGING, V28, P158, DOI 10.1002/jmri.21413
   Freeman MAR, 2005, J BIOMECH, V38, P197, DOI 10.1016/j.jbiomech.2004.02.006
   FUKUBAYASHI T, 1980, ACTA ORTHOP SCAND, V51, P871, DOI 10.3109/17453678008990887
   Hill PF, 2000, J BONE JOINT SURG BR, V82B, P1196, DOI 10.1302/0301-620X.82B8.10716
   Hosseini A, 2010, OSTEOARTHR CARTILAGE, V18, P909, DOI 10.1016/j.joca.2010.04.011
   Iwaki H, 2000, J BONE JOINT SURG BR, V82B, P1189, DOI 10.1302/0301-620X.82B8.10717
   Johal P, 2005, J BIOMECH, V38, P269, DOI 10.1016/j.jbiomech.2004.02.008
   Karrholm J, 2000, J BONE JOINT SURG BR, V82B, P1201, DOI 10.1302/0301-620X.82B8.10715
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   KLINE M, 1936, AM MATH MON, V43, P555
   Li G, 1999, J BIOMECH ENG-T ASME, V121, P657, DOI 10.1115/1.2800871
   Li GA, 2004, J BIOMECH ENG-T ASME, V126, P314, DOI 10.1115/1.1691448
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   *MINT MED, 2009, MITK 3M3
   Moro-Oka TA, 2008, J ORTHOP RES, V26, P428, DOI 10.1002/jor.20488
   *MRC UK, 2010, ETH RES GUID
   Nakagawa S, 2000, J BONE JOINT SURG BR, V82B, P1199, DOI 10.1302/0301-620X.82B8.10718
   PALASTANGA N, 2006, ANATOMY HUMAN MOVEME, P356
   Pinskerova V, 2004, J BONE JOINT SURG BR, V86B, P925, DOI 10.1302/0301-620x.86b6.14589
   Struik D.J., 1961, LECT CLASSICAL DIFFE, P1
   *VIS COMP LAB STI, 2009, MESHLAB
   Williams A, 2004, KNEE, V11, P81, DOI 10.1016/j.knee.2003.12.004
   Wilson DR, 2009, MED CLIN N AM, V93, P67, DOI 10.1016/j.mcna.2008.08.004
   Wretenberg P, 2002, CLIN BIOMECH, V17, P477, DOI 10.1016/S0268-0033(02)00036-0
   ZUPPINGER H, 1904, ANAT EMBRYOL, V25, P701
NR 30
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2011
VL 27
IS 2
SI SI
BP 121
EP 128
DI 10.1007/s00371-010-0535-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 722QR
UT WOS:000287449900005
DA 2024-07-18
ER

PT J
AU Bernabei, D
   Ganovelli, F
   Pietroni, N
   Cignoni, P
   Pattanaik, S
   Scopigno, R
AF Bernabei, D.
   Ganovelli, F.
   Pietroni, N.
   Cignoni, P.
   Pattanaik, S.
   Scopigno, R.
TI Real-time single scattering inside inhomogeneous materials
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Subsurface scattering; GPU computing
ID MODEL
AB In this paper we propose a novel technique to perform real-time rendering of translucent inhomogeneous materials, one of the most well-known problems of computer graphics. The developed technique is based on an adaptive volumetric point sampling, done in a preprocessing stage, which associates to each sample the optical depth for a predefined set of directions. This information is then used by a rendering algorithm that combines the object's surface rasterization with a ray tracing algorithm, implemented on the graphics processor, to compose the final image. This approach allows us to simulate light scattering phenomena for inhomogeneous isotropic materials in real time with an arbitrary number of light sources. We tested our algorithm by comparing the produced images with the result of ray tracing and showed that the technique is effective.
C1 [Bernabei, D.; Ganovelli, F.; Pietroni, N.; Cignoni, P.; Scopigno, R.] CNR, ISTI, I-56100 Pisa, Italy.
   [Bernabei, D.; Pattanaik, S.] Univ Cent Florida, Orlando, FL 32816 USA.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); State
   University System of Florida; University of Central Florida
RP Ganovelli, F (corresponding author), CNR, ISTI, I-56100 Pisa, Italy.
EM danielebernabei@gmail.com; fabio.ganovelli@gmail.com;
   pietroni@isti.cnr.it; sumant@cs.ucf.edu
RI Cignoni, Paolo/B-7192-2012; scopigno, roberto/AAH-7645-2020
OI Cignoni, Paolo/0000-0002-2686-8567; PIETRONI, NICO/0000-0002-8271-2102
FU EG [231809]; Regione Toscana initiative "START"
FX The authors wish to thank Marco Di Benedetto from the Visual Computing
   Lab of ISTI-CNR for his precious GPU tips and tricks and Matteo
   Prayer-Galletti from the Universita di Firenze for modeling the
   jellyfish shown in Fig. 8. The research leading to these results has
   received funding from the EG 7FP IP "3D-COFORM" project (2008-2012, n.
   231809) and from the Regione Toscana initiative "START".
CR BIRI V, 2006, J WSCG, V14, P65
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   CARR NA, 2003, HWWS 03, P51
   CEREZO E, 2005, VIS COMPUT
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   GEIST R, 2004, EUR S REND NORRK SWE, P355
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   HANRAHAN P, 1993, SIGGRAPH 93, P165, DOI DOI 10.1145/166117.166139
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Harris MJ, 2001, COMPUT GRAPH FORUM, V20, pC76, DOI 10.1111/1467-8659.00500
   HEGEMAN K, 2005, I3D 05, P117
   Hou QM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360618
   JENSEN H.W., 1998, SIGGRAPH 98 Conference Proceedings, Annual Conference Series, P311
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   JENSEN HW, 2002, SIGGRAPH 02 P 29 ANN, P576
   JIMENEZ JR, 2005, SCCG 05, P211
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   KNISS J, 2002, VIS C IEEE
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Mertens T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P130
   Mertens T, 2005, COMPUT GRAPH FORUM, V24, P41, DOI 10.1111/j.1467-8659.2005.00827.x
   NARASIMHAN SG, 2003, CVPR 03, P665
   PREMOZE S, 2003, EUR S REND 2003, P1
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   RUSHMEIER HE, 1988, THESIS CORNELL U ITH
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Stam J, 1995, SPRING COMP SCI, P41
   STAM J, 1994, GRAPH INTER, P51
   Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309
   WALTER B, 2009, ACM T GRAPH, V28
   WANG R, 2005, SIGGRAPH 05, P1202
   WANG Y, 2010, COMPUT GRAPH FORUM, V29, P1
   Zhou K, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P116, DOI 10.1109/PG.2007.48
NR 37
TC 2
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 583
EP 593
DI 10.1007/s00371-010-0449-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800019
DA 2024-07-18
ER

PT J
AU Han, S
   Nijdam, NA
   Schmid, J
   Kim, J
   Magnenat-Thalmann, N
AF Han, Seunghyun
   Nijdam, Niels A.
   Schmid, Jerome
   Kim, Jinman
   Magnenat-Thalmann, Nadia
TI Collaborative telemedicine for interactive multiuser segmentation of
   volumetric medical images
SO VISUAL COMPUTER
LA English
DT Article
DE Telemedicine; Teleradiology; Multi-user segmentation
AB Telemedicine has evolved rapidly in recent years to enable unprecedented access to digital medical data, such as with networked image distribution/sharing and online (distant) collaborative diagnosis, largely due to the advances in telecommunication and multimedia technologies. However, interactive collaboration systems which control editing of an object among multiple users are often limited to a simple "locking" mechanism based on a conventional client/server architecture, where only one user edits the object which is located in a specific server, while all other users become viewers. Such systems fail to provide the needs of a modern day telemedicine applications that demand simultaneous editing of the medical data distributed in diverse local sites. In this study, we introduce a novel system for telemedicine applications, with its application to an interactive segmentation of volumetric medical images. We innovate by proposing a collaborative mechanism with a scalable data sharing architecture which makes users interactively edit on a single shared image scattered in local sites, thus enabling collaborative editing for, e.g., collaborative diagnosis, teaching, and training. We demonstrate our collaborative telemedicine mechanism with a prototype image editing system developed and evaluated with a user case study. Our result suggests that the ability for collaborative editing in a telemedicine context can be of great benefit and hold promising potential for further research.
C1 [Han, Seunghyun; Nijdam, Niels A.; Schmid, Jerome; Kim, Jinman; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, Geneva, Switzerland.
C3 University of Geneva
RP Han, S (corresponding author), Univ Geneva, MIRALab, Geneva, Switzerland.
EM han@miralab.ch; nijdam@miralab.ch; schmid@miralab.ch;
   jinman.kim@miralab.ch; thalmann@miralab.ch
RI Kim, Jin Man/HJO-8987-2023; Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960; Schmid, Jerome/0000-0003-2464-8971;
   kim, jinman/0000-0001-5960-1060; Nijdam, Niels/0000-0002-0172-6319
FU European Union [NoE-IST-2006-038419, MRTN-CT-2006-035763]
FX We would like to thank our partners, the University Hospital of Geneva
   and the University College London for providing us with the MRI
   datasets. This work is supported by the InterMedia (NoE-IST-2006-038419)
   and the 3D Anatomical Human (MRTN-CT-2006-035763) projects funded by the
   European Union.
CR CONSTANTINESCU L, 2007, IEEE P ENG MED BIOL, P2799
   Costa MJ, 2007, LECT NOTES COMPUT SC, V4791, P252
   Delaney D, 2006, PRESENCE-TELEOP VIRT, V15, P465, DOI 10.1162/pres.15.4.465
   Delingette H, 1999, INT J COMPUT VISION, V32, P111, DOI 10.1023/A:1008157432188
   Dollimore J., 2005, DISTRIBUTED SYSTEMS, V4th
   Eugster PT, 2003, ACM COMPUT SURV, V35, P114, DOI 10.1145/857076.857078
   GILLES B, 2006, LNCS, P289
   HAN S, 2007, P 3 INT C COLL TECHN
   Heimann T, 2007, LECT NOTES COMPUT SC, V4584, P1
   Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004
   KAINMULLER D, 2009, P IEEE ENG MED BIOL, P6345
   Lee D, 2007, PRESENCE-VIRTUAL AUG, V16, P125, DOI 10.1162/pres.16.2.125
   LEWIS JR, 1995, INT J HUM-COMPUT INT, V7, P57, DOI 10.1080/10447319509526110
   Marescaux J, 2001, NATURE, V413, P379, DOI 10.1038/35096636
   Morillo P, 2005, IEEE T PARALL DISTR, V16, P637, DOI 10.1109/TPDS.2005.83
   Olabarriaga SD, 2001, MED IMAGE ANAL, V5, P127, DOI 10.1016/S1361-8415(00)00041-4
   Park S, 2008, COMPUT METH PROG BIO, V89, P248, DOI 10.1016/j.cmpb.2007.11.012
   Rialle V, 2003, COMPUT METH PROG BIO, V72, P257, DOI 10.1016/S0169-2607(02)00161-X
   SCHMID J, 2008, LNCS 1, P119
   Schmid J, 2009, LECT NOTES COMPUT SC, V5903, P13, DOI 10.1007/978-3-642-10470-1_2
   Schroeder W., 2003, The ITK software guide
   Simmross-Wattenberg F, 2005, J BIOMED INFORM, V38, P431, DOI 10.1016/j.jbi.2005.03.001
   Singhal S., 1999, Networked Virtual Environments
   Snel JG, 2002, IEEE T MED IMAGING, V21, P888, DOI 10.1109/TMI.2002.803127
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   WOOTTON R, 2006, INTRO TELEMEDICINE, P2
   Zhang JG, 2000, IEEE T INF TECHNOL B, V4, P178, DOI 10.1109/4233.845212
NR 27
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 639
EP 648
DI 10.1007/s00371-010-0445-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800024
DA 2024-07-18
ER

PT J
AU Guitián, JAI
   Gobbetti, E
   Marton, F
AF Iglesias Guitian, Jose Antonio
   Gobbetti, Enrico
   Marton, Fabio
TI View-dependent exploration of massive volumetric models on large-scale
   light field displays
SO VISUAL COMPUTER
LA English
DT Article
DE Volume rendering; High-field displays; Illustrative visualization;
   Massive datasets; Virtual reality; View-dependent visualization; 3D
   interaction
ID ENHANCEMENT; LEVEL
AB We report on a light-field display based virtual environment enabling multiple naked-eye users to perceive detailed multi-gigavoxel volumetric models as floating in space, responsive to their actions, and delivering different information in different areas of the workspace. Our contributions include a set of specialized interactive illustrative techniques able to provide different contextual information in different areas of the display, as well as an out-of-core CUDA-based raycasting engine with a number of improvements over current GPU volume raycasters. The possibilities of the system are demonstrated by the multi-user interactive exploration of 64 Gvoxel data sets on a 35 Mpixel light field display driven by a cluster of PCs.
C1 [Iglesias Guitian, Jose Antonio; Gobbetti, Enrico; Marton, Fabio] Ctr Adv Studies Res & Dev Sardinia CRS4, Visual Comp Grp, Pula, Italy.
   [Gobbetti, Enrico] Ctr Adv Studies Res & Dev Sardinia CRS4, Adv Comp & Commun Program, Pula, Italy.
RP Guitián, JAI (corresponding author), Ctr Adv Studies Res & Dev Sardinia CRS4, Visual Comp Grp, Pula, Italy.
EM gobbetti@crs4.it
RI Marton, Fabio/KBC-4179-2024; Gobbetti, Enrico/O-2188-2015; Guitian, Jose
   A. Iglesias/A-9718-2017
OI Gobbetti, Enrico/0000-0003-0831-2458; Guitian, Jose A.
   Iglesias/0000-0002-0817-1010; Marton, Fabio/0000-0001-8611-1921
FU EU [MRTN-CT-2006-035763]
FX This work is partially supported by the EU Marie Curie Program under the
   3DANATOMICALHUMAN project (MRTN-CT-2006-035763).
CR Agus M, 2008, COMPUT GRAPH FORUM, V27, P231, DOI 10.1111/j.1467-8659.2008.01120.x
   Agus M, 2009, VISUAL COMPUT, V25, P883, DOI 10.1007/s00371-009-0311-y
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1559, DOI 10.1109/TVCG.2006.96
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Cignoni P, 2005, COMPUT GRAPH-UK, V29, P125, DOI 10.1016/j.cag.2004.11.012
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   DAYAL A, 2005, RENDERING TECHNIQUES, P265
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Havran V., 1998, P SPRING C COMPUTER, P130
   HERZOG R, 2010, P I3D
   Huang RZ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P355
   Jones A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276427
   KRAUS M, 2008, IEEE EG S VOL POINTB, P97
   Ljung P., 2006, P VOL GRAPH, P39
   LUO Y, 2009, 2 3D PHYS HUM WORKSH
   Niski K, 2007, IEEE T VIS COMPUT GR, V13, P1352, DOI 10.1109/TVCG.2007.70587
   Popov S, 2007, COMPUT GRAPH FORUM, V26, P415, DOI 10.1111/j.1467-8659.2007.01064.x
   ROPINSKI T, 2009, EUROGRAPHICS TUTORIA, V4
   Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015
   Segovia B., 2006, P 21 ACM SIGGRAPHEUR, P53, DOI DOI 10.1145/1283900.1283909
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   VOLLRATH JE, 2006, P VOL GRAPH, P55
   Yang L, 2008, COMPUT GRAPH FORUM, V27, P1183, DOI 10.1111/j.1467-8659.2008.01256.x
NR 23
TC 21
Z9 21
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1037
EP 1047
DI 10.1007/s00371-010-0453-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800063
DA 2024-07-18
ER

PT J
AU Chang, J
   Zhang, JJ
   Zia, RH
AF Chang, Jian
   Zhang, Jian J.
   Zia, Rehan
TI Modelling deformations in car crash animation
SO VISUAL COMPUTER
LA English
DT Article
DE Crash; Animation; Deformation; Physically based modelling
ID CONTACT
AB In this paper, we present a prototype of a deformation engine to efficiently model and render the damaged structure of vehicles in crash scenarios. We introduce a novel system architecture to accelerate the computation, which is traditionally an extremely expensive task. We alter a rigid body simulator to predict trajectories of cars during a collision and formulate a correction procedure to estimate the deformations of the collapsed car structures within the contact area. Non-linear deformations are solved based on the principle of energy conservation. Large plastic deformations resulting from collisions are modelled as a weighted combination of deformation examples of beams which can be produced using classical mechanics.
C1 [Chang, Jian; Zhang, Jian J.; Zia, Rehan] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Zhang, JJ (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
EM jchang@bournemouth.ac.uk; jzhang@bournemouth.ac.uk; rehanzia@gmail.com
OI Zhang, Jian/0000-0002-7069-5771; Chang, Jian/0000-0003-4118-147X
CR Ambrósio JAC, 2003, INT J CRASHWORTHINES, V8, P73
   [Anonymous], PHYS BASED MODELING
   [Anonymous], 2005, ACMEUROGRAPHICS S CO
   Baraff David, 1992, Dynamic Simulation of Non-penetrating Rigid Bodies
   BARRENECHEA AM, 1984, SUR, P21
   BELYTSCHKO T, 1988, P 7 INT C VEH STRUCT, P93
   BERGOU M, 2008, SIGGRAPH 08, P1
   Bertails F, 2006, ACM T GRAPHIC, V25, P1180, DOI 10.1145/1141911.1142012
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Chang J, 2004, COMPUT ANIMAT VIRT W, V15, P211, DOI 10.1002/cav.23
   Chang J, 2007, COMPUT ANIMAT VIRT W, V18, P429, DOI 10.1002/cav.197
   Coquillart S., 1990, J. Computer Graphics, V24, P187, DOI DOI 10.1145/97880.97900
   *GAMESINVESTOR, 2007, MON GAMESINVESTOR, V2
   Genta G., 1997, MOTOR VEHICLE DYNAMI
   GLADDEN JR, 2005, PHYS REV LETT, V94, P1
   Gregoire M., 2006, P 2006 ACM S SOLID P, P95
   GRZESIKIEWICZ G, 2007, EVU ANN M 2007 EUR A
   Happian-Smith J., 2002, An Introduction to Modern Vehicle Design
   Irving G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239464
   Jones N., 1989, Structural Impact
   Kuschfeldt S, 1998, IEEE COMPUT GRAPH, V18, P60, DOI 10.1109/38.689666
   Miller G. S. P., 1988, Computer Graphics, V22, P169, DOI 10.1145/378456.378508
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   MULLER M, 2004, SCA 04, P141
   Muller M., 2002, P 2002 ACM SIGGRAPHE, P49, DOI DOI 10.1145/545261.545269
   *NCAC, 2009, NCAC MOD
   Nilsson K, 2002, MODEL SIMUL MATER SC, V10, P569, DOI 10.1088/0965-0393/10/5/307
   Pai DK, 2002, COMPUT GRAPH FORUM, V21, P347, DOI 10.1111/1467-8659.00594
   PAULY M, 2005, SIGGRAPH 05, P957
   Rodrigues T., 2005, Proceedings of the 2005 ACM SIGCHI International Conference on Advances in computer entertainment technology - ACE'05, P330, DOI DOI 10.1145/1178477.1178539
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   SMITH L, 2006, A WARD REAL PHYS DON
   Sousa L, 2008, MULTIBODY SYST DYN, V19, P133, DOI 10.1007/s11044-007-9093-z
   Spillmann J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P63
   TERZOPOULOS D, 1988, SIGGRAPH 88, P269
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Vaughn DG, 2006, EUR J MECH A-SOLID, V25, P1, DOI 10.1016/j.euromechsol.2005.09.003
   Vaughn DG, 2005, J APPL MECH-T ASME, V72, P139, DOI 10.1115/1.1825437
   WIEDERMANN J, 2002, 500 3D OBJECTS
   WOJTAN C, 2008, SIGGRAPH 08, P1
   YORK R, 1999, 1999010104 SAE, P1
NR 41
TC 1
Z9 1
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1063
EP 1072
DI 10.1007/s00371-009-0386-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000003
DA 2024-07-18
ER

PT J
AU Lee, HY
   Hong, JM
   Kim, CH
AF Lee, Ho-Young
   Hong, Jeong-Mo
   Kim, Chang-Hun
TI Interchangeable SPH and level set method in multiphase fluids
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Fluid simulation; Physically based modeling; Bubbles; SPH; Level set;
   Grid-based simulation; Multiphase fluids
ID ANIMATION; BUBBLES; WATER
AB Subgrid-scale fluid is difficult to represent realistically in a grid-based fluid simulation. We show how to describe such small-scale details effectively, even on a coarse grid, by using escaped particles. The simulation of these particles with SPH (smooth particle hydrodynamics) allows the illustration of dynamic and realistic animation of fluids. Particles modeled by SPH have a force which leads them to merge if they are within a certain range. This reduces the accuracy of a simulation. Consequently, aggregated particles which form volumes large enough to be described by the level set method will be simulated inefficiently by particles. We address this problem with a new method in which details too small for the grid are represented by particles, while the level set method with a grid is used to describe merged particles on the grid.
C1 [Lee, Ho-Young; Kim, Chang-Hun] Korea Univ, Dept Comp Sci, Seoul, South Korea.
   [Hong, Jeong-Mo] Dongguk Univ, Dept Comp Sci, Seoul, South Korea.
C3 Korea University; Dongguk University
RP Kim, CH (corresponding author), Korea Univ, Dept Comp Sci, Seoul, South Korea.
EM flymist@korea.ac.kr; jmhong@dongguk.edu; chkim@korea.ac.kr
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   [Anonymous], EUROGRAPHICS
   Becker M., 2006, P 2007 ACM SIGGRAPH, DOI [10.1109/PCTHEALTH.2006.361656, DOI 10.1109/PCTHEALTH.2006.361656]
   Chorin A. J., 1997, Journal of Computational Physics, V135, P118, DOI 10.1006/jcph.1997.5716
   Cleary PW, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239548, 10.1145/1276377.1276499]
   DESBRUN M, 1996, 6 EUR WORKSH COMP AN, P61
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   HONG JM, 2008, ACM T GRAPHIC, V48, P1
   KIM B, 2005, EUR WORKSH NAT PHEN, V1, P2
   Kim J., 2006, Proc ACM SIGGRAPH/Eurograph Symp Comp Anim, SCA '06, P335
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Magnaudet J, 2000, ANNU REV FLUID MECH, V32, P659, DOI 10.1146/annurev.fluid.32.1.659
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Muller M., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P154
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
NR 22
TC 14
Z9 21
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 713
EP 718
DI 10.1007/s00371-009-0339-z
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300037
DA 2024-07-18
ER

PT J
AU Yang, YZ
   Wei, YC
   Liu, CX
   Peng, QS
   Matsushita, Y
AF Yang, Yingzhen
   Wei, Yichen
   Liu, Chunxiao
   Peng, Qunsheng
   Matsushita, Yasuyuki
TI An improved belief propagation method for dynamic collage
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Dynamic collage; Photo collage; Belief propagation; Factor graph;
   Optimization
ID LEVEL
AB This paper presents a new photo browsing technique, dynamic collage. Although previous photo collage techniques have innate advantages for viewing several photos collectively, they only focus on a static two-dimensional arrangement of photos so that the scalability is limited. In dynamic collage, new photos are incrementally inserted into the collage one by one while the old photos are removed accordingly, the positions of photos in the canvas are updated with a local and incremental manner to form a new layout so as to maximize visibility of all the important information embedded in the current collage meanwhile maintaining the visual continuity of two successive collages. To achieve this goal, a carefully designed optimization method based on belief propagation is employed. Unlike most traditional applications of belief propagation on pairwise MRF, we apply belief propagation on factor graph to optimize terms which cannot be represented by pairwise restricted belief propagation. We propose a novel approximate method to reduce the computation complexity, and this approximate method suggests a direction for using belief propagation on factor graph to optimize high order potential functions similar to ours.
C1 [Yang, Yingzhen; Liu, Chunxiao; Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Yang, Yingzhen; Wei, Yichen; Matsushita, Yasuyuki] Microsoft Res Asia, Visual Comp Grp, Beijing, Peoples R China.
C3 Zhejiang University; Microsoft; Microsoft Research Asia
RP Peng, QS (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
EM yzyang@microsoft.com; yichenw@microsoft.com; liuchunxiao@cad.zju.edu.cn;
   peng@cad.zju.edu.cn; yasumat@microsoft.com
RI Yang, Yingzhen/AAU-6048-2020
OI Matsushita, Yasuyuki/0000-0002-1935-4752
CR [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P CVPR 2007 2007 IEE
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   Chen J., 2006, P HUMAN LANGUAGE TEC, P25
   Christel M.G., 2002, ACM Multimedia, P561
   Compagno L., 2001, FAO Species Catalogue for Fishery Purposes, V2. Bullhead, mackerel and carpet sharks
   Diakopoulos N., 2005, P UIST, P183
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Geigel J, 2003, IEEE MULTIMEDIA, V10, P16, DOI 10.1109/MMUL.2003.1237547
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Wang JL, 2006, CHINESE J ASTRON AST, V6, P354, DOI 10.1088/1009-9271/6/3/11
   Liu T., 2007, Proc. International Conference on Advanced Intelligent Mechatronics, P1
   MEI T, 2005, P ACM MULT SING NOV, P439
   Rother C, 2005, PROC CVPR IEEE, P589
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Tang KL, 2005, PROC CVPR IEEE, P132
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang T, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1479
NR 20
TC 11
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 431
EP 439
DI 10.1007/s00371-009-0346-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300007
DA 2024-07-18
ER

PT J
AU Qin, HX
   Yang, J
   Zhu, YM
AF Qin, Hongxing
   Yang, Jie
   Zhu, Yuemin
TI Nonuniform bilateral filtering for point sets and surface attributes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Point set denoising; Surface attributes filtering; Bilateral filtering;
   Nonuniform sampling
ID MEAN SHIFT
AB With the proliferation of three-dimensional (3D) scanning tools and the popularity of point sets in geometry processing and rendering, there is a need for developing smoothing techniques for the point sets and surface attributes defined at these points. In this paper, we present a nonuniform bilateral filtering (NBF) method for point sets and surface attributes based on local geometry feature. In order to adapt the algorithm to irregular sampling, local sampling density is introduced to traditional bilateral filtering, and a global approach for volume preservation is proposed. Experiments show that our approach is stable, effective and easy to use.
C1 [Qin, Hongxing; Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
   [Zhu, Yuemin] Univ Lyon 1, CNRS, CREATIS, INSA Lyon,Inserm Unit 630,UMR 5220, F-69622 Villeurbanne, France.
C3 Shanghai Jiao Tong University; Universite Claude Bernard Lyon 1;
   Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); CNRS - Institute for
   Engineering & Systems Sciences (INSIS); Institut National de la Sante et
   de la Recherche Medicale (Inserm)
RP Qin, HX (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
EM redstar_q@sjtu.edu.cn; jieyang@sjtu.edu.cn;
   yue-min.zhu@creatis.insa-lyon.fr
RI Zhu, Yuemin/K-7292-2014; Yang, Jie/JCD-9867-2023
OI Zhu, Yuemin/0000-0001-6814-1449; 
CR Alexander M, 2001, INTERNETWEEK, P21
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   [Anonymous], 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004
   Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   CHOUDHURY P, 2003, P EUR S REND, P186
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Duguet F, 2004, ROBUST HIGHER ORDER
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   FLEISHMAN S, 2005, P SIGGRAPH 05, P544
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Hu GF, 2006, VISUAL COMPUT, V22, P147, DOI 10.1007/s00371-006-0372-0
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Lange C, 2005, COMPUT AIDED GEOM D, V22, P680, DOI 10.1016/j.cagd.2005.06.010
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451
   Pauly Mark., 2002, Multiresolution modeling of pointsampled geometry
   SCHALL O, 2005, EUR S POINT BAS GRAP, P71, DOI DOI 10.2312/SPBG/SPBG05/071-077
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   YANG YL, 2006, EUR S GEOM PROC
NR 23
TC 5
Z9 7
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1067
EP 1074
DI 10.1007/s00371-007-0206-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500007
DA 2024-07-18
ER

PT J
AU Charneau, S
   Aveneau, L
   Fuchs, L
AF Charneau, Sylvain
   Aveneau, Lilian
   Fuchs, Laurent
TI Exact, robust and efficient full visibility computation in Plucker space
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE plucker space; polygonto-polygon visibility; exact solutions; visibility
   precomputation; BSP-tree for CSG
AB We present a set of new techniques to compute an exact polygon-to-polygon visibility in Plucker space. The contributions are based on the definition of the minimal representation of lines stabbing two convex polygons. The new algorithms are designed to indicate useless computations, which results in more compact visibility data, faster to exploit, and in a reduced computation time. We also define a simple robust and exact solution to handle degeneracies, where previous methods proposed aggressive solutions.
C1 Univ Poitiers, SIC Lab, F-86962 Futuroscope, France.
C3 Universite de Poitiers
RP Charneau, S (corresponding author), Univ Poitiers, SIC Lab, EA 4103,Bat SP2MI,Teleport 2,BP 30179, F-86962 Futuroscope, France.
EM chameau@sic.univ-poitiers.fr; aveneau@sic.univ-poitiers.fr;
   fuchs@sic.univ-poitiers.fr
RI Maroteaux, Luc/H-4585-2019
OI Maroteaux, Luc/0000-0002-9499-8603; Aveneau, Lilian/0000-0003-3129-1149
CR [Anonymous], 1998, Algorithmic geometry
   BITTNER J, 2002, THESIS CZECH TU PRAG
   CHARNEAU S, PLUCKER SPACE POLYGO
   *FREE SOFTW FDN, GMP GNU MULT PREC AR
   Haumont Denis., 2005, Proceedings of the Sixteenth Eurographics Conference on Rendering Techniques, P211
   Laine S, 2005, ACM T GRAPHIC, V24, P1156, DOI 10.1145/1073204.1073327
   Mora F, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P191
   MORA F, 2005, P WINT SCH COM GRAPH, P87
   NAYLOR B, 1990, P ACM SIGGRAPH, P115
   Nirenstein S., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P191
   NIRENSTEIN S, 2003, THESIS U CAPE TOWN
   Plucker Julius, 1865, PHILOS T ROYAL SOC L, V155, P3
   PU F, 1999, INT WORKSH ALG ENG E, V32, P94
   Sommerville D.M.L.Y., 1959, Analytical geometry of three dimensions
   Wald Ingo., 2004, Realtime Ray Tracing and Interactive Global Illumination
NR 15
TC 5
Z9 11
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 773
EP 782
DI 10.1007/s00371-007-0129-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600017
DA 2024-07-18
ER

PT J
AU Passalis, G
   Theoharis, T
   Kakadiaris, IA
AF Passalis, Georgios
   Theoharis, Theoharis
   Kakadiaris, Ioannis A.
TI PTK: A novel depth buffer-based shape descriptor for three-dimensional
   object retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE depth buffer; object retrieval; symmetry
AB The increase in availability and use of digital three-dimensional (3D) synthetic or scanned objects, makes the availability of basic database operations, such as retrieval, necessary. Retrieval methods are based on the extraction of a compact shape descriptor; the challenge is to design a shape descriptor that describes the original object in sufficient detail to make accurate 3D object retrieval possible. Building on previous work, this paper proposes a novel depth buffer-based shape descriptor (called PTK) that encompasses symmetry, eigenvalue-related weighting and an object thickness related measure to provide an accuracy surpassing previous state-of-the-art methods. An evaluation of the novel method's parameters and a direct comparison to other approaches are carried out using publicly available and widely used databases.
C1 Univ Athens, Comp Graph Grp, Dept Informat & Telecommun, Ilisia 15784, Greece.
   Univ Houston, Computat Biomed Lab, Dept Comp Sci, Houston, TX 77204 USA.
C3 National & Kapodistrian University of Athens; University of Houston
   System; University of Houston
RP Passalis, G (corresponding author), Univ Athens, Comp Graph Grp, Dept Informat & Telecommun, Ilisia 15784, Greece.
EM passalis@di.uoa.gr; theotheo@di.uoa.gr; ikakadia@central.uh.edu
RI Theoharis, Theoharis/AAN-2555-2020
OI Kakadiaris, Ioannis/0000-0002-0591-1079
CR Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207
   [Anonymous], 2004, P 6 ACM SIGMM INT WO
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Everitt C, 2001, INTERACTIVE ORDER IN
   FROME A, 2004, P EUR C COMP VIS
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HLAVATY T, 2003, SEM GEOM GRAPH TEACH
   Kazhdan M., 2003, S GEOMETRY PROCESSIN, P167
   KAZHDAN M, 2004, THESIS PRINCETON U
   KRUGER T, 2003, P 4 EUR WORKSH IM AN, P391
   Laga H, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P490
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papaioannou G, 2002, IEEE T PATTERN ANAL, V24, P114, DOI 10.1109/34.982888
   Passalis G, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P374
   SHILANE P, 2004, P SHAP MOD INT IEEE
   Siarry P, 1997, ACM T MATH SOFTWARE, V23, P209, DOI 10.1145/264029.264043
   Strang G., 1988, LINEAR ALGEBRA ITS A
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   VRANIC D, 2004, THESIS LEIPZIG U
   VRANIC DV, CONTENT BASED CLASSI
NR 21
TC 23
Z9 25
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2007
VL 23
IS 1
BP 5
EP 14
DI 10.1007/s00371-006-0037-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 113RB
UT WOS:000242613900002
DA 2024-07-18
ER

PT J
AU Dischler, JM
   Zara, F
AF Dischler, Jean-Michel
   Zara, Florence
TI Real-time structured texture synthesis and editing using image-mesh
   analogies
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE texture; synthesis; editing
AB We present a novel texture synthesis technique designed to reproduce at real-time frame-rates example texture images, with a special focus on patterns characterized by structural arrangements. Unlike current pixel-, patch- or texton-based schemes, that operate in image space, our approach is structural. We propose to assimilate texture images to corresponding 2D geometric meshes (called texture meshes). Our analysis mainly consists in generating automatically these meshes, while synthesis is then based on the creation of new vertex/polygon distributions matching some arrangement map. The output texture image is obtained by rasterizing the previously generated polygons using graphics hardware capabilities, which guarantees high speed performance. By operating in geometry space instead of image/pixel space, the proposed structural approach has a major advantage over current techniques: beyond pure texture reproduction, it allows us to define various tools, which allow users to further modify locally or globally and in real-time structural components of textures. By controlling the arrangement map, users can substitute new meshes in order to completely modify the structural appearance of input textures, yet maintaining a certain visual resemblance with the initial example image.
C1 ULP, CNRS, UMR 7005, LSIIT IGG, Illkirch Graffenstaden, France.
   UCBL, CNRS, UMR 5205, LIRIS, Villeurbanne, France.
C3 Universites de Strasbourg Etablissements Associes; Universite de
   Strasbourg; Centre National de la Recherche Scientifique (CNRS);
   Universite Claude Bernard Lyon 1; Institut National des Sciences
   Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS)
RP Dischler, JM (corresponding author), ULP, CNRS, UMR 7005, LSIIT IGG, Illkirch Graffenstaden, France.
EM dischler@lsiit.u-strasbg.fr; florence.zara@liris.cnrs.fr
OI Zara, Florence/0000-0002-0118-7204
CR [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], SIGGRAPH
   [Anonymous], 1992, R. woods digital image processing
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Dischler JM, 1998, COMPUT GRAPH FORUM, V17, pC87, DOI 10.1111/1467-8659.00256
   DISCHLER JM, 2002, EUROGRAPHICS 2002, P401
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Grigorescu SE, 2002, IEEE T IMAGE PROCESS, V11, P1160, DOI 10.1109/TIP.2002.804262
   Idrissa M, 2002, PATTERN RECOGN LETT, V23, P1095, DOI 10.1016/S0167-8655(02)00056-9
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre L, 2000, PROC GRAPH INTERF, P77
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Legakis J, 2001, COMP GRAPH, P309, DOI 10.1145/383259.383293
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Liu ZQ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P184, DOI 10.1109/PCCGA.2002.1167854
   Matusik W, 2005, ACM T GRAPHIC, V24, P787, DOI 10.1145/1073204.1073262
   MIYATA K, 1990, SIGGRAPH 90 P, P387
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   WEI LY, 2003, GRAPH 03, P1
   WORLEY S, 1996, SIGGRAPH 96, P291
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Zalesny A, 2005, INT J COMPUT VISION, V62, P161, DOI 10.1007/s11263-005-4640-7
   Zelinka S, 2004, ACM T GRAPHIC, V23, P930, DOI 10.1145/1027411.1027413
   Zhang W, 2002, INT J NONLINEAR SCI, V3, P295, DOI 10.1515/IJNSNS.2002.3.3-4.295
NR 29
TC 11
Z9 14
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 926
EP 935
DI 10.1007/s00371-006-0077-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000035
DA 2024-07-18
ER

PT J
AU Krüger, J
   Schneider, J
   Westermann, R
AF Krueger, Jens
   Schneider, Jens
   Westermann, Ruediger
TI Compression and rendering of iso-surfaces and point sampled geometry
SO VISUAL COMPUTER
LA English
DT Article
DE data compression; GPU programming; huge point clouds; deferred shading
AB In this paper we present a streaming compression scheme for gigantic point sets including per-point normals. This scheme extends on our previous Duodecim approach [21] in two different ways. First, we show how to use this approach for the compression and rendering of high-resolution iso-surfaces in volumetric data sets. Second, we use deferred shading of point primitives to considerably improve rendering quality. Iso-surface reconstruction is performed in a hexagonal close packing (HCP) grid, into which the initial data set is resampled. Normals are resampled from the initial domain using volumetric gradients. By incremental encoding, only slightly more than 3 bits per surface point and 5 bits per surface normal are required at high fidelity. The compressed data stream can be decoded in the graphics processing unit (GPU). Decoded point positions are saved in graphics memory, and they are then used on the GPU again to render point primitives. In this way high quality gigantic data sets can directly be rendered from their compressed representation in local GPU memory at interactive frame rates (see Fig. 1).
C1 Tech Univ Munich, Comp Graph & Visualizat Grp, D-8000 Munich, Germany.
C3 Technical University of Munich
RP Krüger, J (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, D-8000 Munich, Germany.
EM jens.krueger@in.tum.de; jens.schneider@in.tum.de; westermann@in.tum.de
OI Schneider, Jens/0000-0002-0546-2816
CR Alexander M, 2001, INTERNETWEEK, P21
   *ATI, 2004, SUP OPENGL EXT
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   BOTSCH M, 2002, P 13 EUR WORKSH REND, P53
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   CHEN B, 2001, VIS 01 P C VIS 01
   Cohen JD, 2001, IEEE VISUAL, P37, DOI 10.1109/VISUAL.2001.964491
   Conway J. H., 1987, Sphere-packings, Lattices, and Groups
   Dachsbacher C, 2003, ACM T GRAPHIC, V22, P657, DOI 10.1145/882262.882321
   Dey TK, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P155, DOI 10.1109/VISUAL.2002.1183770
   GERSHO RMG, 1992, KLUWER INT SERIES EN
   Gobbetti E, 2005, ACM T GRAPHIC, V24, P878, DOI 10.1145/1073204.1073277
   GOBBETTI E., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P113
   GROSS M, SIGGRAPH 04 COURS NO
   Grossman J. P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P181
   *IBM CORP, T221 FLAT PAN MON
   ISENBURG M, 2004, UCRLCONF201992 LLNL
   JOHNSON NW, 1966, CANADIAN J MATH, V18, P169, DOI 10.4153/CJM-1966-021-8
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   KRUGER J, 2005, P S POINT BAS GRAPH
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Levoy M., 1985, Tech. Rep. 85-022
   LEVOY M, 2000, P ACM SIGGRAPH EUR C
   Livnat Y, 1998, VISUALIZATION '98, PROCEEDINGS, P175, DOI 10.1109/VISUAL.1998.745300
   Livnat Y, 1996, IEEE T VIS COMPUT GR, V2, P73, DOI 10.1109/2945.489388
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MATTAUSCH O, 2004, CESGG 2004
   *MICR, 2004, SHAD MOD 3 SPEC
   MROZ L, 2001, P IEEE TVCG S VIS 20, P180
   Neophytou N, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P97, DOI 10.1109/SWG.2002.1226515
   Ochotta T., 2004, EUR S POINT BAS GRAP
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   REN L, 2002, P EUR 2002, P371
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   SAUPE D, 2001, P VIS MOD VIS 01, P330
   WASCHBUSCH M, 2004, EUR S POINT BAS GRAP
   WESTERMANN R, 1998, COMPUTER GRAPHICS, P291
   Zwicker M, 2004, PROC GRAPH INTERF, P247
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
   1999, CYBERWARE 3D SCANNER
NR 41
TC 1
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2006
VL 22
IS 8
BP 517
EP 530
DI 10.1007/s00371-006-0026-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 088TD
UT WOS:000240833100001
DA 2024-07-18
ER

PT J
AU Jang, J
   Wonka, P
   Ribarsky, W
   Shaw, CD
AF Jang, J
   Wonka, P
   Ribarsky, W
   Shaw, CD
TI Punctuated simplification of man-made objects
SO VISUAL COMPUTER
LA English
DT Article
DE mesh simplification; feature detection; level-of-detail
ID RECOGNITION; EXTRACTION
AB We present a simplification algorithm for manifold polygonal meshes of plane-dominant models. Models of this type are likely to appear in man-made environments. While traditional simplification algorithms focus on generality and smooth meshes, the approach presented here considers a specific class of man-made models. By detecting and classifying edge loops on the mesh and providing a guided series of binary mesh partitions, our approach generates a series of simplified models, each of which better respects the semantics of these kinds of models than conventional approaches do. A guiding principle is to eliminate simplifications that do not make sense in constructed environments. This, coupled with the concept of "punctuated simplification", leads to an approach that is both efficient and delivers high visual quality. Comparative results are given.
C1 Georgia Inst Technol, GVU, Ctr Comp, Atlanta, GA 30332 USA.
   Georgia Inst Technol, GVU, Coll Comp, Atlanta, GA 30332 USA.
   Arizona State Univ, Dept Comp Sci & Engn, Tempe, AZ 85287 USA.
   Arizona State Univ, PRISM, Tempe, AZ 85287 USA.
   Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
   Charlotte Visualizat Ctr, Charlotte, NC 28223 USA.
   Simon Fraser Univ Surrey, Sch Interact Arts & Technol, Surrey, BC V3T 5X3, Canada.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology; Arizona
   State University; Arizona State University-Tempe; Arizona State
   University; Arizona State University-Tempe; University of North
   Carolina; University of North Carolina Charlotte; Simon Fraser
   University
RP Georgia Inst Technol, GVU, Ctr Comp, Atlanta, GA 30332 USA.
EM jang@cc.gatech.edu; peter.wonka@asu.edu; ribarsky@uncc.edu; shaw@sfu.ca
OI Shaw, Christopher/0000-0002-6940-7971
CR [Anonymous], 2000, Computational Geometry Algorithms and Applications
   Botsch M., 2002, OpenSG Symp
   Cohen J.D., 1998, P 25 ANN C COMP GRAP, P115
   COORS V, 2001, 1 INT S SMART GRAPH
   DEFLORIANI L, 1989, IEEE T PATTERN ANAL, V11, P785, DOI 10.1109/34.31442
   El-Sana J, 1998, IEEE T VIS COMPUT GR, V4, P133, DOI 10.1109/2945.694955
   ERIKSON C., 2001, I3D 01 P 2001 S INTE, P111, DOI [10.1145/364338.364376, DOI 10.1145/364338.364376]
   Früh C, 2001, PROC CVPR IEEE, P31
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   GAVANKAR P, 1990, COMPUT AIDED DESIGN, V22, P442, DOI 10.1016/0010-4485(90)90109-P
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, P SIGGRAPH 97, P189, DOI DOI 10.1145/258734.258843
   Jang J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P473, DOI 10.1109/VISUAL.2003.1250409
   Jepson W., 1996, Presence: Teleoperators and Virtual Environments, P72
   Kho Y., 2003, P S INTERACTIVE 3D G, P123
   KIM YS, 1992, COMPUT AIDED DESIGN, V24, P461, DOI 10.1016/0010-4485(92)90027-8
   LINDSTROM P, P IEEE VIS 98, P279
   LINDSTROM Peter., 1996, P ACM SIGGRAPH 96, P109
   LOW KL, 1997, P ACM S INT 3D GRAPH, P75
   LU Y, 1999, 2 INT MESH ROUNDT SA, P269
   LU Y, 1999, P ASME DES AUT C
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   MAREFAT M, 1990, IEEE T PATTERN ANAL, V12, P949, DOI 10.1109/34.58868
   *MAY, 2004, MAY VERS 6 0
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Pojar Erik., 2003, Proceedings of the 2003 symposium on Interactive 3D graphics, P127
   PRATT M, 1985, R85ASPP01 CAM I INC
   RIBELLES J, 2001, P ASME DES ENG TECHN
   Rossignac J., 1993, Geometric Modeling in Computer Graphics, P455
   WILLIAMS N, 2003, P 2003 S INT 3D GRAP, P113
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
   WOO T, 1982, P C CAD CAM TECHN ME
   Wu MC, 1996, COMPUT AIDED DESIGN, V28, P603, DOI 10.1016/0010-4485(95)00075-5
NR 34
TC 7
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 136
EP 145
DI 10.1007/s00371-005-0355-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 018GC
UT WOS:000235751200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, H
   Chang, EC
   Huang, ZY
   Zheng, ZJ
AF Yu, H
   Chang, EC
   Huang, ZY
   Zheng, ZJ
TI Fast rendering of foveated volumes in wavelet-based representation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE volume rendering; wavelet foveation; focus plus context visualization
AB A foveated volume can be viewed as a blending of multiple regions, each with a different level of resolution. It can be efficiently represented in the wavelet domain by retaining a small number of wavelet coefficients. We exploit the arrangement of those wavelet coefficients to achieve fast volume rendering. The running time is O(n(2)+m), where n is the width of the rendered image, and m is the number of wavelet coefficients retained for the foveated volume. Our algorithm consists of two phases. The first phase is a fast reconstruction of the super-voxels from the wavelet coefficients, and the second phase renders the super-voxels by carefully tracking rays with different thickness in the super-voxels. No expensive preprocessing on the wavelet coefficients is required. Hence, it is possible to interactively modify different viewing parameters like the transfer functions. A potential application of our algorithm is in remote visualization of large volume data-sets.
C1 Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
C3 National University of Singapore
RP Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
EM yuhang@comp.nus.edu.sg; changec@comp.nus.edu.sg;
   huangzy@comp.nus.edu.sg; zhengzhi@comp.nus.edu.sg
OI Chang, Ee-Chien/0000-0003-4613-0866
CR BASU A, 1998, IEEE T SYSTEMS MAN C
   BASU A, 1993, IEEE SYSTEM MAN CYBE
   BETHEL W, 2000, SUPERCOMPUTING 00, P59
   CHANG EC, 2000, J APPL COMPUTATIONAL
   Colombo C., 1996, IMAGE TECHNOLOGY, P109
   Lacroute P., 1994, FAST VOLUME RENDERIN
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   LEVOY M, 1990, COMPUTER GRAPHICS
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   PICCAND S, 2005, SPIE MED IMAGING
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Yu H, 2004, IEEE IMAGE PROC, P2515
   Zhou Jianlong., 2004, Proceedings of Bildverarbeitung fur die Medizin04, P199
NR 13
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 735
EP 744
DI 10.1007/s00371-005-0331-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400026
DA 2024-07-18
ER

PT J
AU Cermák, M
   Skala, V
AF Cermák, M
   Skala, V
TI Polygonization of implicit surfaces with sharp features by edge-spinning
SO VISUAL COMPUTER
LA English
DT Article
DE implicit function; polygonization; curvature; sharp features
ID TRIANGULATION
AB This paper presents an adaptive approach for polygonization of implicit surfaces. The algorithm generates a well-shaped triangular mesh with respect to a given approximation error. The error is proportional to a local surface curvature estimation. Polygonization of surfaces of high curvature, as well as surfaces with sharp features, is possible using a simple technique combined with a particle system approach. The algorithm is based on a surface tracking scheme, and it is compared with other algorithms based on a similar principle, such as the marching cube and the marching triangle algorithms.
C1 Univ W Bohemia, Dept Comp Sci & Engn, Plzen, Czech Republic.
C3 University of West Bohemia Pilsen
RP Univ W Bohemia, Dept Comp Sci & Engn, Plzen, Czech Republic.
EM cermakm@kiv.zcu.cz; skala@kiv.zcu.cz
RI Skala, Vaclav/F-9141-2011
OI Skala, Vaclav/0000-0001-8886-4281
CR Adzhiev V, 2000, COMPUT GRAPH-UK, V24, P67, DOI 10.1016/S0097-8493(99)00138-7
   Akkouche S, 2001, COMPUT GRAPH FORUM, V20, P67, DOI 10.1111/1467-8659.00479
   ALLGOWER EL, 1987, SIAM J NUMER ANAL, V24, P452, DOI 10.1137/0724033
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   [Anonymous], GRAPHICS GEMS
   [Anonymous], 1997, Introduction to Implicit Surfaces
   BAUMGART B., 1975, NATL COMPUTER C AFIP, P589, DOI DOI 10.1145/1499949.1500071
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   Bloomenthal J., 1995, THESIS U CALGARY
   Cermák M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P36, DOI 10.1109/CGI.2004.1309190
   CERNAK M, 2002, P INT C ALG 2002 SLO, P245
   CERNAK M, 2002, P INT C ECI 2002 SLO, P302
   CERNAK M, 2002, P INT C ICCVG 2002 Z, P174
   FIGUEIREDO LH, 1992, THESIS I MATEMATICA
   FIGUEIREDO LH, 1992, P GRAPH INT, V92, P250
   Frédéroc T, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P283
   Hartmann E, 1998, VISUAL COMPUT, V14, P95, DOI 10.1007/s003710050126
   Hilton A, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P381, DOI 10.1109/ICIP.1996.560840
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Karkanis T, 2001, IEEE COMPUT GRAPH, V21, P60, DOI 10.1109/38.909016
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Ohtake Y, 2003, VISUAL COMPUT, V19, P115, DOI 10.1007/s00371-002-0181-z
   Ohtake Y., 2002, SMA'02: Proceedings of the seventh ACM symposium on Solid modeling and applications, (New York, NY, USA), P171
   RVACHOV AM, DEFINITION R FUNCTIO
   SCHMIDT MFW, 1993, VISUAL COMPUT, V10, P10
   SHAPIRO V, 1999, P 5 ACM S SOL MOD AP, P258
   TAUBIN G, 1994, ACM T GRAPHIC, V13, P3, DOI 10.1145/174462.174531
   UHLIR K, 2003, P C NET TECHN U W BO, P87
   Velho L, 1999, ACM T GRAPHIC, V18, P329, DOI 10.1145/337680.337717
   VELHO L., 1996, J GRAPH TOOLS, V1, P5
   2001, MVE MODULAR VISUALIZ
NR 31
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2005
VL 21
IS 4
BP 252
EP 264
DI 10.1007/s00371-005-0286-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 928MD
UT WOS:000229274900004
DA 2024-07-18
ER

PT J
AU Akkouche, S
   Galin, E
AF Akkouche, S
   Galin, E
TI Implicit surface reconstruction from contours
SO VISUAL COMPUTER
LA English
DT Article
DE implicit surfaces; polygonization; stratification; surface
   reconstruction
AB This paper presents a fast and efficient surface reconstruction method from contour data sets. The reconstructed surface is defined as an implicit surface. We need not create any geometric skeleton and the blending of the three dimensional contour functions enables us to avoid the correspondence and the branching problems that occur in geometrical methods. Tests carried out with medical scanner data-sets show that the reconstruction may be performed at interactive rates.
C1 Univ Lyon 1, CNRS, LIRIS, F-69622 Villeurbanne, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Claude
   Bernard Lyon 1; Institut National des Sciences Appliquees de Lyon - INSA
   Lyon
RP Univ Lyon 1, CNRS, LIRIS, 43 Blvd 11 Novembre 1918, F-69622 Villeurbanne, France.
EM samir.akkouche@liris.cnrs.fr; eric.galin@liris.cnrs.fr
RI Galin, Eric/X-1938-2019
OI Galin, Eric/0000-0002-5946-4112
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   [Anonymous], 1998, Algorithmic geometry
   Barequet G, 1996, IEEE VISUAL, P149, DOI 10.1109/VISUAL.1996.567804
   BITTAR E, 1995, COMPUT GRAPH FORUM, V14, pC457, DOI 10.1111/j.1467-8659.1995.cgf143_0457.x
   Crespin B, 1996, COMPUT GRAPH FORUM, V15, pC165, DOI 10.1111/1467-8659.1530165
   Ferley E, 1997, COMPUT GRAPH FORUM, V16, P283, DOI 10.1111/1467-8659.00195
   JONES MW, 1994, COMPUT GRAPH FORUM, V13, pC75, DOI 10.1111/1467-8659.1330075
   MEYERS D, 1992, ACM T GRAPHIC, V11, P228, DOI 10.1145/130881.131213
   MURAKI S, 1991, COMP GRAPH, V25, P227, DOI 10.1145/127719.122743
   Oliva JM, 1996, COMPUT GRAPH FORUM, V15, pC397, DOI 10.1111/1467-8659.1530397
   PASKO A, 1996, P IMPL SURF 96 2 WOR, P163
   SAVCHENKO VV, 1995, COMPUT GRAPH FORUM, V14, P181, DOI 10.1111/1467-8659.1440181
   Savchenko VV, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P4, DOI 10.1109/CGI.1998.694244
   SHAPIRO V, 1994, COMPUT AIDED GEOM D, V11, P153, DOI 10.1016/0167-8396(94)90030-2
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
NR 15
TC 5
Z9 6
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2004
VL 20
IS 6
BP 392
EP 401
DI 10.1007/s00371-002-0199-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 846DU
UT WOS:000223296100004
DA 2024-07-18
ER

PT J
AU Hart, JC
   Baker, B
   Michaelraj, J
AF Hart, JC
   Baker, B
   Michaelraj, J
TI Structural simulation of tree growth and response
SO VISUAL COMPUTER
LA English
DT Article
DE natural phenomena; trees; L-systems; statics
AB Each tree is unique because of the physical environment it experiences over the course of its life. Environmental factors shape a tree within the bounds of its genotype. Only by modeling the environmental influences can we create realistic models of trees. To this end, we constructed a structural simulation that calculates the mass of each branch of the tree to emulate the mechanisms the tree uses to balance its weight, and that estimates the photosynthesis return of the leaves to simulate phototropism. Our effort is motivated by a desire to construct a predictive tool that can be used by both those in computer graphics and forest management, with applications in image synthesis, dendrochronology, mensuration and the simulation of forest succession.
C1 Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   US EPA, Ogden Profess Serv, Corvallis, OR 97333 USA.
   Alias Wavefront, Toronto, ON M5A 1J7, Canada.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   United States Environmental Protection Agency
RP Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.
EM jch@cs.uiuc.edu; brent@heart.cor.epa.gov; jmichael@aw.sgi.com
CR ARVO J, 1988, P AUSGR 88 MELB AUST
   Avery T.E., 1994, FOREST MEASUREMENTS, V4th
   Barzel R., 1988, Computer Graphics, V22, P179, DOI 10.1145/378456.378509
   Beer F., 1984, Vector Mechanics for Engineers: Statics, Vfourth
   Bloomenthal J., 1985, Computer Graphics, V19, P305, DOI 10.1145/325165.325249
   de Reffye P., 1988, Computer Graphics, V22, P151, DOI 10.1145/378456.378505
   Deussen O., 1998, P 25 ANN C COMP GRAP
   Esau Katerine., 1960, ANATOMY SEEDS PLANTS
   *FOR PROD LAB, 1974, WOOD HDB WOOD ENG MA
   Gartner B, 1995, COMMUNICATION
   Greene N., 1989, Computer Graphics, V23, P175, DOI 10.1145/74334.74351
   HART J, 1996, P 1996 WORKSH IMPL S
   HART JC, 1995, 1 INT WORKSH IMPL SU
   HART JC, 1991, P 18 ANN C COMP GRAP
   HOLTON M, 1994, COMPUT GRAPH FORUM, V13, P57, DOI 10.1111/1467-8659.1310057
   HORN H S, 1971, P144
   JIRASEK C, 1998, P W COMP GRAPH S WHI
   JIRASEK C, 1999, P W COMP GRAPH S BAN
   LORIMER ND, 1994, NC170 USDA FOR SERV
   Mattheck C., 1991, Trees: The Mechanical Design
   MECH R, 1996, P 23 ANN C COMP GRAP
   Oppenheimer P. E., 1986, Computer Graphics, V20, P55, DOI 10.1145/15886.15892
   Prusinkiewicz P., 1988, Computer Graphics, V22, P141, DOI 10.1145/378456.378503
   PRUSINKIEWICZ P, 1994, P 21 ANN C COMP GRAP
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Schweingruber F.H., 1993, Trees and Wood in Dendrochronology: Morphological, anatomical, and treering analytical characteristics of tress frequently used in dendrochronology
   THOMPSON DW, 1961, GROWTH FORM ABRIDGED
   Viennot X. G., 1989, Computer Graphics, V23, P31, DOI 10.1145/74334.74336
   WEBER J, 1995, P 22 ANN C COMP GRAP
   WEGHORST H, 1984, ACM T GRAPHIC, V3, P52, DOI 10.1145/357332.357335
NR 30
TC 20
Z9 24
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 151
EP 163
DI 10.1007/s00371-002-0189-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 691AU
UT WOS:000183583300008
DA 2024-07-18
ER

PT J
AU Kang, RH
   Sang, LX
   Yang, L
   Yang, K
   Hao, RF
   Zhang, HL
   Sang, SB
AF Kang, Rihui
   Sang, Luxiao
   Yang, Le
   Yang, Kun
   Hao, Runfang
   Zhang, Hulin
   Sang, Shengbo
TI 3D printer vision calibration system based on embedding Sobel bilateral
   filter in least squares filtering algorithm
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Bio-3D printing; Calibration; Sobel; Contour detection
ID SCAFFOLDS
AB To address the calibration challenge in 3D printing technology, an improved calibration system has been developed, which facilitates the widespread use of bionic stents. Crucially, this system only requires a reference object with known dimensions for automatic calibration. To acquire the required compensation coefficients, various operations are conducted, including printing cell scaffolds as test cube, capturing images, preprocessing images, detecting contours, and classifying with the K-means algorithm. During the image preprocessing stage, an Embedding Bilateral Filter is employed within a least squares filtering-based method, combined with the Sobel operator, to increase the accuracy in obtaining pixel gradient values. Finally, the capability of the developed printer calibration system is demonstrated by a series of print tests; compared to caliper measurements, this approach considerably reduces the time taken, improving calibration efficiency by 98.51% and enhancing accuracy by 9.62%. This development has the potential to enhance the precision and reliability of 3D printing, which is crucial when it comes to producing medical devices and implants. Overall, this is a promising advancement that could have far-reaching implications for the medical industry.
C1 [Kang, Rihui; Sang, Luxiao; Yang, Kun; Hao, Runfang; Zhang, Hulin; Sang, Shengbo] Taiyuan Univ Technol, Coll Elect Informat & Opt Engn, Shanxi Key Lab Micro Nano Sensors & Artificial Int, Taiyuan, Peoples R China.
   [Kang, Rihui] Shanxi Res Inst 6D Artificial Intelligence Biomed, Taiyuan, Peoples R China.
   [Yang, Le] Commun Univ Shanxi, Interact Media Lab, Jinzhong, Peoples R China.
   [Sang, Shengbo] Taiyuan Univ Technol, Minist Educ, Key Lab Adv Transducers & Intelligent Control Syst, Taiyuan, Peoples R China.
C3 Taiyuan University of Technology; Communication University of Shanxi;
   Taiyuan University of Technology
RP Sang, SB (corresponding author), Taiyuan Univ Technol, Coll Elect Informat & Opt Engn, Shanxi Key Lab Micro Nano Sensors & Artificial Int, Taiyuan, Peoples R China.; Sang, SB (corresponding author), Taiyuan Univ Technol, Minist Educ, Key Lab Adv Transducers & Intelligent Control Syst, Taiyuan, Peoples R China.
EM sunboa-sang@tyut.edu.cn
RI Zhang, Hulin/K-6216-2017
OI Zhang, Hulin/0000-0003-4899-1491
FU National Natural Science Foundation of China
FX No Statement Available
CR Alphonse AS, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13061104
   Chang Q, 2023, J PARALLEL DISTR COM, V177, P160, DOI 10.1016/j.jpdc.2023.03.004
   Cheng SW, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22030926
   Cheng Z., 2022, VISUAL COMPUT, P1
   Escot L, 2023, APPL MATH COMPUT, V436, DOI 10.1016/j.amc.2022.127498
   Gao GF, 2014, BIOTECHNOL J, V9, P1304, DOI 10.1002/biot.201400305
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   Lee D, 2011, OPT ENG, V50, DOI 10.1117/1.3607414
   Li JW, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23073490
   Liu F, 2022, BIOMATERIALS, V284, DOI 10.1016/j.biomaterials.2022.121485
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Lu FF, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23052546
   Matai I, 2020, BIOMATERIALS, V226, DOI 10.1016/j.biomaterials.2019.119536
   Trung NT, 2023, COMPUT SYST SCI ENG, V44, P535, DOI 10.32604/csse.2023.026011
   Park JH, 2015, BIOMATERIALS, V62, P106, DOI 10.1016/j.biomaterials.2015.05.008
   Poongodi R, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms222413347
   Qu YD, 2005, IMAGE VISION COMPUT, V23, P11, DOI 10.1016/j.imavis.2004.07.003
   Ravoor J, 2021, ACS APPL BIO MATER, V4, P8129, DOI 10.1021/acsabm.1c00949
   Richbourg NR, 2019, J TISSUE ENG REGEN M, V13, P1275, DOI 10.1002/term.2859
   Roseti L, 2017, MAT SCI ENG C-MATER, V78, P1246, DOI 10.1016/j.msec.2017.05.017
   Schouten M, 2022, ADDIT MANUF, V56, DOI 10.1016/j.addma.2022.102890
   Shen HY, 2019, RAPID PROTOTYPING J, V25, P343, DOI 10.1108/RPJ-03-2018-0052
   Sitthi-Amorn P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766962
   Sodupe-Ortega E, 2018, MATERIALS, V11, DOI 10.3390/ma11081402
   Stopp S, 2008, RAPID PROTOTYPING J, V14, P167, DOI 10.1108/13552540810878030
   Su CY, 2022, GELS-BASEL, V8, DOI 10.3390/gels8110748
   Taghizadeh M, 2022, GREEN CHEM, V24, P62, DOI 10.1039/d1gc01799c
   Tran TT, 2018, J PHYS CONF SER, V1082, DOI 10.1088/1742-6596/1082/1/012080
   Vora HD, 2020, PROG ADDIT MANUF, V5, P319, DOI 10.1007/s40964-020-00142-6
   Wang JC, 2021, BIOMICROFLUIDICS, V15, DOI 10.1063/5.0037274
   Wu SD, 2022, ADV HEALTHC MATER, V11, DOI 10.1002/adhm.202201021
   Xue J., 2023, Regen. Biomater, V12, P032
   Yang SF, 2001, TISSUE ENG, V7, P679, DOI 10.1089/107632701753337645
   Zhang M, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz6725
   Zhang Y., 2023, Vis. Comput, V56, P1
   Zhou J, 2021, BIOACT MATER, V6, P1711, DOI 10.1016/j.bioactmat.2020.11.027
NR 36
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 22
PY 2023
DI 10.1007/s00371-023-03187-0
EA DEC 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DC1N7
UT WOS:001129743300005
DA 2024-07-18
ER

PT J
AU Dash, AK
   Balaji, KV
   Dogra, DP
   Kim, BG
AF Dash, Ajaya Kumar
   Balaji, Koniki Venkata
   Dogra, Debi Prosad
   Kim, Byung-Gyu
TI Interactions with 3D virtual objects in augmented reality using natural
   gestures
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Augmented reality; Deep learning; Interaction with virtual objects
ID HAND POSE ESTIMATION
AB Markers are the backbone of various cross-domain augmented reality (AR) applications available to the research community. However, the use of markers may limit anywhere augmentation. As smart sensors are being deployed across the large spectrum of consumer electronic (CE) products, it is becoming inevitable to rely upon natural gestures to render and interact with such CE products. It provides limitless options for augmented reality applications. This paper focuses on the use of the human palm as the natural target to render 3D virtual objects and interact with the virtual objects in a typical AR set-up. While printed markers are comparatively easier to detect for camera pose estimation, palm detection can be challenging as a replacement for physical markers. To mitigate this, we have used a two-stage palm detection model that helps to track multiple palms and the related key-points in real-time. The detected key-points help to calculate the camera pose before rendering the 3D objects. After successfully rendering the virtual objects, we use intuitive, one-handed (uni-manual) natural gestures to interact with them. A finite state machine (FSM) has been proposed to detect the change in gestures during interactions. We have validated the proposed interaction framework using a few well-known 3D virtual objects that are often used to demonstrate scientific concepts to students in various grades. Our framework has been found to perform better as compared to SOTA methods. Average precision of 96.5% (82.9% SSD+Mobilenet) and FPS of 58.27 (37.93 SSD+Mobilenet) have been achieved. Also, to widen the scope of the work, we have used a versatile gesture dataset and tested it with neural network-based models to detect gestures. The approach fits perfectly into the proposed AR pipeline at 46.83 FPS to work in real-time. This reveals that the proposed method has good potential to mitigate some of the challenges faced by the research community in the interactive AR space.
C1 [Dash, Ajaya Kumar] IIIT Bhubaneswar, Dept Comp Sci, Bhubaneswar 751003, India.
   [Dash, Ajaya Kumar; Balaji, Koniki Venkata; Dogra, Debi Prosad] IIT Bhubaneswar, Sch Elect Sci, Bhubaneswar 752050, India.
   [Kim, Byung-Gyu] Sookmyung Womens Univ, Seoul, South Korea.
C3 International Institute of Information Technology, Bhubaneswar; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bhubaneswar; Sookmyung Women's University
RP Dash, AK (corresponding author), IIIT Bhubaneswar, Dept Comp Sci, Bhubaneswar 751003, India.; Dash, AK (corresponding author), IIT Bhubaneswar, Sch Elect Sci, Bhubaneswar 752050, India.
EM ajaya@iiit-bh.ac.in; kvb10@iitbbs.ac.in; dpdogra@iitbbs.ac.in;
   bg.kim@sookmyung.ac.kr
OI DASH, AJAYA KUMAR/0000-0002-6542-8038
CR Baek S, 2018, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR.2018.00869
   Bazarevsky V, 2019, Arxiv, DOI [arXiv:1907.05047, 10.48550/arXiv.1907.05047, DOI 10.48550/ARXIV.1907.05047]
   Besancon L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4727, DOI 10.1145/3025453.3025863
   Bozgeyikli E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P778, DOI 10.1109/VR50410.2021.00105
   Dai JF, 2016, ADV NEUR IN, V29
   Fiala M, 2010, IEEE T PATTERN ANAL, V32, P1317, DOI 10.1109/TPAMI.2009.146
   Froehlich B., 2006, SIGCHI C HUMAN FACTO, P191, DOI 10.1145/1124772.1124802
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Ge LH, 2018, LECT NOTES COMPUT SC, V11217, P489, DOI 10.1007/978-3-030-01261-8_29
   Ge LH, 2017, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR.2017.602
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gurav RM, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INSTRUMENTATION AND CONTROL (ICIC), P974, DOI 10.1109/IIC.2015.7150886
   Ha TJ, 2006, LECT NOTES COMPUT SC, V4161, P354
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huber P.J., 1992, Robust Estimation of a Location Parameter, P492, DOI [DOI 10.1007/978-1-4612-4380-935, DOI 10.1007/978-1-4612-4380-9_35]
   Jang Y, 2015, IEEE T VIS COMPUT GR, V21, P501, DOI 10.1109/TVCG.2015.2391860
   Kang SK, 2008, ICHIT 2008: INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, PROCEEDINGS, P229, DOI 10.1109/ICHIT.2008.292
   Kato H., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P85, DOI 10.1109/IWAR.1999.803809
   Kerdvibulvech C, 2019, LECT NOTES COMPUT SC, V11786, P233, DOI 10.1007/978-3-030-30033-3_18
   Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61
   Kim JC, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11041738
   Kingma D. P., 2014, arXiv
   Krichenbauer M, 2018, IEEE T VIS COMPUT GR, V24, P1038, DOI 10.1109/TVCG.2017.2658570
   Lee T, 2007, ELEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P83
   Lee T, 2009, IEEE T VIS COMPUT GR, V15, P355, DOI 10.1109/TVCG.2008.190
   Li R, 2019, PATTERN RECOGN, V93, P251, DOI 10.1016/j.patcog.2019.04.026
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mofarreh-Bonab M, 2022, VISUAL COMPUT, V38, P2023, DOI 10.1007/s00371-021-02263-7
   Olson E, 2011, IEEE INT CONF ROBOT
   Pacchierotti C, 2017, IEEE T HAPTICS, V10, P580, DOI 10.1109/TOH.2017.2689006
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Pfeuffer K, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P99, DOI 10.1145/3131277.3132180
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schweighofer G, 2006, IEEE T PATTERN ANAL, V28, P2024, DOI 10.1109/TPAMI.2006.252
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Sinha A, 2016, PROC CVPR IEEE, P4150, DOI 10.1109/CVPR.2016.450
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wu WB, 2017, IEEE INT CONF COMP V, P623, DOI 10.1109/ICCVW.2017.79
   Yessou H, 2020, INT GEOSCI REMOTE SE, P1349, DOI 10.1109/IGARSS39084.2020.9323583
   Zhang F, 2020, Arxiv, DOI arXiv:2006.10214
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 44
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 11
PY 2023
DI 10.1007/s00371-023-03175-4
EA DEC 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AU0R1
UT WOS:001120854900001
DA 2024-07-18
ER

PT J
AU Liu, P
   Liu, JL
AF Liu, Peng
   Liu, Jianlei
TI Knowledge-guided multi-perception attention network for image dehazing
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Attention; Image dehazing; Knowledge guidance; Muti-perception
ID PHYSICAL-MODEL
AB Image dehazing is an important computer vision task that aims to restore clear images from blurry, hazy images. Most of the existing deep learning dehazing methods are result-oriented, ignoring the intermediate steps and it is also difficult to deploy cumbersome deep models on devices with limited resources. In addition, the attention mechanisms based on convolution kernels within a fixed window size cannot provide additional flexibility for mapping from hazy images to clear images. This work presents a novel knowledge distillation method that guides the intermediate process of dehazing to improve the performance of image dehazing networks. Specifically, we train a teacher network on clear images, which can learn useful features from clear images (ground truth), and then we select the deep layers of the network, i.e., the decoding process, to transfer these features to a lightweight student network. Moreover, we design a muti-perception attention module and apply a heterogeneous design to this module for the teacher network and the student network to extract multiscale and multilevel features of hazy images, thus enhancing the expressive ability of the student network. We conduct experiments on several public image dehazing datasets, and the results show that our method achieves a good trade-off between reducing the parameter size and maintaining a high-quality dehazing effect compared with other algorithms.
C1 [Liu, Peng; Liu, Jianlei] Qufu Normal Univ, Sch Cyber Sci & Engn, Jining 273165, Peoples R China.
C3 Qufu Normal University
RP Liu, JL (corresponding author), Qufu Normal Univ, Sch Cyber Sci & Engn, Jining 273165, Peoples R China.
EM 15269169075@163.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/icip.2019.8803046, 10.1109/ICIP.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen GB, 2017, ADV NEUR IN, V30
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guo HF, 2022, 2022 IEEE 2ND INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND SOFTWARE ENGINEERING (ICICSE 2022), P105, DOI 10.1109/ICICSE55337.2022.9828891
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hong M, 2020, PROC CVPR IEEE, P3459, DOI 10.1109/CVPR42600.2020.00352
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim G, 2023, IEEE T IMAGE PROCESS, V32, P631, DOI 10.1109/TIP.2022.3231122
   Lan YW, 2023, J ELECTRON IMAGING, V32, DOI 10.1117/1.JEI.32.1.013002
   Lan YW, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.1036465
   Lan YW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-19132-5
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Liu J., 2023, IET Image Process
   Luo P, 2016, AAAI CONF ARTIF INTE, P3560
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Su YZ, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109700
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang N, 2022, CIRC-CARDIOVASC QUAL, V15, P416, DOI 10.1161/CIRCOUTCOMES.121.008552
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu HY, 2020, IEEE COMPUT SOC CONF, P1975, DOI 10.1109/CVPRW50498.2020.00247
   Ye CX, 2020, Arxiv, DOI arXiv:1905.11926
   Yi WC, 2024, VISUAL COMPUT, V40, P2293, DOI 10.1007/s00371-023-02917-8
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zheng LR, 2023, IEEE T MULTIMEDIA, V25, P6794, DOI 10.1109/TMM.2022.3214780
   Zheng YT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3204890
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 47
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 2
PY 2023
AR s00371-023-03177-2
DI 10.1007/s00371-023-03177-2
EA DEC 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z9QB1
UT WOS:001115341900001
DA 2024-07-18
ER

PT J
AU He, WT
   Song, BP
   Zhang, N
   Xiang, J
   Pan, RR
AF He, Wentao
   Song, Bingpeng
   Zhang, Ning
   Xiang, Jun
   Pan, Ruru
TI Modeling and realization of image-based garment texture transfer
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Texture; Garment; Transfer; Modeling; Texture mapping
AB We present an automated framework founded on texture transfer, facilitating the substitution of textures in garment images with specified ones for applications in garment design and online presentation. In contrast to previous methodologies, our approach achieves seamless texture transfer from a single image while preserving fold variations and shadow intricacies. Given a garment image and a texture image, we initially extract pixel-aligned features from the garment image and construct a parametric model of the garment through spatial sampling. The mesh structure is subsequently validated employing the Marching Cube algorithm. We then enhance the model quality through mesh optimization using a variational approach. Finally, we optimize parallax mapping to execute the texture transfer from the source texture image. Experimental results convincingly demonstrate the effectiveness of our method in achieving texture transfer in garment images while maintaining the fidelity of folds and shadows.
C1 [He, Wentao; Song, Bingpeng; Zhang, Ning; Xiang, Jun; Pan, Ruru] Jiangnan Univ, Key Lab Ecotext, Minist Educ, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Pan, RR (corresponding author), Jiangnan Univ, Key Lab Ecotext, Minist Educ, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
EM prrsw@jiangnan.edu.cn
FU National Natural Science Foundation of China [61976105, 62202202];
   National Natural Science Foundation of China [KYCX22_2342]; Postgraduate
   Research & Practice Innovation Program of Jiangsu Province
FX This work was supported by National Natural Science Foundation of China
   (No. 61976105 and No. 62202202) and Postgraduate Research & Practice
   Innovation Program of Jiangsu Province (No. KYCX22_2342).
CR Albahar B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480559
   Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127
   Bartle A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925896
   Bertiche Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P344, DOI 10.1007/978-3-030-58565-5_21
   Bertiche H, 2023, PROC CVPR IEEE, P459, DOI 10.1109/CVPR52729.2023.00052
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Deschaintre Valentin., 2023, The Visual Language of Fabrics
   Feng WW, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731049
   Field DA, 2000, INT J NUMER METH ENG, V47, P887, DOI 10.1002/(SICI)1097-0207(20000210)47:4<887::AID-NME804>3.3.CO;2-8
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Guan XY, 2021, VISUAL COMPUT, V37, P2553, DOI 10.1007/s00371-021-02216-0
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hu XR, 2023, VISUAL COMPUT, V39, P3347, DOI 10.1007/s00371-023-02999-4
   Ianina A, 2022, PROC CVPR IEEE, P13276, DOI 10.1109/CVPR52688.2022.01293
   Isik M, 2023, Arxiv, DOI arXiv:2305.06356
   Jafarian Y, 2023, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR52729.2023.00449
   Jiang LG, 2019, COMPUT AIDED DESIGN, V106, P30, DOI 10.1016/j.cad.2018.08.002
   Kim T.Y., 2008, P 2008 ACM SIGGRAPHE, P49
   Kwon Youngjoong, 2021, ADV NEURAL INFORM PR, V34, P24741
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Meng YW, 2012, COMPUT AIDED DESIGN, V44, P721, DOI 10.1016/j.cad.2012.03.006
   Meng YW, 2012, COMPUT AIDED DESIGN, V44, P68, DOI 10.1016/j.cad.2010.11.008
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pei LJ, 2022, COGN NEURODYNAMICS, V16, P229, DOI 10.1007/s11571-021-09701-1
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Rodriguez-Pardo C, 2023, COMPUT GRAPH FORUM, V42, P149, DOI 10.1111/cgf.14750
   Rodriguez-Pardo C, 2023, IEEE T VIS COMPUT GR, V29, P2914, DOI 10.1109/TVCG.2022.3143615
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Siddiqui Y, 2022, LECT NOTES COMPUT SC, V13663, P72, DOI 10.1007/978-3-031-20062-5_5
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang H., 2010, ACM SIGGRAPH 2010 PA, P1
   Wang TF, 2021, INT CONF 3D VISION, P268, DOI 10.1109/3DV53792.2021.00037
   Wang TY, 2018, Arxiv, DOI arXiv:1806.11335
   Zhang M, 2021, COMPUT GRAPH FORUM, V40, P399, DOI 10.1111/cgf.142642
   Zhou B, 2013, COMPUT GRAPH FORUM, V32, P85, DOI 10.1111/cgf.12215
   Zhu YX, 2019, SCI PROGRAMMING-NETH, V2019, DOI 10.1155/2019/8069373
   Zurdo JS, 2013, IEEE T VIS COMPUT GR, V19, P149, DOI 10.1109/TVCG.2012.79
NR 40
TC 0
Z9 0
U1 7
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 19
PY 2023
DI 10.1007/s00371-023-03153-w
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6QB6
UT WOS:001106475200001
DA 2024-07-18
ER

PT J
AU Yuan, XY
   Fu, DY
   Han, SC
AF Yuan, Xinyang
   Fu, Daoyong
   Han, Songchen
TI Vision-based aircraft pose estimation with dual attention module for
   global feature extraction in complex airport scenes
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Airport safety surveillance; Aircraft pose estimation; Airport Surface
   datasets; Attention mechanism
ID MACAQUES
AB Addressing the intricate task of aircraft pose estimation in challenging airport environments, we introduce a dual attention module (DAM) inspired by the human visual system. Comprising two innovative sub-modules, the Inverse Parallel Multi-Spectral Channel Attention Module (IPMS-CAM) and the Gaussian Spatial Attention Module (G-SAM), the DAM collaboratively captures global aircraft features. Specifically, the IPMS-CAM employs the discrete cosine transform and integrates attention mechanism to adapt to different levels of visual cues, effectively addressing the "where to look" challenge within complex airport scenes. In contrast, the G-SAM emulates the spatial attention process of the cerebral cortex, effectively tackling the "what to see" problem, and thereby emphasizing pertinent information while intelligently filtering out extraneous details. Our proposed approach is rigorously evaluated through experiments on two distinct airport surface datasets. The results substantiate the compelling competitiveness of our method when juxtaposed against prevailing attention mechanism techniques. Our approach achieves a noteworthy performance advancement of approximately 3% over the baseline method, all while incurring negligible computational overhead.
C1 [Yuan, Xinyang; Fu, Daoyong; Han, Songchen] Sichuan Univ, Sch Aeronaut & Astronaut, Chengdu 610207, Sichuan, Peoples R China.
C3 Sichuan University
RP Han, SC (corresponding author), Sichuan Univ, Sch Aeronaut & Astronaut, Chengdu 610207, Sichuan, Peoples R China.
EM hansongchen@scu.edu.cn
OI Fu, Daoyong/0000-0002-6874-2436
FU This study was supported by the Key R amp;D project of Sichuan Province,
   China (No.2022YFG0153). [2022YFG0153]; Key R amp;D project of Sichuan
   Province, China
FX This study was supported by the Key R &D project of Sichuan Province,
   China (No.2022YFG0153).
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Allen M, 2023, FMRI of visual system what about FMRI of the visual system?
   Boujamza A, 2022, IFAC PAPERSONLINE, V55, P450, DOI 10.1016/j.ifacol.2022.07.353
   Breuers MG, 2001, P SOC PHOTO-OPT INS, V4379, P472, DOI 10.1117/12.445395
   Buschmann S, 2016, VISUAL COMPUT, V32, P371, DOI 10.1007/s00371-015-1185-9
   Cao ZW, 2022, LECT NOTES COMPUT SC, V13672, P737, DOI 10.1007/978-3-031-19775-8_43
   Cao ZW, 2021, IEEE WINT CONF APPL, P1187, DOI 10.1109/WACV48630.2021.00123
   Chen C, 2021, VISUAL COMPUT, V37, P2139, DOI 10.1007/s00371-020-01975-6
   Chen Y., 2017, Adv. Neural Inf. Process. Syst.
   El Marady AAW, 2017, 2017 12TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND SYSTEMS (ICCES), P182, DOI 10.1109/ICCES.2017.8275300
   Fu D., 2019, Math. Prob. Eng.
   Fu D., 2022, IEEE Trans. Aerospace Electron. Syst.
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo Q, 2021, IEEE T GEOSCI REMOTE, V59, P7570, DOI 10.1109/TGRS.2020.3027762
   Han PF, 2017, VISUAL COMPUT, V33, P1185, DOI 10.1007/s00371-016-1281-5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hmam H, 2000, P SOC PHOTO-OPT INS, V4067, P1198, DOI 10.1117/12.386709
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ince IF, 2022, VISUAL COMPUT, V38, P1845, DOI 10.1007/s00371-022-02418-0
   Kingma D, 2014, C LEARNING REPRESENT, P12
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Li XB, 2021, MEASUREMENT, V185, DOI 10.1016/j.measurement.2021.110032
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu HJ, 2022, NEUROCOMPUTING, V506, P158, DOI 10.1016/j.neucom.2022.07.054
   Liu L, 2022, RELIAB ENG SYST SAFE, V221, DOI 10.1016/j.ress.2022.108330
   Liu YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3267271
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Liu YC, 2021, VISUAL COMPUT, V37, P1769, DOI 10.1007/s00371-020-01937-y
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   MERIGAN WH, 1986, VISION RES, V26, P1751, DOI 10.1016/0042-6989(86)90125-2
   MERIGAN WH, 1989, J NEUROSCI, V9, P776
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Mures OA, 2024, VISUAL COMPUT, V40, P2695, DOI 10.1007/s00371-023-02972-1
   Paszke A, 2019, ADV NEUR IN, V32
   Perl E, 2006, IEEE AERO EL SYS MAG, V21, P24, DOI 10.1109/MAES.2006.275302
   Puranik TG, 2020, TRANSPORT RES C-EMER, V120, DOI 10.1016/j.trc.2020.102819
   Pytka J, 2022, MEASUREMENT, V195, DOI 10.1016/j.measurement.2022.111130
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi LK, 2021, INT J REMOTE SENS, V42, P4241, DOI 10.1080/01431161.2021.1892858
   Singh AK, 2020, IEEE-CAA J AUTOMATIC, V7, P1308, DOI 10.1109/JAS.2020.1003303
   Smith Matthew, 2018, Proceedings on Privacy Enhancing Technologies, V2018, P105, DOI 10.1515/popets-2018-0023
   STONE J, 1983, PARALLEL PATHWAYS VI
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Teng XC, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19092165
   Thai P, 2022, TRANSPORT RES C-EMER, V137, DOI 10.1016/j.trc.2022.103590
   VanPhat T., 2021, 2021 IEEE INT C MULT, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Ling, 2011, 2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2011), P1920, DOI 10.1109/FSKD.2011.6019888
   Wang Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181062
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang W., 2022, Adv. Neural Inf. Process. Syst., V35, P12826
   Wang Y., 2022, Comput. Intell. Neurosci., P1
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu ZJ, 2020, IEEE ACCESS, V8, P122147, DOI 10.1109/ACCESS.2020.3007182
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Zhao Y, 2021, IEEE GEOSCI REMOTE S, V18, P662, DOI 10.1109/LGRS.2020.2981255
NR 62
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 6
PY 2023
DI 10.1007/s00371-023-03110-7
EA OCT 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T2RO8
UT WOS:001076510300001
DA 2024-07-18
ER

PT J
AU Russel, NS
   Selvaraj, A
AF Russel, Newlin Shebiah
   Selvaraj, Arivazhagan
TI Ownership of abandoned object detection by integrating carried object
   recognition and context sensing
SO VISUAL COMPUTER
LA English
DT Article
DE Abandoned object detection; Carried object recognition; Canonical
   correlation analysis; Social context; Gait analysis
ID VISUAL SURVEILLANCE; TRACKING; SYSTEM
AB Abandoned baggage poses a potential threat to public safety, which needs to be monitored to avoid catastrophic effects. Identifying left baggage, the owner of the baggage, and the intention of the owner leaving the baggage are the challenging tasks in an automated video surveillance system. This paper aims at detecting abandoned objects by recognising and tracking the person carrying the object with context sensing. Here, a well-organised background modelling strategy and background subtraction in the HSV colour plane is proposed that yield a complete foreground image. The retrieved foreground blobs were identified as pedestrians, luggage, or other moving items using the prior model built with deep features. Further, deep features with kernel canonical correlation analysis and the cosine similarity index are used for tracking the person by re-identification in successive frames. The gait energy image generated for the tracked individual is identified for the condition of the carried baggage, and thus the owner of the bag is recognised. The duration of stay of the person carrying an object and the behavioural cues displayed by the person serve as the defining features for threat assessment. The PETS 2006, AVSS 2007, and ABODA data sets were used to evaluate the performance of the proposed method, and the experiments demonstrate promising results comparable to state-of-the-art techniques. The results suggest that the proposed method can effectively detect abandoned objects in public places by recognising and tracking the person carrying the object and using context sensing. The method's performance is comparable to state-of-the-art techniques and can be used in real-world scenarios to enhance public safety.
C1 [Russel, Newlin Shebiah; Selvaraj, Arivazhagan] Mepco Schlenk Engn Coll, Ctr Image Proc & Pattern Recognit, Dept Elect & Commun Engn, Sivakasi 626005, Tamil Nadu, India.
C3 Mepco Schlenk Engineering College
RP Russel, NS (corresponding author), Mepco Schlenk Engn Coll, Ctr Image Proc & Pattern Recognit, Dept Elect & Commun Engn, Sivakasi 626005, Tamil Nadu, India.
EM newlinshebiah@mepcoeng.ac.in
OI R, Newlin Shebiah/0000-0002-0835-5848
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Arivazhagan S, 2019, MULTIMED TOOLS APPL, V78, P10933, DOI 10.1007/s11042-018-6618-9
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Ben XY, 2019, PATTERN RECOGN, V90, P87, DOI 10.1016/j.patcog.2019.01.017
   Chen Z, 2020, VISUAL COMPUT, V36, P425, DOI 10.1007/s00371-019-01631-8
   Choi S, 2021, J SUPERCOMPUT, V77, P9248, DOI 10.1007/s11227-021-03641-7
   Chu J, 2020, IEEE ACCESS, V8, P856, DOI 10.1109/ACCESS.2019.2961778
   Chu J, 2018, IEEE ACCESS, V6, P19959, DOI 10.1109/ACCESS.2018.2815149
   cvg.rdg.ac, US
   cvg.reading.ac, US
   Dahi I, 2017, COMPUT VIS IMAGE UND, V158, P141, DOI 10.1016/j.cviu.2017.01.008
   Dwivedi Neelam, 2020, Procedia Computer Science, V171, P1979, DOI 10.1016/j.procs.2020.04.212
   eecs.qmul.ac, US
   Ess A, 2008, PROC CVPR IEEE, P1857
   Fan QF, 2013, IEEE I CONF COMP VIS, P2736, DOI 10.1109/ICCV.2013.340
   Ferryman J, 2013, PATTERN RECOGN LETT, V34, P789, DOI 10.1016/j.patrec.2013.01.018
   Foggia P., 2015, VISAPP, DOI [10.5220/0005306803850391, DOI 10.5220/0005306803850391]
   Ghadiri F, 2019, PATTERN RECOGN, V89, P134, DOI 10.1016/j.patcog.2018.12.009
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   iis.sinica.edu, US
   Kalsotra R, 2022, VISUAL COMPUT, V38, P4151, DOI 10.1007/s00371-021-02286-0
   Ketab F, 2023, VISUAL COMPUT, V39, P6699, DOI 10.1007/s00371-022-02757-y
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krusch P, 2017, IEEE IMAGE PROC, P4352, DOI 10.1109/ICIP.2017.8297104
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li SM, 2020, IEEE ACCESS, V8, P122772, DOI 10.1109/ACCESS.2020.3007261
   Li X., 2010, P INT C PATT REC, DOI [10.1109/ICPR.2010.1155, DOI 10.1109/ICPR.2010.1155]
   Li X, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107376
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Liao W., 2017, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VIV-1-W1, P19, DOI [10.5194/isprs-annals-IV-1-W1-19-2017, DOI 10.5194/ISPRS-ANNALS-IV-1-W1-19-2017]
   Lin CY, 2016, J VIS COMMUN IMAGE R, V39, P181, DOI 10.1016/j.jvcir.2016.05.024
   Lin K, 2015, IEEE T INF FOREN SEC, V10, P1359, DOI 10.1109/TIFS.2015.2408263
   Lisanti G., 2014, Proceedings of the International Conference on Distributed Smart Cameras, p10:1
   Liu Xiya, 2012, 2012 International Conference on Computer Science and Service System (CSSS), P2293, DOI 10.1109/CSSS.2012.569
   Manonmani T, 2020, SIGNAL IMAGE VIDEO P, V14, P537, DOI 10.1007/s11760-019-01581-7
   Min WD, 2018, SIGNAL PROCESS, V144, P238, DOI 10.1016/j.sigpro.2017.09.024
   Nam Y, 2016, MULTIMED TOOLS APPL, V75, P7003, DOI 10.1007/s11042-015-2625-2
   Nejatishahidin N, 2022, IEEE INT C INT ROBOT, P13105, DOI 10.1109/IROS47612.2022.9981452
   Park H, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19235114
   Porikli F, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/197875
   Prathiba T, 2021, WIRELESS PERS COMMUN, V116, P411, DOI 10.1007/s11277-020-07721-4
   Prathiba T, 2021, J AMB INTEL HUM COMP, V12, P6215, DOI 10.1007/s12652-020-02190-w
   Ribeiro D, 2017, PATTERN RECOGN, V61, P641, DOI 10.1016/j.patcog.2016.05.027
   Satybaldina D. Zh, 2021, IOP Conference Series: Materials Science and Engineering, V1069, DOI 10.1088/1757-899X/1069/1/012046
   Serbetci A, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107319
   Shyam D, 2018, IEEE INT CON MULTI
   Singh A, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P352, DOI 10.1109/AVSS.2009.74
   Smeureanu S, 2018, EUR SIGNAL PR CONF, P1775, DOI 10.23919/EUSIPCO.2018.8553156
   Song CF, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106988
   Szwoch G, 2016, MULTIMED TOOLS APPL, V75, P761, DOI 10.1007/s11042-014-2324-4
   Tian SJ, 2020, VISUAL COMPUT, V36, P1219, DOI 10.1007/s00371-019-01730-6
   Tian YL, 2011, IEEE T SYST MAN CY C, V41, P565, DOI 10.1109/TSMCC.2010.2065803
   Tripathi RK, 2019, MULTIMED TOOLS APPL, V78, P7585, DOI 10.1007/s11042-018-6472-9
   Tripathi Ravi Nath., 2013, 2013 International Conference on Emerging Trends in Communication, Control, Signal Processing and Computing Applications (C2SPCA), P1, DOI DOI 10.1109/C2SPCA.2013.6749390
   Tripathi RK., 2021, ARTIF INTELL REV, V54, P1, DOI [10.1007/s10462-020-09800-9, DOI 10.1007/S10462-020-09800-9]
   Tripathi RK., 2020, INT J MACH INTELL SE, V3, P31, DOI [10.1504/IJMISSP.2020.106774, DOI 10.1504/IJMISSP.2020.106774]
   Uddin Md Zasim, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0041-z
   Vallathan G, 2021, J SUPERCOMPUT, V77, P3242, DOI 10.1007/s11227-020-03387-8
   Wahyono, 2016, IEEE T IND INFORM, V12, P2247, DOI 10.1109/TII.2016.2605582
   Wei X, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107195
   Wu WY, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107424
   Yuan Y, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-0496-6
   Zeng YL, 2015, SENSORS-BASEL, V15, P6885, DOI 10.3390/s150306885
   Zhang HL, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107232
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang YQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041010
NR 66
TC 0
Z9 0
U1 11
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4401
EP 4426
DI 10.1007/s00371-023-03089-1
EA OCT 2023
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001079965400001
DA 2024-07-18
ER

PT J
AU Hu, XR
   Chang, Q
   Huang, JJ
   Luo, RQ
   Wang, BC
   Hu, C
AF Hu, Xinrong
   Chang, Qing
   Huang, Junjie
   Luo, Ruiqi
   Wang, Bangchao
   Hu, Chang
TI HSSAN: hair synthesis with style-guided spatially adaptive normalization
   on generative adversarial network
SO VISUAL COMPUTER
LA English
DT Article
DE Hairstyle transfer; GANs; Image synthesis; Style-guided spatially
   adaptive normalization
ID IMAGE SYNTHESIS
AB Hair synthesis plays a crucial role in generating facial images, but the complex textures and varied shapes of hair create obstacles in creating genuine images of hair on photographs utilizing generative adversarial networks. This research paper proposes an inventive normalization technique, HSSAN (Hair Style-Guided Spatially Adaptive Normalization), that incorporates four connected phases, each set exclusively for hair feature attributes, and uses them to improve the generator to generate hairstyle transfer images. The hair synthesizer generator utilizes several HSSAN residual blocks in the network framework, while the input modules comprise only an appearance module and a background module. Furthermore, a regularized loss function is introduced to regulate the style vector. Through the network, realistic hair generation images can be generated. We employed the FFHQ dataset to perform our experiments and observed that our methodology generates hair images surpassing existing generative adversarial network-based methods in terms of visual realism and Frechet Inception Distance.
C1 [Hu, Xinrong; Chang, Qing; Huang, Junjie; Luo, Ruiqi; Wang, Bangchao; Hu, Chang] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.
   [Hu, Xinrong; Chang, Qing; Huang, Junjie; Luo, Ruiqi; Wang, Bangchao; Hu, Chang] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
C3 Wuhan Textile University
RP Huang, JJ (corresponding author), Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Peoples R China.; Huang, JJ (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
EM hxr@wtu.edu.cn; 2115063012@mail.wtu.edu.cn; jjhuang@wtu.edu.cn;
   rqluo@wtu.edu.cn; bcwang@wtu.wdu.cn; 2115063016@mail.wtu.edu.cn
RI Huang, Junjie/U-1939-2018
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Chi J, 2017, VISUAL COMPUT, V33, P981, DOI 10.1007/s00371-017-1387-4
   Dolhansky B, 2018, PROC CVPR IEEE, P7902, DOI 10.1109/CVPR.2018.00824
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lin C., 2022, VISUAL COMPUT, P1
   Luo LJ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462026
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Olszewski Kyle, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7444, DOI 10.1109/CVPR42600.2020.00747
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qiu H, 2019, COMPUT GRAPH FORUM, V38, P403, DOI 10.1111/cgf.13847
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Saha R, 2021, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR46437.2021.00202
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan Z., 2020, ARXIV
   Ulyanov Dmitry, 2016, arXiv
   Wang H.-F., 2022, ARXIV
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Xiao C., 2021, ARXIV
   Zhang M, 2019, VIS INFORM, V3, P102, DOI 10.1016/j.visinf.2019.06.001
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
NR 31
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3311
EP 3318
DI 10.1007/s00371-023-02998-5
EA JUL 2023
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001026397800004
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Luo, FF
   Li, QS
   Liu, XR
   Hu, L
AF Liu, Shengjun
   Luo, Feifan
   Li, Qinsong
   Liu, Xinru
   Hu, Ling
TI AWEDD: a descriptor simultaneously encoding multiscale extrinsic and
   intrinsic shape features
SO VISUAL COMPUTER
LA English
DT Article
DE Local descriptor; Shape matching; Anisotropic wavelets; Anisotropic
   energy decomposition
AB We construct a novel descriptor called anisotropic wavelet energy decomposition descriptor (AWEDD) for non-rigid shape analysis, based on anisotropic diffusion geometry. We first extend the Dirichlet energy of the vertex coordinate function to an anisotropic version, then use multiscale anisotropic spectral manifold wavelets to decompose the Dirichlet energy to all vertices and collect local energy at each vertex to form AWEDD. AWEDD simultaneously encodes multiscale extrinsic and intrinsic shape features, which are more informative and robust than purely intrinsic or extrinsic descriptors. And the introduction of anisotropy endows AWEDD with stronger abilities of feature discrimination and intrinsic symmetry identification. Our results demonstrate that AWEDD is more discriminative than current state-of-the-art descriptors. In addition, we show that AWEDD is an excellent choice of the initial inputs for various shape analysis approaches, such as functional map pipelines and deep convolutional architectures.
C1 [Liu, Shengjun; Luo, Feifan; Li, Qinsong; Liu, Xinru] Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha, Peoples R China.
   [Hu, Ling] Hunan First Normal Univ, Sch Math & Stat, Changsha, Peoples R China.
   [Li, Qinsong] Cent South Univ, Big Data Inst, Changsha, Peoples R China.
C3 Central South University; Hunan First Normal University; Central South
   University
RP Hu, L (corresponding author), Hunan First Normal Univ, Sch Math & Stat, Changsha, Peoples R China.
EM shjliu.cg@csu.edu.cn; 197668499@qq.com; qinsli.cg@foxmail.com;
   liuxinru@csu.edu.cn; huling.cg@foxmail.com
RI Hu, Ling/AAA-5764-2020; Liu, Xinru/KEH-2341-2024
FU Natural Science Foundation of China [62172447, 61876191]; Hunan
   Provincial Natural Science Foundation of China [2021JJ30172,
   2023JJ40769]; Open Project Program of the State Key Laboratory of
   Multimodal Artificial Intelligence Systems [202200025]
FX AcknowledgementsThis work was supported by the Natural Science
   Foundation of China (Nos. 62172447, 61876191), Hunan Provincial Natural
   Science Foundation of China (No. 2021JJ30172, 2023JJ40769), and the Open
   Project Program of the State Key Laboratory of Multimodal Artificial
   Intelligence Systems (No. 202200025).
CR Andreux M, 2015, LECT NOTES COMPUT SC, V8928, P299, DOI 10.1007/978-3-319-16220-1_21
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Cosmo L, 2022, INT J COMPUT VISION, V130, P1474, DOI 10.1007/s11263-022-01610-y
   Cosmo Luca, 2016, EUR WORKSH 3D OBJ RE, V2, P12
   Defferrard M, 2016, ADV NEUR IN, V29
   Donati N, 2022, PROC CVPR IEEE, P732, DOI 10.1109/CVPR52688.2022.00082
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hu L, 2021, PROC CVPR IEEE, P14531, DOI 10.1109/CVPR46437.2021.01430
   Hu Ling, 2019, Journal of Zhejiang University (Engineering Science), V53, P761, DOI 10.3785/j.issn.1008-973X.2019.04.017
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Lahner Z., 2016, P 3DOR, V2, DOI 10.2312
   Leonardi N, 2013, IEEE T SIGNAL PROCES, V61, P3357, DOI 10.1109/TSP.2013.2259825
   Li L., 2022, ARXIV
   Li QS, 2021, COMPUT GRAPH FORUM, V40, P81, DOI 10.1111/cgf.14120
   Li Qinsong, 2020, P IEEECVF C COMPUTER, P14658
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Manay S, 2004, LECT NOTES COMPUT SC, V2034, P87
   Melzi S., 2019, EUR WORKSH 3D OBJ RE, V7
   Melzi S, 2019, PROC CVPR IEEE, P4624, DOI 10.1109/CVPR.2019.00476
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Melzi S, 2016, INT CONF 3D VISION, P470, DOI 10.1109/3DV.2016.57
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Panine M., 2022, ARXIV
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   Pickup D, 2016, INT J COMPUT VISION, V120, P169, DOI 10.1007/s11263-016-0903-8
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Robinette K. M., 1999, 2 INT C 3D DIG IM MO, P380, DOI DOI 10.1109/IM.1999.805368
   Rodola E., 2017, Computer Graphics Forum, V36, P222, DOI 10.1111/cgf.12797
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Shamai G, 2017, PROC CVPR IEEE, P3624, DOI 10.1109/CVPR.2017.386
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vestner M, 2017, INT CONF 3D VISION, P517, DOI 10.1109/3DV.2017.00065
   Wang Y., 2019, P IEEECVF C COMPUTER
   Wang Y., 2019, P IEEECVF C COMPUTER, P6231
   Wang YQ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392443
   Wang YB, 2019, SECUR COMMUN NETW, DOI 10.1155/2019/2437062
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
NR 52
TC 0
Z9 0
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2537
EP 2554
DI 10.1007/s00371-023-02935-6
EA JUL 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001025599200003
DA 2024-07-18
ER

PT J
AU Lin, SQ
   Masood, A
   Li, TY
   Huang, GY
   Dai, RP
AF Lin, Shiqun
   Masood, Anum
   Li, Tingyao
   Huang, Gengyou
   Dai, Rongping
TI Deep learning-enabled automatic screening of SLE diseases and LR using
   OCT images
SO VISUAL COMPUTER
LA English
DT Article
DE Systemic lupus erythematosus (SLE); Lupus erythematosus retinopathy
   (LR); Optical coherence tomography (OCT); Deep learning; Noninvasive
   imaging
ID SYSTEMIC-LUPUS-ERYTHEMATOSUS; OPTICAL COHERENCE TOMOGRAPHY; SEVERE
   VASOOCCLUSIVE RETINOPATHY; NERVE-FIBER LAYER; ANTIPHOSPHOLIPID SYNDROME;
   OCULAR MANIFESTATIONS; ASSOCIATIONS; PATTERNS
AB Optical coherence tomography (OCT) is a noninvasive imaging technique that enables the visualization of tissue microstructure in vivo. Recent studies have suggested that OCT can be used for detecting and monitoring retinal changes over time in patients with systemic lupus erythematosus (SLE), an auto-immune disease that damages various organs, including the eye itself. This research work discusses the potential of using OCT as a screening tool for SLE. OCT provides a detailed view of the retina, allowing the detection of subtle changes that may indicate early-stage SLE-related eye disease to screen SLE patients. The use of OCT as a screening tool may help to identify lupus erythematosus retinopathy (LR) and facilitate earlier interventions, ultimately improving patient outcomes. In addition, we used deep learning-based automated screening using OCT images of SLE patients. We present a novel deep-learning model combining a pre-trained CNN, a multi-scale module, a pooling module, and an FC classifier. Our prediction model for SLE disease has outperformed the state-of-the-art method using the in-house dataset from Peking Union Medical College Hospital. Our model achieved a higher AUC indicating a high correlation between the ground truth and predicted output. However, further studies are needed to determine the sensitivity and specificity of OCT in detecting SLE and to establish appropriate screening protocols for this patient population.
C1 [Lin, Shiqun; Dai, Rongping] Chinese Acad Med Sci, Peking Union Med Coll Hosp, Peking Union Med Coll, Dept Ophthalmol, Beijing, Peoples R China.
   [Masood, Anum] Norwegian Univ Sci & Technol, Dept Circulat & Med Imaging, Trondheim, Norway.
   [Li, Tingyao; Huang, Gengyou] Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China.
C3 Chinese Academy of Medical Sciences - Peking Union Medical College;
   Peking Union Medical College; Peking Union Medical College Hospital;
   Norwegian University of Science & Technology (NTNU); Shanghai Jiao Tong
   University
RP Dai, RP (corresponding author), Chinese Acad Med Sci, Peking Union Med Coll Hosp, Peking Union Med Coll, Dept Ophthalmol, Beijing, Peoples R China.
EM shiqun-lin@163.com; anum.masood@ntnu.no; tingyaolee@sjtu.edu.cn;
   huanggengyou@sjtu.edu.cn; derricka@sina.com
RI Masood, Anum/KAM-4917-2024; chen, chen/KHW-7024-2024; yang,
   xiao/KHT-9445-2024; LI, yue/KHC-6771-2024; Zhao, Hang/KCL-7278-2024; Li,
   Bo/KHX-7246-2024; Wang, Jinyang/JXN-8650-2024; zhou,
   yuwei/KHD-4127-2024; li, li/KHE-5750-2024; Li, Yuanyuan/KEH-6935-2024;
   yang, ying/KHW-9378-2024; Yan, Xin/KGL-5903-2024; Liu,
   Jiacheng/KHX-5326-2024; CHEN, BING/KHX-6659-2024; zhang,
   zheng/KHY-8870-2024; li, yan/KFQ-3850-2024; Liu, Yan/KFQ-1417-2024; Li,
   Yan/KFQ-9244-2024
OI Masood, Anum/0000-0001-5411-8969; Li, Yuanyuan/0000-0002-4955-1159; Liu,
   Jiacheng/0000-0002-0518-3577; 
CR Altan G, 2022, ENG SCI TECHNOL, V34, DOI 10.1016/j.jestch.2021.101091
   Aringer M, 2019, ARTHRITIS RHEUMATOL, V71, P1400, DOI [10.1002/art.40930, 10.1136/annrheumdis-2018-214819]
   ARONSON AJ, 1979, ARCH INTERN MED, V139, P1312, DOI 10.1001/archinte.139.11.1312
   Au A, 2004, CLIN EXP OPHTHALMOL, V32, P87, DOI 10.1046/j.1442-9071.2004.00766.x
   Azevedo L., 2019, EC OPHTHALMOL, V10, P01
   Bergmeister R, 1929, WIEN MED WCHNSCHR, V79, P1116
   Butendieck RR, 2012, J RHEUMATOL, V39, P1095, DOI 10.3899/jrheum.111462
   Cancro MP, 2009, J CLIN INVEST, V119, P1066, DOI 10.1172/JCI38010
   Ceccarelli F, 2023, INT J MOL SCI, V24, DOI 10.3390/ijms24054514
   Cervera R, 2002, ARTHRITIS RHEUM-US, V46, P1019, DOI 10.1002/art.10187
   Fouad E, 2015, SAUDI J OPHTHALMOL, V29, P169, DOI 10.1016/j.sjopt.2014.09.005
   Gao N, 2017, LUPUS, V26, P1182, DOI 10.1177/0961203317698050
   Gordon C., 2016, Systemic Lupus Erythematosus
   Guo S., 2022, RETINA-J RET VIT DIS, P10
   Hao HY, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2021.101956
   Hasan R., 2019, ACTUAL PROBLEMS SYST
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong-Kee N, 2014, CLIN OPHTHALMOL, V8, P2359, DOI 10.2147/OPTH.S71712
   HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169
   JABS DA, 1986, ARCH OPHTHALMOL-CHIC, V104, P558
   Jorge AM, 2022, LUPUS, V31, P1296, DOI 10.1177/09612033221114805
   KARPIK AG, 1985, CLIN IMMUNOL IMMUNOP, V35, P295, DOI 10.1016/0090-1229(85)90091-1
   Kegerreis B, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-45989-0
   Koonce B., 2021, Resnet 34. Convolutional Neural Networks with Swift for Tensorflow: Image Recognition and Dataset Categorization, P51
   Lee WJ, 2013, RHEUMATOL INT, V33, P247, DOI 10.1007/s00296-011-2139-9
   Leone P, 2021, BIOMEDICINES, V9, DOI 10.3390/biomedicines9111626
   Leuchten N, 2018, ARTHRIT CARE RES, V70, P428, DOI 10.1002/acr.23292
   Li F, 2019, BIOMED OPT EXPRESS, V10, P6204, DOI 10.1364/BOE.10.006204
   Li X., 2021, J MED IMAG HLTH INFO, V11, P1341, DOI [10.1166/jmihi.2021.3378, DOI 10.1166/JMIHI.2021.3378]
   Liu GY, 2015, LUPUS, V24, P1169, DOI 10.1177/0961203315582285
   Liu R, 2022, QUANT IMAG MED SURG, V12, P823, DOI 10.21037/qims-21-359
   Matthiesen R, 2021, EBIOMEDICINE, V70, DOI 10.1016/j.ebiom.2021.103504
   Mimier-Janczak M, 2021, J CLIN MED, V10, DOI 10.3390/jcm10132887
   Mizuno Y, 2020, ANN RHEUM DIS, V79, DOI 10.1136/annrheumdis-2018-214751
   Montehermoso A, 1999, SEMIN ARTHRITIS RHEU, V28, P326, DOI 10.1016/S0049-0172(99)80017-1
   Palejwala NV, 2012, AUTOIMMUN DIS, V2012, DOI 10.1155/2012/290898
   Pan L. T., 1998, Annals Academy of Medicine Singapore, V27, P21
   Pelegrin L, 2023, RHEUMATOLOGY, V62, P2475, DOI 10.1093/rheumatology/keac626
   Rajabi E, 2022, LUPUS, V31, P820, DOI 10.1177/09612033221093548
   Sahu DK, 2008, INDIAN J OPHTHALMOL, V56, P72, DOI 10.4103/0301-4738.37605
   Seth G, 2018, RHEUMATOL INT, V38, P1495, DOI 10.1007/s00296-018-4083-4
   Shen ZQ, 2021, Arxiv, DOI arXiv:2009.08453
   Shi W.Q., 2021, FRONT MED, P2467
   Shulman S, 2017, LUPUS, V26, P1420, DOI 10.1177/0961203317703496
   Silpa-archa S, 2016, BRIT J OPHTHALMOL, V100, P135, DOI 10.1136/bjophthalmol-2015-306629
   Sivaraj RR, 2007, RHEUMATOLOGY, V46, P1757, DOI 10.1093/rheumatology/kem173
   STAFFORDBRADY FJ, 1988, ARTHRITIS RHEUM-US, V31, P1105, DOI 10.1002/art.1780310904
   Stanga PE, 2003, OPHTHALMOLOGY, V110, P15, DOI 10.1016/S0161-6420(02)01563-4
   Stojanowski J, 2022, BMC NEPHROL, V23, DOI 10.1186/s12882-022-02978-2
   Ushiyama O, 2000, ANN RHEUM DIS, V59, P705, DOI 10.1136/ard.59.9.705
   Viedma IA, 2022, NEUROCOMPUTING, V507, P247, DOI 10.1016/j.neucom.2022.08.021
   Wang J, 2019, BIOMED OPT EXPRESS, V10, P6057, DOI 10.1364/BOE.10.006057
   Zhao YJ, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-20845-w
   Zhou Yuan, 2022, Comput Intell Neurosci, V2022, P7167066, DOI 10.1155/2022/7167066
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
   Zou KH, 2007, CIRCULATION, V115, P654, DOI 10.1161/CIRCULATIONAHA.105.594929
NR 56
TC 2
Z9 2
U1 5
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3259
EP 3269
DI 10.1007/s00371-023-02945-4
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001025875300002
DA 2024-07-18
ER

PT J
AU Xu, B
   Wang, CQ
   Liu, Y
   Zhou, YJ
AF Xu, Bin
   Wang, Congqing
   Liu, Yang
   Zhou, Yongjun
TI An anchor-based convolutional network for the near-surface camouflaged
   personnel detection of UAVs
SO VISUAL COMPUTER
LA English
DT Article
DE Unmanned aerial vehicles (UAVs); Camouflaged personnel detection;
   Anchor-based convolutional network; Efficient channel attention (ECA);
   Receptive fields block (RFB)
AB With the lightweight and dexterous design, unmanned aerial vehicles (UAVs) are widely used to perform near-surface target detection missions in various complex environments. The camouflaged personnel detection in the images captured by the UAVs plays an essential role in the information acquisition, which determines the success of detection missions. However, the camouflaged personnel are not easily discovered due to the high similarity between the camouflage style and the background. In addition, there is only available few labeled sample images. To address above-mentioned problems, a camouflaged personnel dataset is first established, and a novel anchor-based method is then proposed to detect the camouflaged personnel. In addition, the efficient channel attention and the improved receptive fields block are added in the anchor-based convolutional network to focus on more features about the camouflaged targets. Besides, the non-maximum suppression is applied to determine the optimal bounding box on the targets. The quantitative results and visualization effects demonstrate that the mean average precision of the proposed method can reach 85%, and the recall can reach 83% on the developed dataset.
C1 [Xu, Bin; Wang, Congqing; Liu, Yang] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 211106, Peoples R China.
   [Zhou, Yongjun] Near Surface Detect Lab Sci & Technol, Wuxi 214000, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Wang, CQ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 211106, Peoples R China.
EM wangcq@nuaa.edu.cn
RI Liu, Yang/AAG-5661-2019
OI Liu, Yang/0000-0002-1798-9033
FU Project of Science and Technology on Near-Surface Detection Laboratory
   of China [TCGZ2019A006]
FX AcknowledgementsThe work described in this paper was partially supported
   by the Project of Science and Technology on Near-Surface Detection
   Laboratory of China (Grant No. TCGZ2019A006).
CR Boult TE, 2001, P IEEE, V89, P1382, DOI 10.1109/5.959337
   Chen JYC, 2010, ERGONOMICS, V53, P940, DOI 10.1080/00140139.2010.500404
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Daoudi S., 2021, Ing. Syst. Inf., V26, P59
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan JR, 2020, DEF TECHNOL, V16, P150, DOI 10.1016/j.dt.2019.09.002
   Fan JW, 2023, VISUAL COMPUT, V39, P319, DOI 10.1007/s00371-021-02331-y
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong KC, 2021, VISUAL COMPUT, V37, P371, DOI 10.1007/s00371-020-01805-9
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Kong T., 2019, PROC IEEE COMPUT SOC
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2023, DEF TECHNOL, V21, P176, DOI 10.1016/j.dt.2021.09.004
   Liu YC, 2021, VISUAL COMPUT, V37, P1769, DOI 10.1007/s00371-020-01937-y
   Liu Y, 2021, PATTERN RECOGN LETT, V145, P118, DOI 10.1016/j.patrec.2021.02.001
   Roy K, 2022, VISUAL COMPUT, V38, P2801, DOI 10.1007/s00371-021-02157-8
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Yang X., 2021, DEF TECHNOL, DOI [10.1016/j.dt.2020.08.007, DOI 10.1016/J.DT.2020.08.007]
   Yuan WQ, 2023, VISUAL COMPUT, V39, P5199, DOI 10.1007/s00371-022-02654-4
   Zeng LP, 2023, VISUAL COMPUT, V39, P2165, DOI 10.1007/s00371-022-02471-9
   Zhang HL, 2021, IEEE CONF COMPUT, DOI 10.1109/INFOCOMWKSHPS51825.2021.9484578
   Zheng YF, 2019, IEEE SIGNAL PROC LET, V26, P29, DOI 10.1109/LSP.2018.2825959
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu B.J., 2020, P IEEE COMPUTER SOC, DOI DOI 10.48550/ARXIV.2007.03496
   Zhu Qihui, 2021, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/1982/1/012082
NR 32
TC 2
Z9 2
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1659
EP 1671
DI 10.1007/s00371-023-02877-z
EA MAY 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000993953600001
DA 2024-07-18
ER

PT J
AU Shen, X
   Wang, HB
   Cui, TX
   Guo, ZC
   Fu, XP
AF Shen, Xin
   Wang, Huibing
   Cui, Tianxiang
   Guo, Zhicheng
   Fu, Xianping
TI Multiple information perception-based attention in YOLO for underwater
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Underwater object detection; Information perception; Attention
   mechanism; YOLO detector
AB Underwater object detection is a prerequisite for underwater robots to achieve autonomous operation and ocean exploration. However, poor imaging quality, harsh underwater environments, and concealed underwater targets greatly aggravate the difficulty of underwater object detection. In order to reduce underwater background interference and improve underwater object perception, we propose a multiple information perception-based attention module (MIPAM), which is mainly composed of five processes. In information preprocessing, spatial downsampling and channel splitting control parameters and computations of attention module by reducing dimension sizes. In information collection, channel-level information collection and spatial-level information collection enhance the semantic information expression by perceiving multi-dimensional dependency information, multi-dimensional structure information and multi-dimensional global information. In information interaction, channel-driven information interaction and spatial-driven information interaction stimulate the intrinsic interaction potential by further perceiving multi-dimensional diversity information. Adaptive feature fusion further improves the information interaction quality by allocating learnable parameters. In attention activation, the multi-branch structure enhances the attention calibration efficiency by generating multiple attention. In information postprocessing, channel concatenation and spatial upsampling realize the plug-and-play of attention module by restoring original feature states. In order to meet the high-precision and real-time requirements for underwater object detection, we integrate MIPAM into YOLO detectors. The experimental results indicate that our work brings significant performance gains for underwater detection tasks. Our work also provides some performance improvements for other detection tasks, which shows the ideal generalization ability.
C1 [Shen, Xin; Wang, Huibing; Cui, Tianxiang; Guo, Zhicheng; Fu, Xianping] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Fu, Xianping] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Dalian Maritime University; Peng Cheng Laboratory
RP Fu, XP (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.; Fu, XP (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM shenxin@dlmu.edu.cn; huibing.wang@dlmu.edu.cn; a1065242944@dlmu.edu.cn;
   gzc15735162249@dlmu.edu.cn; fxp@dlmu.edu.cn
RI Shen, Xin/JBI-6913-2023
FU National Natural Science Foundation of China [61370142, 61802043,
   61272368, 62176037, 62002041]; Fundamental Research Funds for the
   Central Universities [3132016352, 3132021238]; Dalian Science and
   Technology Innovation Fund [2018J12GX037, 2019J11CY001, 2021JJ12GX028];
   Liaoning Revitalization Talents Program [XLYC1908007]; Liaoning Doctoral
   Research Start-up Fund Project Grant [2021-BS-075]; China Postdoctoral
   Science Foundation [3620080307]
FX The authors gratefully acknowledge the financial supports from the
   National Natural Science Foundation of China under Grant 61370142, Grant
   61802043, Grant 61272368, Grant 62176037 and Grant 62002041, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   3132016352 and Grant 3132021238, in part by the Dalian Science and
   Technology Innovation Fund under Grant 2018J12GX037, Grant 2019J11CY001
   and Grant 2021JJ12GX028, in part by Liaoning Revitalization Talents
   Program under Grant XLYC1908007, in part by the Liaoning Doctoral
   Research Start-up Fund Project Grant 2021-BS-075, and in part by the
   China Postdoctoral Science Foundation under Grant 3620080307.
CR [Anonymous], UND ROB PICK CONT
   [Anonymous], 2022, YOLOV6 SINGL STAG OB
   Ayob A.F., 2021, P 11 NAT TECHN SEM U, P87, DOI 10.1007/978-981-15-5281-6_7
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen YP, 2018, ADV NEUR IN, V31
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Cho H, 2015, J MAR SCI TECH-JAPAN, V20, P180, DOI 10.1007/s00773-014-0294-x
   Chuang MC, 2016, IEEE T IMAGE PROCESS, V25, P1862, DOI 10.1109/TIP.2016.2535342
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao ZL, 2019, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR.2019.00314
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   [黄海宁 Huang Haining], 2019, [中国科学院院刊, Bulletin of the Chinese Academy of Sciences], V34, P264
   Jalal A, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101088
   Jian MW, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116088
   Jian MW, 2019, APPL SOFT COMPUT, V80, P425, DOI 10.1016/j.asoc.2019.04.025
   Jian MW, 2018, J VIS COMMUN IMAGE R, V53, P31, DOI 10.1016/j.jvcir.2018.03.008
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Jocher G., 2021, YOLOV5
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Knausgard K.M., 2022, APPL INTELL, P1
   Lee H, 2019, IEEE I CONF COMP VIS, P1854, DOI 10.1109/ICCV.2019.00194
   Li X., 2019, ARXIV
   Li XR, 2023, VISUAL COMPUT, V39, P1307, DOI 10.1007/s00371-022-02407-3
   Li Z., 2022, VISUAL COMPUT, P1
   Lin WH, 2020, INT CONF ACOUST SPEE, P2588, DOI [10.1109/icassp40776.2020.9053829, 10.1109/ICASSP40776.2020.9053829]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Moniruzzaman M, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P41, DOI [10.1109/dicta47822.2019.8946048, 10.1109/eict48899.2019.9068752]
   Pan TS, 2021, SIGNAL IMAGE VIDEO P, V15, P941, DOI 10.1007/s11760-020-01818-w
   Park J., 2018, ARXIV
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Tharwat A, 2018, FISH RES, V204, P324, DOI 10.1016/j.fishres.2018.03.008
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu FQ, 2022, NEURAL COMPUT APPL, V34, P14881, DOI 10.1007/s00521-022-07264-8
   Xu FQ, 2021, NEURAL COMPUT APPL, V33, P3637, DOI 10.1007/s00521-020-05217-7
   Yang H, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209630
   Yang J., 2021, arXiv
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Zhang H., 2021, arXiv
   Zhang LY, 2022, FUTURE GENER COMP SY, V126, P163, DOI 10.1016/j.future.2021.07.011
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zongxin Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11791, DOI 10.1109/CVPR42600.2020.01181
NR 52
TC 5
Z9 5
U1 33
U2 69
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1415
EP 1438
DI 10.1007/s00371-023-02858-2
EA MAY 2023
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000994739900001
DA 2024-07-18
ER

PT J
AU Wang, MY
   Hu, LH
   Bai, YT
   Yao, XL
   Hu, JH
   Zhang, SL
AF Wang, Mingyang
   Hu, Lihua
   Bai, Yuting
   Yao, Xiaoling
   Hu, Jianhua
   Zhang, Sulan
TI AMNet: a new RGB-D instance segmentation network based on attention and
   multi-modality
SO VISUAL COMPUTER
LA English
DT Article
DE Instance segmentation; RGB-D images; Multi-modality; Attention-based
   network
AB As a common task in computer vision, instance segmentation has the advantage of distinguishing each instance at the pixel level. Compared to RGB instance segmentation, RGB-D instance segmentation has a better result in the low-contrast scenes due to the extra depth information. However, the challenge of RGB-D instance segmentation is that the RGB and depth information cannot be fused and used effectively. To tackle this problem, we proposed an RGB-D instance segmentation network based on attention and multi-modality(AMNet). First, a multi-modal feature fusion framework with an independent asymmetric RGB-D feature extraction structure is designed based on the SOLOV2 network to use the RGB and depth information effectively. Second, an attention-based multi-modality fused module (AMFM) is designed to obtain more accurate fusion feature maps by feature weighting. Finally, extensive experiments on the NYU Depth v2 dataset verify that our proposed AMNet achieves 9.2% relative improvement in average precision over the SOLOV2 and outperforms other advanced instance segmentation networks, especially in low-contrast scenes.
C1 [Wang, Mingyang; Hu, Lihua; Yao, Xiaoling; Zhang, Sulan] Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Shanxi, Peoples R China.
   [Bai, Yuting] China Natl Light Ind, Key Lab Ind Internet & Big Data, Beijing 100048, Peoples R China.
   [Hu, Jianhua] Zhongke Ruizhi Luoyang Digital Technology Corp, Luoyang, Peoples R China.
C3 Taiyuan University of Science & Technology
RP Hu, LH (corresponding author), Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Shanxi, Peoples R China.
EM S20202011048@stu.tyust.edu.cn; hlh@tyust.edu.cn
RI Bai, Yu-ting/AAW-2554-2020
OI Hu, Lihua/0000-0003-2212-7187
FU National Natural Science Foundation of China(NSFC) [62273248]; Key
   Laboratory Project of Beijing Technology and Business University
   [IIBD-2021-KF08]; Computer Vision Joint Training Demonstration Base of
   Taiyuan University of Science and Technology [JD2022005]
FX This work is supported by the National Natural Science Foundation of
   China(NSFC) under Grant No.62273248, the Key Laboratory Project of
   Beijing Technology and Business University(IIBD-2021-KF08), the Computer
   Vision Joint Training Demonstration Base of Taiyuan University of
   Science and Technology(JD2022005).
CR Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Champ J, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11373
   Couprie C., 2013, 1 INT C LEARNING REP, P1
   Deng L, 2019, ARXIV
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Fang Y., 2021, P IEEECVF INT C COMP
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2017, IEEE INT WORKSH MULT
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Jiang J., 2018, ARXIV
   Lamba S, 2020, VISUAL COMPUT, V36, P989, DOI 10.1007/s00371-019-01713-7
   Li H., 2018, ARXIV
   Li HT, 2022, VISUAL COMPUT, V38, P1759, DOI 10.1007/s00371-021-02103-8
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma D., 2011, Virtual Reality Augmented Reality in Industry, V2011th
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   ornek Evin Pinar, 2022, arXiv
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Qian XX, 2023, VISUAL COMPUT, V39, P87, DOI 10.1007/s00371-021-02315-y
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romera-Paredes B., 2016, EUR C COMP VIS SPRIN
   Shao L., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1807.08894
   Shi WJ, 2020, IEEE T MED ROBOT BIO, V2, P382, DOI 10.1109/TMRB.2020.3009527
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Wang X., 2020, Advances in Neural information processing systems, V33, P17721, DOI DOI 10.48550/ARXIV.2003.10152
   Xiang Y., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2007.15157
   Xie C., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2007.08073
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   [张旭东 Zhang Xudong], 2020, [控制与决策, Control and Decision], V35, P1561
   Zhao PH, 2021, IEEE ACCESS, V9, P154435, DOI 10.1109/ACCESS.2021.3128536
   Zhou WJ, 2021, IEEE INTELL SYST, V36, P73, DOI 10.1109/MIS.2020.2999462
NR 41
TC 0
Z9 0
U1 5
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1311
EP 1325
DI 10.1007/s00371-023-02850-w
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000976793600001
DA 2024-07-18
ER

PT J
AU Wu, ZW
   Jia, T
   Wu, YH
   Zeng, ZK
   Liang, F
AF Wu, Ziwei
   Jia, Tong
   Wu, Yunhe
   Zeng, Zhikang
   Liang, Feng
TI Bmsmlet: boosting multi-scale information on multi-level aggregated
   features for salient object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Multi-scale information; Multi-level
   information; Aggregated features
ID ATTENTION; NETWORK; MODEL
AB Nowadays, salient object detection methods based on deep learning have become a research focus. Therefore, how to reveal the representation mechanism and association rules of features at different levels and scales in order to improve the accuracy of salient object detection is a key issue to be solved. This paper proposes a salient object detection method to boost multi-scale information on multi-level aggregated features, which can accurately and flexibly aggregate multi-scale feature information via effective multi-level feature utilization. First, a scalable feature pyramid module is proposed, which can aggregate deep feature information in shallow features, thus obtaining aggregated features between different levels. Then, the global information enhancement module is built in the bottom-up network path to make up for the lost or weakened feature information in the process of multi-scale integration feature transmission. Next, internally convolve each level of aggregated feature via self-interactive module to enrich multi-scale information and improve the multi-scale representation ability of aggregated features. Finally, the global associativity loss function is designed to solve the noise caused by multi-scale variation so as to optimize the network training process, which effectively compensates for the deficiency of cross-entropy in the salient object detection task. The experimental results on four public datasets show that the performance of the proposed method has improved in contrast to the state-of-the-art methods.
C1 [Wu, Ziwei; Jia, Tong; Wu, Yunhe; Zeng, Zhikang; Liang, Feng] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Jia, Tong] Northeastern Univ, Minist Educ, Key Lab Data Analyt & Optimizat Smart Ind, Shenyang, Peoples R China.
C3 Northeastern University - China; Northeastern University - China
RP Jia, T (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.; Jia, T (corresponding author), Northeastern Univ, Minist Educ, Key Lab Data Analyt & Optimizat Smart Ind, Shenyang, Peoples R China.
EM jiatong@ise.neu.edu.cn
RI Wu, Yunhe/JWA-3597-2024
OI Wu, Yunhe/0000-0001-7049-7161
FU National Natural Science Foundation of China [U22A2063, 62173083]
FX Research is supported by the National Natural Science Foundation of
   China under Grant U22A2063 and 62173083.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Cao ZW, 2022, LECT NOTES COMPUT SC, V13672, P737, DOI 10.1007/978-3-031-19775-8_43
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng ZY, 2022, LECT NOTES COMPUT SC, V13698, P514, DOI 10.1007/978-3-031-19839-7_30
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Dai JF, 2016, ADV NEUR IN, V29
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Guo HY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107787
   Harel Jonathan, 2006, Adv. Neural Inf. Process. Syst., V19
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiao J, 2021, APPL INTELL, V51, P6881, DOI 10.1007/s10489-020-02147-8
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Liang YH, 2021, NEUROCOMPUTING, V422, P22, DOI 10.1016/j.neucom.2020.09.033
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Piao YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4116, DOI 10.1109/ICCV48922.2021.00410
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rutishauser U., 2004, P IEEE COMP SOC C CO, DOI DOI 10.1109/CVPR.2004.1315142
   Shen LQ, 2013, MULTIMED TOOLS APPL, V63, P709, DOI 10.1007/s11042-011-0893-z
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang QS, 2016, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.2016.64
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
NR 60
TC 0
Z9 0
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1131
EP 1144
DI 10.1007/s00371-023-02836-8
EA APR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000968292200001
DA 2024-07-18
ER

PT J
AU Ryou, D
   Youwang, K
   Oh, TH
AF Ryou, Donghun
   Youwang, Kim
   Oh, Tae-Hyun
TI Multi-stage adaptive rank statistic pruning for lightweight human 3D
   mesh recovery model
SO VISUAL COMPUTER
LA English
DT Article
DE Human mesh recovery; Lightweight neural networks; Pruning; Low-rank
   matrix
AB We present a rank statistic adaptive multi-stage pruning method to find lightweight neural networks for 3D human mesh recovery while minimizing accuracy drop. We observe that some feature maps often have prominent low-rank patterns regardless of input human images. Furthermore, even after pruning, feature channels that should have been pruned according to pruning criteria frequently re-appear in test time. From these observations, we design rank statistic adaptive multi-stage pruning; thereby, we can prune more filters with recovering mesh reconstruction accuracy. We demonstrate that, for DenseNet121, 60.0% of parameters and 67.9% of FLOPs are saved while maintaining comparable accuracy to that of the original full model. This is a notable improvement compared to the competing method based on the L1 filter pruning, where the error is increased by 17.55% at the same pruning rate.
C1 [Ryou, Donghun; Youwang, Kim; Oh, Tae-Hyun] POSTECH, Dept Elect Engn, Pohang, South Korea.
   [Oh, Tae-Hyun] POSTECH, Grad Sch AI, Pohang, South Korea.
   [Oh, Tae-Hyun] Yonsei Univ, Inst Convergence Res & Educ Adv Technol, Seoul, South Korea.
C3 Pohang University of Science & Technology (POSTECH); Pohang University
   of Science & Technology (POSTECH); Yonsei University
RP Oh, TH (corresponding author), POSTECH, Dept Elect Engn, Pohang, South Korea.; Oh, TH (corresponding author), POSTECH, Grad Sch AI, Pohang, South Korea.; Oh, TH (corresponding author), Yonsei Univ, Inst Convergence Res & Educ Adv Technol, Seoul, South Korea.
EM taehyun@postech.ac.kr
RI Kim, Youwang/HTN-0293-2023; Oh, Tae-Hyun/D-7854-2016
OI Kim, Youwang/0000-0002-7508-9260; Oh, Tae-Hyun/0000-0003-0468-1571
FU Institute of Information& communications Technology Planning& Evaluation
   (IITP) - Korea government (MSIT) [RS-2022-00164860, 2022-000290,
   2019-0-01906]; MSIT; NIPA
FX This work was supported by Institute of Information& communications
   Technology Planning& Evaluation (IITP) grant funded by the Korea
   government (MSIT) (No. RS-2022-00164860, Development of human digital
   twin technology based on dynamic behavior modeling and
   human-object-space interaction; No. 2022-000290, Visual Intelligence for
   Space-Time Understanding and Generation based on Multi-layered Visual
   Common Sense; No. 2019-0-01906, Artificial Intelligence Graduate School
   Program (POSTECH)). This research was results of a study on the "HPC
   Support" Project, supported by the MSIT and NIPA.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cho J., 2022, EUROPEAN C COMPUTER
   Dean J., 2015, NIPS DEEP LEARNING R
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Han S, 2015, ADV NEUR IN, V28
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hyeon-Woo N., 2022, INT C LEARNING REPRE
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Johnson S., 2010, P BRIT MACH VIS C, P12
   Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kim Y., 2021, IEEE INT C COMPUTER
   Kim Y., 2021, BMVC
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Krishnamoorthi R., 2018, ARXIV
   Lin K., 2021, 2021 IEEECVF INT C C
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mitsuno K, 2021, INT C PATT RECOG, P1089, DOI 10.1109/ICPR48806.2021.9413113
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Oh TH, 2018, IEEE T PATTERN ANAL, V40, P376, DOI 10.1109/TPAMI.2017.2677440
   Renda Alex, 2020, INT C LEARN REPR
   Tan C.M.J, 2020, INT C MACHINE LEARNI
   Tu C.-H., 2020, IEEE IJCNN, P1
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x
   Zhu Michael, 2018, 6 INT C LEARN REPR I
NR 31
TC 0
Z9 0
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 535
EP 543
DI 10.1007/s00371-023-02798-x
EA MAR 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000945767400001
DA 2024-07-18
ER

PT J
AU Cun, QQ
   Tong, XJ
   Wang, Z
   Zhang, M
AF Cun, Qiqi
   Tong, Xiaojun
   Wang, Zhu
   Zhang, Miao
TI A new chaotic image encryption algorithm based on dynamic DNA coding and
   RNA computing
SO VISUAL COMPUTER
LA English
DT Article
DE Hyperchaotic system; DNA encode; RNA coding; Amino acid substitution
   box; Replacement sequence generator
ID CRYPTANALYSIS; IMPROVEMENT; SYSTEM
AB In order to improve the complexity of the chaotic system and ensure the relevant security indicators of the cryptographic algorithm, a new chaotic image encryption algorithm based on dynamic DNA coding and RNA computing is proposed. In this paper, we first construct a four-dimensional hyperchaotic system with more complex dynamics and then use the plaintextrelated keystream generated by the hyperchaotic system to dynamically DNA encode the plaintext image, then perform RNA coding conversion and amino acid substitution box generation, and finally use an improved replacement sequence generator to generate pseudo-random sequences for replacement operations to generate the final ciphertext image. Theoretical analysis and simulation results show that the proposed algorithm has excellent performance in security indicators such as key space, the number of pixels change rate, the number average changing intensity, entropy, clipping attack, noise attack, and chosen plaintext attack. Therefore, the algorithm has higher security.
C1 [Cun, Qiqi; Tong, Xiaojun; Wang, Zhu; Zhang, Miao] Harbin Inst Technol, Comp Sci & Technol, Weihai 264200, Peoples R China.
C3 Harbin Institute of Technology
RP Tong, XJ (corresponding author), Harbin Inst Technol, Comp Sci & Technol, Weihai 264200, Peoples R China.
EM tong_xiaojun@163.com
FU Shandong Provincial Natural Science Foundation;  [ZR2019MF054]; 
   [61902091]
FX AcknowledgementsThis work was supported by the following projects and
   foundations: project ZR2019MF054 supported by Shandong Provincial
   Natural Science Foundation, the National Natural Science Foundation of
   China (No.61902091).
CR Abbasi AA, 2020, OPT LASER TECHNOL, V132, DOI 10.1016/j.optlastec.2020.106465
   Belazi A, 2019, IEEE ACCESS, V7, P36667, DOI 10.1109/ACCESS.2019.2906292
   Chai XL, 2017, SIGNAL PROCESS-IMAGE, V52, P6, DOI 10.1016/j.image.2016.12.007
   Chen JX, 2018, NONLINEAR DYNAM, V93, P2399, DOI 10.1007/s11071-018-4332-9
   Cun QQ, 2021, OPTIK, V243, DOI 10.1016/j.ijleo.2021.167286
   Elamrawy F., 2018, INT J SIGNAL PROCESS, V3, P27
   Fu XQ, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2827165
   Guan MM, 2019, IET IMAGE PROCESS, V13, P1535, DOI 10.1049/iet-ipr.2019.0051
   Hu T, 2017, SIGNAL PROCESS, V134, P234, DOI 10.1016/j.sigpro.2016.12.008
   Hu T, 2017, NONLINEAR DYNAM, V87, P51, DOI 10.1007/s11071-016-3024-6
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Jain A, 2016, MULTIMED TOOLS APPL, V75, P5455, DOI 10.1007/s11042-015-2515-7
   Jeng FG, 2015, SIGNAL PROCESS-IMAGE, V34, P45, DOI 10.1016/j.image.2015.03.003
   Jithin KC, 2020, J INF SECUR APPL, V50, DOI 10.1016/j.jisa.2019.102428
   Kang XJ, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115670
   Li Z, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10217469
   Liu HJ, 2012, APPL SOFT COMPUT, V12, P1457, DOI 10.1016/j.asoc.2012.01.016
   Liu H, 2017, 3D RES, V8, DOI 10.1007/s13319-016-0114-7
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Pak C, 2019, MULTIMED TOOLS APPL, V78, P12027, DOI 10.1007/s11042-018-6739-1
   Sasikaladevi N, 2019, MULTIMED TOOLS APPL, V78, P11675, DOI 10.1007/s11042-018-6711-0
   Signing VRF, 2021, MULTIMED TOOLS APPL, V80, P32689, DOI 10.1007/s11042-021-11180-9
   Wan YJ, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020171
   Wang T, 2020, OPT LASER TECHNOL, V132, DOI 10.1016/j.optlastec.2020.106355
   Wang XY, 2017, MULTIMED TOOLS APPL, V76, P6229, DOI 10.1007/s11042-016-3311-8
   Xiong ZG, 2019, MULTIMED TOOLS APPL, V78, P31035, DOI 10.1007/s11042-018-7081-3
   Yadollahi M, 2020, J INF SECUR APPL, V53, DOI 10.1016/j.jisa.2020.102505
   Yang S, 2022, PHYS SCRIPTA, V97, DOI 10.1088/1402-4896/ac59fa
   Zefreh EZ, 2020, MULTIMED TOOLS APPL, V79, P24993, DOI 10.1007/s11042-020-09111-1
   Zhan K, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.013021
   Zhang L, 2020, MULTIMED TOOLS APPL, V79, P20753, DOI 10.1007/s11042-020-08835-4
   Zhang W, 2016, SIGNAL PROCESS, V118, P36, DOI 10.1016/j.sigpro.2015.06.008
   Zhang X, 2018, IEEE PHOTONICS J, V10, DOI [10.1109/JPHOT.2018.2859257, 10.1109/JPHOT.2018.2818715]
   Zhang YQ, 2020, OPT LASER ENG, V128, DOI 10.1016/j.optlaseng.2020.106040
   Zhen P, 2016, MULTIMED TOOLS APPL, V75, P6303, DOI 10.1007/s11042-015-2573-x
   Zhou MJ, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107484
NR 36
TC 13
Z9 13
U1 15
U2 57
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6589
EP 6608
DI 10.1007/s00371-022-02750-5
EA JAN 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000908655600001
DA 2024-07-18
ER

PT J
AU Majuran, S
   Ramanan, A
AF Majuran, Shajini
   Ramanan, Amirthalingam
TI A single-stage fashion clothing detection using multilevel visual
   attention
SO VISUAL COMPUTER
LA English
DT Article
DE Clothes classification; Clothing detection; DeepFashion; Fashion
   analysis; Global attention block
AB Fashion is defined as a prevailing custom or style of dress, etiquette, and socialising. In recent years, fashion clothing analysis has attracted extensive attention from many researchers due to the introduction of large-scale datasets and the use of deep learning techniques. In this work, we propose a single-stage attention-based network for fashion clothing detection and classification. The proposed network is a single-stage detection which benefits from adopting multidimensional features through a multilevel architecture, so that the semantic gap between the lower- and upper-level features from different levels of feature representation is resolved. Besides, the network is structured based on multilevel contextual features retrieved using attention blocks in a global manner that implements a strong visual attention. Further, the classification and detection branches maintain fewer trainable parameters; thus, the model not only shows efficiency but also the testing results show state-of-the-art performance in fashion clothing detection and classification evaluated on large-scale DeepFashion2 dataset.
C1 [Majuran, Shajini; Ramanan, Amirthalingam] Univ Jaffna, Fac Sci, Dept Comp Sci, Jaffna, Sri Lanka.
C3 University Jaffna
RP Majuran, S (corresponding author), Univ Jaffna, Fac Sci, Dept Comp Sci, Jaffna, Sri Lanka.
EM shayu.kiri@gmail.com; a.ramanan@univ.jfn.ac.lk
RI Ramanan, Amirthalingam/AAO-1674-2020
OI Ramanan, Amirthalingam/0000-0002-7110-1277; Majuran,
   Shajini/0000-0003-2981-1141
CR Ak KE, 2019, IEEE I CONF COMP VIS, P10540, DOI 10.1109/ICCV.2019.01064
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   [Anonymous], 2017, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2017.683
   [Anonymous], COMPUT VIS PATTERN R
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cao KR, 2020, FUTURE INTERNET, V12, DOI 10.3390/fi12110178
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen M, 2019, IEEE INT CONF COMP V, P3101, DOI 10.1109/ICCVW.2019.00374
   Chen YY, 2019, FRONT MICROBIOL, V10, DOI 10.3389/fmicb.2019.00205
   Cheng W., 2020, ARXIV
   Chuen-Tsai S., 2011, DIGRA 2011 C THINK D, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906
   Ferreira BQ, 2019, IEEE INT CONF COMP V, P3125, DOI 10.1109/ICCVW.2019.00380
   Florea GA, 2020, FUTURE INTERNET, V12, DOI 10.3390/fi12080133
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Kang WC, 2019, PROC CVPR IEEE, P10524, DOI 10.1109/CVPR.2019.01078
   Kim HJ, 2021, IEEE ACCESS, V9, P11694, DOI 10.1109/ACCESS.2021.3051424
   Kingma D. P., 2014, arXiv
   Kokul T, 2017, IEEE IMAGE PROC, P2602, DOI 10.1109/ICIP.2017.8296753
   Lee S, 2019, ELECTRON LETT, V55, P745, DOI 10.1049/el.2019.0660
   Lee S, 2019, IEEE INT CONF COMP V, P3153, DOI 10.1109/ICCVW.2019.00387
   Li PZ, 2019, IEEE IMAGE PROC, P3038, DOI [10.1109/icip.2019.8803394, 10.1109/ICIP.2019.8803394]
   Li YX, 2019, IEEE INT CON MULTI, P820, DOI 10.1109/ICME.2019.00146
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu J., 2018, EUROPEAN C COMPUTER, P30
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Mohammadi S.O., 2021, ARXIV
   Paszke A., 2017, ADV NEURAL INF PROCE, V9, P1, DOI DOI 10.1017/CB09781107707221.009
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shajini M., 2021, ELECT LETT COMPUT VI, V20, P83, DOI [10.5565/rev/elcvia.1409, DOI 10.5565/REV/ELCVIA.1409]
   Shajini M, 2021, VISUAL COMPUT, V37, P1517, DOI 10.1007/s00371-020-01885-7
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Si CY, 2018, PROC CVPR IEEE, P118, DOI 10.1109/CVPR.2018.00020
   Sidnev A., 2020, P WINTER C APPL COMP, P1
   Sidnev A, 2019, IEEE INT CONF COMP V, P3201, DOI 10.1109/ICCVW.2019.00399
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Tan M., 2020, PROC IEEECVF C COMPU, P1
   Tan MCC, 2020, CONT PERFORM INTERAC, P1, DOI 10.1007/978-3-030-34686-7_1
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Yang S., 2016, DINOFLAGELLATES 2 GO, V28, P1
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhou X., 2019, arXiv
   Zhu QQ, 2022, IEEE T CYBERNETICS, V52, P11709, DOI 10.1109/TCYB.2021.3070577
NR 60
TC 1
Z9 1
U1 4
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6609
EP 6623
DI 10.1007/s00371-022-02751-4
EA DEC 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000905441300001
DA 2024-07-18
ER

PT J
AU Zhang, PC
   Geng, J
   Liu, YP
   Yang, SX
AF Zhang, Pengcheng
   Geng, Juan
   Liu, Yapeng
   Yang, Shouxin
TI Robust principal component analysis based on tensor train rank and
   Schatten <i>p</i>-norm
SO VISUAL COMPUTER
LA English
DT Article
DE Tensor robust principal component analysis; Tensor train rank; Schatten
   p-norm; High-dimensional data
ID NUCLEAR NORM; MATRIX; SPARSE; COMPLETION; IMAGE; REGULARIZATION
AB For a given data, robust principal component analysis (RPCA) aims to exactly recover the low-rank and sparse components from it. To date, as the convex relaxations of tensor rank, a number of tensor nuclear norms have been defined and applied to approximate the tensor rank because of their convexity. However, those relaxations may make the solution seriously deviate from the original solution for real-world data recovery. In this paper, we define the tensor Schatten p-norm based on tensor train rank and propose a new model for tensor robust principal component analysis (named TTSp). We solve the proposed model iteratively by using the ADMM algorithm. In addition, a tensor augmentation tool called ket augmentation is introduced to convert lower-order tensors to higher-order tensors to exploit the low-TT-rank structure. We report higher PSNR and SSIM values in numerical experiments to image recovery problems which demonstrate the superiority of our method. Further experiments on real data also illustrate the effectiveness of the proposed method.
C1 [Zhang, Pengcheng; Geng, Juan; Liu, Yapeng; Yang, Shouxin] Hebei Univ Econ & Business, Sch Math & Stat, Shijiazhuang 050000, Hebei, Peoples R China.
C3 Hebei University of Economics & Business
RP Geng, J (corresponding author), Hebei Univ Econ & Business, Sch Math & Stat, Shijiazhuang 050000, Hebei, Peoples R China.
EM zpc126@163.com; hebeigengjuan@163.com; nlsanlxx@163.com;
   15255157066@163.com
RI zhang, pengcheng/GSN-0543-2022
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439
   Bhardwaj A, 2016, VISUAL COMPUT, V32, P591, DOI 10.1007/s00371-015-1075-1
   Bovik AC., 2010, HDB IMAGE VIDEO PROC, P90
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao WF, 2016, IEEE T IMAGE PROCESS, V25, P4075, DOI 10.1109/TIP.2016.2579262
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737
   Chen Y, 2020, IEEE T CYBERNETICS, V50, P3556, DOI 10.1109/TCYB.2019.2936042
   Croux C, 2000, BIOMETRIKA, V87, P603, DOI 10.1093/biomet/87.3.603
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Gao QX, 2021, IEEE T PATTERN ANAL, V43, P2133, DOI 10.1109/TPAMI.2020.3017672
   Gao SQ, 2020, J SCI COMPUT, V82, DOI 10.1007/s10915-019-01108-9
   Gao Z, 2014, IEEE T PATTERN ANAL, V36, P1975, DOI 10.1109/TPAMI.2014.2314663
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126
   Huang B, 2015, PAC J OPTIM, V11, P339
   Ji H, 2011, SIAM J IMAGING SCI, V4, P1122, DOI 10.1137/100817206
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z
   Ladas N, 2021, VISUAL COMPUT, V37, P2221, DOI 10.1007/s00371-020-01981-8
   Larsson V, 2016, INT J COMPUT VISION, V120, P194, DOI 10.1007/s11263-016-0904-7
   Latorre J.I., 2005, arXiv
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Lin CL, 2019, I S BIOMED IMAGING, P1878, DOI [10.1109/isbi.2019.8759441, 10.1109/ISBI.2019.8759441]
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Lu CY, 2016, PROC CVPR IEEE, P5249, DOI 10.1109/CVPR.2016.567
   Lu CY, 2016, IEEE T IMAGE PROCESS, V25, P829, DOI 10.1109/TIP.2015.2511584
   Luo Q, 2021, VISUAL COMPUT, V37, P1899, DOI 10.1007/s00371-020-01951-0
   Maddalena L., 2015, CORR ARXIV150604051
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nie FP, 2012, IEEE DATA MINING, P566, DOI 10.1109/ICDM.2012.160
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Parekh A, 2016, IEEE SIGNAL PROC LET, V23, P493, DOI 10.1109/LSP.2016.2535227
   Peng C, 2020, INFORM SCIENCES, V513, P581, DOI 10.1016/j.ins.2019.09.074
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Sun WW, 2020, IEEE GEOSCI REMOTE S, V17, P107, DOI 10.1109/LGRS.2019.2915315
   Tang G., 2011, P ANN C INF SCI SYST, P1, DOI DOI 10.1109/CISS.2011.5766144
   Tu ZG, 2017, PATTERN RECOGN, V72, P285, DOI 10.1016/j.patcog.2017.07.028
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xie Q, 2018, IEEE T PATTERN ANAL, V40, P1888, DOI 10.1109/TPAMI.2017.2734888
   Xu Y., 2013, ARXIV
   Xue ZC, 2019, VISUAL COMPUT, V35, P1549, DOI 10.1007/s00371-018-1555-1
   Yang JH, 2020, APPL MATH MODEL, V81, P711, DOI 10.1016/j.apm.2020.01.039
   Yang JH, 2020, APPL MATH COMPUT, V367, DOI 10.1016/j.amc.2019.124783
   Zhang H., 2011, ARXIV, DOI DOI 10.48550/ARXIV.1112.3946
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
NR 49
TC 1
Z9 1
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5849
EP 5867
DI 10.1007/s00371-022-02699-5
EA NOV 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000877416900001
DA 2024-07-18
ER

PT J
AU Qin, WN
   Wen, HR
   Li, F
AF Qin, Wenning
   Wen, Haoran
   Li, Feng
TI Fabric defect detection algorithm based on residual energy distribution
   and Gabor feature fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect detection; Residual energy distribution; Gabor feature
   fusion; Signal-to-noise ratio
ID INSPECTION
AB Gabor filter is a time-frequency combined analysis method, which is suitable for detecting local anomalies in periodic textures. Gabor-based methods mainly include the optimal channel method and multi-channel fusion method. Compared with the optimal channel method, the multi-channel fusion method can obtain more complete image features and has advantages in detecting mixed and isotropic defects. However, the multi-channel fusion method has the problem of feature redundancy and poor anti-noise ability, which reduces the effect of the algorithm. Therefore, a novel fabric defect detection algorithm based on residual energy distribution and Gabor feature fusion is proposed. First, we use a relatively complete bank of Gabor filters to extract the testing and template image features under different channels and calculate the residual energy between them. Then we use the max-to-mean ratio (MMR) metric to measure the saliency of each channel's defect features and use nonlinear normalization to calculate the weight of each channel. Finally, fuse the multi-channel Gabor features according to the weights. In addition, we optimize the parameters using the signal-to-noise ratio (SNR) indicator and genetic algorithm. Experiments show that the proposed algorithm has advantages over the current state-of-the-art defect detection algorithms.
C1 [Qin, Wenning; Wen, Haoran; Li, Feng] Donghua Univ, Sch Comp Sci & Technol, Shanghai 201600, Peoples R China.
C3 Donghua University
RP Wen, HR (corresponding author), Donghua Univ, Sch Comp Sci & Technol, Shanghai 201600, Peoples R China.
EM 2202417@mail.dhu.edu.cn; 2202404@mail.dhu.edu.cn; lifeng@dhu.edu.cn
OI Li, Feng/0000-0001-6128-3663
CR Amirkhani A, 2022, VISUAL COMPUT, V38, P1929, DOI 10.1007/s00371-021-02256-6
   Bao X., 2021, LOW RANK DECOMPOSITI
   Chang XZ, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/3709821
   Cruz AC, 2015, PATTERN RECOGN LETT, V52, P40, DOI 10.1016/j.patrec.2014.10.001
   Hu CS, 2011, ANN FOREST
   Imamura A., 2021, GABOR FILTER INCORPO
   Jikun Zhou, 2015, 2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC). Proceedings, P224, DOI 10.1109/IHMSC.2015.242
   Jing JF, 2014, J IND TEXT, V44, P40, DOI 10.1177/1528083713490002
   Kurt M, 2021, VISUAL COMPUT, V37, P307, DOI 10.1007/s00371-020-01800-0
   Li CL, 2019, IEEE ACCESS, V7, P83962, DOI 10.1109/ACCESS.2019.2925196
   Li F, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3067221
   Li F, 2020, TEXT RES J, V90, P776, DOI 10.1177/0040517519879904
   Li Yihong, 2020, 2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), P788, DOI 10.1109/ICPICS50287.2020.9202242
   Li YY, 2019, J TEXT I, V110, P487, DOI 10.1080/00405000.2018.1489951
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Ng MK, 2014, IEEE T AUTOM SCI ENG, V11, P943, DOI 10.1109/TASE.2014.2314240
   Rasheed A, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/8189403
   Tsang CSC, 2016, PATTERN RECOGN, V51, P378, DOI 10.1016/j.patcog.2015.09.022
   Wu Y., 2021, J SHANGHAI JIAOTONG, V26, P231
   Xu YY, 2021, MEASUREMENT, V178, DOI 10.1016/j.measurement.2021.109316
   Yang H, 2019, PATTERN RECOGN, V91, P345, DOI 10.1016/j.patcog.2019.03.003
   Zhang ZH, 2020, IEEE T IND INFORM, V16, P6787, DOI 10.1109/TII.2020.2972290
NR 23
TC 3
Z9 3
U1 11
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5971
EP 5985
DI 10.1007/s00371-022-02706-9
EA OCT 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000875523800001
DA 2024-07-18
ER

PT J
AU Liu, X
   Wang, MH
   Wang, AZ
   Liu, SS
   Pi, XY
AF Liu, Xia
   Wang, Minghui
   Wang, Anzhi
   Liu, Shanshan
   Pi, Xinyu
TI Light field reconstruction via attention maps of hybrid networks
SO VISUAL COMPUTER
LA English
DT Article
DE Light field; Reconstruction; Angular super-resolution; Convolutional
   neural network
AB Compared with hardware-dependent methods, light field (LF) reconstruction algorithms enable requiring the densely sampled light field (DSLF) more economical and convenient. Most of the current LF reconstruction methods either directly apply multiple convolutional layers to the input views to generate DSLF or warp the input views to novel viewpoints based on the estimated depth and then blend to obtain the DSLF. These two types of methods produce either blurry results in texture areas or distortions near the boundaries of depth discontinuities. In this paper, we propose an end-to-end learning-based approach, which combines the characteristics of the above two methods from a parallel and complementary perspective, to reconstruct a high-quality light field. Our method consists of three sub-networks, i.e., two parallel sub-networks termed as ASR-Net and Warp-Net, and the refine sub-network termed as Refine-Net. ASR-Net directly learns an intermediate DSLF through deep convolutional layers, and Warp-Net warps the input views based on the estimated depth to obtain other intermediate DSLFs that maintain high-frequency texture information. These intermediate DSLFs are adaptively fused through the learned attention maps, and a fusion DSLF with the advantages of two types of intermediate DSLFs is obtained. Finally, the residual map learned from the intermediate DSLFs is added to the fusion DSLF to get a better one. Comprehensive experiments demonstrate the superiority of the proposed method on several LF datasets compared with the state-of-the-art approaches.
C1 [Liu, Xia; Wang, Minghui; Liu, Shanshan; Pi, Xinyu] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
   [Wang, Anzhi] Guizhou Normal Univ, Sch Big Data & Comp Sci, Guiyang, Peoples R China.
C3 Sichuan University; Guizhou Normal University
RP Wang, MH (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
EM wangminghui@scu.edu.cn
FU National Key Research and Development Program of China [2016YFB0700802];
   Innovative Youth Fund Program of the State Oceanic Administration of
   China [2015001]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB0700802 and in part by
   the Innovative Youth Fund Program of the State Oceanic Administration of
   China under Grant 2015001.
CR [Anonymous], The (New) Stanford Light Field Archive
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524
   Fiss J, 2014, IEEE INT CONF COMPUT
   Gul MSK, 2018, IEEE T IMAGE PROCESS, V27, P2146, DOI 10.1109/TIP.2018.2794181
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jin J., 2021, ARXIV
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Jin J, 2022, IEEE T PATTERN ANAL, V44, P1819, DOI 10.1109/TPAMI.2020.3026039
   Jones A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276427
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kingma D. P., 2014, arXiv
   Kubota A, 2007, IEEE T IMAGE PROCESS, V16, P269, DOI 10.1109/TIP.2006.884938
   Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Li R., 2021, LIGHT FIELD RECONSTR
   Liu X, 2022, VISUAL COMPUT, V38, P2839, DOI 10.1007/s00371-021-02159-6
   Long J., 2014, ADV NEURAL INF PROCE, V57, P601
   Lytro, US
   Meng N., 2021, IEEE T IMAGE PROCESS, V5, P1
   Meng N, 2021, IEEE T PATTERN ANAL, V43, P873, DOI 10.1109/TPAMI.2019.2945027
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Navarro J, 2021, PATTERN ANAL APPL, V24, P1319, DOI 10.1007/s10044-021-00956-2
   Raj A. S., Stanford lytro light field archive
   Raytrix, About us
   Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Sun X, 2016, IEEE IJCNN, P367, DOI 10.1109/IJCNN.2016.7727222
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vijayanarasimhan S, 2017, ARXIV
   Wang YL, 2018, LECT NOTES COMPUT SC, V11206, P340, DOI 10.1007/978-3-030-01216-8_21
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wu G., 2019, ARXIV
   Wu G., 2020, ARXIV
   Wu G., 2017, IEEE C COMPUTER VISI
   Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463
   Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yeung HWF, 2018, LECT NOTES COMPUT SC, V11210, P138, DOI 10.1007/978-3-030-01231-1_9
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZT, 2015, PROC CVPR IEEE, P3800, DOI 10.1109/CVPR.2015.7299004
   Zhou WH, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103874
   Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050
   Zhussip M, 2019, PROC CVPR IEEE, P10247, DOI 10.1109/CVPR.2019.01050
NR 54
TC 2
Z9 2
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5027
EP 5040
DI 10.1007/s00371-022-02644-6
EA OCT 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000871518200001
DA 2024-07-18
ER

PT J
AU Zhong, YY
   Jia, T
   Xi, KQ
   Li, WH
   Chen, DY
AF Zhong, Yangyang
   Jia, Tong
   Xi, Kaiqi
   Li, Wenhao
   Chen, Dongyue
TI Dual-stream stereo network for depth estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Depth estimation; Convolutional neural network; Dual-stream network;
   Stereo matching
AB Depth-estimation is an important task for autonomous driving, 3D object detection and recognition, scene understanding, and other fields. To improve the quality of depth estimation in high-frequency edge detail area, this paper presents DSS-Net, a dual-stream stereo network, combining a bottom-up steam based on scene understanding and a top-down stream based on parallax local optimization. Firstly, in the bottom-up stream, deep features containing high-level semantic information are extracted by a deep network, and then, a coarse estimate of the disparity is computed according to depth intervals classified based on deep features. In the up-bottom stream, the model uses the high-resolution shallow features with rich details and the initial coarse disparity to construct the local dense matching cost in the parallax neighborhood of each pixel of the initial disparity map, and uses stacked multiple hourglass networks to refine the parallax diagram in several stages. We achieve results on the Scene Flow, KITTI 2012 and 2015 datasets, showing that our method has high precision.
C1 [Zhong, Yangyang; Jia, Tong; Xi, Kaiqi; Li, Wenhao; Chen, Dongyue] Northeastern Univ, Coll Informat Sci & Engn, Shenyang, Peoples R China.
   [Jia, Tong] Northeastern Univ, Key Lab Data Analyt & Optimizat Smart Ind, Minist Educ, Shenyang, Peoples R China.
C3 Northeastern University - China; Northeastern University - China
RP Jia, T (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang, Peoples R China.; Jia, T (corresponding author), Northeastern Univ, Key Lab Data Analyt & Optimizat Smart Ind, Minist Educ, Shenyang, Peoples R China.
EM jiatong@ise.neu.edu.cn
FU National Natural Science Foundation of China [62173083]; Major Program
   of National Natural Science Foundation of China [71790614]; 111 Project
   [B16009]
FX Research is supported by the National Natural Science Foundation of
   China under Grant 62173083 and is partly supported by the Major Program
   of National Natural Science Foundation of China (71790614) and the 111
   Project (B16009).
CR [Anonymous], 2002, Proceedings of the 15th International Conference on Neural Information Processing Systems
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen XZ, 2015, ADV NEUR IN, V28
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Eigen D., 2014, arXiv
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Frank Eibe, 2001, EUR C MACH LEARN, P145, DOI 10.1007/3-540-44795-413
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A., 2012, CVPR
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Kunii Y, 2008, IEEE ASME INT C ADV, P302, DOI 10.1109/AIM.2008.4601677
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li Z, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534001
   LUO WJ, 2016, PROC CVPR IEEE, P5695, DOI DOI 10.1109/CVPR.2016.614
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   MENZE M, 2015, PROC CVPR IEEE, P3061, DOI DOI 10.1109/CVPR.2015.7298925
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Shen ZL, 2021, PROC CVPR IEEE, P13901, DOI 10.1109/CVPR46437.2021.01369
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wu W, 2018, ARXIV
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Zagoruyko S., 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhu ZD, 2019, C IND ELECT APPL, P1789, DOI [10.1109/iciea.2019.8834193, 10.1109/ICIEA.2019.8834193]
NR 35
TC 0
Z9 0
U1 5
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5343
EP 5357
DI 10.1007/s00371-022-02663-3
EA OCT 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000865913700001
DA 2024-07-18
ER

PT J
AU Wang, XY
   Chen, X
AF Wang, Xingyuan
   Chen, Xuan
TI Image encryption algorithm based on cross-scrambling and rapid-mode
   diffusion
SO VISUAL COMPUTER
LA English
DT Article
DE Two-dimensional chaotic map; Image encryption; Cross-scrambling;
   Diffusion; Security evaluation
ID SEMI-TENSOR PRODUCT; MATRIX; CHAOS; MAP; ENTROPY
AB An efficient encryption algorithm largely depends on the chaotic performance of chaotic map. The stronger the chaotic performance of chaotic map, the higher the security of the algorithm. For the purpose of improving the chaotic performance, we put forward a novel two-dimensional Sine-embedded-coupling map (2D-SECM) in this paper. By comparing with other 2D chaotic maps, 2D-SECM has more excellent pseudo-random characteristics, unpredictability, and wider range of chaos. For the purpose of studying its application, we put forward an image encryption approach based on 2D-SECM (SECM-IEA). SECM-IEA includes bit-level scrambling based on cross-transform and a round of fast diffusion processing. Simulation experiment and security evaluation reveal that SECM-IEA can resist common types of attacks and has more excellent security than some algorithms. It is demonstrated that SECM has better chaotic performance from the perspective of encryption performance.
C1 [Wang, Xingyuan; Chen, Xuan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Wang, Xingyuan] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
C3 Dalian Maritime University; Guangxi Normal University
RP Wang, XY; Chen, X (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.; Wang, XY (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
EM xywang@dlmu.edu.cn; cx125829@163.com
OI Chen, Xuan/0000-0002-0628-3633
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key R&D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 universities'
   Funding Projects Introducing Innovation Team Program [2019GXRC031];
   Research Fund of Guangxi Key Lab of Multi-source Information Mining
   Security [MIMS20-M-02]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103), Jinan City '20 universities' Funding Projects
   Introducing Innovation Team Program (No: 2019GXRC031), Research Fund of
   Guangxi Key Lab of Multi-source Information Mining & Security (No:
   MIMS20-M-02).
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Bhattacharjee T, 2022, MULTIMED TOOLS APPL, V81, P18755, DOI 10.1007/s11042-022-12451-9
   Cheng SS, 2021, INT J BIFURCAT CHAOS, V31, DOI 10.1142/S021812742150125X
   Devi RS, 2019, INT J THEOR PHYS, V58, P1937, DOI 10.1007/s10773-019-04088-6
   Ding Y, 2023, VISUAL COMPUT, V39, P1517, DOI 10.1007/s00371-022-02426-0
   Dou YQ, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062187
   Gao XH, 2021, OPT LASER TECHNOL, V142, DOI 10.1016/j.optlastec.2021.107252
   Hosny KM, 2023, VISUAL COMPUT, V39, P1027, DOI 10.1007/s00371-021-02382-1
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P505, DOI 10.1007/s11042-021-11384-z
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Kang XJ, 2019, IEEE T CIRC SYST VID, V29, P1595, DOI 10.1109/TCSVT.2018.2851983
   Kaur M, 2022, SOFT COMPUT, V26, P2689, DOI 10.1007/s00500-021-06423-8
   Khan JS, 2021, J INF SECUR APPL, V58, DOI 10.1016/j.jisa.2020.102711
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li TY, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23050510
   Li T, 2020, IEEE ACCESS, V8, P13792, DOI 10.1109/ACCESS.2020.2966264
   Li XL, 2007, EPILEPSY RES, V77, P70, DOI 10.1016/j.eplepsyres.2007.08.002
   Liu CY, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/3837209
   Mansouri A, 2021, MULTIMED TOOLS APPL, V80, P21955, DOI 10.1007/s11042-021-10757-8
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Richman JS, 2000, AM J PHYSIOL-HEART C, V278, pH2039
   Shi YD, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23101297
   Skokos C, 2016, LECT NOTES PHYS, V915, P221, DOI 10.1007/978-3-662-48410-4_7
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Wang XY, 2021, INFORM SCIENCES, V579, P128, DOI 10.1016/j.ins.2021.07.096
   Wang XY, 2021, OPT LASER TECHNOL, V143, DOI 10.1016/j.optlastec.2021.107316
   Wang XY, 2021, CHAOS SOLITON FRACT, V150, DOI 10.1016/j.chaos.2021.111117
   Wang XY, 2021, CHAOS SOLITON FRACT, V147, DOI 10.1016/j.chaos.2021.110962
   Wang XY, 2021, MULTIMED TOOLS APPL, V80, P591, DOI 10.1007/s11042-020-09688-7
   Wang XY, 2020, INFORM SCIENCES, V539, P195, DOI 10.1016/j.ins.2020.06.030
   Wang XY, 2020, INFORM SCIENCES, V507, P16, DOI 10.1016/j.ins.2019.08.041
   Wang XY, 2012, SIGNAL PROCESS, V92, P1101, DOI 10.1016/j.sigpro.2011.10.023
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Xian YJ, 2021, INFORM SCIENCES, V547, P1154, DOI 10.1016/j.ins.2020.09.055
   Yang CH, 2022, SOFT COMPUT, V26, P1727, DOI 10.1007/s00500-022-06745-1
   Zhang SJ, 2021, MATH COMPUT SIMULAT, V190, P723, DOI 10.1016/j.matcom.2021.06.012
   Zhang X, 2021, MULTIMED TOOLS APPL, V80, P8809, DOI 10.1007/s11042-020-09465-6
   Zheng JY, 2020, IET IMAGE PROCESS, V14, P2310, DOI 10.1049/iet-ipr.2019.1340
   Zhu HG, 2021, MATH COMPUT SIMULAT, V185, P754, DOI 10.1016/j.matcom.2021.02.009
   Zhu HG, 2019, IEEE ACCESS, V7, P14081, DOI 10.1109/ACCESS.2019.2893538
   Zhu SQ, 2019, IEEE ACCESS, V7, P147106, DOI 10.1109/ACCESS.2019.2946208
NR 43
TC 2
Z9 2
U1 6
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5041
EP 5068
DI 10.1007/s00371-022-02645-5
EA AUG 2022
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000842597900001
DA 2024-07-18
ER

PT J
AU Singh, MK
   Saini, I
   Sood, N
AF Singh, Mayank Kumar
   Saini, Indu
   Sood, Neetu
TI Log exponential shrinkage: a denoising technique for breast ultrasound
   images
SO VISUAL COMPUTER
LA English
DT Article
DE Breast cancer; Clinical diagnosis; Ultrasound; Speckle noise; Noise
   variance estimator; Wavelet thresholding
ID ALGORITHM; REMOVAL
AB The process of uncontrolled fatal growth of tissue that invades its surrounding parts is called cancer. The cancer in breast is the most commonly encountered cancer among women. It accounts for millions of deaths around the globe. Early detection of breast cancer by ultrasound (US)-based diagnosis can have major positive influence on life expectancy of a patient. Unlike other diagnosis methods, the US-based diagnosis is harmless, economical, and allows frequent use. Only limitation it has is the presence of noise. So, we propose an US denoising technique named log exponential shrinkage (LES). In LES, US image was initially processed for additive noise. A scale-based thresholding function and a novel speckle noise variance estimator are the main contribution of this study. The estimator utilizes sub-band coefficients of a transformed image to evaluate the noise intensity. The threshold to filter out the noisy coefficient was calculated using the noise variance. Ultimately, the thresholding exponential function was used to modify noisy coefficients. The efficiency of LES was compared with other denoising techniques in terms of no-reference, full reference quality metrics and edge preservative metrics. The performance was tested on one synthetic image and two breast cancer datasets. To comprehend the extent of advancement by LES, a percentage improvement table has been provided in the study, having 300% better results. The novel estimator has improved the error in estimation by 95.8% at the optimal noise intensity. With such promising results, LES might find application in breast cancer diagnosis using US.
C1 [Singh, Mayank Kumar; Saini, Indu; Sood, Neetu] Dr BR AmbedkarNatl Inst Technol, Dept Elect & Commun Engn, Jalandhar, Punjab, India.
C3 National Institute of Technology (NIT System); Dr B R Ambedkar National
   Institute of Technology Jalandhar
RP Singh, MK (corresponding author), Dr BR AmbedkarNatl Inst Technol, Dept Elect & Commun Engn, Jalandhar, Punjab, India.
EM mayankks.ec.19@nitj.ac.in; sainii@nitj.ac.in; soodn@nitj.ac.in
RI Saini, Indu/P-6034-2015; Sood, Neetu/X-8624-2019
OI Saini, Indu/0000-0003-3983-913X; Sood, Neetu/0000-0001-7475-8672
CR ABDOU IE, 1979, P IEEE, V67, P753, DOI 10.1109/PROC.1979.11325
   Achim A, 2001, IEEE T MED IMAGING, V20, P772, DOI 10.1109/42.938245
   Ahmad N, 2022, VISUAL COMPUT, V38, P2751, DOI 10.1007/s00371-021-02153-y
   Al-Dhabyani W, 2020, DATA BRIEF, V28, DOI 10.1016/j.dib.2019.104863
   Anaya-Isaza A, 2021, IEEE ACCESS, V9, P152206, DOI 10.1109/ACCESS.2021.3127862
   Andria G, 2013, IEEE T INSTRUM MEAS, V62, P2270, DOI 10.1109/TIM.2013.2255978
   Arce GR, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P263, DOI 10.1016/B978-0-12-374457-9.00012-3
   Chen HB, 2021, PHYS ENG SCI MED, V44, P207, DOI 10.1007/s13246-020-00969-x
   da Silva EAB, 2005, ELECTRICAL ENGINEERING HANDBOOK, P891, DOI 10.1016/B978-012170960-0/50064-5
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626
   Dutt V, 1995, ULTRASONIC IMAGING, V17, P251, DOI 10.1006/uimg.1995.1012
   Elyasi I, 2016, MEASUREMENT, V91, P55, DOI 10.1016/j.measurement.2016.05.025
   Febin IP, 2022, VISUAL COMPUT, V38, P1413, DOI 10.1007/s00371-021-02076-8
   Gupta N, 2005, IEEE T MED IMAGING, V24, P743, DOI 10.1109/TMI.2005.847401
   Jarosik P, 2021, IEEE INT ULTRA SYM, DOI 10.1109/IUS52206.2021.9593591
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Kokil P, 2020, COMPUT METH PROG BIO, V194, DOI 10.1016/j.cmpb.2020.105477
   Kumar A, 2022, P I MECH ENG H, V236, P12, DOI 10.1177/09544119211039317
   Lin CH, 2010, J DIGIT IMAGING, V23, P246, DOI 10.1007/s10278-008-9172-6
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mukherjee S., 2019, INT C SMART MULT, P87
   Mulmule PV, 2023, VISUAL COMPUT, V39, P2381, DOI 10.1007/s00371-022-02463-9
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Prabusankarlal K. M., 2018, Applied Computing and Informatics, V14, P48, DOI 10.1016/j.aci.2017.01.002
   Randhawa SK, 2019, MULTIDIM SYST SIGN P, V30, P1545, DOI 10.1007/s11045-018-0616-y
   Rodrigues C, 2019, IEEE LAT AM T, V17, P1800, DOI 10.1109/TLA.2019.8986417
   ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267
   Sathish D, 2019, VISUAL COMPUT, V35, P57, DOI 10.1007/s00371-017-1447-9
   Shui PL, 2005, IEEE SIGNAL PROC LET, V12, P681, DOI 10.1109/LSP.2005.855555
   Singh BK, 2016, INT J BIOMED ENG TEC, V20, DOI 10.1504/IJBET.2016.074197
   Singh BK, 2015, IETE TECH REV, V32, P384, DOI 10.1080/02564602.2015.1019943
   Singh Prateek Kumar, 2021, ISH Journal of Hydraulic Engineering, V27, P23, DOI 10.1080/09715010.2018.1505562
   Stanke L, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185301
   Synnevåg JF, 2007, ULTRASON, P1545, DOI 10.1109/ULTSYM.2007.389
   Venkatanath N, 2015, NATL CONF COMMUN
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xu HP, 2023, VISUAL COMPUT, V39, P99, DOI 10.1007/s00371-021-02316-x
   Yap MH, 2008, J APPL CLIN MED PHYS, V9, P181, DOI 10.1120/jacmp.v9i4.2741
   Young G.A., 2019, INT STAT REV, V87, P178, DOI [10.1111/insr.12315, DOI 10.1111/INSR.12315]
   Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081
   Zhang XM, 2009, IEEE SIGNAL PROC LET, V16, P295, DOI 10.1109/LSP.2009.2014293
   Zhao JW, 2018, VISUAL COMPUT, V34, P1677, DOI 10.1007/s00371-017-1441-2
   Zhou YY, 2019, BIOMED SIGNAL PROCES, V48, P104, DOI 10.1016/j.bspc.2018.09.011
NR 48
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4901
EP 4914
DI 10.1007/s00371-022-02636-6
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000839465000001
DA 2024-07-18
ER

PT J
AU Qian, YL
   Shi, J
   Sun, HQ
   Chen, YY
   Wang, Q
AF Qian, Yinling
   Shi, Jian
   Sun, Hanqiu
   Chen, Yanyun
   Wang, Qiong
TI Vector solid texture synthesis using unified RBF-based representation
   and optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Vector texture; Solid texture; Texture modeling
AB Solid textures are essential for modeling virtual internal materials. Existing approaches either generate raster solid textures or only focus on vector representation. To facilitate efficient synthesis and intuitive editing of vector solid texture, we propose the novel solid texture representation, named radial basis function (RBF) solid texture. An RBF solid texture consists of a set of spatially distributed RBF instances. Each RBF instance encapsulates a 3D position, an RGB color and a signed distance field (SDF) value. Such a representation is resolution independent, compact in storage and capable of supporting efficient random access with an indexing uniform grid. We directly synthesize RBF solid texture from raster exemplar by minimizing an energy function, which encodes the position, color and SDF difference between output volumetric RBF instances and input example planar RBF instances. The minimization process iteratively updates output RBF instances with an EM algorithm. Our experiments show that our algorithm can produce RBF solid textures in high efficiency and compact storage for a variety of exemplars, including stochastic patterns or more structured patterns. Furthermore, RBF solid textures we proposed benefit intuitive editing for either region-based and RBF-based effects.
C1 [Qian, Yinling; Wang, Qiong] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen, Peoples R China.
   [Shi, Jian] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
   [Sun, Hanqiu] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Chen, Yanyun] Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; Institute of Software, CAS
RP Wang, Q (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Shenzhen, Peoples R China.
EM wangqiong@siat.ac.cn
OI Qian, Yinling/0000-0002-3982-5021
FU Key-Area Research and Development Program of Guangdong Province, China
   [2019B010149002, 2020B010165004]; National Natural Science Foundation of
   China [62072452, 61802386]; Natural Science Foundation of Guangdong
   Province [2020A1515010357, 2021A1515011869]; Shenzhen Science and
   Technology Program [JCYJ20200109115627045, JCYJ20200109114233670,
   JCYJ20180507182410327]
FX This work was supported by Key-Area Research and Development Program of
   Guangdong Province, China (2019B010149002, 2020B010165004), National
   Natural Science Foundation of China (62072452, 61802386), Natural
   Science Foundation of Guangdong Province (2020A1515010357,
   2021A1515011869), Shenzhen Science and Technology Program (Nos.
   JCYJ20200109115627045, JCYJ20200109114233670, JCYJ20180507182410327).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 2009, Proceedings of the 7th International Symposium on Non-Photorealistic Animation and Rendering (NPAR), DOI DOI 10.1145/1572614.1572623
   Chen JT, 2010, VISUAL COMPUT, V26, P253, DOI 10.1007/s00371-009-0408-3
   Chen K., 2013, P ACM SIGGRAPH S INT, P145
   Chen YS, 2004, VISUAL COMPUT, V20, P650, DOI 10.1007/s00371-004-0262-2
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   Dischler JM, 1998, COMPUT GRAPH FORUM, V17, pC87, DOI 10.1111/1467-8659.00256
   Dong Y, 2008, COMPUT GRAPH FORUM, V27, P1165, DOI 10.1111/j.1467-8659.2008.01254.x
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P460, DOI 10.1109/TVCG.2012.129
   GHAZANFARPOUR D, 1995, COMPUT GRAPH-UK, V19, P413, DOI 10.1016/0097-8493(95)00011-Z
   Ghazanfarpour D, 1996, COMPUT GRAPH FORUM, V15, pC311, DOI 10.1111/1467-8659.1530311
   Gilet G, 2010, COMPUT GRAPH FORUM, V29, P1411, DOI 10.1111/j.1467-8659.2010.01738.x
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Ijiri T, 2008, COMPUT GRAPH FORUM, V27, P429, DOI 10.1111/j.1467-8659.2008.01140.x
   Jagnow R, 2004, ACM T GRAPHIC, V23, P329, DOI 10.1145/1015706.1015724
   Jagnow R, 2008, ACM T APPL PERCEPT, V4, DOI 10.1145/1278760.1278765
   Kopf J., 2007, ACM T GRAPHIC, P2, DOI [10.1145/1276377.1276380, DOI 10.1145/1239451.1239453]
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Liu AJ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980255
   Ma CY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461921
   Ma CY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964957
   Palacios J., 2016, SIGGRAPH ASIA 2016 T, P18
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Pietroni N, 2007, COMPUT GRAPH FORUM, V26, P637, DOI 10.1111/j.1467-8659.2007.01087.x
   Pietroni N, 2010, IEEE COMPUT GRAPH, V30, P74, DOI 10.1109/MCG.2009.153
   Qian Y., 2015, P 21 ACM S VIRTUAL R, P27
   Qin XJ, 2007, IEEE T VIS COMPUT GR, V13, P379, DOI 10.1109/TVCG.2007.31
   Seo MK, 2014, VISUAL COMPUT, V30, P271, DOI 10.1007/s00371-013-0843-z
   Shu Y, 2014, VISUAL COMPUT, V30, P877, DOI 10.1007/s00371-014-0951-4
   Takayama K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866202
   Takayama K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360652
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Urs RD, 2014, IEEE T IMAGE PROCESS, V23, P1820, DOI 10.1109/TIP.2014.2307477
   Wang LD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778823
   Wang LD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024201
   Wei L.-Y., 2003, ACM SIGGRAPH 2003 SK, P1, DOI DOI 10.1145/965400.965507
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang GX, 2013, GRAPH MODELS, V75, P104, DOI 10.1016/j.gmod.2012.10.006
   Zhang GX, 2011, GRAPH MODELS, V73, P59, DOI 10.1016/j.gmod.2010.10.006
   Zhang H, 2017, COMPUT AIDED GEOM D, V52-53, P285, DOI 10.1016/j.cagd.2017.03.015
   Zhao X., ARXIV210203973 2021
NR 45
TC 1
Z9 1
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3963
EP 3977
DI 10.1007/s00371-022-02541-y
EA JUL 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000819264000001
DA 2024-07-18
ER

PT J
AU Dashti, S
   Prakash, E
   Navarro-Newball, AA
   Hussain, F
   Carroll, F
AF Dashti, Sarah
   Prakash, Edmond
   Navarro-Newball, Andres Adolfo
   Hussain, Fiaz
   Carroll, Fiona
TI PotteryVR: virtual reality pottery
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual pottery; Usability; Evaluation; Interaction; Methods; Creative
   technology
ID ENVIRONMENT; CREATIVITY; GAME
AB Handcrafting ceramic pottery in the traditional method or virtual reality (VR) with intricate surface details is still challenging for the ceramic and graphic artist. Free-form pottery modeling can be efficiently geometrically modeled with the right tools with detailed 3D print outputs, yet challenging to be manufactured using traditional art. The new advanced pottery VR simulation is a promising method to recreate the traditional pottery simulation for a better experience with some barriers. The challenges that arise from surface detail in pottery are a tedious task accomplished by mesh blending and retopology. This paper focuses on refining the VP application's performance by adding unique sound resonance as a more likely infinite geometric phenomenon textures, blending it into the basic shapes. This paper combines creativity and visual computing technologies such as VR, mesh blending, fixing errors, and 3D printing to bring the ceramic artist's imagination to life. We have used sound resonance with virtual pottery (VP) systems refinements to demonstrate several standard pottery methods from free form deformed pottery, retopology, mesh blended for surface details, and 3D printed pottery with materials including polymer and ceramic resins.
C1 [Dashti, Sarah] Cardiff Metropolitan Univ, Creat Technol, Cardiff, Wales.
   [Hussain, Fiaz] Cardiff Metropolitan Univ, Cardiff Sch Arts & Design, Cardiff, Wales.
   [Carroll, Fiona] Cardiff Metropolitan Univ, Creat Comp Res Ctr, Cardiff, Wales.
   [Prakash, Edmond] Univ Creat Arts, Ctr Creat Technol, Farnham, Surrey, England.
   [Navarro-Newball, Andres Adolfo] Pontificia Univ Javeriana, Comp Sci, Cali, Colombia.
C3 Cardiff Metropolitan University; Cardiff Metropolitan University;
   Cardiff Metropolitan University; University for the Creative Arts - UK;
   Pontificia Universidad Javeriana
RP Dashti, S (corresponding author), Cardiff Metropolitan Univ, Creat Technol, Cardiff, Wales.
EM klam2006@hotmail.com; Eprakash@gmail.com; ANavarro@javerianacali.edu.co;
   FHussain@cardiffmet.ac.uk; FCarroll@cardiffmet.ac.uk
RI Carroll, Fiona/GRN-7895-2022; Hussain, Muhammad Fiaz/KFA-1166-2024
OI Carroll, Fiona/0000-0002-9967-2207; Dashti, Dr.
   Sarah/0000-0001-8963-5245
CR Navarro-Newball AA, 2016, MULTIMED TOOLS APPL, V75, P11699, DOI 10.1007/s11042-015-2667-5
   Amabile TM, 2012, J CREATIVE BEHAV, V46, P3, DOI 10.1002/jocb.001
   Amabile TM, 1996, ACAD MANAGE J, V39, P1154, DOI 10.5465/256995
   Arango JSM., 2016, P 2016 VIRT REAL INT, P1
   Ashdown N., 1996, VIRTUAL REALITY MODE
   Banerjee A, 2017, PEDIATR ONCOL-BERLIN, P1, DOI 10.1007/978-3-319-30789-3_1
   Biswas R, 2015, VISUAL COMPUT, V31, P787, DOI 10.1007/s00371-015-1101-3
   Bowman DA, 2001, PRESENCE-TELEOP VIRT, V10, P75, DOI 10.1162/105474601750182333
   Cai RF, 2021, COMPUT IND, V124, DOI 10.1016/j.compind.2020.103325
   Casu A, 2015, SMART TOOLS APPS GRA, P77, DOI [10.2312/stag.20151294, DOI 10.2312/STAG.20151294]
   Chang YS, 2022, EDUC STUD-UK, V48, P341, DOI 10.1080/03055698.2020.1754767
   Chiang PY, 2018, IEEE COMPUT GRAPH, V38, P74, DOI 10.1109/MCG.2018.021951634
   Cho S., 2012, ASIL INSIGHT NO MORE, P1
   Dashti S, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P25, DOI 10.1109/CW52790.2021.00012
   Dashti S, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P133, DOI 10.1109/CW49994.2020.00028
   de Klerk R, 2019, AUTOMAT CONSTR, V103, P104, DOI 10.1016/j.autcon.2019.03.009
   Devendorf Laura., 2015, Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, P555
   Ellis G., 2006, P BELIV 2006 AVI WOR, P1, DOI DOI 10.1145/1168149.1168152
   Eschenbrenner B, 2008, J DATABASE MANAGE, V19, P91, DOI 10.4018/jdm.2008100106
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   Fulvio JM, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16161-3
   Gao ZH, 2019, MULTIMED TOOLS APPL, V78, P26569, DOI 10.1007/s11042-019-07843-3
   Gao ZH, 2018, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2018), P126, DOI 10.1145/3198910.3234659
   Graessler I., 2019, Proceedings of the International Conference on Engineering Design, ICED, 2019-Augus, P2011, DOI DOI 10.1017/DSI.2019.207
   Guan JQ, 2023, INTERACT LEARN ENVIR, V31, P2016, DOI 10.1080/10494820.2021.1871631
   Huang HF, 2022, INTERACT LEARN ENVIR, V30, P848, DOI 10.1080/10494820.2019.1691605
   Jia JY, 2004, VISUAL COMPUT, V20, P457, DOI 10.1007/s00371-004-0252-4
   Kolivandet al H., 2020, SMART CITIES REVISED, P233
   Lee JH, 2021, INT J FASH DES TECHN, V14, P48, DOI 10.1080/17543266.2020.1858350
   [林莹莹 Lin Yingying], 2020, [图学学报, Journal of Graphics], V41, P57
   Lin YJ, 2021, EDUC INF TECHNOL, V26, P4487, DOI 10.1007/s10639-021-10472-9
   Liu YS, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P157, DOI 10.1109/CW49994.2020.00034
   Liu YS, 2005, VISUAL COMPUT, V21, P915, DOI 10.1007/s00371-005-0306-2
   Liverani A, 2004, VISUAL COMPUT, V20, P554, DOI 10.1007/s00371-004-0258-y
   Lteif Fayez Chris, 2021, Human Interaction, Emerging Technologies and Future Applications IV. Proceedings of the 4th International Conference on Human Interaction and Emerging Technologies: Future Applications (IHIET - AI 2021). Advances in Intelligent Systems and Computing (AISC 1378), P200, DOI 10.1007/978-3-030-74009-2_25
   Mahdjoub M, 2010, COMPUT AIDED DESIGN, V42, P402, DOI 10.1016/j.cad.2009.02.009
   Martinez D, 2010, VISUAL COMPUT, V26, P619, DOI 10.1007/s00371-010-0499-x
   MingTang Li, 2020, 2020 International Conference on Virtual Reality and Visualization (ICVRV), P352, DOI 10.1109/ICVRV51359.2020.00098
   Mu SY, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P121, DOI 10.1109/CW52790.2021.00026
   Muller Mathias, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P47, DOI 10.1007/978-3-319-39907-2_5
   Norman Don, 2010, Interactions, V17, DOI 10.1145/1836216.1836228
   Pietrowicz M., 2009, WORKSHOP COMPUTATION
   Pitkänen K, 2019, FABLEARN EUROPE 2019 - CONFERENCE ON CREATIVITY AND MAKING IN EDUCATION, DOI 10.1145/3335055.3335061
   Qin H, 2021, INT C COMP SUPP COOP, P638, DOI 10.1109/CSCWD49262.2021.9437791
   Quinn G., 2018, P IASS ANN S, P1
   Rieuf Vincent, 2015, Journal of Design Research, V13, P78, DOI 10.1504/JDR.2015.067233
   Rieuf V, 2017, DESIGN STUD, V48, P43, DOI 10.1016/j.destud.2016.11.001
   Shamsuzzoha A, 2021, INTERACT LEARN ENVIR, V29, P1339, DOI 10.1080/10494820.2019.1628072
   Shi AL, 2022, INTERACT LEARN ENVIR, V30, P721, DOI 10.1080/10494820.2019.1681467
   Smith S, 2020, INT J WORK-INTEGR L, V21, P193
   Sourin A, 2001, VISUAL COMPUT, V17, P258, DOI 10.1007/s003710100109
   Stanko-Kaczmarek M, 2012, CREATIVITY RES J, V24, P304, DOI 10.1080/10400419.2012.730003
   Thornhill-Miller B, 2016, J COGN EDUC PSYCHOL, V15, P102, DOI 10.1891/1945-8959.15.1.102
   Klock ACT, 2018, 33RD ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, P2006, DOI 10.1145/3167132.3167347
   Tschoerner B, 2021, VISUAL COMPUT, V37, P3063, DOI 10.1007/s00371-021-02251-x
   UTPLUS Interactive VR STEAM Application, 2018, DOJAGI KOREAN POTTER
   Villagrasa S., 2014, P 2 INT C TECHN EC E, P171, DOI [DOI 10.1145/2669711, 10.1145/2669711.2669896, DOI 10.1145/2669711.2669896]
   Wang WM, 2017, VISUAL COMPUT, V33, P949, DOI 10.1007/s00371-017-1386-5
   Wang WT, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI [10.1145/3290607.3312816, 10.23919/aces48530.2019.9060498]
   Yang Liu, 2017, 2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP), DOI 10.1109/RSIP.2017.7958806
   Yang XZ, 2018, ETR&D-EDUC TECH RES, V66, P1231, DOI 10.1007/s11423-018-9604-z
   Zaidi SFM, 2018, PROCEEDINGS OF THE 16TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2018), DOI 10.1145/3284398.3284431
NR 62
TC 2
Z9 2
U1 4
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4035
EP 4055
DI 10.1007/s00371-022-02521-2
EA JUN 2022
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000813592000003
PM 35756580
OA Green Published, Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Wang, GH
   Gan, X
   Cao, QC
   Zhai, QY
AF Wang, Gaihua
   Gan, Xin
   Cao, Qingcheng
   Zhai, Qianyu
TI MFANet: Multi-scale feature fusion network with attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Object detection; Multi-feature fusion; Attention
   mechanism
AB In order to improve the detection accuracy of the network, it proposes multi-scale feature fusion and attention mechanism net (MFANet) based on deep learning, which integrates pyramid module and channel attention mechanism effectively. Pyramid module is designed for feature fusion in the channel and space dimensions. Channel attention mechanism obtains feature maps in different receptive fields, which divides each feature map into two groups and uses different convolutions to obtain weights. Experimental results show that our strategy boosts state-of-the-arts by 1-2% box AP on object detection benchmarks. Among them, the accuracy of MFANet reaches 34.2% in box AP on COCO dataset. Compared with the current typical algorithms, the proposed method achieves significant performance in detection accuracy.
C1 [Wang, Gaihua; Gan, Xin; Cao, Qingcheng; Zhai, Qianyu] Hubei Univ Technol, Sch Elect & Elect Engn, Wuhan 430068, Peoples R China.
   [Wang, Gaihua] Hubei Univ Technol, Hubei Key Lab High Efficiency Utilizat Solar Ener, Wuhan 430068, Peoples R China.
C3 Hubei University of Technology; Hubei University of Technology
RP Gan, X (corresponding author), Hubei Univ Technol, Sch Elect & Elect Engn, Wuhan 430068, Peoples R China.
EM 20130006@hbut.edu.cn; 1989556916@qq.com; 2692556245@qq.com;
   zhaiqianyu233@163.com
OI Zhai, Qianyu/0000-0003-1783-4573
FU National Key RAMP;D Program of China [2017YFB1302400]
FX This work is supported in part by the National Key R&D Program of China
   under Grant 2017YFB1302400.
CR Boulgouris NV, 2006, PATTERN RECOGN, V39, P969, DOI 10.1016/j.patcog.2005.10.013
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chaudhry H., 2019, INT J COMPUTAT VIS R, V9, P228, DOI DOI 10.1504/IJCVR.2019.099435
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Daxiang Li, 2021, 2021 3rd International Conference on Natural Language Processing (ICNLP), P179, DOI 10.1109/ICNLP52887.2021.00036
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mei JT, 2021, IEEE SENS J, V21, P16915, DOI 10.1109/JSEN.2021.3078455
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sugiura M, 2015, CEREB CORTEX, V25, P2806, DOI 10.1093/cercor/bhu077
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tan M., 2020, P IEEE CVF C COMP VI
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Verschae R, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00029
   Wang GH, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-00585-z
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Xiao YZ, 2020, MULTIMED TOOLS APPL, V79, P23729, DOI 10.1007/s11042-020-08976-6
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
NR 35
TC 6
Z9 6
U1 7
U2 50
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2969
EP 2980
DI 10.1007/s00371-022-02503-4
EA MAY 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000793634200001
DA 2024-07-18
ER

PT J
AU Wu, L
   Wu, CH
   Fan, Y
   Chen, NN
AF Wu, Ling
   Wu, Conghai
   Fan, Yong
   Chen, Niannian
TI Shape reconstruction from depth gradient with artificially periodized
   boundaries
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Surface reconstruction; Gradient field; Transform-based
   integration; Boundary treatment
ID WAVE-FRONT RECONSTRUCTION; PHASE RETRIEVAL; INTEGRATION; SURFACE;
   TRANSPORT; EQUATIONS
AB For reconstructing the shape of an object from measured depth gradient field, the most popular two kinds are FFT-based integration methods and least-squares integration methods. For the former, it is time-efficient which should be attributed to the employment of FFT algorithms. However, the FFT algorithm usually implies periodic boundary which may be improper for non-periodic problems. This makes them less accurate than those least-squares integration methods. To avoid this deficiency, an algorithm is proposed by modifying the derivatives of endpoints to meet the numerical difference expression on a periodic domain. And the compact finite difference schemes are introduced to reduce the numerical error caused by the finite difference approximation of derivatives. The results of numerical tests show that the accuracy and robustness of the new non-iterative artificially-periodized-boundaries method are close to that of the state-of-art least-squares integration approach. Furthermore, its computational complexity is the same as a standard FFT-based integration approach.
C1 [Wu, Ling; Fan, Yong; Chen, Niannian] Southwest Univ Sci & Technol, Dept Comp Sci & Technol, Mianyang, Sichuan, Peoples R China.
   [Wu, Conghai] China Aerodynam Res & Dev Ctr, State Key Lab Aerodynam, Mianyang, Sichuan, Peoples R China.
C3 Southwest University of Science & Technology - China
RP Fan, Y (corresponding author), Southwest Univ Sci & Technol, Dept Comp Sci & Technol, Mianyang, Sichuan, Peoples R China.
EM wuling751@126.com; sc_fanyong@qq.com
FU National Natural Science Foundation of China [11732016]; Sichuan Science
   and Technology Program [2018JZ0076]
FX This work was supported by National Natural Science Foundation of China
   (No. 11732016) and Sichuan Science and Technology Program (No.
   2018JZ0076).
CR [Anonymous], 1997, ADV COMPUTER VISION
   Badri H, 2016, IEEE T IMAGE PROCESS, V25, P3562, DOI 10.1109/TIP.2016.2570548
   Bahr Martin, 2017, [Computational Visual Media, 计算可视媒体], V3, P107
   Bao G, 2016, INVERSE PROBL, V32, DOI 10.1088/0266-5611/32/8/085002
   Bon P, 2012, APPL OPTICS, V51, P5698, DOI 10.1364/AO.51.005698
   Durou J., 2008, P 1 INT WORKSH PHOT
   Elster C, 2000, APPL OPTICS, V39, P5353, DOI 10.1364/AO.39.005353
   FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909
   Ghiglia D.C., 1998, 2 DIMENSIONAL PHASE
   Harker M, 2008, PROC CVPR IEEE, P565
   Harker M, 2015, J MATH IMAGING VIS, V51, P46, DOI 10.1007/s10851-014-0505-4
   Horn B.K., 1989, Obtaining shape from shading information
   HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3
   Hu GF, 2006, VISUAL COMPUT, V22, P147, DOI 10.1007/s00371-006-0372-0
   Huang L, 2017, OPT LASER ENG, V91, P221, DOI 10.1016/j.optlaseng.2016.12.004
   Huang L, 2015, OPT LASER ENG, V64, P1, DOI 10.1016/j.optlaseng.2014.07.002
   Karaçali B, 2003, COMPUT VIS IMAGE UND, V92, P78, DOI 10.1016/S1077-3142(03)00095-X
   LELE SK, 1992, J COMPUT PHYS, V103, P16, DOI 10.1016/0021-9991(92)90324-R
   Li GH, 2013, J OPT SOC AM A, V30, P1448, DOI 10.1364/JOSAA.30.001448
   Li WS, 2004, P SOC PHOTO-OPT INS, V5457, P300, DOI 10.1117/12.546002
   Liu SJ, 2021, VISUAL COMPUT, V37, P2485, DOI 10.1007/s00371-021-02213-3
   Pan RJ, 2012, VISUAL COMPUT, V28, P155, DOI 10.1007/s00371-011-0604-9
   Quéau Y, 2018, J MATH IMAGING VIS, V60, P609, DOI 10.1007/s10851-017-0777-6
   Quéau Y, 2018, J MATH IMAGING VIS, V60, P576, DOI 10.1007/s10851-017-0773-x
   Scherr T., 2017, THESIS, DOI [10.11588/heidok.00023979, DOI 10.11588/HEIDOK.00023979]
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103
   Talmi A, 2006, J OPT SOC AM A, V23, P288, DOI 10.1364/JOSAA.23.000288
   Volkov VV, 2002, MICRON, V33, P411, DOI 10.1016/S0968-4328(02)00017-3
   Wu L., 2019, INFRARED LASER ENG, DOI [10.3788/IRLA201948.0825002, DOI 10.3788/IRLA201948.0825002]
   WU ZQ, 1988, COMPUT VISION GRAPH, V43, P53, DOI 10.1016/0734-189X(88)90042-4
   Xie ZF, 2012, VISUAL COMPUT, V28, P1195, DOI 10.1007/s00371-011-0668-6
   Zuo C, 2014, OPT EXPRESS, V22, P9220, DOI 10.1364/OE.22.009220
NR 33
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2097
EP 2110
DI 10.1007/s00371-022-02467-5
EA MAY 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000789788500001
DA 2024-07-18
ER

PT J
AU Ma, J
   Chen, JJ
   Yang, C
AF Ma, Ji
   Chen, Jinjin
   Yang, Chang
TI Using optimized gaussian mixture model rules and global tracking graph
   for feature extraction and tracking in time-varying data
SO VISUAL COMPUTER
LA English
DT Article
DE Feature extraction and tracking; Time-varying data; Gaussian mixture
   model rules; Global tracking graph; Volume rendering; Animation
ID VISUALIZATION
AB Advances in computational power and numerical simulations have made many complex scientific simulations possible, generating large-scale and complex time-varying data. How we effectively extract and track the features contained in time-varying data play a crucial role in helping scientists recognize and understand the trendsD and dynamic behaviors behind these simulations. Many feature extraction and tracking methods often require the user to initially feed numerous feature data, e.g., a volume into their models for feature extraction, and then track the feature locally by comparing the features at two consecutive time steps. In this paper, we propose a different method to achieve feature extraction and tracking. For feature extraction, our method simply requires the user to label a feature on two slices in the time-varying data, and then, it generates a set of optimized Gaussian mixture model rules that can be used to automatically extract the feature at each time step in the time-varying data. Based on the extracted feature at each time step, our tracking method can create a global tracking graph that will record all possible tracking information of this feature across all time steps and thus achieve a global feature tracking. To demonstrate the effectiveness of our method, we applied several time-varying datasets from scientific simulations to it. Furthermore, to validate our method, we both qualitatively and quantitatively compared its feature tracking results against the ones from two state-of-the-art feature tracking techniques by referring to the ground truth. The experiment results showed that our method could generate the most accurate feature tracking results than the two compared state-of-the-art techniques.
C1 [Ma, Ji; Yang, Chang] Zhejiang Univ Technol, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
   [Chen, Jinjin] Commun Univ Zhejiang, Sch Design & Art, Hangzhou, Peoples R China.
C3 Zhejiang University of Technology; Communication University of Zhejiang
RP Ma, J (corresponding author), Zhejiang Univ Technol, Sch Comp Sci & Technol, Hangzhou, Peoples R China.; Chen, JJ (corresponding author), Commun Univ Zhejiang, Sch Design & Art, Hangzhou, Peoples R China.
EM maji@zjut.edu.cn; chenjinj@cuz.edu.cn
FU National Natural Science Foundation of China [61902350]; Open Project
   Program of the State Key Lab of CAD&CG of Zhejiang University [A2111]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 61902350, and by the Open Project Program of the
   State Key Lab of CAD&CG of Zhejiang University under Grant No. A2111.
CR Aldrich G., 2016, SCI VISUALIZATION CO
   Bai ZH, 2020, J VISUAL-JAPAN, V23, P745, DOI 10.1007/s12650-020-00654-x
   Bao XK, 2018, J VISUAL-JAPAN, V21, P511, DOI 10.1007/s12650-017-0461-3
   Bouchachia A., 2011, 2011 10th International Conference on Machine Learning and Applications and Workshops, V2, P47
   Bremer PT, 2011, IEEE T VIS COMPUT GR, V17, P1307, DOI 10.1109/TVCG.2010.253
   Camarri S., 2005, 17 C MECC TEOR APPL
   CRAWFIS RA, 1993, VISUALIZATION 93, PROCEEDINGS, P261
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Dutta S, 2016, IEEE T VIS COMPUT GR, V22, P837, DOI 10.1109/TVCG.2015.2467436
   Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001
   Krone M, 2013, COMPUT GRAPH FORUM, V32, P331, DOI 10.1111/cgf.12120
   Kumpf A, 2019, IEEE T VIS COMPUT GR, V25, P98, DOI 10.1109/TVCG.2018.2864901
   Ma J, 2021, J VISUAL-JAPAN, V24, P545, DOI 10.1007/s12650-020-00724-0
   McLachlan G., 2000, WILEY SER PROB STAT, DOI 10.1002/0471721182
   Obermaier Harald., 2013, Eurographics Conference on Visualization (EuroVis 2013) Short Papers, P43
   Oesterling P., 2015, Topological Methods in Data Analysis and Visualization, P87, DOI [10.1007/978-3-319-44684-45, DOI 10.1007/978-3-319-44684-45]
   Ozer S, 2014, IEEE T VIS COMPUT GR, V20, P377, DOI 10.1109/TVCG.2013.117
   Reinders F, 2001, VISUAL COMPUT, V17, P55, DOI 10.1007/PL00013399
   Sahner J, 2007, IEEE T VIS COMPUT GR, V13, P980, DOI 10.1109/TVCG.2007.1053
   Saikia H, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13163
   Salzbrunn T, 2008, VISUAL COMPUT, V24, P1039, DOI 10.1007/s00371-007-0204-x
   SAMTANEY R, 1994, COMPUTER, V27, P20, DOI 10.1109/2.299407
   Sauer F, 2014, IEEE T VIS COMPUT GR, V20, P2565, DOI 10.1109/TVCG.2014.2346423
   Shen EY, 2015, VISUAL COMPUT, V31, P441, DOI 10.1007/s00371-014-0940-7
   Shusen Liu, 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P73, DOI 10.1109/LDAV.2012.6378978
   Silver D, 1997, IEEE T VIS COMPUT GR, V3, P129, DOI 10.1109/2945.597796
   Takle J, 2012, IEEE CONF VIS ANAL, P243, DOI 10.1109/VAST.2012.6400532
   Valsangkar AA, 2019, IEEE T VIS COMPUT GR, V25, P1460, DOI 10.1109/TVCG.2018.2810068
   von Funck W, 2008, IEEE T VIS COMPUT GR, V14, P1396, DOI 10.1109/TVCG.2008.163
   Wang YH, 2011, IEEE T VIS COMPUT GR, V17, P1560, DOI 10.1109/TVCG.2011.97
   Widanagamaachchi W., 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P9, DOI 10.1109/LDAV.2012.6378962
   Widanagamaachchi W, 2017, IEEE PAC VIS SYMP, P101, DOI 10.1109/PACIFICVIS.2017.8031584
   Widanagamaachchi W, 2015, SYMP LARG DATA ANAL, P9, DOI 10.1109/LDAV.2015.7348066
   Wolfgang, 2014, APPL MED IMAGE PROCE
   Zhang HJ, 2018, VISUAL COMPUT, V34, P531, DOI 10.1007/s00371-017-1359-8
NR 35
TC 0
Z9 0
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1869
EP 1892
DI 10.1007/s00371-022-02451-z
EA MAR 2022
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000778219000001
DA 2024-07-18
ER

PT J
AU Hu, JW
   Liu, RX
   Chen, ZH
   Wang, DZ
   Zhang, YJ
   Xie, BL
AF Hu, Jianwei
   Liu, Ruixin
   Chen, Zhonghui
   Wang, Dengzhun
   Zhang, Yongjun
   Xie, Benliang
TI Octave convolution-based vehicle detection using frame-difference as
   network input
SO VISUAL COMPUTER
LA English
DT Article
DE Vehicle detection; Three-frame-difference; Octave convolution;
   Multi-input neural network
AB Vehicle detection in video frames has been treated the same way detecting vehicle for an isolated image. However, the models designed for the isolated image are blind to fast-moving vehicles and cannot localize the moving targets partially occluded in the scene. In this case, we figure out a way to combine the classic moving target detection method with the neural network method. In this work, first, we propose to add three-differential-frames into the neural network of Yolov3 as the second input which contains the motion information on the front and back frames to detect vehicles partially occluded; second, we reform the network by using Octave Convolution to reduce memory and computational cost while boosting accuracy. We experimentally show that by using the aforementioned methods together, compared with using original YOLOv3 on UA-DETRAC data set, AP is increased by 2.31%, recall is increased by 4.01%, and precision is increased by 3.10%. We demonstrate that the proposed method is indeed effective.
C1 [Hu, Jianwei; Liu, Ruixin; Chen, Zhonghui; Wang, Dengzhun; Xie, Benliang] Guizhou Univ, Coll Big Data & Informat Engn, Guiyang, Peoples R China.
   [Zhang, Yongjun] Guizhou Univ, Coll Comp Sci & Technol, Guiyang, Peoples R China.
C3 Guizhou University; Guizhou University
RP Xie, BL (corresponding author), Guizhou Univ, Coll Big Data & Informat Engn, Guiyang, Peoples R China.
EM 1172951257@qq.com; 627803670@qq.com; 1091980332@qq.com;
   1150771449@qq.com; zyj6667@126.com; blxie@gzu.edu.cn
RI Liu, Ruixin/AAB-7916-2020; Zhang, Yongjun/M-1094-2013
OI Xie, Benliang/0000-0002-9854-8300
FU National Natural Science Foundation of China [61562009]; Open Fund
   Project in Semiconductor Power Device Reliability Engineering Center of
   Ministry of Education [ERCMEKFJJ2019-06]; Guizhou University Introduced
   Talent Research Project [2015-29]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61562009), the Open Fund Project in Semiconductor Power
   Device Reliability Engineering Center of Ministry of Education (No.
   ERCMEKFJJ2019-06), and the Guizhou University Introduced Talent Research
   Project (No. 2015-29).
CR Ahmed E.H., 2016, House Price Estimation from Visual and Textual Features
   Caiyuan C., 2012, LECT NOTES ELECT ENG, P459, DOI DOI 10.1007/978-3-642-27296-771
   Carreira J., 2018, arXiv
   Chandrasekar KS, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102905
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen YF, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC), P754, DOI 10.1109/ICNC.2015.7378085
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Cui XX, 2020, CHIN CONTR CONF, P7595, DOI 10.23919/CCC50068.2020.9189659
   Girshick R., 2013, IEEE Comput. Soc., P580
   Harikrishnan PM, 2021, APPL INTELL, V51, P4714, DOI 10.1007/s10489-020-02127-y
   He H, 2018, 2018 WRC SYMPOSIUM ON ADVANCED ROBOTICS AND AUTOMATION (WRC SARA), P141, DOI 10.1109/WRC-SARA.2018.8584221
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lyu S, 2018, MED C CONTR AUTOMAT, P1, DOI 10.1109/MED.2018.8442736
   Redmon J., 2018, P IEEE C COMP VIS PA
   Sengar SS, 2017, SIGNAL IMAGE VIDEO P, V11, P1357, DOI 10.1007/s11760-017-1093-8
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Teoh SS, 2012, MACH VISION APPL, V23, P831, DOI 10.1007/s00138-011-0355-7
   Tsai LW, 2007, IEEE T IMAGE PROCESS, V16, P850, DOI 10.1109/TIP.2007.891147
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
NR 21
TC 3
Z9 3
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1503
EP 1515
DI 10.1007/s00371-022-02425-1
EA MAR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000765191600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Sathish, S
   Sumithra, MG
   Mohanasundaram, K
AF Sathish, S.
   Sumithra, M. G.
   Mohanasundaram, K.
TI Shading and texture constrained retinex for correcting vignetting on
   dermatological macro images
SO VISUAL COMPUTER
LA English
DT Article
DE Dermatological photography; Illumination-correction; Image processing;
   Retinex decomposition; Vignetting error
ID ILLUMINATION; ENHANCEMENT
AB Because of the flexibility and availability of high-resolution digital cameras, dermatological photography is considered as a good alternative to dermoscopy. However, uneven background illumination on the dermatological photographs makes their automated analysis troublesome. Equalization of the uneven background illumination is helpful to make the automated analysis of the dermatological photographs more sensitive and specific. A customized algorithm for equalizing the uneven background illumination on dermatological photographs is proposed in this paper. The illumination-corrected image is reconstructed from the gamma-corrected illumination component in Hue Value Saturation (HSV) color space. The Retinex decomposition of the value component is formulated as a non-convex optimization problem. Constraints within the cost function are derived from the shading and texture priors. The shading and texture priors are computed respectively from the derivatives of the illumination and texture priors. On 137 dermatological photographs, the values of Average Gradient of the Illumination Component, Lightness Order Error, Sparse Feature Fidelity, Visual Saliency-based Index, Visual Information Fidelity and the computational time exhibited by the proposed devignetting algorithm are 0.1895 +/- 0.0386, 232.9553 +/- 140.7912, 0.9783 +/- 0.0106, 0.9903 +/- 0.0021, 0.7063 +/- 0.0396 and 2.0272 +/- 0.4319 (sec). The proposed algorithm is able to equalize the uneven background illumination without scaling or boosting it intolerably. It produces output images that are natural in appearance and free from structural/color artefacts. The loss of salient information is negligible in the proposed algorithm. It is computationally fast, as well.
C1 [Sathish, S.] Veltech Multitech Dr Rangarajan DrSakunthala Engn, Chennai 600062, Tamil Nadu, India.
   [Sumithra, M. G.; Mohanasundaram, K.] KPR Inst Engn & Technol, Coimbatore 641407, Tamil Nadu, India.
C3 Vel Tech Multi Tech Dr.Rangarajan Dr.Sakunthala Engineering College
RP Sathish, S (corresponding author), Veltech Multitech Dr Rangarajan DrSakunthala Engn, Chennai 600062, Tamil Nadu, India.
EM sathishdir@gmail.com; sumithrapalanisamy74@gmail.com;
   kumohanasundaram@gmail.com
RI k, Mohana sundaram/L-6630-2015; G, SUMITHRA M/AAY-1560-2020
OI k, Mohana sundaram/0000-0002-9508-5910; G, SUMITHRA
   M/0000-0002-0504-2061; sankaran, sathish/0000-0001-9569-1518
CR Agudo A, 2022, VISUAL COMPUT, V38, P3937, DOI 10.1007/s00371-021-02238-8
   Ben Salah K, 2022, VISUAL COMPUT, V38, P1833, DOI 10.1007/s00371-021-02108-3
   Bulut F, 2022, VISUAL COMPUT, V38, P2239, DOI 10.1007/s00371-021-02281-5
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   García-García B, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63146-w
   Glaister J, 2013, IEEE T BIO-MED ENG, V60, P1873, DOI 10.1109/TBME.2013.2244596
   Gupta N, 2022, VISUAL COMPUT, V38, P2315, DOI 10.1007/s00371-021-02114-5
   Hajabdollahi M, 2020, COMPUT MED IMAG GRAP, V82, DOI 10.1016/j.compmedimag.2020.101729
   Hameed N, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112961
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Kang S. B., 2000, PROC EUR C COMPUT VI, P640, DOI [10.1007/3-540-45053-X_41, DOI 10.1007/3-540-45053-X_41]
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P534, DOI 10.1109/LSP.2014.2357015
   MacLellan AN, 2021, J AM ACAD DERMATOL, V85, P353, DOI 10.1016/j.jaad.2020.04.019
   Norton KA, 2012, SKIN RES TECHNOL, V18, P290, DOI 10.1111/j.1600-0846.2011.00569.x
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Shamsudeen Fousia M., 2019, Informatics in Medicine Unlocked, V14, P82, DOI 10.1016/j.imu.2018.10.001
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Srinivas K, 2020, IET IMAGE PROCESS, V14, P668, DOI 10.1049/iet-ipr.2019.0781
   Tian QC, 2018, SIGNAL PROCESS, V153, P210, DOI 10.1016/j.sigpro.2018.07.022
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Xie FY, 2016, IEEE T BIO-MED ENG, V63, P1248, DOI 10.1109/TBME.2015.2493580
   Yang YY, 2020, VISUAL COMPUT, V36, P717, DOI 10.1007/s00371-019-01651-4
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang N, 2020, ARTIF INTELL MED, V102, DOI 10.1016/j.artmed.2019.101756
   Zhang YB, 2018, IET IMAGE PROCESS, V12, P2147, DOI 10.1049/iet-ipr.2018.5634
   Zheng YJ, 2009, IEEE T PATTERN ANAL, V31, P2243, DOI 10.1109/TPAMI.2008.263
   Zhou M, 2018, IEEE T BIO-MED ENG, V65, P521, DOI 10.1109/TBME.2017.2700627
NR 30
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 693
EP 709
DI 10.1007/s00371-021-02368-z
EA JAN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741976300001
DA 2024-07-18
ER

PT J
AU Carensac, S
   Pronost, N
   Bouakaz, S
AF Carensac, Samuel
   Pronost, Nicolas
   Bouakaz, Saida
TI Optimizations for predictive-corrective particle-based fluid simulation
   on GPU
SO VISUAL COMPUTER
LA English
DT Article
DE Fluid simulation; SPH; Animation; GPU
ID SPH
AB The use of particles-based simulations to produce fluid animations is nowadays a frequently used method by both the industrial and research sectors. Although there are many variations of the smoothed particle hydrodynamics (SPH) algorithm currently being used, they all have the common characteristic of being highly parallel in nature. They are therefore frequently implemented on graphics processing units (GPUs) to benefit of high computation capacities of modern GPUs. However, such optimizations require specific optimizations to make use of the full capacity of the GPU, with sometimes optimizations being contradictory to optimizations used in CPU implementations. In this paper, we explored various optimizations on a GPU implementation of a recent particle-based fluid simulation algorithm using an iterative pressure solver. In particular, we focused on CPU optimizations that have not been thoroughly studied for GPU implementations: the indexing for the neighbor's structure, the frequency of the sorting of the fluid particles, the use of lookup tables for the kernel function computations and the use of a warm-start to improve the performance of the iterative pressure solver. We show that some of these optimizations are only effective for very specific hardware configurations and sometimes even impact the performance negatively. We also show that the warm-start reduces the computation time but introduces a cyclic instability in the simulation. We propose a solution to reduce this instability without requiring to modify the implementation of the fluid algorithm.
C1 [Carensac, Samuel; Pronost, Nicolas; Bouakaz, Saida] Univ Claude Bernard Lyon 1, Univ Lyon, CNRS LIRIS UMR 5205, Villeurbanne, France.
C3 Universite Claude Bernard Lyon 1
RP Pronost, N (corresponding author), Univ Claude Bernard Lyon 1, Univ Lyon, CNRS LIRIS UMR 5205, Villeurbanne, France.
EM samuel.carensac.research@gmail.com; nicolas.pronost@univ-lyon1.fr;
   saida.boualcaz@univ-lyon1.fr
OI PRONOST, NICOLAS/0000-0003-4499-509X
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Antuono M, 2010, COMPUT PHYS COMMUN, V181, P532, DOI 10.1016/j.cpc.2009.11.002
   Band S, 2020, COMPUT GRAPH FORUM, V39, P531, DOI 10.1111/cgf.13890
   Baruffa F, 2017, 2017 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING & SIMULATION (HPCS), P381, DOI 10.1109/HPCS.2017.64
   Bender J., 2015, P 14 ACM SIGGRAPH EU, P147, DOI DOI 10.1145/2786784.2786796
   Bender J., 2017, INTERACTIVE COMPUTER
   Bender J, 2017, IEEE T VIS COMPUT GR, V23, P1193, DOI 10.1109/TVCG.2016.2578335
   Bender Jan, 2019, Motion, Interaction and Games, DOI [DOI 10.1145/3359566.3360077, 10.1145/3359566.3360077]
   Bilotta G, 2018, High performance parallel computing, DOI [10.5772/intechopen.81755, DOI 10.5772/INTECHOPEN.81755]
   Cummins SJ, 1999, J COMPUT PHYS, V152, P584, DOI 10.1006/jcph.1999.6246
   Domínguez JM, 2013, COMPUT PHYS COMMUN, V184, P617, DOI 10.1016/j.cpc.2012.10.015
   Durand Marie, 2012, 9 WORKSH VIRT REAL I, P69
   Franzen P., 2015, VRIPHYS
   Goswami P., 2010, P 2010 ACM SIGGRAPHE, P55
   Harada T., 2007, P COMP GRAPH INT
   Haverkort H, 2017, J COMPUT GEOM, V8, P206
   Hoetzlein R. C., 2014, FAST FIXED RADIUS NE
   Huang KM, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3321360
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Koschier Dan., 2019, Eurographics 2019-Tutorials, DOI DOI 10.2312/EGT.20191035
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Rustico E, 2014, IEEE T PARALL DISTR, V25, P43, DOI 10.1109/TPDS.2012.340
   Skilling J, 2004, AIP CONF PROC, V707, P381, DOI 10.1063/1.1751381
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Verma K, 2017, IEEE HIGH PERF EXTR
   Weiler M, 2018, COMPUT GRAPH FORUM, V37, P145, DOI 10.1111/cgf.13349
   Winchenbach R, 2020, COMPUT GRAPH FORUM, V39, P527, DOI 10.1111/cgf.14090
   Winchenbach R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417829
   Winkler D, 2018, COMPUT PHYS COMMUN, V225, P140, DOI 10.1016/j.cpc.2017.12.014
NR 32
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 983
EP 995
DI 10.1007/s00371-021-02379-w
EA JAN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741630200003
DA 2024-07-18
ER

PT J
AU Ye, HZ
   Zhu, Q
   Yao, Y
   Jin, YC
   Zhang, DQ
AF Ye, Haizhou
   Zhu, Qi
   Yao, Yuan
   Jin, Yichao
   Zhang, Daoqiang
TI Pairwise feature-based generative adversarial network for incomplete
   multi-modal Alzheimer's disease diagnosis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Incomplete multi-modal; GAN; AD; Medical imaging
AB Magnetic resonance imaging (MRI) and positron emission tomography (PET) are widely used in diagnosis of Alzheimer's disease (AD). In practice, incomplete modality problem is unavoidable due to the cost of data acquisition. Deep learning based models especially generative adversarial networks (GAN) are usually adopted to impute missing images. However, there are still some problems: (1) there are many regions unrelated to the disease and have little significance in the actual diagnosis in brain images, which are very cumbersome to generate. (2) The image generated by GAN would introduce noises causing the poor performance in the diagnostic model. To address these problems, a pairwise feature-based generation adversarial network is proposed. Specifically, features from the original brain images are extracted firstly. For the paired data without modality loss, the extracted MRI features are used as input to generate its corresponding PET features, which not only reduces the scale of the model, but also ensures the direct correlation between the generated features and the diagnosis. In addition, the available real PET features of the paired samples are added as label to constrain the generated ones. Finally, the attention mechanism is adopted in both the generator and discriminator, which can effectively retain the structural information of the feature itself. A large number of experiments have demonstrated that our proposed method has achieved promising results in the diagnosis of AD.
C1 [Ye, Haizhou; Zhu, Qi; Yao, Yuan; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Jin, Yichao] Indeed Singapore Pte Ltd, Singapore 049315, Singapore.
C3 Nanjing University of Aeronautics & Astronautics
RP Zhu, Q (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM haizhouye@163.com; zbuqinuaa@163.com; y_yao@nuaa.edu.cn;
   jinyichao@indeed.com; dqzhang@nuaa.edu.cn
FU National Natural Science Foundation of China [62076129, 61501230,
   61732006, 61876082, 61861130366]; National Science and Technology Major
   Project [2018ZX10201002]; National Key RAMP;D Program of China
   [2018YFC2001600, 2018YFC2001602]
FX This work was supported in part by National Natural Science Foundation
   of China (Nos. 62076129, 61501230, 61732006, 61876082 and 61861130366),
   National Science and Technology Major Project (No. 2018ZX10201002), and
   the National Key R&D Program of China (Grant Nos.: 2018YFC2001600,
   2018YFC2001602).
CR Ashburner J, 2000, NEUROIMAGE, V11, P805, DOI 10.1006/nimg.2000.0582
   Basavegowda HS, 2020, CAAI T INTELL TECHNO, V5, P22, DOI 10.1049/trit.2019.0028
   Cummings J, 2019, ALZH DEMENT-TRCI, V5, P272, DOI 10.1016/j.trci.2019.05.008
   Goodfellow I. J., 2014, ARXIV PREPR ARXIV 14
   Guarino A, 2019, FRONT AGING NEUROSCI, V10, DOI 10.3389/fnagi.2018.00437
   Ieracitano C, 2020, NEURAL NETWORKS, V123, P176, DOI 10.1016/j.neunet.2019.12.006
   Kirova AM, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/748212
   Kitamura Y, 2017, NEUROL RES, V39, P231, DOI 10.1080/01616412.2017.1281195
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Lin E, 2021, INT J MOL SCI, V22, DOI 10.3390/ijms22157911
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Pan YS, 2019, LECT NOTES COMPUT SC, V11766, P137, DOI 10.1007/978-3-030-32248-9_16
   Pan YS, 2018, LECT NOTES COMPUT SC, V11072, P455, DOI 10.1007/978-3-030-00931-1_52
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Salimans T, 2016, ADV NEUR IN, V29
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P273, DOI 10.1006/nimg.2001.0978
   Weller Jason, 2018, F1000Res, V7, DOI 10.12688/f1000research.14506.1
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Yang JC, 2021, CAAI T INTELL TECHNO, V6, P46, DOI 10.1049/cit2.12001
   Yoon J, 2018, PR MACH LEARN RES, V80
   Zhang DQ, 2012, NEUROIMAGE, V59, P895, DOI 10.1016/j.neuroimage.2011.09.069
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhou T, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101630
   Zhu CB, 2020, CAAI T INTELL TECHNO, V5, P1, DOI 10.1049/trit.2019.0034
NR 25
TC 7
Z9 7
U1 4
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2235
EP 2244
DI 10.1007/s00371-021-02354-5
EA JAN 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000741634600007
DA 2024-07-18
ER

PT J
AU Araujo, V
   Dalmoro, B
   Musse, SR
AF Araujo, Victor
   Dalmoro, Bruna
   Musse, Soraia Raupp
TI Analysis of charisma, comfort and realism in CG characters from a gender
   perspective
SO VISUAL COMPUTER
LA English
DT Article
DE Uncanny valley; Perceived comfort; Perceived realism; Perceived
   charisma; Cg characters; Gender
ID UNCANNY VALLEY
AB Realistic characters from movies, games and simulations can make viewers feel strange (discomfort), an effect known as the Uncanny Valley (UV) theory. However, can the genders of CG characters and the genders of viewers change perceived comfort? In addition, can the genders (both characters and viewers) also influence the perceived charisma? Can the realism of a character also influence these aspects? This work aims to evaluate the perception of women and men about female and male characters, created using Computer Graphics (CG), presented in various media (movies, games, computer simulations, among others). Our goal is to answer the following questions: (i) How does the comfort perceived by people of both tested genders (female and male) relate to the genders of the characters? and (ii) Is the charisma influenced by the realism of the characters, considering the subjects and genders of the characters? We conducted perceptual studies on characters created using CG in images and videos through questionnaires. Our results indicated that the gender of the subjects and characters affected comfort, charisma and perceived realism. In addition, we also revisited the aspect of the UV theory (perception of comfort and human likeness) and found coherent curves compared to many works in the literature.
C1 [Araujo, Victor; Dalmoro, Bruna; Musse, Soraia Raupp] Pontificia Univ Catolica Rio Grande do Sul, Porto Alegre, RS, Brazil.
C3 Pontificia Universidade Catolica Do Rio Grande Do Sul
RP Musse, SR (corresponding author), Pontificia Univ Catolica Rio Grande do Sul, Porto Alegre, RS, Brazil.
EM victor.flavio@acad.pucrs.br; bruna.dalmoro@edu.pucrs.br;
   soraia.musse@pucrs.br
RI Flavio De Andrade Araujo, Victor/HTN-2769-2023; Musse, Soraia
   Raupp/AAS-3787-2021; Musse, Soraia Raupp R/G-4801-2012
CR Adair-Toteff C, 2005, J CLASS SOCIOL, V5, P189, DOI 10.1177/1468795X05053491
   [Anonymous], 2003, Journal of Classical Sociology, DOI [10.1177/1468795X03003001692, DOI 10.1177/1468795X03003001692]
   Bailey J.D., 2017, P AUSTR COMP SCI WEE, DOI [DOI 10.1145/3014812.3014876, 10.1145/3014812.3014876]
   Cheetham Marcus, 2013, Front Psychol, V4, P108, DOI 10.3389/fpsyg.2013.00108
   Dill Vanderson, 2012, Intelligent Virtual Agents. Proceedings 12th International Conference, IVA 2012, P511, DOI 10.1007/978-3-642-33197-8_62
   Draude C, 2011, AI SOC, V26, P319, DOI 10.1007/s00146-010-0312-4
   Goethals GR., 2014, Conceptions of leadership, DOI [10.1057/9781137472038, DOI 10.1057/9781137472038]
   Groves K.S., 2005, Journal of Leadership and Organizational Studies, V11, P30, DOI [DOI 10.1177/107179190501100303, 10.1177/107179190501100303]
   Hall JK, 2010, COGNITION EMOTION, V24, P629, DOI 10.1080/02699930902906882
   Hyde J, 2016, ACM T APPL PERCEPT, V13, DOI 10.1145/2851499
   Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529
   Kätsyri J, 2017, INT J HUM-COMPUT ST, V97, P149, DOI 10.1016/j.ijhcs.2016.09.010
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Laue Cheyenne., 2017, Multimodal Technologies and Interaction, V1, P2, DOI DOI 10.3390/MTI1010002
   Ma L, 2019, COMPUT GRAPH FORUM, V38, P470, DOI 10.1111/cgf.13586
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Riggio R.E., 1998, Encyclopedia of Mental Health, V1, P387
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Tinwell A, 2013, COMPUT HUM BEHAV, V29, P1617, DOI 10.1016/j.chb.2013.01.008
   Weber M., 1947, The theory of social and economic organisation, P328
   Wisessing P, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3383195
   Zell E., 2019, ACM SIGGRAPH 2019 CO, P1, DOI 10.1145/3305366.3328101
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2767130
NR 26
TC 5
Z9 5
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2685
EP 2698
DI 10.1007/s00371-021-02214-2
EA JUL 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA UL2RI
UT WOS:000669266500001
DA 2024-07-18
ER

PT J
AU Deng, NC
   Ye, JN
   Chen, N
   Yang, XB
AF Deng, Nianchen
   Ye, Jiannan
   Chen, Nuo
   Yang, Xubo
TI Towards Stereoscopic On-vehicle AR-HUD
SO VISUAL COMPUTER
LA English
DT Article
DE Stereoscopic; Light field; Augmented reality; Head-up display;
   Calibration
ID CALIBRATION; DISPLAYS
AB On-vehicle AR-HUD (head-up display) is a driving assistant system, which can let drivers see navigation and warning information directly through the windshield with an easy-to-perceive augmented reality form. Traditional AR-HUD can only produce the content at a specific distance, which causes uncomfortable experiences such as frequent refocusing and unnatural floating of virtual objects. This paper proposed an innovative AR-HUD system, which can provide stereoscopic scenes to the driver. The system is composed of two traditional HUD displays and supports parallax by additive light field factorization. Optical paths and illumination of two displays are precisely calibrated for both views to ensure the combination of lights is as expected. The factorization algorithm is optimized for the system. With GPU acceleration, the system can run in real time. The system is cheap and simple to transform from a traditional AR-HUD system, which presents a feasible scheme to achieve a fusion-enhanced augmented reality assistant for driving.
C1 [Deng, Nianchen; Ye, Jiannan; Chen, Nuo; Yang, Xubo] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University
RP Deng, NC (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM dengnianchen@sjtu.edu.cn; yangxubo@sjtu.edu.cn
RI Deng, Nianchen/GXZ-4749-2022; zhang, jingxing/KCY-4726-2024; jing,
   wang/KCZ-2144-2024; Chen, Nuo/JZD-0344-2024
OI Deng, Nianchen/0000-0002-5292-266X; 
FU SAIC Foundation [2018310031004252]
FX Research grants (funding agencies and grant number): SAIC Foundation,
   2018310031004252
CR Bauer A, 2012, OPT EXPRESS, V20, P14906, DOI 10.1364/OE.20.014906
   Chiu HP, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P423, DOI 10.1109/VR.2018.8447560
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456
   Fuchs, 2018, P COMP GRAPH INT 201, P169, DOI [10.1145/3208159.3208190, DOI 10.1145/3208159.3208190]
   Gao X, 2019, IEEE IMAGE PROC, P4355, DOI [10.1109/icip.2019.8803608, 10.1109/ICIP.2019.8803608]
   Genc Y, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P165, DOI 10.1109/ISAR.2000.880940
   Gershun A., 1936, J MATH PHYS
   Gershun Andrei, 1939, J MATH PHYS, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Huang FC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601122
   Hyesun Park, 2013, Design, User Experience, and Usability. User Experience in Novel Technological Environments. Second International Conference, DUXU 2013 Held as Part of HCI International 2013. Proceedings. LNCS 8014, P393, DOI 10.1007/978-3-642-39238-2_43
   Itoh Y, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P75, DOI 10.1109/3DUI.2014.6798846
   Jun H, 2016, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VR.2016.7504693
   Klemm M, 2017, COMPUT GRAPH-UK, V64, P51, DOI 10.1016/j.cag.2017.02.001
   KyeongHoon Park, 2011, 2011 IEEE International Symposium on VR Innovation (ISVRI), P261, DOI 10.1109/ISVRI.2011.5759648
   Lagoo R, 2019, IEEE CONSUM ELECTR M, V8, P79, DOI 10.1109/MCE.2019.2923896
   Lawton G, 2011, COMPUTER, V44, P17, DOI 10.1109/MC.2011.3
   Lee S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925971
   Li A, 2015, APPL OPTICS, V54, P2441, DOI 10.1364/AO.54.002441
   Li K, 2020, ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020), P178, DOI 10.1109/ISMAR-Adjunct51615.2020.00055
   Merenda Coleman, 2016, 2016 IEEE VR 2016 Workshop on Perceptual and Cognitive Issues in AR (PERCAR), P1, DOI 10.1109/PERCAR.2016.7562419
   Moser KR, 2016, P IEEE VIRT REAL ANN, P233, DOI 10.1109/VR.2016.7504739
   Owen CB, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P70, DOI 10.1109/ISMAR.2004.28
   Park HS, 2013, ETRI J, V35, P1038, DOI 10.4218/etrij.13.2013.0041
   Plopski A, 2015, IEEE T VIS COMPUT GR, V21, P481, DOI 10.1109/TVCG.2015.2391857
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Tuceryan M, 2002, PRESENCE-TELEOP VIRT, V11, P259, DOI 10.1162/105474602317473213
   Ueno K, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P168, DOI 10.1109/ISMAR.2015.48
   Wetzstein G, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185576
   Wientapper F, 2013, INT SYM MIX AUGMENT, P189, DOI 10.1109/ISMAR.2013.6671779
   WU W, 2009, P 2009 WORKSH AMB ME, P21
   Yoon C, 2014, I C INF COMM TECH CO, P601, DOI 10.1109/ICTC.2014.6983221
   Yoshida, 2011, DIGITAL HOLOGRAPHY 3
   Zhan T, 2018, OPT EXPRESS, V26, P4863, DOI 10.1364/OE.26.004863
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 35
TC 5
Z9 5
U1 12
U2 70
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2527
EP 2538
DI 10.1007/s00371-021-02209-z
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000667441600004
DA 2024-07-18
ER

PT J
AU Chakraborty, A
   Sharma, R
AF Chakraborty, Abhik
   Sharma, Raksha
TI A deep crystal structure identification system for X-ray diffraction
   patterns
SO VISUAL COMPUTER
LA English
DT Article
DE X-ray diffraction pattern; Crystal structure prediction; Deep learning;
   Convolutional neural network; Computer vision
ID BRAGG-DIFFRACTION
AB The experimental purpose of X-ray diffraction is to analyze crystalline material structure at the atomic and molecular levels. Such experiments are known as X-ray crystallography. Traditionally, human experts do it with some domain knowledge. X-ray crystallography is useful in numerous domains, e.g., physics, chemistry, and biology. It is tough to own manual physics of diffraction patterns to see a crystal structure with a colossal data set. A convolutional neural network (CNN) is a deep neural network that maps an input image into a high-dimensional space. CNN produces an affordable function for image classification. This paper uses an extension of the convolutional neural network to predict crystal structure from diffraction patterns. We propose a machine-enabled method to predict crystallographic size and space group from a limited number of XRD patterns for small films. We overcome the problem of scarce data within the development of building materials by combining the learning model of moderately monitored equipment, a physics information-enhancing strategy using data generated from the Inorganic Crystal Structure Database, and test data. We compare our approach with a large variety of typical addition as modern machine learning-based classification techniques for crystal structure prediction. Results show that our proposed system outperforms all the baselines by a significant margin for the crystal structure prediction task. Results also show the impact of the number of layers in the all-convolutional neural network architecture for crystal structure prediction.
C1 [Chakraborty, Abhik; Sharma, Raksha] Indian Inst Technol Roorkee, Dept Comp Sci, Roorkee, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee
RP Sharma, R (corresponding author), Indian Inst Technol Roorkee, Dept Comp Sci, Roorkee, Uttar Pradesh, India.
EM achakraborty@cs.iitr.ac.in; raksha.sharma@cs.iitr.ac.in
CR Agatonovic-Kustrin S, 2000, J PHARMACEUT BIOMED, V22, P985, DOI 10.1016/S0731-7085(00)00256-9
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   Bouthillier X., 2015, ARXIV150608700
   Gilmore CJ, 2004, J APPL CRYSTALLOGR, V37, P231, DOI 10.1107/S002188980400038X
   Itano WM, 1998, SCIENCE, V279, P686, DOI 10.1126/science.279.5351.686
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Kozuma M, 1999, PHYS REV LETT, V82, P871, DOI 10.1103/PhysRevLett.82.871
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee D, 2007, APPL SPECTROSC, V61, P1398, DOI 10.1366/000370207783292127
   Obeidat SM, 2011, SPECTROSC-BIOMED APP, V26, P141, DOI 10.1155/2011/894143
   Park JK, 2019, VISUAL COMPUT, V35, P1615, DOI 10.1007/s00371-018-1561-3
   Park WB, 2017, IUCRJ, V4, P486, DOI 10.1107/S205225251700714X
   Press W.H., 1990, Comput. Phys., V4, P669, DOI [DOI 10.1063/1.4822961, 10.1063/1.4822961]
   SEYSEN M, 1993, COMBINATORICA, V13, P363, DOI 10.1007/BF01202355
   Sheldrick GM, 2015, ACTA CRYSTALLOGR C, V71, P3, DOI [10.1107/S2053229614024218, 10.1107/S0108767307043930, 10.1107/S2053273314026370]
   Springenberg J. T., 2015, ARXIV PREPRINT ARXIV
   Tatlier M, 2011, NEURAL COMPUT APPL, V20, P365, DOI 10.1007/s00521-010-0386-4
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   WEIDEMULLER M, 1995, PHYS REV LETT, V75, P4583, DOI 10.1103/PhysRevLett.75.4583
   WILLIS BTM, 1969, ACTA CRYSTALL A-CRYS, VA 25, P277, DOI 10.1107/S0567739469000441
   Zhou X., 2019, arXiv
NR 22
TC 14
Z9 15
U1 4
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1275
EP 1282
DI 10.1007/s00371-021-02165-8
EA JUN 2021
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000665770500002
DA 2024-07-18
ER

PT J
AU Sharma, D
   Selwal, A
AF Sharma, Deepika
   Selwal, Arvind
TI HyFiPAD: a hybrid approach for fingerprint presentation attack detection
   using local and adaptive image features
SO VISUAL COMPUTER
LA English
DT Article
DE Fingerprint biometrics; Spoof attacks; Presentation attack detection;
   Image features; Sequential model
ID LIVENESS DETECTION; TEXTURE CLASSIFICATION; PERSPIRATION; DESCRIPTOR;
   SCALE
AB With the pervasiveness of secured biometric authentication applications, the fingerprint-based identification system has fascinated much attention recently. However, the major detriment is their recognition sensors are vulnerable to presentation or spoofing attacks from fake fingerprint artifacts. To resolve these issues, a viable anti-deception countermeasure known as presentation attack detection (PAD) mechanism is developed. As handcrafted feature-based classification techniques exhibit encouraging results in computer vision, they are widely employed in fingerprint spoof detection. Notably, the single-feature-based techniques do not perform uniformly over different spoofing and sensing technologies. In this research work, we expound a new hybrid fingerprint presentation attack detection approach (HyFiPAD) that discriminates live and fake fingerprints using majority voting ensemble build on three local and adaptive textural image features. We propose a new descriptor (i.e., a variant of LBP) which is termed as Local Adaptive Binary Pattern (LABP). Thus, the notion of proposed LABP is used to extract more detailed micro-textural features from the fingerprint images. Our LABP features are combined with an existing Complete Local Binary Pattern (CLBP) descriptor to learn two respective SVM classifiers and additionally a sequential model is trained with the manually extracted Binary Statistical Image Features (BSIF). The experiments are performed on benchmark anti-spoofing datasets namely; LivDet 2009, LivDet 2011, LivDet 2013, and LivDet 2015, where an average classification error rate (ACER) of 4.11, 3.19, 2.88, and 2.97% is, respectively, achieved. The overall experimental analysis of the HyFiPAD demonstrates superiority against majority of the state-of-the-art methods. In addition, the proposed technique yields a promising performance on cross-database and cross-sensor liveness detection tests, claiming good generalization capability.
C1 [Sharma, Deepika; Selwal, Arvind] Cent Univ Jammu, Dept Comp Sci & Informat Technol, Samba 181143, Jammu & Kashmir, India.
C3 Central University of Jammu
RP Sharma, D (corresponding author), Cent Univ Jammu, Dept Comp Sci & Informat Technol, Samba 181143, Jammu & Kashmir, India.
EM sharmadeepika749@gmail.com
RI Selwal, Arvind/HTR-1625-2023
OI Selwal, Arvind/0000-0002-1075-6966
CR Abhyankar A, 2009, PATTERN RECOGN, V42, P452, DOI 10.1016/j.patcog.2008.06.012
   Agarwal S, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113160
   Alshdadi AA, 2020, BIOMED SIGNAL PROCES, V61, DOI 10.1016/j.bspc.2020.102039
   [Anonymous], 2017, IJCCI
   [Anonymous], 2010, INT J COMPUT ELECT E
   Antonelli A, 2006, LECT NOTES COMPUT SC, V3832, P221
   Baldisserra D, 2006, LECT NOTES COMPUT SC, V3832, P265
   Bhanarkar A, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P166, DOI 10.1109/ICIIP.2013.6707575
   Chugh T, 2018, IEEE T INF FOREN SEC, V13, P2190, DOI 10.1109/TIFS.2018.2812193
   Coli P, 2008, INT J IMAGE GRAPH, V8, P495, DOI 10.1142/S0219467808003209
   de Souza GB, 2019, J ARTIF INTELL SOFT, V9, P41, DOI 10.2478/jaiscr-2018-0023
   Drahansky M, 2006, 2006 IEEE INFORMATION ASSURANCE WORKSHOP, P42, DOI 10.1109/IAW.2006.1652075
   Dubey RK, 2016, IEEE T INF FOREN SEC, V11, P1461, DOI 10.1109/TIFS.2016.2535899
   Espinoza M, 2011, FORENSIC SCI INT, V204, P41, DOI 10.1016/j.forsciint.2010.05.002
   Galbally J., 2009, 2009 First IEEE International Conference on Biometrics, Identity and Security (BIdS), P1
   Galbally J., 2019, HDB BIOMETRIC ANTISP, DOI 10.1007/978-3-319-92627-8_1
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Ghiani L., 2013, INT CONF BIOMETR, P1
   Ghiani L, 2017, IET BIOMETRICS, V6, P224, DOI 10.1049/iet-bmt.2016.0007
   Ghiani L, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Ghiani L, 2012, INT C PATT RECOG, P537
   González-Soler LJ, 2021, IEEE ACCESS, V9, P5806, DOI 10.1109/ACCESS.2020.3048756
   Gragnaniello D, 2014, ELECTRON LETT, V50, P439, DOI 10.1049/el.2013.4044
   Gragnaniello Diego, 2013, Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS), P46, DOI 10.1109/BIOMS.2013.6656148
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jia J, 2007, LECT NOTES COMPUT SC, V4642, P309
   Jia XF, 2014, INFORM SCIENCES, V268, P91, DOI 10.1016/j.ins.2013.06.041
   Jian W, 2021, IEEE ACCESS, V9, P2229, DOI 10.1109/ACCESS.2020.3047723
   Jiang YL, 2018, J FOOD QUALITY, DOI 10.1155/2018/4931202
   Jung HY, 2018, ELECTRON LETT, V54, P564, DOI 10.1049/el.2018.0621
   Kannala J, 2012, INT C PATT RECOG, P1363
   Kim W, 2017, IEEE SIGNAL PROC LET, V24, P51, DOI 10.1109/LSP.2016.2636158
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazimul Limnd T. P., 2017, 2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS), P731, DOI 10.1109/ICECDS.2017.8389533
   Lee JE, 2008, 2008 BIOMETRICS SYMPOSIUM (BSYM), P1, DOI [10.1109/PLASMA.2008.4591032, 10.1109/BSYM.2008.4655515]
   Li QQ, 2014, INT C WAVEL ANAL PAT, P13, DOI 10.1109/ICWAPR.2014.6961283
   Lu MY, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATIONS (CSA), P77, DOI 10.1109/CSA.2015.79
   Marasco E., 2010, Biometric Measurements and Systems for Security and Medical Applications (BIOMS), 2010 IEEE Workshop on, P8
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Martinsen OG, 2007, IEEE T BIO-MED ENG, V54, P891, DOI 10.1109/TBME.2007.893472
   Memon S., 2011, 2011 19th Telecommunications Forum Telfor (TELFOR), P619, DOI 10.1109/TELFOR.2011.6143624
   Minaee S., BIOMETRIC RECOGNITIO
   Minaee S., 2019, Deep-emotion: Facial expression recognition using attentional convolutional network
   Moon YS, 2005, ELECTRON LETT, V41, P1112, DOI 10.1049/el:20052577
   Mura V, 2015, INT CONF BIOMETR THE
   Nikam Shankar Bhausaheb, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P675, DOI 10.1109/ICETET.2008.134
   Nogueira RF, 2016, IEEE T INF FOREN SEC, V11, P1206, DOI 10.1109/TIFS.2016.2520880
   Nogueira RF, 2014, 2014 IEEE WORKSHOP ON BIOMETRIC MEASUREMENTS AND SYSTEMS FOR SECURITY AND MEDICAL APPLICATIONS (BIOMS) PROCEEDINGS, P22, DOI 10.1109/BIOMS.2014.6951531
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Park E., 2016, LECT NOTE INFORM
   Park EY, 2019, IMMUNOPHARM IMMUNOT, V41, P477, DOI 10.1080/08923973.2019.1628044
   Parthasaradhi STV, 2005, IEEE T SYST MAN CY C, V35, P335, DOI 10.1109/TSMCC.2005.848192
   Pereira L.F.A., 2016, IEEE INT C IMAGE PRO, V49, P13
   Ratha NK, 2001, LECT NOTES COMPUT SC, V2091, P223
   Reddy PV, 2008, IEEE T BIOMED CIRC S, V2, P328, DOI 10.1109/TBCAS.2008.2003432
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Uliyan DM, 2020, ENG SCI TECHNOL, V23, P264, DOI 10.1016/j.jestch.2019.06.005
   Vapnik V., 1963, AUTOMAT REM CONTR, V24, P774, DOI DOI 10.12691/JGG-2-3-9
   Wang CG, 2015, LECT NOTES COMPUT SC, V9428, P241, DOI 10.1007/978-3-319-25417-3_29
   Xia ZH, 2020, IEEE T SYST MAN CY-S, V50, P1526, DOI 10.1109/TSMC.2018.2874281
   Xia ZH, 2017, SIGNAL IMAGE VIDEO P, V11, P381, DOI 10.1007/s11760-016-0936-z
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yuan CS, 2020, IEEE T COGN DEV SYST, V12, P461, DOI 10.1109/TCDS.2019.2920364
   Yuan CS, 2019, IEEE ACCESS, V7, P26953, DOI 10.1109/ACCESS.2019.2901235
   Yuan CS, 2018, J INTERNET TECHNOL, V19, P1499, DOI 10.3966/160792642018091905021
   Zhang YL, 2020, IEEE ACCESS, V8, P183391, DOI 10.1109/ACCESS.2020.3027846
   Zhang YL, 2020, IEEE ACCESS, V8, P84141, DOI 10.1109/ACCESS.2020.2990909
   Zhang YL, 2019, IEEE ACCESS, V7, P91476, DOI 10.1109/ACCESS.2019.2927357
   Zhang YL, 2014, LECT NOTES COMPUT SC, V8833, P191, DOI 10.1007/978-3-319-12484-1_21
NR 73
TC 13
Z9 13
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2999
EP 3025
DI 10.1007/s00371-021-02173-8
EA JUN 2021
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000661442800001
DA 2024-07-18
ER

PT J
AU Wang, YD
   Cao, XT
   Miao, X
AF Wang, Yiding
   Cao, Xiaotong
   Miao, Xia
TI Cross-device recognition of dorsal hand vein images by two-stage
   coarse-to-fine matching
SO VISUAL COMPUTER
LA English
DT Article
DE Biometric identification; Cross-device interoperability; Distinctive
   efficient robust features; Hand vein images; Image segmentation
   adjustment
ID FEATURES
AB This paper addresses the problem of poor biometric recognition performance that is caused by greatly increased variations among the hand vein patterns of the same individuals as a result of using different imaging devices under different acquisition conditions and presents a novel solution based on two-stage coarse-to-fine matching. In particular, a global pattern descriptor is proposed as a geometrical reference for optimum image segmentation of vein patterns without significant over-segmentation and under-segmentation. In order to accommodate large cross-device variations, a control parameter is introduced to allow adjustment of segmented vein patterns, thereby enabling not only intra-class pattern similarities but also inter-class pattern dissimilarities to be increased. Furthermore, overlapping of principal vein patterns is proposed as a criterion for global coarse matching to reduce the number of candidates for identification, and distinctive efficient robust features are employed to provide a biological vision-based descriptor of salient local pattern characteristics for fine matching. Using a large dataset of 2000 cross-device hand vein images captured from two different near-infrared imaging devices and 100 hands, the efficacy of the proposed approach for a cross-device biometric system is demonstrated, with a recognition performance shown to be compatible to that of a single-device biometric system.
C1 [Wang, Yiding; Cao, Xiaotong; Miao, Xia] North China Univ Technol, Coll Informat Engn, Beijing, Peoples R China.
C3 North China University of Technology
RP Cao, XT (corresponding author), North China Univ Technol, Coll Informat Engn, Beijing, Peoples R China.
EM wangyd@ncut.edu.cn; caoxiaotong99@163.com; 1658259764@qq.com
RI Wang, Yixuan/GZK-6559-2022; wang, ya/HQZ-7558-2023; Wang,
   Yiru/JMB-2281-2023; Wang, Yijun/GXW-1763-2022; Wang,
   Yiping/IZQ-2052-2023; wang, yixuan/JGM-3893-2023; wang,
   yiran/IAP-0414-2023; wang, yi/KBB-3614-2024; wang, yixuan/GXW-2866-2022;
   wang, yi/HOF-6668-2023; Wang, Yin/HCI-9352-2022; Wu,
   Yiping/JJF-6185-2023; Wang, yanru/JAX-5241-2023; wang, yi/GVT-8516-2022;
   Wang, Yu/GZL-9655-2022
OI Wu, Yiping/0009-0000-6223-5786; 
FU National Natural Science Fund Committee of China [61673021]
FX This work was supported by the National Natural Science Fund Committee
   of China (NSFC no. 61673021).
CR Cross JM, 1995, 29TH ANNUAL 1995 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P20, DOI 10.1109/CCST.1995.524729
   Daubechies I., 1992, 10 LECT WAVELETS
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ding Y, 2005, 2005 IEEE International Conference on Mechatronics and Automations, Vols 1-4, Conference Proceedings, P2106
   Fleites FC, 2015, IEEE T MULTIMEDIA, V17, P1068, DOI 10.1109/TMM.2015.2433213
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Kang W, 2016, IEEE T MULTIMED, V18, P1, DOI [10.1109/TMM.2015.2505019, DOI 10.1109/TMM.2015.2505019]
   Khan NAM, 2009, P WORLD AC SCI ENG T, V37, P1091
   Kumar A, 2016, IEEE T IMAGE PROCESS, V99, P1
   Kumar A, 2009, IEEE T IMAGE PROCESS, V18, P2127, DOI 10.1109/TIP.2009.2023153
   Li H, 2016, IEEE T MULTIMED, V18, P1, DOI [10.1109/TMM.2016.2588538, DOI 10.1109/TMM.2016.2588538]
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mo DM, 2019, IEEE T MULTIMEDIA, V21, P3038, DOI 10.1109/TMM.2019.2916093
   Niblack W., 1986, An Introduction to Image Processing
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan J S., 2015, 2015 INT C INT INF H
   Pan ZY, 2019, IEEE ACCESS, V7, P90608, DOI 10.1109/ACCESS.2019.2927230
   Raghavendra R., 2017, P IEEE C COMP VIS PA, P144, DOI DOI 10.1109/TPAMI.2007.250596
   RODIECK R. W., 1965, VISION RES, V5, P583, DOI 10.1016/0042-6989(65)90033-7
   Ryu SJ, 2018, IEEE SENS J, V18, P7593, DOI 10.1109/JSEN.2018.2859815
   Sauvola J, 2000, PATTERN RECOGN, V33, P225, DOI 10.1016/S0031-3203(99)00055-2
   Standring S., 2005, GRAYS ANATOMY, V39th
   Tan CW, 2014, IEEE T IMAGE PROCESS, V23, P3962, DOI 10.1109/TIP.2014.2337714
   Tsai TJ, 2015, IEEE T MULTIMEDIA, V17, P1550, DOI 10.1109/TMM.2015.2454332
   Wang L, 2007, IET COMPUT VIS, V1, P113, DOI 10.1049/iet-cvi:20070009
   Wang YD, 2009, ICCIT: 2009 FOURTH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCES AND CONVERGENCE INFORMATION TECHNOLOGY, VOLS 1 AND 2, P1614, DOI 10.1109/ICCIT.2009.261
   Weng DW, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2409739
   Zhong DX, 2019, IEEE T INF FOREN SEC, V14, P3140, DOI 10.1109/TIFS.2019.2912552
   Zhong DX, 2019, IET BIOMETRICS, V8, P159, DOI 10.1049/iet-bmt.2018.5056
   Zhuang D., 2006, INTELLIGENT CONTROL, P10197
NR 31
TC 1
Z9 1
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3595
EP 3610
DI 10.1007/s00371-021-02190-7
EA JUN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000659034300004
DA 2024-07-18
ER

PT J
AU Huang, ZH
   Zhao, HM
   Zhan, J
   Li, HK
AF Huang, Zhihui
   Zhao, Huimin
   Zhan, Jin
   Li, Huakang
TI A multivariate intersection over union of SiamRPN network for visual
   tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Multivariate intersection over union; Scale invariance;
   SiamRPN
AB SiamPRN algorithm performs well in visual tracking, but it is easy to drift under occlusion and fast motion scenes because it uses l(1)-smooth loss function to measure the regression location of bounding box. In this paper, we propose a multivariate intersection over union (MIOU) loss in SiamRPN tracking framework. Firstly, MIOU loss includes three geometric factors in regression: the overlap area ratio, the center distance ratio, and the aspect ratio, which can better reflect the coincidence degree of target box and prediction box. Secondly, we improve the definition of aspect ratio loss to avoid gradient explosion, improve the optimization performance of prediction box. Finally, based on SiamPRN tracker, we compared the tracking performance of l1-smooth loss, IOU loss, GIOU loss, DIOU loss, and MIOU loss. Experimental results show that the MIOU loss has better target location regression than other loss functions on the OTB2015 and VOT2016 benchmark, especially for the challenges of occlusion, illumination change and fast motion.
C1 [Huang, Zhihui; Zhao, Huimin; Zhan, Jin; Li, Huakang] Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.
C3 Guangdong Polytechnic Normal University
RP Zhan, J (corresponding author), Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Peoples R China.
EM zhihuihuanggd@foxmail.com; zhaohuimin@gpnu.edu.cn;
   gszhanjin@gpnu.edu.cn; Iihualcang2020@163.com
FU National Natural Science Foundation of China [61772144, 62072122];
   Education Dept. of Guangdong Province [2019KSYS009]; Foreign Science and
   Technology Cooperation Plan Project of Guangzhou Science Technology and
   Innovation Commission [201807010059]
FX This research was funded by National Natural Science Foundation of China
   (Nos. 61772144, 62072122), and Education Dept. of Guangdong Province
   (No.2019KSYS009), Foreign Science and Technology Cooperation Plan
   Project of Guangzhou Science Technology and Innovation Commission (No.
   201807010059).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rubinstein R., 1999, Methodology and computing in applied probability, V1, P127
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tschannerl J, 2019, INFORM FUSION, V51, P189, DOI 10.1016/j.inffus.2019.02.005
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang N, 2013, P ADV NEURAL INFORM
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Z, 2018, NEUROCOMPUTING, V287, P68, DOI 10.1016/j.neucom.2018.01.076
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xia, 2020, PATTERN RECOGNIT, V8
   Xia HR, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20123370
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan YJ, 2018, COGN COMPUT, V10, P94, DOI 10.1007/s12559-017-9529-6
   Yeung, 2015, ARXIV2015
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zabalza J, 2016, NEUROCOMPUTING, V185, P1, DOI 10.1016/j.neucom.2015.11.044
   Zhang TZ, 2015, PROC CVPR IEEE, P150, DOI 10.1109/CVPR.2015.7298610
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou XZ, 2014, IEEE IMAGE PROC, P843, DOI 10.1109/ICIP.2014.7025169
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 34
TC 10
Z9 10
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2739
EP 2750
DI 10.1007/s00371-021-02150-1
EA MAY 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000650092900003
OA hybrid
DA 2024-07-18
ER

PT J
AU Ünlü, R
   Kiris, R
AF Unlu, Ramazan
   Kiris, Recep
TI Detection of damaged buildings after an earthquake with convolutional
   neural networks in conjunction with image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE VGG16; VGG-19; NASNet; Transfer learning; Damaged building detection
AB Detecting damaged buildings after an earthquake as quickly as possible is important for emergency teams to reach these buildings and save the lives of many people. Today, damaged buildings after the earthquake are carried out by the survivors contacting the authorities or using some air vehicles such as helicopters. In this study, AI-based systems were tested to detect damaged or destroyed buildings by integrating into street camera systems after unexpected disasters. For this purpose, we have used VGG-16, VGG-19, and NASNet convolutional neural network models which are often used for image recognition problems in the literature to detect damaged buildings. In order to effectively implement these models, we have first segmented all the images with the K-means clustering algorithm. Thereafter, for the first phase of this study, segmented images labeled "damaged buildings" and "normal" were classified and the VGG-19 model was the most successful model with a 90% accuracy in the test set. Besides, as the second phase of the study, we have created a multiclass classification problem by labeling segmented images as "damaged buildings," "less damaged buildings," and "normal." The same three architectures are used to achieve the most accurate classification results on the test set. VGG-19 and VGG-16, and NASNet have achieved considerable success in the test set with about 70%, 67%, and 62% accuracy, respectively.
C1 [Unlu, Ramazan] Gumushane Univ, Dept Management & Informat Syst, Gumushanevi Kampusu, TR-29100 Baglarbasi Mahallesi, Gumushane, Turkey.
   [Kiris, Recep] Gumushane Univ, Dept Emergency & Disaster Management, Gumushanevi Kampusu, TR-29100 Baglarbasi Mahallesi, Gumushane, Turkey.
C3 Gumushane University; Gumushane University
RP Ünlü, R (corresponding author), Gumushane Univ, Dept Management & Informat Syst, Gumushanevi Kampusu, TR-29100 Baglarbasi Mahallesi, Gumushane, Turkey.
EM ramazanunlu@gumushane.edu.tr
RI ÜNLÜ, RAMAZAN/C-3695-2019; Ünlü, Ramazan/GWR-0229-2022
OI ÜNLÜ, RAMAZAN/0000-0002-1201-195X; 
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Dell'Acqua F, 2012, P IEEE, V100, P2876, DOI 10.1109/JPROC.2012.2196404
   Dong LG, 2013, ISPRS J PHOTOGRAMM, V84, P85, DOI 10.1016/j.isprsjprs.2013.06.011
   Duarte D, 2017, INT ARCH PHOTOGRAMM, V42-2, P93, DOI 10.5194/isprs-archives-XLII-2-W6-93-2017
   Duarte D, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101636
   Dubois D, 2014, IEEE J-STARS, V7, P4167, DOI 10.1109/JSTARS.2014.2336236
   Eguchi RT, 2010, GEOTECH ENVIRON, V2, P295, DOI 10.1007/978-90-481-2238-7_15
   Fujita A, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P5, DOI 10.23919/MVA.2017.7986759
   Galarreta JF, 2015, NAT HAZARD EARTH SYS, V15, P1087, DOI 10.5194/nhess-15-1087-2015
   Gerke M, 2011, PHOTOGRAMM ENG REM S, V77, P885, DOI 10.14358/PERS.77.9.885
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110
   Kerle N, 2019, INT ARCH PHOTOGRAMM, V42-3, P187, DOI 10.5194/isprs-archives-XLII-3-W8-187-2019
   Kerle N, 2013, NAT HAZARD EARTH SYS, V13, P97, DOI 10.5194/nhess-13-97-2013
   Kerle N, 2010, INT J APPL EARTH OBS, V12, P466, DOI 10.1016/j.jag.2010.07.004
   Liang Y, 2019, VISUAL COMPUT, V36, P1
   Liu X, 2018, INT GEOSCI REMOTE SE, P7137, DOI 10.1109/IGARSS.2018.8518078
   Lu CH, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081318
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Lue E, 2014, APPL GEOGR, V52, P46, DOI 10.1016/j.apgeog.2014.04.014
   Marin C, 2015, IEEE T GEOSCI REMOTE, V53, P2664, DOI 10.1109/TGRS.2014.2363548
   Masri SF, 1996, J ENG MECH-ASCE, V122, P350, DOI 10.1061/(ASCE)0733-9399(1996)122:4(350)
   Novikov G, 2018, LECT NOTES BUS INF P, V320, P347, DOI 10.1007/978-3-319-93931-5_25
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Puente I, 2013, MEASUREMENT, V46, P2127, DOI 10.1016/j.measurement.2013.03.006
   Rupnik E, 2018, EUR J REMOTE SENS, V51, P543, DOI 10.1080/22797254.2018.1458584
   Schweier C, 2006, B EARTHQ ENG, V4, P177, DOI 10.1007/s10518-006-9005-2
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vetrivel A, 2016, ISPRS ANN PHOTO REM, V3, P355, DOI 10.5194/isprsannals-III-3-355-2016
   Vetrivel A, 2018, ISPRS J PHOTOGRAMM, V140, P45, DOI 10.1016/j.isprsjprs.2017.03.001
   Voigt S, 2016, SCIENCE, V353, P247, DOI 10.1126/science.aad8728
   WU X, 1992, COMPUT STRUCT, V42, P649, DOI 10.1016/0045-7949(92)90132-J
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhou SR, 2018, CMC-COMPUT MATER CON, V57, P11, DOI 10.32604/cmc.2018.02617
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 36
TC 10
Z9 10
U1 3
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 685
EP 694
DI 10.1007/s00371-020-02043-9
EA JAN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604490300004
DA 2024-07-18
ER

PT J
AU Li, M
   Zhang, MX
   Niu, DM
   Hassan, MU
   Zhao, XY
   Li, N
AF Li, Mai
   Zhang, Mingxuan
   Niu, Dongmei
   Hassan, Muhammad Umair
   Zhao, Xiuyang
   Li, Na
TI Point set registration based on feature point constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Computer graphics; Point set registration; Point-based models;
   Volumetric registration
AB Point set registration is a fundamental task in computer graphics. We present a novel volumetric registration method for three-dimensional solid shapes. The input data include a pair of three-dimensional point sets: a point set of a complete bone and another one from an incomplete bone, such as a hand bone with a hole in the wrist. We achieve the registration by deforming the complete model toward the incomplete model in the guidance of feature point constraints. Our method first performs an initial alignment owing to given data in an arbitrary position, orientation and scale, and then performs a volumetric registration that utilizes as much volumetric information as possible. Our solution is more adaptive to different sceneries such as the volume data have foramen, outlier and hole, and more accurate in comparison with both state-of-the-art rigid and non-rigid registration algorithms.
C1 [Li, Mai; Zhang, Mingxuan; Niu, Dongmei; Hassan, Muhammad Umair; Zhao, Xiuyang] Univ Jinan, Sch Informat Sci & Engn, Jinan, Peoples R China.
   [Li, Na] Qilu Inst Technol, Sch Comp Sci & Informat Engn, Jinan, Peoples R China.
C3 University of Jinan
RP Zhao, XY (corresponding author), Univ Jinan, Sch Informat Sci & Engn, Jinan, Peoples R China.
EM xiuyangzhao@gmail.com
RI Nusa, Nuhammad/JXY-5819-2024; Hassan, Muhammad Umair/L-9364-2016
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Alexiadis D S., 2012, P IM AN MULT INT SER, P1
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Bergevin R, 1996, IEEE T PATTERN ANAL, V18, P540, DOI 10.1109/34.494643
   Bernardini F, 2002, IEEE COMPUT GRAPH, V22, P59, DOI 10.1109/38.974519
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Cao GG, 2009, J MED INSTRUM, V33, P11
   CHEN Y, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2724, DOI 10.1109/ROBOT.1991.132043
   Deng W, 2017, VISUAL COMPUT, V34, P1399
   Fujiwara K, 2011, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2011.6126411
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Haehnel D., 2003, IJCAI'03, P915
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Ikemoto L, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P434, DOI 10.1109/IM.2003.1240279
   Jones MW, 2006, IEEE T VIS COMPUT GR, V12, P581, DOI 10.1109/TVCG.2006.56
   Kang D, 2017, IEEE INT C COMPUT, P504, DOI 10.1109/CSE-EUC.2017.95
   Kuhn H. W., 2010, NAV RES LOG, V52, P83
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Postelnicu G, 2007, LECT NOTES COMPUT SC, V4584, P675
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Sahillioglu Y, 2010, COMPUT VIS IMAGE UND, V114, P334, DOI 10.1016/j.cviu.2009.12.003
   Sahillioglu Y, 2015, MED IMAGE ANAL, V23, P15, DOI 10.1016/j.media.2015.03.005
   Sahillioglu Y, 2012, IEEE T PATTERN ANAL, V34, P2203, DOI 10.1109/TPAMI.2012.26
   Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169
   Sorkine O, 2007, S GEOM PROC, V4, P109, DOI [10.1145/1073204.1073323, DOI 10.1145/1073204.1073323]
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Zhang R, 2015, GRAPH MODELS, V79, P1, DOI 10.1016/j.gmod.2015.01.003
   Zhi-Quan Cheng, 2010, Proceedings of the Shape Modeling International (SMI 2010), P37, DOI 10.1109/SMI.2010.21
NR 31
TC 4
Z9 4
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1725
EP 1738
DI 10.1007/s00371-019-01771-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000559645500002
DA 2024-07-18
ER

PT J
AU Liu, SL
   Wang, XC
   Wu, ZK
   Seah, HS
AF Liu, Shaolong
   Wang, Xingce
   Wu, Zhongke
   Seah, Hock Soon
TI Shape correspondence based on Kendall shape space and RAG for 2D
   animation
SO VISUAL COMPUTER
LA English
DT Article
DE Vectorized 2D animation; Shape correspondence; Kendall shape space;
   Region adjacency graph
AB We introduce a 2D vectorized shape correspondence method based on Kendall shape space and region adjacency graph. We regard shape correspondence in 2D animation as a weighted bipartite graph matching problem and optimize it by means of various weights, including geometric attribute, local topological information, and global topological information in 2D animation drawing. The measuring method in Kendall shape space is introduced to determine similarity between two regions and a local adjacent region matching method is presented to associate the vicinity of the corresponding region by utilizing local topology information. Ultimately, we propose a globally optimal shape matching method, which exploits the global topological information to get the final result of the shape correspondence. Our approach can efficiently match associated regions that have exaggerated deformation and unstable topology between two shapes in 2D animation. It is also robust to the similarity transformation of shapes.
C1 [Liu, Shaolong; Wang, Xingce; Wu, Zhongke] Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Seah, Hock Soon] Nanyang Technol Univ NTU, Sch Engn & Comp Sci, Singapore, Singapore.
C3 Beijing Normal University; Nanyang Technological University
RP Wu, ZK (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
EM zwu@bnu.edu.cn
OI Shaolong, Liu/0000-0002-9265-0777
FU National Key R&D Program of China [2017YFB1002604, 2017YFB1402105];
   National Key Cooperation between the BRICS of China [2017YFE0100500];
   National Nature Science Foundation of China [61972041]; China
   Scholarship Council [201806040084]
FX The authors want to thank the anonymous reviewers for their constructive
   comments. This research was partially supported by the National Key R&D
   Program of China (No. 2017YFB1002604, No. 2017YFB1402105), the National
   Key Cooperation between the BRICS of China (No. 2017YFE0100500), the
   National Nature Science Foundation of China(No. 61972041), and the China
   Scholarship Council (No. 201806040084). Additionally, many thanks to
   Jiang jie, Liew Hongze and Wang Yanchao for their instructive advice and
   useful suggestions.
CR [Anonymous], INFORM MATH MODELLIN
   Bezerra H, 2006, SIBGRAPI, P71
   Cai JP, 2017, VISUAL COMPUT, V33, P1307, DOI 10.1007/s00371-016-1221-4
   Chang CW, 1997, J VISUAL COMP ANIMAT, V8, P165, DOI 10.1002/(SICI)1099-1778(199707)8:3<165::AID-VIS157>3.0.CO;2-2
   Kanamori Y, 2013, INT CONF IMAG VIS, P483, DOI 10.1109/IVCNZ.2013.6727062
   Kanamori Yoshihiro, 2012, SIGGRAPH ASIA 2012 T
   KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81
   Lladós J, 2001, IEEE T PATTERN ANAL, V23, P1137, DOI 10.1109/34.954603
   Lv CL, 2019, NEUROCOMPUTING, V355, P155, DOI 10.1016/j.neucom.2019.04.050
   Lv CL, 2019, PATTERN RECOGN, V88, P458, DOI 10.1016/j.patcog.2018.12.006
   Madeira JS, 1996, VISUAL COMPUT, V12, P1
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Qiu J, 2005, COMPUT ANIMAT VIRT W, V16, P463, DOI 10.1002/cav.86
   Qiu J, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P175
   Qiu J, 2008, INT J COMPUT GAMES T, V2008, DOI 10.1155/2008/135398
   Skora D., 2004, P 3 INT S NONPH AN R, P121, DOI DOI 10.1145/987657.987677
   Trémeau A, 2000, IEEE T IMAGE PROCESS, V9, P735, DOI 10.1109/83.841950
   Trigo PG, 2009, IEICE T INF SYST, VE92D, P1289, DOI 10.1587/transinf.E92.D.1289
   Valencia CE, 2016, BOL SOC MAT MEX, V22, P1, DOI 10.1007/s40590-015-0065-7
   WANG CH, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P440, DOI 10.1109/ICCV.1995.466906
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhu HC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925872
NR 22
TC 4
Z9 4
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2457
EP 2469
DI 10.1007/s00371-020-01958-7
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000560267200001
DA 2024-07-18
ER

PT J
AU Goswami, P
AF Goswami, Prashant
TI A survey of modeling, rendering and animation of clouds in computer
   graphics
SO VISUAL COMPUTER
LA English
DT Article
DE Clouds; Modeling; Rendering; Animation
ID 3-DIMENSIONAL NUMERICAL-MODEL; SIMULATION
AB Clouds play an important role in enhancing the realism of outdoor scenes in computer graphics (CG). Realistic cloud generation is a challenging task, which entails processes such as modeling, photorealistic rendering and simulation of the clouds. To these ends, several techniques have been proposed within the CG community in the last 4 decades with one or more of the above stated focuses. The growth of modern hardware has also enabled development of techniques that can achieve cloud display and animation at interactive frame rates. In this survey, we review the prominent work in the domain and also summarize the evolution of the research over the time.
C1 [Goswami, Prashant] BTH Karlskrona, Karlskrona, Sweden.
C3 Blekinge Institute Technology
RP Goswami, P (corresponding author), BTH Karlskrona, Karlskrona, Sweden.
EM prashant.goswami@bth.se
OI Goswami, Prashant/0000-0002-6920-9983
FU Blekinge Institute of Technology
FX Open access funding provided by Blekinge Institute of Technology.
CR Alldieck T., 2014, MODELLING CLOUDS HEM
   Barbosa CWF, 2015, COMPUT ANIMAT VIRT W, V26, P367, DOI 10.1002/cav.1657
   Bi SB, 2016, ISPRS INT J GEO-INF, V5, DOI 10.3390/ijgi5060086
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Bouthors A., 2004, MODELING CLOUDS SHAP
   Bouthors A, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P173
   Bouthors Antoine., 2006, Proceedings of the 2nd Eurographics Workshop on Natural Phenomena, (NPH'06), Vienna, Austria, September 5, P41
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chandrasekhar S., 1950, RAD TRANSFER
   Dobashi Y, 1999, VISUAL COMPUT, V15, P471, DOI 10.1007/s003710050193
   Dobashi Y, 1998, PACIFIC GRAPHICS '98, PROCEEDINGS, P53, DOI 10.1109/PCCGA.1998.731998
   Dobashi Y, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P390, DOI 10.1109/PCCGA.2001.962896
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   Dobashi Y., 2006, P CASA
   Dobashi Y., 2009, INTERACTIVE REALISTI
   Dobashi Y, 2007, VISUAL COMPUT, V23, P697, DOI 10.1007/s00371-007-0146-3
   Dobashi Y, 2017, VIS INFORM, V1, P1, DOI 10.1016/j.visinf.2017.01.001
   Dobashi Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366164
   Dobashi Y, 2010, COMPUT GRAPH FORUM, V29, P2083, DOI 10.1111/j.1467-8659.2010.01795.x
   Dobashi Y, 2010, SCI CHINA INFORM SCI, V53, P920, DOI 10.1007/s11432-010-0070-4
   Dobashi Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360693
   Dobashi Yoshinori., 1998, Proceedings of Computer Graphics and Imaging (CGI'98), Halifax, Canada, June 1-4, P251
   Duarte RPM, 2017, COMPUT GRAPH-UK, V67, P103, DOI 10.1016/j.cag.2017.06.005
   Ebert D. S., 1990, Computer Graphics, V24, P357, DOI 10.1145/97880.97918
   Ebert D.S., 1997, ACM SIGGRAPH VIS P A, V147
   Elek O, 2012, COMPUT GRAPH-UK, V36, P1109, DOI 10.1016/j.cag.2012.10.002
   Elhaddad A., 2016, P 29 INT C COMP AN S, P131
   Elinas P., 2000, Journal of Graphics Tools, V5, P33, DOI 10.1080/10867651.2000.10487531
   Fan X., 2014, J COMPUT, V25, P11
   Gardner G. Y., 1985, Computer Graphics, V19, P297, DOI 10.1145/325165.325248
   Goswami P., 2017, P 13 WORKSH VIRT REA, P1
   Goswami P., 2019, SMART TOOLS APPL GRA
   Goswami P, 2015, PROCEEDINGS - I3D 2015, P135, DOI 10.1145/2699276.2721396
   Griffith E.J., 2010, THESIS
   Grudziski J., 2009, COMP VIS COMP GRAPH, P117
   Harris J, 2003, GRAPHIS, P92
   Harris M. J., 2003, THESIS
   Harris MJ, 2001, COMPUT GRAPH FORUM, V20, pC76, DOI 10.1111/1467-8659.00500
   Hogfeldt R., 2016, THESIS
   Holton J.R., 1972, INTRO DYNAMIC METEOR, DOI DOI 10.1119/1.1987371
   Hufnagel R, 2007, PROCEEDINGS OF THE NINTH IASTED INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND IMAGING, P54
   Hufnagel Roland., 2012, Journal of WSCG, V20, P205
   Igawa N, 2004, SOL ENERGY, V77, P137, DOI 10.1016/j.solener.2004.04.016
   Iwasaki K., 2011, REAL TIME RENDERING
   Jhou WC, 2016, IEEE T MULTIMEDIA, V18, P4, DOI 10.1109/TMM.2015.2500031
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kallweit S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130880
   KLASSEN RV, 1987, ACM T GRAPHIC, V6, P215, DOI 10.1145/35068.35071
   Koehler O., 2009, THESIS
   Kol T.R., 2013, THESIS
   Kowsuwan N, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P387, DOI 10.1109/ISPACS.2009.5383819
   Kusumoto K., 2012, ACM SIGGRAPH ASIA PO
   Liao Horng-Shyang., 2004, Proceedings of the 2004 ACM SIGGRAPH International Conference on Virtual Reality Continuum and its Applications in Industry, P19
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Mathew R, 2005, ELEC COMP C, P1
   Matsuoka D, 2017, INT J MODEL SIMUL SC, V8, DOI 10.1142/S1793962317500519
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Max N., 1992, Proceedings. Visualization '92 (Cat. No.92CH3201-1), P179, DOI 10.1109/VISUAL.1992.235210
   Max N., 2004, THE J, V12, P277
   Max N. L., 1986, Computer Graphics, V20, P117, DOI 10.1145/15886.15899
   Miller Brett., 2012, ACM SIGGRAPH Talks
   Miyazaki R, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P363, DOI 10.1109/PCCGA.2001.962893
   Miyazaki R., 2004, FAST RENDERING METHO
   Miyazaki R., 2002, EUROGRAPHICS
   Mizuno R, 2004, J INF SCI ENG, V20, P219
   Mizuno R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P440, DOI 10.1109/PCCGA.2003.1238291
   Neyret F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P147
   Neyret F., 2000, RR3947 INRIA
   Neyret F., 1997, 8 EUR WORKSH COMP AN, P113
   Nishita T., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P218, DOI 10.1109/PCCGA.1999.803365
   Nishita T., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P379, DOI 10.1145/237170.237277
   Overby D, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P469, DOI 10.1109/PCCGA.2002.1167904
   Peng KC, 2013, IEEE I CONF COMP VIS, P2152, DOI 10.1109/ICCV.2013.267
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Premoze S., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P52
   Qin B, 2005, Computer Graphics, Imaging and Vision: New Trends, P285
   Qiu H., 2013, WSEAS T COMPUTERS, V12, P331
   Riley K, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P279, DOI 10.1109/VISUAL.2003.1250383
   RILEY K, 2004, P EUR S REND 2004 JU, P375
   ROSA M. G., 2013, THESIS
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   Sakas G., 1992, Computer Graphics Forum, V11, pC107, DOI 10.1111/1467-8659.1130107
   Sakas G., 1993, Visual Computer, V9, P200, DOI 10.1007/BF01901724
   Sakas G, 2001, PHOTOREALISTIC RENDE
   SCHLESINGER RE, 1975, J ATMOS SCI, V32, P934, DOI 10.1175/1520-0469(1975)032<0934:ATDNMO>2.0.CO;2
   SCHLESINGER RE, 1978, J ATMOS SCI, V35, P690, DOI 10.1175/1520-0469(1978)035<0690:ATDNMO>2.0.CO;2
   Schneider A., 2015, ADV REAL TIME RENDER
   Schpok J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P160
   Sethi M., 1997, THESIS
   Soares PMM, 2004, Q J ROY METEOR SOC, V130, P3365, DOI 10.1256/qj.03.223
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stiver M, 2010, LECT NOTES COMPUT SC, V6133, P1, DOI 10.1007/978-3-642-13544-6_1
   Suzuki K., 2019, MATH INSIGHTS ADV CO, V32, P109
   TAKAYA Y, 1993, J ATMOS SCI, V50, P574, DOI 10.1175/1520-0469(1993)050<0574:TMOUSO>2.0.CO;2
   Trembilski A, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P453
   Trembilski A., 2002, P 2002 ACM S APPL CO, P785
   Ummenhoffer T., 2005, REAL TIME RENDERING
   Wang N., 2003, ACM SIGGRAPH 2003 SK
   Wang Wenke, 2012, 2012 International Conference on Virtual Reality and Visualization (ICVRV 2012), P69, DOI 10.1109/ICVRV.2012.19
   Webanck A, 2018, COMPUT GRAPH FORUM, V37, P431, DOI 10.1111/cgf.13373
   Wijbenga JAM, 1995, COMPUT CARDIOL, P129, DOI 10.1109/CIC.1995.482589
   Wither J., 2008, P 5 EUR C SKETCH BAS, P113
   Xu JB, 2009, 2009 INTERNATIONAL SYMPOSIUM ON COMPUTER NETWORK AND MULTIMEDIA TECHNOLOGY (CNMT 2009), VOLUMES 1 AND 2, P69
   Yaeger L., 1986, Computer Graphics, V20, P85, DOI 10.1145/15886.15895
   Ye Z., 2014, THESIS
   Yu CM, 2011, J INF SCI ENG, V27, P891
   Yuan C., 2013, MODELING LARGE SCALE
   Yuan CQ, 2014, COMPUT GRAPH FORUM, V33, P288, DOI 10.1111/cgf.12350
   Yusov E., 2014, High-Performance Rendering of Realistic Cumulus Clouds Using Pre-computed Lighting. pages, P127
NR 109
TC 5
Z9 5
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1931
EP 1948
DI 10.1007/s00371-020-01953-y
EA AUG 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000559670300001
OA hybrid
DA 2024-07-18
ER

PT J
AU Sahillioglu, Y
AF Sahillioglu, Yusuf
TI Recent advances in shape correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Shape correspondence; Shape matching; Survey
ID MAPS; REPRESENTATION; OPTIMIZATION; REGISTRATION; DEFORMATION;
   DESCRIPTOR; ALGORITHM; ALIGNMENT
AB Important new developments have appeared since the most recent direct survey on shape correspondence published almost a decade ago. Our survey covers the period from 2011, their stopping point, to 2019, inclusive. The goal is to present the recent updates on correspondence computation between surfaces or point clouds embedded in 3D. Two tables summarizing and classifying the prominent, to our knowledge, papers of this period, and a large section devoted to their discussion lay down the foundation of our survey. The discussion is carried out in chronological order to reveal the distribution of various types of correspondence methods per year. We also explain our classification criteria along with the most basic solution examples. We finish with conclusions and future research directions.
C1 [Sahillioglu, Yusuf] METU, Dept Comp Engn, Ankara, Turkey.
C3 Middle East Technical University
RP Sahillioglu, Y (corresponding author), METU, Dept Comp Engn, Ankara, Turkey.
EM ys@ceng.metu.edu.tr
OI Sahillioglu, Yusuf/0000-0002-7997-4232
CR Aflalo Y, 2016, INT J COMPUT VISION, V118, P380, DOI 10.1007/s11263-016-0883-8
   Aigerman N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982412
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818099
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766921
   Aigerman N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601158
   Alhashim I, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818088
   Alhashim I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601102
   Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2017, IEEE C COMPUTER VISI
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2016, IEEE C COMP VIS PATT
   Azencot O, 2019, COMPUT GRAPH FORUM, V38, P13, DOI 10.1111/cgf.13786
   Azencot O, 2016, COMPUT GRAPH FORUM, V35, P55, DOI 10.1111/cgf.12963
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Biasotti S., 2014, Eurographics (state of the art reports), P135, DOI DOI 10.2312/EGST.20141039
   Boscaini D., 2016, ADV NEURAL INFORM PR, P3197
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Brunton A, 2014, GRAPH MODELS, V76, P70, DOI 10.1016/j.gmod.2013.11.003
   Carrière M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692
   Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236
   Corman E, 2015, COMPUT GRAPH FORUM, V34, P129, DOI 10.1111/cgf.12702
   Corman É, 2015, LECT NOTES COMPUT SC, V8928, P283, DOI 10.1007/978-3-319-16220-1_20
   Cosmo L, 2017, COMPUT GRAPH FORUM, V36, P209, DOI 10.1111/cgf.12796
   Cosmo L, 2019, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2019.00771
   Denitto M, 2017, IEEE I CONF COMP VIS, P4270, DOI 10.1109/ICCV.2017.457
   Dubrovina A, 2011, ADV DATA SCI ADAPT, V3, P203, DOI 10.1142/S1793536911000829
   Dyke R. M., 2019, EUR WORKSH 3D OBJ RE
   Dyke RM, 2019, COMPUT AIDED GEOM D, V71, P142, DOI 10.1016/j.cagd.2019.04.014
   Eisenberger M, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13785
   Ezuz D, 2019, COMPUT GRAPH FORUM, V38, P121, DOI 10.1111/cgf.13624
   Ezuz D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3202660
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P165, DOI 10.1111/cgf.13254
   Fish N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982409
   Ganapathi-Subramanian V, 2016, COMPUT GRAPH FORUM, V35, P121, DOI 10.1111/cgf.12969
   Gehre A, 2018, COMPUT GRAPH FORUM, V37, P1, DOI 10.1111/cgf.13337
   Groueix T, 2019, COMPUT GRAPH FORUM, V38, P123, DOI 10.1111/cgf.13794
   Guo H, 2016, VISUAL COMPUT, V32, P1511, DOI 10.1007/s00371-015-1136-5
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hu R, 2018, COMPUT GRAPH FORUM, V37, P603, DOI 10.1111/cgf.13385
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Huang QX, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366186
   Huang QX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601111
   Huang RQ, 2019, COMPUT GRAPH FORUM, V38, P187, DOI 10.1111/cgf.13799
   Huang RQ, 2017, COMPUT GRAPH FORUM, V36, P151, DOI 10.1111/cgf.13253
   Jacobson A., 2014, ACM SIGGRAPH 2014 CO, V24
   Kim VG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601117
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Kim VG, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185550
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kovnatsky A, 2015, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2015.7298692
   Küpçü E, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102808
   Lahner Z., 2016, P EUROGRAPHICS WORKS
   Lee SC, 2019, COMPUT GRAPH FORUM, V38, P27, DOI 10.1111/cgf.13787
   LI X, 2014, ACM COMPUT SURV, V47, p34:1
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Lim Isaak, 2018, P EUR C COMP VIS ECC
   Litany O, 2017, COMPUT GRAPH FORUM, V36, P247, DOI 10.1111/cgf.13123
   Litany O, 2016, COMPUT GRAPH FORUM, V35, P135, DOI 10.1111/cgf.12970
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Liu TQ, 2012, COMPUT GRAPH FORUM, V31, P1607, DOI 10.1111/j.1467-8659.2012.03166.x
   Liu ZB, 2013, J COMPUT SCI TECH-CH, V28, P836, DOI 10.1007/s11390-013-1382-9
   Mandad M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073671
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Melzi S, 2018, COMPUT GRAPH FORUM, V37, P20, DOI 10.1111/cgf.13309
   Melzi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3144454
   Nogneng D, 2018, COMPUT GRAPH FORUM, V37, P179, DOI 10.1111/cgf.13352
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ovsjanikov M, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12167
   Ovsjanikov M, 2011, COMPUT GRAPH FORUM, V30, P1503, DOI 10.1111/j.1467-8659.2011.02024.x
   Panozzo D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461935
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Pokrass J., 2011, P SCAL SPAC VAR METH, P2
   Poulenard A, 2018, COMPUT GRAPH FORUM, V37, P13, DOI 10.1111/cgf.13487
   Ren J, 2019, COMPUT GRAPH FORUM, V38, P39, DOI 10.1111/cgf.13788
   Ren J, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275040
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P700, DOI 10.1111/cgf.13160
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P222, DOI 10.1111/cgf.12797
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Rodolà E, 2012, PROC CVPR IEEE, P182, DOI 10.1109/CVPR.2012.6247674
   Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P121, DOI 10.1111/cgf.12480
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P63, DOI 10.1111/cgf.12278
   Sahillioglu Y, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12007
   Sahillioglu Y, 2012, COMPUT GRAPH FORUM, V31, P2233, DOI 10.1111/j.1467-8659.2012.03216.x
   Sahillioglu Y, 2011, COMPUT GRAPH FORUM, V30, P1461, DOI 10.1111/j.1467-8659.2011.02020.x
   Sahillioglu Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3243593
   Sahillioglu Y, 2015, COMPUT GRAPH-UK, V53, P156, DOI 10.1016/j.cag.2015.10.003
   Sahillioglu Y, 2015, MED IMAGE ANAL, V23, P15, DOI 10.1016/j.media.2015.03.005
   Sahillioglu Y, 2012, IEEE T PATTERN ANAL, V34, P2203, DOI 10.1109/TPAMI.2012.26
   Shapira N, 2014, COMPUT GRAPH FORUM, V33, P281, DOI 10.1111/cgf.12453
   Shoham M, 2019, COMPUT GRAPH FORUM, V38, P55, DOI 10.1111/cgf.13789
   Shtern A, 2015, COMPUT VIS IMAGE UND, V140, P21, DOI 10.1016/j.cviu.2015.02.004
   Solomon J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925903
   Solomon J, 2012, COMPUT GRAPH FORUM, V31, P1617, DOI 10.1111/j.1467-8659.2012.03167.x
   Tam GKL, 2014, COMPUT GRAPH FORUM, V33, P137, DOI 10.1111/cgf.12439
   Tam GKL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2517967
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tevs A, 2011, COMPUT GRAPH FORUM, V30, P543, DOI 10.1111/j.1467-8659.2011.01879.x
   van Kaick O, 2013, COMPUT GRAPH FORUM, V32, P189, DOI 10.1111/cgf.12084
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P553, DOI 10.1111/j.1467-8659.2011.01893.x
   van Kaick Oliver., 2010, Proc. of Eurographics State-of-the-art Report, P1
   Vestner M., 2017, P COMP VIS PATT REC
   Vestner M, 2017, INT CONF 3D VISION, P517, DOI 10.1109/3DV.2017.00065
   Wang L, 2018, COMPUT GRAPH FORUM, V37, P27, DOI 10.1111/cgf.13488
   Yoshiyasu Y, 2016, COMPUT GRAPH-UK, V60, P9, DOI 10.1016/j.cag.2016.07.002
   Zhang ZY, 2013, COMPUT GRAPH FORUM, V32, P355, DOI 10.1111/cgf.12243
   Zheng XP, 2017, IEEE I CONF COMP VIS, P938, DOI 10.1109/ICCV.2017.107
   Zheng YY, 2014, COMPUT GRAPH FORUM, V33, P115, DOI 10.1111/cgf.12309
   Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20
   Zhu CY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073613
NR 115
TC 71
Z9 76
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1705
EP 1721
DI 10.1007/s00371-019-01760-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700010
DA 2024-07-18
ER

PT J
AU Kukreja, S
   Kasana, G
   Kasana, SS
AF Kukreja, Sonal
   Kasana, Geeta
   Kasana, Singara Singh
TI Extended visual cryptography-based copyright protection scheme for
   multiple images and owners using LBP-SURF descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE Copyright protection; SURF; LBP; Extended visual cryptography;
   Normalized correlation; Multiple cover images; Multiple owners
ID WATERMARKING SCHEME; DIGITAL IMAGES; ROBUST; CLASSIFICATION; HYBRID
AB Existing visual cryptography (VC)-based copyright protection schemes (Amiri and Mohaddam in Multimed Tools Appl 75(14):8527-8543, 2016; Liu and Wu in IET Inf Secur 5(2):121-128, 2011) for multiple images provide meaningless shares to the owners. These shares create a suspicion that some secret information is shared. Also, these schemes require the share of every owner to prove the copyright. If any of the ownership share is not available, the copyright of these owners cannot be verified. This makes the usage of schemes restricted. To address these issues, an extended visual cryptography-based copyright protection scheme is proposed for multiple images with multiple owners. This scheme provides meaningful ownership shares to the owners, and their copyright can be verified by using a qualified set of owner shares. In this scheme, three types of shares are used, i.e., master share, ownership share and key share. The proposed scheme ensures robustness against different geometrical attacks, especially the rotation attack, asLBPandSURFtogether represent the host image efficiently. There is no restriction on watermark size, asSURFgives a flexibility to select any number of feature points. Usage ofLBPensures no false positive cases. Each of the ownership shares is created using the master share and the watermark. The ownership share is used to create a key share which is stored with the Trusting Authority (TA). To prove the copyright of multiple images, the ownership images and key share are superimposed to retrieve the watermark. The experimental results show that the scheme clearly verifies the copyright of digital images and is robust against several image processing attacks while having high imperceptibility. Comparisons with the existing copyright protection schemes show better performance of the proposed scheme.
C1 [Kukreja, Sonal; Kasana, Geeta; Kasana, Singara Singh] Thapar Institue Engn & Technol, Comp Sci & Engn Dept, Patiala, Punjab, India.
C3 Thapar Institute of Engineering & Technology
RP Kukreja, S (corresponding author), Thapar Institue Engn & Technol, Comp Sci & Engn Dept, Patiala, Punjab, India.
EM sonal.kukreja@thapar.edu; gkasana@thapar.edu; singara@thapar.edu
RI Kukreja, Dr. Sonal/HLQ-2114-2023
CR Amiri T, 2016, MULTIMED TOOLS APPL, V75, P8527, DOI 10.1007/s11042-015-2770-7
   ATENIESE G, 1996, THEORETICAL COMPUTER, V250, P1
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chang CC, 2002, IMAGING SCI J, V50, P133, DOI 10.1080/13682199.2002.11784400
   Chen TH, 2009, COMPUT STAND INTER, V31, P1, DOI 10.1016/j.csi.2007.09.001
   Cheng YM, 2006, VISUAL COMPUT, V22, P845, DOI 10.1007/s00371-006-0069-4
   Cox IJ, 2002, EURASIP J APPL SIG P, V2002, P126, DOI 10.1155/S1110865702000525
   Devi BP, 2016, SPRINGERPLUS, V5, DOI 10.1186/s40064-016-2733-0
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Fatahbeygi A, 2019, J INF SECUR APPL, V45, P71, DOI 10.1016/j.jisa.2019.01.005
   Hou YC, 2000, 2000 5TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I-III, P992, DOI 10.1109/ICOSP.2000.891692
   Hou YC, 2016, TURK J ELECTR ENG CO, V24, P4063, DOI 10.3906/elk-1405-180
   Hsieh S-L, 2005, P WORLD ACAD SCI ENG, V10
   Hsu CS, 2005, OPT ENG, V44, DOI 10.1117/1.1951647
   Hurrah NN, 2019, FUTURE GENER COMP SY, V94, P654, DOI 10.1016/j.future.2018.12.036
   Hwang R.-J, 2000, TAMKANG J SCI ENG, V3, P97
   Karami S., 2017, P NEWF EL COMP ENG C
   Kutter M, 1999, PROC SPIE, V3657, P226, DOI 10.1117/12.344672
   Lee J.-S., 2014, COMPUTER SCI INFORM
   Li GD, 2019, VISUAL COMPUT, V35, P1267, DOI 10.1007/s00371-018-1574-y
   Liu F, 2011, IET INFORM SECUR, V5, P121, DOI 10.1049/iet-ifs.2009.0183
   Lou DC, 2007, COMPUT STAND INTER, V29, P125, DOI 10.1016/j.csi.2006.02.003
   Murali P, 2018, OPTIK, V170, P242, DOI 10.1016/j.ijleo.2018.04.050
   Naor M, 1994, LECT NOTES COMPUTER, V950, P1, DOI [10.1007/BFb0053419, DOI 10.1007/BFB0053419]
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Prabhakar C.J., 2012, P 8 IND C COMP VIS G
   Rawat S, 2012, AEU-INT J ELECTRON C, V66, P955, DOI 10.1016/j.aeue.2012.04.004
   Roy S, 2017, MULTIMED TOOLS APPL, V76, P3577, DOI 10.1007/s11042-016-3902-4
   Shao ZH, 2016, SIGNAL PROCESS-IMAGE, V48, P12, DOI 10.1016/j.image.2016.09.001
   Thanki R, 2019, J INF SECUR APPL, V46, P231, DOI 10.1016/j.jisa.2019.03.017
   Voloshynovskiy S, 2001, IEEE COMMUN MAG, V39, P118, DOI 10.1109/35.940053
   Wang CC, 2000, IEICE T FUND ELECTR, VE83A, P1589
   Wang MS, 2009, COMPUT STAND INTER, V31, P757, DOI 10.1016/j.csi.2008.09.003
   Wang MS, 2007, OPT ENG, V46, DOI 10.1117/1.2746906
   Wang YP, 2009, IEEE T VIS COMPUT GR, V15, P285, DOI 10.1109/TVCG.2008.101
   Wu JH, 2005, VISUAL COMPUT, V21, P848, DOI 10.1007/s00371-005-0311-5
   Xue M., 2019, SSL NOVEL IMAGE HASH
   Zhao C, 2015, INT J INF SECUR PRIV, V9, P1, DOI 10.4018/IJISP.2015040101
   Zhu C, 2013, PATTERN RECOGN, V46, P1949, DOI 10.1016/j.patcog.2013.01.003
NR 39
TC 8
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1481
EP 1498
DI 10.1007/s00371-020-01883-9
EA JUL 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000551368400001
DA 2024-07-18
ER

PT J
AU Schoentgen, A
   Zehnder, J
   Poulin, P
   Thomaszewski, B
   Meseure, P
   Darles, E
AF Schoentgen, Arnaud
   Zehnder, Jonas
   Poulin, Pierre
   Thomaszewski, Bernhard
   Meseure, Philippe
   Darles, Emmanuelle
TI A density-accurate tracking solution for smoke upresolution
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based animation; Smoke simulation; Fluid control
ID ANIMATION
AB Controlling smoke simulations is a notoriously challenging and tedious task, usually requiring many trial-and-error iterations that prevent using expensive computations at high resolutions. Unfortunately, naively going from a more efficient low-resolution simulation to a high-quality high-resolution simulation usually results in a different behavior of smoke animation. Moreover, the longer the animation, the more different the result. We propose a tracking procedure where we optimally modify the velocity field of the simulation in order to make the smoke density distribution closely follow the low-resolution density in both space and time. We demonstrate the benefits of our approach by accurately tracking various 2D and 3D simulations. The resulting animations are predictable, preserving the coarse density distribution of the low-resolution guides, while being enhanced with plausible high-frequency details.
C1 [Schoentgen, Arnaud; Zehnder, Jonas; Poulin, Pierre; Thomaszewski, Bernhard] Univ Montreal, Montreal, PQ, Canada.
   [Schoentgen, Arnaud; Meseure, Philippe; Darles, Emmanuelle] Univ Poitiers, Poitiers, France.
C3 Universite de Montreal; Universite de Poitiers
RP Schoentgen, A (corresponding author), Univ Montreal, Montreal, PQ, Canada.; Schoentgen, A (corresponding author), Univ Poitiers, Poitiers, France.
EM arnaudschoentgen@gmail.com
OI Thomaszewski, Bernhard/0000-0001-8086-7664; Schoentgen,
   Arnaud/0000-0001-5762-3450
FU NSERC; Universite de Montreal
FX Pierre Poulin acknowledges financial support from NSERC and the
   Universite de Montreal.
CR Adachi Y, 2012, PROCEEDINGS OF THE 1ST INTERNATIONAL CONFERENCE ON 3D MATERIALS SCIENCE, P37
   Ando R, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766935
   Bergou M, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239501
   Chu MY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073643
   Eckert ML, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356545
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Gregson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601147
   Huang Ruoguan., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'11, P177, DOI DOI 10.1145/2019406.2019430
   Inglis T, 2017, COMPUT GRAPH FORUM, V36, P354, DOI 10.1111/cgf.13084
   Kim B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356560
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Liu SG, 2014, COMPUT ANIMAT VIRT W, V25, P475, DOI 10.1002/cav.1574
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Mercier O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818115
   Milliez A, 2018, COMPUT GRAPH FORUM, V37, P115, DOI 10.1111/cgf.13517
   Nielsen MB, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964978
   Nielsen MB, 2010, COMPUT GRAPH FORUM, V29, P705, DOI 10.1111/j.1467-8659.2009.01640.x
   Nielsen MichaelB., 2009, SCA '09: Proc. of the 2009 ACM SIGGRAPH/Eurographics Symp. on Comput. Anim, P217
   Pan ZR, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3016963
   Sato S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201398
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Thuerey N., 2016, MantaFlow
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Xiao XY, 2019, COMPUT GRAPH FORUM, V38, P431, DOI 10.1111/cgf.13649
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Yuan Z, 2011, ACM T GRAPHIC, V30, P6, DOI DOI 10.1145/2070781.2024170
   Zhang S, 2015, PROCEEDINGS - I3D 2015, P61, DOI 10.1145/2699276.2699287
NR 30
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2299
EP 2311
DI 10.1007/s00371-020-01889-3
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000547790100001
DA 2024-07-18
ER

PT J
AU Zikiou, N
   Lahdir, M
   Helbert, D
AF Zikiou, Nadia
   Lahdir, Mourad
   Helbert, David
TI Support vector regression-based 3D-wavelet texture learning for
   hyperspectral image compression
SO VISUAL COMPUTER
LA English
DT Article
DE Remote sensing; Hyperspectral image; Lossless compression; Image coding;
   Spectral classification; Texture feature learning
ID LOSSLESS COMPRESSION; QUALITY ASSESSMENT; CLASSIFICATION; EFFICIENT;
   ALGORITHM; COMPONENT; CNN
AB Hyperspectral imaging is known for its rich spatial-spectral information. The spectral bands provide the ability to distinguish substances spectra which are substantial for analyzing materials. However, high-dimensional data volume of hyperspectral images is problematic for data storage. In this paper, we present a lossy hyperspectral image compression system based on the regression of 3D wavelet coefficients. The 3D wavelet transform is applied to sparsely represent the hyperspectral images (HSI). A support vector machine regression is then applied on wavelet details and provides vector supports and weights which represent wavelet texture features. To achieve the best possible overall rate-distortion performance after regression, entropy encoding based on run-length encoding and arithmetic encoding is used. To preserve the spatial pertinent information of the image, the lowest sub-band wavelet coefficients are furthermore encoded by a lossless coding with differential pulse code modulation. Spectral and spatial redundancies are thus substantially reduced. Experimental tests are performed over several HSI from airborne and spaceborne sensors and compared with the main existing algorithms. The obtained results show that the proposed compression method has high performances in terms of rate distortion and spectral fidelity. Indeed, high PSNRs and classification accuracies, which could exceed 40.65 dB and 75.8%, respectively, are observed for all decoded HSI images and overpass those given by many cited famous methods. In addition, the evaluation of detection and compression over various bands shows that spectral information is preserved using our compression method.
C1 [Zikiou, Nadia; Lahdir, Mourad] UMMTO, Lab Anal & Modeling Random Phenomena LAMPA, BP 17 RP, Tizi Ouzou 15000, Algeria.
   [Helbert, David] Univ Poitiers, CNRS, Xlim, 11 Bd Marie & Pierre Curie,BP 30179,86962, F-86962 Chasseneuil, France.
C3 Universite Mouloud Mammeri de Tizi Ouzou; Universite de Poitiers; Centre
   National de la Recherche Scientifique (CNRS)
RP Zikiou, N (corresponding author), UMMTO, Lab Anal & Modeling Random Phenomena LAMPA, BP 17 RP, Tizi Ouzou 15000, Algeria.
EM zikiounadia@gmail.com; mlahdir@yahoo.fr; david.helbert@xlim.fr
RI Lahdir, Mourad/U-2852-2018
OI Lahdir, Mourad/0000-0001-7334-6959; Helbert, David/0000-0001-6518-1509;
   ZIKIOU, Nadia/0000-0001-7943-5143
CR Amrani N, 2016, IEEE T GEOSCI REMOTE, V54, P5616, DOI 10.1109/TGRS.2016.2569485
   Andries B, 2017, VISUAL COMPUT, V33, P1121, DOI 10.1007/s00371-016-1269-1
   [Anonymous], J COMPUTER COMMUNICA
   [Anonymous], 1998, EL IM 99
   [Anonymous], INT COMP VIS IM PROC
   [Anonymous], INT J SCI ENG INVEST
   [Anonymous], IEEE T GEOSCI REMOTE
   [Anonymous], INT J ADV RES COMPUT
   [Anonymous], 2013, INT J ADV RES ELECT
   [Anonymous], 2014 1 INT IEEE IM P
   [Anonymous], 2011, J SIGNAL INF PROC, DOI DOI 10.4236/JSIP.2011.24045
   Aulí-Llinàs F, 2013, INFORM SCIENCES, V239, P266, DOI 10.1016/j.ins.2013.03.027
   Boussakta S, 2004, IEEE T SIGNAL PROCES, V52, P992, DOI 10.1109/TSP.2004.823472
   Du Q, 2007, IEEE GEOSCI REMOTE S, V4, P201, DOI 10.1109/LGRS.2006.888109
   Du Q, 2014, IEEE J-STARS, V7, P2237, DOI 10.1109/JSTARS.2013.2274527
   Fang LY, 2017, J OPT SOC AM A, V34, P252, DOI 10.1364/JOSAA.34.000252
   García-Vílchez F, 2011, IEEE GEOSCI REMOTE S, V8, P253, DOI 10.1109/LGRS.2010.2062484
   Govindan P, 2015, IET SIGNAL PROCESS, V9, P267, DOI 10.1049/iet-spr.2014.0186
   He LT, 2019, VISUAL COMPUT, V35, P151, DOI 10.1007/s00371-017-1440-3
   Hegde G, 2012, INT J SIGNAL IMAGING, V5, P158, DOI 10.1504/IJSISE.2012.049851
   HOWARD PG, 1994, P IEEE, V82, P857, DOI 10.1109/5.286189
   Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619
   Huang BC, 2015, SPECTROSC LETT, V48, P528, DOI 10.1080/00387010.2014.920888
   Jiao RH, 2005, LECT NOTES COMPUT SC, V3644, P747
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   Karami A., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P809, DOI 10.1109/ISSPA.2012.6310664
   Karami A, 2012, IEEE J-STARS, V5, P444, DOI 10.1109/JSTARS.2012.2189200
   Khiari-Hili N, 2016, IET IMAGE PROCESS, V10, P409, DOI 10.1049/iet-ipr.2015.0239
   Kim BJ, 1997, IEEE DATA COMPR CONF, P251, DOI 10.1109/DCC.1997.582048
   Lee H, 2017, IEEE T IMAGE PROCESS, V26, P4843, DOI 10.1109/TIP.2017.2725580
   Lee MS, 2015, IET IMAGE PROCESS, V9, P1057, DOI 10.1049/iet-ipr.2014.0229
   Li W, 2017, IEEE T GEOSCI REMOTE, V55, P844, DOI 10.1109/TGRS.2016.2616355
   Li YC, 2007, LECT NOTES COMPUT SC, V4681, P922
   MALLAT SG, 1989, IEEE T ACOUST SPEECH, V37, P2091, DOI 10.1109/29.45554
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Mercer J, 1909, PHILOS T R SOC LOND, V209, P415, DOI 10.1098/rsta.1909.0016
   Meyer Y., 1992, Wavelets and applications
   Mou LC, 2017, IEEE T GEOSCI REMOTE, V55, P3639, DOI 10.1109/TGRS.2016.2636241
   Pearlman WA, 2004, IEEE T CIRC SYST VID, V14, P1219, DOI 10.1109/TCSVT.2004.835150
   Prathap I, 2014, COMM COM INF SC, V467, P230
   Qian YT, 2013, IEEE J-STARS, V6, P499, DOI 10.1109/JSTARS.2012.2232904
   Ran LY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17102421
   Reddy B.E., 2012, Signal Image Process, V3, P213, DOI [10.5121/sipij.2012.3216, DOI 10.5121/SIPIJ.2012.3216]
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Tang X, 2006, HYPERSPECTRAL DATA COMPRESSION, P273, DOI 10.1007/0-387-28600-4_10
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Vapnik V., 2013, The nature of statistical learning theory
   Venugopal D, 2016, OPTIK, V127, P754, DOI 10.1016/j.ijleo.2015.10.154
   Villa A, 2011, IEEE T GEOSCI REMOTE, V49, P4865, DOI 10.1109/TGRS.2011.2153861
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Q, 2010, EXPERT SYST APPL, V37, P2388, DOI 10.1016/j.eswa.2009.07.057
   Zhang LF, 2015, NEUROCOMPUTING, V147, P358, DOI 10.1016/j.neucom.2014.06.052
   Zhang MM, 2018, IEEE T IMAGE PROCESS, V27, P2623, DOI 10.1109/TIP.2018.2809606
   Zhong ZL, 2018, IEEE T GEOSCI REMOTE, V56, P847, DOI 10.1109/TGRS.2017.2755542
NR 56
TC 18
Z9 18
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1473
EP 1490
DI 10.1007/s00371-019-01753-z
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000013
DA 2024-07-18
ER

PT J
AU Song, MF
AF Song, Mofei
TI A personalized active method for 3D shape classification
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape classification; Active learning; Online learning
ID ANNOTATION; RETRIEVAL
AB To efficiently and flexibly classify 3D shape in a user-adaptive way, this paper proposes a novel interactive system by incorporating active learning, online learning and user intervention. Given a shape collection, our system iteratively alternates the interactive annotation and verification until the labels of all the shapes are confirmed explicitly by the users. The main advantage is that it provides faster interactive classification rates than alternative approaches. Our system achieves this goal by a unified active learning algorithm that selects the shapes to be annotated or verified for the next iteration. The shape selection step is solved by maximizing a probability model for simulating the time cost of human input during manual intervention. The selected shapes are then pushed to the users along with their category predictions, which are generated by a proposed weighted blending framework. After manually confirming or refining the automatic predictions, we use an extended soft confidence-weighted learning method to update the classifier incrementally for the subsequent active selection and shape classification in turn. Experimental results demonstrated the effectiveness of the proposed method.
C1 [Song, Mofei] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Song, Mofei] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Southeast University - China; Nanjing University
RP Song, MF (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.; Song, MF (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM songmf@seu.edu.cn
OI Song, Mofei/0000-0002-9912-1560
FU National Natural Science Foundation of China [61906036]; Open Research
   Project of State Key Laboratory of Novel Software Technology (Nanjing
   University) [KFKT2019B02]
FX This work is supported by National Natural Science Foundation of China
   (61906036), the Open Research Project of State Key Laboratory of Novel
   Software Technology (Nanjing University) (KFKT2019B02).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2012, ICML
   Barra V, 2014, VISUAL COMPUT, V30, P1247, DOI 10.1007/s00371-014-0926-5
   BOYKO A, 2014, P 27 ANN ACM S US IN, P33, DOI DOI 10.1145/2642918.2647418
   Chang Angel X., 2015, arXiv
   CHEN L, 2015, ICMR
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DUAN M, 2009, P INT C IM VID RETR, P1
   Giorgi D, 2010, VISUAL COMPUT, V26, P1321, DOI 10.1007/s00371-010-0524-0
   GIORGI D, 2009, P 2 EUR C 3D OBJ RET, P45
   Hoi SCH, 2014, J MACH LEARN RES, V15, P495
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373
   Kapoor A, 2007, IEEE I CONF COMP VIS, P134
   KRISHNAKUMAR A, 2007, TECH REP, V42
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Leng B, 2015, NEUROCOMPUTING, V168, P761, DOI 10.1016/j.neucom.2015.05.048
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Liu YM, 2011, IEEE T PATTERN ANAL, V33, P1022, DOI 10.1109/TPAMI.2010.142
   López-Sastre RJ, 2013, COMPUT GRAPH-UK, V37, P473, DOI 10.1016/j.cag.2013.04.003
   Marini S, 2011, VISUAL COMPUT, V27, P1005, DOI 10.1007/s00371-011-0612-9
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Qi CR, 2017, ADV NEUR IN, V30
   Qian ZM, 2015, SIGNAL PROCESS-IMAGE, V34, P61, DOI 10.1016/j.image.2015.03.008
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Song MF, 2017, PROC INT C TOOLS ART, P469, DOI 10.1109/ICTAI.2017.00078
   Song MF, 2018, LECT NOTES COMPUT SC, V10704, P291, DOI 10.1007/978-3-319-73603-7_24
   Song MF, 2017, GRAPH MODELS, V89, P14, DOI 10.1016/j.gmod.2017.01.001
   Song MF, 2015, COMPUT AIDED GEOM D, V35-36, P192, DOI 10.1016/j.cagd.2015.03.009
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tabia H, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2438653.2438668
   Toldo R, 2010, VISUAL COMPUT, V26, P1257, DOI 10.1007/s00371-010-0519-x
   Valentin J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2751556
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   WELCH WJ, 1982, J STAT COMPUT SIM, V15, P17, DOI 10.1080/00949658208810560
   Wong YS, 2015, COMPUT GRAPH FORUM, V34, P447, DOI 10.1111/cgf.12574
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   XU Z, 2003, ADV INFORM RETRIEVAL, P11
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Zhang FQ, 2015, COMPUT AIDED DESIGN, V58, P2, DOI 10.1016/j.cad.2014.08.008
NR 42
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 497
EP 514
DI 10.1007/s00371-020-01819-3
EA FEB 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000516203900001
DA 2024-07-18
ER

PT J
AU Ardakani, HK
   Mousavinia, A
   Safaei, F
AF Ardakani, H. K.
   Mousavinia, A.
   Safaei, Farzad
TI Four points: one-pass geometrical camera calibration algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Camera calibration; Single image; Machine vision; Camera parameters
ID 3D RECONSTRUCTION; VIDEO
AB Conventional geometrical camera calibration algorithms are usually based on running some iterative algorithms on test images obtained carefully from reference objects with precisely known pattern. Providing these test images and running the iterative algorithms are often time-consuming and sometimes costly. In addition, they are usually very sensitive to image distortions. To overcome these problems, an efficient and practical camera calibration method using a single rectangular reference object is proposed. The reference object can be as simple as an A4-size paper placed on a table. Using the coordinate of four corner points of reference image, generate eight equations. This paper first describes an analytical method to solve the equations and then provides a step-by-step algorithm. The proposed algorithm is evaluated using simulated images generated with both Autodesk 3ds Max software and Microsoft Camera Calibration data set. The results show that the accuracy of the proposed method is very close to the best ones available, while its sensitivity to distortion and computational load is the least. In addition, the required reference object is the simplest one.
C1 [Ardakani, H. K.; Mousavinia, A.] KN Toosi Univ Technol, Elect & Comp Engn Fac, Tehran, Iran.
   [Safaei, Farzad] Univ Wollongong, Fac Engn & Informat Sci, Wollongong, NSW, Australia.
C3 K. N. Toosi University of Technology; University of Wollongong
RP Ardakani, HK (corresponding author), KN Toosi Univ Technol, Elect & Comp Engn Fac, Tehran, Iran.
EM h.k.ardakani@gmail.com; moosavie@eetd.kntu.ac.ir;
   farzad_safaei@uow.edu.au
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   Avinash N, 2008, J MATH IMAGING VIS, V30, P221, DOI 10.1007/s10851-007-0052-3
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Bajramovic F, 2008, P BRIT MACH VIS C, V2, P382
   Beardsley P., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P416
   Cao XC, 2006, IEEE T IMAGE PROCESS, V15, P3614, DOI 10.1109/TIP.2006.881940
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen HT, 2017, IEEE T CIRC SYST VID, V27, P2555, DOI 10.1109/TCSVT.2016.2595319
   Chen HT, 2012, MULTIMED TOOLS APPL, V60, P641, DOI 10.1007/s11042-011-0833-y
   Cipolla R., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P382
   de França JA, 2010, PATTERN RECOGN, V43, P1180, DOI 10.1016/j.patcog.2009.08.001
   El Hazzat S, 2018, VISUAL COMPUT, V34, P1443, DOI 10.1007/s00371-017-1451-0
   Frosio I, 2016, VISUAL COMPUT, V32, P663, DOI 10.1007/s00371-015-1089-8
   Grammatikopoulos L, 2007, ISPRS J PHOTOGRAMM, V62, P64, DOI 10.1016/j.isprsjprs.2007.02.002
   Hammarstedt P, 2005, IEEE I CONF COMP VIS, P317
   Hemayed EE, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P351, DOI 10.1109/AVSS.2003.1217942
   Hu MC, 2011, IEEE T MULTIMEDIA, V13, P266, DOI 10.1109/TMM.2010.2100373
   Inamoto N., 2004, Proceedings of ACM ACE, V74, P42
   Johnson FC, 2003, INFORM RES, V8
   Kanatani, 2007, P IAPR C MACH VIS AP, P178
   Kang S.B., FLEXIBLE NEW TECHNIQ
   Lee J.H, 2012, INT C PATT REC
   Liu M, 2016, OPT EXPRESS, V24, P2026, DOI 10.1364/OE.24.012026
   Lu FX, 2018, VISUAL COMPUT, V34, P753, DOI 10.1007/s00371-018-1540-8
   Lv FJ, 2006, IEEE T PATTERN ANAL, V28, P1513, DOI 10.1109/TPAMI.2006.178
   MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171
   Miyagawa I, 2010, IEEE T IMAGE PROCESS, V19, P1528, DOI 10.1109/TIP.2010.2042118
   Qi F, 2007, PATTERN RECOGN, V40, P1785, DOI 10.1016/j.patcog.2006.11.001
   Shi JL, 2018, VISUAL COMPUT, V34, P377, DOI 10.1007/s00371-016-1339-4
   Shi KF, 2012, IEEE T IMAGE PROCESS, V21, P3806, DOI 10.1109/TIP.2012.2195013
   TRIGGS B, 1997, PROC CVPR IEEE, P609, DOI DOI 10.1109/CVPR.1997.609388
   Wang GH, 2005, IMAGE VISION COMPUT, V23, P311, DOI 10.1016/j.imavis.2004.07.008
   Wang L., 2007, 2007 IEEE 11 INT C C
   Wang L, 2011, LECT NOTES COMPUT SC, V6761, P660, DOI 10.1007/978-3-642-21602-2_72
   Wu FC, 2005, PATTERN RECOGN, V38, P755, DOI 10.1016/j.patcog.2004.11.005
   Xu C, 2008, IEEE T MULTIMEDIA, V10, P325
   Xu G, 2016, APPL OPTICS, V55, P2653, DOI 10.1364/AO.55.002653
   Xu G, 2016, J OPT SOC KOREA, V20, P107, DOI 10.3807/JOSK.2016.20.1.107
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao ZJ, 2008, IEEE T IMAGE PROCESS, V17, P2393, DOI 10.1109/TIP.2008.2005562
   Zhu GY, 2009, IEEE T MULTIMEDIA, V11, P49, DOI 10.1109/TMM.2008.2008918
NR 42
TC 4
Z9 4
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 413
EP 424
DI 10.1007/s00371-019-01632-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300014
DA 2024-07-18
ER

PT J
AU Inunganbi, S
   Choudhary, P
   Manglem, K
AF Inunganbi, Sanasam
   Choudhary, Prakash
   Manglem, Khumanthem
TI Meitei Mayek handwritten dataset: compilation, segmentation, and
   character recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Meitei Mayek; Indian language; Character recognition; Segmentation
ID TEXT LINE SEGMENTATION; WORD SEGMENTATION; LOCALIZATION; SPECTRUM;
   FOURIER; SYSTEM
AB A peculiar Indian Script Meitei Mayek has experienced a resurgence in the last few years and gets very little attention in handwriting research due to recently insurgence and limited sources. The objective of this paper is two folds; firstly, develop two different datasets: Mayek27 having 4900 isolated Meitei Mayek alphabets and MM (Meitei Mayek) dataset of 189 full-length handwritten text page. Secondly, develop a recognition system on the Mayek27 dataset using convolutional neural network and segmentation algorithms (text-lines, words, and characters) on the full-length Meitei Mayek handwritten text. A recognition rate of 99.02% is achieved using three layers of convolutional layers with a filter size of 3x3 with 16, 32, and 96 kernels. In MM text dataset, the text-line and word segmentation are performed concurrently on 809 lines by tracking space between lines in a novel approach based on horizontal projection histogram and monitoring vertical projection histogram along the run-length of segmentation. Various constraints like skew, curve, close, and touching text-lines are incorporated, and the segmentation algorithm results are 91.84% and 88.96% for text-line and word, respectively. Furthermore, characters are segmented by headline removal, and connected component analysis achieves an accuracy of 91.12%.
C1 [Inunganbi, Sanasam; Manglem, Khumanthem] Natl Inst Technol, Imphal 795004, Manipur, India.
   [Choudhary, Prakash] Natl Inst Technol, Hamirpur, Himachal Prades, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Manipur; National Institute of Technology (NIT System);
   National Institute of Technology Hamirpur
RP Inunganbi, S (corresponding author), Natl Inst Technol, Imphal 795004, Manipur, India.
EM inung.sam@gmail.com; choudharyprakash87@gmail.com; manglem@gmail.com
RI Choudhary, Dr. Prakash/ABE-2494-2021; Singh, Khumanthem/AFZ-2177-2022
OI Choudhary, Dr. Prakash/0000-0003-4337-7273; Singh,
   Khumanthem/0000-0002-6698-1185; Inunganbi, Sanasam/0000-0002-7879-1039
CR [Anonymous], 2013, INT J COMPUTER APPL
   [Anonymous], 2010, ARXIV10024048
   [Anonymous], 2012, P INT C ADV EL EL CO
   [Anonymous], 2016, IEICE TECHNICAL REPO
   Arivazhagan M, 2007, PROC SPIE, V6500, DOI 10.1117/12.704538
   Badrinath GS, 2011, APPL SOFT COMPUT, V11, P4267, DOI 10.1016/j.asoc.2010.05.031
   Bianne-Bernard AL, 2011, IEEE T PATTERN ANAL, V33, P2066, DOI 10.1109/TPAMI.2011.22
   Dash KS, 2015, IET IMAGE PROCESS, V9, P874, DOI 10.1049/iet-ipr.2015.0146
   dos Santos Rodolfo P., 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P651, DOI 10.1109/ICDAR.2009.183
   Drabycz S, 2009, J DIGIT IMAGING, V22, P696, DOI 10.1007/s10278-008-9138-8
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Hammouda G, 2020, VISUAL COMPUT, V36, P279, DOI 10.1007/s00371-018-1604-9
   Hassan T, 2015, IEEE CONF IMAGING SY, P100
   He J, 2003, PROC INT CONF DOC, P498
   Ilmi N, 2016, 2016 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (ICOICT)
   INUNGANBI S, 2018, INT J NAT LANG COMPU, V7, P99
   Inunganbi S, 2018, LECT NOTES ARTIF INT, V10752, P519, DOI 10.1007/978-3-319-75420-8_49
   INUNGANBI SC, 2018, INT C INT SYST DES A
   Javed M, 2013, NAT CONF COMPUT VIS
   Jindal P, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN ENGINEERING & COMPUTATIONAL SCIENCES (RAECS)
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   KAHAN S, 1987, IEEE T PATTERN ANAL, V9, P274, DOI 10.1109/TPAMI.1987.4767901
   Kise K, 1998, COMPUT VIS IMAGE UND, V70, P370, DOI 10.1006/cviu.1998.0684
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar CJ, 2015, 2015 COMMUNICATION, CONTROL AND INTELLIGENT SYSTEMS (CCIS), P186, DOI 10.1109/CCIntelS.2015.7437905
   Laishram R, 2014, IEEE I C COMP INT CO, P997
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Y, 2008, IEEE T PATTERN ANAL, V30, P1313, DOI 10.1109/TPAMI.2007.70792
   Lim J, 2019, VISUAL COMPUT, V35, P71, DOI 10.1007/s00371-017-1453-y
   Liu CL, 2005, PROC INT CONF DOC, P121, DOI 10.1109/ICDAR.2005.119
   Louloudis G, 2009, PATTERN RECOGN, V42, P3169, DOI 10.1016/j.patcog.2008.12.016
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MAHMOUD SA, 1994, PATTERN RECOGN, V27, P815, DOI 10.1016/0031-3203(94)90166-X
   Mansinha L, 1997, PHYSICA A, V239, P286, DOI 10.1016/S0378-4371(96)00487-6
   MARING KA, 2014, IJCSIT, V1
   Marti UV, 2001, PROC INT CONF DOC, P159, DOI 10.1109/ICDAR.2001.953775
   MOHAMMADI S, 2019, VIS COMPUT
   Mowlaei A, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P923, DOI 10.1109/ICDSP.2002.1028240
   NAGY G, 1992, COMPUTER, V25, P10, DOI 10.1109/2.144436
   OGORMAN L, 1993, IEEE T PATTERN ANAL, V15, P1162, DOI 10.1109/34.244677
   Pal U, 2003, PROC INT CONF DOC, P1128
   Pirlo G, 2011, IEEE T FUZZY SYST, V19, P780, DOI 10.1109/TFUZZ.2011.2131658
   Pirlo G, 2012, IEEE T IMAGE PROCESS, V21, P3827, DOI 10.1109/TIP.2012.2199328
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Ryu J, 2015, IEEE SIGNAL PROC LET, V22, P1161, DOI 10.1109/LSP.2015.2389852
   Saha S, 2013, 2013 FIFTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE, COMMUNICATION SYSTEMS AND NETWORKS (CICSYN), P3, DOI 10.1109/CICSYN.2013.11
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   SHRIDHAR M, 1984, PATTERN RECOGN, V17, P515, DOI 10.1016/0031-3203(84)90049-9
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stockwell RG, 1996, IEEE T SIGNAL PROCES, V44, P998, DOI 10.1109/78.492555
   Su TH, 2007, PROC INT CONF DOC, P899
   Surinta O, 2013, PROC INT CONF DOC, P165, DOI 10.1109/ICDAR.2013.40
   Thokchom T, 2010, J COMPUT, V5, P1570, DOI 10.4304/jcp.5.10.1570-1574
   Wang DH, 2012, PATTERN RECOGN, V45, P3661, DOI 10.1016/j.patcog.2012.04.020
   Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264
   Weliwitage C, 2005, DIGITAL IMAGE COMPUT, P27
   Wu YC, 2017, PATTERN RECOGN, V65, P251, DOI 10.1016/j.patcog.2016.12.026
   Zahour A, 2001, PROC INT CONF DOC, P281
   Zahour A, 2007, PROC INT CONF DOC, P138
   Zhou XD, 2014, PATTERN RECOGN, V47, P1904, DOI 10.1016/j.patcog.2013.12.002
   Zhou XD, 2013, IEEE T PATTERN ANAL, V35, P2413, DOI 10.1109/TPAMI.2013.49
NR 61
TC 10
Z9 10
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 291
EP 305
DI 10.1007/s00371-020-01799-4
EA JAN 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000515935700001
DA 2024-07-18
ER

PT J
AU Du, Q
   Da, FP
AF Du, Qiao
   Da, Feipeng
TI Block dictionary learning-driven convolutional neural networks for
   fewshot face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Fewshot face recognition; Block dictionary learning; Convolutional
   neural networks; Sparse loss
ID SINGLE-SAMPLE; REPRESENTATION
AB Fewshot face recognition (FFR) in less constrained environment is an important but challenging task due to the lack of sufficient sample information and the impact of occlusion. In this paper, a novel approach called block dictionary learning (BDL) is proposed, which combines sparse representation with convolutional neural networks to address the FFR problem. Based on the key-point locations of face images, the images are divided into four block regions for local feature extraction. Then, highly compact and discriminative features of both holistic and segmented parts are generated by CNN, which further compensates for the shortage of samples. Moreover, the sparse loss is introduced to optimize the performance of CNN by increasing the inter-class variations of features; thus, it develops a global-to-local dictionary learning algorithm to improve the robustness of BDL against complex variations. Finally, extensive experiments on AR and Extended Yale B datasets significantly demonstrate the effectiveness of BDL in comparison with other FFR methods.
C1 [Du, Qiao; Da, Feipeng] Southeast Univ, Sch Automat, Nanjing, Peoples R China.
C3 Southeast University - China
RP Du, Q (corresponding author), Southeast Univ, Sch Automat, Nanjing, Peoples R China.
EM 137729694@qq.com
FU Natural National Science Foundation of China [51475092, 61462072];
   Natural Science Foundation of Jiangsu Province of China [BK20181269,
   BK20160693]; Priority Academic Program Development of Jiangsu Higher
   Education Institutions; Fundamental Research Funds for the Central
   Universities
FX Part of this research was carried out at Key Laboratory of Measurement
   and Control of Complex Systems of Engineering, Nanjing, China.
   Acknowledgements. This work was supported by the Natural National
   Science Foundation of China (Grant Nos. 51475092, 61462072) and Natural
   Science Foundation of Jiangsu Province of China (Grant Nos. BK20181269,
   BK20160693). This project was funded by the Priority Academic Program
   Development of Jiangsu Higher Education Institutions, the Fundamental
   Research Funds for the Central Universities.
CR ABDELLATEF E, 2019, VIS COMPUT
   Agrawal A, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.215
   Benavente R, 1998, 24 COMP VIS CTR
   Chu YJ, 2019, VISUAL COMPUT, V35, P239, DOI 10.1007/s00371-017-1468-4
   Deng WH, 2014, PATTERN RECOGN, V47, P3738, DOI 10.1016/j.patcog.2014.06.020
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Ding RX, 2015, J VIS COMMUN IMAGE R, V30, P35, DOI 10.1016/j.jvcir.2015.03.001
   Gao GW, 2020, INFORM SCIENCES, V506, P19, DOI 10.1016/j.ins.2019.08.004
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   Gao QX, 2008, APPL MATH COMPUT, V205, P726, DOI 10.1016/j.amc.2008.05.019
   GAO SH, 2010, ECCV
   Gao SH, 2015, INT J COMPUT VISION, V111, P365, DOI 10.1007/s11263-014-0750-4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2019, IEEE T IMAGE PROCESS, V28, P791, DOI 10.1109/TIP.2018.2870946
   Hu CH, 2015, NEUROCOMPUTING, V160, P287, DOI 10.1016/j.neucom.2015.02.032
   Jafri R, 2009, J INF PROCESS SYST, V5, P41, DOI 10.3745/JIPS.2009.5.2.041
   Ji HK, 2017, PATTERN RECOGN, V62, P125, DOI 10.1016/j.patcog.2016.08.007
   Jolliffe I., 2011, International Encyclopedia of Statistical Science, P1094, DOI [DOI 10.1007/978-3-642-04898-2_455, 10.1007/978-3-642-04898-2_455]
   Kingma D. P., 2014, arXiv
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Liu F, 2016, INFORM SCIENCES, V346, P198, DOI 10.1016/j.ins.2016.02.001
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Meina Kan, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P193, DOI 10.1109/FG.2011.5771397
   Pei TW, 2017, PATTERN RECOGN, V64, P305, DOI 10.1016/j.patcog.2016.11.016
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shang K, 2018, APPL MATH COMPUT, V320, P99, DOI 10.1016/j.amc.2017.07.058
   Sun Yi, 2014, NEURIPS, DOI DOI 10.1007/978-3-030-01252-6_48
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wright J, 2010, IEEE T INFORM THEORY, V56, P3540, DOI 10.1109/TIT.2010.2048473
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yang M., 2010, ECCV
   Yang M, 2013, IEEE I CONF COMP VIS, P689, DOI 10.1109/ICCV.2013.91
   Yi Dong, 2014, ARXIV14117923
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu PF, 2015, LECT NOTES COMPUT SC, V9005, P34, DOI 10.1007/978-3-319-16811-1_3
NR 37
TC 6
Z9 6
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 663
EP 672
DI 10.1007/s00371-020-01802-y
EA JAN 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000508690200001
DA 2024-07-18
ER

PT J
AU Ganapathi, II
   Ali, SS
   Prakash, S
AF Ganapathi, Iyyakutti Iyappan
   Ali, Syed Sadaf
   Prakash, Surya
TI Geometric statistics-based descriptor for 3D ear recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; 3D ear recognition; Shape descriptor; Geometric statistics;
   Local features; Identification accuracy
ID OBJECT RECOGNITION; SURFACE; HISTOGRAMS; BIOMETRICS; SIGNATURES; IMAGES;
   SHAPE
AB Several feature keypoint detection and description techniques have been proposed in the literature for 3D shape recognition. These techniques work well in discriminating different classes of shapes; however, they fail when used for comparing highly similar objects such as 3D ear or face in biometric applications. In this paper, we propose an efficient feature keypoint detection and description technique using geometric statistics for representation and matching of highly similar 3D objects and demonstrate its effectiveness in 3D ear-based biometric recognition. To compute the descriptor, we first extract feature keypoints from the 3D data by making use of surface variations followed by defining a descriptor vector for each keypoint. The descriptor vector is generated using three components. To compute the first component, concentric spheres that divide the space around a keypoint into annular regions are considered. Points falling in the annular regions are projected onto a plane perpendicular to the oriented normal of the keypoint. Lower-order moments of the 2D histogram of the spatial distribution of these projected points for each annular region are computed and used to define the first component of the descriptor vector. Next, component of the descriptor vector is computed using histograms of the inner products of the normals of the keypoint and its neighbours. The third component of the descriptor vector encodes the signed distances of the neighbours of the keypoint from the projection plane. Before concatenating individual components of the descriptor vector, the values are normalized to a common scale. Experiments on University of Notre Dame public database-Collection J2 (UND-J2) have achieved a rank-1 and rank-2 identification rates of 98.60% and 100%, respectively, with an equal error rate of 1.50%. Comparative results show the superiority of the proposed descriptor in recognizing highly similar objects like human ear.
C1 [Ganapathi, Iyyakutti Iyappan; Ali, Syed Sadaf; Prakash, Surya] Indian Inst Technol Indore, Pattern Anal & Machine Intelligence Lab, Discipline Comp Sci & Engn, Indore 453552, MP, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore
RP Ganapathi, II (corresponding author), Indian Inst Technol Indore, Pattern Anal & Machine Intelligence Lab, Discipline Comp Sci & Engn, Indore 453552, MP, India.
EM phd1501101002@iiti.ac.in; phd1301101006@iiti.ac.in; surya@iiti.ac.in
RI Ganapathi, Iyyakutti Iyappan/CAH-5689-2022; ali, Syed
   mansoor/I-7636-2019
OI Ganapathi, Iyyakutti Iyappan/0000-0001-6312-5765; Ali, Syed
   Sadaf/0000-0002-0198-8319
CR Abaza A, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431221
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bhanu B, 2008, ADV PATTERN RECOGNIT, P1
   Cadavid S, 2008, IEEE T INF FOREN SEC, V3, P709, DOI 10.1109/TIFS.2008.2007239
   Chen H, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P123
   Chen H, 2009, IEEE T PATTERN ANAL, V31, P172, DOI 10.1109/TPAMI.2008.176
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Chen H, 2007, IEEE T PATTERN ANAL, V29, P718, DOI 10.1109/TPAMI.2007.1005
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   Ding ZX, 2013, IEEE IMAGE PROC, P4166, DOI 10.1109/ICIP.2013.6738858
   Dong X, 2011, ENRGY PROCED, V11, P1103, DOI 10.1016/j.egypro.2011.10.365
   Emersic Z, 2017, NEUROCOMPUTING, V255, P26, DOI 10.1016/j.neucom.2016.08.139
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Islam SMS, 2011, INT J COMPUT VISION, V95, P52, DOI 10.1007/s11263-011-0436-0
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Liu YH, 2015, PATTERN RECOGN LETT, V53, P9, DOI 10.1016/j.patrec.2014.10.014
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Passalis G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P39, DOI 10.1109/AVSS.2007.4425283
   Pflug A, 2012, IET BIOMETRICS, V1, P114, DOI 10.1049/iet-bmt.2011.0003
   Prakash S., 2015, EAR BIOMETRICS 2D 3D, V10
   Prakash S, 2014, NEUROCOMPUTING, V140, P317, DOI 10.1016/j.neucom.2014.03.007
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   STEIN F, 1992, IEEE T PATTERN ANAL, V14, P1198, DOI 10.1109/34.177385
   Sun XP, 2015, J COMPUT SCI TECH-CH, V30, P565, DOI 10.1007/s11390-015-1546-x
   Sun XP, 2014, GRAPH MODELS, V76, P402, DOI 10.1016/j.gmod.2014.03.003
   Yan P, 2005, LECT NOTES COMPUT SC, V3546, P503
   Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067
   Yan Ping, 2006, 2006 3 INT S 3D DATA, P326, DOI DOI 10.1109/3DPVT.2006.25
   Yu F., 2011, 3 DIMENSIONAL MODEL
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zeng H, 2010, INT CONF SIGN PROCES, P1694, DOI 10.1109/ICOSP.2010.5656140
   Zhang Y., 2006, Electrical and Computer Engineering, P202, DOI [DOI 10.1109/CCECE.2006.277433, 10.1109/CCECE.2006.277433]
   Zhou JiangJu Zhou JiangJu, 2011, China Condiment, P98
   Zhou W, 2019, VISUAL COMPUT, V35, P489, DOI 10.1007/s00371-018-1478-x
NR 35
TC 12
Z9 13
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 161
EP 173
DI 10.1007/s00371-018-1593-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800013
DA 2024-07-18
ER

PT J
AU Du, XT
   Yuan, JB
   Hu, L
   Dai, YK
AF Du, Xiaotong
   Yuan, Jiabin
   Hu, Liu
   Dai, Yuke
TI Description generation of open-domain videos incorporating multimodal
   features and bidirectional encoder
SO VISUAL COMPUTER
LA English
DT Article
DE Multimodal features; Video description; Natural language; Long
   short-term memory
AB Describing open-domain videos in natural language is a major challenge for video understanding and can largely fulfill its potential in a host of applications, such as assisting blind people and managing massive videos. This paper presents an updated sequence-to-sequence video to text model (MM-BiS2VT), which incorporates multimodal feature fusion and bidirectional language structure and aims at optimizing conventional methods. The model totally considered four features-RGB images, optical flow, spatiotemporal and audio features. RGB images and optical flow features were extracted by ResNet152. And with the help of the improved three-dimensional convolutional neural networks model, spatiotemporal feature was included. As a vital factor to increase the accuracy of results, audio feature was also added to make up for visual information. After combining these features by a feature fusion method, bidirectional long short-term memory units (BiLSTMs) was adopted to generate descriptive sentences. The results indicate that fusing multimodal features could gain better sentences over other methods and BiLSTMs plays a significant role as well to improve the accuracy of the outputs, which means the works in this paper could be an available reference for computer vision and video processing.
C1 [Du, Xiaotong; Yuan, Jiabin; Hu, Liu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.
   [Dai, Yuke] Nanjing Univ Aeronaut & Astronaut, Coll Aerosp Engn, Nanjing, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics
RP Du, XT (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.
EM xtdu@nuaa.edu.cn
RI Du, Xiaotong/GQA-5502-2022
OI Du, Xiaotong/0000-0003-4026-3529
FU Research and Industrialization for Intelligent Video Processing
   Technology based on GPUs Parallel Computing of the Science and
   Technology Supported Program of Jiangsu Province [BY 2016003-11];
   Application platform and Industrialization for efficient cloud computing
   for Big data of the Science and Technology Supported Program of Jiangsu
   Province [BA2015052]
FX This work was supported by Research and Industrialization for
   Intelligent Video Processing Technology based on GPUs Parallel Computing
   of the Science and Technology Supported Program of Jiangsu Province (BY
   2016003-11) and the Application platform and Industrialization for
   efficient cloud computing for Big data of the Science and Technology
   Supported Program of Jiangsu Province (BA2015052). We thank all the
   shared achievements of selfless antecessors.
CR [Anonymous], P 28 C UNC ART INT
   [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   [Anonymous], 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2984066
   [Anonymous], JOINT IEEE INT WORKS
   [Anonymous], COMPUTER SCI
   [Anonymous], 2014, DEEP CAPTIONING MULT
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], ACM MULT C
   [Anonymous], COMPUT SCI
   [Anonymous], 2014, ARXIV
   [Anonymous], 2015, AB INITTO CALCULATIO
   Chen N, 2013, VISUAL COMPUT, V29, P1221, DOI 10.1007/s00371-013-0785-5
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   d'Angelo E., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1885, DOI 10.1109/ICIP.2011.6115836
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Derpanis KG, 2012, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2012.6247815
   Giannakopoulos T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0144610
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Mirzaei MR, 2014, VISUAL COMPUT, V30, P245, DOI 10.1007/s00371-013-0841-1
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Peris A, 2016, LECT NOTES COMPUT SC, V9887, P3, DOI 10.1007/978-3-319-44781-0_1
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Shaikh AA, 2013, VISUAL COMPUT, V29, P969, DOI 10.1007/s00371-012-0751-7
   Shaw D, 2011, IMPACT OF THE ECONOMIC CRISIS ON EAST ASIA: POLICY RESPONSES FROM FOUR ECONOMIES, P190
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Thomason J., 2014, COLING, P1218
   Venugopalan S., 2014, COMPUT SCI
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
NR 38
TC 5
Z9 5
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1703
EP 1712
DI 10.1007/s00371-018-1591-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300002
DA 2024-07-18
ER

PT J
AU Souza, MRE
   Pedrini, H
AF Roberto e Souza, Marcos
   Pedrini, Helio
TI Motion energy image for evaluation of video stabilization
SO VISUAL COMPUTER
LA English
DT Article
DE Motion energy image; Video stabilization; Handheld camera; Qualitative
   evaluation
ID ALGORITHMS
AB Large volumes of video content have been generated through the development of compact and portable cameras. Examples of applications that have been benefited from such growth of multimedia data include business conferencing, telemedicine, surveillance and security, entertainment, distance learning and robotics. Video stabilization is the process of detecting and removing undesired motion or instabilities from a video stream caused during the acquisition stage when handling the camera. In this work, we introduce and analyze a novel visual representation based on motion energy image for qualitative evaluation of video stabilization approaches. Experiments conducted on different video sequences are performed to demonstrate the effectiveness of the visual representation as qualitative measure for evaluating video stability.
C1 [Roberto e Souza, Marcos; Pedrini, Helio] Univ Estadual Campinas, Inst Comp, BR-13083852 Campinas, SP, Brazil.
C3 Universidade Estadual de Campinas
RP Pedrini, H (corresponding author), Univ Estadual Campinas, Inst Comp, BR-13083852 Campinas, SP, Brazil.
EM helio@ic.unicamp.br
FU Sao Paulo Research Foundation (FAPESP) [2017/12646-3, 2014/12236-1];
   National Council for Scientific and Technological Development (CNPq)
   [305169/2015-7]; Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [14/12236-1, 17/12646-3] Funding Source: FAPESP
FX The authors are thankful to SAo Paulo Research Foundation (FAPESP Grants
   #2017/12646-3 and #2014/12236-1) and National Council for Scientific and
   Technological Development (CNPq Grant #305169/2015-7) for their
   financial support.
CR Amanatiadis AA, 2010, IEEE T INSTRUM MEAS, V59, P1755, DOI 10.1109/TIM.2009.2028216
   [Anonymous], 2013, Iberoamerican Congress on Pattern Recognition
   [Anonymous], 2012, MOTION HIST IMAGES A
   Battiato S, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P825, DOI 10.1109/ICIAP.2007.4362878
   Borgo R, 2012, COMPUT GRAPH FORUM, V31, P2450, DOI 10.1111/j.1467-8659.2012.03158.x
   Chang HC, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P29, DOI 10.1109/ICME.2004.1394117
   Chang JY, 2002, IEEE T CONSUM ELECTR, V48, P108, DOI 10.1109/TCE.2002.1010098
   Chen BY, 2008, COMPUT GRAPH FORUM, V27, P1805, DOI 10.1111/j.1467-8659.2008.01326.x
   Chen BH, 2016, ENG APPL ARTIF INTEL, V54, P39, DOI 10.1016/j.engappai.2016.05.004
   Chen D, 2012, CONTEMPORARY ERGONOMICS AND HUMAN FACTORS 2012, P253
   Choi S, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1897, DOI 10.1109/IROS.2009.5354240
   Ertürk S, 2002, REAL-TIME IMAGING, V8, P317, DOI 10.1006/rtim.2001.0278
   Gonzalez R.C., 2002, Digital Image Processing, V2nd
   Grundmann M, 2011, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2011.5995525
   Huang TS, 2013, IMAGE SEQUENCE ANAL
   Jia RM, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL III, PROCEEDINGS, P485, DOI 10.1109/AICI.2009.489
   Joshi N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766954
   Ko SJ, 1998, IEEE T CONSUM ELECTR, V44, P617, DOI 10.1109/30.713172
   Kumar S, 2011, IEEE T IMAGE PROCESS, V20, P3406, DOI 10.1109/TIP.2011.2156420
   Lin CT, 2009, IEEE T CIRC SYST VID, V19, P427, DOI 10.1109/TCSVT.2009.2013508
   Litvin A., 2003, ELECT IMAGING 2003, P663
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Marcenaro L, 2001, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2001.959025
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Morimoto C, 1996, C PATT REC, V3, P284
   Cirne MVM, 2014, LECT NOTES COMPUT SC, V8827, P901, DOI 10.1007/978-3-319-12568-8_109
   Niskanen M, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P405
   Puglisi G, 2011, IEEE T CIRC SYST VID, V21, P1390, DOI 10.1109/TCSVT.2011.2162689
   Qu Hui, 2013, P VIS COMM IM PROC, P1, DOI DOI 10.1109/VCIP.2013.6706422
   Ratakonda K., 1998, ISCAS '98. Proceedings of the 1998 IEEE International Symposium on Circuits and Systems (Cat. No.98CH36187), P69, DOI 10.1109/ISCAS.1998.698760
   Ryu YG, 2012, IEEE SIGNAL PROC LET, V19, P223, DOI 10.1109/LSP.2012.2188286
   Schoeffmann K, 2009, IEEE INT CON MULTI, P658, DOI 10.1109/ICME.2009.5202582
   Shen Y, 2009, IEEE T CONSUM ELECTR, V55, P1714, DOI 10.1109/TCE.2009.5278047
   Shukla D, 2015, SIGNAL IMAGE VIDEO P, V9, P1287, DOI 10.1007/s11760-013-0584-5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang JL, 2009, IEEE T CIRC SYST VID, V19, P945, DOI 10.1109/TCSVT.2009.2020252
   Zheng Q, 2017, GLOB J COMPUT SCI TE, V17, P1
NR 38
TC 1
Z9 1
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1769
EP 1781
DI 10.1007/s00371-018-1572-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300007
DA 2024-07-18
ER

PT J
AU Reimann, M
   Klingbeil, M
   Pasewaldt, S
   Semmo, A
   Trapp, M
   Döllner, J
AF Reimann, Max
   Klingbeil, Mandy
   Pasewaldt, Sebastian
   Semmo, Amir
   Trapp, Matthias
   Doellner, Juergen
TI Locally controllable neural style transfer on mobile devices
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 17th International Conference on Cyberworlds (CW)
CY OCT 03-05, 2018
CL Fraunhofer Singapore, Singapore, SINGAPORE
SP Nanyang Technol Univ Singapore, ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, ACM
HO Fraunhofer Singapore
DE Non-photorealistic rendering; Style transfer; Neural networks; Mobile
   devices; Interactive control; Expressive rendering
ID ARTIST
AB Mobile expressive rendering gained increasing popularity among users seeking casual creativity by image stylization and supports the development of mobile artists as a new user group. In particular, neural style transfer has advanced as a core technology to emulate characteristics of manifold artistic styles. However, when it comes to creative expression, the technology still faces inherent limitations in providing low-level controls for localized image stylization. In this work, we first propose a problem characterization of interactive style transfer representing a trade-off between visual quality, run-time performance, and user control. We then present MaeSTrO, a mobile app for orchestration of neural style transfer techniques using iterative, multi-style generative and adaptive neural networks that can be locally controlled by on-screen painting metaphors. At this, we enhance state-of-the-art neural style transfer techniques by mask-based loss terms that can be interactively parameterized by a generalized user interface to facilitate a creative and localized editing process. We report on a usability study and an online survey that demonstrate the ability of our app to transfer styles at improved semantic plausibility.
C1 [Reimann, Max; Pasewaldt, Sebastian] Digital Masterpieces GmbH, Potsdam, Germany.
   [Klingbeil, Mandy] Digital Masterpieces GmbH, Prod, Potsdam, Germany.
   [Reimann, Max; Pasewaldt, Sebastian; Trapp, Matthias] Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
   [Klingbeil, Mandy] Hasso Plattner Inst, Comp Graph Grp, Potsdam, Germany.
   [Semmo, Amir] Univ Potsdam, Hasso Plattner Inst, Visual Comp & Visual Analyt Grp, Potsdam, Germany.
   [Doellner, Juergen] Univ Potsdam, Hasso Plattner Inst, Anal Planning & Construct Complex Syst, Potsdam, Germany.
C3 University of Potsdam; University of Potsdam; University of Potsdam;
   University of Potsdam
RP Reimann, M (corresponding author), Digital Masterpieces GmbH, Potsdam, Germany.; Reimann, M (corresponding author), Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
EM Max.Reimann@digitalmasterpieces.com;
   Mandy.Klingbeil@digitalmasterpieces.com;
   Sebastian.Pasewaldt@digitalmasterpieces.com;
   Amir.Semmo@hpi.uni-potsdam.de; Matthias.Trapp@hpi.uni-potsdam.de;
   Juergen.Doellner@hpi.uni-potsdam.de
RI Semmo, Amir/KPA-5814-2024; Trapp, Matthias/J-4456-2014; Reimann,
   Max/JDH-5860-2023
OI Trapp, Matthias/0000-0003-3861-5759; Reimann, Max/0000-0003-2146-4229;
   Semmo, Amir/0000-0002-1553-4940
FU Federal Ministry of Education and Research (BMBF), Germany [01IS15041]
FX We would like to thank the anonymous reviewers for their valuable
   feedback. This work was funded by the Federal Ministry of Education and
   Research (BMBF), Germany, for the AVA project 01IS15041.
CR [Anonymous], 2017, ARXIV170506830
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Bakhshi S., 2015, P INT AAAI C WEB SOC, V9, P12
   Berry M, 2014, J CREAT TECHNOL, V4, P81
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Champandard A.J., 2016, ARXIV161204337
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Dev K, 2013, IEEE COMPUT GRAPH, V33, P22, DOI 10.1109/MCG.2013.20
   Doug DeCarlo AnthonySantella., 2004, NONPHOTOREALISTIC AN, P71
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gooch A.A., 2010, NPAR 10 P 8 INT S NO, P165
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isenberg T., 2016, PROC NPAR SER EXPRES, P8996
   Jing Y., 2018, ARXIV 1705 04058
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Keep D, 2014, PALGRAVE PIVOT, P14, DOI 10.1057/9781137469816.0006
   Klingbeil M, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3132803
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YJ, 2017, ADV NEUR IN, V30
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liao J., 2017, ACM Trans. Graph.
   Lin T., 2014, ARXIV 1405 0312
   Liu X.C., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, page, P1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Reimann M, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P9, DOI 10.1109/CW.2018.00016
   Rudner Richard., 1951, The Journal of Aesthetics and Art Criticism, V10, P67, DOI DOI 10.2307/426789
   Salesin D.H, 2002, NPAR
   Seims J, 1999, COMPUT GRAPHICS-US, V33, P52, DOI 10.1145/563666.563685
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Semmo A., 2016, P ACM SIGGRAPH AS S
   Semmo A., 2017, ACM SIGGRAPH 2017 AP, P1
   Semmo Amir., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, V5, P1, DOI DOI 10.1145/3092919.3092920
   Shneiderman B, 2007, COMMUN ACM, V50, P20, DOI 10.1145/1323688.1323689
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tanno R, 2017, LECT NOTES COMPUT SC, V10133, P446, DOI 10.1007/978-3-319-51814-5_39
   Toyoura M., 2017, T COMPUTATIONAL SCI, P29, DOI [10.1007/978-3-662-56006-8_3, DOI 10.1007/978-3-662-56006-8_3]
   Toyoura M, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P73, DOI 10.1109/CW.2016.18
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Ulyanov Dmitry, 2016, arXiv
   Winnemoller H, 2013, COMPUTATIONAL IMAGIN, P353, DOI DOI 10.1007/978-1-4471-4519-6_17
   Zhang H., 2017, MULTISTYLE GENERATIV
   Zhao H., 2017, ARXIV170809641
NR 47
TC 8
Z9 8
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1531
EP 1547
DI 10.1007/s00371-019-01654-1
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JD5IZ
UT WOS:000490018000004
DA 2024-07-18
ER

PT J
AU Ajili, I
   Mallem, M
   Didier, JY
AF Ajili, Insaf
   Mallem, Malik
   Didier, Jean-Yves
TI Human motions and emotions recognition inspired by LMA qualities
SO VISUAL COMPUTER
LA English
DT Article
DE Motion recognition; Emotion recognition; Laban movement analysis;
   Features importance; Machine learning; Human perception
ID CLASSIFICATION; SEGMENTATION; DESCRIPTORS; RETRIEVAL; SYSTEM; MODEL
AB The purpose of this paper is to describe human motions and emotions that appear on real video images with compact and informative representations. We aimed to recognize expressive motions and analyze the relationship between human body features and emotions. We propose a new descriptor vector for expressive human motions inspired from the Laban movement analysis method (LMA), a descriptive language with an underlying semantics that allows to qualify human motion in its different aspects. The proposed descriptor is fed into a machine learning framework including, random decision forest, multi-layer perceptron and two multiclass support vector machines methods. We evaluated our descriptor first for motion recognition and second for emotion recognition from the analysis of expressive body movements. Preliminary experiments with three public datasets, MSRC-12, MSR Action 3D and UTkinect, showed that our model performs better than many existing motion recognition methods. We also built a dataset composed of 10 control motions (move, turn left, turn right, stop, sit down, wave, dance, introduce yourself, increase velocity, decrease velocity). We tested our descriptor vector and achieved high recognition performance. In the second experimental part, we evaluated our descriptor with a dataset composed of expressive gestures performed with four basic emotions selected from Russell's Circumplex model of affect (happy, angry, sad and calm). The same machine learning methods were used for human emotions recognition based on expressive motions. A 3D virtual avatar was introduced to reproduce human body motions, and three aspects were analyzed (1) how expressed emotions are classified by humans, (2) how motion descriptor is evaluated by humans, (3) what is the relationship between human emotions and motion features.
C1 [Ajili, Insaf; Mallem, Malik; Didier, Jean-Yves] Univ Paris Saclay, Univ Evry, IBISC, Site Pelvoux,UFR ST 36, F-91020 Evry, France.
C3 Universite Paris Cite; Universite Paris Saclay
RP Ajili, I (corresponding author), Univ Paris Saclay, Univ Evry, IBISC, Site Pelvoux,UFR ST 36, F-91020 Evry, France.
EM insafajili@gmail.com; Malik.Mallem@ufrst.univ-evry.fr;
   Jean-Yves.Didier@ufrst.univ-evry.fr
RI MALLEM, Malik/P-6389-2017
OI MALLEM, Malik/0000-0002-2471-7028; Didier, Jean-Yves/0000-0002-9863-5471
FU Strategic Research Initiatives project iCODE by University Paris Saclay
FX We would like to thank the staff of the University of Evry Val d'Essonne
   for participating in our datasets. This work was partially supported by
   the Strategic Research Initiatives project iCODE accredited by
   University Paris Saclay.
CR Ajili I, 2017, IEEE ROMAN, P1115, DOI 10.1109/ROMAN.2017.8172443
   Ajili I, 2017, PROCEDIA COMPUT SCI, V112, P554, DOI 10.1016/j.procs.2017.08.168
   [Anonymous], VIS COMPUT
   [Anonymous], P 33 COMP GRAPH INT
   [Anonymous], MASTERY MOVEMENT
   [Anonymous], S COMP AN
   [Anonymous], CLASSIFICATION REGRE
   [Anonymous], HDB COGNITION EMOTIO
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Arlot S., 2009, A Survey of Cross-validation Procedures for Model Selection
   Aviezer H, 2008, PSYCHOL SCI, V19, P724, DOI 10.1111/j.1467-9280.2008.02148.x
   Aviezer H, 2011, EMOTION, V11, P1406, DOI 10.1037/a0023578
   Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Bland JM, 1997, BRIT MED J, V314, P572, DOI 10.1136/bmj.314.7080.572
   Bouchard D, 2007, LECT NOTES ARTIF INT, V4722, P37
   Chi D, 2000, COMP GRAPH, P173, DOI 10.1145/344779.352172
   Cimen G, 2013, COMPUT ANIMAT VIRT W, V24, P355, DOI 10.1002/cav.1509
   de Gelder B, 2009, PHILOS T R SOC B, V364, P3475, DOI 10.1098/rstb.2009.0190
   Díaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3
   Durupinar F, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983620
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Glowinski D, 2011, IEEE T AFFECT COMPUT, V2, P106, DOI 10.1109/T-AFFC.2011.7
   Gong D, 2014, IEEE T PATTERN ANAL, V36, P1414, DOI 10.1109/TPAMI.2013.244
   Hripcsak G, 2005, J AM MED INFORM ASSN, V12, P296, DOI 10.1197/jamia.M1733
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Junejo IN, 2014, VISUAL COMPUT, V30, P259, DOI 10.1007/s00371-013-0842-0
   Knight H, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1475, DOI 10.1109/IROS.2016.7759240
   Lehrmann AM, 2014, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2014.171
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Nishimura K, 2012, IEEE INT CONF FUZZY
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Quinlan J. R., 1992, Proceedings of the 5th Australian Joint Conference on Artificial Intelligence. AI '92, P343
   RUSSELL JA, 1994, PSYCHOL BULL, V115, P102, DOI 10.1037/0033-2909.115.1.102
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Samadani AA, 2013, INT CONF AFFECT, P343, DOI 10.1109/ACII.2013.63
   Senecal S, 2016, COMPUT ANIMAT VIRT W, V27, P311, DOI 10.1002/cav.1714
   Slama R, 2014, INT C PATT RECOG, P3499, DOI 10.1109/ICPR.2014.602
   Song Y, 2013, I S BIOMED IMAGING, P198
   Truong A, 2016, VISUAL COMPUT, V32, P83, DOI 10.1007/s00371-014-1057-8
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yumer ME, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925955
NR 47
TC 21
Z9 23
U1 1
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1411
EP 1426
DI 10.1007/s00371-018-01619-w
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IU7PN
UT WOS:000483775900006
DA 2024-07-18
ER

PT J
AU Jokanovic, S
AF Jokanovic, Simo
TI Two-dimensional line segment-triangle intersection test: revision and
   enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Segment-triangle intersection; Cross product; Out-of-order execution;
   Single instruction multiple data; Dependency chain
AB Two-dimensional line segment-triangle intersection test is a part of some 3D triangle-triangle intersection test algorithms. It is the kind of algorithms dealing with intersection of one triangle and line segment obtained as the intersection of the other triangle with the plane which the first triangle lies on. There appeared a number of algorithms each proclaiming its efficiency against to its predecessors with respect to the number of operations. In the paper, we seek out the minimal set of operations. Applying divide and conquer paradigm, we split the operations needed into (a) fixed part consisting of core arithmetic operations and (b) variable part dealing with logical reasoning. As we come to the set of core arithmetic operations that cannot be further minified we realize previous algorithms come to the same set in spite of their different strategies. Further improvement we sought in the area of modern processor architectures. We exploit modern CPU's parallel processing capabilities like Single instruction, multiple data vectorization, parallel-out-of-order execution and branch prediction and enlighten their strengths and weaknesses for usage in this type of algorithms.
C1 [Jokanovic, Simo] Univ Banja Luka, Fac Mech Engn, Banja Luka, Bosnia & Herceg.
C3 University of Banja Luka (UNIBL)
RP Jokanovic, S (corresponding author), Univ Banja Luka, Fac Mech Engn, Banja Luka, Bosnia & Herceg.
EM simo.jokanovic@mf.unibl.org
CR Botsch M., GEOMETRIC MODELING B
   Devillers O., 2002, RR4488 INRIA
   Fog A, 2017, OPTIMIZING SOFTWARE
   Gottschalk S., 1998, P IMA C MATH SURF, P3
   Held M., 1997, Journal of Graphics Tools, V2, P25, DOI 10.1080/10867651.1997.10487482
   Intel Corporation, 2016, 248966033 INT CORP
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Moller T., 1997, J. Graph. Tools, V2, P25, DOI [DOI 10.1080/10867651.1997.10487472, 10.1080/10867651.1997.10487472]
   Shen J. P., 2005, Modern Processor Design
   Tang K., 2007, MATH METHODS ENG SCI
   Tropp O, 2006, COMPUT ANIMAT VIRT W, V17, P527, DOI 10.1002/cav.115
   Wei LY, 2014, COMPUT ANIMAT VIRT W, V25, P553, DOI 10.1002/cav.1558
NR 12
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1347
EP 1359
DI 10.1007/s00371-018-01614-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900002
DA 2024-07-18
ER

PT J
AU Sasaki, K
   Iizuka, S
   Simo-Serra, E
   Ishikawa, H
AF Sasaki, Kazuma
   Iizuka, Satoshi
   Simo-Serra, Edgar
   Ishikawa, Hiroshi
TI Learning to restore deteriorated line drawing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Image restoration; Line drawings; Image manipulation; Convolutional
   neural network
ID IMAGE; VECTORIZATION; COMPLETION
AB We propose a fully automatic approach to restore aged old line drawings. We decompose the task into two subtasks: the line extraction subtask, which aims to extract line fragments and remove the paper texture background, and the restoration subtask, which fills in possible gaps and deterioration of the lines to produce a clean line drawing. Our approach is based on a convolutional neural network that consists of two sub-networks corresponding to the two subtasks. They are trained as part of a single framework in an end-to-end fashion. We also introduce a new dataset consisting of manually annotated sketches by Leonardo da Vinci which, in combination with a synthetic data generation approach, allows training the network to restore deteriorated line drawings. We evaluate our method on challenging 500-year-old sketches and compare with existing approaches with a user study, in which it is found that our approach is preferred 72.7% of the time.
C1 [Sasaki, Kazuma] Waseda Univ, Sch Fundamental Sci & Engn, Engn, Tokyo, Japan.
   [Sasaki, Kazuma] Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo, Japan.
   [Iizuka, Satoshi; Simo-Serra, Edgar; Ishikawa, Hiroshi] Waseda Univ, Tokyo, Japan.
C3 Waseda University; Waseda University; Waseda University
RP Sasaki, K (corresponding author), Waseda Univ, Sch Fundamental Sci & Engn, Engn, Tokyo, Japan.; Sasaki, K (corresponding author), Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo, Japan.
EM milky_kaid.lc@ruri.waseda.jp; iizuka@aoni.waseda.jp;
   esimo@aoni.waseda.jp; hfs@waseda.jp
FU JST ACT-I [JPMJPR16U3, JPMJPR16UD]; JST CREST [JPMJCR14D1]
FX This work was partially supported by JST ACT-I Grant No. JPMJPR16U3, JST
   ACT-I, Grant No. JPMJPR16UD, and JST CREST Grant No. JPMJCR14D1.
CR [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2017, ACM T GRAPH
   [Anonymous], ACM T GRAPH
   [Anonymous], 2016, COMPUTER VISION PATT
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2012, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, P ADV NEUR INF PROC
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chiang JY, 1998, PATTERN RECOGN, V31, P1541, DOI 10.1016/S0031-3203(97)00157-X
   CUGINI U, 1984, COMPUT GRAPH, V8, P337, DOI 10.1016/0097-8493(84)90033-5
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Favreau JD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925946
   Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212
   HAN CC, 1994, PATTERN RECOGN, V27, P261, DOI 10.1016/0031-3203(94)90058-2
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jain V., 2009, PROC ADV NEURAL INFO, P769
   Janssen RDT, 1997, COMPUT VIS IMAGE UND, V65, P38, DOI 10.1006/cviu.1996.0484
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Simo-Serra E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3132703
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Simonyan K, 2015, IEEE INT C ICLR
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xie J., 2012, ADV NEURAL INFORM PR, P341
NR 32
TC 9
Z9 12
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1077
EP 1085
DI 10.1007/s00371-018-1528-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400029
DA 2024-07-18
ER

PT J
AU Wang, BY
   Zhang, T
   Wang, XG
AF Wang, Baoyan
   Zhang, Tie
   Wang, Xingang
TI Salient object detection based on Laplacian similarity metrics
SO VISUAL COMPUTER
LA English
DT Article
DE Superpixels; Saliency; Background prior; Similarity metrics
ID IMAGE; INTEGRATION; MODEL
AB Background prior has become a novel viewpoint and made progresses in salient object detection. Most existing salient object detection algorithms based on background prior take boundaries as backgrounds and neglect nonbackground factors of boundaries, which is in fact unreasonable. Thus it is necessary to combine background prior with the analysis of boundary property. In this paper, the probability values computed by Mahalanobis distance are used to describe the likelihood of boundary superpixels belonging to backgrounds, which is viewed as a method for analyzing boundary properties. Meanwhile, some cues should be integrated with the obtained probability values for saliency computation. Inspired by the theory of Laplacian similarity metrics, two-stage complementary metrics are established according to different clusters in which two-stage queries lie, and a two-stage detection algorithm (SLSM) of salient objects is thus proposed by combining two-stage complementary similarity metrics with the probability values. Furthermore, when the detailed clusters (dense or sparse) of queries in each detection stage are ignored, an additional unified similarity metric is also constructed. Through the combination of the unified similarity metric and the proposed method for analyzing the boundary properties, another baseline algorithm (SLSMU) is also created. The results of experiments in which these two proposed algorithms are applied to four datasets demonstrate each of the two algorithms outperforms some existing state-of-the-art methods in terms of the different metrics.
C1 [Wang, Baoyan] Northeastern Univ, Coll Informat Sci & Engn, 3-11 Wenhua Rd, Shenyang 110819, Liaoning, Peoples R China.
   [Zhang, Tie] Northeastern Univ, Coll Sci, 3-11 Wenhua Rd, Shenyang 110819, Liaoning, Peoples R China.
   [Wang, Xingang] Northeastern Univ Qinhuangdao, Sch Control Engn, 143 Taishan Rd, Qinhuangdao 066004, Peoples R China.
C3 Northeastern University - China; Northeastern University - China;
   Northeastern University - China
RP Wang, BY (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, 3-11 Wenhua Rd, Shenyang 110819, Liaoning, Peoples R China.; Wang, XG (corresponding author), Northeastern Univ Qinhuangdao, Sch Control Engn, 143 Taishan Rd, Qinhuangdao 066004, Peoples R China.
EM wangbaoyan2005@163.com; wangxingang1217@126.com
FU National Natural Science Foundation of China [51475086]; Colleges and
   Universities of Liaoning Province Science and Technology Research
   Projects [2014020026]; Dr. Fund of Northeastern University at
   Qinhuangdao [XNB2015006]; Colleges and Universities in Hebei Province
   Science and Technology Research Fund [QN2016310]
FX The work is supported by National Natural Science Foundation of China
   (No. 51475086), Colleges and Universities of Liaoning Province Science
   and Technology Research Projects (No. 2014020026), Dr. Fund of
   Northeastern University at Qinhuangdao (No. XNB2015006), and Colleges
   and Universities in Hebei Province Science and Technology Research Fund
   (No. QN2016310). We are particularly grateful to Mingming Cheng group
   with Media Computing Lab, CCCE&CS, Nankai University, for providing free
   evaluation codes of MATLAB: http://mmcheng.net/zh/salobjbenchmark/.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang W., 2015, J ELECT INF TECHNOL, V37, P130
   Klein DA, 2010, IEEE INT C INT ROBOT, P772, DOI 10.1109/IROS.2010.5650583
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2014, PR IEEE COMP DESIGN, P521, DOI 10.1109/ICCD.2014.6974732
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Mingming C., 2014, MSRA10K SALIENT OBJE
   Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin CC, 2014, NEUROCOMPUTING, V129, P378, DOI 10.1016/j.neucom.2013.09.021
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Shen H, 2013, CHINESE J AERONAUT, V26, P1211, DOI 10.1016/j.cja.2013.07.038
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Tavakoli HR, 2011, LECT NOTES COMPUT SC, V6688, P666, DOI 10.1007/978-3-642-21227-7_62
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu XM, 2015, PROC CVPR IEEE, P1949, DOI 10.1109/CVPR.2015.7298805
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361
   Zhao HL, 2009, VISUAL COMPUT, V25, P973, DOI 10.1007/s00371-008-0308-y
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 44
TC 5
Z9 5
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 645
EP 658
DI 10.1007/s00371-017-1404-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100005
DA 2024-07-18
ER

PT J
AU Zhang, M
   Chen, SM
   Shu, ZY
   Xin, SQ
   Zhao, JY
   Jin, G
   Zhang, R
   Beyerer, J
AF Zhang, Meng
   Chen, Shuangmin
   Shu, Zhenyu
   Xin, Shi-Qing
   Zhao, Jieyu
   Jin, Guang
   Zhang, Rong
   Beyerer, Juergen
TI Fast algorithm for 2D fragment assembly based on partial EMD
SO VISUAL COMPUTER
LA English
DT Article
DE Fragment assembly; Partial EMD; Contour features; Lebesgue measure
ID JIGSAW PUZZLES; DISTANCE; POINTS
AB 2D Fragment assembly is an important research topic in computer vision and pattern recognition, and has a wide range of applications such as relic restoration and remote sensing image processing. The key to this problem lies in utilizing contour features or visual cues to find the optimal partial matching. Considering that previous algorithms are weak in predicting the best matching configuration of two neighboring fragments, we suggest using the earth mover's distance, based on length/property correspondence, to measure the similarity, which potentially matches a point on the first contour to a desirable destination point on the second contour. We further propose a greedy algorithm for 2D fragment assembly by repeatedly assembling two neighboring fragments into a composite one. Experimental results on map-piece assembly and relic restoration show that our algorithm runs fast, is insensitive to noise, and provides a novel solution to the fragment assembly problem.
C1 [Zhang, Meng; Chen, Shuangmin; Xin, Shi-Qing; Zhao, Jieyu; Jin, Guang; Zhang, Rong] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Zhejiang, Peoples R China.
   [Shu, Zhenyu] Zhejiang Univ, Ningbo Inst Technol, Sch Informat Sci & Engn, Ningbo 315100, Zhejiang, Peoples R China.
   [Beyerer, Juergen] Fraunhofer Inst Optron Syst Technol & Image Explo, Karlsruhe, Germany.
C3 Ningbo University; Zhejiang University; Fraunhofer Gesellschaft
RP Chen, SM (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Zhejiang, Peoples R China.
EM chenshuangmin@nbu.edu.cn
FU NSF of China [61300168, 61571247, 11226328]; NSF of Zhejiang
   [LZ16F030001, LY13F020018]; Open Research Fund of Zhejiang
   First-foremost Key Subject [XKXL1521, XKXL1406, XKXL1429]; International
   Science and Technology Cooperation Project of Zhejiang [2013C24027]
FX We are grateful to the editors and anonymous reviewers for their
   insightful comments and suggestions. This work is supported by NSF of
   China (61300168, 61571247, 11226328), NSF of Zhejiang (LZ16F030001,
   LY13F020018), the Open Research Fund of Zhejiang First-foremost Key
   Subject (XKXL1521, XKXL1406, XKXL1429), and the International Science
   and Technology Cooperation Project of Zhejiang (2013C24027).
CR Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   Alt H, 2004, ALGORITHMICA, V38, P45, DOI 10.1007/s00453-003-1042-5
   Altantsetseg E, 2014, VISUAL COMPUT, V30, P929, DOI 10.1007/s00371-014-0959-9
   Ancuti C, 2009, VISUAL COMPUT, V25, P677, DOI 10.1007/s00371-009-0353-1
   [Anonymous], ANN APPL STAT
   [Anonymous], SPRINGER INT
   [Anonymous], IC9806 U CAMP
   [Anonymous], 2008, Computer Vision and Pattern Recognition
   [Anonymous], IEEE COMPUT IN PRESS
   [Anonymous], HDB COMPUTATIONAL GE
   [Anonymous], DISCRET COMPUT GEOM
   [Anonymous], 2008, PROC C ORG COMMITTEE
   BAXTER LA, 1992, NAV RES LOG, V39, P833, DOI 10.1002/1520-6750(199210)39:6<833::AID-NAV3220390608>3.0.CO;2-L
   Bo C, 2008, 2008 INTERNATIONAL CONFERENCE ON EMBEDDED SOFTWARE AND SYSTEMS SYMPOSIA, PROCEEDINGS, P447, DOI 10.1109/ICESS.Symposia.2008.78
   Buchin K, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P645
   Chung MG, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P877, DOI 10.1109/ICOSP.1998.770751
   Cui M, 2009, PATTERN RECOGN LETT, V30, P1, DOI 10.1016/j.patrec.2008.08.013
   Cui M, 2007, VISUAL COMPUT, V23, P607, DOI 10.1007/s00371-007-0164-1
   Domokos C, 2016, IEEE T PATTERN ANAL, V38, P195, DOI 10.1109/TPAMI.2015.2450726
   Driemel A, 2013, SIAM J COMPUT, V42, P1830, DOI 10.1137/120865112
   Dyken C, 2009, J GEOGR SYST, V11, P273, DOI 10.1007/s10109-009-0078-8
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Gao Wei, 2014, Computer Integrated Manufacturing Systems, V20, P1615, DOI 10.13196/j.cims.2014.07.gaowei.1615.10.20140711
   Giguere M., US Patent, Patent No. 6015150
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Grauman K, 2004, PROC CVPR IEEE, P220
   GU P, 1995, INT J PROD RES, V33, P3069, DOI 10.1080/00207549508904862
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Huang ZH, 1996, IEEE T IMAGE PROCESS, V5, P1473, DOI 10.1109/83.536895
   Kanezaki A, 2010, VISUAL COMPUT, V26, P1269, DOI 10.1007/s00371-010-0521-3
   Khan MS, 2011, IEEE C EVOL COMPUTAT, P655
   Latecki LJ, 2007, PATTERN RECOGN, V40, P3069, DOI 10.1016/j.patcog.2007.03.004
   Liu HR, 2008, INT J COMPUT VISION, V80, P104, DOI 10.1007/s11263-008-0131-y
   Maheshwari A, 2014, ALGORITHMICA, V69, P641, DOI 10.1007/s00453-013-9758-3
   MILLER JM, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P69, DOI 10.1109/ROBOT.1989.99969
   Pal A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P625
   Parikh D., 2007, PROC IEEE WORKSHOP A, P14
   PORRILL J, 1991, IMAGE VISION COMPUT, V9, P45, DOI 10.1016/0262-8856(91)90048-T
   Richter F, 2013, IEEE T MULTIMEDIA, V15, P582, DOI 10.1109/TMM.2012.2235415
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Shu X, 2011, IMAGE VISION COMPUT, V29, P286, DOI 10.1016/j.imavis.2010.11.001
   Shuralyov D., 2011, Proceedings 2011 IEEE Symposium on 3D User Interfaces (3DUI 2011), P139, DOI 10.1109/3DUI.2011.5759244
   Song YQ, 2012, VISUAL COMPUT, V28, P475, DOI 10.1007/s00371-011-0643-2
   Wang J, 2014, COMPUT AIDED DESIGN, V50, P27, DOI 10.1016/j.cad.2014.01.003
   Wang XC, 2015, VISUAL COMPUT, V31, P1135, DOI 10.1007/s00371-015-1100-4
   WEBSTER RW, 1991, IEEE T SYST MAN CYB, V21, P1271, DOI 10.1109/21.120080
   Xu CJ, 2009, IEEE T PATTERN ANAL, V31, P180, DOI 10.1109/TPAMI.2008.199
   ZHENG YF, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2598, DOI 10.1109/ROBOT.1991.132019
NR 48
TC 8
Z9 9
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1601
EP 1612
DI 10.1007/s00371-016-1303-3
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600009
DA 2024-07-18
ER

PT J
AU Feng, XB
   Zhu, DM
   Wang, ZQ
   Wei, Y
AF Feng, Xiaobing
   Zhu, Dengming
   Wang, Zhaoqi
   Wei, Yi
TI A geometric control of fire motion editing
SO VISUAL COMPUTER
LA English
DT Article
DE Fire animation; Motion control; Data-driven
ID ANIMATION
AB In this paper, we present a control technique to editing the fire motion with the geometry goal shape, which is designed without connection to physical parameters and physical equation solving. To fulfill this, controlling elements are extracted from the input curves conveying the target shape of fire animation. Then, a force field is obtained according to these controlling elements, which would drive the fire towards the target shape. Moreover, to optimize the particles' position, a geometric topology model is proposed to maintain the visual details while generating the fire motion under the external force field frame-by-frame. Experimental results show that our method can generate desirable fire shape under simple interaction.
C1 [Feng, Xiaobing; Zhu, Dengming; Wang, Zhaoqi; Wei, Yi] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Feng, Xiaobing] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Feng, XB (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.; Feng, XB (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM fengxiaobing@ict.ac.cn
FU National Natural Science Foundation of China [61173067, 61379085,
   61532002, 61303157]; National High Technology R&D Program of China
   [2015AA016401]
FX This work is funded by the National Natural Science Foundation of China
   (Grant Nos. 61173067, 61379085, 61532002, 61303157) and the National
   High Technology R&D Program of China (Grant No. 2015AA016401).
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Chang Y., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P111, DOI DOI 10.1145/1643928.1643954
   Dobashi Y., 2015, EUROGRAPHICS 2015
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   Fuller AR, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P175
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Hong Y, 2010, VISUAL COMPUT, V26, P1217, DOI 10.1007/s00371-009-0403-8
   Horvath C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531347
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Lamorlette A, 2002, ACM T GRAPHIC, V21, P729, DOI 10.1145/566570.566644
   Masood A, 2007, J VIS COMMUN IMAGE R, V18, P264, DOI 10.1016/j.jvcir.2006.12.002
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Pighin Frederic., 2004, SCA'04: Proceedings ofthe 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P223, DOI 10.1145/1028523.1028552
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   Sato S., 2012, Proceedings of the digital production symposium, P37
   Sato Syuhei, 2014, SIGGRAPH ASIA 2014 T
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Yi Zhang, 2011, 2011 IEEE Electrical Power & Energy Conference (EPEC), P187, DOI 10.1109/EPEC.2011.6070193
   Zhang GJ, 2011, VISUAL COMPUT, V27, P199, DOI 10.1007/s00371-010-0526-y
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 24
TC 1
Z9 1
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 585
EP 595
DI 10.1007/s00371-016-1283-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300004
DA 2024-07-18
ER

PT J
AU Cai, N
   Su, ZH
   Lin, ZN
   Wang, H
   Yang, ZJ
   Ling, BWK
AF Cai, Nian
   Su, Zhenghang
   Lin, Zhineng
   Wang, Han
   Yang, Zhijing
   Ling, Bingo Wing-Kuen
TI Blind inpainting using the fully convolutional neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Blind inpainting; Deep learning; Convolutional neural
   network
ID IMAGE
AB Most of existing inpainting techniques require to know beforehandwhere those damaged pixels are, i.e., non-blind inpainting methods. However, in many applications, such information may not be readily available. In this paper, we propose a novel blind inpainting method based on a fully convolutional neural network. We term this method as blind inpainting convolutional neural network (BICNN). It purely cascades three convolutional layers to directly learn an end-to-end mapping between a pre-acquired dataset of corrupted/ground truth subimage pairs. Stochastic gradient descent with standard backpropagation is used to train the BICNN. Once the BICNN is learned, it can automatically identify and remove the corrupting patterns from a corrupted image without knowing the specific regions. The learned BICNN takes a corrupted image of any size as input and directly produces a clean output by only one pass of forward propagation. Experimental results indicate that the proposed method can achieve a better inpainting performance than the existing inpainting methods for various corrupting patterns.
C1 [Cai, Nian; Su, Zhenghang; Lin, Zhineng; Yang, Zhijing; Ling, Bingo Wing-Kuen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Wang, Han] Guangdong Univ Technol, Sch Electromech Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology
RP Cai, N (corresponding author), Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM cainian@gdut.edu.cn
FU National Natural Science Foundation of China [61001179, 61372173,
   61471132, 61201393]; Guangdong Higher Education Engineering Technology
   Research Center [501130144]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant Nos. 61001179, 61372173, 61471132 and
   61201393) and the Guangdong Higher Education Engineering Technology
   Research Center (No. 501130144). We also thank to Rolf Kohler for his
   help that he provides a lot of materials in [20] to us.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, NIPS
   [Anonymous], ARXIV150100092
   [Anonymous], 2006, Notes on convolutional neural networks
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Fadili MJ, 2009, COMPUT J, V52, P64, DOI 10.1093/comjnl/bxm055
   Glorot X., 2010, P INT C ART INT STAT, P249
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   Jia Y., 2014, P 22 ACM INT C MULT, P675
   Köhler R, 2014, LECT NOTES COMPUT SC, V8753, P523, DOI 10.1007/978-3-319-11752-2_43
   Le Meur O, 2013, IEEE T IMAGE PROCESS, V22, P3779, DOI 10.1109/TIP.2013.2261308
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098
NR 28
TC 56
Z9 62
U1 2
U2 50
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 249
EP 261
DI 10.1007/s00371-015-1190-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400011
OA Bronze
DA 2024-07-18
ER

PT J
AU Conde-Rodríguez, F
   Torres, JC
   García-Fernández, AL
   Feito-Higueruela, FR
AF Conde-Rodriguez, Francisco
   Torres, Juan-Carlos
   Garcia-Fernandez, Angel-Luis
   Feito-Higueruela, Francisco-Ramon
TI A comprehensive framework for modeling heterogeneous objects
SO VISUAL COMPUTER
LA English
DT Article
DE Object modeling; Heterogeneous objects; Bezier
AB Many real objects are heterogeneous. They are composed of diverse materials, which are present in varying proportions. Materials inside the solid do not have to be uniformly distributed. So, methods capable of accurately model not only the boundary of the solid, but also the distribution of material in every single point of its interior, are needed. In this paper we propose a new framework for modeling heterogeneous objects. The framework is comprehensive as it characterizes precisely heterogeneous objects, defines an adequate mathematical model that captures the essence of such objects, and a computational representation to represent the modeled objects in a computer. Our framework is based on B,zi,r hyperpatches and solves the main problems of this mathematical tool. We have implemented it completely in order to check whether it is possible to precisely model real objects.
C1 [Conde-Rodriguez, Francisco; Garcia-Fernandez, Angel-Luis; Feito-Higueruela, Francisco-Ramon] Univ Jaen, EPS, Dept Informat, Campus Lagunillas S-N, Jaen 23071, Spain.
   [Torres, Juan-Carlos] Univ Granada, Dept Lenguajes & Sistemas Informat, ETSIIT, C Periodista Manuel Saucedo Aranda S-N, E-18071 Granada, Spain.
C3 Universidad de Jaen; University of Granada
RP Conde-Rodríguez, F (corresponding author), Univ Jaen, EPS, Dept Informat, Campus Lagunillas S-N, Jaen 23071, Spain.
EM fconde@ujaen.es; jctorres@ugr.es; algarcia@ujaen.es; ffeito@ujaen.es
RI García-Fernández, Ángel-Luis/E-4257-2012; Feito, Francisco
   F.R./M-1672-2014; Torres, Juan Carlos/C-2432-2012
OI García-Fernández, Ángel-Luis/0000-0002-8183-7130; Conde-Rodriguez,
   Francisco/0000-0001-6793-1377; Torres, Juan Carlos/0000-0002-0327-7748
FU Spanish Ministry of Economy and Competitiveness [TIN2014 58218-R,
   TIN2014-60956-R]; ERDF funds
FX This work has been partially funded by the Spanish Ministry of Economy
   and Competitiveness through Grants TIN2014 58218-R and TIN2014-60956-R
   with ERDF funds.
CR [Anonymous], 1988, An introduction to solid modeling
   CASALE MS, 1985, IEEE COMPUT GRAPH, V5, P45, DOI 10.1109/MCG.1985.276402
   Hua J, 2005, J COMPUT INF SCI ENG, V5, P149, DOI 10.1115/1.1881352
   Hua J., 2004, P 9 ACM S SOLID MODE, P47
   Jackson TR, 1999, MATER DESIGN, V20, P63, DOI 10.1016/S0261-3069(99)00011-4
   Kou XY, 2007, COMPUT AIDED DESIGN, V39, P284, DOI 10.1016/j.cad.2006.12.007
   Kumar V, 1998, J MECH DESIGN, V120, P659, DOI 10.1115/1.2829329
   Lasser D., 1985, Computer-Aided Geometric Design, V2, P145, DOI 10.1016/0167-8396(85)90018-4
   Mortenson M., 1985, Geometric Modeling
   Ochiai K., 2007, US Patent, Patent No. [7,201,198, 7201198]
   Qian XP, 2003, J MECH DESIGN, V125, P416, DOI 10.1115/1.1582877
   REQUICHA AAG, 1980, COMPUT SURV, V12, P436
   Rodriguez F.A. Conde, 2001, P EUR 2001
   Schmitt B, 2004, VISUAL COMPUT, V20, P130, DOI 10.1007/s00371-003-0236-9
   Warkhedkar RM, 2009, COMPUT AIDED DESIGN, V41, P586, DOI 10.1016/j.cad.2008.10.016
   Yang PH, 2007, COMPUT AIDED DESIGN, V39, P95, DOI 10.1016/j.cad.2006.10.005
NR 16
TC 11
Z9 11
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2017
VL 33
IS 1
BP 17
EP 31
DI 10.1007/s00371-015-1149-0
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2JM
UT WOS:000392313200004
DA 2024-07-18
ER

PT J
AU Fan, X
   Feng, YY
   Chai, Z
   Gu, X
   Luo, ZX
AF Fan, Xin
   Feng, Yuyao
   Chai, Zhi
   Gu, Xianfeng David
   Luo, Zhongxuan
TI Image morphing with conformal welding
SO VISUAL COMPUTER
LA English
DT Article
DE Image morphing; Conformal welding; Signature; Laplacian constraint
ID EXTRACTION
AB We address the issue of deforming an image of a source object to that of a target. Previous works including barycentric coordinates and functional maps can hardly enforce shape consistency, especially for the objects with complex nested shape components. We leverage the conformal welding theory that maps 2D shapes (planar contours) to the automorphisms of the unit circle, named shape signatures. Conformal welding enables us to apply the Laplacian constraint to deformations in the signature space (or unit circle domain), which renders efficiency and flexibility. Additionally, we are able to fully reconstruct complex shape contours from deformed signatures, and hence generate the morphed images for target shapes. The experiments on complex shape contours and facial images, where multiple components exist, validate the effectiveness of the proposed approach.
C1 [Fan, Xin; Feng, Yuyao; Luo, Zhongxuan] Dalian Univ Technol, Sch Software, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
   [Chai, Zhi; Gu, Xianfeng David] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Dalian University of Technology; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Fan, X (corresponding author), Dalian Univ Technol, Sch Software, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM xin.fan@ieee.org
RI Chai, Zhi/KIG-1970-2024
OI Gu, Xianfeng David/0000-0001-8226-5851
FU Natural Science Foundation of China (NSFC) [61272371, 61328206]; Civil
   Aviation Administration of China [U1233110]; NSFC [U1233110]
FX This work is partially supported by the Natural Science Foundation of
   China (NSFC) under Grant Nos. 61272371 and 61328206, and also by the
   Civil Aviation Administration of China and NSFC jointly funded project
   (U1233110).
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   ARAD N, 1994, CVGIP-GRAPH MODEL IM, V56, P161, DOI 10.1006/cgip.1994.1015
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Chen TL, 2014, J APPL STAT, V41, P242, DOI 10.1080/02664763.2013.838667
   Dykstra C, 1999, IEEE T NUCL SCI, V46, P673, DOI 10.1109/23.775597
   Fan X., 2015, NEUROCOMPUTING
   Fan X, 2015, IEEE T IMAGE PROCESS, V24, P1164, DOI 10.1109/TIP.2015.2390976
   Gao L, 2013, COMPUT GRAPH FORUM, V32, P449, DOI 10.1111/cgf.12065
   Gu X.D., 2008, Computational Conformal Geometry
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Hu WY, 2014, IEEE J EM SEL TOP C, V4, P70, DOI 10.1109/JETCAS.2014.2298259
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Kabul I, 2011, COMPUT GRAPH FORUM, V30, P2341, DOI 10.1111/j.1467-8659.2011.02067.x
   Liu LG, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P111
   Lui LM, 2014, IEEE T PATTERN ANAL, V36, P1384, DOI 10.1109/TPAMI.2013.215
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sharon E, 2006, INT J COMPUT VISION, V70, P55, DOI 10.1007/s11263-006-6121-z
   Shi R, 2011, IEEE NUCL SCI CONF R, P3137, DOI 10.1109/NSSMIC.2011.6152571
   Szeptycki Przemyslaw, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1505, DOI 10.1109/ICPR.2010.372
   Tal A, 1999, COMPUT GRAPH FORUM, V18, pC339, DOI 10.1111/1467-8659.00354
   Wei J, 2013, IEEE ICCE, P1, DOI 10.1109/ICCE.2013.6486769
   Whitaker RT, 2000, IEEE T IMAGE PROCESS, V9, P1849, DOI 10.1109/83.877208
   Wu EH, 2013, VISUAL COMPUT, V29, P311, DOI 10.1007/s00371-012-0734-8
   Xiao XZ, 2010, COMPUT ANIMAT VIRT W, V21, P289, DOI 10.1002/cav.343
   Yang F., 2012, Proceedings of the 2012 Graphics Interace Conference, P93
   Yang WW, 2014, VISUAL COMPUT, V30, P919, DOI 10.1007/s00371-014-0955-0
   Yang WW, 2009, COMPUT GRAPH-UK, V33, P414, DOI 10.1016/j.cag.2009.03.007
   Yap PT, 2009, NEUROIMAGE, V47, P549, DOI 10.1016/j.neuroimage.2009.04.055
   Zeng W, 2011, IEEE INT C COMM ICC, P1
   Zeng W., 2009, ACM Symposium on Solid and Physical Modeling (SPM'09), P89
   Zhang CZ, 2002, IEEE T IMAGE PROCESS, V11, P1249, DOI 10.1109/TIP.2002.804277
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhu L, 2007, IEEE T IMAGE PROCESS, V16, P1481, DOI 10.1109/TIP.2007.896637
NR 38
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1191
EP 1203
DI 10.1007/s00371-015-1188-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400011
DA 2024-07-18
ER

PT J
AU Zhao, L
   Zhang, Y
   Yin, BC
   Sun, YF
   Hu, YL
   Piao, XL
   Wu, QJ
AF Zhao, Lu
   Zhang, Yong
   Yin, Baocai
   Sun, Yanfeng
   Hu, Yongli
   Piao, Xinglin
   Wu, Qianjun
TI Fisher discrimination-based -norm sparse representation for face
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Sparse representation; l(2),(1)-Norm; Face recognition; Fisher
   discriminant
ID ILLUMINATION; EIGENFACES; ALGORITHM
AB In recent years, sparse representation-based classification (SRC) has made great progress in face recognition (FR). However, SRC emphasizes noise sparsity too much and it is not suitable for the real world. In this paper, we propose a robust -norm Sparse Representation framework that constrains the noise penalty by the -norm. The -norm takes advantage of both the discriminative nature of the -norm and the systemic representation of the -norm. In addition, we use the nuclear norm to constrain the coefficient matrix. Motivated by the Fisher criterion, we propose the Fisher discriminant-based -norm sparse representation method for FR which utilizes a supervised approach. Thus, we consider the within-class scatter and between-class scatter when all of the label information is available. The paper shows that the model can provide stronger discriminant power than the classical sparse representation models and can be solved by the alternating direction method of multiplier. Additionally, it is robust to the contiguous occlusion noise. Extensive experiments demonstrate that our method achieves significantly better results than SRC and some other sparse representation methods for FR when addressing large regions with contiguous occlusion.
C1 [Zhao, Lu; Zhang, Yong; Yin, Baocai; Sun, Yanfeng; Hu, Yongli; Piao, Xinglin; Wu, Qianjun] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
   [Yin, Baocai] Collaborat Innovat Ctr Elect Vehicles Beijing, Beijing, Peoples R China.
C3 Beijing University of Technology; Dalian University of Technology
RP Zhao, L (corresponding author), Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM blue198486@163.com; zhangyong2010@bjut.edu.cn; ybc@bjut.edu.cn;
   yfsun@bjut.edu.cn; huyongli@bjut.edu.cn; piaoxinglin1987@gmail.com;
   qianju.wu@bjut.edu.cn
RI Zhang, Yong/AAW-8880-2021; Zhang, Yongzhe/AAF-5787-2021
OI Zhang, Yong/0000-0001-6650-6790; Hu, Yongli/0000-0003-0440-438X; Kwan,
   Jenny/0000-0002-3388-9980
FU National Natural Foundation of China [61390510, 61300065, 61370119,
   61171169]; Beijing Natural Science Foundation [4132013, 4142010];
   Beijing science and technology project [Z151100002115040]; PHR(IHLB)
FX The research project was supported by the National Natural Foundation of
   China under Grant No. 61390510, 61300065, 61370119, 61171169 and Beijing
   Natural Science Foundation No. 4132013, 4142010 and supported by the
   Beijing science and technology project No. Z151100002115040, and also
   supported by PHR(IHLB).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], VIS COMPUT
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chao YW, 2011, IEEE IMAGE PROC, P761, DOI 10.1109/ICIP.2011.6116666
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Ding C., 2006, P 23 INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880
   Gang Hua, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   Gao QX, 2013, IEEE T IMAGE PROCESS, V22, P2521, DOI 10.1109/TIP.2013.2249077
   Gao QX, 2012, PATTERN RECOGN, V45, P3717, DOI 10.1016/j.patcog.2012.03.024
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Grave G. R., 2011, Adv. Neural Inf. Process.Syst., V24, P2187
   Huang K., 2007, P 19 INT C NEUR INF, P609, DOI DOI 10.7551/MITPRESS/7503.003.0081
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Jia HJ, 2009, PROC CVPR IEEE, P136, DOI 10.1109/CVPRW.2009.5206862
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li ZF, 2009, IEEE T PATTERN ANAL, V31, P755, DOI 10.1109/TPAMI.2008.174
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CY, 2013, J VIS COMMUN IMAGE R, V24, P111, DOI 10.1016/j.jvcir.2012.05.003
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Ocegueda O, 2013, IEEE T PATTERN ANAL, V35, P728, DOI 10.1109/TPAMI.2012.126
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Ortiz EG, 2013, PROC CVPR IEEE, P3531, DOI 10.1109/CVPR.2013.453
   Ou WH, 2014, PATTERN RECOGN, V47, P1559, DOI 10.1016/j.patcog.2013.10.017
   Qian JJ, 2014, IEEE COMPUT SOC CONF, P21, DOI 10.1109/CVPRW.2014.9
   Quan W, 2015, VISUAL COMPUT, V31, P1307, DOI 10.1007/s00371-014-1012-8
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tan XY, 2009, IEEE T INF FOREN SEC, V4, P217, DOI 10.1109/TIFS.2009.2020772
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Vinje WE, 2000, SCIENCE, V287, P1273, DOI 10.1126/science.287.5456.1273
   Wagner A, 2009, PROC CVPR IEEE, P597, DOI 10.1109/CVPRW.2009.5206654
   Wang J, 2014, IEEE T CYBERNETICS, V44, P2368, DOI 10.1109/TCYB.2014.2307067
   Wanger A., 2012, IEEE T PATTERN ANAL, V34, P372
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang JF, 2009, SIAM J IMAGING SCI, V2, P569, DOI 10.1137/080730421
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yang M, 2011, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2011.5995393
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Yuen PC, 2002, PATTERN RECOGN, V35, P1247, DOI 10.1016/S0031-3203(01)00101-7
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang N, 2013, NEUROCOMPUTING, V111, P13, DOI 10.1016/j.neucom.2012.12.012
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zheng ZL, 2014, PATTERN RECOGN, V47, P3502, DOI 10.1016/j.patcog.2014.05.001
   Zhou ZH, 2009, IEEE I CONF COMP VIS, P1050, DOI 10.1109/ICCV.2009.5459383
NR 53
TC 12
Z9 12
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2016
VL 32
IS 9
BP 1165
EP 1178
DI 10.1007/s00371-015-1169-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4DE
UT WOS:000382161400009
DA 2024-07-18
ER

PT J
AU Kim, T
   Lee, J
   Kim, CH
AF Kim, TaeHyeong
   Lee, Jung
   Kim, Chang-Hun
TI Physics-inspired controllable flame animation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Flame animation; Physics inspired; Temperature control; Fluid control
ID SMOKE
AB We propose a novel method conceptualized from the properties of physics where in particular the shape of a flame is determined by temperature that enables a control mechanism for the intuitive shaping of a flame. We focused on a trade-off issue from computer graphics whereby the turbulent flow that expresses the characteristics of the flame has a tendency to shift continuously, whereas the velocity constraints that contain a fluid within a target shape have a tendency to force movement in a particular direction. Trade-off made it difficult for animation designers to maintain a flame within the intended target shape. This paper resolves the issue by enabling the flame to be controlled without any velocity constraints by using the following two techniques: First, we model the temperature and force of the explosion generated by the combustion of explosive gaseous fuel and apply it to certain regions. Second, we expand the space of the interface between the fuel and the burned products, classifying that space into four regions and controlling the target shape of the flame by delicate adjustments to the temperature in each region. Experiments show that the flame maintains the appearance of dynamic movement while preserving the detailed 3D shapes specified by the scene designers.
C1 [Kim, TaeHyeong; Kim, Chang-Hun] Korea Univ, Interdisciplinary Program Visual Informat Proc, Seoul, South Korea.
   [Lee, Jung] Hallym Univ, Dept Convegence Software, Chunchon, Gangwon Do, South Korea.
C3 Korea University; Hallym University
RP Kim, CH (corresponding author), Korea Univ, Interdisciplinary Program Visual Informat Proc, Seoul, South Korea.
EM chkim@korea.ac.kr
OI Kim, TaeHyeong/0000-0002-1862-9208
CR [Anonymous], 2008, Fluid Simulation for Computer Graphics
   Fattal R, 2004, ACM T GRAPHIC, V23, P441, DOI 10.1145/1015706.1015743
   Feldman BE, 2003, ACM T GRAPHIC, V22, P708, DOI 10.1145/882262.882336
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Hong JM, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239498
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Hong Y, 2010, VISUAL COMPUT, V26, P1217, DOI 10.1007/s00371-009-0403-8
   Horvath C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531347
   Jeong S, 2013, COMPUT GRAPH FORUM, V32, P225, DOI 10.1111/cgf.12230
   Kawada G., 2011, P 2011 ACM SIGGRAPHE, P167
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Myungjoo Kang, 2000, Journal of Scientific Computing, V15, P323, DOI 10.1023/A:1011178417620
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Raveendran Karthik., 2012, Controlling liquids using meshes, P255
   Ross Garry., 2012, The Hunger Games
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Sethian J. A., 1999, LEVEL SET METHODS FA
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Shin SH, 2007, COMPUT ANIMAT VIRT W, V18, P447, DOI 10.1002/cav.202
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Yang B, 2013, COMPUT GRAPH-UK, V37, P775, DOI 10.1016/j.cag.2013.05.001
   Yngve GD, 2000, COMP GRAPH, P29, DOI 10.1145/344779.344801
NR 25
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 871
EP 880
DI 10.1007/s00371-016-1267-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600019
DA 2024-07-18
ER

PT J
AU Ortner, T
   Sorger, J
   Piringer, H
   Hesina, G
   Gröller, E
AF Ortner, Thomas
   Sorger, Johannes
   Piringer, Harald
   Hesina, Gerd
   Groeller, Eduard
TI Visual analytics and rendering for tunnel crack analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE 3D real-time rendering; Visual analytics; Integration of spatial and
   non-spatial data; Methodology
ID VISUALIZATION; MODELS; SYSTEM; FOCUS
AB The visual analysis of surface cracks plays an essential role in tunnel maintenance when assessing the condition of a tunnel. To identify patterns of cracks, which endanger the structural integrity of its concrete surface, analysts need an integrated solution for visual analysis of geometric and multivariate data to decide if issuing a repair project is necessary. The primary contribution of this work is a design study, supporting tunnel crack analysis by tightly integrating geometric and attribute views to allow users a holistic visual analysis of geometric representations and multivariate attributes. Our secondary contribution is Visual Analytics and Rendering, a methodological approach which addresses challenges and recurring design questions in integrated systems. We evaluated the tunnel crack analysis solution in informal feedback sessions with experts from tunnel maintenance and surveying. We substantiated the derived methodology by providing guidelines and linking it to examples from the literature.
C1 [Ortner, Thomas; Sorger, Johannes; Hesina, Gerd] VRVis Res Ctr, Vienna, Austria.
   [Piringer, Harald] VRVis Res Ctr, Visual Anal Grp, Vienna, Austria.
   [Groeller, Eduard] TU Wien, Vienna, Austria.
C3 Technische Universitat Wien
RP Ortner, T (corresponding author), VRVis Res Ctr, Vienna, Austria.
EM ortner@vrvis.at
RI Gröller, Eduard/AAH-2111-2020
OI Gröller, Eduard/0000-0002-8569-4149
CR [Anonymous], 2010, Proceedings of Graphics Interface 2010
   Berger W, 2011, COMPUT GRAPH FORUM, V30, P911, DOI 10.1111/j.1467-8659.2011.01940.x
   Berger W, 2010, IEEE INT CONF INF VI, P140, DOI 10.1109/IV.2010.30
   Boukhelifa N, 2003, INTERNATIONAL CONFERENCE ON COORDINATED AND MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P76, DOI 10.1109/CMV.2003.1215005
   Buchholz H, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P629, DOI 10.1109/IV.2005.117
   Butkiewicz T, 2008, IEEE T VIS COMPUT GR, V14, P1165, DOI 10.1109/TVCG.2008.149
   Chang R., 2006, P ACM SIGGRAPH, V6, P130
   Chang R, 2007, IEEE T VIS COMPUT GR, V13, P1169, DOI 10.1109/TVCG.2007.70574
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Fernando R., 2004, GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics
   Ferreira N, 2015, IEEE CONF VIS ANAL, P97, DOI 10.1109/VAST.2015.7347636
   Fuchs R, 2009, COMPUT GRAPH FORUM, V28, P1670, DOI 10.1111/j.1467-8659.2009.01429.x
   Gresh DL, 2000, IEEE VISUAL, P489, DOI 10.1109/VISUAL.2000.885739
   Hauser H., 2006, SCI VISUALIZATION VI, P305, DOI [DOI 10.1007/3-540-30790-7_18, 10.1007/354030790718]
   Jern M, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P85, DOI 10.1109/CMV.2007.21
   Jianu R, 2009, IEEE T VIS COMPUT GR, V15, P1449, DOI 10.1109/TVCG.2009.141
   Kehrer J, 2013, IEEE T VIS COMPUT GR, V19, P495, DOI 10.1109/TVCG.2012.110
   Matkovic K, 2008, IEEE INT CONF INF VI, P215, DOI 10.1109/IV.2008.87
   Migut M. A., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P141, DOI 10.1109/VAST.2011.6102451
   North C., 2000, Proceedings of the the working conference on Advanced visual interfaces (AVI) 2000, P128, DOI [DOI 10.1145/345513.345282, 10.1145/345513.345282]
   Ortner T., 2016, IEEE T VIS COMPUT GR
   Ortner T., 2010, P REAL CORP
   Paar G., 2006, SPIE P 2 3 DIM METH, V6382
   Piringer H, 2010, COMPUT GRAPH FORUM, V29, P983, DOI 10.1111/j.1467-8659.2009.01684.x
   Ribicic H, 2013, IEEE T VIS COMPUT GR, V19, P1062, DOI 10.1109/TVCG.2012.175
   Roberts JC, 2007, CMV 2007: FIFTH INTERNATIONAL CONFERENCE ON COORDINATED & MULTIPLE VIEWS IN EXPLORATORY VISUALIZATION, PROCEEDINGS, P61, DOI 10.1109/CMV.2007.20
   Schmidt J, 2014, IEEE CONF VIS ANAL, P153, DOI 10.1109/VAST.2014.7042491
   Semmo A, 2012, COMPUT GRAPH FORUM, V31, P885, DOI 10.1111/j.1467-8659.2012.03081.x
   Sorger J., 2015, P EUR S VIS MOD VIS
   Sorger J, 2016, IEEE T VIS COMPUT GR, V22, P290, DOI 10.1109/TVCG.2015.2468011
   Tory M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P151, DOI 10.1109/INFVIS.2004.59
   Trapp Matthias., 2011, Advances in 3D Geo-Information Sciences, P197
   Viola I, 2006, IEEE T VIS COMPUT GR, V12, P933, DOI 10.1109/TVCG.2006.152
   Wang Baldonado M. Q., 2000, P WORK C ADV VIS INT, P110, DOI [10.1145/345513.345271, DOI 10.1145/345513.345271]
   Weaver C, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P159, DOI 10.1109/INFVIS.2004.12
NR 35
TC 6
Z9 6
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 859
EP 869
DI 10.1007/s00371-016-1257-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600018
PM 31148881
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhong, GY
   Liu, RS
   Cao, JJ
   Su, ZX
AF Zhong, Guangyu
   Liu, Risheng
   Cao, Junjie
   Su, Zhixun
TI A generalized nonlocal mean framework with object-level cues for
   saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Generalized nonlocal mean; Saliency detection; Objectness cue
AB Nonlocal mean (NM) is an efficient method for many low-level image processing tasks. However, it is challenging to directly utilize NM for saliency detection. This is because that conventional NM method can only extract the structure of the image itself and is based on regular pixel-level graph. However, saliency detection usually requires human perceptions and more complex connectivity of image elements. In this paper, we propose a novel generalized nonlocal mean (GNM) framework with the object-level cue which fuses the low-level and high-level cues to generate saliency maps. For a given image, we first use uniqueness to describe the low-level cue. Second, we adopt the objectness algorithm to find potential object candidates, then we pool the object measures onto patches to generate two high-level cues. Finally, by fusing these three cues as an object-level cue for GNM, we obtain the saliency map of the image. Extensive experiments show that our GNM saliency detector produces more precise and reliable results compared to state-of-the-art algorithms.
C1 [Zhong, Guangyu; Cao, Junjie; Su, Zhixun] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Liu, Risheng] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Cao, JJ (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM guangyuzhonghikari@gmail.com; rsliu@dlut.edu.cn; jjcao1231@gmail.com;
   zxsu@dlut.edu.cn
FU National Natural Science Foundation of China [61300086, 61363048,
   61173102, 61173103, 91230103]; Fundamental Research Funds for the
   Central Universities; Open Project Program of the State Key Laboratory
   of CADAMP;CG, Zhejiang University, Zhejiang, China [A1404]; National
   Science and Technology Major Project [2013ZX04005021]
FX Risheng Liu is supported by the National Natural Science Foundation of
   China (No. 61300086), the Fundamental Research Funds for the Central
   Universities and the Open Project Program of the State Key Laboratory of
   CAD&CG, Zhejiang University, Zhejiang, China (No. A1404). Junjie Cao is
   supported by the National Natural Science Foundation of China (No.
   61363048 and 61173102). Zhixun Su is supported by National Natural
   Science Foundation of China (Nos. 61173103 and 91230103) and National
   Science and Technology Major Project (No. 2013ZX04005021)).
CR Achanta R., 2008, ICVS
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2013, ICCV
   [Anonymous], 2006, NIPS
   [Anonymous], TPAMI
   [Anonymous], 2013, CVPR
   [Anonymous], 2005, CVPR
   [Anonymous], 2012, CVPR
   [Anonymous], 2011, BMVC
   [Anonymous], 2013, ICCV
   [Anonymous], MIT Saliency Benchmark
   [Anonymous], 2009, ICCV
   [Anonymous], 2012, CVPR
   [Anonymous], 2013, CVPR
   [Anonymous], 2012, ECCV
   [Anonymous], 2009, CVPR
   [Anonymous], 2013, CVPR
   [Anonymous], 2012, C COMP VIS PATT REC, DOI DOI 10.1109/CVPR.2012.6248031
   Aytekin C., 2014, ICPR
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V., 2009, CVPR
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia Y., 2013, ICCV
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P., 2013, ICCV
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Li X., 2013, ICCV
   LINDENBAUM M, 1994, PATTERN RECOGN, V27, P1, DOI 10.1016/0031-3203(94)90013-2
   Ma Y. F., 2003, INT C MULTIMEDIA
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Pan J., 2013, ICIP
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shi Y., 2014, TVC, P1
   Teuber HL, 1955, ANNU REV PSYCHOL, V6, P267, DOI 10.1146/annurev.ps.06.020155.001411
   Tomasi C., 1998, INT C COMP VIS ICCV
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang J., 2010, SPIE
   Yaroslavsky L.P., 1985, Digital Picture Processing: An Introduction
NR 47
TC 12
Z9 12
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 611
EP 623
DI 10.1007/s00371-015-1077-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800007
DA 2024-07-18
ER

PT J
AU Wang, GF
   Wang, B
   Zhong, F
   Qin, XY
   Chen, BQ
AF Wang, Guofeng
   Wang, Bin
   Zhong, Fan
   Qin, Xueying
   Chen, Baoquan
TI Global optimal searching for textureless 3D object tracking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE 3D tracking; 3D-2D correspondence; Global optimization; Dynamic
   programming
ID REAL-TIME; SEGMENTATION
AB Textureless 3D object tracking of the object's position and orientation is a considerably challenging problem, for which a 3D model is commonly used. The 3D-2D correspondence between a known 3D object model and 2D scene edges in an image is standardly used to locate the 3D object, one of the most important problems in model-based 3D object tracking. State-of-the-art methods solve this problem by searching correspondences independently. However, this often fails in highly cluttered backgrounds, owing to the presence of numerous local minima. To overcome this problem, we propose a new method based on global optimization for searching these correspondences. With our search mechanism, a graph model based on an energy function is used to establish the relationship of the candidate correspondences. Then, the optimal correspondences can be efficiently searched with dynamic programming. Qualitative and quantitative experimental results demonstrate that the proposed method performs favorably compared to the state-of-the-art methods in highly cluttered backgrounds.
C1 [Wang, Guofeng; Wang, Bin; Zhong, Fan; Qin, Xueying; Chen, Baoquan] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
C3 Shandong University
RP Qin, XY (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM wangguofeng525@gmail.com; binwangsdu@gmail.com; zhongfan@sdu.edu.cn;
   qxy@sdu.edu.cn; baoquan.chen@gmail.com
RI Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
FU 973 program of China [2015CB352500]; 863 program of China
   [2015AA016405]; NSF of China [61173070, 61202149]
FX The authors gratefully acknowledge the anonymous reviewers for their
   comments to help us to improve our paper, and also thank for their
   enormous help in revising this paper. This work is supported by 973
   program of China (No. 2015CB352500), 863 program of China (No.
   2015AA016405), and NSF of China (Nos. 61173070, 61202149).
CR [Anonymous], IEEE ACM INT S MIX A
   [Anonymous], 2011, P ACM S US INT SOFTW
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], IEEE I CONF COMP VIS
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Brown JA, 2012, IEEE T VIS COMPUT GR, V18, P68, DOI 10.1109/TVCG.2011.34
   Comport AI, 2006, IEEE T VIS COMPUT GR, V12, P615, DOI 10.1109/TVCG.2006.78
   Dambreville S, 2008, LECT NOTES COMPUT SC, V5303, P169, DOI 10.1007/978-3-540-88688-4_13
   Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620
   Hinterstoisser S., 2007, P IEEE INT C COMPUTE, P1
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Kim K, 2010, VISUAL COMPUT, V26, P1145, DOI 10.1007/s00371-010-0490-6
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Marchand É, 2001, IMAGE VISION COMPUT, V19, P941, DOI 10.1016/S0262-8856(01)00054-3
   Prisacariu VA, 2012, INT J COMPUT VISION, V98, P335, DOI 10.1007/s11263-011-0514-3
   Rosenhahn B, 2007, INT J COMPUT VISION, V73, P243, DOI 10.1007/S11263-006-9965-3
   Schmaltz C, 2012, MACH VISION APPL, V23, P557, DOI 10.1007/s00138-010-0317-5
   Seo BK, 2014, IEEE T VIS COMPUT GR, V20, P99, DOI 10.1109/TVCG.2013.94
   Vacchetti L, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P48, DOI 10.1109/ISMAR.2004.24
   Vacchetti L, 2004, IEEE T PATTERN ANAL, V26, P1385, DOI 10.1109/TPAMI.2004.92
   Wuest H, 2005, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P62
NR 23
TC 28
Z9 38
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 979
EP 988
DI 10.1007/s00371-015-1098-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500023
DA 2024-07-18
ER

PT J
AU Zhang, X
   Sun, FC
   Liu, GC
   Ma, Y
AF Zhang, Xin
   Sun, Fuchun
   Liu, Guangcan
   Ma, Yi
TI Non-blind deblurring of structured images with geometric deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Non-Blind deconvolution; Geometric deformation; Total variation
ID CAMERA
AB Non-blind deconvolution, which is to restore a sharp version of a given blurred image when the blur kernel is known, is a fundamental step in image deblurring. While the problem has been extensively studied, existing methods have conveniently ignored an important fact that deformation can significantly affect the statistical characteristics of an image and introduce additional blurring effect. In this paper, we show how to enhance non-blind deconvolution by recovering and undoing the deformation while deconvolving a given blurred image. We show that this is the case for almost all popular regularizers that have been proposed for image deblurring such as total variation and its variants. We conduct extensive simulations and experiments on real images and verify that the incorporation of geometric deformation in deconvolution can significantly improve the final deblurring results. Combined with existing blur kernel estimation techniques, our method can also be used to enhance blind image deblurring.
C1 [Zhang, Xin; Sun, Fuchun] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Liu, Guangcan] Univ Illinois, Champaign, IL USA.
   [Ma, Yi] Microsoft Res Asia, Visual Comp Grp, Beijing, Peoples R China.
C3 Tsinghua University; University of Illinois System; University of
   Illinois Urbana-Champaign; Microsoft Research Asia; Microsoft
RP Zhang, X (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM xinzhang1111@gmail.com
RI Liu, Guangcan/J-1391-2014
OI Liu, Guangcan/0000-0002-9428-4387
FU National Basic Research Program (973 Program) of China [2013CB329403];
   ONR [N00014-09-1-0230]; NSF [CCF 09-64215]; NSF IIS [11-16012]
FX X. Zhang and F. Sun are supported by the National Basic Research Program
   (973 Program) of China (No. 2013CB329403). Yi Ma is partially supported
   by the funding of ONR N00014-09-1-0230, NSF CCF 09-64215, NSF IIS
   11-16012.
CR Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Joshi N, 2009, PROC CVPR IEEE, P1550, DOI 10.1109/CVPRW.2009.5206802
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268
   Landi G, 2012, NUMER ALGORITHMS, V60, P169, DOI 10.1007/s11075-011-9517-y
   Levin A., 2006, P NEURAL INFORM PROC, V19, P841
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Liu G., 2012, ABS12092082 ARXIV
   Mignotte M, 2006, IEEE T IMAGE PROCESS, V15, P1973, DOI 10.1109/TIP.2006.873446
   Nagy JG, 2004, NUMER ALGORITHMS, V36, P73, DOI 10.1023/B:NUMA.0000027762.08431.64
   RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Tai YW, 2011, IEEE T PATTERN ANAL, V33, P1603, DOI 10.1109/TPAMI.2010.222
   Tsumuraya F, 1996, J OPT SOC AM A, V13, P1532, DOI 10.1364/JOSAA.13.001532
   Weiss Y., 2007, IEEE C COMP VIS PATT, P1
   Wiener N., 1964, Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yuan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360673
   Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x
NR 25
TC 7
Z9 7
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 131
EP 140
DI 10.1007/s00371-014-0920-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800004
DA 2024-07-18
ER

PT J
AU Shrivastava, N
   Tyagi, V
AF Shrivastava, Nishant
   Tyagi, Vipin
TI An effective scheme for image texture classification based on binary
   local structure pattern
SO VISUAL COMPUTER
LA English
DT Article
DE Local binary pattern; Texture classification; Rotation invariance;
   Information retrieval
ID INVARIANT; ROTATION; SCALE
AB Effectiveness of local binary pattern (LBP) features is well proven in the field of texture image classification and retrieval. This paper presents a more effective completed modeling of the LBP. The traditional LBP has a shortcoming that sometimes it may represent different structural patterns with same LBP code. In addition, LBP also lacks global information and is sensitive to noise. In this paper, the binary patterns generated using threshold as a summation of center pixel value and average local differences are proposed. The proposed local structure patterns (LSP) can more accurately classify different textural structures as they utilize both local and global information. The LSP can be combined with a simple LBP and center pixel pattern to give a completed local structure pattern (CLSP) to achieve higher classification accuracy. In order to make CLSP insensitive to noise, a robust local structure pattern (RLSP) is also proposed. The proposed scheme is tested over three representative texture databases viz. Outex, Curet, and UIUC. The experimental results indicate that the proposed method can achieve higher classification accuracy while being more robust to noise.
C1 [Shrivastava, Nishant; Tyagi, Vipin] Jaypee Univ Engn & Technol, Dept Comp Sci & Engn, Guna 473226, MP, India.
   [Tyagi, Vipin] Indian Sci Congress Assoc, Engn Sci Sect, Kolkata, India.
C3 Department of Science & Technology (India); Indian Science Congress
   Association (ISCA)
RP Tyagi, V (corresponding author), Jaypee Univ Engn & Technol, Dept Comp Sci & Engn, Guna 473226, MP, India.
EM nishantuit@gmail.com; dr.vipin.tyagi@gmail.com
RI Shrivastava, Nishant/A-2784-2019; Tyagi, Vipin/I-2451-2013
OI Shrivastava, Nishant/0000-0001-9626-2301; Tyagi,
   Vipin/0000-0003-4994-3686
CR Chen J.L., 1992, IC 92 1992 INT C AC, V1-5, pC69
   COHEN FS, 1991, IEEE T PATTERN ANAL, V13, P192, DOI 10.1109/34.67648
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   DAVIS LS, 1981, PATTERN RECOGN, V13, P219, DOI 10.1016/0031-3203(81)90098-4
   DUVERNOY J, 1984, APPL OPTICS, V23, P828, DOI 10.1364/AO.23.000828
   EICHMANN G, 1988, COMPUT VISION GRAPH, V41, P267, DOI 10.1016/0734-189X(88)90102-8
   Goyal RK, 1995, IEEE IND ELEC, P1290, DOI 10.1109/IECON.1995.483983
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Heikkilä M, 2006, LECT NOTES COMPUT SC, V4338, P58
   KASHYAP RL, 1986, IEEE T PATTERN ANAL, V8, P472, DOI 10.1109/TPAMI.1986.4767811
   Khellah F., 2011, IEEE T IMAGE PROCESS, V19
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Martens G, 2010, VISUAL COMPUT, V26, P915, DOI 10.1007/s00371-010-0455-9
   Murula S., 2012, IEEE T IMAGE PROCESS, V21
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Porter R, 1997, INT CONF ACOUST SPEE, P3157, DOI 10.1109/ICASSP.1997.595462
   Sahami S, 2013, VISUAL COMPUT, V29, P1245, DOI 10.1007/s00371-012-0766-0
   Subrahmanyam M, 2012, SIGNAL PROCESS, V92, P1467, DOI 10.1016/j.sigpro.2011.12.005
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4
   Varma M, 2003, PROC CVPR IEEE, P691
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Xu Y, 2010, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2010.5540217
   Xu Y, 2009, INT J COMPUT VISION, V83, P85, DOI 10.1007/s11263-009-0220-6
   Xu Yong, 2006, 2006 IEEE COMPUTER S, V2, P1932
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang JG, 2002, PATTERN RECOGN, V35, P735, DOI 10.1016/S0031-3203(01)00074-7
   Zhao Y, 2013, NEUROCOMPUTING, V106, P68, DOI 10.1016/j.neucom.2012.10.017
NR 30
TC 29
Z9 31
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1223
EP 1232
DI 10.1007/s00371-013-0887-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300003
DA 2024-07-18
ER

PT J
AU Wang, YK
   Zhong, F
   Peng, QS
   Qin, XY
AF Wang, Yanke
   Zhong, Fan
   Peng, Qunsheng
   Qin, Xueying
TI Depth map enhancement based on color and depth consistency
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Depth map; Enhancement; Markov random field; Kinect
ID IMAGE; MODELS
AB Current low-cost depth sensing techniques, such as Microsoft Kinect, still can achieve only limited precision. The resultant depth maps are often found to be noisy, misaligned with the color images, and even contain many large holes. These limitations make it difficult to be adopted by many graphics applications. In this paper, we propose a computational approach to address the problem. By fusing raw depth values with image color, edges and smooth priors in a Markov random field optimization framework, both misalignment and large holes can be eliminated effectively, our method thus can produce high-quality depth maps that are consistent with the color image. To achieve this, a confidence map is estimated for adaptive weighting of different cues, an image inpainting technique is introduced to handle large holes, and contrasts in the color image are also considered for an accurate alignment. Experimental results demonstrate the effectiveness of our method.
C1 [Wang, Yanke; Zhong, Fan; Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Qin, Xueying] Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
C3 Shandong University; Zhejiang University
RP Qin, XY (corresponding author), Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
EM qxy@sdu.edu.cn
RI Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
FU 973 program of China [2009CB320802]; NSF of China [U1035004, 61173070,
   61202149]; National Science and Technology Pillar Program
   [2012BAF10B03-3]
FX The authors gratefully acknowledge the anonymous reviewers for their
   comments to help us to improve our paper. This work is supported by 973
   program of China (No. 2009CB320802), NSF of China (Nos. U1035004,
   61173070, 61202149), the National Science and Technology Pillar Program
   (No. 2012BAF10B03-3).
CR [Anonymous], EUR C COMP VIS FLOR
   [Anonymous], NVIDIA CUD C PROGR G
   [Anonymous], DEPTH MAPPING USING
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bornemann F, 2007, J MATH IMAGING VIS, V28, P259, DOI 10.1007/s10851-007-0017-6
   Breckon TP, 2012, PATTERN RECOGN, V45, P172, DOI 10.1016/j.patcog.2011.04.021
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cui Y, 2010, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2010.5540082
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Diebel J., 2005, P 18 INT C NEUR INF, V18, P291
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Huhle B, 2010, COMPUT VIS IMAGE UND, V114, P1336, DOI 10.1016/j.cviu.2009.11.004
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Nalpantidis L, 2008, INT J OPTOMECHATRONI, V2, P435, DOI 10.1080/15599610802438680
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Richardt C, 2012, COMPUT GRAPH FORUM, V31, P247, DOI 10.1111/j.1467-8659.2012.03003.x
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Strasdat H, 2010, IEEE INT CONF ROBOT, P2657, DOI 10.1109/ROBOT.2010.5509636
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Yang Q., 2007, IEEE C COMPUTER VISI
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 29
TC 16
Z9 16
U1 0
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1157
EP 1168
DI 10.1007/s00371-013-0896-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800009
DA 2024-07-18
ER

PT J
AU Wang, SC
   Luo, SW
   Huang, YP
   Zheng, JY
   Dai, P
   Han, Q
AF Wang, Shengchun
   Luo, Siwei
   Huang, Yaping
   Zheng, Jiang Yu
   Dai, Peng
   Han, Qiang
TI Railroad online: acquiring and visualizing route panoramas of rail
   scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Route panorama; Video visualization; Forward motion video; Railway
   safety
ID REPRESENTATION; MOSAICS
AB A patrol type of surveillance has been performed everywhere from police city patrol to railway inspection. Different from static cameras or sensors distributed in a space, such surveillance has its benefits of low cost, long distance, and efficiency in detecting infrequent changes. However, the challenges are how to archive daily recorded videos in the limited storage space and how to build a visual representation for quick and convenient access to the archived videos. We tackle the problems by acquiring and visualizing route panoramas of rail scenes. We analyze the relation between train motion and the video sampling and the constraints such as resolution, motion blur and stationary blur etc. to obtain a desirable panoramic image. The route panorama generated is a continuous image with complete and non-redundant scene coverage and compact data size, which can be easily streamed over the network for fast access, maneuver, and automatic retrieval in railway environment monitoring. Then, we visualize the railway scene based on the route panorama rendering for interactive navigation, inspection, and scene indexing.
C1 [Wang, Shengchun; Luo, Siwei; Huang, Yaping] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
   [Zheng, Jiang Yu] Indiana Univ Purdue Univ, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA.
   [Dai, Peng; Han, Qiang] China Acad Railway Sci, Infrastruct Inspect Res Inst, Beijing, Peoples R China.
C3 Beijing Jiaotong University; Indiana University System; Indiana
   University Indianapolis
RP Huang, YP (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
EM yphuang@bjtu.edu.cn; jzheng@cs.iupui.edu; daipeng_iic@qq.com
FU National Nature Science Foundation of China [61272354, 61273364,
   61105119]; Fundamental Research Funds for the Central Universities
   [2012JBM039, 2011JBZ005]
FX This work is supported by National Nature Science Foundation of China
   (61272354, 61273364,61105119) and Fundamental Research Funds for the
   Central Universities (2012JBM039, 2011JBZ005).
CR Alippi C, 2000, IEEE T INSTRUM MEAS, V49, P559, DOI 10.1109/19.850395
   Borgo R, 2012, COMPUT GRAPH FORUM, V31, P2450, DOI 10.1111/j.1467-8659.2012.03158.x
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446
   Kopf J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778833
   Li QY, 2012, IEEE T SYST MAN CY C, V42, P1531, DOI 10.1109/TSMCC.2012.2198814
   Lin J, 2009, PROC IEEE INT SYMP, P764
   Micusik Branislav, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2906, DOI 10.1109/CVPRW.2009.5206535
   Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346
   Peleg S, 2000, IEEE T PATTERN ANAL, V22, P1144, DOI 10.1109/34.879794
   Román A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50
   Rubaai A, 2003, IEEE T IND APPL, V39, P374, DOI 10.1109/TIA.2003.809443
   Zheng JY, 2008, INT J COMPUT VISION, V78, P169, DOI 10.1007/s11263-007-0080-x
   Zheng JY, 2006, IEEE T VIS COMPUT GR, V12, P155, DOI 10.1109/TVCG.2006.37
   ZHENG JY, 1992, INT J COMPUT VISION, V9, P55
   Zheng JY, 2003, IEEE MULTIMEDIA, V10, P57, DOI 10.1109/MMUL.2003.1218257
   Zheng JY, 1998, COMPUT VIS IMAGE UND, V72, P237, DOI 10.1006/cviu.1998.0678
   Zhu ZG, 2004, IEEE T PATTERN ANAL, V26, P226, DOI 10.1109/TPAMI.2004.1262190
   Zomet A, 2003, IEEE T PATTERN ANAL, V25, P741, DOI 10.1109/TPAMI.2003.1201823
NR 19
TC 5
Z9 10
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 1045
EP 1057
DI 10.1007/s00371-013-0911-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200008
DA 2024-07-18
ER

PT J
AU Lii, SY
   Wong, SK
AF Lii, Shing-Yeu
   Wong, Sai-Keung
TI Ice melting simulation with water flow handling
SO VISUAL COMPUTER
LA English
DT Article
DE Ice melting; Marching cubes; Metaballs
AB In this paper, we propose a new approach based on a particle-based model for ice melting simulation. Each particle has an attribute called virtual water. The amount of the virtual water of an ice particle indicates the amount of water surrounding the ice particle. The transfer of the virtual water is performed between the exterior ice particles so as to simulate the thin layer of water flow on the surface. Our approach also handles the transition between the virtual water and the water particles. We compute the isosurface of a density field defined by the ice particles and the virtual water. A simple ray tracing method is adopted for rendering the objects. We report the experimental results of several ice melting simulations with water flow and water drops.
C1 [Lii, Shing-Yeu; Wong, Sai-Keung] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Wong, SK (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM f030258631@hotmail.com; cswingo@cs.nctu.edu.tw
FU National Science Council of ROC (Taiwan) [NSC-101-2221-E-009-157]
FX This work was supported by the National Science Council of ROC (Taiwan)
   (NSC-101-2221-E-009-157).
CR Chen S, 1997, J COMPUT PHYS, V135, P8, DOI 10.1006/jcph.1997.5721
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Fujisawa M, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P249
   Fujishiro I, 2001, SPRING EUROGRAP, P69
   Génevaux O, 2003, PROC GRAPH INTERF, P31
   Iwasaki K, 2010, COMPUT GRAPH FORUM, V29, P2215, DOI 10.1111/j.1467-8659.2010.01810.x
   Kanamori Y, 2008, COMPUT GRAPH FORUM, V27, P351, DOI 10.1111/j.1467-8659.2008.01132.x
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Losasso F, 2006, IEEE T VIS COMPUT GR, V12, P343, DOI 10.1109/TVCG.2006.51
   Madrazo C., 2009, The Journal of the Society for Art and Science, V8, P35
   NISHITA T, 1994, COMPUT GRAPH FORUM, V13, pC271, DOI 10.1111/1467-8659.1330271
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Terzopoulos D., 1991, Journal of Visualization and Computer Animation, V2, P68, DOI 10.1002/vis.4340020208
   Wojtan Christopher., 2007, Eurographics Workshop on Natural Phenomena, P15, DOI DOI 10.2312/NPH/NPH07/015-022
   Zhao Y, 2006, COMPUT GRAPH-UK, V30, P519, DOI 10.1016/j.cag.2006.03.009
NR 17
TC 15
Z9 16
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 531
EP 538
DI 10.1007/s00371-013-0878-1
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100006
DA 2024-07-18
ER

PT J
AU Jung, Y
   Kim, J
   Eberl, S
   Fulham, M
   Feng, DD
AF Jung, Younhyun
   Kim, Jinman
   Eberl, Stefan
   Fulham, Micheal
   Feng, David Dagan
TI Visibility-driven PET-CT visualisation with region of interest (ROI)
   segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Multi-modality volume rendering; Visibility histogram; Transfer
   function; PET-CT imaging; Image segmentation
AB Multi-modality (MM) positron emission tomography-computed tomography (PET-CT) visualises biological and physiological functions (from PET) as region of interests (ROIs) within a higher resolution anatomical reference frame (from CT). The need to efficiently assess and assimilate the information from these co-aligned volumes simultaneously has stimulated new visualisation techniques that combine 3D volume rendering with interactive transfer functions to enable efficient manipulation of these volumes. However, in typical MM volume rendering visualisation, the transfer functions for the volumes are manipulated in isolation with the resulting volumes being fused, thus failing to exploit the spatial correlation that exists between the aligned volumes. Such lack of feedback makes MM transfer function manipulation complex and time consuming. Further, transfer function alone is often insufficient to select the ROIs when they have similar voxel properties to those of non-relevant regions.
   In this study, we propose a new ROI-based MM visibility-driven transfer function (m (2)-vtf) for PET-CT visualisation. We present a novel 'visibility' metric, a fundamental optical property that represents how much of the ROIs are visible to the users, and use it to measure the visibility of the ROIs in PET in relation to how it is affected by transfer function manipulations to its counterpart CT. To overcome the difficulty in ROI selection, we provide an intuitive ROI selection tool based on automated PET segmentation. We further present a MM transfer function automation where the visibility metrics from the PET ROIs are used to automate its CT's transfer function. Our GPU implementation achieved an interactive visualisation of PET-CT with efficient and intuitive transfer function manipulations.
C1 [Jung, Younhyun; Kim, Jinman; Eberl, Stefan; Fulham, Micheal; Feng, David Dagan] Univ Sydney, Biomed & Multimedia Informat Technol BMIT Res Grp, Sydney, NSW 2006, Australia.
   [Eberl, Stefan; Fulham, Micheal] Royal Prince Alfred Hosp, Dept Mol Imaging, Sydney, NSW, Australia.
   [Fulham, Micheal] Univ Sydney, Sydney Med Sch, Sydney, NSW 2006, Australia.
   [Feng, David Dagan] Shanghai Jiao Tong Univ, Med X Res Inst, Shanghai 200030, Peoples R China.
C3 University of Sydney; University of Sydney; NSW Health; Royal Prince
   Alfred Hospital; University of Sydney; Shanghai Jiao Tong University
RP Kim, J (corresponding author), Univ Sydney, Biomed & Multimedia Informat Technol BMIT Res Grp, Sydney, NSW 2006, Australia.
EM jinman.kim@sydney.edu.au
RI Kim, Jin/AAS-5810-2021; Kim, Jin Man/HJO-8987-2023
OI Kim, Jin/0000-0002-7667-9588; Feng, Dagan/0000-0002-3381-214X; Fulham,
   Michael/0000-0003-0602-6319; kim, jinman/0000-0001-5960-1060; Jung,
   Younhyun/0000-0003-0552-2281
FU Australian Research Council (ARC)
FX We would like to thank our collaborators at the Royal Prince Alfred
   (RPA) Hospital. This research was funded by Australian Research Council
   (ARC) grants.
CR [Anonymous], P VIS COMP BIOL MED
   Barton RR, 1996, MANAGE SCI, V42, P954, DOI 10.1287/mnsc.42.7.954
   Bi L, 2012, IEEE ENG MED BIO, P5335, DOI 10.1109/EMBC.2012.6347199
   Bordoloi UD, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P487
   Bramon R, 2012, IEEE T VIS COMPUT GR, V18, P1574, DOI 10.1109/TVCG.2011.280
   Caban JJ, 2008, IEEE T VIS COMPUT GR, V14, P1364, DOI 10.1109/TVCG.2008.169
   Cai WL, 1999, COMPUT GRAPH FORUM, V18, pC359, DOI 10.1111/1467-8659.00356
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Correa CD, 2009, IEEE T VIS COMPUT GR, V15, P1465, DOI 10.1109/TVCG.2009.189
   Jung Y, 2012, IEEE ENG MED BIO, P2696, DOI 10.1109/EMBC.2012.6346520
   Kim J., 2002, Nuclear Science Symposium Conference Record, V3, P1580
   Kim J, 2007, IEEE T INF TECHNOL B, V11, P161, DOI 10.1109/TITB.2006.875669
   Kim J, 2007, COMPUT SCI ENG, V9, P20, DOI 10.1109/MCSE.2007.22
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8
   Manousopoulos P, 2009, EUR J OPER RES, V192, P594, DOI 10.1016/j.ejor.2007.09.017
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Preim B., 2007, M KAUFMANN SERIES CO
   Rosset A, 2006, RADIOGRAPHICS, V26, P299, DOI 10.1148/rg.261055066
   Ruiz M, 2011, IEEE T VIS COMPUT GR, V17, P1932, DOI 10.1109/TVCG.2011.173
   Tzeng F.-Y., 2004, S DATA VISUALISATION, P17, DOI DOI 10.2312/VISSYM/VISSYM04/017-024
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   *VOR, VOL REND ENG
   Wahl RL, 2009, J NUCL MED, V50, p122S, DOI 10.2967/jnumed.108.057307
   Wang YH, 2011, COMPUT GRAPH FORUM, V30, P2117, DOI 10.1111/j.1467-8659.2011.02045.x
NR 29
TC 16
Z9 17
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 805
EP 815
DI 10.1007/s00371-013-0833-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400033
DA 2024-07-18
ER

PT J
AU Tong, J
   Liu, LG
   Zhou, J
   Pan, ZG
AF Tong, Jing
   Liu, Ligang
   Zhou, Jin
   Pan, Zhigeng
TI Mona Lisa alive Create self-moving objects using hollow-face illusion
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Hollow-face illusion; Nonrealistic modeling; Geometric modeling; Human
   perception
ID BAS-RELIEF; PERCEPTION; IMAGES; SHAPE; MASK
AB This paper presents a novel approach for creating self-moving objects using hollow-face illusion. Given a clip of character animation, our approach generates a static object. Looking at the object at different views, a similar deformation can be observed. To accomplish this challenging mission, we give qualitative and quantitative analysis of hollow-face illusion. Methodology in computer vision and human perception are utilized to design the algorithm. A static object is first generated to satisfy the relative motion illusion constraints. The illusion is then strengthened by back projecting the object to the 3D face space. Considering both "bottom-up" visual signal and "top-down" knowledge, the intended illusion can be generated. Experiments have shown the effectiveness of our algorithm. For example, expression varying illusion on an oil painting can be created by our method. The self-moving objects can be used in applications such as design, entertainment, advertisement, and public safety.
C1 [Tong, Jing] Hohai Univ, Coll IOT Engn, Changzhou, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
   [Zhou, Jin] Hangzhou Dianzi Univ, Inst Appl Math & Engn Computat, Hangzhou, Zhejiang, Peoples R China.
   [Pan, Zhigeng] Hangzhou Normal Univ, Digital Media & HCI Res Ctr, Hangzhou, Zhejiang, Peoples R China.
C3 Hohai University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Hangzhou Dianzi University; Hangzhou Normal
   University
RP Pan, ZG (corresponding author), Hangzhou Normal Univ, Digital Media & HCI Res Ctr, Hangzhou, Zhejiang, Peoples R China.
EM tongjing.cn@gmail.com; lgliu@ustc.edu.cn; zhoujin10@gmail.com;
   zhigengpan@gmail.com
OI Pan, Zhi-geng/0000-0003-0717-5850
FU National Natural Science Foundation of China [61202284, 61170318,
   61070071, 61222206]; National Social Science Foundation of China
   [12AZD120]
FX We would like to thank the anonymous reviewers for their constructive
   comments. We thank Yu Shi and Linlin Xu for their help on the
   experiments. This work was supported jointly by the National Natural
   Science Foundation of China (61202284, 61170318, 61070071, 61222206) and
   National Social Science Foundation of China (12AZD120).
CR Alexa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778797
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Baran I, 2012, COMPUT GRAPH FORUM, V31, P603, DOI 10.1111/j.1467-8659.2012.03039.x
   Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611
   Bermano A, 2012, COMPUT GRAPH FORUM, V31, P593, DOI 10.1111/j.1467-8659.2012.03038.x
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bohrn I, 2010, PSYCHOL SCI, V21, P378, DOI 10.1177/0956797610362192
   Borgeat L, 2007, IEEE COMPUT GRAPH, V27, P60, DOI 10.1109/MCG.2007.162
   Chi MT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360661
   Dima D, 2009, NEUROIMAGE, V46, P1180, DOI 10.1016/j.neuroimage.2009.03.033
   Elber G., 2010, ADV ARCHITECTURAL GE, P175
   Gregory RL, 1997, PHILOS T ROY SOC B, V352, P1121, DOI 10.1098/rstb.1997.0095
   HILL H, 1994, PERCEPTION, V23, P1335, DOI 10.1068/p231335
   Kontsevich LL, 2004, VISION RES, V44, P1493, DOI 10.1016/j.visres.2003.11.027
   Króliczak G, 2006, BRAIN RES, V1080, P9, DOI 10.1016/j.brainres.2005.01.107
   Liu BX, 2004, VISION RES, V44, P2135, DOI 10.1016/j.visres.2004.03.024
   Livingstone MS, 2000, SCIENCE, V290, P1299, DOI 10.1126/science.290.5495.1299b
   Mitra NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618502
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   Oliva A, 2006, ACM T GRAPHIC, V25, P527, DOI 10.1145/1141911.1141919
   Papathomas T. V., 2008, ROLLING EYES HOLLOW
   Papathomas TV, 2008, SPATIAL VISION, V21, P79, DOI 10.1163/156856808782713852
   Papathomas TV, 2004, PERCEPTION, V33, P1129, DOI 10.1068/p5086
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Sela G, 2007, VISUAL COMPUT, V23, P219, DOI 10.1007/s00371-006-0095-2
   Song WH, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P211, DOI 10.1109/SMI.2007.9
   Weyrich T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239483
   Wu TP, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731051
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 30
TC 2
Z9 3
U1 0
U2 33
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 535
EP 544
DI 10.1007/s00371-013-0799-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400008
DA 2024-07-18
ER

PT J
AU Wang, X
   Yang, WW
   Peng, HY
   Wang, GZ
AF Wang, Xun
   Yang, Wenwu
   Peng, Haoyu
   Wang, Guozheng
TI Shape-aware skeletal deformation for 2D characters
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Shape deformation; Skeleton; 2D Character Animation; Shape matching
ID INTERPOLATION; MANIPULATION
AB This paper presents a skeleton-based method for deforming 2D characters. While previous skeleton-based methods drive the shape deformation by binding the skeleton to the shape, our method does so by propagating the skeleton transformations over the shape. In this way, the tedious process of weight selection in previous skeleton-based methods is not required. Also, the propagation allows us to consider the geometric characteristics of the shape such that local shape distortion can be effectively avoided. Experimental results demonstrate that our method allows real-time deformation and generates visually pleasing results.
C1 [Wang, Xun; Yang, Wenwu; Peng, Haoyu; Wang, Guozheng] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou, Peoples R China.
C3 Zhejiang Gongshang University
RP Yang, WW (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou, Peoples R China.
EM wwyang@zjgsu.edu.cn
FU Natural Science Foundation of China [61003189, 61170098]; National Basic
   Research Program of China [2009CB320801]; Natural Science Foundation of
   Zhejiang Province [LY12F02025, Z1101243]; Science and Technology Agency
   projects of Zhejiang Province [2012C33074, 2012R10041-16]; National High
   Technology Research and Development Program of China (863 Program)
   [2013AA013701]
FX We would like to thank the anonymous reviewers for their helpful
   comments. This research was partially funded by the Natural Science
   Foundation of China (Nos. 61003189, 61170098), the National Basic
   Research Program of China (No. 2009CB320801), the Natural Science
   Foundation of Zhejiang Province (Nos. LY12F02025, Z1101243), the Science
   and Technology Agency projects of Zhejiang Province (Nos. 2012C33074,
   2012R10041-16), and the National High Technology Research and
   Development Program of China (863 Program, No. 2013AA013701).
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   [Anonymous], ACM T GRAPH
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Botsch M, 2007, COMPUT GRAPH FORUM, V26, P339, DOI 10.1111/j.1467-8659.2007.01056.x
   Bregler C, 2002, ACM T GRAPHIC, V21, P399, DOI 10.1145/566570.566595
   Craig J., 2005, Addison Wesley Series in Electrical and Computer Engineering: Control Engineering
   Davis TA, 2004, ACM T MATH SOFTWARE, V30, P196, DOI 10.1145/992200.992206
   Gain J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409629
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Jacobson A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024199
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Sederberg T. W., 1993, Computer Graphics Proceedings, P15, DOI 10.1145/166117.166118
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Shewchuk J. R., 1996, Applied Computational Geometry. Towards Geometric Engineering. FCRC'96 Workshop, WACG'96. Selected Papers, P203, DOI 10.1007/BFb0014497
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sykora Daniel, 2009, P 7 INT S NONPHOTORE, P25, DOI DOI 10.1145/1572614.1572619.3,7
   Weber O, 2007, COMPUT GRAPH FORUM, V26, P265, DOI 10.1111/j.1467-8659.2007.01048.x
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Xu K, 2009, COMPUT GRAPH-UK, V33, P391, DOI 10.1016/j.cag.2009.03.022
   Yan HB, 2008, IEEE T VIS COMPUT GR, V14, P693, DOI 10.1109/TVCG.2008.28
   Yang WW, 2008, VISUAL COMPUT, V24, P495, DOI 10.1007/s00371-008-0230-3
   Yang WW, 2012, COMPUT GRAPH FORUM, V31, P2249, DOI 10.1111/j.1467-8659.2012.03218.x
   Yang WW, 2009, COMPUT ANIMAT VIRT W, V20, P175, DOI 10.1002/cav.285
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
NR 30
TC 6
Z9 8
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 545
EP 553
DI 10.1007/s00371-013-0817-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400009
DA 2024-07-18
ER

PT J
AU Zhang, LX
   Song, HZ
   Ou, ZM
   Fu, Y
   Zhang, XL
AF Zhang, Lixia
   Song, Hongzhi
   Ou, Zhaoming
   Fu, Yi
   Zhang, Xiaolong (Luke)
TI Image retargeting with multifocus fisheye transformation
SO VISUAL COMPUTER
LA English
DT Article
DE Image retargeting; Multifocus conflict; Fisheye transformation
ID VISUAL-ATTENTION; ADAPTIVE IMAGE; MODEL
AB Image retargeting is a technique to adapt an original image to diverse screen sizes and aspect ratios on computing devices. This paper focuses on adapting large images for a small display. Some existing methods, such as scaling, cropping, and fisheye warping, are often flawed because they may lose the necessary details of the image, scarify content entirety, or fail to address the content continuity of images with multiple focuses. To address this issue, this paper presents a new fisheye-based approach to retarget images with multiple focuses. With fisheye-based image transformation, this approach can emphasize the focused areas of the image without completely discarding unfocused contents. With a multifocus conflict solution scheme, this approach offers a continuous content transition among multiple focused areas. With image retargeting algorithms implemented with this approach, we conducted experiments to study user preferences of retargeted images under different algorithms. The results of the experiments show that our approach is appropriate for image retargeting.
C1 [Zhang, Lixia; Song, Hongzhi; Ou, Zhaoming; Fu, Yi] South China Agr Univ, Coll Informat, Res Ctr Human Comp Interact, Guangzhou 510642, Guangdong, Peoples R China.
   [Zhang, Xiaolong (Luke)] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
C3 South China Agricultural University; Pennsylvania Commonwealth System of
   Higher Education (PCSHE); Pennsylvania State University; Pennsylvania
   State University - University Park
RP Song, HZ (corresponding author), South China Agr Univ, Coll Informat, Res Ctr Human Comp Interact, Guangzhou 510642, Guangdong, Peoples R China.
EM hz.song@scau.edu.cn
RI ZHANG, XIAOLONG/IZQ-4553-2023
OI Zhang, Xiaolong/0000-0002-6828-4930
FU National Natural Science Foundation of China [60875045]; Scientific
   Research Fund for the Returned Overseas Chinese Scholars, Ministry of
   Education of China [31]
FX This work was jointly supported by National Natural Science Foundation
   of China (No. 60875045), and the Scientific Research Fund for the
   Returned Overseas Chinese Scholars, Ministry of Education of China (2007
   No. 31).
CR [Anonymous], 2005, P 18 ANN ACM S US IN
   [Anonymous], P 2 INT C INT SCI
   [Anonymous], P 17 ACM INT C MULT
   [Anonymous], WORKSH COMP ATT APPL
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 2009, IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bederson B. B., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P217, DOI 10.1145/354401.354782
   Bederson B. B., 2004, ACM Transactions on Computer-Human Interaction, V11, P90, DOI 10.1145/972648.972652
   Brand M, 2009, LECT NOTES COMPUT SC, V5627, P33, DOI 10.1007/978-3-642-02611-9_4
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Cho T. S., 2008, IEEE C COMP VIS PATT
   Ciocca G, 2007, IEEE T CONSUM ELECTR, V53, P1622, DOI 10.1109/TCE.2007.4429261
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Furnas G W, 1982, TECHNICAL REPORT
   Furnas G. W., 1986, ACM Sigchi Bull., V17, P16, DOI DOI 10.1145/22339.22342
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Golub E, 2007, PHOTOCROPR 1 STEP CO
   Guo C. L., 2008, IEEE C COMP VIS PATT
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   [黄志勇 HUANG ZhiYong], 2011, [中国科学. 信息科学, Scientia Sinica Informationis], V41, P863
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin Y., 2010, VISUAL COMPUT, V26, P6
   Kim J.-H, 2009, EUR SIGN PROC C
   Kim JS, 2009, PROC CVPR IEEE, P1730, DOI 10.1109/CVPRW.2009.5206666
   Kwon TM, 1998, MULTIDIM SYST SIGN P, V9, P93, DOI 10.1023/A:1008261806662
   Liu H., 2004, IMIR, P84
   Liu Z., 2010, OPT ENG, V49
   Lixia Zhang, 2010, Proceedings of the 2010 International Conference on Information and Automation (ICIA 2010), P1840, DOI 10.1109/ICINFA.2010.5512242
   Ma M, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P710, DOI 10.1109/CCNC.2004.1286964
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   RAO R, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P318, DOI 10.1145/191666.191776
   Ren TW, 2009, IEEE INT CON MULTI, P406, DOI 10.1109/ICME.2009.5202520
   Ren TW, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1397, DOI 10.1109/ICME.2008.4607705
   Robertson G. G., 1993, Sixth Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/168642.168652
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang J, 2006, LECT NOTES COMPUT SC, V4035, P385
   Wang SF, 2009, INT CONF ACOUST SPEE, P1049, DOI 10.1109/ICASSP.2009.4959767
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   WIKIPEDIA, MAXOSXDOCK
   Yang C. C., 1999, Digital 99 Libraries. Fourth ACM Conference on Digital Libraries, P258, DOI 10.1145/313238.313444
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang M., 2005, IEEE INT C MULT EXP
NR 51
TC 4
Z9 7
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 407
EP 420
DI 10.1007/s00371-012-0744-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200008
DA 2024-07-18
ER

PT J
AU Cao, LJ
   Men, CG
   Ji, RR
AF Cao, Liujuan
   Men, Chaoguang
   Ji, Rongrong
TI Nonlinear scrambling-based reversible watermarking for 2D-vector maps
SO VISUAL COMPUTER
LA English
DT Article
DE Reversible watermarking; Nonlinear scrambling; Copyright protection;
   Security; Vector map
AB The reversible watermarking technique is suitable for vector maps due to its reversibility after watermark extraction. In this paper, a novel reversible watermarking scheme based on the idea of nonlinear scrambling is proposed. It begins with feature point extraction. To avoid the high-precision vector data being illegally used by unauthorized users, the algorithm nonlinearly scrambles the relative position of feature points. Then based on the proposed reversible embedding, both scrambled feature points and non-feature points are taken as cover data, the coordinates of which are modified to embed both watermark data and feature point identification data. Finally, combined with the scrambling secret key, the original vector data can be exactly recovered with watermark extraction. Comprehensive experimental results validate that the scheme could effectively prevent the high-precision vector data from being illegally used with maintaining the basic shape of each polyline, simultaneously.
C1 [Cao, Liujuan; Men, Chaoguang] Harbin Engn Univ, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Ji, Rongrong] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Harbin Engineering University; Columbia University
RP Cao, LJ (corresponding author), Harbin Engn Univ, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM caoliujuan@hrbeu.edu.cn; menchaoguang@hrbeu.edu.cn; rrji@ee.columbia.edu
CR Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Feng J.B., 2006, IJ Network Security, V2, P161
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   Niu XM, 2006, INT J INNOV COMPUT I, V2, P1301
   [邵承永 SHAO Chengyong], 2007, [中国图象图形学报, Journal of Image and Graphics], V12, P206
   Tian J., 2002, Proceedings of workshop on multimedia and security p, P19
   van Leest A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P731
   Voigt M, 2005, P SOC PHOTO-OPT INS, V5681, P409, DOI 10.1117/12.588195
   Voigt M, 2004, P 2004 MULT SEC WORK, P160, DOI DOI 10.1145/1022431.1022459
   Wang X, 2007, IEEE T INF FOREN SEC, V2, P311, DOI 10.1109/TIFS.2007.902677
   Wu HT, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P801
   Xuan GR, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P312
   Yang B, 2004, PROC SPIE, V5306, P405, DOI 10.1117/12.527216
NR 14
TC 24
Z9 27
U1 1
U2 32
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2013
VL 29
IS 3
BP 231
EP 237
DI 10.1007/s00371-012-0732-x
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AI
UT WOS:000316783800006
DA 2024-07-18
ER

PT J
AU Shim, H
   Adelsberger, R
   Kim, JD
   Rhee, SM
   Rhee, T
   Sim, JY
   Gross, M
   Kim, C
AF Shim, Hyunjung
   Adelsberger, Rolf
   Kim, James Dokyoon
   Rhee, Seon-Min
   Rhee, Taehyun
   Sim, Jae-Young
   Gross, Markus
   Kim, Changyeong
TI Time-of-flight sensor and color camera calibration for multi-view
   acquisition
SO VISUAL COMPUTER
LA English
DT Article
DE Depth sensing; Multi-modal sensor fusion; Multi-view acquisition; 3D
   video processing
AB This paper presents a multi-view acquisition system using multi-modal sensors, composed of time-of-flight (ToF) range sensors and color cameras. Our system captures the multiple pairs of color images and depth maps at multiple viewing directions. In order to ensure the acceptable accuracy of measurements, we compensate errors in sensor measurement and calibrate multi-modal devices. Upon manifold experiments and extensive analysis, we identify the major sources of systematic error in sensor measurement and construct an error model for compensation. As a result, we provide a practical solution for the real-time error compensation of depth measurement. Moreover, we implement the calibration scheme for multi-modal devices, unifying the spatial coordinate for multi-modal sensors.
   The main contribution of this work is to present the thorough analysis of systematic error in sensor measurement and therefore provide a reliable methodology for robust error compensation. The proposed system offers a real-time multi-modal sensor calibration method and thereby is applicable for the 3D reconstruction of dynamic scenes.
C1 [Shim, Hyunjung; Kim, James Dokyoon; Rhee, Seon-Min; Rhee, Taehyun; Kim, Changyeong] Samsung Elect, Samsung Adv Inst Technol, Giheung, South Korea.
   [Adelsberger, Rolf; Gross, Markus] ETH, Zurich, Switzerland.
   [Sim, Jae-Young] Ulsan Natl Inst Sci & Technol, Ulsan, South Korea.
C3 Samsung; Swiss Federal Institutes of Technology Domain; ETH Zurich;
   Ulsan National Institute of Science & Technology (UNIST)
RP Shim, H (corresponding author), Samsung Elect, Samsung Adv Inst Technol, Giheung, South Korea.
EM kateshim@gmail.com
RI Shim, Hyunjung/AAS-3610-2021
OI Gross, Markus/0009-0003-9324-779X
CR BATLLE J, 1998, PATTERN RECOGN, P1, DOI DOI 10.1109/ROBOT.1197.620027
   Bouguet Y.-Y., CAMERA CALIBRATION T
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FRYER JG, 1986, PHOTOGRAMM ENG REM S, V52, P51
   Fuchs S., 2007, P DAGM DYN3D WORKSH
   Fuchs S., 2008, COMPUTER VISION PATT, P1, DOI DOI 10.1109/CVPR.2008.4587828
   GUAN L, 2008, 3DPVT
   Guan L., 2008, P EUR C COMP VIS
   Gut O., THESIS
   Kahlmann T., 2005, C VID 8 PART IS T SP
   Kahlmann T., 2007, OPTICS E
   Kim Y.M., 2008, P COMP VIS PATT REC
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Payne A., 2008, SPIE NEWSROOM
   Scharstein D, 1996, PROC CVPR IEEE, P852, DOI 10.1109/CVPR.1996.517171
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 19
TC 16
Z9 21
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2012
VL 28
IS 12
BP 1139
EP 1151
DI 10.1007/s00371-011-0664-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 029ZZ
UT WOS:000310538700001
DA 2024-07-18
ER

PT J
AU Wuhrer, S
   Shu, C
   Xi, PC
AF Wuhrer, Stefanie
   Shu, Chang
   Xi, Pengcheng
TI Landmark-free posture invariant human shape correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Shape correspondence; Template fitting
ID REGISTRATION
AB We consider the problem of computing accurate point-to-point correspondences among a set of human bodies in varying postures using a landmark-free approach. The approach learns the locations of the anthropometric landmarks present in a database of human models in strongly varying postures and uses this knowledge to automatically predict the locations of these anthropometric landmarks on a newly available scan. The predicted landmarks are then used to compute point-to-point correspondences between a rigged template model and the newly available scan.
C1 [Wuhrer, Stefanie; Shu, Chang; Xi, Pengcheng] Natl Res Council Canada, Ottawa, ON, Canada.
   [Shu, Chang] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
C3 National Research Council Canada; Carleton University
RP Wuhrer, S (corresponding author), Natl Res Council Canada, Ottawa, ON, Canada.
EM stefanie.wuhrer@nrc-cnrc.gc.ca; chang.shu@nrc-cnrc.gc.ca;
   pengcheng.xi@nrc-cnrc.gc.ca
RI Xi, Pengcheng/Q-4808-2018; Xi, Pengcheng/N-9404-2019
OI Xi, Pengcheng/0000-0003-3236-5234
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen Brett., 2006, Proc. of the ACM SIGGRAPH/Eurographics symposium on Computer animation, P147
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2009, IEEE C COMP VIS PATT
   Arya Sunil., 1993, P 4 ANN ACM SIAM S D, P271
   Azouz Z.B., 2006, 3D DATA PROCESSING V
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Chang W, 2009, COMPUT GRAPH FORUM, V28, P447, DOI 10.1111/j.1467-8659.2009.01384.x
   Chang W, 2008, COMPUT GRAPH FORUM, V27, P1459, DOI 10.1111/j.1467-8659.2008.01286.x
   Dryden I., 2002, Statistical Shape Analysis
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   HASLER N, 2009, COMPUT GRAPH FORUM, V2
   HUANG Q, 2008, COMPUT GRAPH FORUM, V27
   Jain Varun., 2007, INT J SHAPE MODELING, V13, P101, DOI DOI 10.1142/S0218654307000968
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Pauly M., 2005, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P23
   Robinette K., 1999, 3 D DIGITAL IMAGING, P180
   Rusinkiewicz S., 2001, C 3D DIG IM MOD JUN
   VANKAICK O, 2010, EUROGRAPHICS STATE A
   Wuhrer S., 2010, CAN C COMP ROB VIS
   ZHANG H, 2008, COMPUT GRAPH FORUM, V27
NR 27
TC 17
Z9 18
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2011
VL 27
IS 9
BP 843
EP 852
DI 10.1007/s00371-011-0557-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 807TK
UT WOS:000293922400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rosen, P
   Popescu, V
AF Rosen, Paul
   Popescu, Voicu
TI An evaluation of 3-D scene exploration using a multiperspective image
   framework
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Multiperspective images; Occlusions; Navigation; Interactive 3-D scene
   exploration; Visual interfaces; User study
ID VISUALIZATION; TRANSPARENCY
AB Multiperspective images (MPIs) show more than what is visible from a single viewpoint and are a promising approach for alleviating the problem of occlusions. We present a comprehensive user study that investigates the effectiveness of MPIs for 3-D scene exploration. A total of 47 subjects performed searching, counting, and spatial orientation tasks using both conventional and multiperspective images. We use a flexible MPI framework that allows trading off disocclusion power for image simplicity. The framework also allows rendering MPI images at interactive rates, which enables investigating interactive navigation and dynamic 3-D scenes. The results of our experiments show that MPIs can greatly outperform conventional images. For searching, subjects performed on average 28% faster using an MPI. For counting, accuracy was on average 91% using MPIs as compared to 42% for conventional images.
C1 [Rosen, Paul] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Popescu, Voicu] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 Utah System of Higher Education; University of Utah; Purdue University
   System; Purdue University
RP Rosen, P (corresponding author), Univ Utah, Sci Comp & Imaging Inst, 72 S Cent Campus Dr,Room 3750, Salt Lake City, UT 84112 USA.
EM prosen@sci.utah.edu; popescu@utah.edu
RI Rosen, Paul/GXM-8609-2022; Rosen, Paul/AAN-1370-2021
OI Rosen, Paul/0000-0002-0873-9518; Rosen, Paul/0000-0002-0873-9518
CR Agrawala M, 2000, SPRING COMP SCI, P125
   [Anonymous], RENDERING TECHNIQUES
   [Anonymous], SIGGRAPH 97
   [Anonymous], ECCV 04
   [Anonymous], SIGGRAPH 07
   [Anonymous], P EUR 2005 COMP GRAP
   [Anonymous], SCA 02
   [Anonymous], SIGGRAPH 94
   [Anonymous], SIGGRAPH 97
   [Anonymous], SIGGRAPH 98
   [Anonymous], I3D 03
   [Anonymous], CVPRW 03
   [Anonymous], VRST 08
   [Anonymous], VIS 04
   [Anonymous], CHI 05
   [Anonymous], P EUR 2003 COMP GRAP
   [Anonymous], P 15 EUR WORKSH REND
   Bares W. H., 1999, IUI 99. 1999 International Conference on Intelligent User Interfaces, P119, DOI 10.1145/291080.291101
   Bavoil L., 2005, IEEE VISUALIZATION, P18
   Brosz J, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P33
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1077, DOI 10.1109/TVCG.2006.140
   Burns M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409107
   Carpendale MST, 1997, IEEE COMPUT GRAPH, V17, P42, DOI 10.1109/38.595268
   Coleman Patrick., 2004, NPAR 04, P129, DOI 1145/987657.987678
   Cui JA, 2010, IEEE T VIS COMPUT GR, V16, P1235, DOI 10.1109/TVCG.2010.127
   Darken R.P., 1996, P SIGCHI C HUMAN FAC, P142, DOI DOI 10.1145/238386.238459
   Degener P, 2008, IEEE T VIS COMPUT GR, V14, P1452, DOI 10.1109/TVCG.2008.124
   Diepstraten J, 2002, COMPUT GRAPH FORUM, V21, P317, DOI 10.1111/1467-8659.t01-1-00591
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Elmqvist N, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1769
   Fitzmaurice G, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P7
   Hachet M, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P83
   Hanson AJ, 1997, VISUALIZATION '97 - PROCEEDINGS, P175, DOI 10.1109/VISUAL.1997.663876
   Huang JB, 2005, IEEE T VIS COMPUT GR, V11, P584, DOI 10.1109/TVCG.2005.82
   Jankun-Kelly TJ, 2001, IEEE T VIS COMPUT GR, V7, P275, DOI 10.1109/2945.942695
   Khan Azam., 2005, I3D 05, P73
   Kreuseler M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P49, DOI 10.1109/INFVIS.2004.2
   McCrae James., 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games, I3D '09, P7, DOI [10.1145/1507149.1507151, DOI 10.1145/1507149.1507151, 10.1145/1507149.15071512, DOI 10.1145/1507149.15071512]
   Popescu V, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618504
   Qu HM, 2009, IEEE T VIS COMPUT GR, V15, P1547, DOI 10.1109/TVCG.2009.144
   Ray N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356683
   Roberts JC, 2000, PROC SPIE, V3960, P176, DOI 10.1117/12.378894
   Steed A., 1997, VRST'97. ACM Symposium on Virtual Reality Software and Technology 1997, P173, DOI 10.1145/261135.261167
   Tan D. S., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P418, DOI 10.1145/365024.365307
NR 44
TC 4
Z9 6
U1 2
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 623
EP 632
DI 10.1007/s00371-011-0599-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600021
PM 22661796
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Tychonievich, LA
   Jones, MD
AF Tychonievich, L. A.
   Jones, M. D.
TI Delaunay deformable mesh for the weathering and erosion of 3D terrain
SO VISUAL COMPUTER
LA English
DT Article
DE Terrain; Weathering; Geometry
AB Computer-generated erosion and weathering are important to convey setting and mood in computer generated images. Heightmap based landforms are good for distant scenes, but inadequate for scenes containing concave rock formations. Voxel based terrain editing algorithms do admit concave surfaces but do not scale. We introduce weathering on triangulated surface meshes, using a memory efficient modification of the Delaunay deformable model. This structure allows the freedom of an unorganized point cloud, the geometric information and visualization of a surface mesh, and the topological freedom of volumetric approaches-all while scaling linearly with surface complexity. We implement both spheroidal weathering and hydraulic erosion algorithms on this structure and demonstrate that the resulting terrain is visually plausible at modest computational cost.
C1 [Jones, M. D.] Brigham Young Univ, Provo, UT 84602 USA.
   [Tychonievich, L. A.] Univ Virginia, Charlottesville, VA USA.
C3 Brigham Young University; University of Virginia
RP Jones, MD (corresponding author), Brigham Young Univ, Provo, UT 84602 USA.
EM lat7h@virginia.edu; mike.jones@byu.edu
CR Alliez P, 2005, ACM T GRAPHIC, V24, P617, DOI 10.1145/1073204.1073238
   *AUT, 2008, AUT MAYA 2008
   Bargteil AW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239467
   Benes B, 2006, COMPUT ANIMAT VIRT W, V17, P99, DOI 10.1002/cav.77
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   CHEN Y, 2005, SIGGRAPH 05, P1127
   Choi S, 2002, SIAM PROC S, P135
   CUTLER B, 2002, SIGGRAPH 02, P302
   Devillers O., 2002, International Journal of Foundations of Computer Science, V13, P163, DOI 10.1142/S0129054102001035
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Ito T, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P244, DOI 10.1109/CGI.2003.1214475
   JOE B, 1989, SIAM J SCI STAT COMP, V10, P718, DOI 10.1137/0910044
   Jones MD, 2010, IEEE T VIS COMPUT GR, V16, P81, DOI 10.1109/TVCG.2009.39
   Kelley A. D., 1988, Computer Graphics, V22, P263, DOI 10.1145/378456.378519
   Lachaud J O, 1999, Med Image Anal, V3, P187, DOI 10.1016/S1361-8415(99)80012-7
   McInerney T, 1997, LECT NOTES COMPUT SC, V1205, P23, DOI 10.1007/BFb0029221
   Milligan M., 2003, Geology of Utah's Parks and Monuments, P421
   MULLEN P, 2007, SIGGRAPH 07, P66, DOI DOI 10.1145/1275808.1276459
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Nagashima K, 1997, VISUAL COMPUT, V13, P456, DOI 10.1007/s003710050117
   PONS JP, 2007, IEEE C COMP VIS PATT
   SIBSON R, 1978, COMPUT J, V21, P243, DOI 10.1093/comjnl/21.3.243
   STAVA O, 2008, EUR ACM S COMP AN EU
   *STICHT BLEND FDN, 2009, STICHT BLEND FDN BLE
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 26
TC 9
Z9 11
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2010
VL 26
IS 12
BP 1485
EP 1495
DI 10.1007/s00371-010-0506-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 678WA
UT WOS:000284112400005
DA 2024-07-18
ER

PT J
AU Ding, M
   Tong, RF
AF Ding, Meng
   Tong, Ruo-Feng
TI Content-aware copying and pasting in images
SO VISUAL COMPUTER
LA English
DT Article
DE Image pasting; Image cloning; Matting; Gradient domain; Laplace equation
AB We present a content-aware image copy-and-paste technique which combines ideas from both matting and gradient-based methods. We modify the diffusion process used in the gradient-based approach to use the alpha matte for the cloned area as a weight function to control intensity interpolation. This ensures that the color style of the significant parts of the selected region is preserved when pasting. We use a framework based on mean-value coordinates to implement our approach, allowing us to provide a parallel implementation suitable for use on a GPU. Experimental results show the advantages of our method.
C1 [Ding, Meng; Tong, Ruo-Feng] Zhejiang Univ, Dept Comp Sci & Engn, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Ding, M (corresponding author), Zhejiang Univ, Dept Comp Sci & Engn, Hangzhou 310027, Zhejiang, Peoples R China.
EM dmm@zju.edu.cn; trf@zju.edu.cn
FU National High-Tech Research and Development Program of China
   [2009AA01Z330]; National Basic Research Project of China [2006CB303106]
FX This work is supported by the National High-Tech Research and
   Development Program of China (Project Number 2009AA01Z330) and the
   National Basic Research Project of China (Project Number 2006CB303106).
CR *AD, 2003, PHOTOSHOP 7 0 US GUI
   [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chuang YY, 2001, PROC CVPR IEEE, P264
   FARBMAN Z, 2009, SIGGRAPH 09, P1
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   GEORGIEV T, 2004, WORKSH APPL COMP VIS, P1
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LEVIN A, 2006, IEEE COMP SOC C COMP
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Liu JY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531375
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793
   Sun Y, 2004, WIREL COMMUN MOB COM, V4, P315, DOI 10.1002/wcm.215
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Wang J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239460
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
NR 21
TC 39
Z9 44
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 721
EP 729
DI 10.1007/s00371-010-0448-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800032
DA 2024-07-18
ER

PT J
AU Amara, Y
   Marsault, X
AF Amara, Yacine
   Marsault, Xavier
TI A GPU Tile-Load-Map architecture for terrain rendering: theory and
   applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Shape Modeling and Applications
CY JUN 04-06, 2008
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH, EUROGRAPHICS, Comp Graph Soc
DE Terrain rendering; GPU architecture; Level of detail; Data
   amplification; Seed model
AB This paper describes a robust, modular, complete GPU architecture-the Tile-Load-Map (TLM)-designed for the real-time visualization of wide textured terrains created with arbitrary meshes. It extends and completes our previous succinct paper Amara et al. (ISVC 2007, Part 1, Lecture Notes in Computer Science, vol. 4841, pp. 586-597, Springer, Berlin, 2007) by giving further technical and implementation details. It provides new solutions to problems that had been left unresolved, in the context of a joint use of OpenGL and CUDA, optimized on the G80 graphics chip. We explain the crucial components of the shaders, and emphasize the progress we have proposed, while resolving some difficulties. We show that this texturing architecture is well suited to current challenges, and takes into account most of the distinctive aspects of terrain rendering. Finally, we demonstrate how the design of the TLM facilitates the integration of geomatic input-data into procedural selection/rendering tasks on the GPU, and immediate applications to amplification.
C1 [Marsault, Xavier] Ecole Architecture Lyon, CNRS, MAP ARIA, UMR 694, F-69512 Vaulx En Velin, France.
   [Amara, Yacine] USTHB, Algiers, Algeria.
C3 Centre National de la Recherche Scientifique (CNRS); University Science
   & Technology Houari Boumediene
RP Marsault, X (corresponding author), Ecole Architecture Lyon, CNRS, MAP ARIA, UMR 694, 3 Rue Maurice Audin, F-69512 Vaulx En Velin, France.
EM amara.yacine@gmail.com; xavier.marsault@aria.archi.fr
CR *AMAP, BOTANIQUE BIOINFORMA
   Amara Y, 2007, LECT NOTES COMPUT SC, V4841, P586
   [Anonymous], WORKSH GEN PURP PROC
   [Anonymous], CUDA OCCUPANCY CALCU
   [Anonymous], CUDA PROGRAMMING GUI
   ASIRVATHAM A, 2005, GPUGEMS2
   BALOGH A, 2003, THESIS U BUDAPEST
   BITTNER J, 2003, J ENV PLAN
   BLANCHET J, 2005, INT C APPL STOCH MOD
   BLANCHET J, 2007, MODELES MARKOVIENS E
   BOULANGER K, 2006, 1809 IR
   BRUNETON E, 2008, REAL TIME RENDERING
   DACHSBACHER C, 2003, ACM T GRAPH
   Dachsbacher C., 2006, INTERACTIVE TERRAIN
   DECAUDIN P, 2004, EUR S REND NORRK SWE
   DECORET X, 2003, P ACM SIGGR
   DEUSSEN O, 1998, COMPUTER GRAPHICS SI
   DEUSSEN O, 2002, P IEEE VIS C
   Döllner J, 2000, IEEE VISUAL, P227
   *EMG, 2001, EING PREM ATL VIV 3D
   FUHRMANN A, 2005, EUR WORKSH NAT PHEN
   GILET G, 2005, EUR WORKSH NAT PHEN
   Hwa LM, 2005, IEEE T VIS COMPUT GR, V11, P355, DOI 10.1109/TVCG.2005.65
   KRAUS M, 2002, GRAPHICS HARDWARE
   Lane B, 2002, PROC GRAPH INTERF, P69
   LEFEBVRE S, 2005, THESIS J FOURIER U G
   LEFEBVRE S, 2005, MICROSOFT RES SIGGRA
   LEFOHN AE, 2006, ACM T GRAPH
   LINDSTROM P, 2002, IEEE T VIS COMPUT GR
   LIVNY Y, 2008, VIS COMPUT, V12
   MEYER A, 1998, JOURNEES FRANCOPHONE, P261
   Pajarola R, 2007, VISUAL COMPUT, V23, P583, DOI [10.1007/s00371-007-0163-2, 10.1007/S00371-007-0163-2]
   PREMOZE S, 1999, GEOSPECIFIC RENDERIN
   *RGD73 74, REG GEST DONN DEUX S
   SCHNEIDER J, 2006, 11 WORKSH VIS MOD VI
   Schneider J, 2006, JOURNAL WSCG, V14, P49
   Seoane A, 2007, WSCG 2007, FULL PAPERS PROCEEDINGS I AND II, P177
   SMITH AR, 1984, P SIGGR
   TANNER CHRISTOPHERC., 1998, P SIGGRAPH 1998, P151
   TRIASSANZ R, 2005, 14 C IM AN FINL
   WELLS D, 2005, GENERATING ENHANCED
   WINZEN J, 2003, INTERACTIVE VISUALIZ
NR 42
TC 7
Z9 7
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2009
VL 25
IS 8
BP 805
EP 824
DI 10.1007/s00371-008-0305-1
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 465NI
UT WOS:000267593700007
DA 2024-07-18
ER

PT J
AU Terraz, O
   Guimberteau, G
   Mérillou, S
   Plemenos, D
   Ghazanfarpour, D
AF Terraz, Olivier
   Guimberteau, Guillaume
   Merillou, Stephane
   Plemenos, Dimitri
   Ghazanfarpour, Djamchid
TI 3Gmap L-systems: an application to the modelling of wood
SO VISUAL COMPUTER
LA English
DT Article
DE Geometric modelling; L-systems; Natural phenomena; Wood modelling and
   rendering
ID SIMULATION
AB In this paper an extension of L-systems is proposed, based on three-dimensional (3D) generalized maps that allow an easier control of the internal structure of 3D objects. A first and original application of this extension is also proposed: wood modelling by growth simulation. Numerous other applications of our work are possible, in the area of computer graphics, as well as in botanical science.
C1 [Terraz, Olivier; Guimberteau, Guillaume; Merillou, Stephane; Plemenos, Dimitri; Ghazanfarpour, Djamchid] Univ Limoges, CNRS, XLIM, UMR 6172, Limoges, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite de
   Limoges
RP Terraz, O (corresponding author), Univ Limoges, CNRS, XLIM, UMR 6172, Limoges, France.
EM olivier.terraz@xlim.fr
FU French National Agency for Research (ANR) [ANR-06-MDCA-004-01]
FX The authors would like to thank Prof. Pascal Lienhardt for helpful
   discussions and comments. This research is supported by the French
   National Agency for Research (ANR) under agreement ANR-06-MDCA-004-01.
CR [Anonymous], 1958, Information and Control, DOI [DOI 10.1016/S0019-9958(58)90082-2, 10.1016/S0019-9958(58)90082-2]
   BARRYLENGER A, 1999, TRANSFORMATION UTILI
   BERTRAND Y, 1993, P INT JOINT C CAAP F, P75
   Buchanan JW, 1998, COMPUT GRAPH FORUM, V17, pC105, DOI 10.1111/1467-8659.00258
   Cutler B, 2002, ACM T GRAPHIC, V21, P302
   de Reffye P., 1988, Computer Graphics, V22, P151, DOI 10.1145/378456.378505
   DEUSSEN O, 1998, P SIGGRAPH 98, P275, DOI DOI 10.1145/280814.280898
   Dischler JM, 1999, IEEE COMPUT GRAPH, V19, P66, DOI 10.1109/38.736470
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   DUFOURD JF, 1991, SIGGR S SOL MOD CAD, P61
   FRACCHIA FD, 1990, COMPUTER ANIMATION 90, P3
   FRANCON J, 1994, ARTIFICIAL LIFE AND VIRTUAL REALITY, P23
   Fuhrer M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P217, DOI 10.1109/PCCGA.2004.1348352
   GHAZANFARPOUR D, 1995, COMPUT GRAPH-UK, V19, P413, DOI 10.1016/0097-8493(95)00011-Z
   Ghazanfarpour D, 1996, COMPUT GRAPH FORUM, V15, pC311, DOI 10.1111/1467-8659.1530311
   GUIMBERTEAU G, 2006, TECH SCI INF TSI, V25, P735
   Hart JC, 2003, VISUAL COMPUT, V19, P151, DOI 10.1007/s00371-002-0189-4
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   JONES H, 2000, EUR 2000
   Lienhardt P, 1994, INT J COMPUT GEOM AP, V4, P275, DOI 10.1142/S0218195994000173
   Mech Radomir., 1996, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. SIGGRAPH'96, P397, DOI [10.1145/237170.237279, DOI 10.1145/237170.237279]
   NOSER H, 1993, 1 PAC C COMP GRAPH A, P133
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Peachey D. R., 1985, Computer Graphics, V19, P279, DOI 10.1145/325165.325246
   Perlin K., 1989, SIGGRAPH'89: Proceedings of the 16th annual conference on Computer graphics and interactive techniques', P253
   Prusinkiewicz P, 2001, COMP GRAPH, P289, DOI 10.1145/383259.383291
   Prusinkiewicz P, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P110
   Prusinkiewicz P, 1999, INT WORKSH AGTIVE 99, P395
   Prusinkiewicz P, 1997, PLANTS ECOSYSTEMS AD, P1
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   REEVES WT, 1985, P SIGGRAPH 85, P313
   Smith C., 2004, Proceedings of the 4th international workshop on functional-structural plant models, p, P365
   Smith J, 2001, COMPUT GRAPH FORUM, V20, P81, DOI 10.1111/1467-8659.t01-1-00202
   SPICHER A, 2005, 7 INT C ART EV, P189
   SPICHER A, 2004, LNCS
   Strnad D, 2004, COMPUT GRAPH FORUM, V23, P173, DOI 10.1111/j.1467-8659.2004.00751.x
   Szafran N, 2003, PLANT GROWTH MODELING AND APPLICATIONS, PROCEEDINGS, P372
   TERRAZ O, 1995, 6 EUR WORKSH AN SIM, P104
   VIGUE J, 2002, GRAND LIVRE BOIS
   YESSIOS C, 1979, SIGGRAPH COMPUT GRAP, V13, P190
NR 41
TC 8
Z9 11
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 165
EP 180
DI 10.1007/s00371-008-0212-5
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700006
DA 2024-07-18
ER

PT J
AU Chaudhuri, P
   Papagiannakis, G
   Magnenat-Thalmann, N
AF Chaudhuri, Parag
   Papagiannakis, George
   Magnenat-Thalmann, Nadia
TI Self adaptive animation based on user perspective
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE self adaptive character animation; animation blending; augmented and
   virtual reality
AB In this paper we present a new character animation technique in which the animation adapts itself based on the change in the user's perspective, so that when the user moves and their point of viewing the animation changes, then the character animation adapts itself in response to that change. The resulting animation, generated in real-time, is a blend of key animations provided a priori by the animator. The blending is done with the help of efficient dual-quaternion transformation blending. The user's point of view is tracked using either computer vision techniques or a simple user-controlled input modality, such as mouse-based input. This tracked point of view is then used to suitably select the blend of animations. We show a way to author and use such animations in both virtual as well as augmented reality scenarios and demonstrate that it significantly heightens the sense of presence for the users when they interact with such self adaptive animations of virtual characters.
C1 [Chaudhuri, Parag; Papagiannakis, George; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
C3 University of Geneva
RP Chaudhuri, P (corresponding author), Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
EM parag@miralab.unige.ch; papagiannakis@miralab.unige.ch;
   thalmann@miralab.unige.ch
RI papagiannakis, george/AAI-7973-2020; Thalmann, Nadia/AAK-5195-2021
OI papagiannakis, george/0000-0002-2977-9850; Thalmann,
   Nadia/0000-0002-1459-5960
CR Alexa M, 2002, ACM T GRAPHIC, V21, P380, DOI 10.1145/566570.566592
   [Anonymous], SIGGRAPH 85
   [Anonymous], 1968, Mathematical papers
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   BALCISOY S, 2001, P 2 INT S MIX REAL Y
   Barakonyi Istvan, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P145, DOI 10.1109/ISMAR.2006.297806
   BARAKONYI I, 2006, ACM SIGGRAPH 2006 SK
   Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581
   BUCK I, 2000, NPAR 2000, P101
   Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124
   Cavazza M, 2002, IEEE INTELL SYST, V17, P17, DOI 10.1109/MIS.2002.1024747
   Chaudhuri P., 2007, View-Dependent Character Animation
   ENCARNACAO J, 2004, PRESENCE INTERACTION
   Fitzgibbon AW, 2003, INT S VIDEO COMP, V5, P18
   Igarashi T., 2005, Proceedings of the 2005 acm siggraph/eurographics symposium on computer animation, P107, DOI DOI 10.1145/1073368.1073383
   KATO H, 2007, HUMAN INTERFACE TECH
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   LI Y, 2002, SIGGRAPH, P465
   LOYALL AB, 2004, SCA 04, P59
   Masuko S, 2007, COMPUT GRAPH FORUM, V26, P303, DOI 10.1111/j.1467-8659.2007.01052.x
   McCarthy J.M., 1990, Introduction to Theoretical Kinematics
   Ngo T, 2000, COMP GRAPH, P403, DOI 10.1145/344779.344964
   *OSG, 2007, OPENSCENEGRAPH 2 0
   Papagiannakis G, 2005, COMPUT ANIMAT VIRT W, V16, P11, DOI 10.1002/cav.53
   Piekarski W, 2002, COMMUN ACM, V45, P36, DOI 10.1145/502269.502291
   Rademacher P, 1999, COMP GRAPH, P439, DOI 10.1145/311535.311612
   Ren L, 2005, ACM T GRAPHIC, V24, P1303, DOI 10.1145/1095878.1095882
   SHAO W, 2005, SCA 05, P19
   Shin HJ, 2001, ACM T GRAPHIC, V20, P67, DOI 10.1145/502122.502123
   Simon G, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P120, DOI 10.1109/ISAR.2000.880935
   VINAYAGAMOORTHY V, 2006, EUR C STAT ART REP, P21
   WAGNER D, 2006, ACM INT C P SERIES, V266
NR 34
TC 4
Z9 4
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 525
EP 533
DI 10.1007/s00371-008-0233-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800008
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Shammaa, MH
   Suzuki, H
   Michikawa, T
AF Shammaa, M. Haitham
   Suzuki, Hiromasa
   Michikawa, Takashi
TI Registration of CAD mesh models with CT volumetric model of assembly of
   machine parts
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE computed tomography; volumetric model; triangular mesh model;
   three-dimensional registration
AB X-ray CT (computed tomography) has experienced tremendous growth in industrial application in recent years, and acquiring information about the mechanical parts from CT data has been a great challenge for researchers. This paper presents a new method for the registration of a CT volumetric model of an assembly of parts with a CAD mesh model of a part of the assembly using ICP (iterative closest point) registration method. A few steps to extract feature points should be done as a preprocessing step of the volumetric model and the mesh model before applying the ICP registration algorithm, since the volumetric model and the mesh model are different in their data representation. This preprocessing step is important in order to unify the input of the ICP algorithm, and contributes to the robustness and the speed of the registration process.
C1 Univ Tokyo, Res Ctr Adv Sci & Technol, Tokyo, Japan.
C3 University of Tokyo
RP Shammaa, MH (corresponding author), Univ Tokyo, Res Ctr Adv Sci & Technol, Tokyo, Japan.
EM haitham@den.rcast.u-tokyo.ac.jp; suzuki@den.rcast.u-tokyo.ac.jp;
   michi@den.rcast.u-tokyo.ac.jp
OI Michikawa, Takashi/0000-0002-0606-668X
CR [Anonymous], 1970, PICTURE PROCESSING P
   [Anonymous], INT J PATTERN RECOGN
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Davis L.S., 1975, Comput. Graph. Image Process, V4, P248, DOI DOI 10.1016/0146-664X(75)90012-X
   Goshtasby AA, 2005, 2-D AND 3-D IMAGE REGISTRATION FOR MEDICAL, REMOTE SENSING, AND INDUSTRIAL APPLICATIONS, P1
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   LYVERS EP, 1988, IEEE T PATTERN ANAL, V10, P927, DOI 10.1109/34.9114
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Modersitzki J., 2004, NUMER MATH SCI COMP
   Moravec H.P, 1980, OBSTACLE AVOIDANCE N
   Rohr K., 1994, Journal of Mathematical Imaging and Vision, V4, P139, DOI 10.1007/BF01249893
   ROSSL C, 2000, P AAAI S SMART GRAPH, P71
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Trajkovic M, 1998, IMAGE VISION COMPUT, V16, P75, DOI 10.1016/S0262-8856(97)00056-5
   Zheng ZQ, 1999, PATTERN RECOGN LETT, V20, P149, DOI 10.1016/S0167-8655(98)00134-2
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 19
TC 4
Z9 4
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 965
EP 974
DI 10.1007/s00371-007-0171-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400002
DA 2024-07-18
ER

PT J
AU Lee, KJ
   Kim, DH
   Yun, ID
   Lee, SU
AF Lee, Kyong Joon
   Kim, Dong Hwan
   Yun, Il Dong
   Lee, Sang Uk
TI Three-dimensional oil painting reconstruction with stroke based
   rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE reconstruction; nonphotorealistic rendering; three-dimensional modeling
ID SHAPE; REFLECTANCE
AB We present an algorithm to generate a three-dimensional (3-D) oil painting using artistic filters and sample 3-D structures. The artistic filter works as a two-dimensional (2-D) rendering technique a for non-photorealistic effect based on the algorithm introduced in [8]. We modified this method to properly work with a 3-D image, enhancing the efficiency of the filter as well. The required 3-D structure model is obtained from actual brushstroke samples provided by an artist. We employ a photometric stereo technique in constructing a 3-D model for a brushstroke sample, since this method is capable of capturing sophisticated variation details in brushstroke textures. A 3-D oil painting is completed by incorporating the 3-D model to the brushstrokes rendered by the artistic filter. The created image would show a natural illumination change as the position of the light source varies, providing a realistic effect to the image. We can further enhance the study by analyzing a real painting and by directly applying the current algorithm to synthesize a 3-D oil painting in the style of the original real painting.
C1 Seoul Natl Univ, Sch Elect Engn, Seoul 151742, South Korea.
   Hankuk Univ Foreign Studies, Sch Elect & Control Engn, Yongin 449771, South Korea.
C3 Seoul National University (SNU); Hankuk University Foreign Studies
RP Lee, KJ (corresponding author), Seoul Natl Univ, Sch Elect Engn, Seoul 151742, South Korea.
EM kjoon@cvl.snu.ac.kr; donagory@diehard.snu.ac.kr; yun@hufs.ac.kr;
   sanguk@ipl.snu.ac.kr
RI Lee, Kyoung Ho/J-5570-2012
OI Kim, Dong Hwan/0000-0002-4345-8308; Lee, Kyong Joon/0000-0002-0658-433X
CR Agrawal A, 2005, IEEE I CONF COMP VIS, P174, DOI 10.1109/ICCV.2005.31
   [Anonymous], 1760, Photometria sive de mensura et gradibus luminis, colorum et umbrae
   BAXTER W, 2004, P 2 ANN S NONPH AN R
   DEMPSTER AP, 1977, J ROYAL STAT SOC, V39
   Goldman DB, 2005, IEEE I CONF COMP VIS, P341
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HAYS J, 2004, P 2 ANNS NONPH AN RE
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, P453
   HERTZMANN A, 2002, P 2 ANN S NONPH AN R
   Kowalski MA, 1999, COMP GRAPH, P433, DOI 10.1145/311535.311607
   Lee KM, 1997, COMPUT VIS IMAGE UND, V67, P143, DOI 10.1006/cviu.1997.0522
   Litwinowicz P., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, SIGGRAPH '97, P407
   NAYAR SK, 1990, IEEE T ROBOTIC AUTOM, V6, P418, DOI 10.1109/70.59367
   Nishino K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P599, DOI 10.1109/ICCV.2001.937573
   OREN M, 1995, INT J COMPUT VISION, V14, P227, DOI 10.1007/BF01679684
   PARK BG, 2001, P 13 WORKSH IM PROC, P555
   ROSALES R, 2003, P IEEE INT C COMP VI
   SALISBURY MP, 1999, P SIGGRAPH, P401
   Solomon F, 1996, IEEE T PATTERN ANAL, V18, P449, DOI 10.1109/34.491627
   TAGARE HD, 1991, IEEE T PATTERN ANAL, V13, P133, DOI 10.1109/34.67643
   Wandell B. A, 1995, Foundations of vision
   WOODHAM RJ, 1978, 457 MIT AI LAB
   Yedidia J.S., 2003, EXPLORING ARTIFICIAL, P239, DOI DOI 10.5555/779343.779352
NR 25
TC 5
Z9 7
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 873
EP 880
DI 10.1007/s00371-007-0142-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600027
DA 2024-07-18
ER

PT J
AU Singh, S
   Faloutsos, P
AF Singh, Shawn
   Faloutsos, Petros
TI The photon pipeline revisited - A hardware architecture to accelerate
   photon mapping
SO VISUAL COMPUTER
LA English
DT Article
DE photon mapping; global illumination; graphics hardware
AB With the development of real-time ray tracing in recent years, it is now very interesting to ask if real-time performance can be achieved for high-quality rendering algorithms based on ray tracing. In this paper, we propose a pipelined architecture to implement reverse photon mapping. Our architecture can use real-time ray tracing to generate photon points and camera points, so the main challenge is how to implement the gathering phase that computes the final image. Traditionally, the gathering phase of photon mapping has only allowed coarse-grain parallelism, and this situation has been a source of inefficiency, cache thrashing, and limited throughput. To avail fine-grain pipelining and data parallelism, we arrange computations so that photons can be processed independently, similar to the way that triangles are efficiently processed in traditional real-time graphics hardware. We employ several techniques to improve cache behavior and to reduce communication overhead. Simulations show that the bandwidth requirements of this architecture are within the capacity of current and future hardware, and this suggests that photon mapping may be a good choice for real-time performance in the future.
C1 Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
C3 University of California System; University of California Los Angeles
RP Singh, S (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
EM shawnsin@cs.ucla.edu
CR Abert O, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P161
   ABRAMSON IS, 1982, ANN STAT, V10, P1217, DOI 10.1214/aos/1176345986
   Akenine-Moller T., 2019, Real-time rendering
   [Anonymous], 1993, P 3 INT C COMP GRAPH
   [Anonymous], 2006, PROC 2006 ACMIEEE C, DOI [DOI 10.1145/1188455.1188549, 10.1145/1188455.1188549]
   [Anonymous], P 9 INT WORKSH ART I
   [Anonymous], 2004, THESIS SAARLAND U
   CHEN SE, 1991, SIGGRAPH 91 P 18 ANN, P15
   Christensen P. H., 1999, J GRAPHICS TOOLS, V4, P1, DOI DOI 10.HTTP://WWW.TANDF0NLINE.C0M/D0I/ABS/10.1080/10867651.1999.10487505
   Collins Steven., 1994, In Fifth Eurographics Workshop on Rendering, P119
   Dutre Philip., 2002, ADV GLOBAL ILLUMINAT
   Havran V, 2005, COMPUT GRAPH FORUM, V24, P323, DOI 10.1111/j.1467-8659.2005.00857.x
   HAVRAN V, 2000, THESIS CZECH TU PRAG
   HECKBERT PS, 1990, SIGGRAPH 90 P 17 ANN, P145
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   JENSEN HW, 1995, COMPUT GRAPH, V19, P215, DOI 10.1016/0097-8493(94)00145-O
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   KELLER A, 1997, SIGGRAPH 97, P49
   LARSEN BD, 2004, RENDERING TECHNIQUES, P123
   LAVIGNOTTE F, 2003, GRAPHITE 03, P203
   MA VCH, 2002, HWWS 02 P ACM SIGGRA, P89
   MacDonald J. D., 1990, Visual Computer, V6, P153, DOI 10.1007/BF01911006
   MOLNAR S, 1994, IEEE COMPUT GRAPH, V14, P23, DOI 10.1109/38.291528
   *NVIDIA CUDA, 2007, PROGR GUID VERS 08
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Purcell T., 2003, SIGGRAPHEUROGRAPHICS, P41
   REINHARD E, 1998, CSEXT1998147, P1
   Shirley P, 1995, SPRING COMP SCI, P219
   Silverman B.W., 1985, Density estimation for statistics and data analysis
   SINGH S, 2006, GRAPHITE 06 P 4 INT, P33
   STEINHURST J, 2007, THESIS U N CAROLINA
   STEINHURST J, 2005, GI 05 P 2005 C GRAPH, P97
   TERRELL GR, 1992, ANN STAT, V20, P1236, DOI 10.1214/aos/1176348768
   Wald I., 2002, P 13 EUROGRAPHICS WO
   WALD I, 2001, STAR EUROGRAPHICS 20, P21
   WOOP EBS, 2006, P IEEE S INT RAY TRA
   Woop S, 2005, ACM T GRAPHIC, V24, P434, DOI 10.1145/1073204.1073211
NR 37
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2007
VL 23
IS 7
BP 479
EP 492
DI 10.1007/s00371-007-0123-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 182DL
UT WOS:000247485200004
DA 2024-07-18
ER

PT J
AU Allerkamp, D
   Böettcher, G
   Wolter, FE
   Brady, AC
   Qu, JG
   Summers, IR
AF Allerkamp, Dennis
   Boettcher, Guido
   Wolter, Franz-Erich
   Brady, Alan C.
   Qu, Jianguo
   Summers, Ian R.
TI A vibrotactile approach to tactile rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2005 HAPTEX Workshop
CY DEC 01, 2005
CL Hannover, GERMANY
DE tactile rendering; vibrotactile perception; bistimulus theory; Brownian
   surfaces
ID STIMULATION; FINGERTIP; CHANNELS
AB While moving our fingertip over a fine surface we experience a sensation that gives us an idea of its properties. A satisfactory simulation of this feeling is still an unsolved problem. In this paper, we describe a rendering strategy based on vibrations that play an important role in the tactile exploration of fine surfaces. To produce appropriate excitation patterns we use an array of vibrating contactor pins. Similar to the colour model in computer graphics, we simulate arbitrary vibrations as a superposition of only two sinewaves. Each sinewave is intended for the excitation of a specific population of mechanoreceptors. We carried out first tests of our rendering strategy on Brownian surfaces of different fractal dimensions.
C1 Leibniz Univ Hannover, Welfenlab, Div Comp Graph, Comp Graph Dept,HAPTEX Project, D-30167 Hannover, Germany.
   Univ Exeter, Biomed Phys Grp, Exeter EX4 4QJ, Devon, England.
   Peninsula MR Res Ctr, Exeter, Devon, England.
C3 Leibniz University Hannover; University of Exeter; University of Exeter
RP Allerkamp, D (corresponding author), Leibniz Univ Hannover, Welfenlab, Div Comp Graph, Comp Graph Dept,HAPTEX Project, D-30167 Hannover, Germany.
EM allerkamp@gdv.uni-hannover.de; boettcher@gdv.uni-hannover.de;
   few@gdv.uni-hannover.de; alan.c.brady@exeter.ac.uk; j.qu@exeter.ac.uk;
   i.r.summers@exeter.ac.uk
RI Wolter, Franz-Erich/AAV-3008-2020; Wolter, Franz - Erich/B-1672-2014;
   Wolter, Franz-Erich/JAC-5956-2023
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494
CR [Anonymous], 1996, Computer graphics: principles and practice
   Bergmann M., 1999, Proc. of the First PHANToM Users Research Symposium, P9
   BERNSTEIN LE, 1989, J ACOUST SOC AM, V85, P397, DOI 10.1121/1.397690
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Cornsweet T.N., 1970, Visual Perception
   Falconer K., 2004, Fractal Geometry: Mathematical Foundations and Applications
   Gescheider GA, 2004, BEHAV BRAIN RES, V148, P35, DOI 10.1016/S0166-4328(03)00177-3
   Gescheider GA, 2002, SOMATOSENS MOT RES, V19, P114, DOI 10.1080/08990220220131505
   Handley C, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P273, DOI 10.1109/CGI.1998.694278
   Hollins M, 2001, SOMATOSENS MOT RES, V18, P253
   HUANG G, 2003, SCA 03, P52
   Johnson KO, 2000, J CLIN NEUROPHYSIOL, V17, P539, DOI 10.1097/00004691-200011000-00002
   Johnson KO, 2001, CURR OPIN NEUROBIOL, V11, P455, DOI 10.1016/S0959-4388(00)00234-8
   Katz D., 1989, THE WORLD OF TOUCH
   Kawabata S., 1980, STANDARDIZATION ANAL, V2nd
   Rao A., 1990, TAXONOMY TEXTURE DES
   SCHULZE M, 2005, THESIS HANNOVER
   Summers IR, 2005, J ACOUST SOC AM, V118, P2527, DOI 10.1121/1.2031979
   Summers IR, 2002, J ACOUST SOC AM, V112, P2118, DOI 10.1121/1.1510140
   SUMMERS IR, 2001, P EUROHAPTICS 2001 B, P26
   TSAI PS, 1994, IMAGE VISION COMPUT, V12, P487, DOI 10.1016/0262-8856(94)90002-7
   VERRILLO RT, 1983, PERCEPT PSYCHOPHYS, V33, P379, DOI 10.3758/BF03205886
NR 22
TC 38
Z9 42
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2007
VL 23
IS 2
BP 97
EP 108
DI 10.1007/s00371-006-0031-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 125NZ
UT WOS:000243450200002
DA 2024-07-18
ER

PT J
AU Sharf, A
   Blumenkrants, M
   Shamir, A
   Cohen-Or, D
AF Sharf, Andrei
   Blumenkrants, Marina
   Shamir, Ariel
   Cohen-Or, Daniel
TI SnapPaste: an interactive technique for easy mesh composition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE interactive tools; user-interface; cut-and-paste; snapping; meshes
ID REGISTRATION
AB Editing and manipulation of existing 3D geometric objects are a means to extend their repertoire and promote their availability. Traditionally, tools to compose or manipulate objects defined by 3D meshes are in the realm of artists and experts. In this paper, we introduce a simple and effective user interface for easy composition of 3D mesh-parts for non-professionals. Our technique borrows from the cut-and-paste paradigm where a user can cut parts out of existing objects and paste them onto others to create new designs. To assist the user attach objects to each other in a quick and simple manner, many applications in computer graphics support the notion of "snapping". Similarly, our tool allows the user to loosely drag one mesh part onto another with an overlap, and lets the system snap them together in a graceful manner. Snapping is accomplished using our Soft-ICP algorithm which replaces the global transformation in the ICP algorithm with a set of point-wise locally supported transformations. The technique enhances registration with a set of rigid to elastic transformations that account for simultaneous global positioning and local blending of the objects. For completeness of our framework, we present an additional simple mesh-cutting tool, adapting the graph-cut algorithm to meshes.
C1 Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
   Interdisciplinary Ctr Herzliya, Herzliyya, Israel.
C3 Tel Aviv University; Reichman University
RP Sharf, A (corresponding author), Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
EM asharf@tau.ac.il
RI Sharf, Andrei/F-1370-2012
OI Sharf, Andrei/0000-0002-3963-4508
CR Angelidis A, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P63, DOI 10.1109/SMI.2004.1314494
   [Anonymous], 2001, 3 INT C 3D DIG IM MO
   BENDELS GH, 2003, P EUR ACM SIGGRAPH S, P207
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   BIERMANN H, 2002, P ACM SIGGRAPH 02, P312, DOI DOI 10.1145/566570.566583
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Feldmar J, 1996, INT J COMPUT VISION, V18, P99, DOI 10.1007/BF00054998
   FU H, 2004, P 3 INT C GEOM MOD P
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Hassner T, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P72, DOI 10.1109/SMI.2005.31
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   Kanai T, 1999, PROC GRAPH INTERF, P148
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KOBBELT L, 1998, P SIGGRAPH 98, P105, DOI DOI 10.1145/280814.280831
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   MUSETH K, 2002, P SIGGRAPH, P330
   Nealen A, 2005, ACM T GRAPHIC, V24, P1142, DOI 10.1145/1073204.1073324
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Scheidegger CE, 2005, EUR S GEOM PROC
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   THIRION JP, 1995, P C MED ROB COMP ASS
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 31
TC 59
Z9 79
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 835
EP 844
DI 10.1007/s00371-006-0068-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000026
DA 2024-07-18
ER

PT J
AU Yu, XD
   Wang, L
   Tian, Q
   Xue, P
AF Yu, XD
   Wang, L
   Tian, Q
   Xue, P
TI A novel multi-resolution video representation scheme based on kernel PCA
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Multimedia Modelling (MMM 2004)
CY JAN 05-07, 2004
CL Brisbane, AUSTRALIA
SP Queensland Univ Technol, Ctr Informat Technol Innovat, Deakin Univ, Sch Informat Technol, Apple Comp, Int Federat Automat Control, Emu Design
DE kernel PCA; video summarization; video representation
ID COMPONENT ANALYSIS; RETRIEVAL; ABSTRACTION
AB Content-based video analysis calls for efficient and flexible video representation. In this paper, a novel multi-resolution video representation (MRVR) scheme is proposed and realized by performing the kernel principal component analysis (KPCA) on the low-level visual features extracted from a video sequence. By simply changing the kernel parameters or the dimensionality of the subspace, this scheme can represent video content from coarser to finer levels in the subspace, according to its intrinsic structure. An application of keyframe extraction is investigated to show the advantages of this representation scheme. Furthermore, based on this scheme, a two-level video summarization approach is proposed to represent long video sequences. The experimental results of both short and long video sequences have demonstrated the effectiveness and flexibility of the proposed video representation scheme.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   Agcy Sci Technol & Res, Inst Infocomm Res, Singapore, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM exdyu.assoc@ntu.edu.sg; elwang@ntu.edu.sg; tian@i2r.a-star.edu.sg;
   epxue@ntu.edu.sg
RI Xue, Ping/A-5155-2011; Wang, Lei/AAL-9684-2020; Wang, Lei/D-9079-2013
OI Wang, Lei/0000-0002-0961-0441
CR Aigrain P, 1996, MULTIMED TOOLS APPL, V3, P179, DOI 10.1007/BF00393937
   [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 2000, P 2000 ACM WORKSH MU, DOI DOI 10.1145/357744.357942
   Bezdek James C., 1981, PATTERN RECOGN
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387
   Cristianini N, 1999, ADV NEUR IN, V11, P204
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Day YF, 1999, MULTIMEDIA SYST, V7, P409, DOI 10.1007/s005300050142
   GONG Y, 2000, P INT C COMP VIS PAT, V2, P2179
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133
   KOBLA V, 1998, P SPIE C STOR RETR S, V3312, P81
   Ngo CW, 2002, IEEE T MULTIMEDIA, V4, P446, DOI 10.1109/TMM.2002.802022
   Sahouria E, 1999, IEEE T CIRC SYST VID, V9, P1290, DOI 10.1109/76.809163
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Yeo BL, 1999, MULTIMEDIA SYST, V7, P269, DOI 10.1007/s005300050129
NR 20
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 357
EP 370
DI 10.1007/s00371-006-0013-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500008
DA 2024-07-18
ER

PT J
AU Park, S
   Guo, XH
   Shin, HY
   Qin, H
AF Park, S
   Guo, XH
   Shin, HY
   Qin, H
TI Surface completion for shape and appearance
SO VISUAL COMPUTER
LA English
DT Article
DE hole filling; active contour method; Poisson equation
AB In this paper, we present a new surface content completion system that can effectively repair both shape and appearance from scanned, incomplete point set inputs. First, geometric holes can be robustly identified from noisy and defective data sets without the need for any normal or orientation information. The geometry and texture information of the holes can then be determined either automatically from the models' context, or interactively from users' selection. We use local parameterizations to align patches in order to extract their curvature-driven digital signature. After identifying the patch that most resembles each hole region, the geometry and texture information can be completed by warping the candidate region and gluing it onto the hole area. The displacement vector field for the exact alignment process is computed by solving a Poisson equation with boundary conditions. Our experiments show that the unified framework, founded upon the techniques of deformable models, local parameterization, and PDE modeling, can provide a robust and elegant solution for content completion of defective, complex point surfaces.
C1 Korea Adv Inst Sci & Technol, VMS Lab Dept Ind Engn, Taejon 305701, South Korea.
   SUNY Stony Brook, Dept Comp Sci, Ctr Visual Comp, Stony Brook, NY 11794 USA.
C3 Korea Advanced Institute of Science & Technology (KAIST); State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Korea Adv Inst Sci & Technol, VMS Lab Dept Ind Engn, Taejon 305701, South Korea.
EM parksy@vmslab.kaist.ac.kr; xguo@cs.sunysb.edu; hyshin@kaist.ac.kr;
   qin@cs.sunysb.edu
RI Shin, Hayong/C-1586-2011
OI Park, Seyoun/0000-0002-6987-9362
FU Direct For Computer & Info Scie & Enginr [0830183] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems
   [0830183] Funding Source: National Science Foundation
CR Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   [Anonymous], 2005, P 3 EUR S GEOM PROC
   Bajaj ChandrajitL., 1995, Proc. Conf. on Computer graphics and interactive techniques, P109
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Biermann H, 2002, ACM T GRAPHIC, V21, P312, DOI 10.1145/566570.566583
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Clarenz U, 2004, COMPUT AIDED GEOM D, V21, P427, DOI 10.1016/j.cagd.2004.02.004
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Fu HB, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P173
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Johnson AE, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P234, DOI 10.1109/IM.1997.603871
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   [Краевский В.В. Kraevsky V.V.], 2005, [Педагогика, Pedagogika], P13
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Masuda T, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P1003
   Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   PAGE DL, 2001, P INT C COMP VIS PAT, V1, P162
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PODOLAK J, 2005, P EUR S GEOM PROC, P33
   SAVCHENKO V, 2002, P COMP GRAPH INT BRA
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Verdera J, 2003, P INT C IM PROC BARC
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Whitaker RT, 1998, INT J COMPUT VISION, V29, P203, DOI 10.1023/A:1008036829907
   Xie H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P259, DOI 10.1109/VISUAL.2004.101
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
   Zwicker M, 2002, ACM T GRAPHIC, V21, P322, DOI 10.1145/566570.566584
NR 34
TC 28
Z9 28
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2006
VL 22
IS 3
BP 168
EP 180
DI 10.1007/s00371-006-0374-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 028UP
UT WOS:000236514600003
DA 2024-07-18
ER

PT J
AU Song, Y
   Chen, YY
   Tong, X
   Lin, S
   Shi, JY
   Guo, BN
   Shum, HY
AF Song, Y
   Chen, YY
   Tong, X
   Lin, S
   Shi, JY
   Guo, BN
   Shum, HY
TI Shell radiance texture functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE subsurface scattering; mesostructure; texture mapping; real-time
   rendering
AB The appearance of an inhomogeneous translucent material depends substantially on its volumetric variations and their effects upon subsurface scattering. For efficient rendering that accounts for both surface mesostructures and volumetric variations of such materials, shell texture functions have precomputed irradiance within a volume with respect to incoming illumination, but even with this irradiance data a fair amount of runtime computation is still required. Rather than precompute volume irradiance, we introduce the shell radiance texture function (SRTF), which relates incoming illumination more directly to outgoing surface radiance by representing a set of subsurface transport components from which surface radiance can be calculated without ray marching or runtime evaluation of dipole diffusion. Using this precomputed SRTF information, inhomogeneous objects can be rendered in real time with distant local lighting or global lighting.
C1 Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
   Zhejiang Univ, Dept Comp Sci & Engn, Hangzhou 310027, Zhejiang Prov, Peoples R China.
   Microsoft Res Asia, Visual Comp Grp, Beijing, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Zhejiang University; Microsoft;
   Microsoft Research Asia
RP Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
EM songying@cad.zju.edu.cn; yachen@microsoft.com; xtong@microsoft.com;
   stevelin@microsoft.com; jyshi@cad.zju.edu.cn; bainguo@microsoft.com;
   hshum@microsoft.com
OI Tong, Xin/0000-0001-8788-2453; , Ying/0000-0003-4813-0245
CR BLASI P, 1994, EUR WORKSH REND, P173
   CARR NA, 2003, P ACM SIGGRAPH EUROG, P51
   Chen Y, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P343, DOI 10.1109/ICDM.2004.10025
   Cook R. L., 1984, Computers & Graphics, V18, P223
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   GOESELE M, 2004, P SIGGRAPH2004, P835
   HAO X, 2003, S INT 3D GRAPH, P75
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Ma W.-C., 2005, I3D '05: Proceedings of the 2005 sym109 posium on Interactive 3D graphics and games, P187
   Müller G, 2005, COMPUT GRAPH FORUM, V24, P83, DOI 10.1111/j.1467-8659.2005.00830.x
   Müller G, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P198, DOI 10.1109/CGI.2004.1309211
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   Premoze S., 2004, EUR S REND
   Sattler Mirko., 2003, Proceedings of the 14th Eurographics workshop on Rendering, EGRW '03, P167
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Sloan PP, 2003, ACM T GRAPHIC, V22, P370, DOI 10.1145/882262.882279
   Stam J, 1995, SPRING COMP SCI, P41
   WANG L, 2003, P ACM SIGGRAPH
   WANG X, 2004, P EUR S REND
   WOOD D, 2000, P ACM SIGGRAPH
NR 26
TC 3
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 774
EP 782
DI 10.1007/s00371-005-0320-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400030
DA 2024-07-18
ER

PT J
AU Wu, W
   Heng, PA
AF Wu, W
   Heng, PA
TI An improved scheme of an interactive finite element model for 3D
   soft-tissue cutting and deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE finite element model; condensation; graphics processing unit; surgical
   simulation; soft-tissue deformation
AB As a safe and feasible alternative to enriching and enhancing traditional surgical training, virtual-reality-based surgical simulators have been investigated for a long time. But it is still a challenge for researchers to accurately depict the behavior of human tissue without losing the flexibility of simulation. In this paper, we propose an improved scheme of an interactive finite element model for simulating the surgical process of organ deformation, cutting, dragging, and poking, which can maximally compromise the flexibility and reality of soft-tissue models. The scheme is based on our hybrid condensed finite element model for surgical simulation, which consists of the operational region and nonoperational region. Different optimizing methods applied to these regions make a contribution to the speedup of the calculation. Considering in a real surgical operation, dragging or poking operations are also necessary for surgeons to examine surrounding tissues of the pathological focus. The calculation within the area newly applied with forces in the nonoperational region is handled in our new scheme. The algorithm is modified accordingly in order to cope with this aspect. The design and implementation of the approach are presented. Finally, we provide two models to test our scheme. The results are analyzed and discussed to show the efficiency of our scheme.
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM wwu1@cse.cuhk.edu.hk; pheng@cse.cuhk.edu.hk
OI Heng, Pheng Ann/0000-0003-3055-5034
CR BERKLEY J, 1999, VIRTUAL REALITY RES, V4
   Bro-Nielsen M., 1996, Computer Graphics Forum, V15, pC57, DOI 10.1111/1467-8659.1530057
   Butler D L, 1978, Exerc Sport Sci Rev, V6, P125
   Chandrupatla TirupathiR., 1997, INTRO FINITE ELEMENT, VThird
   CHEN BY, 2002, P IASTED INT C VIS I, P282
   Cotin S, 2000, VISUAL COMPUT, V16, P437, DOI 10.1007/PL00007215
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Davies B W, 1995, Ann R Coll Surg Engl, V77, P299
   DELINGETTE H, 1994, P SOC PHOTO-OPT INS, V2359, P607
   Duck F A., 1990, PHYS PROPERTIES TISS
   Kardestuncer H., 1987, FINITE ELEMENT HDB
   Mollemans W, 2004, LECT NOTES COMPUT SC, V3217, P371
   Ono Y, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P472, DOI 10.1109/CW.2002.1180915
   PARKE FI, 1982, IEEE COMPUT GRAPH, V2, P61
   Picinbono G., 2000, 4018 INRIA
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   WATERS K, 1987, P 14 ANN C COMP GRAP, P17
   Wu W, 2004, COMPUT ANIMAT VIRT W, V15, P219, DOI 10.1002/cav.24
   YANG X, 2002, P INT C DIAGN IM AN, P294
NR 19
TC 40
Z9 51
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 707
EP 716
DI 10.1007/s00371-005-0310-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400023
DA 2024-07-18
ER

PT J
AU Wang, XH
   Devarajan, V
AF Wang, XH
   Devarajan, V
TI 1D and 2D structured mass-spring models with preload
SO VISUAL COMPUTER
LA English
DT Article
DE mass-spring model; continuum mechanics; parameter optimization; preload;
   axisymmetric bending
AB In this paper, the objective is to enrich the existing 1D and 2D mass-spring models with physical accuracy as well as visual realism. It is found that using nonzero preloads on the springs is a necessary condition for the models to approximate their continuum counterparts. First, the parameters of the 1D mass-spring model of a beam are derived based on pure bending and axial action. It is proved that the mass-spring model with this set of parameter has correct characteristics of resistance against lateral displacement, which is one of the most important aspects of the accuracy of the 1D mass-spring model. Then, the method is extended to the 2D mass-spring models of the continuum plate with two different mesh structures. The mass-spring model with equilateral triangle meshes is shown to be physically more accurate than that with rectangular meshes. Finally, the physical accuracy that the mass-spring models with preload can achieve is investigated under different load conditions by comparison with the finite element method (FEM) to demonstrate the efficacy of our approach.
C1 Univ Texas, Arlington, TX 76019 USA.
C3 University of Texas System; University of Texas Arlington
RP Univ Texas, 701 S Nedderman Dr, Arlington, TX 76019 USA.
EM xiuzhongw@yahoo.com; venkat@uta.edu
CR Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bhat Kiran S., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P37
   Breen David E., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques, P365
   BRIDSON R, 2003, P 2003 ACM SIGGRAPH, P28
   BROWN J, 2001, P 4 INT C MED IM COM, P137
   CARIGNAN M, 1992, COMP GRAPH, V26, P99, DOI 10.1145/142920.134017
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   FENNER RT, 1993, MECH SOLIDS
   GELDER A, 1998, J GRAPHICS TOOLS, V3, P21
   Grinspun E., 2003, P 2003 ACM SIGGRAPH, P62
   HOUSE DH, 2000, CLOTH MODELING ANIMA
   Jojic Nebojsa., 1997, INT WORKSHOP SYNTHET, P73
   Lee Y., 1995, SIGGRAPH, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]
   MACIEL A, 2003, P INT S SURG SIM SOF, P74
   Miller G. S. P., 1988, Computer Graphics, V22, P169, DOI 10.1145/378456.378508
   PANC V, 1975, THEORIES ELASTIC PA
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   PROVOT X, 1995, GRAPH INTER, P147
   Reddy J.N., 1998, THEORY ANAL ELASTIC
   Solecki R., 2003, Advanced Mechanics of Materials
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Terzopoulos D., 1989, Proceeding of Graphics Interface, P219
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   TU X., 1994, P ACM SIGGRAPH 94, P43, DOI DOI 10.1145/192161.192170
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   Volino P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P137, DOI 10.1145/218380.218432
   WANG X, 2004, ACM SIGGRAPH INT C V, P317
   WEISSTEIN EW, CIRCLE LATTICE POINT
NR 29
TC 16
Z9 20
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2005
VL 21
IS 7
BP 429
EP 448
DI 10.1007/s00371-005-0303-5
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GE
UT WOS:000230991200001
DA 2024-07-18
ER

PT J
AU Marvie, JE
   Perret, J
   Bouatouch, K
AF Marvie, JE
   Perret, J
   Bouatouch, K
TI The FL-system: a functional L-system for procedural geometric modeling
SO VISUAL COMPUTER
LA English
DT Article
DE L-systems; grammars; object modeling; real-time rendering
AB In this paper, we present an FL-system, an extension of an L-system that allows us to generate any kind of object hierarchy and mesh on the fly. This has been made possible thanks to a modification of the classical L-system rewriting mechanism that produces a string of symbols interpreted afterwards. In our system, terminal symbols are not characters, but functions that can be executed at any step of the rewriting process. Thanks to this extension, our system allows the instantiation of generic objects during the course of the rewriting process as well as their initialization. Therefore, we are able to simulate all of the existing solutions proposed by classical L-systems, but we are also able to generate VRML97 scene graphs and geometry on the fly, since VRML97 nodes are handled as generic objects. As an example, we will show in the second part of this paper how to use our extension to describe building styles that are utilized to generate large sets of different building models. We also present some models of urban features (street lamps, etc.) and plants modeled and generated using FL-systems.
C1 IRISA INRIA Rennes, F-35042 Rennes, France.
C3 Universite de Rennes
RP IRISA INRIA Rennes, Campus Univ Beaulieu,Ave Gen Leclerc, F-35042 Rennes, France.
EM jemarvie@irisa.fr; juperret@irisa.fr; kadi@irisa.fr
RI Perret, Julien/JAC-8803-2023; Perret, Julien/J-9017-2015
OI Perret, Julien/0000-0002-0685-0730
CR Abelson H., 1981, Turtle Geometry
   Bloomenthal J., 1985, Computer Graphics, V19, P305, DOI 10.1145/325165.325249
   Ebert D.S., 2003, TEXTURING MODELING, V3rd
   GIPS J, 1974, THESIS STANFORD U
   Hart J. C., 1992, Proceedings. Graphics Interface '92, P224
   LEYTON M, 2001, LECT NOTES COMPUTER, V2154
   LINDENMAYER A, 1968, J THEOR BIOL, V18, P280, DOI 10.1016/0022-5193(68)90079-9
   MARRIN C, 1997, VRML SPECIFICATION 1
   MARVIE JE, 2003, PI1546 IRISA
   MARVIE JE, 2003, P PAC GRAPH IEEE COM, V2, P389
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Prusinkiewicz P, 2001, COMP GRAPH, P289, DOI 10.1145/383259.383291
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Prusinkiewicz Przemyslaw., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques. SIGGRAPH'94, P351, DOI DOI 10.1145/192161.192254
   SMITH AR, 1984, P 11 ANN C COMP GRAP, P1
   Stiny G., 1975, Pictorial and Formal Aspects of Shape and Shape Grammars, Interdisciplinary Systems Research
   Van Haevre W, 2003, WSCG'2003, VOL 11, NO 3, CONFERENCE PROCEEDINGS, P464
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
NR 18
TC 27
Z9 40
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2005
VL 21
IS 5
BP 329
EP 339
DI 10.1007/s00371-005-0289-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 937OJ
UT WOS:000229935100004
DA 2024-07-18
ER

PT J
AU Assarsson, U
   Akenine-Möller, T
AF Assarsson, U
   Akenine-Möller, T
TI Occlusion culling and <i>z</i>-fail for soft shadow volume algorithms
SO VISUAL COMPUTER
LA English
DT Article
DE soft shadows; graphics hardware; shadow volumes
AB This paper presents a significant improvement of our previously proposed soft shadow volume algorithm for simulating soft shadows. By restructuring the algorithm, we can considerably simplify the computations, introduce efficient occlusion culling with speedups of 3-4 times, thus approaching real-time performance. We can also generalize the algorithm to produce correct shadows even when the eye is inside a shadowed region (using z-fail). We present and evaluate a three pass implementation of the restructured algorithm for near real-time rendering of soft shadows on a computer with a commodity graphics accelerator. However, preferably the rendering of the wedges should be implemented in hardware, and for this we suggest and evaluate a single pass algorithm.
C1 Chalmers, Dept Comp Engn, S-41263 Gothenburg, Sweden.
C3 Chalmers University of Technology
RP Chalmers, Dept Comp Engn, S-41263 Gothenburg, Sweden.
EM tompa@ce.chalmers.se
CR AKENINEMOLLER T, 2002, 13 EUR WORKSH REND E, P309
   ASSARSSON U, 2003, GRAPHICS HARDWARE 20, P33
   Assarsson U., 2003, PROC SIGGRAPH 03, P511
   Crow F., 1977, SIGGRAPH 77, P242
   EVERITT C., 2002, PRACTICAL ROBUST STE
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Greene Ned., 1994, GRAPHICS GEMS, P74
   HAINES E, 2001, GAME DEV C CMP, P335
   Haines E. A., 1994, Photorealistic Rendering in Computer Graphics. Proceedings of the Second Eurographics Workshop on Rendering, P122
   HEIDMANN T, 1991, REAL SHADOWS, P23
   MOREIN S, 2000, SIGGRAPH EUR GRAPH H
   PARKER S, 1999, UUCS98019 TR COMP SC
   SOLER C, 1998, SIGGRAPH 98 C P ANN, P321
   WOO A, 1990, IEEE COMPUT GRAPH, V10, P13, DOI 10.1109/38.62693
NR 14
TC 1
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 601
EP 612
DI 10.1007/s00371-004-0254-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600007
DA 2024-07-18
ER

PT J
AU An, D
   Hu, RH
   Fan, LT
   Chen, ZL
   Liu, ZT
   Zhou, P
AF An, Dong
   Hu, Ronghua
   Fan, Liting
   Chen, Zhili
   Liu, Zetong
   Zhou, Peng
TI STDPNet: a dual-path surface defect detection neural network based on
   shearlet transform
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Shearlet transform; Deep learning; Deep convolutional neural network;
   Defect detection
ID REPRESENTATION
AB Defect detection systems based on machine vision have been widely used as an essential part of intelligent manufacturing systems. However, in traditional object detection methods that rely on images as input, differences in defect areas, blurred images, and complex background interference can seriously impair detection accuracy. To meet these challenges, this paper proposed a dual-path neural network based on shearlet transform (STDPNet) by taking advantage of shearlet transform in multi-scale analysis and combining it with the improved object detection algorithm proposed in this paper. First, images are multi-scale and multi-directional decomposed with shearlet transform, and multi-directional sub-band information is input to the detection network instead of image information. Then, this paper proposed a dual-path object detection network for the differences between different frequency bands and introduced a transfer learning strategy between paths to improve the model performance. Finally, the training results on the NEU surface defect public dataset show that the mean average precision of STDPNet achieves 86.81% at a detection speed of 44.45 f/s, which exceeds that of Faster R-CNN by 12%. Experiments on different datasets prove that the accuracy is significantly superior to other models, and the proposed method is more advantageous compared to other models in large, fuzzy, and indistinguishable defect types.
C1 [An, Dong; Hu, Ronghua; Fan, Liting; Liu, Zetong; Zhou, Peng] Shenyang Jianzhu Univ, Sch Mech Engn, Shenyang 110168, Liaoning, Peoples R China.
   [Chen, Zhili] Shenyang Jianzhu Univ, Sch Comp Sci & Engn, Shenyang 110168, Peoples R China.
C3 Shenyang Jianzhu University; Shenyang Jianzhu University
RP Zhou, P (corresponding author), Shenyang Jianzhu Univ, Sch Mech Engn, Shenyang 110168, Liaoning, Peoples R China.
EM andong@sjzu.edu.cn; zhoupeng@sjzu.edu.cn
FU National Natural Science Foundation of China [51975130]; National
   Science Foundation of China [LJKMZ20220915]; Basic Scientific Research
   Project of Education Department of Liaoning Province
FX This study was supported by the National Science Foundation of China
   (No. 51975130) and Basic Scientific Research Project of Education
   Department of Liaoning Province (LJKMZ20220915).
CR Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Candes E.J., 2000, Curvelets: A Surprisingly Effective Nonadaptive Representation for Objects with Edges, DOI [10.1086/116933, DOI 10.1086/116933]
   Candès E, 2006, MULTISCALE MODEL SIM, V5, P861, DOI 10.1137/05064182X
   Chen K., 2021, J. Build. Eng, V43, DOI DOI 10.1016/J.JOBE.2021.102523
   Chen SH, 2023, VISUAL COMPUT, V39, P1437, DOI 10.1007/s00371-022-02421-5
   Dong YS, 2018, VISUAL COMPUT, V34, P1315, DOI 10.1007/s00371-017-1415-4
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Ghorai S, 2013, IEEE T INSTRUM MEAS, V62, P612, DOI 10.1109/TIM.2012.2218677
   Guo K., 2006, Wavelets and Splines, P189
   Guo K, 2007, SIAM J MATH ANAL, V39, P298, DOI 10.1137/060649781
   Hao RY, 2021, J INTELL MANUF, V32, P1833, DOI 10.1007/s10845-020-01670-2
   Hou WQ, 2024, VISUAL COMPUT, V40, P459, DOI 10.1007/s00371-023-02793-2
   Hou Z, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P275
   Hu P, 2024, VISUAL COMPUT, V40, P1245, DOI 10.1007/s00371-023-02844-8
   Li LL, 2019, J MED IMAG HEALTH IN, V9, P1815, DOI 10.1166/jmihi.2019.2827
   Lin DY, 2020, IEEE IMAGE PROC, P2131, DOI 10.1109/ICIP40778.2020.9190900
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Powers DMW, 2020, Arxiv, DOI arXiv:2010.16061
   Neogi N, 2014, EURASIP J IMAGE VIDE, P1, DOI 10.1186/1687-5281-2014-50
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarkar D, 2022, VISUAL COMPUT, V38, P4457, DOI 10.1007/s00371-021-02308-x
   Schwartz WR, 2011, IEEE IMAGE PROC, P1033, DOI 10.1109/ICIP.2011.6115600
   Shi LK, 2021, VISUAL COMPUT, V37, P1343, DOI 10.1007/s00371-020-01869-7
   Song KC, 2013, APPL SURF SCI, V285, P858, DOI 10.1016/j.apsusc.2013.09.002
   Wu J, 2021, APPL INTELL, V51, P4945, DOI 10.1007/s10489-020-02084-6
   Zhang XL, 2022, APPL INTELL, V52, P2775, DOI 10.1007/s10489-021-02545-6
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
NR 35
TC 0
Z9 0
U1 7
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 26
PY 2023
DI 10.1007/s00371-023-03139-8
EA NOV 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y7FN0
UT WOS:001106882800001
DA 2024-07-18
ER

PT J
AU Mel, M
   Siddiqui, M
   Zanuttigh, P
AF Mel, Mazen
   Siddiqui, Muhammad
   Zanuttigh, Pietro
TI End-to-end learning for joint depth and image reconstruction from
   diffracted rotation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Monocular depth estimation; RPSFs; Image deblurring
ID POINT-SPREAD FUNCTIONS
AB Monocular depth estimation is an open challenge due to the ill-posed nature of the problem at hand. Deep learning techniques proved capable of producing acceptable depth estimation accuracy but the lack of robust depth cues within RGB images severally limits their performance. Coded aperture-based methods using phase and amplitude masks encode strong depth cues within 2D images by means of depth-dependent Point Spread Functions (PSFs) at the price of a reduced image quality. In this paper, we propose a novel end-to-end learning approach for depth from diffracted rotation. A phase mask that produces a Rotating Point Spread Function (RPSF) as a function of defocus is jointly optimized with the weights of a depth estimation neural network. To this aim, we introduce a differentiable physical model of the aperture mask and exploit an accurate simulation of the camera imaging pipeline. Our approach requires a significantly less complex model and less training data, yet it outperforms existing methods for monocular depth estimation on indoor benchmarks. In addition, we address the image degradation problem by incorporating a non-blind and nonuniform image deblurring module to recover the sharp all-in-focus image from its blurred counterpart.
C1 [Mel, Mazen; Zanuttigh, Pietro] Univ Padua, Dept Informat Engn, Padua, Italy.
   [Siddiqui, Muhammad] Sony Europe BV, Zweigniederlassung Deutschland Stuttgart Technol C, Stuttgart, Germany.
C3 University of Padua; Sony Corporation; Sony Deutschland GmbH
RP Mel, M (corresponding author), Univ Padua, Dept Informat Engn, Padua, Italy.
EM mazen.mel@phd.unipd.it
OI MEL, MAZEN/0000-0001-8081-0529
FU Universita degli Studi di Padova; Sony Europe B.V.
FX Open access funding provided by Universita degli Studi di Padova. The
   work was supported by Sony Europe B.V.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Alhashim I, 2019, Arxiv, DOI arXiv:1812.11941
   ALLEN L, 1992, PHYS REV A, V45, P8185, DOI 10.1103/PhysRevA.45.8185
   Anger J, 2018, IEEE IMAGE PROC, P978, DOI 10.1109/ICIP.2018.8451115
   Berlich R, 2018, OPT EXPRESS, V26, P4873, DOI 10.1364/OE.26.004873
   Berlich R, 2016, OPT EXPRESS, V24, P5946, DOI 10.1364/OE.24.005946
   Born M., 2013, PRINCIPLES OPTICS EL, DOI 10.1017/CBO9781139644181
   Chang J, 2019, IEEE I CONF COMP VIS, P10192, DOI 10.1109/ICCV.2019.01029
   Chen XT, 2019, Arxiv, DOI arXiv:1907.06023
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Dong JX, 2021, Arxiv, DOI arXiv:2103.09962
   Eigen D., 2014, arXiv
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Bhat SF, 2020, Arxiv, DOI arXiv:2011.14141
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gong D, 2020, IEEE T NEUR NET LEAR, V31, P5468, DOI 10.1109/TNNLS.2020.2968289
   Greengard A, 2006, OPT LETT, V31, P181, DOI 10.1364/OL.31.000181
   Haim H, 2018, IEEE T COMPUT IMAG, V4, P298, DOI 10.1109/TCI.2018.2849326
   Hao ZX, 2018, INT CONF 3D VISION, P304, DOI 10.1109/3DV.2018.00043
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iskander DR, 2001, IEEE T BIO-MED ENG, V48, P87, DOI 10.1109/10.900255
   Kingma D. P., 2014, arXiv
   Kotlyar VV, 2006, APPL OPTICS, V45, P2656, DOI 10.1364/AO.45.002656
   Kotlyar VV, 2005, J OPT SOC AM A, V22, P849, DOI 10.1364/JOSAA.22.000849
   Kraus M, 2007, COMPUT GRAPH FORUM, V26, P645, DOI 10.1111/j.1467-8659.2007.01088.x
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Kumar R., 2013, P 2013 AMOS TECHN C
   Kumar R., 2015, Three-Dimensional Imaging using a Novel Rotating Point Spread Function Imager
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lee Jin Han, 2019, arXiv
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485
   Mayer N, 2015, Arxiv, DOI arXiv:1512.02134
   Pavani SRP, 2008, OPT EXPRESS, V16, P3484, DOI 10.1364/OE.16.003484
   Piestun R, 2000, J OPT SOC AM A, V17, P294, DOI 10.1364/JOSAA.17.000294
   Prasad S, 2013, OPT LETT, V38, P585, DOI 10.1364/OL.38.000585
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Ranftl R, 2021, Arxiv, DOI arXiv:2103.13413
   Reeves SJ, 2005, IEEE T IMAGE PROCESS, V14, P1448, DOI 10.1109/TIP.2005.854474
   Roider C, 2014, OPT EXPRESS, V22, P4029, DOI 10.1364/OE.22.004029
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Shechtman Y, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.133902
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Wiener N., 1949, Extrapolation, Interpolation, and Smoothing of Stationary Time Series: with Engineering Applications, V113
   Wu Y., 2019, 2019 IEEE INT C COMP, P1
   Xu Li, 2014, P ANN C NEUR INF PRO, P1790
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Zhou CY, 2011, INT J COMPUT VISION, V93, P53, DOI 10.1007/s11263-010-0409-8
NR 52
TC 0
Z9 0
U1 7
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 20
PY 2023
DI 10.1007/s00371-023-03147-8
EA NOV 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y6GA9
UT WOS:001106212200002
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Wang, LP
   Wei, H
AF Wang, Luping
   Wei, Hui
TI Understanding of multiple bending-sloping arched scenes based on angle
   projections
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Unstructured environment; Bending-sloping; Arched; Scene understanding;
   3D reconstruction
ID SEMANTIC SEGMENTATION
AB Scene understanding is crucial for mission planning and monitoring in autonomous systems. Existing approaches have attached importance to understanding structured scenes such as indoor and urban environment. However, for diverse unstructured scenes filled with multiple bending-sloping surfaces, understanding them remains an unsettled issue. Compared to methods through three-dimensional (3D) point clouds and information fusion techniques, monocular understanding enjoys the advantages of less energy consumption and less memory intensive. In this paper, we propose a new methodology to understand multiple bending-sloping surfaces in diverse scenes and establish their 3D reconstruction from a monocular camera. Bending-sloping angle projections are extracted. Through their subclusters and combinations, potential arched surfaces are estimated. Through geometric constraints of integrity and orientation, an arched scene is approximated in 3D reconstruction. Using geometric inference, no prior training is needed, and the approach requires neither the camera being calibrated nor the camera internal parameters. By comparing results to the ground truth, the percentage of incorrectly classified pixels was evaluated. The results proved that the methodology is successful in understanding multiple bending-sloping surfaces, meeting scene understanding requirements in diverse unstructured environments.
C1 [Wang, Luping] Univ Shanghai Sci & Technol, Sch Mech Engn, Lab 3D Scene Understanding & Visual Nav, Jungong Rd 516, Shanghai 200093, Peoples R China.
   [Wei, Hui] Fudan Univ, Sch Comp Sci, Lab Algorithms Cognit Models, Zhangheng Rd 825, Shanghai 201203, Peoples R China.
C3 University of Shanghai for Science & Technology; Fudan University
RP Wang, LP (corresponding author), Univ Shanghai Sci & Technol, Sch Mech Engn, Lab 3D Scene Understanding & Visual Nav, Jungong Rd 516, Shanghai 200093, Peoples R China.
EM 15110240007@fudan.edu.cn; weihui@fudan.edu.cn
RI Wang, Luping/AAF-4089-2020; Wang, Luping/AAZ-4035-2021
OI Wang, Luping/0000-0002-2417-4561
FU This work was supported by the NSFC Project (Project Nos. 62003212,
   61771146 and 61375122), and (in part) by Shanghai Professional Technical
   Service Platform for Intelligent Operation and Maintenance of Renewable
   Energy (22DZ2291800). [62003212, 61771146, 61375122]; NSFC Project
   [22DZ2291800]; Shanghai Professional Technical Service Platform for
   Intelligent Operation and Maintenance of Renewable Energy
FX This work was supported by the NSFC Project (Project Nos. 62003212,
   61771146 and 61375122), and (in part) by Shanghai Professional Technical
   Service Platform for Intelligent Operation and Maintenance of Renewable
   Energy (22DZ2291800).
CR Ahmed IAL, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103177
   Baek J, 2018, IEEE COMPUT SOC CONF, P1074, DOI 10.1109/CVPRW.2018.00142
   Bär A, 2021, IEEE SIGNAL PROC MAG, V38, P42, DOI 10.1109/MSP.2020.2983666
   Bescos B, 2021, IEEE T ROBOT, V37, P433, DOI 10.1109/TRO.2020.3031267
   Bódis-Szomorú A, 2017, COMPUT VIS IMAGE UND, V157, P3, DOI 10.1016/j.cviu.2016.06.002
   Bosilj P, 2020, J FIELD ROBOT, V37, P7, DOI 10.1002/rob.21869
   Lo Bianco LC, 2020, ROBOT AUTON SYST, V133, DOI 10.1016/j.robot.2020.103623
   Cavagna A, 2021, IEEE T PATTERN ANAL, V43, P1394, DOI 10.1109/TPAMI.2019.2946796
   Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823
   Choi W, 2015, INT J COMPUT VISION, V112, P204, DOI 10.1007/s11263-014-0779-4
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ghahremani M, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106240
   GIBSON EJ, 1960, SCI AM, V202, P64, DOI 10.1038/scientificamerican0460-64
   Guindel C, 2018, IEEE INTEL TRANSP SY, V10, P74, DOI 10.1109/MITS.2018.2867526
   Gupta A., 2010, ADV NEURAL INFORM PR, P1288
   HE ZJ, 1995, P NATL ACAD SCI USA, V92, P11155, DOI 10.1073/pnas.92.24.11155
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Hoffman J, 2016, Arxiv, DOI arXiv:1612.02649
   Jiang HZ, 2018, LECT NOTES COMPUT SC, V11215, P20, DOI 10.1007/978-3-030-01252-6_2
   Koenderink JJ, 1996, PERCEPT PSYCHOPHYS, V58, P163, DOI 10.3758/BF03211873
   Kothandaraman D, 2021, IEEE INT CONF COMP V, P3949, DOI 10.1109/ICCVW54120.2021.00442
   Kreso I, 2021, IEEE T INTELL TRANSP, V22, P4951, DOI 10.1109/TITS.2020.2984894
   Lee JK, 2019, INT J COMPUT VISION, V127, P1426, DOI 10.1007/s11263-019-01196-y
   Magerand L, 2020, IEEE T PATTERN ANAL, V42, P430, DOI 10.1109/TPAMI.2018.2849973
   Mathibela B, 2015, IEEE T INTELL TRANSP, V16, P2072, DOI 10.1109/TITS.2015.2393715
   Metzger KA, 2021, INT C PATT RECOG, P7892, DOI 10.1109/ICPR48806.2021.9411987
   Mittal S, 2021, IEEE T PATTERN ANAL, V43, P1369, DOI 10.1109/TPAMI.2019.2960224
   Orsic M, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107611
   Ren YZ, 2017, LECT NOTES COMPUT SC, V10115, P36, DOI 10.1007/978-3-319-54193-8_3
   Shariati A, 2019, IEEE ROBOT AUTOM LET, V4, P950, DOI 10.1109/LRA.2019.2893417
   Straub J, 2018, IEEE T PATTERN ANAL, V40, P235, DOI 10.1109/TPAMI.2017.2662686
   Tassis LM, 2021, COMPUT ELECTRON AGR, V186, DOI 10.1016/j.compag.2021.106191
   Wang LP, 2022, IEEE T INTELL TRANSP, V23, P8544, DOI 10.1109/TITS.2021.3083572
   Wang LP, 2020, IEEE T IMAGE PROCESS, V29, P9345, DOI 10.1109/TIP.2020.3026628
   Wang LP, 2020, IEEE-CAA J AUTOMATIC, V7, P1190, DOI 10.1109/JAS.2020.1003117
   Wang LP, 2020, ENG APPL ARTIF INTEL, V90, DOI 10.1016/j.engappai.2020.103569
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Wei H, 2018, PATTERN RECOGN, V81, P497, DOI 10.1016/j.patcog.2018.04.017
   Wei H, 2018, IEEE T IMAGE PROCESS, V27, P3164, DOI 10.1109/TIP.2018.2818931
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang WD, 2020, IEEE T CYBERNETICS, V50, P2730, DOI 10.1109/TCYB.2019.2895837
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zou CH, 2021, INT J COMPUT VISION, V129, P1410, DOI 10.1007/s11263-020-01426-8
NR 46
TC 0
Z9 0
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 30
PY 2023
DI 10.1007/s00371-023-03133-0
EA OCT 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W4GJ3
UT WOS:001091222300001
DA 2024-07-18
ER

PT J
AU Zhang, ZK
   Yin, SF
   Cao, LC
AF Zhang, Zikang
   Yin, Songfeng
   Cao, Liangcai
TI Age-invariant face recognition based on identity-age shared features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face recognition; Cross-age; Deep learning; Correlation analysis
AB Decoupling the mixed face features to obtain identity features that are not disturbed by age information is the key to achieving cross-age face recognition. Established mainstream identity feature extraction methods decouple facial representations into two components: age-related features and identity-related features, ignoring the fact that some identity-age shared features (ISF) exist, resulting in incomplete extracted identity features and limiting the further improvement of cross-age identity recognition accuracy. We redefine the component of facial representations and propose to introduce identity-age shared features to complement identity features. Specifically, the mixed face features are decoupled into pure age features, pure identity features, and identity-age shared features. The pure identity features and identity-age shared features are coupled to obtain the complete identity features. In addition, a two-stage constraint algorithm is developed to reduce the interference of age-related information on identity recognition and to enhance the independence between identity and age features. Experimental results on benchmark datasets for face aging (FG-NET, AGE-DB30, CALFW and CACD-VS) show that the proposed ISF outperforms state-of-the-art AIFR approaches, including DAL and MTLFace.
C1 [Zhang, Zikang; Yin, Songfeng] Tsinghua Univ, Hefei Inst Publ Safety Res, Hefei 230601, Peoples R China.
   [Cao, Liangcai] Tsinghua Univ, Dept Precis Instruments, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Yin, SF (corresponding author), Tsinghua Univ, Hefei Inst Publ Safety Res, Hefei 230601, Peoples R China.
EM zhangzk@stu.ahjzu.edu.cn; yinsongfeng@tsinghua-hf.edu.cn;
   clc@mail.tsinghua.edu.cn
RI Yin, Songfeng/KRQ-7135-2024
FU This study is supported by the Key Research Development Program of Anhui
   Province (Grant No. 202004d07020006). [202004d07020006]; Key Research
   Development Program of Anhui Province
FX This study is supported by the Key Research Development Program of Anhui
   Province (Grant No. 202004d07020006).
CR Afroze S, 2019, ICIGP 2019: PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING / 2019 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY, P8, DOI 10.1145/3313950.3313961
   Alsubai S, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104545
   An X, 2021, IEEE INT CONF COMP V, P1445, DOI 10.1109/ICCVW54120.2021.00166
   Andrew G., 2013, ICML, P1247
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Bahroun S., 2021, Vis. Comput, V66, P1
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Duong CN, 2017, IEEE I CONF COMP VIS, P3755, DOI 10.1109/ICCV.2017.403
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fu YW, 2016, IEEE T PATTERN ANAL, V38, P563, DOI 10.1109/TPAMI.2015.2456887
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Gong DH, 2013, IEEE I CONF COMP VIS, P2872, DOI 10.1109/ICCV.2013.357
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Huang ZZ, 2023, IEEE T PATTERN ANAL, V45, P7917, DOI 10.1109/TPAMI.2022.3217882
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Wen YD, 2016, PROC CVPR IEEE, P4893, DOI 10.1109/CVPR.2016.529
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xie JC, 2022, IEEE T INF FOREN SEC, V17, P399, DOI 10.1109/TIFS.2022.3142998
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zheng TY, 2017, Arxiv, DOI arXiv:1708.08197
   Zheng TY, 2017, IEEE COMPUT SOC CONF, P503, DOI 10.1109/CVPRW.2017.77
NR 36
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 8
PY 2023
DI 10.1007/s00371-023-03116-1
EA OCT 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T4FU5
UT WOS:001077570200002
DA 2024-07-18
ER

PT J
AU Mahalle, VS
   Kandoi, NM
   Patil, SB
AF Mahalle, Vishwanath S.
   Kandoi, Narendra M.
   Patil, Santosh B.
TI A powerful method for interactive content-based image retrieval by
   variable compressed convolutional info neural networks
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE User feedback; Neural network; Relevant images; Interactive
   content-based image retrieval; Deep learning; Feature extraction;
   Similarity matching
ID FEATURE DESCRIPTOR; TEXTURE FEATURE; TRANSFORM; WAVELET; FUZZY
AB There is a need for efficient methods to retrieve and obtain the visual data that a client need. New methods for content-based image retrieval (CBIR) have emerged due to recent developments in deep neural networks. However, there are still issues with deep neural networks in interactive CBIR systems like the search goal needs to be preset, scrambling and the computational cost is too high for an online environment. By this concern, this manuscript proposes an effective interactive CBIR that accurately retrieves images in response to the image query using variable compressed convolutional info neural networks (VCCINN). The weight of neural network is optimized by the variable info algorithm, and the matching activity is done by recursive density matching. The interactive technique eliminates irrelevant images based on user feedback and only the relevant images are finally retrieved. The overall retrieval performance in caltech-101 (dataset 1) and inria holiday (dataset 2) are 98.17% and 99% respectively. The performance of introduced model is proven by conducting ablation experiment on each component. The differential learning-based introduced image retrieval approach outperforms several existing methods regarding image similarity and retrieval speed.
C1 [Mahalle, Vishwanath S.; Kandoi, Narendra M.] Shri Sant Gajanan Maharaj Coll Engn, Dept Comp Sci & Engn, Shegaon 444203, Maharashtra, India.
   [Patil, Santosh B.] Shri St Gajanan Maharaj Coll Engn, Dept Elect & Telecommun Engn, Shegaon 444203, Maharashtra, India.
RP Mahalle, VS (corresponding author), Shri Sant Gajanan Maharaj Coll Engn, Dept Comp Sci & Engn, Shegaon 444203, Maharashtra, India.
EM vsmahalle@ssgmce.ac.in; nmkandoi@ssgmce.ac.in; sbpatil@ssgmce.ac.in
FU None.
FX None.
CR Ahmadianfar I, 2022, EXPERT SYST APPL, V195, DOI 10.1016/j.eswa.2022.116516
   Alshehri M, 2020, ARAB J SCI ENG, V45, P2957, DOI 10.1007/s13369-019-04235-5
   Anandababu P, 2020, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES (ICICT-2020), P424, DOI 10.1109/icict48043.2020.9112503
   Angulakshmi M, 2018, INT J IMAG SYST TECH, V28, P254, DOI 10.1002/ima.22276
   Arya R, 2019, KNOWL INF SYST, V60, P327, DOI 10.1007/s10115-018-1243-5
   Ashraf R, 2018, J MED SYST, V42, DOI 10.1007/s10916-017-0880-7
   Challa SK, 2022, VISUAL COMPUT, V38, P4095, DOI 10.1007/s00371-021-02283-3
   Chavda S., 2020, SN Comput. Sci., V1, P305
   Chhabra P, 2020, NEURAL COMPUT APPL, V32, P2725, DOI 10.1007/s00521-018-3677-9
   Das P, 2021, IRBM, V42, P245, DOI 10.1016/j.irbm.2020.06.007
   Eisenstat J, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10020140
   Garg M, 2021, NEURAL COMPUT APPL, V33, P1311, DOI 10.1007/s00521-020-05017-z
   Ghozzi Y, 2022, IEEE T FUZZY SYST, V30, P805, DOI 10.1109/TFUZZ.2021.3049900
   Giveki D, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.6533
   He D, 2023, VISUAL COMPUT, V39, P1423, DOI 10.1007/s00371-022-02420-6
   Hor N, 2019, Arxiv, DOI [arXiv:1912.12978, 10.48550/arXiv.1912.12978]
   Hussain S, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114545
   Iida K, 2020, IEEE ACCESS, V8, P200038, DOI 10.1109/ACCESS.2020.3035563
   Joseph Annrose, 2021, Arabian Journal of Geosciences, V14, DOI 10.1007/s12517-021-06990-y
   Kashif M, 2020, J DIGIT IMAGING, V33, P971, DOI 10.1007/s10278-020-00338-w
   Khan A, 2021, IEEE ACCESS, V9, P135608, DOI 10.1109/ACCESS.2021.3116225
   Khan UA, 2021, MULTIMED TOOLS APPL, V80, P26911, DOI 10.1007/s11042-021-10530-x
   Kumar M, 2018, MULTIMED TOOLS APPL, V77, P21557, DOI 10.1007/s11042-017-5587-8
   Kumar RB, 2020, MULTIMED TOOLS APPL, V79, P22277, DOI 10.1007/s11042-020-08953-z
   Kundu MK, 2012, INT J MACH LEARN CYB, V3, P285, DOI 10.1007/s13042-011-0062-8
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liu GH, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/8876480
   Mistry Yogita, 2018, Journal of Electrical Systems and Information Technology, V5, P874, DOI 10.1016/j.jesit.2016.12.009
   Monowar MM, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062188
   Öztürk S, 2023, INFORM SCIENCES, V637, DOI 10.1016/j.ins.2023.118938
   Panigrahi Kshyanaprava Panda, 2020, Progress in Computing, Analytics and Networking. Proceedings of ICCAN 2019. Advances in Intelligent Systems and Computing (AISC 1119), P659, DOI 10.1007/978-981-15-2414-1_66
   Patil Dakshata, 2019, 2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon), P155, DOI 10.1109/COMITCon.2019.8862446
   Pradhan J, 2020, VISUAL COMPUT, V36, P1847, DOI 10.1007/s00371-019-01773-9
   Punithavathi R, 2021, MULTIMED TOOLS APPL, V80, P26889, DOI 10.1007/s11042-021-10998-7
   Putzu L, 2020, MULTIMED TOOLS APPL, V79, P26995, DOI 10.1007/s11042-020-09292-9
   Raghuwanshi G, 2018, MULTIMED TOOLS APPL, V77, P23389, DOI 10.1007/s11042-018-5628-y
   Rajesh M.B., 2020, 2020 4 INT C COMP ME
   Reenadevi R., 2021, ANN ROMANIAN SOC CEL, V25, P5866
   Saritha RR, 2019, CLUSTER COMPUT, V22, pS4187, DOI 10.1007/s10586-018-1731-0
   Sathiamoorthy S, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1941-y
   Sezavar A, 2019, MULTIMED TOOLS APPL, V78, P20895, DOI 10.1007/s11042-019-7321-1
   Shahid F, 2021, NEURAL COMPUT APPL, V33, P13767, DOI 10.1007/s00521-021-06016-4
   Sharma A., 2020, Proc. Comput. Sci, V173, P181, DOI [10.1016/j.procs.2020.06.022, DOI 10.1016/J.PROCS.2020.06.022]
   Singh S, 2020, MULTIMED TOOLS APPL, V79, P17731, DOI 10.1007/s11042-019-08401-7
   Singhal A, 2021, MULTIMED TOOLS APPL, V80, P15901, DOI 10.1007/s11042-020-10319-4
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Reddy TS, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/6781740
   Sunitha T, 2021, EARTH SCI INFORM, V14, P1847, DOI 10.1007/s12145-021-00629-y
   Tang H, 2023, Arxiv, DOI arXiv:2308.03063
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tuyet VTH, 2021, MOBILE NETW APPL, V26, P1300, DOI 10.1007/s11036-021-01762-0
   Vaccaro F, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-021-01260-z
   Vard A, 2011, EXPERT SYST APPL, V38, P11722, DOI 10.1016/j.eswa.2011.03.058
   Vharkate MN, 2021, INT J REMOTE SENS, V42, P5540, DOI 10.1080/01431161.2021.1925373
   Wang XL, 2023, ANN OPER RES, DOI 10.1007/s10479-023-05480-6
   Wang Y, 2022, MULTIMED TOOLS APPL, V81, P31219, DOI 10.1007/s11042-022-12880-6
   Wang ZD, 2021, MULTIMEDIA SYST, V27, P403, DOI 10.1007/s00530-020-00734-w
   Xu YY, 2019, IEEE ACCESS, V7, P160082, DOI 10.1109/ACCESS.2019.2951175
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yang HS, 2023, VISUAL COMPUT, V39, P2177, DOI 10.1007/s00371-022-02472-8
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang K, 2022, COMPUT BIOL MED, V140, DOI 10.1016/j.compbiomed.2021.105096
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   US
NR 70
TC 0
Z9 0
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 1
PY 2023
DI 10.1007/s00371-023-03104-5
EA OCT 2023
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1IK0
UT WOS:001075590000002
DA 2024-07-18
ER

PT J
AU Presnov, D
   Berels, M
   Kolb, A
AF Presnov, Dmitri
   Berels, Marlena
   Kolb, Andreas
TI Pacemod: parametric contour-based modifications for glyph generation
SO VISUAL COMPUTER
LA English
DT Article
DE Glyphs; Icon-based visualization; Contour modification; Visualization
   design; Visual encoding
ID VISUALIZATION; REPRESENTATION
AB Metaphoric glyphs are intuitively related to the underlying problem domain and enhance the readability and learnability of the resulting visualization. Their construction, however, implies an appropriate modification of the base icon, which is a predominantly manual process. In this paper, we introduce the parametric contour-based modification (PACEMOD) approach that lays the foundations of automated, controllable icon manipulations. Technically, the PACEMOD parametric representation utilizes diffusion curves, enriched with new degrees of freedom in arc-length parameterization, which allows for manipulation of the icon contours' geometry and the related color attributes. Moreover, we propose an implementation of our generic approach for a specific, automated design of metaphoric glyphs, based on periodic, wave-like contour modifications. Finally, the practicality of such periodic contour modifications is demonstrated by two visualization examples, which comprise uncertainty visualization of a rain forecast and gradient glyphs applied to COVID-19 data. In summary, with the PACEMOD approach we introduce an instrument that facilitates a user-centered design of metaphoric glyphs and provides a generic basis for potential further implementations according to specific applications.
C1 [Presnov, Dmitri; Berels, Marlena; Kolb, Andreas] Univ Siegen, Comp Graph & Multimedia Syst Grp, Siegen, Germany.
C3 Universitat Siegen
RP Presnov, D (corresponding author), Univ Siegen, Comp Graph & Multimedia Syst Grp, Siegen, Germany.
EM dmitri.presnov@uni-siegen.de; marlena.berels@student.uni-siegen.de;
   andreas.kolb@uni-siegen.de
RI Kolb, Andreas/A-2067-2012
OI , Dmitri/0000-0001-9308-4685
FU Projekt DEAL; Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) [262513311 SFB 1187]
FX Open Access funding enabled and organized by Projekt DEAL. This study
   was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) - Project-ID 262513311 -SFB 1187
CR [Anonymous], 2014, Overview and State-of-the-Art of Uncertainty Visualization
   [Anonymous], 2010, P INT S NONPH AN REN
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   BOEHM W, 1980, COMPUT AIDED DESIGN, V12, P199, DOI 10.1016/0010-4485(80)90154-2
   Borgo R., 2013, P EUR, P39, DOI [DOI 10.2312/CONF/EG2013/STARS/039-063, 10.2312/CONF/EG2013/STARS/039-063]
   Cai Z, 2015, IEEE PAC VIS SYMP, P99, DOI 10.1109/PACIFICVIS.2015.7156363
   Carr DB., 1999, STAT COMPUTING GRAPH, V9, P4
   Casciola G., 2004, MULTIVARIATE APPROXI
   Chung DHS, 2016, COMPUT GRAPH FORUM, V35, P131, DOI 10.1111/cgf.12889
   Chung DHS, 2015, INFORM VISUAL, V14, P76, DOI 10.1177/1473871613511959
   Cunha J.M., 2018, P INT C CASE BASED R, V185
   Cunha JM, 2018, IEEE INT CON INF VIS, P71, DOI 10.1109/iV.2018.00023
   De Boor C., 1978, A practical guide to splines, DOI [10.1007/978-1-4612-6333-3, DOI 10.1007/978-1-4612-6333-3]
   de Oliveira MCF, 2003, IEEE T VIS COMPUT GR, V9, P378, DOI 10.1109/TVCG.2003.1207445
   Draper GM, 2009, IEEE T VIS COMPUT GR, V15, P759, DOI 10.1109/TVCG.2009.23
   Elder JH, 1998, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.1998.698633
   FLURY B, 1981, J AM STAT ASSOC, V76, P757, DOI 10.2307/2287565
   FOWLER B, 1993, IEEE COMPUT GRAPH, V13, P43, DOI 10.1109/38.232098
   Fuchs Johannes, 2015, 6th International Conference on Information Visualization Theory and Applications (VISIGRAPP 2015). Proceedings, P195
   Fuchs J, 2017, IEEE T VIS COMPUT GR, V23, P1863, DOI 10.1109/TVCG.2016.2549018
   Gerrits T., 2017, J WSCG, V25, P31
   Görtler J, 2018, IEEE T VIS COMPUT GR, V24, P719, DOI 10.1109/TVCG.2017.2743959
   He SS, 2015, J COMPUT DES ENG, V2, P218, DOI 10.1016/j.jcde.2015.06.002
   Holliman N.S., 2019, ARXIV
   Jeschke S, 2011, COMPUT GRAPH FORUM, V30, P523, DOI 10.1111/j.1467-8659.2011.01877.x
   Jeschke S, 2016, COMPUT GRAPH FORUM, V35, P71, DOI 10.1111/cgf.12812
   Jeschke S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618462
   Kehrer J, 2013, IEEE T VIS COMPUT GR, V19, P495, DOI 10.1109/TVCG.2012.110
   Keim DA, 1996, IEEE T KNOWL DATA EN, V8, P923, DOI 10.1109/69.553159
   Legg PA, 2012, COMPUT GRAPH FORUM, V31, P1255, DOI 10.1111/j.1467-8659.2012.03118.x
   Li YN, 2015, 8TH INTERNATIONAL SYMPOSIUM ON VISUAL INFORMATION COMMUNICATION AND INTERACTION (VINCI 2015), P121, DOI 10.1145/2801040.2801062
   Liu DY, 2017, IEEE T VIS COMPUT GR, V23, P1, DOI 10.1109/TVCG.2016.2598432
   Lu SF, 2020, IEEE ACCESS, V8, P57158, DOI 10.1109/ACCESS.2020.2982457
   Lu SF, 2019, VISUAL COMPUT, V35, P1027, DOI 10.1007/s00371-019-01671-0
   Maguire E, 2012, IEEE T VIS COMPUT GR, V18, P2603, DOI 10.1109/TVCG.2012.271
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   Nocke T, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P103, DOI 10.1109/IV.2005.58
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   OSORIO R.A., 2008, 6th Theory and Practice of Computer Graphics Conference, P59, DOI DOI 10.2312/LOCALCHAPTEREVENTS/TPCG/TPCG08/059-065
   Prassni JS, 2010, IEEE T VIS COMPUT GR, V16, P1358, DOI 10.1109/TVCG.2010.208
   Presnov D, 2022, J IMAGING, V8, DOI 10.3390/jimaging8110311
   Risch J.S., 2008, ARXIV
   Robert Koch Institut, 2022, COVID 19 DASHB
   Robert Koch Institute, 2022, COR SARS COV 2
   Siirtola H, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P869, DOI 10.1109/IV.2005.123
   STEVENS SS, 1963, J EXP PSYCHOL, V66, P177, DOI 10.1037/h0044984
   Sun T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601187
   Ward MO, 2008, Handbook of Data Visualization, P179, DOI [DOI 10.1007/978-3-540-33037-0_8, DOI 10.1007/978-3-540-33037-08, 10.1007/978-3-540-33037-08]
   Wickham H, 2012, ENVIRONMETRICS, V23, P382, DOI 10.1002/env.2152
   Wilkinson F, 1998, VISION RES, V38, P3555, DOI 10.1016/S0042-6989(98)00039-X
   Xie GF, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661275
   Ying L., 2021, IEEE T VIS COMPUT GR
NR 53
TC 1
Z9 1
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3371
EP 3384
DI 10.1007/s00371-023-03040-4
EA AUG 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001047709300001
OA hybrid
DA 2024-07-18
ER

PT J
AU Shao, WZ
   Deng, HS
   Luo, WW
   Li, JY
   Liu, ML
AF Shao, Wen-Ze
   Deng, Hai-Song
   Luo, Wei-Wei
   Li, Jin-Ye
   Liu, Mei-Lin
TI Revisiting reweighted graph total variation blind deconvolution and
   beyond
SO VISUAL COMPUTER
LA English
DT Article
DE Blind deconvolution; Deblurring; Variational PDEs; Signal processing on
   graphs; Face hallucination
ID IMAGE; REGULARIZATION; REPRESENTATION
AB It is known that image priors are essential to blind deconvolution. Reweighted graph total variation (RGTV), as a new prior to substitute the most classical total variation (TV), has been shown superior to TV and several other cutting-edge models both theoretically and empirically. This paper steps forward, firstly providing a simpler geometric perspective to RGTV in the framework of variational partial differential equations (PDEs), rather than the previous graph spectral interpretation made in the graph frequency domain. Surprisingly, a slight shift of perspective as such finally leads to a huge blind deblurring performance boosting in both accuracy and efficiency as compared to the previously derived numerical approach, which approximates RGTV as the graph L1-Laplacian regularizer. Utilizing the simplified RGTV as reformulated in this paper, another valuable contribution is an exploration of its potentials for blind facial image restoration by combining unsupervised deep facial models. Experimental results of blind face deblurring and blind face hallucination both demonstrate necessity and rationale of a joint model-based and learning-based approach to blind face restoration.
C1 [Shao, Wen-Ze; Luo, Wei-Wei; Li, Jin-Ye; Liu, Mei-Lin] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Deng, Hai-Song] Nanjing Audit Univ, Sch Data Sci & Stat, Nanjing 211815, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing Audit
   University
RP Shao, WZ (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
EM shaowenze@njupt.edu.cn
FU Natural Science Foundation of China [61771250, 61972213, 11901299];
   Fundamental Research Funds for the Central Universities [30918014108]
FX AcknowledgementsThe work was supported in part by the Natural Science
   Foundation of China (61771250, 61972213, 11901299) and in part by the
   Fundamental Research Funds for the Central Universities (30918014108).
CR Asim M, 2020, IEEE T COMPUT IMAG, V6, P1493, DOI 10.1109/TCI.2020.3032671
   Aubert G., 2002, MATH PROBLEMS IMAGE, DOI [10.1007/b97428, DOI 10.1007/B97428]
   Bai YC, 2019, IEEE T IMAGE PROCESS, V28, P1404, DOI 10.1109/TIP.2018.2874290
   Berger P, 2017, IEEE J-STSP, V11, P842, DOI 10.1109/JSTSP.2017.2726978
   Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126
   Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Couprie C, 2013, SIAM J IMAGING SCI, V6, P1246, DOI 10.1137/120895068
   Dong JX, 2017, IEEE I CONF COMP VIS, P2497, DOI 10.1109/ICCV.2017.271
   Elmoataz A, 2008, IEEE T IMAGE PROCESS, V17, P1047, DOI 10.1109/TIP.2008.924284
   Eqtedaei A, 2024, VISUAL COMPUT, V40, P333, DOI 10.1007/s00371-023-02785-2
   Hidane M, 2013, J MATH IMAGING VIS, V45, P114, DOI 10.1007/s10851-012-0348-9
   Huber P., 2009, Robust statistics, DOI DOI 10.1002/9780470434697
   Jinshan Pan, 2013, IEEE Signal Processing Letters, V20, P841, DOI 10.1109/LSP.2013.2261986
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Li LRH, 2019, INT J COMPUT VISION, V127, P1025, DOI 10.1007/s11263-018-01146-0
   Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804
   Pan JS, 2017, IEEE I CONF COMP VIS, P1077, DOI 10.1109/ICCV.2017.122
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400
   Parvaz R, 2023, VISUAL COMPUT, V39, P2653, DOI 10.1007/s00371-022-02484-4
   Ren DW, 2020, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR42600.2020.00340
   Sapiro G., 2001, GEOMETRIC PARTIAL DI, DOI DOI 10.1017/CBO9780511626319
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Shearer P, 2013, IEEE IMAGE PROC, P572, DOI 10.1109/ICIP.2013.6738118
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wen F, 2021, IEEE T CIRC SYST VID, V31, P2923, DOI 10.1109/TCSVT.2020.3034137
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu ZH, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116050
   Yan YY, 2017, PROC CVPR IEEE, P6978, DOI 10.1109/CVPR.2017.738
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
NR 41
TC 1
Z9 1
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3119
EP 3135
DI 10.1007/s00371-023-03014-6
EA AUG 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001045114100002
DA 2024-07-18
ER

PT J
AU Du, SY
   Song, Y
AF Du, Shiyin
   Song, Ying
TI Multi-exemplar-guided image weathering via texture synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based weathering; Texture synthesis; Image processing; Image
   generation
AB We propose a novel method for generating gradually varying weathering effects from a single image. Time-variant weathering effects tend to appear simultaneously on one object. Compared to previous methods, our method is able to obtain gradually changing weathering effects through simple interactions while keeping texture variations and shading details. We first classify the weathering regions into several stages based on a weathering degree map extracted from the image. For each weathering stage, we automatically extract the corresponding weathering sample, from which a texture image is synthesized subsequently. Then, we generate weathering effects by fusing different textures according to the weathering degree of the image pixels. Finally, in order to maintain the intrinsic shape details of the object during the fusing process, we utilize a new shading-preserving method taking account of the weathering degrees. Experiments show that our method is able to produce visually realistic and time-variant weathering effects interactively.
C1 [Du, Shiyin; Song, Ying] Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310000, Peoples R China.
C3 Zhejiang Sci-Tech University
RP Song, Y (corresponding author), Zhejiang Sci Tech Univ, Sch Comp Sci & Technol, Hangzhou 310000, Peoples R China.
EM ysong@zstu.edu.cn
OI , Ying/0000-0003-4813-0245
FU Key Ramp;D Plan of Zhejiang Province [2023C01047, 2023C01041]
FX This work has been supported by Key R & D Plan of Zhejiang Province Nos.
   2023C01047 and 2023C01041.
CR [Anonymous], 2011, P COMP GRAPH INT OTT
   Bandeira D, 2009, SIBGRAPI, P32, DOI 10.1109/SIBGRAPI.2009.38
   Bellini R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925891
   Bosch C, 2019, COMPUT GRAPH FORUM, V38, P274, DOI 10.1111/cgf.13530
   Bosch C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966399
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chen L.Y., 2021, SIGGRAPH AS 2021 TEC, P1
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Desbenoit B, 2004, COMPUT GRAPH FORUM, V23, P341, DOI 10.1111/j.1467-8659.2004.00765.x
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P411, DOI 10.1145/237170.237280
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Endo Y, 2010, LECT NOTES COMPUT SC, V6133, P160, DOI 10.1007/978-3-642-13544-6_16
   Houdard A., 2021, Scale Space and Variational Methods in Computer Vision, P269
   Iizuka S, 2016, COMPUT GRAPH FORUM, V35, P501, DOI 10.1111/cgf.12850
   Ishitobi A, 2020, VISUAL COMPUT, V36, P2383, DOI 10.1007/s00371-020-01947-w
   Jain N., 2016, 2016 S COLOSSAL DATA, P1, DOI DOI 10.1109/CDAN.2016.7570950
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kider JT, 2011, COMPUT GRAPH FORUM, V30, P257, DOI 10.1111/j.1467-8659.2011.01857.x
   Lasram Anass, 2012, P 4 ACM SIGGRAPHEURO, P115
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Muñoz-Pandiella I, 2018, IEEE T VIS COMPUT GR, V24, P3239, DOI 10.1109/TVCG.2018.2794526
   Santos RLC, 2023, VISUAL COMPUT, V39, P1943, DOI 10.1007/s00371-022-02457-7
   Taka E., 2017, 2017 3DTV C TRUE VIS, P1
   Wang JP, 2006, ACM T GRAPHIC, V25, P754, DOI 10.1145/1141911.1141951
   Xue S, 2008, COMPUT GRAPH FORUM, V27, P617
   Xue S, 2011, COMPUT GRAPH FORUM, V30, P1189, DOI 10.1111/j.1467-8659.2011.01977.x
   Zhou Y, 2017, COMPUT GRAPH FORUM, V36, P199, DOI 10.1111/cgf.13119
NR 30
TC 1
Z9 1
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3691
EP 3699
DI 10.1007/s00371-023-02944-5
EA JUN 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019713300001
DA 2024-07-18
ER

PT J
AU Xia, HY
   Lu, LD
   Song, SX
AF Xia, Haiying
   Lu, Lidan
   Song, Shuxiang
TI Feature fusion of multi-granularity and multi-scale for facial
   expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Multi-granularity; Multi-scale feature;
   Occlusion and pose variation
AB Although great progress has been made in facial expression recognition, it still faces challenges such as occlusion and pose changes in real-world scenario. To address this issue, we propose a simple yet effective multi-granularity and multi-scale feature fusion network (MM-Net) to achieve robust expression recognition without either manually extracting local patches or designing complex sub-networks. Specifically, we use a puzzle generator to divide the image into local regions of different granularity, which are then randomly shuffled and reorganized to form a new input image. By feeding the facial puzzles in order from fine-grained to coarse-grained, the network progressively mines the local fine-grained information, the coarse-grained information, and the global information. Besides, considering the subtle inter-class variation characteristic of different expressions, we use the multi-scale feature fusion strategy in the shallow feature extraction module to obtain global features with detailed information for capturing the subtle differences in facial expression images. Extensive experimental results on three in-the-wild FER benchmarks demonstrate the superiority of the proposed MM-Net compared to state-of-the-art methods.
C1 [Xia, Haiying; Lu, Lidan; Song, Shuxiang] Guangxi Normal Univ, Sch Elect & Informat Engn, Guilin 541004, Peoples R China.
C3 Guangxi Normal University
RP Song, SX (corresponding author), Guangxi Normal Univ, Sch Elect & Informat Engn, Guilin 541004, Peoples R China.
EM xhy22@mailbox.gxnu.edu.cn; 1124069442@qq.com;
   songshuxiang@mailbox.gxnu.edu.cn
RI Yao, Chen/JVD-6226-2023
FU National Natural Science Foundation of China [62106054]; Science and
   Technology Project of Guangxi [2018GXNSFAA281351]; Research Projects of
   Guangxi Normal University (Natural Sciences) [2021JC012]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China (Grant Nos.62106054), the Science and Technology
   Project of Guangxi (Grant 2018GXNSFAA281351) and the Research Projects
   of Guangxi Normal University (Natural Sciences) (2021JC012).
CR Adil B, 2019, 2019 4TH INTERNATIONAL CONFERENCE ON NETWORKING AND ADVANCED SYSTEMS (ICNAS 2019), P69, DOI 10.1109/icnas.2019.8807883
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Cai J, 2021, IEEE IMAGE PROC, P1344, DOI 10.1109/ICIP42928.2021.9506593
   Chattopadhay Aditya, 2018, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P839, DOI 10.1109/WACV.2018.00097
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cheng CL, 2017, 2017 12TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND KNOWLEDGE ENGINEERING (IEEE ISKE), DOI 10.1109/ULTSYM.2017.8091659
   Ding H, 2020, IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020), DOI 10.1109/ijcb48548.2020.9304923
   Duta I. C., 2020, ARXIV
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Gao HX, 2023, NEURAL NETWORKS, V158, P228, DOI 10.1016/j.neunet.2022.11.025
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hammal Z, 2009, J VISION, V9, DOI 10.1167/9.2.22
   Hazourli AR, 2020, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kim Y., 2017, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liao L, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01288-9
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan BW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P566, DOI 10.1145/3343031.3351049
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Paszke A, 2019, ADV NEUR IN, V32
   Cornejo JYR, 2016, INT CONF ACOUST SPEE, P1298, DOI 10.1109/ICASSP.2016.7471886
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siqueira H, 2020, AAAI CONF ARTIF INTE, V34, P5800
   Su C, 2023, PATTERN ANAL APPL, V26, P543, DOI 10.1007/s10044-022-01124-w
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia HY, 2021, IEEE MULTIMEDIA, V28, P20, DOI 10.1109/MMUL.2021.3076834
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang KH, 2017, IEEE T IMAGE PROCESS, V26, P4193, DOI 10.1109/TIP.2017.2689999
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
NR 49
TC 2
Z9 2
U1 5
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2035
EP 2047
DI 10.1007/s00371-023-02900-3
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001003038200002
DA 2024-07-18
ER

PT J
AU Li, YM
   Lin, GF
   He, ML
   Yuan, D
   Liao, KY
AF Li, Yumei
   Lin, Guangfeng
   He, Menglan
   Yuan, Dan
   Liao, Kaiyang
TI Layer similarity guiding few-shot Chinese style transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Few-shot text style transfer; Layer similarity characteristics; Text
   style; Text content
ID FONT GENERATION
AB Few-shot text style transfer faces two main challenges: The first challenge is the limited availability of reference style text, while the second challenge is the varying degrees of differences between the style reference image and the source image. Existing methods mainly focus on the influence of local style and global style feature extraction on text style, but they ignore the crucial role played by the difference between the style reference image and the source image on style characteristics, especially in Chinese, which has its own unique ideograph structure. To address this issue, this paper proposes Layer Similarity Guiding Few-shot Chinese Style Transfer (LSG-FCST). LSG-FCST can not only build a transfer network by encoding the content and style characteristics from low-level to high-level semantics, but it can also discover the similarity characteristics between the style reference image and the source image through the attention mechanism. Furthermore, LSG-FCST can integrate the style features generated by the similarity features of different layers and generate the target image through asymmetric decoding. In the self-built text image dataset, we consider three types of visibility situations for the test images in the training set: seen fonts unseen characters, unseen fonts seen characters, and unseen fonts unseen characters. The experiments show that LSG-FCST outperforms the state-of-the-art methods. The code and dataset can be accessed at https://github.com/LYM1111/ LSG-FCST.
C1 [Li, Yumei; Lin, Guangfeng; He, Menglan; Yuan, Dan; Liao, Kaiyang] Xian Univ Technol, Xian 710048, Peoples R China.
C3 Xi'an University of Technology
RP Lin, GF (corresponding author), Xian Univ Technol, Xian 710048, Peoples R China.
EM lgf78103@xaut.edu.cn
RI Lin, Guangfeng/E-4420-2013
OI Lin, Guangfeng/0000-0002-6191-1102
FU NSFC [61771386]; Research and Development Program of Shaanxi
   [2020SF-359, 2021JQ-487]
FX The authors would like to thank the anonymous reviewers for their
   insightful comments that help improve the quality of this paper.
   Especially, this work was supported by NSFC (Program No.61771386),
   Research and Development Program of Shaanxi (Program No.2020SF-359,
   No.2021JQ-487).
CR Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Baek K., 2021, P IEEE CVF INT C COM, P14154
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Cao B, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3236486
   Cao B, 2020, AAAI CONF ARTIF INTE, V34, P10486
   Cha J., 2020, ECCV, P735
   Chang B, 2018, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2018.00028
   Chang Jie, 2018, BMVC, P290
   Chen X, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P37, DOI 10.1145/3460426.3463606
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P4650, DOI 10.1109/TPAMI.2022.3193587
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Gao Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356574
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   HuANG Y., 2020, Lecture Notes in Computer Science, V12351, P156, DOI 10.1007/
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang X., 2023, P IEEE CVF WINT C AP, P5386
   Jiang Y, 2019, AAAI CONF ARTIF INTE, P4015
   Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440
   Köhler M, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3265051
   Kong YX, 2022, PROC CVPR IEEE, P13472, DOI 10.1109/CVPR52688.2022.01312
   Lang CB, 2023, IEEE T PATTERN ANAL, V45, P10669, DOI 10.1109/TPAMI.2023.3265865
   Lang CB, 2022, PROC CVPR IEEE, P8047, DOI 10.1109/CVPR52688.2022.00789
   Li CH, 2021, IEEE WINT CONF APPL, P433, DOI 10.1109/WACV48630.2021.00048
   Lin G., 2020, ARXIV
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu YB, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3262351
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Odena A, 2017, PR MACH LEARN RES, V70
   Park S., 2021, P IEEE CVF INT C COM, P13900
   Park S, 2021, AAAI CONF ARTIF INTE, V35, P2393
   Ren C., 2019, Austral. J. Intell. Inf. Process. Syst., V16, P19
   Sun D, 2017, ARXIV
   Taigman Y, 2016, ARXIV
   Tian Y., 2017, zi2zi: master Chinese calligraphy with conditional adversarial networks
   Tian Y. C, 2017, Rewrite: Neural style transfer for Chinese fonts
   Wen C, 2021, IEEE WINT CONF APPL, P3881, DOI 10.1109/WACV48630.2021.00393
   Wu YA, 2023, Statistical Signal P, P1, DOI 10.1109/SSP53291.2023.10208076
   Xie YC, 2021, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR46437.2021.00509
   Zeng JS, 2021, AAAI CONF ARTIF INTE, V35, P3270
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YF, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2088, DOI 10.1145/3503161.3548414
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu A, 2020, IEEE T IMAGE PROCESS, V29, P6932, DOI 10.1109/TIP.2020.2995062
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 45
TC 1
Z9 1
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2265
EP 2278
DI 10.1007/s00371-023-02915-w
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001004468700001
DA 2024-07-18
ER

PT J
AU Jiang, YN
   Lin, HW
   Huang, WX
AF Jiang, Yini
   Lin, Hongwei
   Huang, Weixian
TI Fairing-PIA: progressive-iterative approximation for fairing curve and
   surface generation
SO VISUAL COMPUTER
LA English
DT Article
DE Curve and surface fairing; Progressive-iterative approximation;
   Fairing-PIA; Energy minimization; Geometric iteration method
ID B-SPLINE CURVE; INTERPOLATION; CURVATURE; ENERGY; LOOP
AB The fairing curves and surfaces are used extensively in geometric design, modeling, and industrial manufacturing. However, the majority of conventional fairing approaches, which lack sufficient parameters to improve fairness, are based on energy minimization problems. In this study, we develop a novel progressive-iterative approximation method for the fairing curve and surface generation (fairing-PIA). Fairing-PIA is an iteration method that can generate a series of curves (surfaces) by adjusting the control points of B-spline curves (surfaces). In fairing-PIA, each control point is endowed with an individual weight. Thus, the fairing-PIA has many parameters to optimize the shapes of curves and surfaces. Not only a fairing curve (surface) can be generated globally through fairing-PIA, but also the curve (surface) can be improved locally. Moreover, we prove the convergence of the developed fairing-PIA and show that the conventional energy minimization fairing model is a special case of fairing-PIA. Finally, numerical examples indicate that the proposed method is effective and efficient.
C1 [Jiang, Yini; Lin, Hongwei] Zhejiang Univ, Sch Math Sci, Hangzhou 310058, Peoples R China.
   [Huang, Weixian] ZWSOFT, Guangzhou 510623, Peoples R China.
C3 Zhejiang University
RP Lin, HW (corresponding author), Zhejiang Univ, Sch Math Sci, Hangzhou 310058, Peoples R China.
EM hwlin@zju.edu.cn
OI Lin, Hongwei/0000-0002-9337-9624
FU National Natural Science Foundation of China [62272406, 61872316];
   National Key R D Plan of China [2020YFB1708900]
FX AcknowledgementsThis work is supported by the National Natural Science
   Foundation of China under Grant nos. 62272406, 61872316, and the
   National Key R &D Plan of China under Grant no. 2020YFB1708900.
CR Caliò F, 2010, INT J NUMER METH BIO, V26, P1674, DOI 10.1002/cnm.1253
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   de Boor C., 1979, P 1979 ARM NUM AN CO, P01
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Deng CY, 2012, COMPUT AIDED DESIGN, V44, P424, DOI 10.1016/j.cad.2011.12.001
   Finkelstein A., 1997, COMPUT GRAPHICS-US, V11
   Garin G., 1987, Computer-Aided Geometric Design, V4, P91, DOI 10.1016/0167-8396(87)90027-6
   Gilimyanov RF, 2011, AUTOMAT REM CONTR+, V72, P1548, DOI 10.1134/S0005117911070204
   Hagen H., 1991, Computer-Aided Geometric Design, V8, P393, DOI 10.1016/0167-8396(91)90012-Z
   Hamza YF, 2022, J COMPUT SCI TECH-CH, V37, P487, DOI 10.1007/s11390-020-0183-1
   Hamza YF, 2020, COMPUT AIDED GEOM D, V77, DOI 10.1016/j.cagd.2020.101817
   Hashemian A, 2018, COMPUT MATH APPL, V76, P1555, DOI 10.1016/j.camwa.2018.07.007
   Holladay J.C., 1957, MATH TABLES OTHER AI, V11, P233
   Hoschek J., 1993, FUNDAMENTALS COMPUTE
   Hu QQ, 2022, VISUAL COMPUT, V38, P3819, DOI 10.1007/s00371-021-02223-1
   KJELLANDER JAP, 1983, COMPUT AIDED DESIGN, V15, P175, DOI 10.1016/0010-4485(83)90085-4
   Li AM, 2012, APPL MECH MATER, V215-216, P1205, DOI 10.4028/www.scientific.net/AMM.215-216.1205
   Li AM, 2011, ADV MATER RES-SWITZ, V314-316, P1562, DOI 10.4028/www.scientific.net/AMR.314-316.1562
   Li W, 2005, J AIRCRAFT, V42, P1065, DOI 10.2514/1.10394
   Li WS, 2004, COMPUT AIDED GEOM D, V21, P499, DOI 10.1016/j.cagd.2004.03.004
   Lin HW, 2018, J SYST SCI COMPLEX, V31, P1618, DOI 10.1007/s11424-018-7443-y
   Lin HW, 2018, COMPUT AIDED DESIGN, V95, P40, DOI 10.1016/j.cad.2017.10.002
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Liu SJ, 2021, VISUAL COMPUT, V37, P2485, DOI 10.1007/s00371-021-02213-3
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Meier H., 1987, Computer-Aided Geometric Design, V4, P297, DOI 10.1016/0167-8396(87)90004-5
   POTTMANN H, 1990, COMPUT AIDED DESIGN, V22, P241, DOI 10.1016/0010-4485(90)90053-F
   Qi D, 1975, Acta Math Sin, V18, P173
   Rando T.C., 1990, THESIS U CONNECTICUT, Patent No. aAI9102032
   Sariöz E, 2006, OCEAN ENG, V33, P2105, DOI 10.1016/j.oceaneng.2005.11.014
   [史利民 SHI Limin], 2006, [数学研究与评论, Journal of Mathematical Research and Exposition], V26, P735
   Song B., 2021, APPL SOFT COMPUT, V100
   Vassilev TI, 1996, COMPUT AIDED DESIGN, V28, P753, DOI 10.1016/0010-4485(95)00087-9
   VELTKAMP RC, 1995, COMPUT GRAPH FORUM, V14, pC97, DOI 10.1111/j.1467-8659.1995.cgf143_0097.x
   Wang AZ, 2015, INT J WAVELETS MULTI, V13, DOI 10.1142/S0219691315500411
   Wang HD, 2022, VISUAL COMPUT, V38, P591, DOI 10.1007/s00371-020-02036-8
   Wang WY, 2010, COMPUT METHOD APPL M, V199, P290, DOI 10.1016/j.cma.2009.04.003
   Wang XF, 1997, COMPUT AIDED DESIGN, V29, P485, DOI 10.1016/S0010-4485(96)00087-5
   Wang Z., 2021, VISUAL COMPUT, V10
   [王志好 Wang Zhihao], 2018, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V30, P2035
   Westgaard G, 2001, COMPUT AIDED GEOM D, V18, P619, DOI 10.1016/S0167-8396(01)00057-7
   Zhang CM, 2001, COMPUT AIDED DESIGN, V33, P913, DOI 10.1016/S0010-4485(00)00114-7
   Zhang L, 2016, VISUAL COMPUT, V32, P1109, DOI 10.1007/s00371-015-1170-3
   Zhang YF, 2020, IEEE T MULTIMEDIA, V22, P1407, DOI 10.1109/TMM.2019.2943750
NR 45
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1467
EP 1484
DI 10.1007/s00371-023-02861-7
EA MAY 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000987542600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vensila, C
   Wesley, AB
AF Vensila, C.
   Wesley, A. Boyed
TI Multimodal biometrics authentication using extreme learning machine with
   feature reduction by adaptive particle swarm optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; Particle swarm optimization; Local binary pattern; Extreme
   learning machine
ID FEATURE-SELECTION; NEURAL-NETWORK; FUSION
AB The necessity for personal data protection has become increasingly critical in recent years. The single trait-based authentication fails in many cases and hence multimodal biometric traits are utilized in this work. This research work focuses on the utilization of multimodal biometric traits for authentication. The face, fingerprint, and finger vein traits are considered in this work, feature extraction was done by local binary pattern and for the optimization of features, adaptive particle swarm optimization was employed. The extreme learning machine was proposed here for the classification and proficient results were generated. For the iteration count of 40 and population count of 30 of the optimization algorithm, the authentication model sensitivity and specificity are 95.69 and 94.29%. The outcome of this research work paves the way toward proficient security authentication.
C1 [Vensila, C.] Marthandam Manonmaniam Sundaranar Univ, Christian Coll, Dept Comp Sci, Nesamony Mem, Tirunelveli 627012, Tamil Nadu, India.
   [Wesley, A. Boyed] Marthandam Manonmaniam Sundaranar Univ, Nesamony Mem Christian Coll, Dept PG Comp Sci, Tirunelveli 627012, Tamil Nadu, India.
C3 Manonmaniam Sundaranar University; Manonmaniam Sundaranar University
RP Vensila, C (corresponding author), Marthandam Manonmaniam Sundaranar Univ, Christian Coll, Dept Comp Sci, Nesamony Mem, Tirunelveli 627012, Tamil Nadu, India.
EM vensila.c@gmail.com; abwesley2003@gmail.com
CR Ahmadian K, 2013, VISUAL COMPUT, V29, P123, DOI 10.1007/s00371-012-0741-9
   Alay N, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20195523
   Alsaade F., 2010, SCI J KING FAISAL U, V11, P14
   Ammour B, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9010085
   Arora A., 2022, MULTIMED TOOLS APPL, V31, P1
   Chalabi NE., 2022, HDB NATURE INSPIRED, P85, DOI [10.1007/978-3-031-07516-2_5, DOI 10.1007/978-3-031-07516-2_5]
   Chen CH, 2013, I SYMP CONSUM ELECTR, P55
   Cherrat E, 2020, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.248
   Choudhury SH, 2021, APPL SOFT COMPUT, V106, DOI 10.1016/j.asoc.2021.107344
   Daas S, 2020, IET IMAGE PROCESS, V14, P3859, DOI 10.1049/iet-ipr.2020.0491
   El Mehdi Cherrat RA., 2020, J ELECT ENG COMPUTER, V18, P1562
   Elavarasi G., 2022, Proceedings of Data Analytics and Management: ICDAM 2021. Lecture Notes on Data Engineering and Communications Technologies (91), P831, DOI 10.1007/978-981-16-6285-0_64
   Eskandari M, 2014, SIGNAL IMAGE VIDEO P, V8, P995, DOI 10.1007/s11760-012-0411-4
   Fang W, 2010, IETE TECH REV, V27, P336, DOI 10.4103/0256-4602.64601
   Gayathri M, 2021, PATTERN RECOGN LETT, V152, P1, DOI 10.1016/j.patrec.2021.09.016
   Ghayoumi M, 2015, 2015 IEEE/ACIS 14TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS), P131, DOI 10.1109/ICIS.2015.7166582
   Ghoualmi Lamis, 2021, 2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP), P159, DOI 10.1109/ICSIP52628.2021.9688860
   Grosan C, 2007, STUD COMPUT INTELL, V75, P1, DOI 10.1007/978-3-540-73297-6
   Hammad M, 2019, IEEE ACCESS, V7, P26527, DOI 10.1109/ACCESS.2018.2886573
   Han F, 2021, SWARM EVOL COMPUT, V62, DOI 10.1016/j.swevo.2021.100847
   HUO G, 2015, J ELECTRON IMAGING, V24, DOI DOI 10.1117/1.JEI.24.6.063020
   Karthiga R, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1351-0
   Krishneswari K, 2013, J SCI IND RES INDIA, V72, P23
   Kumar A, 2016, INFORM FUSION, V32, P49, DOI 10.1016/j.inffus.2015.09.002
   Larabi-Marie-Sainte S, 2020, STUD INFORM CONTROL, V29, P99, DOI 10.24846/v29i1y202010
   Lim, 2009, CONSTRICTION FACTOR
   Maghooli K, 2004, LECT NOTES COMPUT SC, V3087, P332
   Mehraj Haider, 2020, International Journal of Cloud Computing, V9, P131
   Pawar M.D., 2021, P INT C IOT BAS CONT
   Rajasekar V, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-021-04652-3
   Shanmugavadivu P, 2014, VISUAL COMPUT, V30, P387, DOI 10.1007/s00371-013-0863-8
   Silva PH, 2018, IEEE C EVOL COMPUTAT, P2036, DOI 10.1109/CEC.2018.8477817
   Soleymani S, 2018, IEEE IMAGE PROC, P763, DOI 10.1109/ICIP.2018.8451532
   Soleymani S, 2018, INT C PATT RECOG, P3469, DOI 10.1109/ICPR.2018.8545061
   Song HK, 2020, BMC BIOINFORMATICS, V21, DOI 10.1186/s12859-020-03613-3
   Szczuko P, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062356
   Wang L, 2021, IEEE T NEUR NET LEAR, V32, P853, DOI 10.1109/TNNLS.2020.2979607
   Wong HS, 2022, IEEE T BIG DATA, V8, P1664, DOI 10.1109/TBDATA.2021.3079234
   Wu D, 2018, INT J COMPUT INT SYS, V11, P936, DOI 10.2991/ijcis.11.1.71
   Xiong JY, 2022, CHEM ENG RES DES, V183, P567, DOI 10.1016/j.cherd.2022.06.001
   Xiong Q, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10020217
   Yang J., 2013, BIOMETRIC RECOGNITIO, P433
   Yang JC, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10040096
   Zhou Y, 2023, IEEE T EVOLUT COMPUT, V27, P787, DOI 10.1109/TEVC.2022.3222297
   Zhou Y, 2021, SWARM EVOL COMPUT, V60, DOI 10.1016/j.swevo.2020.100770
   Zhou Y, 2021, INFORM SCIENCES, V547, P841, DOI 10.1016/j.ins.2020.08.083
   Zhou Y, 2020, INFORM SCIENCES, V532, P91, DOI 10.1016/j.ins.2020.05.004
NR 47
TC 1
Z9 1
U1 6
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1383
EP 1394
DI 10.1007/s00371-023-02856-4
EA MAY 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000982760900001
DA 2024-07-18
ER

PT J
AU Wang, XL
   Zhu, XY
   Yan, ZZ
   Ye, ZW
   Du, JY
   Guo, F
   Xu, ZG
   Liu, C
   Mao, CL
AF Wang, Xianglong
   Zhu, Xinyuan
   Yan, Zhongzhen
   Ye, Zhiwei
   Du, Jiangyi
   Guo, Feng
   Xu, Zhigang
   Liu, Chun
   Mao, Cailu
TI Image-only place recognition based on regional aggregating ConvNet
   features for underground parking lots
SO VISUAL COMPUTER
LA English
DT Article
DE Visual place recognition; Regional aggregating ConvNet feature;
   Underground parking lot localization
ID LOCALIZATION
AB Place recognition searches the closest map node to the query node, which is an important task for vehicle localization. Traditional visual place recognition methods for underground parking lots require the deployment of additional location signals, such as WiFi, Bluetooth. This paper utilizes only front-view images to realize place recognition. First, we employ a random coefficient to reduce the dimensionality of the ConvNet features to obtain the CCFs (Condense ConvNet Features). Second, we average the CCFs of a regional zone to obtain the RACF (Regional Aggregating ConvNet Feature). Compared with WiFi, Bluetooth, RACF is extracted from the front-view image and has a superior ability to represent regional zones. Third, we propose a multiscale place recognition method that adopts a coarse-to-fine strategy that greatly reduces time consumption and accelerates precision. Finally, we evaluate the proposed method on the data collected in the underground parking lot of Hubei University of Technology. The experimental results illustrate that the proposed method has high precision with a fast speed.
C1 [Wang, Xianglong; Zhu, Xinyuan; Yan, Zhongzhen; Ye, Zhiwei; Du, Jiangyi; Xu, Zhigang; Liu, Chun] Hubei Univ Technol, Sch Comp Sci, 28 Nanli Rd, Wuhan, Hubei, Peoples R China.
   [Guo, Feng] China Railway Seventh Grp Corp Ltd, Elect Engn Corp Ltd, Yingxie Rd, Zhengzhou, Peoples R China.
   [Mao, Cailu] Baokang Publ Secur Bur, 110 Qingxi Rd, Xiangyang, Hubei, Peoples R China.
C3 Hubei University of Technology
RP Xu, ZG (corresponding author), Hubei Univ Technol, Sch Comp Sci, 28 Nanli Rd, Wuhan, Hubei, Peoples R China.
EM xianglongwang@hbut.edu.cn; 2376435801@qq.com; 12339207@qq.com;
   hgcsyzw@mail.hbut.edu.cn; jydu@hbut.edu.cn; 1719734538@qq.com;
   trhxhyyfhb@21cn.com; liuchun.hust@gmail.com; 40032208@qq.com
RI Xu, Zhigang/AAC-7839-2019; zhu, xin/JXN-3188-2024; zhu, y
   x/IVU-7833-2023; 朱, 欣妍/JZD-6639-2024
FU National Natural Science Foundation of China [61772180, 61502155];
   Project of Xiangyang Research Institute of Hubei University of
   Technology [XYYJ2022C08]; Fujian Provincial Key Laboratory of Data
   intensive Computing; Key Laboratory of intelligent Computing and
   information Processing, Fujian [BD201801]
FX This work presented in this paper was funded by the National Natural
   Science Foundation of China (61502155), the National Natural Science
   Foundation of China (61772180), the Project of Xiangyang Research
   Institute of Hubei University of Technology (No. XYYJ2022C08), Fujian
   Provincial Key Laboratory of Data intensive Computing and Key Laboratory
   of intelligent Computing and information Processing, Fujian: BD201801.
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43
   Chang MY, 2020, IEEE INT C INT ROBOT, P8564, DOI 10.1109/IROS45743.2020.9341549
   Cheng L, 2016, IEEE T INTELL TRANSP, V17, P3629, DOI 10.1109/TITS.2016.2547987
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Gobi R, 2021, MULTIMED TOOLS APPL, V80, P15377, DOI 10.1007/s11042-020-10438-y
   Hausler S, 2021, PROC CVPR IEEE, P14136, DOI 10.1109/CVPR46437.2021.01392
   Hu ZZ, 2017, IEEE IMAGE PROC, P4402, DOI 10.1109/ICIP.2017.8297114
   Huang G, 2020, IEEE INTERNET THINGS, V7, P6748, DOI 10.1109/JIOT.2020.2974928
   Jan YG, 2014, WIRELESS PERS COMMUN, V79, P1129, DOI 10.1007/s11277-014-1921-x
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li SL, 2021, IEEE T VEH TECHNOL, V70, P3087, DOI 10.1109/TVT.2021.3068266
   Lin LL., 2021, IEEE Geosci. Remote Sens. Lett., V99, P1
   Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Nazemzadeh P, 2017, IEEE-ASME T MECH, V22, P2588, DOI 10.1109/TMECH.2017.2762598
   Poulose A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186290
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sünderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986
   Tang LF, 2022, IEEE-CAA J AUTOMATIC, V9, P2121, DOI 10.1109/JAS.2022.106082
   Topak F, 2018, J COMPUT CIVIL ENG, V32, DOI 10.1061/(ASCE)CP.1943-5487.0000778
   Wang CX, 2011, INT J COMPUT INT SYS, V4, P394
   Wang XL, 2019, IET INTELL TRANSP SY, V13, P1736, DOI 10.1049/iet-its.2018.5431
   Ye XY., 2022, IEEE Trans. Industr. Inf., V99, P1
   Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Yuan J, 2021, IEEE T SYST MAN CY-S, V51, P5377, DOI 10.1109/TSMC.2019.2956321
   Zhang KN., 2022, IEEE Trans. Intell. Transp. Syst., V99, P1
   Zhang ZY, 2020, IEEE T MOBILE COMPUT, V19, P1760, DOI 10.1109/TMC.2019.2915221
   Zhao JQ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010161
NR 30
TC 0
Z9 0
U1 8
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1167
EP 1177
DI 10.1007/s00371-023-02838-6
EA MAY 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000980353400001
DA 2024-07-18
ER

PT J
AU Liu, XL
   Shen, FR
   Zhao, J
   Nie, CH
AF Liu, Xiaoliang
   Shen, Furao
   Zhao, Jian
   Nie, Changhai
TI Self-supervised learning of monocular 3D geometry understanding with
   two- and three-view geometric constraints
SO VISUAL COMPUTER
LA English
DT Article
DE 3D geometry understanding; Optical flow estimation; Visual odometry;
   Depth estimation; Self-supervised learning; Dynamic scenes
ID VISUAL ODOMETRY; OPTICAL-FLOW
AB The 3D geometry understanding of dynamic scenes captured by moving cameras is one of the cornerstones of 3D scene understanding. Optical flow estimation, visual odometry, and depth estimation are the three most basic tasks in 3D geometry understanding. In this work, we present a unified framework for joint self-supervised learning of optical flow estimation, visual odometry, and depth estimation with two- and three-view geometric constraints. As we all know, visual odometry and depth estimation are more sensitive to dynamic objects, while optical flow estimation is more difficult to estimate the boundary area moved out of the image. To this end, we use estimated optical flow to help visual odometry and depth estimation process dynamic objects and use a rigid flow synthesized by the estimated pose and depth to help learn the optical flow of the area that moves out of the boundary due to camera motion. In order to further improve the consistency of cross-tasks, we introduce three-view geometric constraints and propose a three-view consistency loss. Finally, experiments on the KITTI data set show that our method can effectively improve the performance of the occluded boundary area and the dynamic object area. Moreover, our method achieves comparable or better performance than other monocular self-supervised state-of-the-art methods in these three subtasks.
C1 [Liu, Xiaoliang; Shen, Furao; Zhao, Jian; Nie, Changhai] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Liu, Xiaoliang; Nie, Changhai] Nanjing Univ, Dept Comp Sci & Technol, Nanjing, Peoples R China.
   [Shen, Furao] Nanjing Univ, Sch Artificial Intelligence, Nanjing, Peoples R China.
   [Zhao, Jian] Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
C3 Nanjing University; Nanjing University; Nanjing University; Nanjing
   University
RP Shen, FR (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Peoples R China.; Shen, FR (corresponding author), Nanjing Univ, Sch Artificial Intelligence, Nanjing, Peoples R China.
EM xiaoliang_liu@smail.nju.edu.cn; frshen@nju.edu.cn; jianzhao@nju.edu.cn;
   changhainie@nju.edu.cn
RI Nie, Changhai/IWM-7504-2023; Liu, Xiaoliang/KFQ-9516-2024
OI Liu, Xiaoliang/0000-0002-3776-6929; Shen, Furao/0000-0002-7285-326X
FU STI 2030Major Projects of China [2021ZD0201300]; National Science
   Foundation of China [62276127]
FX This work was supported in part by the STI 2030Major Projects of China
   under Grant 2021ZD0201300 and by the National Science Foundation of
   China under Grant 62276127.
CR Ahmadi A, 2016, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2016.7532634
   Bian JW, 2019, ADV NEUR IN, V32
   Bian JW, 2021, INT J COMPUT VISION, V129, P2548, DOI 10.1007/s11263-021-01484-6
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Butler D., 2012, Tech. Rep. MPI-IS-TR-006
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716
   Clark R, 2017, AAAI CONF ARTIF INTE, P3995
   Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dosovitskiy A., 2015, IEEE INT C COMPUTER
   Eigen D, 2014, ADV NEUR IN, V27
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Godard C., 2018, ARXIV
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Horn B. K. P., 1981, Proceedings of the SPIE - The International Society for Optical Engineering, V281, P319, DOI 10.1117/12.965761
   Hur Junhwa, 2020, P IEEE CVF C COMP VI, P7396
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Janai J, 2018, LECT NOTES COMPUT SC, V11220, P713, DOI 10.1007/978-3-030-01270-0_42
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li RH, 2018, IEEE T AUTOM SCI ENG, V15, P651, DOI 10.1109/TASE.2017.2664920
   Li Zengyi, 2022, arXiv
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P876
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo C., IEEE T BIG DATA, VPP, P1
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ruder S., ARXIV160904747
   Song YP, 2022, COMPUT AIDED DESIGN, V146, DOI 10.1016/j.cad.2022.103196
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Teed Zachary, 2020, EUR C COMP VIS, P402, DOI [DOI 10.1007/978-3-030-58536-524, DOI 10.1007/978-3-030-58536-5_24]
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang GM, 2022, IEEE T INTELL TRANSP, V23, P308, DOI 10.1109/TITS.2020.3010418
   Wang S., 2017, ICRA, DOI [10.1109/icra.2017.7989236, DOI 10.1109/ICRA.2017.7989236]
   Wang S, 2018, INT J ROBOT RES, V37, P513, DOI 10.1177/0278364917734298
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wang Y, 2019, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2019.00149
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhong YR, 2019, PROC CVPR IEEE, P12087, DOI 10.1109/CVPR.2019.01237
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 60
TC 0
Z9 0
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1193
EP 1204
DI 10.1007/s00371-023-02840-y
EA APR 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000969747600001
DA 2024-07-18
ER

PT J
AU Yang, Y
   Qi, Y
   Qi, SQ
AF Yang, Yue
   Qi, Yong
   Qi, Saiyu
TI Relation-consistency graph convolutional network for image
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Image super-resolution; Graph convolutional; Attention mechanism; Gram
   matrix
AB Convolutional neural networks (CNNs) have been widely exploited in single image super-resolution (SISR) due to their powerful feature representation and the end-to-end training paradigm. Recent CNN-based SISR methods employ attention mechanism to enrich the feature representation and achieve notable performance. However, most of them use attention mechanism to model channel-wise dependencies, and the global relations of image features are not fully explored, thus hindering the discriminative learning capability. To amplify the feature representation, we propose a relation-consistency graph convolutional network (RGCN) for high-quality image rendering. Specifically, we introduce a spatial graph attention (SGA) to encode feature correlations in spatial dimension. Within SGA, the parameter-free Gram matrix is adopted to construct the global dependencies of pixel features, which dynamically measure the pixel-wise spatial relation. Furthermore, we embed a spatial pyramid pooling scheme into SGA to reduce the high complexity of correlation modeling between two pixels. Such an operation efficiently constructs the spatial relations through pixel and region-pooled features. Moreover, we propose a relation-consistency loss to retain the invariant of global relationship across all feature layers. The proposed loss regularizes the consistency between the low-resolution input and its corresponding high-resolution output in terms of the spatial relationships, enabling our network to learn a reasonable mapping and reconstruct more realistic images. Qualitative and quantitative comparison against state-of-the-art SISR methods on benchmark datasets under various degradation models demonstrate the superior performance of our RGCN.
C1 [Yang, Yue; Qi, Yong; Qi, Saiyu] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Qi, Y (corresponding author), Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian, Shaanxi, Peoples R China.
EM yang_yue2015@stu.xjtu.edu.cn; qiy@xjtu.edu.cn; saiyu-qi@xjtu.edu.cn
RI QI, Saiyu/X-2373-2019
OI QI, Saiyu/0000-0002-0394-4432; yang, yue/0000-0002-0288-356X
FU National Natural Science Foundation of China [61672421]; Ministry of
   Education of the People's Republic of China [2020KJ010801]
FX This work was supported by the National Natural Science Foundation of
   China under Grant (No.61672421) and Ministry of Education of the
   People's Republic of China (No. 2020KJ010801).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2015, PROC IEEE C COMPUT V
   Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Gao G., 2022, arXiv
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gori M, 2005, IEEE IJCNN, P729
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2020, IEEE T PATTERN ANAL, V42, P2011, DOI 10.1109/TPAMI.2019.2913372
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Johnson J, 2016, Arxiv, DOI [arXiv:1603.08155, 10.48550/ARXIV.1603.08155, DOI 10.48550/ARXIV.1603.08155, 10.48550/arXiv.1603.08155]
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li M., 2021, VISUAL COMPUT, P1
   Li QL, 2023, IEEE T COGN DEV SYST, V15, P1673, DOI 10.1109/TCDS.2022.3147839
   Li Y., 2018, EUR C COMP VIS WORKS
   Li YH, 2017, Arxiv, DOI [arXiv:1701.01036, DOI 10.48550/ARXIV.1701.01036]
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu D, 2018, ADV NEUR IN, V31
   Loshchilov I., 2019, arXiv
   Loshchilov Ilya, 2016, arXiv
   Lu Wei, 2021, 2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS), P291, DOI 10.1109/CCIS53392.2021.9754620
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Niu ZH, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1630, DOI 10.1109/ICASSP39728.2021.9414039
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Sutskever I, 2014, ADV NEUR IN, V27
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Vassilo K., 2020, P IEEECVF C COMPUTER, P512
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang X., 2018, EUROPEAN C COMPUTER
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yan YY, 2021, IEEE T IMAGE PROCESS, V30, P4905, DOI 10.1109/TIP.2021.3077135
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang X, 2022, VISUAL COMPUT, V38, P4307, DOI 10.1007/s00371-021-02297-x
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YF, 2018, IEEE T IMAGE PROCESS, V27, P3782, DOI 10.1109/TIP.2018.2826139
   Zhang ZW, 2020, Arxiv, DOI [arXiv:1812.04202, DOI 10.1109/TKDE.2020.2981333]
   Zou W., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P930
NR 66
TC 1
Z9 1
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 619
EP 635
DI 10.1007/s00371-023-02805-1
EA APR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000963831000001
DA 2024-07-18
ER

PT J
AU Yadav, KS
   Kirupakaran, AM
   Laskar, RH
AF Yadav, Kuldeep Singh
   Kirupakaran, Anish Monsley
   Laskar, Rabul Hussain
TI End-to-end bare-hand localization system for human-computer interaction:
   a comprehensive analysis and viable solution
SO VISUAL COMPUTER
LA English
DT Article
DE Hand detection; Pixel-wise segmentation; YOLO; Computer vision; Deep
   learning
ID RECOGNITION
AB Accurately localizing bare-hands is crucial for human-computer interaction systems. In real-time systems, computational time plays a pivotal role in achieving this task within a specified timeframe. The localization process may involve detection, tracking, or both, depending on the framework's needs. Most studies on bare-hand detection and tracking have been conducted in controlled environments. However, localizing bare-hands in uncontrolled environments are challenging due to the complexity of variations, such as changes in illumination, rotation, occlusion, scale, pose, speed, and impostor bare-hands. These factors can significantly impact the performance of the models, along with background feature domination effect and motion blur, which further complicate localization. To address these challenges, this paper presents a comprehensive analysis of the most significant deep learning-based hand localization models. We have customized an object detector as a bare-hand localization model by incorporating detection and tracking modules, providing computationally efficient performance while addressing the variations. To evaluate the models' efficiency across a range of scales, we have proposed a scale-based bare-hand database with varying scales from 20 to 200 cm. We have also evaluated these models on various bare-hand benchmark databases.
C1 [Yadav, Kuldeep Singh; Laskar, Rabul Hussain] Natl Inst Technol, Dept Elect & Commun Engn, Silchar, Assam, India.
   [Kirupakaran, Anish Monsley] Indian Inst Technol, Dept Appl Mech, Madras, Tamil Nadu, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Silchar; Indian Institute of Technology System (IIT System);
   Indian Institute of Technology (IIT) - Madras
RP Yadav, KS (corresponding author), Natl Inst Technol, Dept Elect & Commun Engn, Silchar, Assam, India.
EM kuldeep_rs@ece.nits.ac.in
RI Yadav, Kuldeep Singh/JKJ-1356-2023
OI Yadav, Kuldeep Singh/0000-0002-9761-9023
FU SERB IMPRINT [IMP/2018/000098]
FX This research work is funded by the SERB IMPRINT:(IMP/2018/000098)
   project. We thank the department of electronics and communication
   engineering of NITS for providing us with the necessary facilities to
   carry out this work.
CR Abavisani M, 2019, PROC CVPR IEEE, P1165, DOI 10.1109/CVPR.2019.00126
   Avola D, 2020, PATTERN RECOGN LETT, V138, P455, DOI 10.1016/j.patrec.2020.08.014
   Badrinarayanan V, 2016, Arxiv, DOI [arXiv:1511.00561, DOI 10.48550/ARXIV.1511.00561]
   Bhaumik G, 2022, VISUAL COMPUT, V38, P3853, DOI 10.1007/s00371-021-02225-z
   Bhuyan MK., 2008, WORLD ACAD SCI ENG T, V21, P753
   Chalasani T, 2019, IEEE INT CONF COMP V, P4367, DOI 10.1109/ICCVW.2019.00537
   Chen CLP, 2022, IEEE T INTELL TRANSP, V23, P444, DOI 10.1109/TITS.2020.3011937
   Chen LC, 2018, Arxiv, DOI arXiv:1802.02611
   Cruz S.R., Hand detection using deformable part models on an egocentric perspective, P7
   Dadashzadeh A, 2019, Arxiv, DOI arXiv:1806.05653
   Faudzi AAM, 2012, PROCEDIA ENGINEER, V41, P798, DOI 10.1016/j.proeng.2012.07.246
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao Q, 2019, IEEE T IND ELECTRON, V66, P9663, DOI 10.1109/TIE.2019.2898624
   He K., Deep residual learning for image recognition, P9
   Khokhlov I., 2020, P 2020 IEEE 91 VEH T, P1, DOI 10.1109/VTC2020-Spring48590.2020.9128749
   Le THN, 2017, IEEE COMPUT SOC CONF, P1203, DOI 10.1109/CVPRW.2017.159
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Maheswari S., 2017, Biomed. Res., V28, P7
   McBride TJ, 2019, 2019 SOUTHERN AFRICAN UNIVERSITIES POWER ENGINEERING CONFERENCE/ROBOTICS AND MECHATRONICS/PATTERN RECOGNITION ASSOCIATION OF SOUTH AFRICA (SAUPEC/ROBMECH/PRASA), P211, DOI [10.1109/RoboMech.2019.8704839, 10.1109/robomech.2019.8704839]
   Misra S, 2019, MULTIMED TOOLS APPL, V78, P34927, DOI 10.1007/s11042-019-08105-y
   Misra S, 2019, J AMB INTEL HUM COMP, V10, P4901, DOI 10.1007/s12652-019-01189-2
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Monsley KA, 2022, APPL SOFT COMPUT, V114, DOI 10.1016/j.asoc.2021.108122
   Mukherjee S, 2019, EXPERT SYST APPL, V136, P217, DOI 10.1016/j.eswa.2019.06.034
   Narasimhaswamy S., Contextual attention for hand detection in the wild, P10
   Redmon J., YOU ONLY LOOK ONCE U, P10
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, Arxiv, DOI [arXiv:1612.08242, 10.48550/arXiv.1612.08242, DOI 10.48550/ARXIV.1612.08242]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2022, VISUAL COMPUT, V38, P2801, DOI 10.1007/s00371-021-02157-8
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saboo S, 2022, MULTIMEDIA SYST, V28, P183, DOI 10.1007/s00530-021-00811-8
   Saboo S, 2021, MULTIMED TOOLS APPL, V80, P20579, DOI 10.1007/s11042-021-10669-7
   Shan D., Understanding human hands in contact at internet scale, P10
   Sigal L, 2004, IEEE T PATTERN ANAL, V26, P862, DOI 10.1109/TPAMI.2004.35
   Singha J, 2017, MULTIMEDIA SYST, V23, P499, DOI 10.1007/s00530-016-0510-0
   Singha J, 2018, NEURAL COMPUT APPL, V29, P1129, DOI 10.1007/s00521-016-2525-z
   Singha J, 2015, IETE J RES, V61, P597, DOI 10.1080/03772063.2015.1054900
   Skaria S, 2019, IEEE SENS J, V19, P3041, DOI 10.1109/JSEN.2019.2892073
   Sruthi CJ, 2023, VISUAL COMPUT, V39, P6183, DOI 10.1007/s00371-022-02720-x
   Vaila R, 2022, IEEE TETCI, V6, P124, DOI 10.1109/TETCI.2020.3035164
   Xu Kun, 2021, 2021 International Conference on Communications, Information System and Computer Engineering (CISCE), P447, DOI 10.1109/CISCE52179.2021.9445897
   Yadav KS, 2021, 2021 IEEE REGION 10 CONFERENCE (TENCON 2021), P504, DOI 10.1109/TENCON54134.2021.9707451
   Yadav KS, 2022, EXPERT SYST, V39, DOI 10.1111/exsy.12970
   Yadav KS, 2022, MULTIMEDIA SYST, V28, P861, DOI 10.1007/s00530-022-00890-1
   Yadav KS, 2020, MULTIMED TOOLS APPL, V79, P13089, DOI 10.1007/s11042-019-08443-x
   Yanay T, 2020, PERVASIVE MOB COMPUT, V66, DOI 10.1016/j.pmcj.2020.101183
   Zhang M., USING COMPUTER VISIO, P10
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zin TT, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10080904
NR 50
TC 4
Z9 4
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1145
EP 1165
DI 10.1007/s00371-023-02837-7
EA APR 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000962608500001
DA 2024-07-18
ER

PT J
AU Lei, P
   Xu, SC
   Zhang, SY
AF Lei, Peng
   Xu, Shuchang
   Zhang, Sanyuan
TI An art-oriented pixelation method for cartoon images
SO VISUAL COMPUTER
LA English
DT Article
DE Pixelation; Pixel art; Image processing; AOP algorithms
AB Pixel art has evolved from a primitive computer image presentation to an independent digital art style. It is widely used on the internet, for graphic user interface (GUI) design, and game industries. Existing pixelation tools and algorithms generate pixel images with artifacts, color clutter, blurring, and a lack of aesthetics. Generally, aesthetics are the dominant concern for pixel art. In this paper, an art-oriented pixelation (AOP) algorithm is proposed to effectively retain the main features of the original image content and the integrity of essential details with the artistic and aesthetic styles. At the same time, the AOP algorithm enables high-quality pixel image generation of arbitrary size without paired datasets and model training effort. The experimental results demonstrate that the pixel image generated by the AOP algorithm outperforms existing algorithms and tools in terms of aesthetics.
C1 [Lei, Peng; Zhang, Sanyuan] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
   [Xu, Shuchang] Hangzhou Normal Univ, Sch Informat Sci & Engn, Hangzhou, Peoples R China.
   [Xu, Shuchang] Wenzhou Univ, Inst Big data & Informat Technol, Wenzhou, Peoples R China.
C3 Zhejiang University; Hangzhou Normal University; Wenzhou University
RP Xu, SC (corresponding author), Hangzhou Normal Univ, Sch Informat Sci & Engn, Hangzhou, Peoples R China.; Xu, SC (corresponding author), Wenzhou Univ, Inst Big data & Informat Technol, Wenzhou, Peoples R China.
EM leipeng@zju.edu.cn; xusc@hznu.edu.cn; syzhang@cs.zju.edu.cn
OI Xu, Shuchang/0000-0002-4742-2759
FU Beijing Dailybread Co., Ltd.; Soft Science Key Research Project of
   Zhejiang Province;  [2022C25033]
FX AcknowledgmentsThe work is supported by Beijing Dailybread Co., Ltd.,
   and partly supported by the Soft Science Key Research Project of
   Zhejiang Province (No. 2022C25033).
CR Allebach J, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P707, DOI 10.1109/ICIP.1996.560768
   [Anonymous], 2021, RONENNESS PIXELATOR
   [Anonymous], 2019, ONE PIXEL PIXEL PAIN
   [Anonymous], 2020, P MEDEIROS START DRA
   [Anonymous], 2017, KABKA007 PHOTOSHOP Q
   CARLSON RE, 1985, SIAM J NUMER ANAL, V22, P386, DOI 10.1137/0722023
   Gerstner T., 2012, P S NONPH AN REND, P29
   Gerstner T, 2013, COMPUT GRAPH-UK, V37, P333, DOI 10.1016/j.cag.2012.12.007
   Hailan Kuang, 2021, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P476, DOI 10.1109/IMCEC51613.2021.9482118
   Han C., 2018, ACM Trans. Graph., V37, P1
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Kim T., 2017, ARXIV
   Kopf J, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508370
   Mao, 2015, INTRO OPENCV3 PROGRA, P248
   Öztireli AC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766891
   Qian, 2004, PIXEL DESIGN DESIGNE
   Saito, 2020, PIXELME CONVERT YOUR
   Shang YY, 2021, COMPUT GRAPH-UK, V95, P47, DOI 10.1016/j.cag.2021.01.008
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 20
TC 0
Z9 0
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 27
EP 39
DI 10.1007/s00371-022-02763-0
EA JAN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000910753800001
DA 2024-07-18
ER

PT J
AU Dong, YZ
   Peng, C
AF Dong, Yangzi
   Peng, Chao
TI Multi-GPU multi-display rendering of extremely large 3D environments
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-GPU rendering; GPU out-of-core; Inter-GPU load balancing
ID FRAMEWORK
AB In real-time rendering applications, mesh rendering quality suffers from limited GPU memory capacity and display resolution. Due to the increased complexity of models and the demand for higher display resolutions, people have started building commodity workstations with multiple GPUs at a low cost. As a result, more GPU memory is available across multiple GPUs, and a higher display resolution can be achieved by connecting each GPU to a display monitor, resulting in a large tiled display configuration. However, a multi-GPU workstation may not efficiently handle a complex model that cannot fit into the GPU memory, due to (1) the unified configuration treating GPUs as one hardware entity and requiring the same data replicated in all GPUs, and (2) the lack of scalability to reduce, balance, and stream data dynamically between the CPU and GPUs as well as among the GPUs. In this work, we present a fine-grained parallel rendering approach that integrates a view-dependent LOD selection strategy with the inter-GPU load balancing method to ensure each GPU handles the portion of data it rasterizes, without data replication. A new multi-GPU out-of-core method minimizes the amount of data transferred from the CPU to each GPU by taking the advantage of frame-to-frame coherence. A comprehensive evaluation is presented to understand the efficiency and scalability of the execution components over extremely large scenes.
C1 [Dong, Yangzi; Peng, Chao] Rochester Inst Technol, Golisano Coll Comp & Informat Sci, Sch Interact Games & Media, 1 Lomb Mem Dr, Rochester, NY 14623 USA.
C3 Rochester Institute of Technology
RP Peng, C (corresponding author), Rochester Inst Technol, Golisano Coll Comp & Informat Sci, Sch Interact Games & Media, 1 Lomb Mem Dr, Rochester, NY 14623 USA.
EM yd8608@rit.edu; cxpigm@rit.edu
OI Peng, Chao/0000-0001-8838-2469
FU National Science Foundation;  [CNS-1464323]
FX AcknowledgementsThis work was supported by the National Science
   Foundation Grant CNS-1464323. We thank Nvidia for donating the GPU
   device that has been used in this work to run our algorithms and produce
   the experimental results. The Power Plant model is brought through the
   courtesy of the University of North Carolina at Chapel Hill. We also
   thank the RIT MAGIC Center for their technical and logistic support for
   this research.
CR Abraham F, 2004, XVII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P292, DOI 10.1109/SIBGRA.2004.1352973
   Ahrens J, 2008, ULTRA VIS: 2008 WORKSHOP ON ULTRASCALE VISUALIZATION, P24
   Allard J, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P127
   AMD, 2017, CROSSFIRE TECHN
   Argudo O, 2016, COMPUT AIDED DESIGN, V79, P48, DOI 10.1016/j.cad.2016.06.005
   Bethel EW, 2011, IEEE COMPUT GRAPH, V31, P90, DOI 10.1109/MCG.2011.13
   Bhaniramka P, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P119
   Cabiddu D, 2015, COMPUT GRAPH-UK, V51, P81, DOI 10.1016/j.cag.2015.05.015
   Cignoni P, 2003, COMPUT GRAPH FORUM, V22, P505, DOI 10.1111/1467-8659.00698
   CRUZNEIRA C, 1992, COMMUN ACM, V35, P64, DOI 10.1145/129888.129892
   Dong Y., 2019, EUR S PAR GRAPH VIS, DOI [10.2312/pgv.20191111, DOI 10.2312/PGV.20191111]
   Lai DQ, 2015, IEEE T VIS COMPUT GR, V21, P714, DOI 10.1109/TVCG.2015.2398439
   Eilemann S., 2007, CISC VIS NETW IND GL
   Eilemann S., 2012, EUR S PAR GRAPH VIS, DOI [10.2312/EGPGV, DOI 10.2312/EGPGV]
   Eilemann S, 2020, IEEE T VIS COMPUT GR, V26, P1292, DOI 10.1109/TVCG.2018.2870822
   Eilemann S, 2009, IEEE T VIS COMPUT GR, V15, P436, DOI 10.1109/TVCG.2008.104
   Erol F., 2011, P 11 EUROGRAPHICS C, P41
   Febretti A, 2014, 2014 IEEE VIRTUAL REALITY (VR), P9, DOI 10.1109/VR.2014.6802043
   Febretti A, 2013, PROC SPIE, V8649, DOI 10.1117/12.2005484
   Funkhouser T. A., 1993, Computer Graphics Proceedings, P247, DOI 10.1145/166117.166149
   Gobbetti E, 2005, ACM T GRAPHIC, V24, P878, DOI 10.1145/1073204.1073277
   Grosset AVP, 2017, IEEE T VIS COMPUT GR, V23, P1677, DOI 10.1109/TVCG.2016.2542069
   Han MJ, 2020, 2020 IEEE VISUALIZATION CONFERENCE - SHORT PAPERS (VIS 2020), P11, DOI 10.1109/VIS47514.2020.00009
   Harris M., 2007, GPU GEMS, V3, P851
   Hu LA, 2010, IEEE T VIS COMPUT GR, V16, P718, DOI 10.1109/TVCG.2009.101
   Huahai Liu, 2011, Proceedings of the 2011 International Conference on Virtual Reality and Visualization (ICVRV 2011), P172, DOI 10.1109/ICVRV.2011.46
   Humphreys G, 2002, ACM T GRAPHIC, V21, P693, DOI 10.1145/566570.566639
   Kenzel M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201374
   Kontkanen J, 2011, COMPUT GRAPH FORUM, V30, P1353, DOI 10.1111/j.1467-8659.2011.01995.x
   Laine S., 2011, P HIGH PERF GRAPH HP, P79
   Larsen M, 2016, SYMP LARG DATA ANAL, P37, DOI 10.1109/LDAV.2016.7874308
   Melax S., 1998, Game Developer, V11, P44
   MOLNAR S, 1994, IEEE COMPUT GRAPH, V14, P23, DOI 10.1109/38.291528
   Moloney B., 2007, PROC EG S PARALLEL G, P45
   NVIDIA, 2011, Tech. rep.
   NVIDIA, 2017, NVIDIA MOS TECHN US
   Pan Wang, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P103, DOI 10.1109/CAD/Graphics.2011.66
   Peng C, 2012, COMPUT GRAPH FORUM, V31, P393, DOI 10.1111/j.1467-8659.2012.03018.x
   Peng Chao., 2016, COMPUTER AIDED DESIG, V13, P173, DOI DOI 10.1080/16864360.2015.1084184
   Ren XW, 2020, INT S HIGH PERF COMP, P582, DOI 10.1109/HPCA47549.2020.00054
   Samanta R., 1999, Proceedings 1999 EUROGRAPHICS/SIGGRAPH Workshop on Graphics Hardware, P107, DOI 10.1145/311534.311584
   Son M, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105784
   Steiner D., 2016, P 16 EUROGRAPHICS S, P89, DOI [10.2312/pgv.20161185, DOI 10.2312/PGV.20161185]
   Varadhan G, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P69, DOI 10.1109/VISUAL.2002.1183759
   WHITMAN S, 1994, IEEE COMPUT GRAPH, V14, P41, DOI 10.1109/38.291530
   Yoon SE, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P131, DOI 10.1109/VISUAL.2004.86
   Zheng GB, 2011, INT J HIGH PERFORM C, V25, P371, DOI 10.1177/1094342010394383
NR 47
TC 0
Z9 0
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6473
EP 6489
DI 10.1007/s00371-022-02740-7
EA DEC 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000899383000001
DA 2024-07-18
ER

PT J
AU Li, B
   Li, YB
   Wu, QMJ
AF Li, Bin
   Li, Yibing
   Wu, Q. M. Jonathan
TI A multi-scale threshold integration encoding strategy for texture
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Image texture analysis; Feature extraction; Multi-scale analysis;
   Texture classification
ID LOCAL BINARY PATTERN; REPRESENTATION; SCALE
AB As one of fundamental texture classification methods, LBP-based descriptors have attracted considerable attention due to the efficiency, simplicity, and high performance. However, most of binary pattern methods cannot effectively capture the texture information with scale changes. Inspired by this, this paper proposes a multi-scale threshold integration encoding strategy for texture classification. The essence of this strategy is to introduce the multi-scale local texture information in the view of thresholding. Based on this, we propose the local multi-scale center pattern, local multi-scale sign pattern, and local multi-scale magnitude pattern to extract and describe the multi-scale local texture information. Then, the three sub-patterns are jointly combined to generate the final descriptor for texture classification tasks. The experimental results on three popular texture databases significantly demonstrate that the proposed texture descriptor is very discriminative and powerful for visual texture classification tasks.
C1 [Li, Bin; Li, Yibing] Harbin Engn Univ, Coll Informat & Commun Engn, Key Lab Adv Marine Commun & Informat Technol, Minist Ind & Informat Technol, Harbin 150001, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, 401 Sunset Ave, Windsor, ON N9B 3P4, Canada.
C3 Harbin Engineering University; University of Windsor
RP Li, YB (corresponding author), Harbin Engn Univ, Coll Informat & Commun Engn, Key Lab Adv Marine Commun & Informat Technol, Minist Ind & Informat Technol, Harbin 150001, Peoples R China.
EM libin_heu@hrbeu.edu.cn; liyibing0920@126.com; jwu@uwindsor.ca
OI Li, Yibing/0000-0002-8811-6379
FU National Key Research and Development Program of China [2016YFF0102806];
   National Natural Science Foundation of China [51809056]; Natural Science
   Foundation of Heilongjiang Province, China [F2017004]
FX The paper is funded by the National Key Research and Development Program
   of China (Grant No. 2016YFF0102806), the National Natural Science
   Foundation of China (Grant No. 51809056), the Natural Science Foundation
   of Heilongjiang Province, China (Grant No. F2017004).
CR Chakraborty S, 2017, MULTIMED TOOLS APPL, V76, P1201, DOI 10.1007/s11042-015-3111-6
   Chen C, 2016, SIGNAL IMAGE VIDEO P, V10, P745, DOI 10.1007/s11760-015-0804-2
   Davarzani R, 2015, SIGNAL PROCESS, V111, P274, DOI 10.1016/j.sigpro.2014.11.005
   Dong YS, 2019, IEEE T CIRC SYST VID, V29, P3583, DOI 10.1109/TCSVT.2018.2883825
   Dong YS, 2019, IEEE ACCESS, V7, P87931, DOI 10.1109/ACCESS.2019.2924985
   Dong YS, 2018, VISUAL COMPUT, V34, P1315, DOI 10.1007/s00371-017-1415-4
   Duque JC, 2015, LANDSCAPE URBAN PLAN, V135, P11, DOI 10.1016/j.landurbplan.2014.11.009
   Guo ZB, 2023, VISUAL COMPUT, V39, P4267, DOI 10.1007/s00371-022-02589-w
   Guo ZH, 2016, IEEE T IMAGE PROCESS, V25, P687, DOI 10.1109/TIP.2015.2507408
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Hafiane A, 2015, PATTERN RECOGN, V48, P2609, DOI 10.1016/j.patcog.2015.02.007
   Hao Y, 2017, LECT NOTES COMPUT SC, V10666, P199, DOI 10.1007/978-3-319-71607-7_18
   Hu YT, 2017, INT CONF ACOUST SPEE, P1413, DOI 10.1109/ICASSP.2017.7952389
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li Z, 2012, IEEE T IMAGE PROCESS, V21, P2130, DOI 10.1109/TIP.2011.2173697
   Liu L, 2014, IEEE T IMAGE PROCESS, V23, P3071, DOI 10.1109/TIP.2014.2325777
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Nanni L, 2010, ARTIF INTELL MED, V49, P117, DOI 10.1016/j.artmed.2010.02.006
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan ZB, 2020, MULTIMED TOOLS APPL, V79, P5477, DOI 10.1007/s11042-019-08205-9
   Pan ZB, 2019, EXPERT SYST APPL, V120, P319, DOI 10.1016/j.eswa.2018.11.041
   Pan ZB, 2017, EXPERT SYST APPL, V88, P238, DOI 10.1016/j.eswa.2017.07.007
   Shakoor MH, 2017, INT J PATTERN RECOGN, V31, DOI 10.1142/S0218001417500197
   Song KC, 2015, J VIS COMMUN IMAGE R, V33, P323, DOI 10.1016/j.jvcir.2015.09.016
   Song TC, 2018, IEEE SIGNAL PROC LET, V25, P625, DOI 10.1109/LSP.2018.2809607
   Taha H., 2014, SCI WORLD J, P1
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tuceryan M., 1993, HDB PATTERN RECOGNIT, V2, P207, DOI DOI 10.1142/9789814343138_0010
   Vaidya SP, 2023, VISUAL COMPUT, V39, P2245, DOI 10.1007/s00371-022-02406-4
   Nguyen VD, 2014, IEEE T CIRC SYST VID, V24, P263, DOI 10.1109/TCSVT.2013.2254898
   Wang K, 2017, PATTERN RECOGN, V67, P213, DOI 10.1016/j.patcog.2017.01.034
   Wood EM, 2012, REMOTE SENS ENVIRON, V121, P516, DOI 10.1016/j.rse.2012.01.003
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Xu XC, 2021, DIGIT SIGNAL PROCESS, V114, DOI 10.1016/j.dsp.2021.103081
   Xu XC, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106830
   Xu XC, 2020, COGN COMPUT, V12, P224, DOI 10.1007/s12559-019-09673-9
   Xu Y, 2010, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2010.5540217
   Zhang Z, 2017, PATTERN RECOGN, V67, P263, DOI 10.1016/j.patcog.2017.02.021
   Zhao Y, 2016, NEUROCOMPUTING, V207, P354, DOI 10.1016/j.neucom.2016.05.016
   Zhao Y, 2013, NEUROCOMPUTING, V106, P68, DOI 10.1016/j.neucom.2012.10.017
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 42
TC 0
Z9 0
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5747
EP 5761
DI 10.1007/s00371-022-02693-x
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000885420600002
DA 2024-07-18
ER

PT J
AU Hong, ZC
   Wu, QX
AF Hong, Zhongcheng
   Wu, Qiuxia
TI Self-supervised monocular depth estimation via two mechanisms of
   attention-aware cost volume
SO VISUAL COMPUTER
LA English
DT Article
DE Depth estimation; Attention mechanism; Self-supervised learning; Cost
   volume
AB Self-supervised monocular depth estimation takes advantage of adjacent frame images as supervision signals for training, which has made a significant improvement in recovering holistic scene geometry. However, owing to these methods do not pay attention to the details of images, and the predicted depth maps are imprecise, where some small objects are neglected, object boundaries are blurred, as well as the predictions lack global consistency. Inspired by the excellent ability of the attention scheme to focus on details, we address these issues by using multi-frames to construct 3D cost volume and taking into account attention awareness for the cost volume so that the network is more inclined to learn important information from the cost volume. In this paper, we propose two mechanisms of attention-aware cost volume: voxel-wise attention-aware (VAA) network and recurrent attention-aware (RAA) network. For the VAA network, 3D convolution is exploited to reweight the 3D cost volume so as to enhance essential areas of the cost volume while suppressing unimportant areas. Therefore, our proposed VAA network can autonomously select the required details. For the RAA network, 3D cost volume is sequentially refined along the depth dimension with 2D convolutions, thereby expanding the receptive field in the depth range and achieving better global consistency. Experiments demonstrate that our methods outperform other self-supervised methods on the KITTI and Cityscapes datasets.
C1 [Hong, Zhongcheng; Wu, Qiuxia] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 South China University of Technology
RP Wu, QX (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM hong1061628369@gmail.com; qxwu@scut.edu.cn
OI Wu, Qiuxia/0000-0002-2284-7806
FU National Natural Science Foundation of China [61772225]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515010558, 2021A1515011972]
FX The work presented in this paper is partially supported by Grants from
   National Natural Science Foundation of China (No. 61772225), Guangdong
   Basic and Applied Basic Research Foundation (Nos. 2020A1515010558 and
   2021A1515011972).
CR Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Bozorgtabar B, 2019, IEEE I CONF COMP VIS, P4209, DOI 10.1109/ICCV.2019.00431
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273
   Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716
   Eigen D., 2014, arXiv
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fan C, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21216956
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A., 2012, CVPR
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L, 2021, NEUROCOMPUTING, V440, P251, DOI 10.1016/j.neucom.2021.01.126
   Hongwei Yi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P766, DOI 10.1007/978-3-030-58545-7_44
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Lai ZT, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21206780
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee J, 2019, PR MACH LEARN RES, V97
   Li H., 2020, arXiv
   Li YZ, 2021, VISUAL COMPUT, V37, P2567, DOI 10.1007/s00371-021-02206-2
   Lienen J, 2021, PROC CVPR IEEE, P14590, DOI 10.1109/CVPR46437.2021.01436
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu P, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010054
   Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258
   Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377
   Lyu X., 2020, CORR
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Patil V, 2020, IEEE ROBOT AUTOM LET, V5, P6813, DOI 10.1109/LRA.2020.3017478
   Pilzer A, 2018, INT CONF 3D VISION, P587, DOI 10.1109/3DV.2018.00073
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Shi JL, 2021, VISUAL COMPUT, V37, P815, DOI 10.1007/s00371-020-01832-6
   Tonioni A, 2020, IEEE T PATTERN ANAL, V42, P2396, DOI 10.1109/TPAMI.2019.2940948
   Wang J, 2020, AAAI CONF ARTIF INTE, V34, P9169
   Wang R, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165476
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson J, 2021, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR46437.2021.00122
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu ZH, 2020, PROC CVPR IEEE, P1946, DOI 10.1109/CVPR42600.2020.00202
   Zhang ZY, 2018, PATTERN RECOGN, V83, P430, DOI 10.1016/j.patcog.2018.05.016
   Zhao BG, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041383
   Zhao T, 2022, VISUAL COMPUT, V38, P1619, DOI 10.1007/s00371-021-02092-8
   Zhou H, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3050, DOI 10.1109/ICASSP39728.2021.9413680
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 56
TC 2
Z9 2
U1 3
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5937
EP 5951
DI 10.1007/s00371-022-02704-x
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000876842100001
DA 2024-07-18
ER

PT J
AU Li, DJ
   Gao, WR
AF Li, Dajin
   Gao, Wenran
TI Neural style transfer based on deep feature synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Deep neural network; Texture synthesis;
   Feature synthesis
ID IMAGE
AB Neural Style Transfer makes full use of the high-level features of deep neural networks, so stylized images can represent content and style features on high-level semantics. But neural networks are end-to-end black box systems. Previous style transfer models are based on the overall features of the image when constructing the target image, so they cannot effectively intervene in the content and style representations. This paper presents a locally controllable nonparametric neural style transfer model. We treat style transfer as a feature matching process independent of neural networks and propose a deep-to-shallow feature synthesis algorithm. The target feature map is synthesized layer by layer in the deep feature space and then transformed into the target image. Because the feature synthesis is a local manipulation on feature maps, it is easy to control the local texture structure, content details and texture distribution. Based on our synthesis algorithm, we propose a multi-exemplar synthesis method that can make local stroke directions better match content semantics or combine multiple styles into a single image. Our experiments show that our model can produce more impressive results than previous methods.
C1 [Li, Dajin] Shandong Normal Univ, Commun Sch, Jinan 250014, Peoples R China.
   [Gao, Wenran] Shandong Normal Univ, Phys & Elect Sch, Jinan 250014, Peoples R China.
C3 Shandong Normal University; Shandong Normal University
RP Li, DJ (corresponding author), Shandong Normal Univ, Commun Sch, Jinan 250014, Peoples R China.
EM ldjwqc@163.com; Wenrangao@163.com
FU National Natural Science Foundation of China [61340019]
FX This work was supported by the National Natural Science Foundation of
   China (Project No. 61340019).
CR [Anonymous], 2006, Proceedings of the 17th Eurographics Conference on Rendering Techniques, DOI DOI 10.2312/EGWR/EGSR06/361-370
   [Anonymous], 2021, COMPUT ELECTR ENG, V95
   Ashikhmin M, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1210863
   Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405
   Barnes C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766934
   Bénard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Champandard AJ, ARXIV160301768
   Chen DD, 2021, IEEE T PATTERN ANAL, V43, P2373, DOI 10.1109/TPAMI.2020.2964205
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Dong LX, 2014, MULTIMED TOOLS APPL, V69, P605, DOI 10.1007/s11042-012-1126-9
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Fiser J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925948
   Frigo O, 2019, VISUAL COMPUT, V35, P429, DOI 10.1007/s00371-018-1474-1
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Gao JJ, 2019, IEEE ACCESS, V7, P104168, DOI 10.1109/ACCESS.2019.2931037
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang ZX, 2019, COMPUT GRAPH FORUM, V38, P469, DOI 10.1111/cgf.13853
   Jamriska O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323006
   Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee H, 2011, COMPUT GRAPH-UK, V35, P81, DOI 10.1016/j.cag.2010.11.008
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li SH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1716, DOI 10.1145/3123266.3123425
   Li YT, 2017, ADV NEUR IN, V30
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Reimann M, 2022, VISUAL COMPUT, DOI 10.1007/s00371-077-07518-x
   Risser E., 2017, ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Wang GY, 2006, ACM T GRAPHIC, V25, P1360, DOI 10.1145/1183287.1183292
   Wang MY, 2014, IEEE T VIS COMPUT GR, V20, P1451, DOI 10.1109/TVCG.2014.2303984
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Winnemoller H., 2011, P ACM SIGGRAPH EUR S, P147
   Yamaguchi S., 2015, ACM SIGGRAPH ASIA 20
   Ye W., 2022, VISUAL COMPUT, P1
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhang SQ, 2022, VISUAL COMPUT, V38, P2125, DOI 10.1007/s00371-021-02272-6
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zhang YX, 2020, IEEE T IMAGE PROCESS, V29, P4085, DOI 10.1109/TIP.2020.2969081
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhao HH, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2020.106655
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 59
TC 2
Z9 2
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5359
EP 5373
DI 10.1007/s00371-022-02664-2
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000854726000002
DA 2024-07-18
ER

PT J
AU Chen, LF
   Wan, L
AF Chen, Lifang
   Wan, Li
TI CTUNet: automatic pancreas segmentation using a channel-wise transformer
   and 3D U-Net
SO VISUAL COMPUTER
LA English
DT Article
DE Auto pancreas segmentation; 3D channel transformer; 3D U-Net; Deep
   learning
AB Diabetes, pancreatic cancer, and pancreatitis are all diseases of the pancreas, which seriously threaten people's lives. The pancreas has a special anatomical structure, its size, shape, and position are variable, and it is highly similar to other surrounding deep abdominal tissues, so achieving accurate segmentation is still one of the most challenging tasks in the field of medical image segmentation. We propose a new network CTUNet that combines Transformer and 3D U-Net, which can achieve high-precision automatic segmentation of the pancreas. We deploy the Transformer on skip connections to coordinate global explicit features and guide the network learning. In view of pancreas reciprocity and shape variability, we design a Pancreas Attention module and add it to each encoder to further enhance the ability to extract context information and learn distinct features. In addition, in the decoder, we use a novel Feature Concatenation module with an attention mechanism to further promote the fusion of different levels of features and alleviate the problem of loss of down-sampling feature information. We train and test our model on the NIH dataset and evaluate with Dice Similarity Coefficient, Jaccard Index, Precision, and Recall. Experimental results show that our proposed model outperforms most existing pancreas segmentation methods.
C1 [Chen, Lifang; Wan, Li] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Dadao, Wuxi 214000, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Chen, LF (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Dadao, Wuxi 214000, Jiangsu, Peoples R China.
EM may7366@163.com; q1031439207@163.com
CR Andonie R, 2019, J MEMBRANE COMPUT, V1, P279, DOI 10.1007/s41965-019-00023-0
   Cai J, 2017, ARXIV
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   Dmitriev K, 2016, MED IMAGING 2016 IMA, P628
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Farag A, 2017, IEEE T IMAGE PROCESS, V26, P386, DOI 10.1109/TIP.2016.2624198
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Knolle M, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0255397
   Kronman A, 2016, INT J COMPUT ASS RAD, V11, P369, DOI 10.1007/s11548-015-1285-z
   Lei T., 2020, Medical Image Segmentation Using Deep Learning: A Survey
   Li J., 2022, ARXIV
   Li Jianning, 2022, SPARSE CONVOLUTIONAL, P2
   Li MY, 2021, IEEE ACCESS, V9, P140965, DOI 10.1109/ACCESS.2021.3118718
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Lu L, 2019, IEEE ACCESS, V7, P172871, DOI 10.1109/ACCESS.2019.2956550
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Peery AF, 2019, GASTROENTEROLOGY, V156, P254, DOI [10.1053/j.gastro.2018.08.063, 10.1053/j.gastro.2015.08.045]
   Rickmann AM, 2020, IEEE T MED IMAGING, V39, P2461, DOI 10.1109/TMI.2020.2972059
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth HR, 2015, PROC SPIE, V9413, DOI 10.1117/12.2081420
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012
   Sha Y., 2021, arXiv
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent A, 2011, LANCET, V378, P607, DOI 10.1016/S0140-6736(10)62307-0
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang H, 2021, ARXIV
   Wang WZ, 2020, I S BIOMED IMAGING, P207, DOI [10.1109/ISBI45749.2020.9098473, 10.1109/isbi45749.2020.9098473]
   Wang Y, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2021.101958
   Wang ZH, 2019, IEEE IMAGE PROC, P1415, DOI [10.1109/ICIP.2019.8803103, 10.1109/icip.2019.8803103]
   Xue J, 2021, IEEE T CYBERNETICS, V51, P2153, DOI 10.1109/TCYB.2019.2955178
   Yang M., 2021, VISUAL COMPUT, P1
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu QH, 2018, PROC CVPR IEEE, P8280, DOI 10.1109/CVPR.2018.00864
   Yuyin Zhou, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P693, DOI 10.1007/978-3-319-66182-7_79
   Zagoruyko S., 2016, ARXIV
   Zhao TY, 2021, PROC CVPR IEEE, P13738, DOI 10.1109/CVPR46437.2021.01353
   Zhou Y, 2016, ARXIV
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhu WT, 2019, MED PHYS, V46, P576, DOI 10.1002/mp.13300
   Zhu ZT, 2018, INT CONF 3D VISION, P682, DOI 10.1109/3DV.2018.00083
   Zhuang HM, 2023, VISUAL COMPUT, V39, P2207, DOI 10.1007/s00371-021-02322-z
NR 47
TC 15
Z9 16
U1 3
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5229
EP 5243
DI 10.1007/s00371-022-02656-2
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000849999800001
DA 2024-07-18
ER

PT J
AU Rao, J
   Ke, AH
   Liu, G
   Ming, Y
AF Rao, Jian
   Ke, Aihua
   Liu, Gang
   Ming, Yue
TI MS-GAN: multi-scale GAN with parallel class activation maps for image
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Image reconstruction; Generative adversarial networks; Parallel class
   activation maps; Spectral normalization
AB Recently, image reconstruction has been a research hotspot in the field of deep learning. For image reconstruction, generative adversarial networks (GANs) have obtained some remarkable results, but the existing methods based on GANs have not achieved satisfactory reconstructed results in quality. In order to improve the quality, we propose a more effective multi-scale GAN for image reconstruction and the proposed method is called MS-GAN. The generator of MS-GAN uses an improved U-net to capture the important details from the sparse inputs. In MS-GAN, the parallel class activation maps (P-CAMs) and spectral normalization (SN) are added to U-net. P-CAMs are composed of two parallel class activation maps (CAMs) and can specifically guide the generator to focus on the important details of the images for a more realistic visual effect. For the training process, MS-GAN consists of two phases: the generating phase and the refinement phase. The generating phase is to use binary sparse edges and color domains to generate the preliminary images. The refinement phase is to further improve the quality of the preliminary images. Experimental verifications are conducted on some datasets, which include edges2shoes, edges2handbags and Getchu. Experimental results show that our approach outperforms the existing state-of-the-art methods. The images reconstructed by MS-GAN is more photo-realistic in terms of visual effects.
C1 [Rao, Jian; Ming, Yue] Hubei Univ Technol, Sch Arts & Design, Wuhan 430068, Hubei, Peoples R China.
   [Ke, Aihua; Liu, Gang] Hubei Univ Technol, Sch Comp Sci, Wuhan 430068, Hubei, Peoples R China.
C3 Hubei University of Technology; Hubei University of Technology
RP Ke, AH (corresponding author), Hubei Univ Technol, Sch Comp Sci, Wuhan 430068, Hubei, Peoples R China.
EM keaihuaaa@outlook.com
RI Liu, Gang/AAU-3119-2020
OI Liu, Gang/0000-0002-0489-2638
FU Ministry of Education New Liberal Arts Research and Reform Practice
   Project [2021160043]; Hubei Provincial Department of Education
   Philosophy and Social Science Youth Project: Research on Chinese Ancient
   Porcelain Restoration Based on Virtual Simulation Technology [21Q072]; 
   [HBCIR2020Z005];  [HBCY1914]
FX The work described in this paper was funded by 2021 Ministry of
   Education New Liberal Arts Research and Reform Practice Project "The
   Construction of Interdisciplinary Innovation and Entrepreneurship
   Practice Teaching System Based on Design Empowerment" (2021160043) and
   2021 Hubei Provincial Department of Education Philosophy and Social
   Science Youth Project: Research on Chinese Ancient Porcelain Restoration
   Based on Virtual Simulation Technology (21Q072). Any conclusions or
   recommendations stated here are those of the authors and do not
   necessarily reflect the official position of the sponsors (Grant Nos.
   HBCIR2020Z005 and HBCY1914).
CR Almahairi A, 2018, PR MACH LEARN RES, V80
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bock S., 2018, IMPROVEMENT CONVERGE
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Datta L., 2020, ABS200406632
   Dekel T, 2018, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR.2018.00370
   Ding X., 2020, ABS201107466
   Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384
   Gozde U., 2018, ABS180307422
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jiang Y., 2021, NEURIPS, P14745
   Jing Y., 2019, DYNAMIC INSTANCE NOR
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Kaneko T, 2019, INT CONF ACOUST SPEE, P6820, DOI [10.1109/ICASSP.2019.8682897, 10.1109/icassp.2019.8682897]
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Kim J., 2019, U GAT IT UNSUPERVISE
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li, 2018, AUTOMATIC ANIME CHAR
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Li YT, 2017, ADV NEUR IN, V30
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liu G, 2019, ENG LET, V27, P396
   Liu J., 2020, International Conference on Neural Computing for Advanced Applications, P47
   Liu MY, 2017, ADV NEUR IN, V30
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu Y., 2020, INT J APPL MATH, V50, P853
   Miyato T, 2018, INT C LEARN REPR
   Nazeri K., 2019, ARXIV190100212
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shi CL, 2020, PATTERN RECOGN LETT, V138, P520, DOI 10.1016/j.patrec.2020.08.021
   Song Yuhang, 2018, ARXIV180503356
   Sun Y, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11020045
   Sun YL, 2020, IEEE T INF FOREN SEC, V15, P2679, DOI 10.1109/TIFS.2020.2975921
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wang KF, 2017, IEEE-CAA J AUTOMATIC, V4, P588, DOI 10.1109/JAS.2017.7510583
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei-Ta Chu, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P99, DOI 10.1007/978-3-030-67832-6_9
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xiao C.F., 2021, ABS210907874
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   You S., 2019, PI REC PROGR IMAGE R
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XH, 2017, INT CONF DIGIT SIG
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 58
TC 5
Z9 5
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 2111
EP 2126
DI 10.1007/s00371-022-02468-4
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000785702200002
DA 2024-07-18
ER

PT J
AU Etemad, K
   Samavati, F
   Dawson, P
AF Etemad, Katayoon
   Samavati, Faramarz
   Dawson, Peter
TI Multi-scale physicalization of polar heritage at risk in the western
   canadian arctic
SO VISUAL COMPUTER
LA English
DT Article
DE Physicalization; Heritage; Interaction; Focus plus Context;
   Visualization; 3d Modeling
ID YUKON-TERRITORY; HERSCHEL ISLAND; SCALE MODELS; FORT-CONGER; 3D
AB The digital preservation of heritage resources has emerged as an essential method for communicating the significance of artifacts, buildings, and landscapes to descendant communities and the wider public. While virtual representations are becoming more commonplace, physical representations (physicalization) of heritage sites via 3D printing are used to a lesser degree. Physicalization provides new perspectives through the interplay between touch and vision and can facilitate a deeper understanding of the history being conveyed. This paper discusses how the physical models of heritage buildings and landscape features on Qikiqtaruk/Herschel Island Territorial Park were created from terrestrial laser scanning and UAV photogrammetry data. We demonstrate how to use this physicalization of polar heritage to communicate the significance of the buildings and landscape of the island to the local Indigenous communities and global audiences, as well as how they are being threatened by climate change. We also explore the transformation of a cove on the island into puzzles and data sculptures. In addition to the Cove, the fabricating of important buildings on larger scales has been a requirement. This multi-scale printing raises the issue of connecting the large-scale buildings with their small instances/copies on the Island (similar to focus + context visualization in the digital form). Due to the limitation of physicalization compared with digital representations, new methods, metaphors and designs are needed for supporting focus + context visualization. We have designed and implemented several such methods in our specific physicalization of heritage buildings and landscape on Qikiqtaruk/Herschel Island. We presented our physicalizations to the members of the Inuvialuit community of Aklavik NWT and received a positive response.
C1 [Etemad, Katayoon; Samavati, Faramarz; Dawson, Peter] Univ Calgary, Calgary, AB, Canada.
C3 University of Calgary
RP Etemad, K (corresponding author), Univ Calgary, Calgary, AB, Canada.
EM ketemad@ucalgary.ca; samavati@ucalgary.ca; pcdawson@ucalgary.ca
OI Etemad, Katayoon/0000-0003-1732-133X; Dawson, Peter/0000-0002-5090-6391
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
FX We wish to thank the members of the Inuvialuit community of Aklavik NWT
   for their helpful feedback. The authors would also like to thank
   Christian Thomas and Brent Riley (Yukon Government) and Richard Gordon
   (Senior Park Ranger, Herschel Island/Qikiqtaruk Territorial Park) for
   their assistance and support. This work was supported in part by the
   Yukon Government and the Natural Sciences and Engineering Research
   Council of Canada (NSERC).
CR Allahverdi K, 2018, COMPUT GRAPH FORUM, V37, P439, DOI 10.1111/cgf.13432
   Ang KD, 2019, COMPUT GRAPH-UK, V85, P42, DOI 10.1016/j.cag.2019.09.004
   [Anonymous], 2013, Evaluating the Efciency of Physical Visualizations, DOI DOI 10.1145/2470654.2481359
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   Bertulli MM, 2013, ARCTIC, V66, P312
   Burn ChristopherR., 2012, Herschel Island Qikiqtaryuk: a Natural and Cultural History of Yukon's Arctic Island
   Burstyn J, 2015, LECT NOTES COMPUT SC, V9296, P332, DOI 10.1007/978-3-319-22701-6_25
   Dawson, DIGITALLY PRESERVING
   Dawson P., 2004, DYNAMICS NO SOC, P10
   Dawson P., 2015, IDENTITY HERITAGE, P107, DOI DOI 10.1007/978-3-319-09689-6_11
   Dawson P.C., 2009, Alaska J. Anthropol, V7, P29
   Dawson PC, 2005, J FIELD ARCHAEOL, V30, P443, DOI 10.1179/009346905791072134
   Dawson P, 2007, WORLD ARCHAEOL, V39, P17, DOI 10.1080/00438240601136397
   Dawson P, 2016, OPEN ARCHAEOL, V2, P209, DOI 10.1515/opar-2016-0016
   Dawson P, 2011, J SOC ARCHAEOL, V11, P387, DOI 10.1177/1469605311417064
   Dawson PC, 2013, ARCTIC, V66, P147
   Djavaherpour H, 2021, COMPUT GRAPH FORUM, V40, P569, DOI 10.1111/cgf.14330
   Djavaherpour H, 2017, IEEE COMPUT GRAPH, V37, P61, DOI 10.1109/MCG.2017.38
   Dudek P, 2013, ARCH METALL MATER, V58, P1415, DOI 10.2478/amm-2013-0186
   Florio A, 2019, IEEE INT CON INF VIS, P25, DOI 10.1109/IV.2019.00014
   Georgiou E, 2017, INT ARCH PHOTOGRAMM, V42-2, P275, DOI 10.5194/isprs-archives-XLII-2-W5-275-2017
   Government, 2013, HERSCHEL ISLAND QIKI
   Government C, 2017, FORT QUEB NAT HIST S
   Hasan M, 2016, VISUAL COMPUT, V32, P323, DOI 10.1007/s00371-015-1180-1
   Ioannides Marinos., 2014, 3D Research Challenges in Cultural Heritage: A Roadmap in Digital Heritage Preservation, DOI DOI 10.1007/978-3-662-44630-0
   Jansen Y, 2013, IEEE T VIS COMPUT GR, V19, P2396, DOI 10.1109/TVCG.2013.134
   Kastens KA, 2010, LECT NOTES ARTIF INT, V6222, P112
   Lantuit H, 2012, PERMAFROST PERIGLAC, V23, P39, DOI 10.1002/ppp.1731
   Levy R., 2014, PASTPLAY TEACHING LE, P66
   Levy R., 2005, EDMEDIA INN LEARN AS, P4537
   Levy R.M., EXPLORING ARCTIC CUL
   Mahdavi-Amiri Ali., 2015, P 41 GRAPHICS INTERF, P73, DOI DOI 10.20380/GI2015.10
   McCarthy J, 2014, J ARCHAEOL SCI, V43, P175, DOI 10.1016/j.jas.2014.01.010
   Moorman L, 2020, J GEOGR, V120, P23, DOI 10.1080/00221341.2020.1832138
   Morrison, 2006, CANADIAN ENCY
   Packer JF, 2017, VISUAL COMPUT, V33, P1291, DOI 10.1007/s00371-016-1217-0
   Radosavljevic B, 2016, ESTUAR COAST, V39, P900, DOI 10.1007/s12237-015-0046-0
   Samavati F, 2016, VISUAL COMPUT, V32, P1293, DOI 10.1007/s00371-016-1227-y
   Schmid B, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-274
   Schmitz M, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P253, DOI 10.1145/2807442.2807503
   Schüller C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925914
   Sherlock M., 2016, INTERACTIVE DATA STY
   Slyper R., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P6, DOI 10.1109/ROMAN.2012.6343723
   Stavric M, 2013, ARCHITECTURAL SCALE MODELS IN THE DIGITAL AGE: DESIGN, REPRESENTATION AND MANUFACTURING, P41
   Stusak S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3247, DOI 10.1145/2702123.2702248
   Taher F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3237, DOI 10.1145/2702123.2702604
   Umetani N, 2017, IEEE COMPUT GRAPH, V37, P52, DOI 10.1109/MCG.2017.40
   Willis KDD, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P589
NR 48
TC 1
Z9 1
U1 2
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1717
EP 1729
DI 10.1007/s00371-022-02439-9
EA MAR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000773171600001
DA 2024-07-18
ER

PT J
AU Qu, ZY
   Shi, HB
   Tan, S
   Song, B
   Tao, Y
AF Qu, Zhenyang
   Shi, Hongbo
   Tan, Shuai
   Song, Bing
   Tao, Yang
TI A flow-guided self-calibration Siamese network for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Optical flow; Siamese network; Attention
ID OBJECT TRACKING
AB Existing Siamese-based trackers pay attention to applying online updates to deal with target deformation and occlusion. Despite excellent accuracy and robustness, these trackers are still plagued by model drift due to the cumulative errors from tracking results. Therefore, this paper proposes a flow-guided self-calibration Siamese network to alleviate the model drift. This network focuses on leveraging the rich motion information in the current frame and adaptively optimizing the feature representation of the target. Firstly, to mitigate the lack of motion information during tracking, the optical flow field is estimated before the target location. A gather network is designed carefully to extract the deep motion features from the optical flow. Then, owing to the cumulative errors caused by the tracking result, a novel self-calibration module is introduced to update the appearance model without any tracking results adaptively. The module incorporates appearance features and deep motion features via an attention mechanism. Finally, a synthetic loss function is proposed to obtain expressive deep feature representation by adding a competitive loss between samples to the original loss function. Extensive experiments have demonstrated the effectiveness of the proposed method on benchmarks.
C1 [Qu, Zhenyang; Shi, Hongbo; Tan, Shuai; Song, Bing; Tao, Yang] East China Univ Sci & Technol, Minist Educ, Key Lab Smart Mfg Energy Chem Proc, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology
RP Shi, HB (corresponding author), East China Univ Sci & Technol, Minist Educ, Key Lab Smart Mfg Energy Chem Proc, Shanghai 200237, Peoples R China.
EM hbshi@ecust.edu.cn
RI Shi, Hongbo/E-6963-2016
FU National Natural Science Foundation of China [62073140, 62073141];
   National Natural Science Foundation of Shanghai [19ZR1473200]
FX This work is supported by the National Natural Science Foundation of
   China (No. 62073140, 62073141); National Natural Science Foundation of
   Shanghai (No. 19ZR1473200).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Chen BY, 2019, PATTERN RECOGN, V87, P80, DOI 10.1016/j.patcog.2018.10.005
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2019, PATTERN RECOGN LETT, V124, P74, DOI 10.1016/j.patrec.2018.03.009
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CH, 2021, VISUAL COMPUT, V37, P587, DOI 10.1007/s00371-020-01825-5
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu PD, 2020, INT CONF ACOUST SPEE, P2598, DOI [10.1109/icassp40776.2020.9054590, 10.1109/ICASSP40776.2020.9054590]
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shuang K, 2020, IEEE WINT CONF APPL, P660, DOI [10.1109/wacv45572.2020.9093517, 10.1109/WACV45572.2020.9093517]
   Sugang Ma, 2020, 2020 International Conference on Networking and Network Applications (NaNA), P321, DOI 10.1109/NaNA51271.2020.00062
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384
   Yang S, 2021, VISUAL COMPUT
   Yang TY, 2020, PROC CVPR IEEE, P6717, DOI 10.1109/CVPR42600.2020.00675
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhang DW, 2021, NEUROCOMPUTING, V436, P260, DOI 10.1016/j.neucom.2020.11.046
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
NR 47
TC 2
Z9 2
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 625
EP 637
DI 10.1007/s00371-021-02362-5
EA MAR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000767704900001
DA 2024-07-18
ER

PT J
AU Zhang, RP
   Chen, CY
   Zhang, JC
   Peng, J
   Alzbier, AMT
AF Zhang, Ripei
   Chen, Chunyi
   Zhang, Jiacheng
   Peng, Jun
   Alzbier, Ahmed Mustafa Taha
TI 360-degree visual saliency detection based on fast-mapped convolution
   and adaptive equator-bias perception
SO VISUAL COMPUTER
LA English
DT Article
DE 360-degree visual saliency detection; Mapped Convolutions; Sampling
   interpolation; Adaptive equatorial bias
ID PREDICTION; MODEL; INTERPOLATION; ATTENTION
AB The geometric distortion of the panoramic image makes the saliency detection method based on traditional 2D convolution invalid. "Mapped Convolution" can effectively solve this problem, which accepts a task- or domain-specific mapping function in the form of an adjacency list that dictates where the convolutional filters sample the input. However, when applied to panorama saliency detection, the method results in additional computational overhead due to repeatedly sampling overlapping regions of adjacent convolution positions along the longitude. In order to solve this problem, we improved the calculation process of "Mapped Convolution". Rather than accessing adjacency list during the convolution, we first sample the panorama based on the adjacency list for only once and obtain a sampled map. This sampling process is called the decoupled sampling of "Mapped Convolution". And then the map is convoluted in traditional 2D way, thus avoiding repeatedly sampling. In this paper, an interpolation method based on the Softmax function is also proposed and applied to the interpolation calculation of decoupled sampling. Compared with common interpolation methods such as linear interpolation, this interpolation method makes our network more efficient during training. We additionally introduce a new adaptive equator bias algorithm allowing for different attention distributions at different longitudes, which is more consistent with viewer's visual behavior. Combining the U-Autoencoder network containing the decoupled sampling with the adaptive equator bias algorithm, we construct a 360-degree visual saliency detection model. We map the original panorama into a cube, and then use the the cube isometric mapping method to remap it into a panorama and input it into the network for training. Then, the crude saliency map output by the decoder is combined with the equator bias map to obtain the final saliency map. The results show that the model proposed is superior to recent state-of-the-art models in terms of computational speed and saliency-map prediction.
C1 [Zhang, Ripei; Chen, Chunyi; Zhang, Jiacheng; Peng, Jun; Alzbier, Ahmed Mustafa Taha] Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
C3 Changchun University of Science & Technology
RP Chen, CY (corresponding author), Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
EM chenchunyi@hotmail.com
RI Zhang, Jiacheng/HTS-3961-2023; Zhang, RIpei/HGC-2847-2022
FU National Natural Science Foundation of China [U19A2063]; Jilin
   Provincial Science & Technology Development Program of China
   [20190302113GX]
FX This work was supported partially by the National Natural Science
   Foundation of China under Grant U19A2063, partially by the Jilin
   Provincial Science & Technology Development Program of China under Grant
   20190302113GX.
CR Azaza A, 2018, INT MULTICONF SYST, P688, DOI 10.1109/SSD.2018.8570418
   Blu T, 2004, IEEE T IMAGE PROCESS, V13, P710, DOI 10.1109/TIP.2004.826093
   Bogdanova I, 2010, COMPUT VIS IMAGE UND, V114, P100, DOI 10.1016/j.cviu.2009.09.003
   Bylinskii Z, 2016, LECT NOTES COMPUT SC, V9909, P809, DOI 10.1007/978-3-319-46454-1_49
   Chang Kang-Tsung., 2009, Computation for Bilinear Interpolation, V5th
   Chang M., 2016, INT C AUGM REAL
   Cheng, 2018, 2018 IEEE CVF C COMP
   Cohen Taco S, 2018, ARXIV PREPRINT ARXIV
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Ding Y, 2018, LECT NOTES COMPUT SC, V11165, P418, DOI 10.1007/978-3-030-00767-6_39
   Eder Marc, 2019, ARXIV PREPRINT ARXIV
   Fang CL, 2018, PROC INT CONF EDU IN, P1, DOI 10.1109/EITT.2018.00009
   Haoran L., 2020, P 28 ACM INT C MULT
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huang XG, 2015, 2015 IEEE International Conference on Applied Superconductivity and Electromagnetic Devices (ASEMD), P262, DOI 10.1109/ASEMD.2015.7453564
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia L., 2014, OBJECT BASED VISUAL
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Junting P., 2017, ARXIV PREPRINT ARXIV
   Keinert B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818131
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Le H., 2018, CHIN C PATT REC COMP, P186
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Lehmann TM, 2001, IEEE T MED IMAGING, V20, P660, DOI 10.1109/42.932749
   Li X., 2021, IEEE GEOSCI REMOTE S
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Martin D., 2020, P IEEE C COMP VIS PA, P1
   Martin D., 2020, CVPR WORKSH COMP VIS
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Pan J., 2017, PROC IEEE C COMPUT V
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sa, 2015, IMPROVED BILINEAR IN
   Schroers C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3225150
   Setlur V., 2004, MUM05 INT C MOB UB M
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967
   University of Nantes Technicolor, 2017, IEEE INT C MULT EXP
   Wenguan Wang, 2018, IEEE Transactions on Image Processing, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Xia C, 2016, IEEE T NEUR NET LEAR, V27, P1227, DOI 10.1109/TNNLS.2015.2512898
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 47
TC 4
Z9 5
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1163
EP 1180
DI 10.1007/s00371-021-02395-w
EA FEB 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000753798900001
DA 2024-07-18
ER

PT J
AU Zhang, M
   Hu, HY
   Li, ZJ
   Chen, J
AF Zhang, Min
   Hu, Haiyang
   Li, Zhongjin
   Chen, Jie
TI Action detection with two-stream enhanced detector
SO VISUAL COMPUTER
LA English
DT Article
DE Action detection; Spatiotemporal localization; Object detection; Anchor
   cuboid
ID ATTENTION
AB Action understanding in videos is a challenging task that has attracted widespread attention in recent years. Most current methods localize bounding box of actors at frame level, and then track or link these detections to form action tubes across frames. These methods often focus on utilizing temporal context in videos while neglecting the importance of the detector itself. In this paper, we present a two-stream enhanced framework to deal with the problem of action detection. Specifically, we devise an appearance and motion detectors in two-stream manner to detect actions, which take k consecutive RGB frames and optical flow images as input respectively. To improve the feature presentation capabilities, anchor refinement sub-module with feature alignment is introduced into the two-stream architecture to generate flexible anchor cuboids. Meanwhile, hierarchical fusion strategy is utilized to concatenate intermediate feature maps for capturing fast moving subjects. Moreover, layer normalization with skip connection is adopted to reduce the internal co-variate shift between network layers, which makes the training process simple and effective. Compared to state-of-the-art methods, the proposed approach yields impressive performance gain on three prevailing datasets: UCF-Sports, UCF-101 and J-HMDB, which confirm the effectiveness of our enhanced detector for action detection.
C1 [Zhang, Min; Hu, Haiyang; Li, Zhongjin; Chen, Jie] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
C3 Hangzhou Dianzi University
RP Hu, HY (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
EM huhaiyang@hdu.edu.cn
OI hu, haiyang/0000-0002-6070-8524; zhang, min/0000-0002-6059-3798
FU National Natural Science Foundation of China [61572251, 61572162,
   61702144, 61802095]; Natural Science Foundation of Zhejiang Province
   [LQ17F020003]; Key Science and Technology Project Foundation of Zhejiang
   Province [2018C01012]
FX This work is partially supported by National Natural Science Foundation
   of China (Grant nos. 61572251, 61572162, 61702144 and 61802095), the
   Natural Science Foundation of Zhejiang Province (LQ17F020003), the Key
   Science and Technology Project Foundation of Zhejiang Province
   (2018C01012).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Bochkovskiy A., 2020, PREPRINT
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Dai C, 2020, APPL SOFT COMPUT, V86, DOI 10.1016/j.asoc.2019.105820
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Dong EN, 2021, VISUAL COMPUT, V37, P567, DOI 10.1007/s00371-020-01824-6
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gong KC, 2021, VISUAL COMPUT, V37, P371, DOI 10.1007/s00371-020-01805-9
   Gu RS, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P163, DOI 10.1109/MIPR.2019.00036
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Li CX, 2019, COGN COMPUT SYST, V1, P20, DOI 10.1049/ccs.2018.0005
   Li D, 2018, LECT NOTES COMPUT SC, V11210, P306, DOI 10.1007/978-3-030-01231-1_19
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li Wei, 2019, ARXIV190701847
   Li YX, 2020, AAAI CONF ARTIF INTE, V34, P11466
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Mandal M, 2021, IEEE T IMAGE PROCESS, V30, P546, DOI 10.1109/TIP.2020.3037472
   Nawaratne R, 2020, IEEE T IND INFORM, V16, P393, DOI 10.1109/TII.2019.2938527
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727
   Saha S., 2016, BMVC
   Simonyan K., 2014, CORR
   Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393
   Soomro K., 2012, ARXIV12120402CS
   Tan M., 2020, P 2020 IEEE CVF C CO, P10
   Trudinger, 2001, CLASSICS MATH
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang DQ, 2019, IEEE INT CONF ROBOT, P8853, DOI [10.1109/ICRA.2019.8794224, 10.1109/icra.2019.8794224]
   Wang XY, 2013, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2013.10
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085
   Wu YT, 2020, INT CONF ACOUST SPEE, P2388, DOI [10.1109/ICASSP40776.2020.9054394, 10.1109/icassp40776.2020.9054394]
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Zhao JJ, 2019, PROC CVPR IEEE, P9927, DOI 10.1109/CVPR.2019.01017
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zhou X., 2019, ABS190407850 ARXIV
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 52
TC 2
Z9 2
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1193
EP 1204
DI 10.1007/s00371-021-02397-8
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000751592300002
DA 2024-07-18
ER

PT J
AU Li, ZX
   Lu, SH
   Dong, YS
   Guo, JY
AF Li, Zhaoxin
   Lu, Shuhua
   Dong, Yishan
   Guo, Jingyuan
TI MSFFA: a multi-scale feature fusion and attention mechanism network for
   crowd counting
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd counting; Multi-scale; Attention mechanism; Mixed loss function
ID CONVOLUTIONAL NEURAL-NETWORK
AB Crowd counting has been a growing hot topic in the computer vision community in recent years due to its extensive applications in the fields of public safety and commercial planning. However, up to now, it has been still a challenging task in realistic scenes owing to large-scale variations and complex background interference. In this paper, we have proposed an efficient end-to-end Multi-Scale Feature Fusion and Attention mechanism CNN network, named as MSFFA. The presented network consists of three parts: the front-end of the low-level feature extractor, the mid-end of the multi-scale feature fusion operator and the back-end of the density map generator. Among them, most significantly, in the mid-end, we stack three MSFF blocks with the residual connection, which on the one hand, makes the network obtain large-scale continuous variations and on the other hand, enhances the information transmission. Meanwhile, a global attention mechanism module is employed to extract effective features in complex background scenes. Our method has been evaluated on three public datasets, including ShanghaiTech, UCF-QNRF and UCF_CC_50. Experimental results show that our method outperforms some existing advanced approaches, indicating its excellent accuracy and stability.
C1 [Li, Zhaoxin; Lu, Shuhua; Dong, Yishan; Guo, Jingyuan] Peoples Publ Secur Univ China, Coll Informat & Cyber Secur, Beijing 102600, Peoples R China.
C3 People's Public Security University of China
RP Lu, SH (corresponding author), Peoples Publ Secur Univ China, Coll Informat & Cyber Secur, Beijing 102600, Peoples R China.
EM lushuhua@ppsuc.edu.cn
FU Public Security Subject Basic Theory Research Project [2021XKZX08];
   Fundamental Research Funds for the Central Universities [2021JKF102];
   Open Research Fund of the Public Security Behavioral Science Laboratory,
   People's Public Security University of China [2020SYS16]
FX This work is partially supported by Public Security Subject Basic Theory
   Research Project (2021XKZX08), Fundamental Research Funds for the
   Central Universities (2021JKF102) and Open Research Fund of the Public
   Security Behavioral Science Laboratory (2020SYS16), People's Public
   Security University of China.
CR Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Cheng JP, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P8107
   Dai F, 2019, ARXIV190609707
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dong L, 2020, INFORM SCIENCES, V528, P79, DOI 10.1016/j.ins.2020.04.001
   Gao JY, 2020, IEEE T CIRC SYST VID, V30, P3486, DOI 10.1109/TCSVT.2019.2919139
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Leibe B, 2005, PROC CVPR IEEE, P878
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu YB, 2021, APPL INTELL, V51, P427, DOI 10.1007/s10489-020-01842-w
   Luo A., 2020, AAAI CONF ARTIF INTE, P11693
   Ma JJ, 2019, NEUROCOMPUTING, V350, P91, DOI 10.1016/j.neucom.2019.03.065
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Miao YQ, 2019, PATTERN RECOGN LETT, V125, P113, DOI 10.1016/j.patrec.2019.04.012
   Oh MH, 2020, AAAI CONF ARTIF INTE, V34, P11799
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Topkaya IS, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P313, DOI 10.1109/AVSS.2014.6918687
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang SZ, 2020, NEUROCOMPUTING, V404, P227, DOI 10.1016/j.neucom.2020.04.139
   Wang YJ, 2020, NEUROCOMPUTING, V411, P1, DOI 10.1016/j.neucom.2020.06.034
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Xie J, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-2969-8
   Zeng LK, 2017, IEEE IMAGE PROC, P465, DOI 10.1109/ICIP.2017.8296324
   Zeng X, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112977
   Zhang B, 2021, NEUROCOMPUTING, V451, P12, DOI 10.1016/j.neucom.2021.04.045
   Zhang L, 2018, IEEE WINT CONF APPL, P1113, DOI 10.1109/WACV.2018.00127
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhang YM, 2019, NEUROCOMPUTING, V329, P144, DOI 10.1016/j.neucom.2018.10.058
   Zhou OETY, 2022, IEEE T PATTERN ANAL, V44, P3602, DOI 10.1109/TPAMI.2021.3056518
   Zhu FS, 2021, NEUROCOMPUTING, V423, P46, DOI 10.1016/j.neucom.2020.09.059
   Zhu M, 2020, PATTERN RECOGN LETT, V135, P279, DOI 10.1016/j.patrec.2020.05.009
NR 43
TC 8
Z9 8
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1045
EP 1056
DI 10.1007/s00371-021-02383-0
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741931800001
DA 2024-07-18
ER

PT J
AU Wu, Y
   Yang, J
AF Wu, Yan
   Yang, Jun
TI Multi-part shape matching by simultaneous partial functional
   correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-part shape matching; Partial functional maps; Fully spectral;
   Hamiltonian eigenvalue equivalence; Upsampling refinement
ID OPTIMIZATION; SIMILARITY; OBJECTS
AB Non-rigid multi-part shape matching has proven to be essential and challenging in many applications. This paper analyzes the aforementioned problem and proposes a novel multi-part shape matching method to simultaneously compute correspondences between a full shape and its multiple parts undergoing a non-rigid deformation. The main idea is to simultaneously integrate the Hamiltonian eigenvalue equivalence strategy as a part regularization term being fully spectral with the partial functional map. Moreover, we introduce a new upsampling refinement approach based upon ZoomOut in conjunction with the regularized point-wise map recovery algorithm to obtain high-quality partial matches. Our method naturally handles various challenges and noise that commonly occur in real scans, like non-rigid deformations, strong partiality, topological noise, and symmetric ambiguity. Finally, we demonstrate superior qualitative and quantitative results on several datasets. We show that our method produces more accurate, smoother results than other competing methods in realistic scenarios.
C1 [Wu, Yan; Yang, Jun] Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Peoples R China.
   [Wu, Yan] Fujian Polytech Normal Univ, Sch Elect & Informat Engn, Fuqing, Peoples R China.
C3 Lanzhou Jiaotong University; Fujian Polytechnic Normal University
RP Yang, J (corresponding author), Lanzhou Jiaotong Univ, Sch Elect & Informat Engn, Lanzhou, Peoples R China.
EM yangj@mail.lzjtu.cn
FU National Natural Science Foundation of China [61862039]
FX This work was supported by the National Natural Science Foundation of
   China [Grant Number 61862039].
CR Aflalo Y, 2016, INT J COMPUT VISION, V118, P380, DOI 10.1007/s11263-016-0883-8
   Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Aigerman N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766921
   Albarelli A, 2015, PATTERN RECOGN, V48, P2209, DOI 10.1016/j.patcog.2015.01.020
   Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Bronstein Alexander M., 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563077
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2009, INT J COMPUT VISION, V84, P163, DOI 10.1007/s11263-008-0147-3
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Chou Y.-C., 2018, P IEEE T VIS COMP GR, P1
   Cohen A, 2020, COMPUT GRAPH FORUM, V39, P555, DOI 10.1111/cgf.13952
   Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P1, DOI 10.1007/978-3-030-58565-5_1
   Cosmo L, 2017, COMPUT GRAPH FORUM, V36, P209, DOI 10.1111/cgf.12796
   Cosmo L, 2016, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2016.10
   Cosmo Luca, 2016, EUR WORKSH 3D OBJ RE, V2, P12
   Eynard D, 2016, INT CONF 3D VISION, P399, DOI 10.1109/3DV.2016.49
   Ezuz D, 2019, COMPUT GRAPH FORUM, V38, P121, DOI 10.1111/cgf.13624
   Ezuz D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3202660
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P165, DOI 10.1111/cgf.13254
   Gasparetto A, 2017, INT CONF 3D VISION, P477, DOI 10.1109/3DV.2017.00061
   Gasparetto A, 2015, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2015.7298726
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Huang QX, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2601097.2601111, 10.1145/2535596]
   Huang RQ, 2020, COMPUT GRAPH FORUM, V39, P265, DOI 10.1111/cgf.14084
   Johnson-Roberson M, 2017, J FIELD ROBOT, V34, P625, DOI 10.1002/rob.21658
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kovnatsky A, 2013, COMPUT GRAPH FORUM, V32, P439, DOI 10.1111/cgf.12064
   Kovnatsky A, 2016, LECT NOTES COMPUT SC, V9909, P680, DOI 10.1007/978-3-319-46454-1_41
   Kovnatsky A, 2015, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2015.7298692
   Lahner Z., 2016, EUR WORKSH 3D OBJ RE
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Litany O, 2017, COMPUT GRAPH FORUM, V36, P247, DOI 10.1111/cgf.13123
   Litany O, 2016, COMPUT GRAPH FORUM, V35, P135, DOI 10.1111/cgf.12970
   Litany O, 2012, LECT NOTES COMPUT SC, V7583, P1, DOI 10.1007/978-3-642-33863-2_1
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Melzi S, 2018, COMPUT GRAPH FORUM, V37, P20, DOI 10.1111/cgf.13309
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Nogneng D, 2018, COMPUT GRAPH FORUM, V37, P179, DOI 10.1111/cgf.13352
   Nogneng D, 2017, COMPUT GRAPH FORUM, V36, P259, DOI 10.1111/cgf.13124
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Papaionnou G, 2001, IEEE COMPUT GRAPH, V21, P53, DOI 10.1109/38.909015
   Pokrass J, 2013, COMPUT GRAPH FORUM, V32, P459, DOI 10.1111/cgf.12066
   Postolache E, 2020, COMPUT GRAPH FORUM, V39, P103, DOI 10.1111/cgf.14072
   Rampini A, 2019, INT CONF 3D VISION, P37, DOI 10.1109/3DV.2019.00014
   Ren J, 2019, COMPUT GRAPH FORUM, V38, P39, DOI 10.1111/cgf.13788
   Rodolà E, 2017, COMPUT GRAPH FORUM, V36, P700, DOI 10.1111/cgf.13160
   Rodola E., 2017, Computer Graphics Forum, V36, P222, DOI 10.1111/cgf.12797
   Rodola E., 2015, P INT S VIS MOD VIS, P25
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Roufosse JM, 2019, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2019.00170
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P121, DOI 10.1111/cgf.12480
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P63, DOI 10.1111/cgf.12278
   Sahillioglu Y, 2020, VISUAL COMPUT, V36, P1705, DOI 10.1007/s00371-019-01760-0
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Torsello A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2441, DOI 10.1109/CVPR.2011.5995565
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wang Y., 2018, ACM T GRAPHIC, V38, P21
   Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171
   Windheuser T, 2011, COMPUT GRAPH FORUM, V30, P1471, DOI 10.1111/j.1467-8659.2011.02021.x
   Wu Y, 2020, COMPUT GRAPH-UK, V92, P99, DOI 10.1016/j.cag.2020.09.004
   [杨军 Yang Jun], 2019, [激光与光电子学进展, Laser & Optoelectronics Progress], V56
   Yang Jun, 2018, Geomatics and Information Science of Wuhan University, V43, P1518, DOI 10.13203/j.whugis20160493
   Yuille A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P344, DOI 10.1109/CCV.1988.590011
NR 72
TC 2
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 393
EP 412
DI 10.1007/s00371-021-02337-6
EA JAN 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000740220700002
DA 2024-07-18
ER

PT J
AU Xu, D
   Li, Z
   Cao, Q
AF Xu, Di
   Li, Zhen
   Cao, Qi
TI Object-based illumination transferring and rendering for applications of
   mixed reality
SO VISUAL COMPUTER
LA English
DT Article
DE Lighting estimation; Illumination transferring; Virtual object
   rendering; Mixed reality applications
AB In applications of augmented reality or mixed reality, rendering virtual objects in real scenes with consistent illumination is crucial for realistic visualization experiences. Prior learning-based methods reported in the literature usually attempt to reconstruct complicated high dynamic range environment maps from limited input, and rely on a separate rendering pipeline to light up the virtual object. In this paper, an object-based illumination transferring and rendering algorithm is proposed to tackle this problem within a unified framework. Given a single low dynamic range image, instead of recovering lighting environment of the entire scene, the proposed algorithm directly infers the relit virtual object. It is achieved by transferring implicit illumination features which are extracted from its nearby planar surfaces. A generative adversarial network is adopted in the proposed algorithm for implicit illumination features extraction and transferring. Compared to previous works in the literature, the proposed algorithm is more robust, as it is able to efficiently recover spatially varying illumination in both indoor and outdoor scene environments. Experiments have been conducted. It is observed that notable experiment results and comparison outcomes have been obtained quantitatively and qualitatively by the proposed algorithm in different environments. It shows the effectiveness and robustness for realistic virtual object insertion and improved realism.
C1 [Xu, Di] Shadow Creator Inc, AI Lab, Beijing, Peoples R China.
   [Li, Zhen] Northwestern Polytech Univ, Xian, Peoples R China.
   [Cao, Qi] Univ Glasgow, Sch Comp Sci, Singapore Campus, Singapore, Singapore.
C3 Northwestern Polytechnical University
RP Cao, Q (corresponding author), Univ Glasgow, Sch Comp Sci, Singapore Campus, Singapore, Singapore.
EM di.xu@ivglass.com; yodlee@mail.nwpu.edu.cn; qi.cao@glasgow.ac.uk
RI Cao, Qi/AAU-1127-2021
OI Cao, Qi/0000-0003-3243-5693
FU National Natural Science Foundation of China [61801391]; Open Project
   Program of the National Laboratory of Pattern Recognition [202000025];
   China Postdoctoral Science Foundation [2018M631193]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61801391, in part by Open Project
   Program of the National Laboratory of Pattern Recognition under Grant
   202000025, in part by China Postdoctoral Science Foundation under Grant
   2018M631193.
CR [Anonymous], 2008, SIGGRAPH 2008 Classes
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Bui G, 2018, VISUAL COMPUT, V34, P829, DOI 10.1007/s00371-018-1550-6
   Calian DA, 2018, COMPUT GRAPH FORUM, V37, P51, DOI 10.1111/cgf.13341
   Chauve AL, 2010, PROC CVPR IEEE, P1261, DOI 10.1109/CVPR.2010.5539824
   Cheng DC, 2018, COMPUT GRAPH FORUM, V37, P213, DOI 10.1111/cgf.13561
   Debevec P.E., 2008, P 24 ANN C COM GRAPH, P1
   Debevec P, 2012, SIGGRAPH '12: SPECIAL INTEREST GROUP ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES CONFERENCE, DOI 10.1145/2343045.2343058
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gardner M.-A., ACM T GRAPH SIGGRAPH
   Gardner MA, 2019, IEEE I CONF COMP VIS, P7174, DOI 10.1109/ICCV.2019.00727
   Garon M, 2019, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2019.00707
   Georgoulis S, 2017, IEEE I CONF COMP VIS, P5180, DOI 10.1109/ICCV.2017.553
   Gkitsas V, 2020, IEEE COMPUT SOC CONF, P2719, DOI 10.1109/CVPRW50498.2020.00328
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Hold-Geoffroy Y, 2019, PROC CVPR IEEE, P6920, DOI 10.1109/CVPR.2019.00709
   Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255
   Jacobs K, 2010, VISUAL COMPUT, V26, P171, DOI 10.1007/s00371-009-0360-2
   Johnson MK, 2011, PROC CVPR IEEE
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Kipf TN, 2017, INT C LEARN REPR
   LeGendre C, 2019, PROC CVPR IEEE, P5911, DOI 10.1109/CVPR.2019.00607
   Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458
   Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Metz L., 2016, INT C LEARN REPR
   Pei SC, 2017, IEEE T MULTIMEDIA, V19, P1956, DOI 10.1109/TMM.2017.2688924
   Reinhard E., 2010, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting
   Ren ZG, 2013, VISUAL COMPUT, V29, P927, DOI 10.1007/s00371-013-0853-x
   Song SR, 2019, PROC CVPR IEEE, P6911, DOI 10.1109/CVPR.2019.00708
   Srinivasan Pratul P., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8077, DOI 10.1109/CVPR42600.2020.00810
   Tarko J, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1177, DOI 10.1109/VR.2019.8798067
   Tsai G, 2011, IEEE I CONF COMP VIS, P121, DOI 10.1109/ICCV.2011.6126233
   Weber H, 2018, INT CONF 3D VISION, P199, DOI 10.1109/3DV.2018.00032
   Wei Xingkui, 2020, ARXIV PREPRINT ARXIV
   Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232
   Wu CL, 2011, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2011.5995388
   Xu D, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P703, DOI [10.1109/VRW50115.2020.00202, 10.1109/VRW50115.2020.00-73]
   Xu D, 2018, IEEE T PATTERN ANAL, V40, P423, DOI 10.1109/TPAMI.2017.2671458
   Xu D, 2014, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2014.198
   Yi RJ, 2018, LECT NOTES COMPUT SC, V11213, P321, DOI 10.1007/978-3-030-01240-3_20
   Zhang JS, 2019, PROC CVPR IEEE, P10150, DOI 10.1109/CVPR.2019.01040
   Zhang JS, 2017, IEEE I CONF COMP VIS, P4529, DOI 10.1109/ICCV.2017.484
   Zhu MH, 2017, VISUAL COMPUT, V33, P1385, DOI 10.1007/s00371-016-1286-0
NR 44
TC 3
Z9 3
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4251
EP 4265
DI 10.1007/s00371-021-02292-2
EA OCT 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000704522700001
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Peng, YQ
   Wang, CX
   Pei, YX
   Li, YJ
AF Peng, Yuqing
   Wang, Chenxi
   Pei, Yixin
   Li, Yingjun
TI Video captioning with global and local text attention
SO VISUAL COMPUTER
LA English
DT Article
DE Video captioning; Global control; Local strengthening; Bidirectional
ID AGGREGATION
AB The task of video captioning is to generate a video description corresponding to the video content, so there are stringent requirements for the extraction of fine-grained video features and the language processing of tag text. A new method using global control of the text and local strengthening during training is proposed in this paper. In this method, the context can be referred to when the training generates text. In addition, more attention is given to important words in the text, such as nouns and predicate verbs, and this approach greatly improves the recognition of objects and provides more accurate prediction of actions in the video. Moreover, in this paper, the authors adopt 2D and 3D multimodal feature extraction for the process of video feature extraction. Better results are achieved by the fine-grained feature capture of global attention and the fusion of bidirectional time flow. The method in this paper obtains good results on both the MSR-VTT and MSVD datasets.
C1 [Peng, Yuqing; Wang, Chenxi; Pei, Yixin; Li, Yingjun] Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
C3 Hebei University of Technology
RP Peng, YQ (corresponding author), Hebei Univ Technol, Sch Artificial Intelligence, Tianjin 300401, Peoples R China.
EM li_tiejun123@163.com
RI wang, chenxi/HMD-9902-2023; li, yingjun/HJB-0161-2022
OI Wang, Chenxi/0000-0001-8910-9501
FU Natural Science Foundation of Hebei Province [F2021202038]
FX This paper is supported by the Natural Science Foundation of Hebei
   Province (F2021202038).
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   [Anonymous], 2014, ARXIV14124729
   [Anonymous], 2012, CORPUS ENGLISH STOP
   Cherian A, 2020, IEEE WINT CONF APPL, P1606, DOI 10.1109/WACV45572.2020.9093291
   Cho K, 2015, IEEE C COMP VIS ICCV, V53, P199
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Jin T, 2019, NEUROCOMPUTING, V370, P118, DOI 10.1016/j.neucom.2019.08.042
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Peng, 2019, HIERARCHICAL VISIONL, P42
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   T Technicolor, 2012, NIPS
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang HY, 2020, PATTERN RECOGN LETT, V130, P327, DOI 10.1016/j.patrec.2018.07.024
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zanfir M, 2016, P AS C COMP VIS
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zheng Q., 2020, P IEEE CVF C COMP VI, P13096
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
NR 33
TC 1
Z9 1
U1 3
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4267
EP 4278
DI 10.1007/s00371-021-02294-0
EA SEP 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000693345100001
DA 2024-07-18
ER

PT J
AU Zhao, DY
   Gu, TT
   Liu, YS
   Gao, SM
   Li, M
AF Zhao, Dengyang
   Gu, Ting Ting
   Liu, Yusheng
   Gao, Shuming
   Li, Ming
TI Constructing self-supporting structures in biscale topology optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Biscale topology optimization; Self-supporting; Print-ready; Additive
   manufacturing
ID DESIGN; COMPOSITES; CODE
AB The self-supporting requirement is very necessary in additive manufacturing so that the printed structure will not collapse during fabrication. Imposing the self-supporting constraint on topology optimization allows for designing a performance optimized structure that is ready-to-print. However, although biscale topology optimization has been widely studied, conducting self-supporting topology optimization separately for the macro-structure and for each micro-structure is not sufficient to produce an overall self-supporting structure, as first observed in this study. The issue is resolved via an approach to bridge the gap between the requirements of self-supporting at the two scales via distinguishing the macro-cells based on their relative locations. In addition, the self-supporting constraint is expressed as a simple quadratic function included in the topology optimization in both scales, and a convolution operator is designed to efficiently implement its detection. Ultimately, a completely self-supporting overall structure is generated within a biscale topology optimization framework and extends its potentiality to produce design to be directly fabricated via additive manufacturing. Performance of the approach is demonstrated via various 2D and 3D examples.
C1 [Zhao, Dengyang; Liu, Yusheng; Gao, Shuming; Li, Ming] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Gu, Ting Ting] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, M (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM liming@cad.zju.edu.cn
FU National Key Research and Development Program [2020YFC2201303]; NSF of
   China [61872320]
FX The valuable comments from the anonymous reviewers are greatly
   appreciated. The work described in the study is partially supported by
   National Key Research and Development Program (No. 2020YFC2201303), and
   the NSF of China (No. 61872320).
CR Abdulle A, 2009, COMPUT METHOD APPL M, V198, P2839, DOI 10.1016/j.cma.2009.03.019
   Andreassen E, 2014, COMP MATER SCI, V83, P488, DOI 10.1016/j.commatsci.2013.09.006
   Bendse M P., 1989, Struct. Optim., V1, P193, DOI DOI 10.1007/BF01650949
   BENDSOE MP, 1988, COMPUT METHOD APPL M, V71, P197, DOI 10.1016/0045-7825(88)90086-2
   Brackett D., 2011, Proceedings of the 24th Solid Freeform Fabrication Symposium (SFF 11 ׳), P6
   Daniel T., 2009, DEV DESIGN RULES SEL
   Dumas J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601153
   Groen JP, 2018, INT J NUMER METH ENG, V113, P1148, DOI 10.1002/nme.5575
   Guo X, 2017, COMPUT METHOD APPL M, V323, P27, DOI 10.1016/j.cma.2017.05.003
   Hu KL, 2015, COMPUT AIDED DESIGN, V65, P1, DOI 10.1016/j.cad.2015.03.001
   Huang X, 2013, COMP MATER SCI, V67, P397, DOI 10.1016/j.commatsci.2012.09.018
   Huang XD, 2010, STRUCT MULTIDISCIP O, V41, P671, DOI 10.1007/s00158-010-0487-9
   Langelaar M, 2017, STRUCT MULTIDISCIP O, V55, P871, DOI 10.1007/s00158-016-1522-2
   Langelaar M, 2016, ADDIT MANUF, V12, P60, DOI 10.1016/j.addma.2016.06.010
   Li H, 2016, COMPUT METHOD APPL M, V309, P453, DOI 10.1016/j.cma.2016.06.012
   Liu, 2017, IEEE T VIS COMPUT GR, P99
   Liu JK, 2016, ADV ENG SOFTW, V100, P161, DOI 10.1016/j.advengsoft.2016.07.017
   Majhi J, 1999, COMP GEOM-THEOR APPL, V12, P241, DOI 10.1016/S0925-7721(99)00003-6
   Ming PB, 2006, J COMPUT PHYS, V214, P421, DOI 10.1016/j.jcp.2005.09.024
   Qian XP, 2017, INT J NUMER METH ENG, V111, P247, DOI 10.1002/nme.5461
   Qian XP, 2013, COMPUT METHOD APPL M, V265, P15, DOI 10.1016/j.cma.2013.06.001
   Sigmund O, 2013, STRUCT MULTIDISCIP O, V48, P1031, DOI 10.1007/s00158-013-0978-6
   SVANBERG K, 1987, INT J NUMER METH ENG, V24, P359, DOI 10.1002/nme.1620240207
   van Dijk NP, 2013, STRUCT MULTIDISCIP O, V48, P437, DOI 10.1007/s00158-013-0912-y
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P117, DOI 10.1111/cgf.12437
   Vatanabe SL, 2016, ADV ENG SOFTW, V100, P97, DOI 10.1016/j.advengsoft.2016.07.002
   Wang CCL, 2013, RAPID PROTOTYPING J, V19, P395, DOI 10.1108/RPJ-02-2012-0013
   Wang MY, 2003, COMPUT METHOD APPL M, V192, P227, DOI 10.1016/S0045-7825(02)00559-5
   Wu J, 2016, COMPUT AIDED DESIGN, V80, P32, DOI 10.1016/j.cad.2016.07.006
   Xia L, 2015, STRUCT MULTIDISCIP O, V52, P1229, DOI 10.1007/s00158-015-1294-0
   Xia L, 2014, COMPUT METHOD APPL M, V278, P524, DOI 10.1016/j.cma.2014.05.022
   XIE YM, 1993, COMPUT STRUCT, V49, P885, DOI 10.1016/0045-7949(93)90035-C
   Xie Y, 2017, VIS INFORM, V1, P9, DOI 10.1016/j.visinf.2017.01.002
   Xu C, 2017, COMPUT STRUCT, V182, P284, DOI 10.1016/j.compstruc.2016.12.006
   Yan X, 2014, COMPUT STRUCT, V133, P103, DOI 10.1016/j.compstruc.2013.12.001
   Zhao DY, 2021, VISUAL COMPUT, V37, P1169, DOI 10.1007/s00371-020-01860-2
   Zuo ZH, 2015, ADV ENG SOFTW, V85, P1, DOI 10.1016/j.advengsoft.2015.02.006
NR 38
TC 4
Z9 5
U1 3
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1065
EP 1082
DI 10.1007/s00371-021-02068-8
EA JUN 2021
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000665770500001
DA 2024-07-18
ER

PT J
AU Tang, PQ
   Li, JJ
   Ding, FF
   Chen, WK
   Li, XF
AF Tang, Peiqi
   Li, Jianjun
   Ding, Feifei
   Chen, Weikun
   Li, Xinfu
TI PSNet: change detection with prototype similarity
SO VISUAL COMPUTER
LA English
DT Article
DE Remote sensing image; Change detection; End-to-end method; Metric
   learning
ID NEURAL-NETWORKS; IMAGES
AB Change detection is a fundamental problem in remote sensing image processing. Due to the great advantages in learning the knowledge representations and the complex relationship from large-scale datasets, deep learning has made great progress in change detection tasks in remote sensing community. However, most of the existing methods based on deep learning for change detection are implemented by learning differences of image pairs directly without paying considerations in influences of unstructured and temporal changes, or called nature changes, such as light and seasonal changes. In this paper, an end-to-end deep learning network for remote sensing image change detection is proposed, aiming to accurately detect the change of regions from a high-resolution image pair by learning prototype similarity, in which the metric learning is used and it is one of the meta-learning methods to learn change prototypes from support image pairs. The similarity between the query image pairs and the change prototypes can be measured by a learnable CNN metric. The experimental results based on the two public change detection datasets of high-resolution satellite images, CDD and BCDD, show that our proposed method performs better than other state-of-the-art change detection methods with an improvement of 3.5% and 0.4%, respectively.
C1 [Tang, Peiqi; Li, Jianjun; Ding, Feifei; Chen, Weikun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
   [Li, Xinfu] China Elect Technol Grp, Inst 36, Jiaxing 100048, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University; China Electronics Technology Group
RP Li, JJ (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Zhejiang, Peoples R China.
EM 791035236@qq.com; lijjcan@gmail.com; dingfeifei7@gmail.com;
   chenweikun@gmail.com; lixinfu@gmail.com
RI Li, Xinfu/AAL-4461-2021; DING, FEI/KLD-8925-2024
OI Li, Jianjun/0000-0001-6658-9709
FU National Science Fund of China [61871170]; Key Research and Development
   Plan of Zhejiang [2021C03131, KY2017210A001]; Key Laboratory of Brain
   Machine Collaborative Intelligence of Zhejiang Province
FX This work was supported in part by National Science Fund of China: No.
   61871170; Key Research and Development Plan of Zhejiang: No. 2021C03131;
   The Basic Research Program of KY2017210A001; Key Laboratory of Brain
   Machine Collaborative Intelligence of Zhejiang Province.
CR Alcantarilla PF, 2018, AUTON ROBOT, V42, P1301, DOI 10.1007/s10514-018-9734-5
   [Anonymous], 2018, INT ARCH PHOTOGRAMM, V42, P565, DOI [https://doi.org/10.5194/isprs-archives-XLII-2-565-2018, 10.5194/isprs-archives-XLII-2-565-2018, DOI 10.5194/ISPRS-ARCHIVES-XLII-2-565-2018]
   Arabi ME, 2018, INT GEOSCI REMOTE SE, P5041, DOI 10.1109/IGARSS.2018.8518178
   Celik T, 2009, IEEE GEOSCI REMOTE S, V6, P772, DOI 10.1109/LGRS.2009.2025059
   Chen Jiacheng, 2020, ARXIV COMPUTER VISIO
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Daudt RC, 2018, IEEE IMAGE PROC, P4063, DOI 10.1109/ICIP.2018.8451652
   Dong N., 2018, BMVC, V4, P4
   Gil-Yepes JL, 2016, ISPRS J PHOTOGRAMM, V121, P77, DOI 10.1016/j.isprsjprs.2016.08.010
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang CQ, 2008, REMOTE SENS ENVIRON, V112, P970, DOI 10.1016/j.rse.2007.07.023
   Hussain M, 2013, ISPRS J PHOTOGRAMM, V80, P91, DOI 10.1016/j.isprsjprs.2013.03.006
   Ji SP, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111343
   Ji SP, 2019, IEEE T GEOSCI REMOTE, V57, P574, DOI 10.1109/TGRS.2018.2858817
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lv PY, 2018, IEEE T GEOSCI REMOTE, V56, P4002, DOI 10.1109/TGRS.2018.2819367
   Ma L, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090761
   Ma WP, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060626
   Papadomanolaki M, 2019, INT GEOSCI REMOTE SE, P214, DOI [10.1109/IGARSS.2019.8900330, 10.1109/igarss.2019.8900330]
   Peng DF, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111382
   Rakelly K., 2018, P ICLR WORKSH
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saha S, 2019, IEEE T GEOSCI REMOTE, V57, P3677, DOI 10.1109/TGRS.2018.2886643
   Snell J, 2017, ADV NEUR IN, V30
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Volpi M, 2013, INT J APPL EARTH OBS, V20, P77, DOI 10.1016/j.jag.2011.10.013
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wu C, 2017, REMOTE SENS ENVIRON, V199, P241, DOI 10.1016/j.rse.2017.07.009
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang MY, 2019, IEEE GEOSCI REMOTE S, V16, P266, DOI 10.1109/LGRS.2018.2869608
   Zhang YJ, 2018, IEEE GEOSCI REMOTE S, V15, P13, DOI 10.1109/LGRS.2017.2763182
NR 33
TC 4
Z9 4
U1 3
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3541
EP 3550
DI 10.1007/s00371-021-02177-4
EA JUN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000663221500001
DA 2024-07-18
ER

PT J
AU Junos, MH
   Khairuddin, ASM
   Thannirmalai, S
   Dahari, M
AF Junos, Mohamad Haniff
   Khairuddin, Anis Salwa Mohd
   Thannirmalai, Subbiah
   Dahari, Mahidzal
TI Automatic detection of oil palm fruits from UAV images using an improved
   YOLO model
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Machine vision; Object detection; Precision agriculture;
   Improved YOLO
ID OBJECT DETECTION
AB Manual harvesting of loose fruits in the oil palm plantation is both time consuming and physically laborious. Automatic harvesting system is an alternative solution for precision agriculture which requires accurate visual information of the targets. Current state-of-the-art one-stage object detection method provides excellent detection accuracy; however, it is computationally intensive and impractical for embedded system. This paper proposed an improved YOLO model to detect oil palm loose fruits from unmanned aerial vehicle images. In order to improve the robustness of the detection system, the images are augmented by brightness, rotation, and blurring to simulate the actual natural environment. The proposed improved YOLO model adopted several improvements; densely connected neural network for better feature reuse, swish activation function, multi-layer detection to enhance detection on small targets and prior box optimization to obtain accurate bounding box information. The experimental results show that the proposed model achieves outstanding average precision of 99.76% with detection time of 34.06 ms. In addition, the proposed model is also light in weight size and requires less training time which is significant in reducing the hardware costs. The results exhibit the superiority of the proposed improved YOLO model over several existing state-of-the-art detection models.
C1 [Junos, Mohamad Haniff; Khairuddin, Anis Salwa Mohd; Dahari, Mahidzal] Univ Malaya, Fac Engn, Dept Elect Engn, Kuala Lumpur 50603, Malaysia.
   [Thannirmalai, Subbiah] Sime Darby Technol Ctr Sdn Bhd, Adv Technol & Robot, Serdang 43400, Selangor, Malaysia.
C3 Universiti Malaya
RP Khairuddin, ASM (corresponding author), Univ Malaya, Fac Engn, Dept Elect Engn, Kuala Lumpur 50603, Malaysia.
EM anissalwa@um.edu.my
RI DAHARI, MAHIDZAL/B-5401-2010; MOHD KHAIRUDDIN, ANIS SALWA/B-5340-2010
OI DAHARI, MAHIDZAL/0000-0002-0432-5596; MOHD KHAIRUDDIN, ANIS
   SALWA/0000-0002-9873-4779
FU RU Grant-Faculty Programme by Faculty of Engineering, University of
   Malaya [GPF042A-2019]; Industry-driven Innovation Grant (IDIG)
   [PPSI-2020-CLUSTER-SD01]
FX The research funding was provided by RU Grant-Faculty Programme by
   Faculty of Engineering, University of Malaya with Project No.
   GPF042A-2019 and Industry-driven Innovation Grant (IDIG) with Project
   No.: PPSI-2020-CLUSTER-SD01.
CR Chen SW, 2017, IEEE ROBOT AUTOM LET, V2, P781, DOI 10.1109/LRA.2017.2651944
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen Y, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131584
   Dias PA, 2018, COMPUT IND, V99, P17, DOI 10.1016/j.compind.2018.03.010
   Dyrmann M., 2017, Advances in Animal Biosciences, V8, P842, DOI [DOI 10.1017/S2040470017000206, 10.1017/S2040470017000206]
   Gené-Mola J, 2019, COMPUT ELECTRON AGR, V162, P689, DOI 10.1016/j.compag.2019.05.016
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Hamza R, 2020, IET IMAGE PROCESS, V14, P561, DOI 10.1049/iet-ipr.2018.6524
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hendry, 2019, IMAGE VISION COMPUT, V87, P47, DOI 10.1016/j.imavis.2019.04.007
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Idrees, MALAYSIA PALM OIL IN
   Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
   Kapach K., 2012, International Journal of Computational Vision and Robotics, V3, P4, DOI [DOI 10.1504/IJCVR.2012.046419, 10.1504/IJCVR.2012.046419]
   Koirala A, 2019, PRECIS AGRIC, V20, P1107, DOI 10.1007/s11119-019-09642-0
   Lee E, 2019, IMAGE VISION COMPUT, V87, P24, DOI 10.1016/j.imavis.2019.04.003
   Li YD, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183781
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu GX, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072145
   Liu J, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00898
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maldonado W, 2016, COMPUT ELECTRON AGR, V127, P572, DOI 10.1016/j.compag.2016.07.023
   Min WD, 2019, IET IMAGE PROCESS, V13, P1041, DOI 10.1049/iet-ipr.2018.6449
   Ozturk T, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103792
   Park SE, 2020, CONSTR BUILD MATER, V252, DOI 10.1016/j.conbuildmat.2020.119096
   Qureshi WS, 2017, PRECIS AGRIC, V18, P224, DOI 10.1007/s11119-016-9458-5
   Ramachandran P., 2017, Searching for activation functions
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sa I, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081222
   Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian YN, 2019, J SENSORS, V2019, DOI 10.1155/2019/7630926
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Villamizar M, 2019, VISUAL COMPUT, V35, P349, DOI 10.1007/s00371-018-01617-y
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yamamoto K, 2014, SENSORS-BASEL, V14, P12191, DOI 10.3390/s140712191
   Zhang PY, 2019, IEEE INT CONF COMP V, P37, DOI 10.1109/ICCVW.2019.00011
   Zhao HP, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071861
   Zhao YS, 2016, COMPUT ELECTRON AGR, V127, P311, DOI 10.1016/j.compag.2016.06.022
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhu Pengfei, 2020, ARXIV
NR 46
TC 46
Z9 48
U1 22
U2 146
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2341
EP 2355
DI 10.1007/s00371-021-02116-3
EA APR 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000638818700001
DA 2024-07-18
ER

PT J
AU Max, N
AF Max, Nelson
TI Global illumination in sparse voxel octrees
SO VISUAL COMPUTER
LA English
DT Article
DE global illumination; Octree; Multiple scattering
ID MULTIPLE-SCATTERING; DIFFUSION
AB This paper enhances the voxel-to-voxel radiance shooting, propagation, and scattering algorithm of Max [20]. It reduces the memory requirements by storing the radiance only on the occupied cells of a sparse voxel octree and by sampling only 24 propagation direction bins instead of 96 or more. It gives a modification of the propagation algorithm to compensate for the larger solid angle of the direction bins and allows for position-dependent fully anisotropic scattering. For a volume of n(3) voxels, the computation time is O(n(3)(log n)(3/2)).
C1 [Max, Nelson] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Max, N (corresponding author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
EM max@cs.ucdavis.edu
CR Bouthors A, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P173
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Cohen M. F., 1988, Computer Graphics, V22, P75, DOI 10.1145/378456.378487
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Elek O, 2014, COMPUT GRAPH-UK, V45, P28, DOI 10.1016/j.cag.2014.08.003
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477933
   Ge LS, 2021, IEEE T VIS COMPUT GR, V27, P3123, DOI 10.1109/TVCG.2019.2963015
   Goral C. M., 1984, Computers & Graphics, V18, P213
   Heitz E., 2012, EUR ACM SIGGRAPH S H, DOI [10.2312/EGGH/HPG12/125-134, DOI 10.2312/EGGH/HPG12/125-134]
   Immel D. S., 1986, Computer Graphics, V20, P133, DOI 10.1145/15886.15901
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kallweit S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130880
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Koerner D, 2014, COMPUT GRAPH FORUM, V33, P178, DOI 10.1111/cgf.12342
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Krivánek J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601219
   Max N., 2004, THE J, V12, P277
   Max N.L., 1994, WORKSHOP RENDERING, P87
   Max N, 2018, VISUAL COMPUT, V34, P443, DOI 10.1007/s00371-017-1352-2
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   PATMORE C, 1993, IFIP TRANS B, V9, P29
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   Stam J, 1995, SPRING COMP SCI, P41
   Tan ZQ, 1989, J HEAT TRANS-T ASME, V111, P141, DOI 10.1115/1.3250636
   Watanabe J, 1960, SECRETS JUDO
NR 27
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1443
EP 1456
DI 10.1007/s00371-021-02078-6
EA MAR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000626398100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Beaini, D
   Achiche, S
   Duperré, A
   Raison, M
AF Beaini, Dominique
   Achiche, Sofiane
   Duperre, Alexandre
   Raison, Maxime
TI Deep green function convolution for improving saliency in convolutional
   neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Green&#8217; s function convolution; Gradient
   integration and sum; Saliency improvement; Deep learning
ID OPTIMIZATION
AB Current saliency methods require to learn large-scale regional features using small convolutional kernels, which is not possible with a simple feed-forward network. Some methods solve this problem by using segmentation into superpixels, while others downscale the image through the network and rescale it back to its original size. The objective of this paper is to show that saliency convolutional neural networks (CNN) can be improved by using a Green's function convolution (GFC) to extrapolate edges features into salient regions. The GFC acts as a gradient integrator, allowing to produce saliency features by filling thin edges directly inside the CNN. Hence, we propose the gradient integration and sum (GIS) layer that combines the edges features with the saliency features. Using the HED and DSS architecture, we demonstrated that adding a GIS layer near the network's output allows to reduce the sensitivity to the parameter initialization, to reduce the overfitting and to improve the repeatability of the training. By simply adding a GIS layer to the state-of-the-art DSS model, there is an absolute increase of 1.6% for the F-measure on the DUT-OMRON dataset, with only 10 ms of additional computation time. The GIS layer further allows the network to perform significantly better in the case of highly noisy images or low-brightness images. In fact, we observed an F-measure improvement of 5.2% when noise was added to the dataset and 2.8% when the brightness was reduced. Since the GIS layer is model agnostic, it can be implemented into different fully convolutional networks. Further, we showed that it outperforms the denseCRF post-processing method and is 40 times faster. A major contribution of the current work is the first implementation of Green's function convolution inside a neural network, which allows the network, via very minor architectural changes and no additional parameters, to operate in the feature domain and in the gradient domain at the same time, thus improving the regional representation via edge filling.
C1 [Beaini, Dominique; Achiche, Sofiane; Duperre, Alexandre; Raison, Maxime] Ecole Polytech Montreal, Montreal, PQ, Canada.
C3 Universite de Montreal; Polytechnique Montreal
RP Beaini, D (corresponding author), Ecole Polytech Montreal, Montreal, PQ, Canada.
EM Dominique.beaini@outlook.com
RI Achiche, Sofiane/A-7154-2012
OI Raison, Maxime/0000-0002-0311-456X
CR [Anonymous], 2000, Mathematical Methods for Physicists
   [Anonymous], 2012, ARXIV12105644CS
   [Anonymous], 2011, OpenCV 2 Computer Vision Application Programming Cookbook: Over 50 recipes to master this library of programming functions for real-time computer vision
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Beaini D, 2019, ARXIV190200176
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cheng M.M, PROC CVPR IEEE
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Corke P., 2011, Robotics, vision and control: fundamental algorithms in MATLAB, V73
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hou Q., PROC CVPR IEEE
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Ilbery P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508426
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T., 2016, ARXIV160403605CS
   Kingma D. P., 2014, arXiv
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Lu Y, 2011, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2011.6126247
   McCann J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360692
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Raison M., 2018, ARXIV180601339CSMATH
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601187
   Tanaka M, FAST SEAMLESS IMAGE
   Tang LZ, 2018, IEEE SIGNAL PROC LET, V25, P491, DOI 10.1109/LSP.2018.2801821
   Torr P.H.S, 2018, ARXIV180309860CS
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yuille A.L, 2014, ARXIV14062807CS
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 44
TC 4
Z9 6
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 227
EP 244
DI 10.1007/s00371-020-01795-8
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000625179800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, MN
   Shang, XP
AF Wang, Monan
   Shang, Xiping
TI An improved simplified PCNN model for salient region detection
SO VISUAL COMPUTER
LA English
DT Article
DE Pulse coupled neural network (PCNN); Actual physical meanings; Pixel
   intensity; Salient region detection
ID COUPLED NEURAL-NETWORK; OBJECT DETECTION; LINKING; DESIGN
AB As PCNN is modulated by using the pulse-coupled synaptic mechanisms, it has a great potential for image processing in a complex real-world environment, especially in images. A new simplified pulse coupled neural network (SPCNN) is proposed. This new model uses the pixel intensity with the actual physical meanings as the input parameters instead of the abstract network parameters in the original SPCNN. In order to achieve this goal, we try to derive the general formulae of dynamic threshold and internal activity of the SPCNN according to the dynamic properties of neurons and then deduce the relationship between the pixel intensity and the abstract parameters. Then, the relationship is transformed into an objective optimization problem to obtain the appropriate abstract parameters. Finally, extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed method and shown improvement on the salient region detection.
C1 [Wang, Monan; Shang, Xiping] Harbin Univ Sci & Technol, Sch Mech & Power Engn, Harbin 150080, Heilongjiang, Peoples R China.
C3 Harbin University of Science & Technology
RP Wang, MN (corresponding author), Harbin Univ Sci & Technol, Sch Mech & Power Engn, Harbin 150080, Heilongjiang, Peoples R China.
EM mnwang@hrbust.edu.cn; 13465817867@163.cn
OI Wang, Monan/0000-0003-0927-6487
FU NSFC [61572159]
FX This study was funded by NSFC (No. 61572159).
CR Berg H, 2008, NEUROCOMPUTING, V71, P1980, DOI 10.1016/j.neucom.2007.10.018
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cao C., 2018, AAAI CONFERENCE ON A
   Chen YL, 2015, IEEE T NEUR NET LEAR, V26, P1682, DOI 10.1109/TNNLS.2014.2351418
   Chen Y, 2011, IEEE T NEURAL NETWOR, V22, P880, DOI 10.1109/TNN.2011.2128880
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan D., 2013, P INT JOINT C ART IN, P698
   Fan D-P, 2020, UC NET UNCERTAINTY I
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao C, 2014, NEURAL PROCESS LETT, V39, P81, DOI 10.1007/s11063-013-9291-z
   Gu XD, 2005, IEEE IJCNN, P1836
   Gu XD, 2008, NEURAL PROCESS LETT, V27, P25, DOI 10.1007/s11063-007-9057-6
   Guo YN, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.2.023022
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jianping S., 2016, IEEE T PATTERN ANAL
   Johnson JL, 1999, IEEE T NEURAL NETWOR, V10, P461, DOI 10.1109/TNN.1999.761704
   JOHNSON JL, 1993, OPT LETT, V18, P1253, DOI 10.1364/OL.18.001253
   JOHNSON JL, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P1279, DOI 10.1109/ICNN.1994.374368
   Kinser JM, 1996, P SOC PHOTO-OPT INS, V2760, P563, DOI 10.1117/12.235951
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lindblad T., 2005, IMAGE PROCESSING USI
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu SW, 2012, IEEE SIGNAL PROC LET, V19, P207, DOI 10.1109/LSP.2012.2187782
   Liu ZY, 2020, NEUROCOMPUTING, V387, P210, DOI 10.1016/j.neucom.2020.01.045
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma YD, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P743
   Ma Yi-de, 2002, Journal of China Institute of Communications, V23, P46
   Ma Yi-de, 2006, Journal of System Simulation, V18, P722
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Ming-Ming C., 2015, IEEE TPAMI
   Mirjalili S, 2016, KNOWL-BASED SYST, V96, P120, DOI 10.1016/j.knosys.2015.12.022
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Ranganath HS, 1997, NEURAL NETWORK IMAGE, DOI 10.1887/0750303123/b365c96
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang W., 2019, ARXIV190409146
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhan K, 2017, ARCH COMPUT METHOD E, V24, P573, DOI 10.1007/s11831-016-9182-3
   Zhan K, 2009, IEEE T NEURAL NETWOR, V20, P1980, DOI 10.1109/TNN.2009.2030585
   Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941
   Zhang P., 2018, ARXIV180802373
   Zhang P., 2018, ARXIV180405142
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhou DG, 2016, PATTERN ANAL APPL, V19, P939, DOI 10.1007/s10044-015-0462-6
NR 58
TC 1
Z9 1
U1 4
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 371
EP 383
DI 10.1007/s00371-020-02020-2
EA NOV 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000591567400001
DA 2024-07-18
ER

PT J
AU Zhao, J
   Xie, YN
   Tang, L
   He, YJ
AF Zhao, Jing
   Xie, Yining
   Tang, Lei
   He, Yongjun
TI Overlapping region reconstruction in nuclei image segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Nuclei reconstruction; Image inpainting; GMM-UBM; Feature abnormal
AB Automatic screening systems play an increasingly important role in the diagnosis of pathologists. Image measurement and classification are the key techniques of automatic screening systems, which directly determine the performance. The distortion in grey and texture after overlapping nuclei segmentation seriously degrades the DNA content measurement and nuclei classification. In order to solve this problem, this paper presents a new method to reconstruct the pixels in overlapping regions based on the GMM-UBM (Gaussian mixture model-universal background model). In this method, a large amount of data are first used to train a GMM (named UBM). Then, the GMM of each nucleus is derived by maximizing a posteriori adaptation with the UBM and the normal grey value of this nucleus. The grey values are randomly generated by the GMM and filled to the overlapping region, with the offset to fine-tuning the Gaussian components. Finally, the image inpainting algorithm is used to repair the connected region. Experimental results show that this method can effectively recover the nucleus features, such as texture, grey and optical density, and improve the accuracy of nucleus measurement and classification.
C1 [Zhao, Jing; Xie, Yining; Tang, Lei; He, Yongjun] Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xuefu Rd, Harbin 150080, Peoples R China.
C3 Harbin University of Science & Technology
RP He, YJ (corresponding author), Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xuefu Rd, Harbin 150080, Peoples R China.
EM jingzhaohlj@163.com; Holywit@163.com
OI He, Yongjun/0000-0002-5156-651X
FU National Natural Science Foundation of China [61673142]; Foundation of
   Education Department of Heilongjiang Province [12511096]; Natural
   Science Foundation of HeiLongjiang Province of China [JJ2019JQ0013,
   F2017013]; University Nursing Program for Young Scholars with Creative
   Talents in Heilongjiang Province [UNPYSCT-2016034]; Outstanding Youth
   Talent Foundation of Harbin of China [2017RAYXJ013]; Research Fund for
   the Doctoral Program of Higher Education of China [20132303120003];
   Science Funds for the Young Innovative Talents of HUST [20152]
FX This research is partly supported by The National Natural Science
   Foundation of China (61673142), the Foundation of Education Department
   of Heilongjiang Province (12511096), Natural Science Foundation of
   HeiLongjiang Province of China (F2017013), Natural Science Foundation of
   HeiLongjiang Province of China (JJ2019JQ0013), University Nursing
   Program for Young Scholars with Creative Talents in Heilongjiang
   Province(UNPYSCT-2016034), Outstanding Youth Talent Foundation of Harbin
   of China (2017RAYXJ013), and the Research Fund for the Doctoral Program
   of Higher Education of China (20132303120003) and the Science Funds for
   the Young Innovative Talents of HUST (20152).
CR [Anonymous], 2018, P 2018 IEEE INT WORK, DOI DOI 10.1109/IWEM.2018.8536658
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   Hueber T, 2015, IEEE-ACM T AUDIO SPE, V23, P2246, DOI 10.1109/TASLP.2015.2464702
   Liao Miao, 2017, Journal of Zhejiang University. Engineering Science, V51, P722, DOI 10.3785/j.issn.1008-973X.2017.04.012
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Rodriguez M, 2017, IEEE T CYBERNETICS, V47, P1769, DOI 10.1109/TCYB.2016.2558447
   Sandeep P, 2016, IEEE T IMAGE PROCESS, V25, P4233, DOI 10.1109/TIP.2016.2588319
   Song J, 2017, IEEE J BIOMED HEALTH, V21, P451, DOI 10.1109/JBHI.2015.2504422
   Song YY, 2017, IEEE T MED IMAGING, V36, P288, DOI 10.1109/TMI.2016.2606380
   Tareef A, 2017, NEUROCOMPUTING, V248, P28, DOI 10.1016/j.neucom.2017.01.093
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Yang C., 2017, P IEEE C COMP VIS PA, P6721, DOI DOI 10.1109/CVPR.2017.434
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu S, 2019, IEEE T IMAGE PROCESS, V28, P1513, DOI 10.1109/TIP.2018.2878331
   Zhang PY, 2017, I S BIOMED IMAGING, P718, DOI 10.1109/ISBI.2017.7950620
   Zhang WJ, 2017, PATTERN RECOGN, V71, P349, DOI 10.1016/j.patcog.2017.06.021
   Zhao Jing, 2018, [The Journal of China Universities of Posts and Telecommunications, 中国邮电高校学报], V25, P83
NR 27
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1623
EP 1635
DI 10.1007/s00371-020-01926-1
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000556624300001
DA 2024-07-18
ER

PT J
AU Getto, R
   Kuijper, A
   Fellner, DW
AF Getto, Roman
   Kuijper, Arjan
   Fellner, Dieter W.
TI Automatic procedural model generation for 3D object variation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D procedural model; 3D generative model; 3D object variation; 3D object
   parameterization
AB 3D objects are used for numerous applications. In many cases not only single objects but also variations of objects are needed. Procedural models can be represented in many different forms, but generally excel in content generation. Therefore this representation is well suited for variation generation of 3D objects. However, the creation of a procedural model can be time-consuming on its own. We propose an automatic generation of a procedural model from a single exemplary 3D object. The procedural model consists of a sequence of parameterizable procedures and represents the object construction process. Changing the parameters of the procedures changes the surface of the 3D object. By linking the surface of the procedural model to the original object surface, we can transfer the changes and enable the possibility of generating variations of the original 3D object. The user can adapt the derived procedural model to easily and intuitively generate variations of the original object. We allow the user to define variation parameters within the procedures to guide a process of generating random variations. We evaluate our approach by computing procedural models for various object types, and we generate variations of all objects using the automatically generated procedural model.
C1 [Getto, Roman; Kuijper, Arjan; Fellner, Dieter W.] Tech Univ Darmstadt, Fraunhoferstr 5, D-64283 Darmstadt, Germany.
   [Kuijper, Arjan; Fellner, Dieter W.] Fraunhofer IGD, Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Getto, R (corresponding author), Tech Univ Darmstadt, Fraunhoferstr 5, D-64283 Darmstadt, Germany.
EM roman.getto@gris.tu-darmstadt.de
OI Fellner, Dieter W./0000-0001-7756-0901
CR [Anonymous], 2011, P 4 EUR C 3D OBJ RET
   [Anonymous], P COMP GRAPH INT C
   [Anonymous], 2014, PROC ACM SIGGRAPH CO
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Averkiou Melinos, 2014, Computer Graphics Forum, V33, P125, DOI 10.1111/cgf.12310
   Bærentzen JA, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601226
   Benes B, 2011, COMPUT GRAPH FORUM, V30, P325, DOI 10.1111/j.1467-8659.2011.01886.x
   Bokeloh M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778841
   Bokeloh M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185574
   Chao I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778775
   Corsini M, 2012, IEEE T VIS COMPUT GR, V18, P914, DOI 10.1109/TVCG.2012.34
   Guo XK, 2014, GRAPH MODELS, V76, P376, DOI 10.1016/j.gmod.2014.03.019
   Havemann S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P350, DOI 10.1109/SMI.2004.1314525
   Jain A, 2012, COMPUT GRAPH FORUM, V31, P631, DOI 10.1111/j.1467-8659.2012.03042.x
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Levi Z, 2015, IEEE T VIS COMPUT GR, V21, P264, DOI 10.1109/TVCG.2014.2359463
   Marini S, 2007, IEEE COMPUT GRAPH, V27, P28, DOI 10.1109/MCG.2007.89
   Marvie JE, 2005, VISUAL COMPUT, V21, P329, DOI 10.1007/s00371-005-0289-z
   Mendez E, 2008, IEEE COMPUT GRAPH, V28, P48, DOI 10.1109/MCG.2008.53
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Ovsjanikov M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964928
   Raab R., 2004, International Journal of Shape Modeling, V10, P1, DOI 10.1142/S0218654304000584
   Ramamoorthi R, 1999, COMP GRAPH, P195, DOI 10.1145/311535.311557
   Schinko C, 2010, LECT NOTES COMPUT SC, V6436, P153
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Thiery JM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508384
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Wang Y, 2011, COMPUT GRAPH FORUM, V30, P287, DOI 10.1111/j.1467-8659.2011.01885.x
   Wu HC, 2012, INT SYMP MICROARCH, P107, DOI 10.1109/MICRO.2012.19
   Xu K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866206
   Yumer ME, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766908
   Yumer ME, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366185
   Zhou Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818074
NR 41
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 53
EP 70
DI 10.1007/s00371-018-1589-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800006
DA 2024-07-18
ER

PT J
AU Wang, L
   Han, SH
AF Wang, Lin
   Han, Soonhung
TI Visual simulation of a capsizing ship in stormy weather condition
SO VISUAL COMPUTER
LA English
DT Article
DE Ship capsizing; Stormy condition; Choppy wave; Particle simulation;
   Visual effects
ID MOTIONS; WATER
AB Ship capsizes in the extreme weather condition are difficult to predict, even though they cause a great number of casualties. Numerical methods have been developed to analyze the capsize phenomena and to predict the possible capsizes. The visual simulation can augment the physical evaluation of ship motion and validation of the experimental results. However, most of the existing visual simulations merely focus on the normal ship motion in general sea state and have less fascinating visual effect (VFX) compared with those in movie industry. In this paper, we propose a visual simulation method to generate stormy wave surface, to simulate wave properties such as splash and wake using particle simulation, to render the wave and its relevant stormy features and finally to compose the visual effects together. Wave simulation is based on the modified Tessendorf grid method of ocean wave, and SPH particles are adopted to simulate the wave properties. Lastly, the integrated rendering is to produce the scene of ship motion in storm condition.
C1 [Wang, Lin; Han, Soonhung] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Wang, L (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM wanglin@kaist.ac.kr; shhan@kaist.ac.kr
RI wang, Lin/GQO-7901-2022; han, soonhung/AAA-5745-2021
OI wang, Lin/0000-0002-7485-4493; han, soonhung/0000-0001-5676-8121
FU iCAD laboratory in mechanical engineering of KAIST
FX The research presented in this paper is conducted as a part of Master
   Thesis, which is funded by iCAD laboratory in mechanical engineering of
   KAIST.
CR Balakhontceva M, 2016, PROCEDIA COMPUT SCI, V80, P2455, DOI 10.1016/j.procs.2016.05.547
   Belenky VL., 2007, Stability and Safety of Ships-Risk of Capsizing
   Claes P, 2009, THESIS
   Dankowski D.H, 2013, FAST EXPLICIT METHOD, DOI [10.15480/882.1125, DOI 10.15480/882.1125]
   Darles E, 2011, COMPUT GRAPH FORUM, V30, P43, DOI 10.1111/j.1467-8659.2010.01828.x
   de Kat J., 1989, T SOC NAVAL ARCH MAR, V97, P139
   He J, 2010, VISUAL COMPUT, V26, P243, DOI 10.1007/s00371-010-0426-1
   Horvath P., 2007, 11 CENTR EUR SEM COM
   Kang Y, 2015, J DEF MODEL SIMUL-AP, V12, P507, DOI 10.1177/1548512914548601
   Kobyliski Lech, 2008, ARCH CIV MECH ENG, V8, P37, DOI [10.1016/S1644-9665(12)60265-9, DOI 10.1016/S1644-9665(12)60265-9]
   Layton AT, 2002, VISUAL COMPUT, V18, P41, DOI 10.1007/s003710100131
   Miandji E, 2009, VISUAL COMPUT, V25, P697, DOI 10.1007/s00371-009-0352-2
   Papanikolaou A, 2007, 10 INT S PRACT DES S
   Paroka D, 2006, J SHIP RES, V50, P187
   Ruponen P., 2007, PROGR FLOODING DAMAG
   Sandaruwan D., 2010, INT J ADV ICT EMERG, V3, P34
   Shyh-Kuang Ueng, 2008, Virtual Reality, V12, P65, DOI 10.1007/s10055-008-0088-8
   Stewart W. K., 1992, Visual Computer, V8, P361, DOI 10.1007/BF01897122
   Tessendorf Jerry, 2001, SIGGRAPH, V1, P5
   Ueng SK, 2013, J MAR SCI TECH-TAIW, V21, P674, DOI 10.6119/JMST-012-1121-1
   Varela JM, 2015, PROCEDIA COMPUT SCI, V51, P2397, DOI 10.1016/j.procs.2015.05.416
   Varela J.M, 2014, VIRTUAL REALITY MODE
   Wang CB, 2013, VISUAL COMPUT, V29, P937, DOI 10.1007/s00371-013-0849-6
   Wu EH, 2007, VISUAL COMPUT, V23, P299, DOI 10.1007/s00371-007-0106-y
   Yang S, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION PROBLEM-SOLVING (ICCP), P150, DOI 10.1109/ICCPS.2014.7062239
   Yeo DJ, 2012, SIMUL-T SOC MOD SIM, V88, P1407, DOI 10.1177/0037549712452128
   Yuksel Cem., 2007, ACM SIGGRAPH 2007 Papers. SIGGRAPH'07, DOI DOI 10.1145/1275808.1276501
   Zhang X., 2004, VRCAI
NR 28
TC 1
Z9 1
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1855
EP 1868
DI 10.1007/s00371-018-1579-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300013
DA 2024-07-18
ER

PT J
AU Michailidis, GT
   Pajarola, R
AF Michailidis, Georgios-Tsampikos
   Pajarola, Renato
TI ASPIRE: Automatic scanner position reconstruction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 36th Computer Graphics International Conference (CGI)
CY JUN 17-20, 2019
CL Calgary, CANADA
SP Biometric Technologies Lab, Univ Calgary, VPR Off, Fac Sci, Comp Sci Dept, Alberta Ingenu, CGS
DE LiDAR reconstruction; Interiors reconstruction; Point cloud processing;
   Point pattern analysis
ID BUILDING MODELS
AB The recent advances in 3D laser range scanning have led to significant improvements in capturing and modeling 3D environments, allowing the creation of highly expressive and semantically rich 3D models from indoor environments, generally known as building information models. Despite the capabilities of state-of-the-art methods to generate faithful architectural 3D building models, the majority of them rely explicitly on the prior knowledge of scanner positions in order to reconstruct them successfully. However, in real-world applications, this metadata information gets typically lost after the point cloud registration, which means that none of these methods could work in practice and the creation of their building models would be impossible. Therefore, we present a novel pipeline that allows to automatically and accurately reconstruct the original scanner positions under very challenging conditions, without requiring any prior knowledge about the environment or the dataset. Being independent from laser range scanner manufacturers, it can be applied to almost every real-world LiDAR application. Our method exploits only information derived from the raw point data and is applicable to all scientific and industrial applications, where the original scan positions typically get lost after registration by the proprietary software provided by the scanner manufacturers. We demonstrate the validity of our approach by evaluating it on several real-world and synthetic indoor environments.
C1 [Michailidis, Georgios-Tsampikos] Univ Zurich, CH-8050 Zurich, Switzerland.
   [Pajarola, Renato] Univ Zurich, Comp Sci, CH-8050 Zurich, Switzerland.
C3 University of Zurich; University of Zurich
RP Michailidis, GT (corresponding author), Univ Zurich, CH-8050 Zurich, Switzerland.
EM gtmichail@ifi.uzh.ch; pajarola@ifi.uzh.ch
OI Pajarola, Renato/0000-0002-6724-526X
FU EU FP7 People Programme (Marie Curie Actions) under REA Grant [290227]
FX We acknowledge Dr. Claudio Mura, Prof. Yasutaka Furukawa, Prof. Satoshi
   Ikehata, Prof. Reinhard Klein and Dr. Sebastian Ochmann for the
   acquisition of the 3D point clouds. The 3D scanning of the datasets
   Cottage, Penthouse, G82, Synth1 and Synth2 has partially been supported
   by the EU FP7 People Programme (Marie Curie Actions) under REA Grant
   Agreement no. 290227.
CR Adan A., 2011, 2011 INT C 3D IM MOD, P275, DOI [DOI 10.1109/3DIMPVT.2011.42, 10.1109/3DIMPVT.2011.42]
   Adán A, 2015, SENSORS-BASEL, V15, P11551, DOI 10.3390/s150511551
   Ahn SJ, 2001, PATTERN RECOGN, V34, P2283, DOI 10.1016/S0031-3203(00)00152-7
   Ambrus R, 2017, IEEE ROBOT AUTOM LET, V2, P749, DOI 10.1109/LRA.2017.2651939
   An LH, 2008, COMPUT STAT DATA AN, V52, P2669, DOI 10.1016/j.csda.2007.09.024
   [Anonymous], 2017, P ISPRS ANN PHOT REM, DOI DOI 10.5194/ISPRS-ANNALS-IV-2-W4-355-2017
   [Anonymous], 2014, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci
   Armeni I., 2016, PROCEEDINGS IEEE INT
   Boehler W., 2003, TECHNICAL REPORT
   Calafiore G, 2002, IEEE T SYST MAN CY A, V32, P269, DOI 10.1109/TSMCA.2002.1021114
   Chen JD, 2017, COMPUTING IN CIVIL ENGINEERING 2017: INFORMATION MODELLING AND DATA ANALYTICS, P34
   Diggle PJ, 2014, Statistical Analysis of Spatial Point Patterns, V3rd
   Elseberg J., 2012, Journal of Software Engineering for Robotics (JOSER), V3, P2
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   GANDER W, 1994, BIT, V34, P558, DOI 10.1007/BF01934268
   Güngör E, 2017, EXPERT SYST APPL, V69, P10, DOI 10.1016/j.eswa.2016.10.022
   Huber P., 2009, Robust statistics, DOI DOI 10.1002/9780470434697
   Ikehata S, 2015, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2015.156
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Lu CT, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P597
   Michailidis G. -T., 2015, P COMP GRAPH INT
   Michailidis GT, 2017, VISUAL COMPUT, V33, P1347, DOI 10.1007/s00371-016-1230-3
   Mura C, 2016, COMPUT GRAPH FORUM, V35, P179, DOI 10.1111/cgf.13015
   Mura C, 2014, COMPUT GRAPH-UK, V44, P20, DOI 10.1016/j.cag.2014.07.005
   Nurunnabi A, 2017, INT ARCH PHOTOGRAMM, V42-1, P63, DOI 10.5194/isprs-archives-XLII-1-W1-63-2017
   Ochmann S, 2016, COMPUT GRAPH-UK, V54, P94, DOI 10.1016/j.cag.2015.07.008
   Oesau S, 2014, ISPRS J PHOTOGRAMM, V90, P68, DOI 10.1016/j.isprsjprs.2014.02.004
   Pearson RK, 2002, IEEE T CONTR SYST T, V10, P55, DOI 10.1109/87.974338
   Pearson RK, 2001, J PROCESS CONTR, V11, P179, DOI 10.1016/S0959-1524(00)00046-9
   ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267
   Sanchez V, 2012, IEEE IMAGE PROC, P1777, DOI 10.1109/ICIP.2012.6467225
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Stambler A, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P118, DOI 10.1109/3DV.2014.65
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Woodford OJ, 2014, INT J COMPUT VISION, V106, P332, DOI 10.1007/s11263-013-0623-2
   Xiong XH, 2013, AUTOMAT CONSTR, V31, P325, DOI 10.1016/j.autcon.2012.10.006
   Yu J., 2010, EURASIP J ADV SIGNAL
NR 38
TC 0
Z9 0
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1209
EP 1221
DI 10.1007/s00371-019-01711-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500002
OA Green Published
DA 2024-07-18
ER

PT J
AU Kán, P
   Kafumann, H
AF Kan, Peter
   Kafumann, Hannes
TI DeepLight: light source estimation for augmented reality using deep
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Light source estimation; Augmented reality; Photometric registration;
   Deep learning
ID ILLUMINATION
AB This paper presents a novel method for illumination estimation from RGB-D images. The main focus of the proposed method is to enhance visual coherence in augmented reality applications by providing accurate and temporally coherent estimates of real illumination. For this purpose, we designed and trained a deep neural network which calculates a dominant light direction from a single RGB-D image. Additionally, we propose a novel method for real-time outlier detection to achieve temporally coherent estimates. Our method for light source estimation in augmented reality was evaluated on the set of real scenes. Our results demonstrate that the neural network can successfully estimate light sources even in scenes which were not seen by the network during training. Moreover, we compared our results with illumination estimates calculated by the state-of-the-art method for illumination estimation. Finally, we demonstrate the applicability of our method on numerous augmented reality scenes.
C1 [Kan, Peter; Kafumann, Hannes] TU Wien, Inst Visual Comp & Human Ctr Technol, Vienna, Austria.
C3 Technische Universitat Wien
RP Kán, P (corresponding author), TU Wien, Inst Visual Comp & Human Ctr Technol, Vienna, Austria.
EM peterkan@peterkan.com
OI Kan, Peter/0000-0001-7437-9955
FU TU Wien (TUW); Austrian research project WWTF [ICT15-015]
FX Open access funding provided by TU Wien (TUW). This research was funded
   by the Austrian research project WWTF ICT15-015. We thank Marc-Ande
   Gardner for providing us with the results of his algorithm through a Web
   service and for the kind explanations of details of the algorithm. We
   are also thankful to Alexander Pacha for advices about deep learning. We
   would like to thank NVIDIA Corporation for the donation of a Titan Xp
   graphics card and the Center for Geometry and Computational Design for
   access to a multi-GPU PC for training our neural networks. We also thank
   Min Kyung Lee, Iana Podkosova and Khrystyna Vasylevska for their support
   with controlled light experiments.
CR Agusanto K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P208, DOI 10.1109/ISMAR.2003.1240704
   [Anonymous], 2005, EUROPEAN C VISUAL ME
   Boom BJ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.105
   Boom BJ, 2015, COMPUT ANIMAT VIRTUA, V28, P5149
   Dante A., 2003, INT C IM PROC, V1
   Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864
   Dong Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661283
   Marques BAD, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P303, DOI 10.5220/0006724303030311
   Elizondo DA, 2017, NEURAL COMPUT APPL, V28, P899, DOI 10.1007/s00521-016-2281-0
   Gardner M.-A., 2017, ACM TOG, V9
   Gruber L, 2015, P IEEE VIRT REAL ANN, P127, DOI 10.1109/VR.2015.7223334
   Gruber L, 2012, INT SYM MIX AUGMENT, P119, DOI 10.1109/ISMAR.2012.6402548
   He K., 2015, CORR ARXIV 1512 0338
   Hold-Geoffroy Y., 2016, ARXIV161106403 CORR
   Jiddi S, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P244, DOI [10.1109/ISMAR-Adjunct.2016.0085, 10.1109/ISMAR-Adjunct.2016.76]
   Kan P., 2014, THESIS
   Kán P, 2015, LECT NOTES COMPUT SC, V9474, P574, DOI 10.1007/978-3-319-27857-5_52
   Karsch K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602146
   Kasper M., 2017, ARXIV170104101 CORR
   Knecht M., 2010, 2010 IEEE International Symposium on Mixed and Augmented Reality, P99, DOI DOI 10.1109/ISMAR.2010.5643556
   Knorr SB, 2014, INT SYM MIX AUGMENT, P113, DOI 10.1109/ISMAR.2014.6948416
   Lopez-Moreno J, 2013, COMPUT GRAPH FORUM, V32, P170, DOI 10.1111/cgf.12195
   Mandl D, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P82, DOI 10.1109/ISMAR.2017.25
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271
   Richter-Trummer T, 2016, INT SYM MIX AUGMENT, P27, DOI 10.1109/ISMAR.2016.18
   Rohmer K, 2014, INT SYM MIX AUGMENT, P29, DOI 10.1109/ISMAR.2014.6948406
   Sato I, 2003, IEEE T PATTERN ANAL, V25, P290, DOI 10.1109/TPAMI.2003.1182093
   Supan P., 2006, INT J VIRTUAL REALIT, V5, P1, DOI DOI 10.20870/IJVR.2006.5.3.2692
   Wagner D., 2007, TECHNICAL REPORT
   Weber M, 2001, P BRIT MACH VIS C, P471
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186
   Zhou WD, 2002, P ANN INT IEEE EMBS, P206, DOI 10.1109/IEMBS.2002.1134458
NR 34
TC 22
Z9 25
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 873
EP 883
DI 10.1007/s00371-019-01666-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200009
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Krayer, B
   Müller, S
AF Krayer, Bastian
   Mueller, Stefan
TI Generating signed distance fields on the GPU with ray maps
SO VISUAL COMPUTER
LA English
DT Article
DE Signed distance fields; Geometric algorithms; Object representation;
   GPGPU
AB Signed distance fields represent objects as distances to the closest surface points with a sign differentiating inside and outside. We present an algorithm to compute a signed distance field from triangle meshes. All data are kept on the GPU, making it ideal for any pure graphics-based context. We split the algorithm into a fast parallel distance transform and a new method of computing the sign. To determine the sign, we compute the winding number for any point using a ray map, a ray-based data structure that preserves geometric meaning while reducing the amount of work to be done for ray tests. Based on that structure, we devise a simple parallel algorithm to sample an exponentially growing number of rays to cope with meshes having deficiencies such as holes or self-intersections. We demonstrate how our method is both fast and able to handle imperfect meshes.
C1 [Krayer, Bastian; Mueller, Stefan] Univ Koblenz Landau, Comp Graph Res Grp, Koblenz, Germany.
C3 University of Koblenz & Landau
RP Krayer, B (corresponding author), Univ Koblenz Landau, Comp Graph Res Grp, Koblenz, Germany.
EM bastiankrayer@uni-koblenz.de; stefanm@uni-koblenz.de
CR Akenine-Moller Tomas., 2005, ACM SIGGRAPH 2005 Courses, P8, DOI DOI 10.1145/1198555.1198747
   Barill G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201337
   Bastos T, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P171, DOI 10.1109/SMI.2008.4547967
   Blelloch GE, 1990, TECH REP
   Bærentzen JA, 2005, IEEE T VIS COMPUT GR, V11, P243, DOI 10.1109/TVCG.2005.49
   Cuntz Nicolas., 2007, EG Short Papers, P1, DOI DOI 10.2312/EGS.20071042
   Fabbri R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1322432.1322434
   Frisken SF, 2000, COMP GRAPH, P249, DOI 10.1145/344779.344899
   Fuhrmann A., 2003, P GRAPHICON 2003, P58
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Hasselgren Jon., 2005, GPU GEMS 2, P677
   Hoff K., 2002, TR02004 UNCCH
   Huttenlocher, 2012, THEORY COMPUT, V8, P415, DOI [10.4086/toc.2012.v008a019, DOI 10.4086/TOC.2012.V008A019]
   HWANG YK, 1992, IEEE T ROBOTIC AUTOM, V8, P23, DOI 10.1109/70.127236
   Jacobson A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461916
   Jian H, 2001, IEEE VISUAL, P247
   Koschier D, 2016, EUR ACM SIGGRAPH S C, P189
   Meijster A, 2000, COMP IMAG VIS, V18, P331
   Rossignac J, 2013, COMPUT AIDED DESIGN, V45, P288, DOI 10.1016/j.cad.2012.10.012
   Schneider J, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P435
   Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591
   Sud A, 2004, COMPUT GRAPH FORUM, V23, P557, DOI 10.1111/j.1467-8659.2004.00787.x
   Sud A., 2006, S INTERACTIVE 3D GRA, P117, DOI 10.1145/1111411.1111432
   Wright Daniel, 2015, ACM SIGGRAPH
   Xu Hongyi, 2014, Proceedings of Graphics Interface 2014, P35
   Yatziv L, 2006, J COMPUT PHYS, V212, P393, DOI 10.1016/j.jcp.2005.08.005
   Zhou Qingnan, 2016, arXiv preprint arXiv:1605.04797
NR 27
TC 6
Z9 6
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 961
EP 971
DI 10.1007/s00371-019-01683-w
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200016
OA Bronze
DA 2024-07-18
ER

PT J
AU Yang, CL
   Pu, JX
   Dong, YS
   Xie, GS
   Si, YN
   Liu, ZH
AF Yang, Chunlei
   Pu, Jiexin
   Dong, Yongsheng
   Xie, Guo-sen
   Si, Yanna
   Liu, Zhonghua
TI Scene classification-oriented saliency detection via the modularized
   prescription
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Scene classification; Modularized prescription;
   Scene complexity expression; Support Vector Machine (SVM)
ID REGION DETECTION; LOW-LEVEL
AB Saliency detection technology has been greatly developed and applied in recent years. However, the performance of current methods is not satisfactory in complex scenes. One of the reasons is that the performance improvement is often carried out through utilizing complicated mathematical models and involving multiple features rather than classifying the scene complexity and respectively detecting saliency. To break this unified detection schema for generating better results, we propose a method of scene classification-oriented saliency detection via the modularized prescription in this paper. Different scenes are described by a scene complexity expression model, and they are analyzed and discriminately detected by different pipelines. This process seems like that doctors can tailor the treatment prescriptions when they meet different symptoms. Moreover, two SVM-based classifiers are trained for scene classification and sky region identification, and the proposed sky region discrimination and erase model can be used to efficiently decrease the saliency interference by the high luminance of the background sky regions. Experimental results demonstrate the effectiveness and superiority of the proposed method in both higher precision and better smoothness, especially for detecting in structure complex scenes.
C1 [Yang, Chunlei; Pu, Jiexin; Dong, Yongsheng; Xie, Guo-sen; Si, Yanna; Liu, Zhonghua] Henan Univ Sci & Technol, 263 Kaiyuan Rd, Luoyang, Peoples R China.
C3 Henan University of Science & Technology
RP Pu, JX (corresponding author), Henan Univ Sci & Technol, 263 Kaiyuan Rd, Luoyang, Peoples R China.
EM pjx2014stu@163.com
RI Chunlei, Yang/KJL-7321-2024; Xie, Guo-Sen/AAL-6674-2020
OI Xie, Guo-Sen/0000-0002-5487-9845
FU International S & T Cooperation Program of Henan [162102410021,
   152102410036]; National Natural Science Foundation of China [U1604153,
   U1504610]
FX This work was supported in part by the International S & T Cooperation
   Program of Henan (No. 162102410021, 152102410036) and the National
   Natural Science Foundation of China (No. U1604153, U1504610).
CR Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   ANDO T, 1995, LINEAR ALGEBRA APPL, V224, P57
   [Anonymous], TPAMI
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, TECH REP
   [Anonymous], 2007, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2007.383017
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Chen ZH, 2015, IET IMAGE PROCESS, V9, P758, DOI 10.1049/iet-ipr.2014.0987
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huang Y, 2014, COMM COM INF SC, V483, P283
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Judd T, 2012, MIT TECHNICAL REPORT
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Li HY, 2017, NEUROCOMPUTING, V226, P212, DOI 10.1016/j.neucom.2016.11.056
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Liu J, 2015, NEUROCOMPUTING, V147, P435, DOI 10.1016/j.neucom.2014.06.041
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qiu YH, 2015, NEUROCOMPUTING, V168, P538, DOI 10.1016/j.neucom.2015.05.073
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Song ML, 2014, INFORM SCIENCES, V281, P573, DOI 10.1016/j.ins.2013.09.036
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu XM, 2014, I C CONT AUTOMAT ROB, P1207, DOI 10.1109/ICARCV.2014.7064487
   Xu M, 2016, PATTERN RECOGN, V60, P348, DOI 10.1016/j.patcog.2016.05.023
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yang CL, 2017, VISUAL COMPUT, V33, P1415, DOI 10.1007/s00371-016-1288-y
   Yang CL, 2017, IEEE SIGNAL PROC LET, V24, P1458, DOI 10.1109/LSP.2017.2737650
   Zhang YD, 2014, INFORM SCIENCES, V281, P586, DOI 10.1016/j.ins.2013.12.043
   Zhu S., 2011, 2011 IEEE Workshop on Microelectronics and Electron Devices, P1
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 41
TC 4
Z9 5
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 473
EP 488
DI 10.1007/s00371-018-1475-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800002
DA 2024-07-18
ER

PT J
AU Yang, H
   Min, K
AF Yang, Heekyung
   Min, Kyungha
TI Importance-based approach for rough drawings
SO VISUAL COMPUTER
LA English
DT Article
DE Rough drawing; Pencil; Charcoal; Convolution; Saliency; DoG
AB We present a framework for producing rough drawings from photographs. Depicting a scene using a series of lines is one of the most effective methods of visual communication. Our framework for rough drawing is comprised of three steps: extracting lines from images, estimating line importance, and producing strokes that express various styles. To extract lines, we employ the widely used difference-of-Gaussian filter approach to devise a fault-correcting line shift scheme. Line importance is estimated by combining gradient and saliency. To obtain an efficient saliency estimation, we propose a stochastic content-based method. Various styles of rough drawings are produced by convoluting adaptive stroke texture segments, which are prepared by sampling real stroke texture images. We test our framework on various images and compare our results with real artwork and other schemes.
C1 [Yang, Heekyung; Min, Kyungha] Sangmyung Univ, Dept Comp Sci, Seoul, South Korea.
C3 Sangmyung University
RP Min, K (corresponding author), Sangmyung Univ, Dept Comp Sci, Seoul, South Korea.
EM minkh@smu.ac.kr
FU National Research Foundation (NRF) of Korea [NRF-2017R1D1A1B03034137,
   NRF-2015R1D1A1A01061415]
FX This research was supported by the grant NRF-2017R1D1A1B03034137 and
   NRF-2015R1D1A1A01061415 from National Research Foundation (NRF) of
   Korea.
CR AlMeraj Z, 2009, COMPUT GRAPH-UK, V33, P496, DOI 10.1016/j.cag.2009.04.004
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   DOOLEY D, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P307, DOI 10.1109/VISUAL.1990.146395
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004
   Hata M, 2012, VISUAL COMPUT, V28, P657, DOI 10.1007/s00371-012-0689-9
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kim Y, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462001
   Lake C., 2000, Proceedings of the first international symposium on Non-photorealistic animation and rendering-NPAR'00, P13, DOI 10.1145/340916.3409185[27]M.S.
   Lee H., 2006, NPAR 2006, P37, DOI [10. 1145/1124728. 1124735, DOI 10.1145/1124728.1124735]
   Li CZ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073675
   Litwinowicz P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P407, DOI 10.1145/258734.258893
   Lu Cewu., 2012, Proc. NPAR, P65
   McCool M., 1992, Proceedings. Graphics Interface '92, P94
   Papari G, 2011, IMAGE VISION COMPUT, V29, P79, DOI 10.1016/j.imavis.2010.08.009
   Salisbury M. P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P101, DOI 10.1145/192161.192185
   SON M, 2007, PROC PAC, V2007, P333, DOI DOI 10.1109/PG.2007.63
   Spicker M., 2015, SIGGRAPH AS 2015 TEC
   Suarez J, 2017, VISUAL COMPUT, V33, P1319, DOI 10.1007/s00371-016-1222-3
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Winnemoller H., 2011, P ACM SIGGRAPH EUR S, P147
   Yang H, 2017, KSII T INTERNET INF, V11, P3143, DOI 10.3837/tiis.2017.06.019
   Yang H, 2012, COMPUT GRAPH FORUM, V31, P1471, DOI 10.1111/j.1467-8659.2012.03143.x
   Yang H, 2011, KSII T INTERNET INF, V5, P1311, DOI 10.3837/tiis.2011.07.006
   Ye C, 2012, P ICEICE, V2012, P233
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
NR 30
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 609
EP 622
DI 10.1007/s00371-018-1490-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800011
DA 2024-07-18
ER

PT J
AU Choi, MG
   Kwon, T
AF Choi, Myung Geol
   Kwon, Taesoo
TI Motion rank: applying page rank to motion data search
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture data; Retrieval; Ranking algorithm; Visualization; Stick
   figure
ID RETRIEVAL
AB As the uses of motion capture data increase, the amount of available motion data on the web also grows. In this paper, we investigate a new method to retrieve and visualize motion data in a similar manner to Google Image Search. The main idea is to represent raw motion data into a series of short animated clip arts, called motion clip arts. Short animated clip arts can be quickly browsed and understood by people even though many of them appear at the same time on the screen. We first temporally segment the raw motion data files into short yet semantically meaningful motion segments. Then, we convert the motion segments into motion clip arts in a way that emphasizes the main motion and minimizes the data size for the efficient transmitting and processing on the web. When a user input query is received, our system first retrieves all the relevant motion clip arts by considering the input keywords and similarity between motions. Then, the retrieved results are re-ranked by our ranking algorithm developed based on the Google ImageRank algorithm. To prove the usability of our method, we build a web-based motion search system with the entire data collections of the CMU motion database. The experimental results show significant improvement, in terms of relevancy, in comparison with the simple keyword-based search interface.
C1 [Choi, Myung Geol] Catholic Univ Korea, Seoul, South Korea.
   [Kwon, Taesoo] Hanyang Univ, Seoul, South Korea.
C3 Catholic University of Korea; Hanyang University
RP Kwon, T (corresponding author), Hanyang Univ, Seoul, South Korea.
EM mgchoi@catholic.ac.kr; taesoobear@gmail.com
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [2016R1D1A1B03930472]; NRF of
   Korea [NRF-MIAXA003-2010-0029744]; Catholic University of Korea,
   Research Fund, 2017
FX We thank the anonymous reviewers for their comments and suggestions.
   This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Education (2016R1D1A1B03930472) and partially supported by the NRF of
   Korea (NRF-MIAXA003-2010-0029744). This work was also supported by the
   Catholic University of Korea, Research Fund, 2017.
CR [Anonymous], 2009, P 2009 ACM SIGGRAPH
   [Anonymous], 2009, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, I3D'09, DOI DOI 10.1145/1507149.1507181
   [Anonymous], 2005, MOT CAPT DAT HDM05
   [Anonymous], 2015, Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, MIG'15
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2010, SCA'10: proceedings of the 2010 ACM SIGGRAPH/Eurographics symposium on computer animation, DOI [10.2312/SCA/SCA10/001-010, DOI 10.2312/SCA/SCA10/001-010]
   [Anonymous], 2013, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'13
   [Anonymous], 2014, P 2014 ACM SIGGRAPHE
   [Anonymous], 2013, ACM T GRAPHIC, DOI DOI 10.1145/2461912.2461968
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Beaudoin P., 2008, P 2008 ACM SIGGRAPHE, P117
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Choi MG, 2012, COMPUT GRAPH FORUM, V31, P2057, DOI 10.1111/j.1467-8659.2012.03198.x
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Hahne B, BVH CONVERSIONS CARN
   Hsu WinstonH., 2007, ACM MM
   Huang SS, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461954
   Jang SJ, 2016, IEEE T VIS COMPUT GR, V22, P21, DOI 10.1109/TVCG.2015.2468292
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kwon TS, 2007, COMPUT ANIMAT VIRT W, V18, P463, DOI 10.1002/cav.185
   Lan RY, 2015, VISUAL COMPUT, V31, P35, DOI 10.1007/s00371-013-0902-5
   Laraba S, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1782
   Li M, 2016, MULTIMED TOOLS APPL, V75, P9205, DOI 10.1007/s11042-016-3480-5
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Myung Geol Choi, 2013, Computer Graphics Forum, V32, P1, DOI 10.1111/cgf.12206
   Numaguchi Naoki, 2011, P 2011 ACM SIGGRAPH, P157
   Page L, 1998, P 7 INT WORLD WID WE, DOI DOI 10.1007/978-3-319-08789-4_10
   Quint A, 2003, IEEE MULTIMEDIA, V10, P99, DOI 10.1109/MMUL.2003.1218261
   Ruxanda MM, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P549, DOI 10.1109/ICME.2008.4607493
   Sedmidubsky J, 2018, MULTIMED TOOLS APPL, V77, P12073, DOI 10.1007/s11042-017-4859-7
   Sedmidubsky J, 2017, LECT NOTES COMPUT SC, V10509, P59, DOI 10.1007/978-3-319-66917-5_5
   Wang X, 2016, MULTIMED TOOLS APPL, V75, P11723, DOI 10.1007/s11042-015-2705-3
   Won J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661271
   Wood Larry., 1990, Graphics Interchange Format Programming Reference, V1, P1
   Wu XM, 2011, IEEE COMPUT GRAPH, V31, P69, DOI 10.1109/MCG.2009.111
   Yang C. C., 2005, P SPL INT TRACKS 14, P906
   Yoo I, 2014, VISUAL COMPUT, V30, P213, DOI 10.1007/s00371-013-0797-1
   Yoshizaki W, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P637
NR 44
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 289
EP 300
DI 10.1007/s00371-018-1498-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600011
DA 2024-07-18
ER

PT J
AU Gai, S
AF Gai, Shan
TI Color image denoising via monogenic matrix-based sparse representation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Sparse representation; Monogenic signal; Monogenic wavelets; Color
   image; Dictionary learning; Image denoising
ID SIGNAL; RECOGNITION
AB Traditional sparse representation models usually treat color image channels independently, which ignore the relationship among the channels, resulting in inaccurate sparse coding coefficients. In this paper, we propose a novel vector-valued sparse representation model for color images using monogenic matrix. The proposed model treats color image as a monogenic matrix, which can transform independent color channels into a whole. In the dictionary learning stage, a corresponding effective dictionary learning method is designed by using monogenic matrix singular value decomposition, which conducts sparse basis selection in monogenic space. Then, a monogenic-based orthogonal matching pursuit algorithm is presented in the sparse coding stage. In order to demonstrate the effectiveness of the proposed sparse representation model, we apply the model to color image denoising. Extensive experimental results on color image denoising manifest that the proposed model outperforms the state-of-the-art schemes.
C1 [Gai, Shan] Nanchang Hangkong Univ, Sch Informat Engn, Nanchang 330063, Jiangxi, Peoples R China.
C3 Nanchang Hangkong University
RP Gai, S (corresponding author), Nanchang Hangkong Univ, Sch Informat Engn, Nanchang 330063, Jiangxi, Peoples R China.
EM gaishan886@163.com
FU National Natural Science Foundation of China [61563037]; Outstanding
   Youth Scheme of Jiangxi Province [20171BCB23057]; Natural Science
   Foundation of Jiangxi Province [20171BAB202018]; Department of Education
   Science and Technology of Jiangxi Province [GJJ150755]
FX This work is partially supported by National Natural Science Foundation
   of China; the Grant Number is 61563037; Outstanding Youth Scheme of
   Jiangxi Province; the Grant Number is 20171BCB23057; Natural Science
   Foundation of Jiangxi Province; the Grant Number is 20171BAB202018;
   Department of Education Science and Technology of Jiangxi Province; the
   Grant Number is GJJ150755.
CR Alessandrini M, 2013, IRBM, V34, P33, DOI 10.1016/j.irbm.2012.12.015
   Bülow T, 2001, IEEE T SIGNAL PROCES, V49, P2844, DOI 10.1109/78.960432
   Candès EJ, 2002, ANN STAT, V30, P784
   Clausel M, 2015, APPL COMPUT HARMON A, V39, P450, DOI 10.1016/j.acha.2014.10.003
   Demarcq G, 2011, J MATH IMAGING VIS, V40, P269, DOI 10.1007/s10851-011-0262-6
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Dong GG, 2014, IEEE SIGNAL PROC LET, V21, P952, DOI 10.1109/LSP.2014.2321565
   Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Fang LY, 2017, IEEE T INSTRUM MEAS, V66, P1646, DOI 10.1109/TIM.2017.2664480
   Fang LY, 2017, IEEE T MED IMAGING, V36, P407, DOI 10.1109/TMI.2016.2611503
   Fang LY, 2013, IEEE T MED IMAGING, V32, P2034, DOI 10.1109/TMI.2013.2271904
   Felsberg M, 2001, IEEE T SIGNAL PROCES, V49, P3136, DOI 10.1109/78.969520
   Gabor D., 1946, Journal of the Institution of Electrical Engineers-part III: radio and communication engineering, V93, P429, DOI [DOI 10.1049/JI-3-2.1946.0074, 10.1049/ji-3-2.1946.0074]
   Held S, 2010, IEEE T IMAGE PROCESS, V19, P653, DOI 10.1109/TIP.2009.2036713
   Hughes JM, 2013, IEEE T IMAGE PROCESS, V22, P4972, DOI 10.1109/TIP.2013.2280188
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kumar A, 2013, PATTERN RECOGN, V46, P73, DOI 10.1016/j.patcog.2012.06.020
   Le Bihan N, 2004, SIGNAL PROCESS, V84, P1177, DOI 10.1016/j.sigpro.2004.04.001
   Le Pennec E, 2005, IEEE T IMAGE PROCESS, V14, P423, DOI 10.1109/TIP.2005.843753
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Olhede SC, 2009, IEEE T SIGNAL PROCES, V57, P3426, DOI 10.1109/TSP.2009.2023397
   Olhede SC, 2014, IEEE T INFORM THEORY, V60, P6491, DOI 10.1109/TIT.2014.2342734
   Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   Soulard R, 2016, IEEE T SIGNAL PROCES, V64, P1535, DOI 10.1109/TSP.2015.2505664
   Soulard R, 2013, IEEE T IMAGE PROCESS, V22, P1070, DOI 10.1109/TIP.2012.2226902
   Unser M, 2009, IEEE T IMAGE PROCESS, V18, P2402, DOI 10.1109/TIP.2009.2027628
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P1315, DOI 10.1109/TIP.2015.2397314
   Yan RM, 2013, IEEE T IMAGE PROCESS, V22, P4689, DOI 10.1109/TIP.2013.2277813
   Yu RY, 2008, IEEE T SIGNAL PROCES, V56, P4263, DOI 10.1109/TSP.2008.925970
   Zeng ZY, 2015, J VIS COMMUN IMAGE R, V33, P85, DOI 10.1016/j.jvcir.2015.08.014
   Zhang L, 2010, IEEE IMAGE PROC, P2677, DOI 10.1109/ICIP.2010.5651885
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zheng P, 2017, PATTERN RECOGN, V63, P206, DOI 10.1016/j.patcog.2016.09.043
NR 40
TC 7
Z9 7
U1 2
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 109
EP 122
DI 10.1007/s00371-017-1456-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900010
DA 2024-07-18
ER

PT J
AU Röhlig, M
   Schmidt, C
   Prakasam, R
   Rosenthal, P
   Schumann, H
   Stachs, O
AF Roehlig, Martin
   Schmidt, Christoph
   Prakasam, Ruby Kala
   Rosenthal, Paul
   Schumann, Heidrun
   Stachs, Oliver
TI Visual analysis of retinal changes with optical coherence tomography
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Euro VA Conference
CY JUN 12-13, 2017
CL Barcelona, SPAIN
DE Visual analysis; Optical coherence tomography; OCT data; Ophthalmology;
   Retina
ID NERVE-FIBER LAYER; INFORMATION VISUALIZATION; IMAGE-ANALYSIS; OCT;
   THICKNESSES; RETINOPATHY; GLAUCOMA
AB Optical coherence tomography (OCT) enables noninvasive high-resolution 3D imaging of the human retina, and thus plays a fundamental role in detecting a wide range of ocular diseases. Despite the diagnostic value of OCT, managing and analyzing resulting data is challenging. We apply two visual analysis strategies for supporting retinal assessment in practice. First, we provide an interface for unifying and structuring data from different sources into a common basis. Fusing that basis with medical records and augmenting it with analytically derived information facilitates thorough investigations. Second, we present a tailored visual analysis tool for presenting, emphasizing, selecting, and comparing different aspects of the attributed data. This enables free exploration, reducing the data to relevant subsets, and focusing on details. By applying both strategies, we effectively enhance the management and the analysis of retinal OCT data for assisting medical diagnoses. Domain experts applied our solution successfully to study early retinal changes in patients suffering from type 1 diabetes mellitus.
C1 [Roehlig, Martin; Schmidt, Christoph; Rosenthal, Paul; Schumann, Heidrun] Univ Rostock, Inst Comp Sci, Rostock, Germany.
   [Prakasam, Ruby Kala; Stachs, Oliver] Univ Rostock, Dept Ophthalmol, Rostock, Germany.
C3 University of Rostock; University of Rostock
RP Röhlig, M (corresponding author), Univ Rostock, Inst Comp Sci, Rostock, Germany.
EM martin.roehlig@uni-rostock.de; christoph.schmidt2@uni-rostock.de;
   rubykala.prakasam@med.uni-rostock.de; research@paul-rosenthal.de;
   heidrun.schumann@uni-rostock.de; oliver.stachs@uni-rostock.de
RI Rosenthal, Paul/ABB-7095-2020
OI Rosenthal, Paul/0000-0001-9409-8931; Ruby Kala,
   Prakasam/0000-0002-5433-4546; Rohlig, Martin/0000-0002-3831-8482
FU German Research Foundation (Project VIES); German Federal Ministry of
   Education and Research (Project TOPOs)
FX This work has been supported by the German Research Foundation (Project
   VIES) and by the German Federal Ministry of Education and Research
   (Project TOPOs).
CR Aaker GD, 2011, ARCH OPHTHALMOL-CHIC, V129, P809, DOI 10.1001/archophthalmol.2011.123
   [Anonymous], 1991, Ophthalmology, V98, P786, DOI [10.1016/S0161-6420(13)38012-9, DOI 10.1016/S0161-6420(13)38012-9]
   [Anonymous], P EUROVA EUROVIS BAR
   Arias-Hernandez R., 2011, P 44 HAW INT C SYST, P1, DOI DOI 10.1109/HICSS.2011.339
   Baghaie A, 2015, QUANT IMAG MED SURG, V5, P603, DOI 10.3978/j.issn.2223-4292.2015.07.02
   Barla P., 2006, Proceedings of the 4th international symposium on Non-photorealistic animation and rendering, P127, DOI DOI 10.1145/1124728.1124749
   Berufsverband der Augenarzte Deutschlands e.V, 2017, Ophthalmologe, V114, P617, DOI 10.1007/s00347-017-0508-9
   Chen Q, 2017, SCI REP-UK, V7, DOI 10.1038/srep41100
   Chen YW, 2016, BMC OPHTHALMOL, V16, DOI 10.1186/s12886-016-0186-4
   De Clerck EEB, 2015, LANCET DIABETES ENDO, V3, P653, DOI 10.1016/S2213-8587(15)00136-9
   Drexler W, 2001, NAT MED, V7, P502, DOI 10.1038/86589
   Duncan MD, 1998, OPT EXPRESS, V2, P540, DOI 10.1364/OE.2.000540
   Ehnes A, 2014, TRANSL VIS SCI TECHN, V3, DOI 10.1167/tvst.3.1.1
   El-Fayoumi D, 2016, INVEST OPHTH VIS SCI, V57, P5355, DOI 10.1167/iovs.16-19988
   Elmqvist N, 2011, INFORM VISUAL, V10, P327, DOI 10.1177/1473871611413180
   Garrouste-Orgeas M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110146
   Garvin MK, 2009, IEEE T MED IMAGING, V28, P1436, DOI 10.1109/TMI.2009.2016958
   Glasser S, 2017, COMPUT GRAPH FORUM, V36, P57, DOI 10.1111/cgf.12994
   Gleicher M, 2011, INFORM VISUAL, V10, P289, DOI 10.1177/1473871611416549
   Glittenberg C, 2009, OPHTHAL SURG LAS IM, V40, P127, DOI 10.3928/15428877-20090301-08
   Hall KW, 2016, COMPUT GRAPH FORUM, V35, P717, DOI 10.1111/cgf.12936
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Kahl S., 2014, P FOR BILDV, P179, DOI [10.5445/KSP/1000043608, DOI 10.5445/KSP/1000043608]
   Keim DA, 2008, LECT NOTES COMPUT SC, V4404, P76, DOI 10.1007/978-3-540-71080-6_6
   Koleva-Georgieva D. N., 2012, EUR OPHTHALMIC REV, V6, P78, DOI [10.17925/EOR.2012.06.02.78, DOI 10.17925/EOR.2012.06.02.78]
   Kosara R, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P97, DOI 10.1109/INFVIS.2001.963286
   Mayer MA, 2010, BIOMED OPT EXPRESS, V1, P1358, DOI 10.1364/BOE.1.001358
   Moisseiev E., 2016, Retinal Physician, V13, P24
   Oat C, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P61
   Odell D, 2011, J OPHTHALMOL, V2011, DOI 10.1155/2011/692574
   Placet V, 2014, J MATER SCI, V49, P8317, DOI 10.1007/s10853-014-8540-5
   Probst Joachim, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7372, DOI 10.1117/12.831785
   Röhlig M, 2017, KLIN MONATSBL AUGENH, V234, P1463, DOI 10.1055/s-0043-121705
   Rosenthal P., 2016, P EUROVIS WORKSH REP, DOI [10.2312/EURORV3, DOI 10.2312/EURORV3]
   Rosenthal P., 2016, UNIFIED OCT EXPLORER
   Schindelin J, 2015, MOL REPROD DEV, V82, P518, DOI 10.1002/mrd.22489
   Schulze Jurgen P, 2013, Stud Health Technol Inform, V184, P387
   Sylwestrzak M, 2011, PROC SPIE, V8091, DOI 10.1117/12.889805
   van Dijk HW, 2009, INVEST OPHTH VIS SCI, V50, P3404, DOI 10.1167/iovs.08-3143
   Wojtkowski M, 2005, OPHTHALMOLOGY, V112, P1734, DOI 10.1016/j.ophtha.2005.05.023
   Yoshimura N, 2014, OCT ATLAS
NR 41
TC 14
Z9 14
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1209
EP 1224
DI 10.1007/s00371-018-1486-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000007
DA 2024-07-18
ER

PT J
AU Liu, SJ
   Xiao, JT
   Hu, L
   Liu, XR
AF Liu, Shengjun
   Xiao, Jintao
   Hu, Ling
   Liu, Xinru
TI Implicit surfaces from polygon soup with compactly supported radial
   basis functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Implicit surface; Quasi-interpolation; Reconstruction; Polygon soup;
   Compactly supported radial basis function
ID QUASI-INTERPOLATION; DISTANCE FIELDS; SCATTERED DATA; RECONSTRUCTION
AB This paper presents a method for generating implicit surfaces from polygon soups based on compactly supported radial basis functions (CSRBFs). The surface is represented as the zero level set of an implicit function which interpolates the polygonal data with their outward normal constraints. By specifying two parameters, the support size and the shape parameter, users can flexibly control the accuracy of the reconstructed surfaces. For determining coefficients of RBFs, our method uses a quasi-interpolation framework to avoid solving a large linear system, which allows processing large meshes efficiently and robustly. Moreover, a relationship between the shape parameter and the support radius is provided for the quasi-solution validity, and an error bound of the reconstructed surfaces approximating the original models is deduced through the rigorous theoretical analysis.
C1 [Liu, Xinru] Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha 410083, Peoples R China.
   [Liu, Shengjun] Cent South Univ, State Key Lab High Performance Mfg Complex, Changsha 410083, Peoples R China.
   [Xiao, Jintao; Hu, Ling] Cent South Univ, Sch Math & Stat, Changsha 410083, Peoples R China.
C3 Central South University; Central South University; Central South
   University
RP Liu, XR (corresponding author), Cent South Univ, Inst Engn Modeling & Sci Comp, Changsha 410083, Peoples R China.
EM shjliu.cg@csu.edu.cn; 152101007@csu.edu.cn; liuxinru@csu.edu.cn
RI Hu, Ling/AAA-5764-2020; Liu, Shengjun/AAI-8456-2020; Liu,
   Xinru/KEH-2341-2024
OI Liu, Xinru/0000-0001-5427-0178
FU Natural Science Foundation of China (NSFC) [61572527, 61602524]
FX This study was supported by the Natural Science Foundation of China
   (NSFC) Grants (Nos. 61572527 and 61602524).
CR Aanæs H, 2003, VISION, MODELING, AND VISUALIZATION 2003, P407
   [Anonymous], 2018, TECHNICAL REPORT
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   Bærentzen JA, 2005, IEEE T VIS COMPUT GR, V11, P243, DOI 10.1109/TVCG.2005.49
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Gois JP, 2013, VISUAL COMPUT, V29, P651, DOI 10.1007/s00371-013-0802-8
   Han XL, 2008, LECT NOTES COMPUT SC, V4975, P541
   Hu L, 2018, COMPUT GRAPH-UK, V70, P39, DOI 10.1016/j.cag.2017.07.028
   Jin XG, 2009, VISUAL COMPUT, V25, P279, DOI 10.1007/s00371-008-0267-3
   Jones MW, 2006, IEEE T VIS COMPUT GR, V12, P581, DOI 10.1109/TVCG.2006.56
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Kanai T, 2006, PROC S GEOMETRY PROC, P21
   Liu SJ, 2016, COMPUT AIDED DESIGN, V78, P147, DOI 10.1016/j.cad.2016.05.001
   Liu SJ, 2013, VISUAL COMPUT, V29, P627, DOI 10.1007/s00371-013-0801-9
   Liu SJ, 2012, COMPUT AIDED GEOM D, V29, P435, DOI 10.1016/j.cagd.2012.03.011
   Macêdo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   Mayers D.F., 2003, INTRO NUMERICAL ANAL
   Ohtake Y, 2005, GRAPH MODELS, V67, P150, DOI 10.1016/j.gmod.2004.06.003
   Pan RJ, 2009, SCI CHINA SER F, V52, P308, DOI 10.1007/s11432-009-0032-x
   Park T, 2012, IEEE T VIS COMPUT GR, V18, P1638, DOI 10.1109/TVCG.2011.286
   Sanchez M, 2015, COMPUT GRAPH FORUM, V34, P277, DOI 10.1111/cgf.12599
   Shen C, 2004, ACM T GRAPHIC, V23, P896, DOI 10.1145/1015706.1015816
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Wendland H., 2004, Scattered data approximation, DOI [10.1017/CBO9780511617539, DOI 10.1017/CBO9780511617539]
   Wu ZM, 2010, INT J COMPUT MATH, V87, P583, DOI 10.1080/00207160802158702
   Yngve G, 2002, IEEE T VIS COMPUT GR, V8, P346, DOI 10.1109/TVCG.2002.1044520
NR 28
TC 3
Z9 4
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 779
EP 791
DI 10.1007/s00371-018-1529-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400004
DA 2024-07-18
ER

PT J
AU Umenhoffer, T
   Szirmay-Kalos, L
   Szécsi, L
   Lengyel, Z
   Marinov, G
AF Umenhoffer, Tamas
   Szirmay-Kalos, Laszlo
   Szecsi, Laszlo
   Lengyel, Zoltan
   Marinov, Gabor
TI An image-based method for animated stroke rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE NPR; Stroke rendering; Hatching
AB This paper presents an image-space stroke rendering algorithm that provides temporally coherent placement of lines at particles that are moving with object surfaces. We generate particles in image space and move them according to an image-space velocity field. Consistent image-space density is achieved by a deterministic rejection-based algorithm that uses low-discrepancy series to filter out overpopulated areas and to fill in underpopulated regions. Our line stabilization method can solve the temporal continuity problems of image-space techniques. The multi-pass algorithm is implemented entirely on the GPU using geometry shaders and vertex transform feedback. Our method provides high-quality results and is implemented as an interactive post processing step. We also provide a wide toolset for artists to control the final rendering style and extended the method to process real-life RGBZ footage.
C1 [Umenhoffer, Tamas; Szirmay-Kalos, Laszlo; Szecsi, Laszlo; Lengyel, Zoltan] Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
   [Marinov, Gabor] Limes Super Ltd, Budapest, Hungary.
C3 Budapest University of Technology & Economics
RP Umenhoffer, T (corresponding author), Budapest Univ Technol & Econ, Dept Control Engn & Informat Technol, Budapest, Hungary.
EM umenhoffer@iit.bme.hu
RI Szirmay-Kalos, Laszlo/H-3853-2012; Umenhoffer, Tamás/H-3732-2012
OI Szirmay-Kalos, Laszlo/0000-0002-8523-2315; 
FU Hungarian Scientific Research Fund [OTKA K-124124]
FX This study was funded by the Hungarian Scientific Research Fund (OTKA
   K-124124).
CR Bénard P, 2010, COMPUT GRAPH FORUM, V29, P1497, DOI 10.1111/j.1467-8659.2010.01747.x
   Benard P., P 2009 S INT 3D GRAP, P121
   Bénard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Bénard P, 2011, COMPUT GRAPH FORUM, V30, P2367, DOI 10.1111/j.1467-8659.2011.02075.x
   Bousseau A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276507
   Breslav S., 2007, P SIGGRAPH 2007
   Cornish D., 2001, No description on Graphics interface 2001, P151
   Cunzi M., 2003, P GRAPHICS INTERFACE
   Elber G, 1999, COMPUT GRAPH FORUM, V18, pC1, DOI 10.1111/1467-8659.00322
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   FINKELSTEIN A., 2010, P I3D 2010
   Fiser J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925948
   Girshick Ahna., 2000, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering 2000, P43, DOI DOI 10.1145/340916.340922
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Kalnins RD, 2002, ACM T GRAPHIC, V21, P755, DOI 10.1145/566570.566648
   Kaplan M., 2000, NPAR, P67, DOI 10.1145/340916.340925
   Kass M., 2011, P SIGGRAPH 2011
   Kim Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409109
   Klein AW, 2000, COMP GRAPH, P527, DOI 10.1145/344779.345075
   Kowalski MA, 1999, COMP GRAPH, P433, DOI 10.1145/311535.311607
   Lake C., 2000, Proceedings of the first international symposium on Non-photorealistic animation and rendering-NPAR'00, P13, DOI 10.1145/340916.3409185[27]M.S.
   Lee H., 2006, NPAR 2006, P37, DOI [10. 1145/1124728. 1124735, DOI 10.1145/1124728.1124735]
   Meier B. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P477, DOI 10.1145/237170.237288
   Niederreiter H, 1992, RANDOM NUMBER GENERA, V63
   PRAUN E, 2001, P SIGGRAPH 2001, P579
   Salisbury M. P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P101, DOI 10.1145/192161.192185
   Singh Mayank, 2010, P COMP AESTH 10, DOI [10.2312/COMPAESTH/COMPAESTH10/025-032, DOI 10.2312/COMPAESTH/COMPAESTH10/025-032]
   Suarez J, 2017, VISUAL COMPUT, V33, P1319, DOI 10.1007/s00371-016-1222-3
   Szirmay- Kalos L., 2015, PERIOD POLYTECH ELEC, V59, P175
   Tamas U, 2011, COMPUT GRAPH FORUM, V30, P533, DOI 10.1111/j.1467-8659.2011.01878.x
   Vanderhaeghe D., 2007, Proc. EGSR '07, P139
   Vergne R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531331
NR 33
TC 6
Z9 8
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 817
EP 827
DI 10.1007/s00371-018-1531-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400007
DA 2024-07-18
ER

PT J
AU Zhang, JZ
   Zheng, JM
   Thalmann, NM
AF Zhang, Juzheng
   Zheng, Jianmin
   Thalmann, Nadia Magnenat
TI MCAEM: mixed-correlation analysis-based episodic memory for
   companion-user interactions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Companion-human interaction; Episodic memory; Computational model;
   Noise-resistant retrieval; Correlation analysis
ID TERM
AB This paper considers episodic memory for companion-human interaction, aiming at improving user experience of interactions by endowing social companions with awareness of past experience. Due to noise and incomplete cues from natural language and speech in real-world interaction, accurate memory retrieval is very challenging and the noise resistance is important in practice. To improve the robustness of companion-human interaction, we propose a mixed-correlation analysis-based episodic memory (MCAEM) model, in which the correlations between memory elements are analyzed and then utilized for memory retrieval. In particular, the correlations are analyzed in three aspects: the relations between elements, importance of attributes and order of events. Based on the mixed-correlation analysis, a new similarity measure is constructed, which has substantially enhanced the noise resistance of memory retrieval. Experiments on a dataset collected from interaction in movies quantitatively evaluate the MCAEM model and compare it with prior work. Also, a user study is conducted to investigate the benefits of integrating the MCAEM model into social companions. The results demonstrate that the companions equipped with the MCAEM model not only have better retrieval performance, but also improve user experience in many aspects.
C1 [Zhang, Juzheng; Thalmann, Nadia Magnenat] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
   [Zheng, Jianmin] Nanyang Technolocial Univ, Sch Comp Sci & Engn, Singapore, Singapore.
C3 Nanyang Technological University
RP Zhang, JZ (corresponding author), Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
EM jzhang19@e.ntu.edu.sg; asjmzheng@ntu.edu.sg; nadiathalmann@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Thalmann, Nadia/AAK-5195-2021
OI Zheng, Jianmin/0000-0002-5062-6226; Thalmann, Nadia/0000-0002-1459-5960
FU National Research Foundation, Prime Ministers Office, Singapore, under
   International Research Centres in Singapore Funding Initiative
   [M4098031]
FX This study was funded by the National Research Foundation, Prime
   Ministers Office, Singapore, under its International Research Centres in
   Singapore Funding Initiative (Grant number: M4098031).
CR [Anonymous], 2011, P INT C AUG AG MULT
   [Anonymous], HUMAN COMPUTER INTER
   [Anonymous], COMPUT ANIMAT VIRTUA
   [Anonymous], P 20 FLOR ART INT RE
   [Anonymous], THESIS
   [Anonymous], 2007, AAAI
   Brom C, 2009, LECT NOTES ARTIF INT, V5773, P42, DOI 10.1007/978-3-642-04380-2_8
   Cambria E., 2012, SENTIC COMPUTING
   Danilava S, 2012, ICAART: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE, VOL. 2, P282, DOI 10.5220/0003834702820289
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Dodd W, 2005, 2005 IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P692
   Faltersack Z., 2011, AAAI 2011 FALL S ADV, P106
   François D, 2009, INTERACT STUD, V10, P324, DOI 10.1075/is.10.3.04fra
   Ho WC, 2005, IEEE C EVOL COMPUTAT, P573
   Howard MW, 2002, J MEM LANG, V46, P85, DOI 10.1006/jmla.2001.2798
   Jockel S, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P1075, DOI 10.1109/ROBIO.2007.4522313
   Kasap Z, 2012, VISUAL COMPUT, V28, P87, DOI 10.1007/s00371-011-0630-7
   Kasap Z, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.26
   Leite Iolanda, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P669, DOI 10.1109/ROMAN.2009.5326256
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   Liu GD, 2013, VISUAL COMPUT, V29, P871, DOI 10.1007/s00371-013-0846-9
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Mueller ST, 2006, INT C LEARN DEV ICDL
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Steyvers M, 2006, TRENDS COGN SCI, V10, P327, DOI 10.1016/j.tics.2006.05.005
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   Wang D, 2015, IEEE T COMP INTEL AI, V7, P123, DOI 10.1109/TCIAIG.2014.2336702
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
NR 29
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1129
EP 1141
DI 10.1007/s00371-018-1537-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400034
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Song, P
   Fu, ZQ
   Liu, LG
AF Song, Peng
   Fu, Zhongqi
   Liu, Ligang
TI Grasp planning via hand-object geometric fitting
SO VISUAL COMPUTER
LA English
DT Article
DE Grasp planning; 3D object; Hand model; Geometric fitting; Grasp quality
ID DECOMPOSITION
AB Grasp planning is crucial for many robotic applications such as object manipulation and object transport. Planning stable grasps is a challenging problem. Many parameters such as object geometry, hand geometry and kinematics, hand-object contacts have to be considered, making the space of grasps too large to be exhaustively searched. This paper presents a general approach for planning grasps on 3D objects based on hand-object geometric fitting. Our key idea is to build a contact score map on a 3D object's voxelization, and apply this score map and a hand's kinematic parameters to find a set of target contacts on the object surface. Guided by these target contacts, we find grasps with a high quality measure by iteratively adjusting the hand pose and joint angles to fit the hand's instantaneous geometric shape with the object's fixed shape, during which the fitting process is speeded up by taking advantage of the discrete volumetric space. We demonstrate the effectiveness of our grasp planning approach on 3D objects of various shapes, poses, and sizes, as well as hand models with different kinematics. A comparison with two state-of-the-art approaches shows that our approach can generate grasps that are more likely to be stable, especially for objects with complex shapes.
C1 [Song, Peng] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
   [Fu, Zhongqi; Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Song, P (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
EM songpeng@ustc.edu.cn; fzq2011@mail.ustc.edu.cn; lgliu@ustc.edu.cn
RI Song, Peng/ABE-7649-2020; Song, Peng/ABH-5214-2020
OI Song, Peng/0000-0003-2734-2783; 
FU Anhui Provincial Natural Science Foundation [1508085QF122]; National
   Natural Science Foundation of China [61403357, 61672482, 11526212]; One
   Hundred Talent Project of the Chinese Academy of Sciences
FX This work is supported in part by Anhui Provincial Natural Science
   Foundation (1508085QF122), National Natural Science Foundation of China
   (61403357, 61672482, 11526212), and the One Hundred Talent Project of
   the Chinese Academy of Sciences.
CR [Anonymous], 2005, ACM SIGGRAPH EUR S C, DOI DOI 10.1145/1073368.1073413
   Attene M., VIS COMPUT, V22, P181
   Aydin Y, 1999, COMPUT GRAPH-UK, V23, P145, DOI 10.1016/S0097-8493(98)00122-8
   Ben Amor H, 2008, COMPUT ANIMAT VIRT W, V19, P445, DOI 10.1002/cav.252
   Berenson D, 2007, IEEE-RAS INT C HUMAN, P42, DOI 10.1109/ICHR.2007.4813847
   Bohg J, 2014, IEEE T ROBOT, V30, P289, DOI 10.1109/TRO.2013.2289018
   Chalmeta R, 2013, COMPUT AIDED DESIGN, V45, P93, DOI 10.1016/j.cad.2012.07.012
   Chao-Hui Shen, 2012, ACM Transactions on Graphics, V31, DOI 10.1145/2366145.2366199
   Ciocarlie MT, 2009, INT J ROBOT RES, V28, P851, DOI 10.1177/0278364909105606
   Ding L, 2014, VISUAL COMPUT, V30, P189, DOI 10.1007/s00371-013-0795-3
   Goldfeder C, 2007, IEEE INT CONF ROBOT, P4679, DOI 10.1109/ROBOT.2007.364200
   Güngör C, 2014, SIG PROCESS COMMUN, P1706, DOI 10.1109/SIU.2014.6830577
   Huebner K, 2008, IEEE INT CONF ROBOT, P1628, DOI 10.1109/ROBOT.2008.4543434
   Kim J, 2013, IEEE T ROBOT, V29, P1424, DOI 10.1109/TRO.2013.2273846
   Kry PG, 2006, ACM T GRAPHIC, V25, P872, DOI 10.1145/1141911.1141969
   Kyota F, 2012, COMPUT GRAPH FORUM, V31, P765, DOI 10.1111/j.1467-8659.2012.03035.x
   Lau M., ACM T GRAPH SIGGRAPH, V35
   Li Y, 2013, IEEE INT CONF ROBOT, P3265, DOI 10.1109/ICRA.2013.6631032
   Li Y, 2007, IEEE T VIS COMPUT GR, V13, P732, DOI [10.1109/TVCG.2007.1033, 10.1109/TVCG.2007.1033.]
   Miller AT, 2004, IEEE ROBOT AUTOM MAG, V11, P110, DOI 10.1109/MRA.2004.1371616
   Miller AT, 2003, IEEE INT CONF ROBOT, P1824, DOI 10.1109/ROBOT.2003.1241860
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   PARK YC, 1992, INT J ROBOT RES, V11, P163, DOI 10.1177/027836499201100301
   Przybylski Markus, 2010, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010), P1592, DOI 10.1109/IROS.2010.5653520
   Roa MA, 2015, AUTON ROBOT, V38, P65, DOI 10.1007/s10514-014-9402-3
   Romon E, 1996, IEEE INT CONF ROBOT, P1795, DOI 10.1109/ROBOT.1996.506972
   Rosales C, 2011, INT J ROBOT RES, V30, P431, DOI 10.1177/0278364910370218
   Sahbani A, 2012, ROBOT AUTON SYST, V60, P326, DOI 10.1016/j.robot.2011.07.016
   Ye Y, 2012, CHIN J NAT MEDICINES, V10, P1, DOI [10.3724/SP.J.1009.2012.00001, 10.1016/S1875-5364(12)60001-6]
   Zhao WP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508412
   Zhou QY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461919
NR 31
TC 20
Z9 25
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 257
EP 270
DI 10.1007/s00371-016-1333-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800009
DA 2024-07-18
ER

PT J
AU Zhu, MH
   Morin, G
   Charvillat, V
   Ooi, WT
AF Zhu, Minhui
   Morin, Geraldine
   Charvillat, Vincent
   Ooi, Wei Tsang
TI Sprite tree: an efficient image-based representation for networked
   virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE 3D image warping; Image-based rendering; Remote rendering; Networked
   virtual environments
ID VISUALIZATION; SIMPLIFICATION; DEVICES
AB We propose a new and efficient image-based representation for networked virtual environments, called the sprite tree. A sprite tree organizes multiple reference images efficiently and compactly for accelerating the rendering of complex virtual scenes. Using our basic construction and rendering methods, the results show that a sprite tree can efficiently organize the pixels from hundreds of distinctive reference images and accelerate the rendering of a complex scene. Furthermore, we propose the sprite view similarity to (i) largely reduce the lighting artifacts in the rendered images, and (ii) significantly reduce the redundancy and the tree size with little loss of the visual quality.
C1 [Zhu, Minhui; Ooi, Wei Tsang] Natl Univ Singapore, Singapore, Singapore.
   [Morin, Geraldine; Charvillat, Vincent] Univ Toulouse, Toulouse, France.
C3 National University of Singapore; Universite de Toulouse
RP Zhu, MH (corresponding author), Natl Univ Singapore, Singapore, Singapore.
EM minhui7zhu@gmail.com; Morin@enseeiht.fr; Vincent.Charvillat@enseeiht.fr;
   ooiwt@comp.nus.edu.sg
RI Ooi, Wei Tsang/HLW-5142-2023; Ooi, Wei Tsang/AAE-7810-2019
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736
CR Aliaga DG, 1996, IEEE VISUAL, P101, DOI 10.1109/VISUAL.1996.567774
   [Anonymous], 2009, P 17 ACM INT C MULT
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Bouatouch K., 2005, RR5447 FRENCH I RES
   Chang CF, 2002, LECT NOTES COMPUT SC, V2532, P1105
   Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571
   Cheng L., 2004, P AS C COMP VIS, P1
   Cohen-Or D, 2002, SPRING EUROGRAP, P75
   Decaudin P, 2009, COMPUT GRAPH FORUM, V28, P2079, DOI 10.1111/j.1467-8659.2009.01354.x
   Décoret X, 2003, ACM T GRAPHIC, V22, P689, DOI 10.1145/882262.882326
   Decoret X, 1999, COMPUT GRAPH FORUM, V18, pC61, DOI 10.1111/1467-8659.00328
   Ghiletiuc J., 2013, P 6 INT C COMP VIS C, V14, P1
   Gortler S.J., 1996, ACM T GRAPH, V23, P43
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hudson T. C., 1999, TECH REP
   Im YH, 2005, IEEE T VIS COMPUT GR, V11, P265
   Iosup A, 2014, P INT WORKSH MASS MU, P1
   Jeschke S., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P181
   Lamberti F, 2007, IEEE T VIS COMPUT GR, V13, P247, DOI 10.1109/TVCG.2007.29
   Lee K, 2012, IEEE ACM T NETWORK, V20, P515, DOI 10.1109/TNET.2011.2172984
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liang HG, 2009, MULTIMED TOOLS APPL, V45, P163, DOI 10.1007/s11042-009-0304-x
   Lischinski D., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P301
   Ma L, 2009, 2009 IEEE YOUTH CONFERENCE ON INFORMATION, COMPUTING AND TELECOMMUNICATION, PROCEEDINGS, P538, DOI 10.1109/YCICT.2009.5382439
   Maciel P. W. C., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P95, DOI 10.1145/199404.199420
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   McMillan Leonard, 1997, THESIS
   Noimark Y, 2003, IEEE COMPUT GRAPH, V23, P58, DOI 10.1109/MCG.2003.1159614
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Popescu V, 1998, VISUALIZATION '98, PROCEEDINGS, P211, DOI 10.1109/VISUAL.1998.745305
   Qu H., 2003, P 14 IEEE VIS 2003 V, P58
   Schaufler G, 1996, COMPUT GRAPH FORUM, V15, pC227, DOI 10.1111/1467-8659.1530227
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Shade J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P75, DOI 10.1145/237170.237209
   Shen S., 2014, P NETW OP SYST SUPP, P13
   Shi S., 2010, Proceeding of ACM international conference on Multimedia, P601
   Shi S., 2012, ACM Trans. Multimedia Comput. Commun. Appl, V8, DOI DOI 10.1145/2348816.2348825
   Sillion F, 1997, COMPUT GRAPH FORUM, V16, pC207, DOI 10.1111/1467-8659.00158
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wilson A, 2003, ACM T GRAPHIC, V22, P678, DOI 10.1145/882262.882325
   Zhao S., 2013, P 4 ACM MULT SYST C, P178
   Zhu Minhui, 2011, P 19 ACM INT C MULT, P183, DOI DOI 10.1145/2072298.2072324
NR 43
TC 2
Z9 3
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1385
EP 1402
DI 10.1007/s00371-016-1286-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100003
DA 2024-07-18
ER

PT J
AU Khmag, A
   Ramli, A
   Al-haddad, SAR
   Yusoff, S
   Kamarudin, NH
AF Khmag, Asem
   Ramli, Abd Rahman
   Al-haddad, S. A. R.
   Yusoff, Suhaimi
   Kamarudin, N. H.
TI Denoising of natural images through robust wavelet thresholding and
   genetic programming
SO VISUAL COMPUTER
LA English
DT Article
DE Gaussian noise; Genetic programming; Image denoising; Nonlinear filters;
   Salt-and-pepper noise; Switching scheme
ID MEDIAN FILTERS; RESTORATION; ALGORITHM; SPARSE
AB Digital images play an essential role in analysis tasks that can be applied in various knowledge domains, including medicine, meteorology, geology, and biology. Such images can be degraded by noise during the process of acquisition, transmission, storage, or compression. The use of local filters in image restoration may generate artifacts when these filters are not well adapted to the image content as a result of the heuristic optimization of local filters. Denoising methods based on learning procedure are more capable than parametric filters for addressing the conflicts between noise suppression and artifact reduction. In this study, we present a nonlinear filtering method based on a two-step switching scheme to remove both salt-and-pepper and additive white Gaussian noises. In the switching scheme, two cascaded detectors are used to detect noise, and two corresponding estimators are employed to effectively and efficiently filter the noise in an image. In the process of training, a method according to patch clustering is utilized, and genetic programming (GP) is subsequently applied to determine the optimum filter (wavelet-domain filter) for each individual cluster, while in testing part, the optimum filter trained beforehand by GP is recovered and used on the inputted corrupted patch. This adaptive structure is employed to cope with several noise types. Experimental and comparative analysis results show that the denoising performance of the proposed method is superior to that of existing denoising methods as per both quantitative and qualitative assessments.
C1 [Khmag, Asem; Ramli, Abd Rahman; Al-haddad, S. A. R.; Yusoff, Suhaimi; Kamarudin, N. H.] UPM, Fac Engn, Seri Kembangan, Malaysia.
C3 Universiti Putra Malaysia
RP Khmag, A (corresponding author), UPM, Fac Engn, Seri Kembangan, Malaysia.
EM khmaj2002@gmail.com; arr@upm.edu.my; sar@upm.edu.my;
   suhaimi1980@gmail.com; hidayu.kamarudin@gmail.com
RI Khmag, Asem/AAH-1051-2019; Al-Haddad, S. A. R./AAM-6449-2020; kamarudin,
   noraziahtulhidayu/AAQ-8508-2021
OI Khmag, Asem/0000-0002-1360-5346; Kamarudin,
   Noraziahtulhidayu/0000-0001-7467-4348
CR [Anonymous], INT REV COMPUT SOFTW
   [Anonymous], BRIT MACH VIS C BMVC
   [Anonymous], PHYS REV E
   [Anonymous], IEEJ T ELECT ELECT E
   [Anonymous], FIELDGUIDE GENETIC P
   Atkins D, 2011, IEEE C EVOL COMPUTAT, P238
   Bouboulis P, 2010, IEEE T IMAGE PROCESS, V19, P1465, DOI 10.1109/TIP.2010.2042995
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196
   Chatterjee P, 2009, IEEE T IMAGE PROCESS, V18, P1438, DOI 10.1109/TIP.2009.2018575
   Chen F, 2009, IEEE T SIGNAL PROCES, V57, P2467, DOI 10.1109/TSP.2009.2018358
   Dabov K., 2009, Signal Processing with Adaptive Sparse Structured Representations (SPARS'09)
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Everingham M., 2005, The PASCAL Visual Object Classes Challenge Results (VOC 2005)
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   HWANG H, 1995, IEEE T IMAGE PROCESS, V4, P499, DOI 10.1109/83.370679
   Isgrò F, 2008, PARALLEL COMPUT, V34, P727, DOI 10.1016/j.parco.2008.09.005
   Jain P., 2013, IETE J. Educ, V54, P108, DOI DOI 10.1080/09747338.2013.10876113
   Jain P, 2015, VISUAL COMPUT, V31, P657, DOI 10.1007/s00371-014-0993-7
   Khmag A, 2015, J MED IMAG HEALTH IN, V5, P1261, DOI 10.1166/jmihi.2015.1523
   Kim EY, 2013, PATTERN RECOGN LETT, V34, P226, DOI 10.1016/j.patrec.2012.09.013
   KO SJ, 1991, IEEE T CIRCUITS SYST, V38, P984, DOI 10.1109/31.83870
   Korürek M, 2010, EXPERT SYST APPL, V37, P1946, DOI 10.1016/j.eswa.2009.07.018
   Koza J. R., 1994, GENETIC PROGRAMMING
   Krommweh J, 2010, J VIS COMMUN IMAGE R, V21, P364, DOI 10.1016/j.jvcir.2010.02.011
   Li Y, 2011, IEEE T CONTR SYST T, V19, P656, DOI 10.1109/TCST.2010.2052257
   Liu Q, 2016, VISUAL COMPUT, V32, P535, DOI 10.1007/s00371-015-1087-x
   Liu Y., 2015, International Journal of Signal Processing, Image Processing and Pattern Recognition, V8, P29, DOI 10.14257/ijsip.2015.8.2.04
   Orchard J, 2008, IEEE IMAGE PROC, P1732, DOI 10.1109/ICIP.2008.4712109
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petrovic N, 2005, LECT NOTES COMPUT SC, V3708, P643
   Petrovic NI, 2008, IEEE T IMAGE PROCESS, V17, P1109, DOI 10.1109/TIP.2008.924388
   Petrovic NI, 2006, LECT NOTES COMPUT SC, V4179, P103
   Porikli F., 2008, IEEE C COMPUTER VISI
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   Rajwade A, 2013, IEEE T PATTERN ANAL, V35, P849, DOI 10.1109/TPAMI.2012.140
   RUDIN LI, 1994, IEEE IMAGE PROC, P31
   Shao L, 2008, IEEE T IMAGE PROCESS, V17, P1772, DOI 10.1109/TIP.2008.2002162
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Stockman George, 2001, Computer Vision
   Thaipanich T, 2010, IEEE T CONSUM ELECTR, V56, P2623, DOI 10.1109/TCE.2010.5681149
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wongsawat Y, 2005, IEEE INT SYMP CIRC S, P5990, DOI 10.1109/ISCAS.2005.1466004
   Xiong B, 2012, IEEE T IMAGE PROCESS, V21, P1663, DOI 10.1109/TIP.2011.2172804
   Yan RM, 2014, SIGNAL PROCESS, V103, P36, DOI 10.1016/j.sigpro.2013.11.019
   Yan RM, 2013, IEEE T IMAGE PROCESS, V22, P4689, DOI 10.1109/TIP.2013.2277813
   Zhang SQ, 2002, IEEE SIGNAL PROC LET, V9, P360, DOI 10.1109/LSP.2002.805310
   Zhano M, 2008, IEEE T IMAGE PROCESS, V17, P2324, DOI 10.1109/TIP.2008.2006658
NR 51
TC 21
Z9 22
U1 1
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1141
EP 1154
DI 10.1007/s00371-016-1273-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chi, J
   Gao, SS
   Zhang, CM
AF Chi, Jing
   Gao, Shanshan
   Zhang, Caiming
TI Interactive facial expression editing based on spatio-temporal coherency
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Facial animation; Editing; Spatio-temporal motion; Adaptive segmentation
ID CAPTURE
AB We present a novel approach for interactively and intuitively editing 3D facial animation in this paper. It determines a new expression by combining the user-specified constraints with the priors contained in a pre-recorded facial expression set, which effectively overcomes the generation of an unnatural expression caused by only user-constraints. The approach is based on the framework of example-based linear interpolation. It adaptively segments the face model into soft regions based on user-interaction. In dependently modeling each region, we propose a new function to estimate the blending weight of each example that matches the user-constraints as well as the spatio-temporal properties of the face set. In blending the regions into a single expression, we present a new criterion that fully exploits the spatial proximity and the spatio-temporal motion consistency over the face set to measure the coherency between vertices and use the coherency to reasonably propagate the influence of each region to the entire face model. Experiments show that our approach, even with inappropriate user's edits, can create a natural expression that optimally satisfies the user-desired goal.
C1 [Chi, Jing; Gao, Shanshan; Zhang, Caiming] Shandong Univ Finance & Econ, Dept Comp Sci & Technol, Jinan, Peoples R China.
C3 Shandong University of Finance & Economics
RP Chi, J (corresponding author), Shandong Univ Finance & Econ, Dept Comp Sci & Technol, Jinan, Peoples R China.
EM peace_world_cj@hotmail.com
RI Zhang, Caiming/AHD-6558-2022
OI Zhang, Caiming/0000-0002-6365-6221; Chi, Jing/0000-0002-0307-0608
FU National Nature Science Foundation of China [61332015, U1609218,
   61303088, 61402261]; Sci-tech Development Project of Jinan City
   [201303021]; Fostering Project of Dominant Discipline and Talent Team of
   Shandong Province Higher Education Institutions
FX The work is supported by National Nature Science Foundation of China
   under Grant 61332015, U1609218, 61303088, 61402261, Sci-tech Development
   Project of Jinan City under Grant 201303021. This work is supported by
   The Fostering Project of Dominant Discipline and Talent Team of Shandong
   Province Higher Education Institutions.
CR Acquaah K., 2015, IEEE INT C EL COMP C, P1
   Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   [Anonymous], 2001, P 2001 S INTERACTIVE
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Cetinaslan O., 2015, EG
   Jin-xiang Chai, 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P193
   Joshi P., 2003, SCA'03: Proc. of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P35
   Lau M, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640446
   Lewis JP, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.41
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li Q, 2008, IEEE COMPUT GRAPH, V28, P76, DOI 10.1109/MCG.2008.120
   Ma Xiaohan., 2009, Proceedings of SCA 2009, P123
   Neumann T., 2013, ACM T GRAPHIC, V32, P2504
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   Qingshan Zhang, 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P177
   Seo Jaewoo, 2011, ACM T GRAPHIC, V30, P6
   Seol Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159519
   Seol Y, 2012, VISUAL COMPUT, V28, P319, DOI 10.1007/s00371-011-0641-4
   Tena J.R., 2011, ACM SIGGRAPH
   Xu F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601210
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
NR 22
TC 5
Z9 6
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 981
EP 991
DI 10.1007/s00371-017-1387-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800028
OA hybrid
DA 2024-07-18
ER

PT J
AU Danelakis, A
   Theoharis, T
   Pratikakis, I
AF Danelakis, Antonios
   Theoharis, Theoharis
   Pratikakis, Ioannis
TI A spatio-temporal wavelet-based descriptor for dynamic 3D facial
   expression retrieval and recognition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Dynamic 3D mesh sequence; Object retrieval; Facial expressions;
   Positional information; Wavelet transformation
ID REAL-TIME; SEQUENCES; TRANSFORM
AB Human emotions are often expressed by facial expressions and are generated by facial muscle movements. In recent years, the analysis of facial expressions has emerged as an active research area due to its various applications such as human-computer interaction, human behavior understanding, biometrics, emotion recognition, computer graphics, driver fatigue detection, and psychology. A novel analysis of dynamic 3D facial expressions using the positional information of automatically detected facial landmarks and the wavelet transformation is presented, which results in the proposed spatio-temporal descriptor. This descriptor is employed within the current paper in a retrieval scheme for dynamic 3D facial expression datasets and is thoroughly evaluated. Experiments have been conducted using the six prototypical expressions of the publicly available BU-4DFE dataset as well as the eight expressions included in the newly released publicly available BP4D-Spontaneous dataset. The obtained retrieval results outperform the retrieval results of the state-of-the-art methodologies. Furthermore, the retrieval results are exploited to achieve unsupervised dynamic 3D facial expression recognition. The aforementioned unsupervised procedure achieves better recognition accuracy compared to supervised dynamic 3D facial expression recognition state-of-the-art techniques.
C1 [Danelakis, Antonios; Theoharis, Theoharis] Univ Athens, Dept Informat & Telecommun, Athens, Greece.
   [Theoharis, Theoharis] Norwegian Univ Sci & Technol NTNU, IDI, Trondheim, Norway.
   [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, Xanthi, Greece.
C3 National & Kapodistrian University of Athens; Norwegian University of
   Science & Technology (NTNU); Democritus University of Thrace
RP Danelakis, A (corresponding author), Univ Athens, Dept Informat & Telecommun, Athens, Greece.
EM a.danelakis@gmail.com
RI Theoharis, Theoharis/AAN-2555-2020; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688
CR [Anonymous], 2012, Computer Vision and Pattern Recognition Workshops CVPRW
   [Anonymous], 2008, A wavelet tour of signal processing: The sparse way
   [Anonymous], IEEE FG 13
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Bovik A, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, pV, DOI 10.1016/B978-012119792-6/50062-0
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Danelakis A, 2014, P 7 EUR WORKSH 3D OB, P1
   Danelakis A, 2015, P 2015 EUROGRAPHICS, P63
   Danelakis A, 2016, VISUAL COMPUT, V32, P257, DOI 10.1007/s00371-015-1142-7
   Danelakis A, 2016, PATTERN RECOGN, V52, P174, DOI 10.1016/j.patcog.2015.10.012
   Danelakis A, 2015, MULTIMED TOOLS APPL, V74, P5577, DOI 10.1007/s11042-014-1869-6
   Dapogny Arnaud, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163111
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Drira H, 2012, INT C PATT RECOG, P1104
   Ekman P., 1978, Facial action coding system
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Jeni LA, 2012, IMAGE VISION COMPUT, V30, P785, DOI 10.1016/j.imavis.2012.02.003
   NAVARRO R, 1991, MULTIDIM SYST SIGN P, V2, P421, DOI 10.1007/BF01937176
   Perakis P, 2014, PATTERN RECOGN, V47, P2783, DOI 10.1016/j.patcog.2014.03.007
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Quiroga RQ, 2001, BRAIN RES PROTOC, V8, P16, DOI 10.1016/S1385-299X(01)00077-0
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sun Y, 2008, LECT NOTES COMPUT SC, V5303, P58, DOI 10.1007/978-3-540-88688-4_5
   Sun Y, 2010, IEEE T SYST MAN CY A, V40, P461, DOI 10.1109/TSMCA.2010.2041659
   Tianhong Fang, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1594, DOI 10.1109/ICCVW.2011.6130440
   Tsalakanidou F, 2009, PROC CVPR IEEE, P763
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   Yi Sun, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563125
   Yin LJ, 2006, INT C PATT RECOG, P1248
   Yin LJ, 2008, IEEE INT CONF AUTOMA, P116
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
NR 33
TC 13
Z9 15
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1001
EP 1011
DI 10.1007/s00371-016-1243-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600031
DA 2024-07-18
ER

PT J
AU Hildebrandt, D
AF Hildebrandt, Dieter
TI Image-based styling
SO VISUAL COMPUTER
LA English
DT Article
DE Styling; Image-based representation; Visualization model;
   Domain-specific language; Taxonomy; Optimization; Interoperability;
   Service-oriented computing
ID VISUALIZATION; DESIGN
AB The same data can be visualized using various visual styles that each is suitable for specific requirements, e.g., 3D geodata visualized using photorealistic, cartographic, or illustrative styles. In contrast to feature-based styling, image-based styling performed in image space at image resolution allows decoupling styling from image generation and output-sensitive, expressive styling. However, leveraging image-based styling is still impeded. No previous approach allows specifying image-based styling expressively with an extensive inventory of composable operators, while providing styling functionality in a service-oriented, interoperable manner. In this article, we present an interactive system for specifying and providing the functionality of image-based styling. As key characteristics, it separates concerns of styling from image generation and facilitates specifying styling as algebraic compositions of high-level operators using a unified 3D model representation. We propose a generalized visualization model, an image-based styling algebra, two declarative DSLs, an operator taxonomy, an operational model, and a standards-based service interface. The approach facilitates expressive specifications of image-based styling for design, description, and analysis and leveraging the functionality of image-based styling in a service-oriented, interoperable, reusable, and composable manner.
C1 [Hildebrandt, Dieter] Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
C3 University of Potsdam
RP Hildebrandt, D (corresponding author), Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
EM dieter.hildebrandt@uni-potsdam.de
CR Adobe Systems Inc, 2010, AD PIX BEND DEV GUID
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], 2013, MATLAB REL
   [Anonymous], 2009, Database Systems: The Complete Book
   Apple Inc, 2007, APPL QUARTZ COMP US
   Baumann P, 2010, GEOINFORMATICA, V14, P447, DOI 10.1007/s10707-009-0087-2
   Bertin J., 1983, SEMIOLOGY GRAPHICS D
   Bousseau A., 2009, THESIS J FOURIER U
   Brinkmann R, 2008, The Art And Science Of Digital Compositing: Techniques For Visual Effects, Animation And Motion Graphics, DOI 10.1016/B978-0-12-370638-6.X0001-6
   Brodlie K., 2007, THEORY PRACTICE COMP
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Buchin K., 2004, P ICA MOUNT CART WOR
   CASNER SM, 1991, ACM T GRAPHIC, V10, P111, DOI 10.1145/108360.108361
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Chang S.F., 1993, THESIS U CALIFORNIA
   Chuah M.C., 1996, P INFOVIS
   Cole F., 2006, EUROGRAPHICS S RENDE, P377
   Dahlstrom E., 2011, SCALABLE VECTOR GRAP
   Dollner J., 2003, P INFOVIS
   Duff T., 1985, P SIGGRAPH, V19
   Eissele M., 2004, P SIM VIS
   ESRI Inc, 2014, ARCGIS 10 2 MAN
   Foley T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1965002
   Gilson O, 2008, COMPUT GRAPH FORUM, V27, P959, DOI 10.1111/j.1467-8659.2008.01230.x
   Glander T., 2013, THESIS U POTSDAM
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Gotz D., 2009, P IUI
   HABER RB, 1990, VISUALIZATION SCI CO, V74, P93
   Haberling C., 2003, THESIS ETH ZURICH
   Heer J, 2010, IEEE T VIS COMPUT GR, V16, P1149, DOI 10.1109/TVCG.2010.144
   Hildebrandt D, 2014, ISPRS INT GEO-INF, V3, P1445, DOI 10.3390/ijgi3041445
   Hildebrandt D, 2014, GEOINFORMATICA, V18, P537, DOI 10.1007/s10707-013-0189-8
   Hirzel M, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2528412
   Imhof E., 1982, CARTOGRAPHIC RELIEF
   Iosifescu Enescu I., 2011, THESIS ETH ZURICH
   ISO, 2008, 197751 ISOIEC
   Jensen HW., 2001, P SIGGRAPH
   Jimenez J, 2012, COMPUT GRAPH FORUM, V31, P355, DOI 10.1111/j.1467-8659.2012.03014.x
   Khronos Group, 2014, OPENCL SPEC V2
   Kosara R., 2001, P INFOVIS
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Maass S., 2009, THESIS U POTSDAM
   McCool M, 2004, ACM T GRAPHIC, V23, P787, DOI 10.1145/1015706.1015801
   McDonnel B, 2009, IEEE T VIS COMPUT GR, V15, P1105, DOI 10.1109/TVCG.2009.191
   McGuire M., 2006, P I3D
   Méndez-Feliu A, 2009, VISUAL COMPUT, V25, P181, DOI 10.1007/s00371-008-0213-4
   Neubauer S., 2007, URB DAT MAN S
   OGC, 2006, SYMB ENC IMPL SPEC V
   OGC, 2009, 3D SYMB ENC IN PRESS
   OGC, 2010, OPENGIS FILT ENC 2 0
   OGC, 2007, STYL LAYER DESCR PRO
   Pan N, 2013, VISUAL COMPUT, V29, P277, DOI 10.1007/s00371-012-0773-1
   Porter T., 1984, P SIGGRAPH, V18
   Ragan-Kelley J., 2012, P SIGGRAPH
   Rio N.D, 2012, THESIS U TEXAS EL PA
   Ritschel T, 2012, COMPUT GRAPH FORUM, V31, P160, DOI 10.1111/j.1467-8659.2012.02093.x
   Ritter G.X., 2000, Handbook of Computer Vision Algonthms in Image Algebra
   Rost Randi J, 2009, OpenGL shading language
   Rusinkiewicz S, 2006, ACM T GRAPHIC, V25, P1199, DOI 10.1145/1141911.1142015
   Schnabel O, 2009, CARTOGR J, V46, P136, DOI 10.1179/000870409X459851
   Seligmann D, 1993, THESIS COLUMBIA U
   Semmo A., 2012, PHOTOGRAMMETRIE FERN, V4
   Shantzis M.A., 1994, P SIGGRAPH, V28
   Shneiderman B., 1996, P VL
   Sousa T., 2008, GAM DEV C PRES
   SYKORA P, 2007, CARTOGRAPHICA, V42, P209
   Tatarchuk N., 2006, P NPH
   Todo H, 2013, VISUAL COMPUT, V29, P473, DOI 10.1007/s00371-013-0811-7
   Tomlin C.D., 1990, GEOGRAPHIC INFORM SY
   Tory M., 2004, P INFOVIS
   Voigt Martin., 2013, International Journal on Advances in Life Sciences, V5, P27
   Wenzel Carsten, 2007, GAM DEV C
   Zhang C, 2004, SIGNAL PROCESS-IMAGE, V19, P1, DOI 10.1016/j.image.2003.07.001
NR 73
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 445
EP 463
DI 10.1007/s00371-015-1073-3
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DI6YD
UT WOS:000373645200004
DA 2024-07-18
ER

PT J
AU Kayhan, SK
AF Kayhan, Sema Koc
TI Efficient robust filtering technique for blocking artifacts reduction
SO VISUAL COMPUTER
LA English
DT Article
DE Deblocking filter; Blocking artifact; Robust M-estimation function
ID DEBLOCKING; DCT; SUPPRESSION
AB This paper presents a new post-processing algorithm based on a robust statistical model to remove the blocking artifacts observed in block discrete cosine transform (BDCT)-based image compression standards. The novelty is the implementation of a new robust weight function for the block artifact reduction. The blocking artifacts in an image are treated as an outlier random variable. The robust formulation aims at eliminating the artifacts outliers, while preserving the edge structures in the restored image. Extensive simulation results and comparative studies demonstrate that the presented method provides superior results in terms of pixel-wise (PSNR) and perceptual (SSIM) measures.
C1 [Kayhan, Sema Koc] Gaziantep Univ, Elect Elect Engn Dept, TR-27000 Gaziantep, Turkey.
C3 Gaziantep University
RP Kayhan, SK (corresponding author), Gaziantep Univ, Elect Elect Engn Dept, TR-27000 Gaziantep, Turkey.
EM skoc@gantep.edu.tr
RI kayhan, sema/AAE-1157-2020
CR [Anonymous], 2013, P 11 JCT VC M
   Apostolopoulos JG, 1999, IEEE T IMAGE PROCESS, V8, P1125, DOI 10.1109/83.777093
   Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Chen T, 2001, IEEE T CIRC SYST VID, V11, P594, DOI 10.1109/76.920189
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Gao WF, 2002, IEEE T CIRC SYST VID, V12, P1150, DOI 10.1109/TCSVT.2002.806817
   Golestaneh SA, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013018
   Hampel FR, 1986, Robust statistics. The approach based on influence functions
   Jain P, 2015, VISUAL COMPUT, V31, P657, DOI 10.1007/s00371-014-0993-7
   JARSKE T, 1994, IEEE T CONSUM ELECTR, V40, P521, DOI 10.1109/30.320837
   Kim J, 2009, IEEE T CONSUM ELECTR, V55, P933, DOI 10.1109/TCE.2009.5174477
   Liew AWC, 2005, IEEE T CIRC SYST VID, V15, P795, DOI 10.1109/TCSVT.2005.848303
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   Luo Y, 2003, IEEE T IMAGE PROCESS, V12, P838, DOI 10.1109/TIP.2003.814252
   MALVAR HS, 1989, IEEE T ACOUST SPEECH, V37, P553, DOI 10.1109/29.17536
   Malvar HS, 1998, IEEE T SIGNAL PROCES, V46, P1043, DOI 10.1109/78.668555
   MPEG Video Group, 2001, N3908 ISOIECJTC1SC29
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Pourreza-Shahri R, 2014, SIGNAL PROCESS-IMAGE, V29, P1079, DOI 10.1016/j.image.2014.09.008
   Rabie T, 2005, IEEE T IMAGE PROCESS, V14, P1755, DOI 10.1109/TIP.2005.857276
   Rousseeuw P.J., 1987, ROBUST REGRESSION OU
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Singh S, 2007, DIGIT SIGNAL PROCESS, V17, P225, DOI 10.1016/j.dsp.2005.08.003
   Strang G., 1997, WAVELETS FILTER BANK
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhai GT, 2009, J VIS COMMUN IMAGE R, V20, P595, DOI 10.1016/j.jvcir.2009.06.004
   Zhang M., 2009, P IS T SPIE ELECT IM, V7257, P1117
   Zou JJ, 2005, IEEE T CIRC SYST VID, V15, P430, DOI [10.1109/TCSVT.2004.842610, 10.1109/TCVST.2004.842610]
NR 32
TC 2
Z9 2
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 417
EP 427
DI 10.1007/s00371-015-1068-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DI6YD
UT WOS:000373645200002
DA 2024-07-18
ER

PT J
AU Sá, AME
   Mello, VM
   Echavarria, KR
   Covill, D
AF Medeiros e Sa, Asla
   Mello, Vincius Moreira
   Echavarria, Karina Rodriguez
   Covill, Derek
TI Adaptive voids Primal and dual adaptive cellular structures for additive
   manufacturing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Additive manufacturing; Cellular structures; 3D printing; Primal-dual
   cellular structures; Geometric modeling; Cell complexes
ID COMPUTER-AIDED-DESIGN; FINITE-ELEMENT; ARCHITECTURE; OPTIMIZATION;
   SCAFFOLDS; REFINEMENT
AB Additive manufacturing processes have the potential to change the way we produce everyday objects. Design for additive manufacturing focuses on dealing with the characteristics and constraints of a given additive process. These constraints include both geometric and material constraints which have a major impact on the feasibility, quality and cost of the printed object. When designing for additive manufacturing, one of the desirable objectives is to reduce the amount of material while maximising the strength of the printed part. For this, the inclusion of cellular structures in the design has been an efficient way to address these constraints while supporting other application-specific requirements. These structures, which are commonly inspired by shapes found in nature, provide high strength while maintaining a low mass. In this paper we propose the adaptive voids algorithm, an automatic approach to generate, given a volume boundary, a parameterised adaptive infill primal and/or dual cellular structure for additive manufacturing. The produced output can potentially be applied in various applications, including design and engineering, architecture, clothing and protective equipment, furniture and biomedical applications.
C1 [Medeiros e Sa, Asla] Escola Matemat Aplicada FGV EMAp, Rio De Janeiro, Brazil.
   [Mello, Vincius Moreira] Univ Fed Bahia, Dept Matemat, Salvador, BA, Brazil.
   [Echavarria, Karina Rodriguez] Univ Brighton, Cultural Informat Res Grp, Brighton, E Sussex, England.
   [Covill, Derek] Univ Brighton, Dept Comp Engn & Math, Brighton, E Sussex, England.
C3 Getulio Vargas Foundation; Universidade Federal da Bahia; University of
   Brighton; University of Brighton
RP Sá, AME (corresponding author), Escola Matemat Aplicada FGV EMAp, Rio De Janeiro, Brazil.
EM asla.sa@fgv.br; vinicius.mello@ufba.br; K.Rodriguez@brighton.ac.uk;
   D.Covill@brighton.ac.uk
RI Echavarria, Karina Rodriguez/AAF-5093-2020; Medeiros e Sa,
   Asla/P-5129-2014
OI Echavarria, Karina Rodriguez/0000-0002-8679-1602; Covill,
   Derek/0000-0001-5891-9624; Medeiros e Sa, Asla/0000-0002-3705-9095
FU Santander-University of Brighton
FX The authors would like to acknowledge the Santander-University of
   Brighton Travel Grants for Staff which supported a research visit
   between the authors. We also acknowledge Trevor Taylor for his efforts
   and help on printing the proposed structures.
CR Alemanno G., 2014, EUR WORKSH GRAPH CUL
   [Anonymous], 2012, 13 INT S VIRT REAL A
   Ashby MF, 2006, PHILOS T R SOC A, V364, P15, DOI 10.1098/rsta.2005.1678
   Atalay F.B., 2004, PROC INT MESHING ROU, P15
   Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Brackett DJ, 2014, COMPUT STRUCT, V138, P102, DOI 10.1016/j.compstruc.2014.03.004
   Brennan-Craddock J, 2012, J PHYS CONF SER, V382, DOI 10.1088/1742-6596/382/1/012042
   Calì J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366149
   Challis VJ, 2014, MATER DESIGN, V63, P783, DOI 10.1016/j.matdes.2014.05.064
   Chen YH, 2011, ADV MATER RES-SWITZ, V213, P628, DOI 10.4028/www.scientific.net/AMR.213.628
   Chiu WK, 2006, RAPID PROTOTYPING J, V12, P214, DOI 10.1108/13552540610682723
   Cook D., 2012, P 23 ANN SOL FREEF F
   Deuss M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661266
   Dumas J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601153
   Fryazinov O, 2013, COMPUT AIDED DESIGN, V45, P26, DOI 10.1016/j.cad.2011.09.007
   Gabbrielli R, 2008, KEY ENG MAT, V361-363, P903, DOI 10.4028/www.scientific.net/KEM.361-363.903
   Harley B.A., 2010, CELLULAR MAT NATURE
   Hildebrand K, 2013, COMPUT GRAPH-UK, V37, P669, DOI 10.1016/j.cag.2013.05.011
   Khoda AKM, 2013, COMPUT AIDED DESIGN, V45, P1507, DOI 10.1016/j.cad.2013.07.003
   Kumar GS, 2009, VIRTUAL PHYS PROTOTY, V4, P91, DOI 10.1080/17452750802688215
   LEWINER T., 2004, VISION MODELING VISU, P299
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Luo LJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366148
   Maire E, 2003, COMPOS SCI TECHNOL, V63, P2431, DOI 10.1016/S0266-3538(03)00276-8
   MAUBACH JM, 1995, SIAM J SCI COMPUT, V16, P210, DOI 10.1137/0916014
   Sá AME, 2014, VISUAL COMPUT, V30, P1321, DOI 10.1007/s00371-013-0883-4
   Melchels FPW, 2010, ACTA BIOMATER, V6, P4208, DOI 10.1016/j.actbio.2010.06.012
   Mello V, 2003, VISUALIZATION AND MATHEMATICS III, P337
   Moroni L, 2006, BIOMATERIALS, V27, P974, DOI 10.1016/j.biomaterials.2005.07.023
   Museth K., 2013, ACM SIGGRAPH 2013 CO, p19:1, DOI 10.1145/2504435.2504454
   Olivares AL, 2009, BIOMATERIALS, V30, P6142, DOI 10.1016/j.biomaterials.2009.07.041
   Panozzo D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461958
   Pasko A, 2011, GRAPH MODELS, V73, P165, DOI 10.1016/j.gmod.2011.03.001
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Ramirez DA, 2011, MAT SCI ENG A-STRUCT, V528, P5379, DOI 10.1016/j.msea.2011.03.053
   Reinhart G, 2013, PROC CIRP, V12, P175, DOI 10.1016/j.procir.2013.09.031
   RIVARA MC, 1987, INT J NUMER METH ENG, V24, P1343, DOI 10.1002/nme.1620240710
   Schroeder C, 2005, COMPUT AIDED DESIGN, V37, P339, DOI 10.1016/j.cad.2004.03.008
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Umetani N., 2013, SIGGRAPH Asia Technical Briefs, P5, DOI DOI 10.1145/2542355.2542361
   Vanek, 2014, COMPUT GRAPH FORUM
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P322, DOI 10.1111/cgf.12353
   Villalpando L, 2014, PROC CIRP, V17, P800, DOI 10.1016/j.procir.2014.02.050
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Wettergreen MA, 2005, COMPUT AIDED DESIGN, V37, P1141, DOI 10.1016/j.cad.2005.02.005
   Wieding J, 2014, J MECH BEHAV BIOMED, V37, P56, DOI 10.1016/j.jmbbm.2014.05.002
   Yoo DJ, 2011, BIOMATERIALS, V32, P7741, DOI 10.1016/j.biomaterials.2011.07.019
   Zhang Z, 2013, MAT SCI ENG C-MATER, V33, P4055, DOI 10.1016/j.msec.2013.05.050
   Zhou Q., 2013, ACM T GRAPHIC, V32, P137
NR 49
TC 30
Z9 33
U1 1
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 799
EP 808
DI 10.1007/s00371-015-1109-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500006
DA 2024-07-18
ER

PT J
AU Hunyadi, L
   Vajk, I
AF Hunyadi, Levente
   Vajk, Istvan
TI Constrained quadratic errors-in-variables fitting
SO VISUAL COMPUTER
LA English
DT Article
DE Parameter estimation; Direct methods; Fitting with constraints;
   Eigenvalue problem
AB We propose an estimation method to fit conics and quadrics to data in the context of errors-in-variables where the fit is subject to constraints. The proposed algorithm is based on algebraic distance minimization and consists of solving a few generalized eigenvalue (or singular value) problems and is not iterative. Nonetheless, the algorithm produces accurate estimates, close to those obtained with maximum likelihood, while the constraints are also guaranteed to be satisfied. Important special cases, fitting ellipses, hyperbolas, parabolas, and ellipsoids to noisy data are discussed.
C1 [Hunyadi, Levente] Budapest Univ Technol & Econ, Dept Automat & Appl Informat, H-1117 Budapest, Hungary.
   [Vajk, Istvan] Budapest Univ Technol & Econ, Dept Automat & Appl Informat, MTA BME Control Res Grp, H-1117 Budapest, Hungary.
C3 Budapest University of Technology & Economics; Budapest University of
   Technology & Economics
RP Hunyadi, L (corresponding author), Budapest Univ Technol & Econ, Dept Automat & Appl Informat, Magyar Tudosok Krt 2 Bldg Q, H-1117 Budapest, Hungary.
EM hunyadi@aut.bme.hu; vajk@aut.bme.hu
RI Vajk, István/H-2249-2012
OI Vajk, Istvan/0000-0002-2818-9162
FU Hungarian Academy of Sciences for control research; European Union;
   European Social Fund through project FuturICT.hu
   [TAMOP-4.2.2.C-11/1/KONV-2012-0013]; Hungarian Government via the
   National Development Agency - Research and Technology Innovation Fund
   [KMR-12-1-2012-0441]
FX We are grateful to the anonymous reviewers for their helpful comments.
   This work was supported by the fund of the Hungarian Academy of Sciences
   for control research, and partially by the European Union and the
   European Social Fund through project FuturICT.hu organized by VIKING
   Zrt. Balatonfured (grant no. TAMOP-4.2.2.C-11/1/KONV-2012-0013), and by
   the Hungarian Government via the National Development Agency financed by
   the Research and Technology Innovation Fund (grant no.
   KMR-12-1-2012-0441).
CR Al-Sharadqah A, 2012, COMPUT STAT DATA AN, V56, P2771, DOI 10.1016/j.csda.2012.02.028
   Chernov N, 2004, COMPUT STAT DATA AN, V47, P713, DOI 10.1016/j.csda.2003.11.008
   Chernov N., 2011, Computer Vision, P285
   Chojnacki W, 2005, J MATH IMAGING VIS, V23, P175, DOI 10.1007/s10851-005-6465-y
   Chojnacki W, 2003, IEEE T PATTERN ANAL, V25, P1172, DOI 10.1109/TPAMI.2003.1227992
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Halir R, 1998, WSCG '98, VOL 1, P125
   Harker M, 2008, IMAGE VISION COMPUT, V26, P372, DOI 10.1016/j.imavis.2006.12.006
   Hunyadi L., 2011, P 18 WORLD C INT FED, P13104
   Hunyadi L, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413500043
   Kanatani K, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P2, DOI 10.1109/3DIM.2005.49
   Kanatani K, 2008, INT J COMPUT VISION, V80, P167, DOI 10.1007/s11263-007-0098-0
   Kanatani K, 2011, COMPUT STAT DATA AN, V55, P2197, DOI 10.1016/j.csda.2010.12.012
   Kukush A, 2004, COMPUT STAT DATA AN, V47, P123, DOI 10.1016/j.csda.2003.10.022
   Li QD, 2004, GEOMETRIC MODELING AND PROCESSING 2004, PROCEEDINGS, P335
   Markovsky I, 2004, NUMER MATH, V98, P177, DOI 10.1007/s00211-004-0526-9
   Matei B, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.854727
   O'Leary P, 2004, J ELECTRON IMAGING, V13, P492, DOI 10.1117/1.1758951
   Schöne R, 2012, J APPL MATH, DOI 10.1155/2012/312985
   TAUBIN G, 1991, IEEE T PATTERN ANAL, V13, P1115, DOI 10.1109/34.103273
   Tisseur F, 2001, SIAM REV, V43, P235, DOI 10.1137/S0036144500381988
   Vajk I, 2003, AUTOMATICA, V39, P2099, DOI 10.1016/j.automatica.2003.07.001
NR 22
TC 13
Z9 15
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1347
EP 1358
DI 10.1007/s00371-013-0885-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400004
DA 2024-07-18
ER

PT J
AU Sipiran, I
   Meruane, R
   Bustos, B
   Schreck, T
   Li, B
   Lu, YJ
   Johan, H
AF Sipiran, Ivan
   Meruane, Rafael
   Bustos, Benjamin
   Schreck, Tobias
   Li, Bo
   Lu, Yijuan
   Johan, Henry
TI A benchmark of simulated range images for partial shape retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Partial shape retrieval; Performance evaluation; Benchmarking
AB In this paper, we address the evaluation of algorithms for partial shape retrieval using a large-scale simulated benchmark of partial views which are used as queries. Since the scanning of real objects is a time-consuming task, we create a simulation that generates a set of views from a target model and at different levels of complexity (amount of missing data). In total, our benchmark contains 7,200 partial views. Furthermore, we propose the use of weighted effectiveness measures based on the complexity of a query. With these characteristics, we aim at jointly evaluating the effectiveness, efficiency and robustness of existing algorithms. As a result of our evaluation, we found that a combination of methods provides the best effectiveness, mainly due to the complementary information that they deliver. The obtained results open new questions regarding the difficulty of the partial shape retrieval problem. As a consequence, potential future directions are also identified.
C1 [Sipiran, Ivan; Schreck, Tobias] Univ Konstanz, Dept Comp & Informat Sci, Constance, Germany.
   [Meruane, Rafael; Bustos, Benjamin] Univ Chile, Dept Comp Sci, Santiago, Chile.
   [Li, Bo; Lu, Yijuan] Texas State Univ, Dept Comp Sci, San Marcos, TX USA.
   [Johan, Henry] Fraunhofer IDM NTU, Singapore, Singapore.
   [Bustos, Benjamin] PRISMA Res Grp, Santiago, Chile.
   [Li, Bo] Texas State Univ, San Marcos, TX USA.
   [Li, Bo] Natl Inst Stand & Technol, Gaithersburg, MD USA.
   [Johan, Henry] Univ Tokyo, Dept Complex Sci & Engn, Tokyo 1138654, Japan.
   [Johan, Henry] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 University of Konstanz; Universidad de Chile; Texas State University
   System; Texas State University San Marcos; Nanyang Technological
   University; Texas State University System; Texas State University San
   Marcos; National Institute of Standards & Technology (NIST) - USA;
   University of Tokyo; Nanyang Technological University
RP Sipiran, I (corresponding author), Univ Konstanz, Dept Comp & Informat Sci, Constance, Germany.
EM ivan.sipiran@gmail.com; rmeruane@dcc.uchile.cl; bebustos@dcc.uchile.cl;
   tobias.schreck@uni-konstanz.de; b_l58@txstate.edu; lu@txstate.edu;
   hjohan@fraunhofer.sg
RI Sipiran, Ivan/AAL-7603-2020; Sipiran, Ivan/GRR-8629-2022; LU,
   YIJUAN/GNM-8769-2022; Bustos, Benjamin/G-1170-2010
OI LU, YIJUAN/0000-0002-9855-8365; Bustos, Benjamin/0000-0002-3955-361X;
   Schreck, Tobias/0000-0003-0778-8665; Sipiran, Ivan/0000-0002-8766-3581
FU EC FP7 STREP Project PRESIOUS [600533]; FONDECYT (Chile) [1140783]; Army
   Research Office [W911NF-12-1-0057]; Texas State University Research
   Enhancement Program (REP); NSF [CRI 1305302]; Fraunhofer IDM@NTU -
   National Research Foundation (NRF)
FX The work of Ivan Sipiran and Tobias Schreck was supported by EC FP7
   STREP Project PRESIOUS, Grant No. 600533. Benjamin Bustos has been
   partially funded by FONDECYT (Chile) Project 1140783. This work of Bo Li
   and Yijuan Lu has been supported by the Army Research Office grant
   W911NF-12-1-0057, Texas State University Research Enhancement Program
   (REP), and NSF CRI 1305302 to Yijuan Lu. Henry Johan is supported by
   Fraunhofer IDM@NTU, which is funded by the National Research Foundation
   (NRF) and managed through the multi-agency Interactive & Digital Media
   Programme Office (IDMPO) hosted by the Media Development Authority of
   Singapore (MDA).
CR [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], 2009, IEEE INT C ROB AUT
   [Anonymous], INT C ROB AUT SHANGH
   [Anonymous], 2013, P EUR WORKSH 3D OBJ, DOI DOI 10.2312/3DOR/3DOR13/049-056
   [Anonymous], 3DOR
   [Anonymous], 1997, THESIS CARNEGIE MELL
   Beecks Christian, 2009, P 17 ACM INT C MULT, P697, DOI DOI 10.1145/1631272.1631391
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Browatzki B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1189, DOI 10.1109/ICCVW.2011.6130385
   Bustos B., 2012, 3D IMAGING ANAL APPL, P265
   Dutagaci Helin., 2009, Proceedings of Eurographics 3DOR, P69, DOI DOI 10.2312/3D0R/3D0R09/069-076
   FROME A, 2004, P EUR C COMP VIS ECC
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Leow WK, 2004, COMPUT VIS IMAGE UND, V94, P67, DOI 10.1016/j.cviu.2003.10.010
   LI B., 2012, EurographicsWorkshop on 3D Object Retrieval, P109
   Li B, 2013, MULTIMED TOOLS APPL, V65, P363, DOI 10.1007/s11042-012-1009-0
   Li B, 2010, LECT NOTES COMPUT SC, V5916, P185
   Liu Y, 2006, P IEEE C COMP VIS PA
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Rusu RB, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P7, DOI 10.1109/IROS.2009.5354763
   Sipiran I., 2013, 3DOR, P81
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sipiran I, 2013, COMPUT GRAPH-UK, V37, P460, DOI 10.1016/j.cag.2013.04.002
   Takahashi S., 2005, IEEE VISUALIZATION, V0, P63
   Veltkamp R., 2007, TECHNICAL REPORT
   Vranic D., 2005, P IEEE INT C MULT EX
NR 27
TC 5
Z9 5
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1293
EP 1308
DI 10.1007/s00371-014-0937-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300008
DA 2024-07-18
ER

PT J
AU Mu, TJ
   Wang, JH
   Du, SP
   Hu, SM
AF Mu, Tai-Jiang
   Wang, Ju-Hong
   Du, Song-Pei
   Hu, Shi-Min
TI Stereoscopic image completion and depth recovery
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Stereoscopic; Completion; Depth inconsistency; Distance metric
ID DISPARITY; PROPAGATION
AB In this paper, we have proposed a novel patch-based method for automatic completion of stereoscopic images and the corresponding depth/disparity maps simultaneously. The missing depths are estimated in local feature space and a patch distance metric is designed to take the appearance, depth gradients and depth inconsistency into account. To ensure the proper stereopsis, we first search for the proper stereoscopic patch in both left and right images according to the distance metric, and then iteratively refine the images. Our method is capable of dealing with general scenes including both frontal-parallel and non-frontal-parallel objects. Experimental results show that our method is superior to previous ones with better stereoscopically consistent content and more plausible completion.
C1 [Mu, Tai-Jiang; Du, Song-Pei; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing 100084, Peoples R China.
   [Wang, Ju-Hong] Tsinghua Tencent Joint Lab Internet Innovat Techn, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tencent
RP Mu, TJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing 100084, Peoples R China.
EM mmmutj@gmail.com
RI Yu, ZH/KBC-6889-2024; Hu, Shi-Min/AAW-1952-2020; wang,
   jing/GRS-7509-2022; Mu, Tai-Jiang/JWO-1381-2024
OI Mu, Tai-Jiang/0000-0002-9197-346X
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61272226/61120106007]; National High Technology
   Research and Development Program of China [2013AA013903]; Tsinghua
   University Initiative Scientific Research Program
FX We would like to thank the anonymous reviewers for their helpful
   comments. This work was supported by the National Basic Research Project
   of China (Project Number 2011CB302205), the Natural Science Foundation
   of China (Project Number 61272226/61120106007), the National High
   Technology Research and Development Program of China (Project Number
   2013AA013903) and Tsinghua University Initiative Scientific Research
   Program.
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2011, 2011 IEEE INT C MULT
   [Anonymous], 2014, P 2014 14 INT WORKSH, DOI DOI 10.1109/ISGT.2014.6816469
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Basha T, 2011, IEEE I CONF COMP VIS, P1816, DOI 10.1109/ICCV.2011.6126448
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Celikcan U, 2013, VISUAL COMPUT, V29, P685, DOI 10.1007/s00371-013-0804-6
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Didyk P., 2013, ACM T GRAPHICS TOG, V32
   Du SP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508387
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   He KM, 2012, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2012.6247665
   Hervieu A., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4101, DOI 10.1109/ICPR.2010.997
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Kellnhofer P, 2013, COMPUT GRAPH FORUM, V32, P143, DOI 10.1111/cgf.12160
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Lee S, 2014, VISUAL COMPUT, V30, P455, DOI 10.1007/s00371-013-0868-3
   Liao M, 2012, IEEE T VIS COMPUT GR, V18, P1079, DOI 10.1109/TVCG.2011.114
   Liu Hong., 2011, Wind speed forecasting for wind energy applications, P1, DOI 10.1109/GeoInformatics.2011.5981193
   Lo WY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866173
   Luo SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366201
   Morse B, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P555, DOI 10.1109/3DIMPVT.2012.59
   Niu YZ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366202
   Pollock B, 2012, IEEE T VIS COMPUT GR, V18, P581, DOI 10.1109/TVCG.2012.58
   Raimbault F, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.1.011005
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Smith BM, 2009, PROC CVPR IEEE, P485, DOI 10.1109/CVPRW.2009.5206793
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Sun J, 2005, PROC CVPR IEEE, P399
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Wang L., 2008, IEEE CVPR, P1
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Yan T, 2013, INT J COMPUT VISION, V102, P293, DOI 10.1007/s11263-012-0593-9
   Zhang GF, 2007, IEEE T VIS COMPUT GR, V13, P686, DOI 10.1109/TVCG.2007.1032
NR 42
TC 20
Z9 20
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 833
EP 843
DI 10.1007/s00371-014-0961-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700025
DA 2024-07-18
ER

PT J
AU El Akkad, N
   Merras, M
   Saaidi, A
   Satori, K
AF El Akkad, Nabil
   Merras, Mostafa
   Saaidi, Abderrahim
   Satori, Khalid
TI Camera self-calibration with varying intrinsic parameters by an unknown
   three-dimensional scene
SO VISUAL COMPUTER
LA English
DT Article
DE Control points; Fundamental matrix; Self-calibration; Varying intrinsic
   parameters; Nonlinear optimization
AB This work proposes a method of camera self-calibration having varying intrinsic parameters from a sequence of images of an unknown 3D object. The projection of two points of the 3D scene in the image planes is used with fundamental matrices to determine the projection matrices. The present approach is based on the formulation of a nonlinear cost function from the determination of a relationship between two points of the scene and their projections in the image planes. The resolution of this function enables us to estimate the intrinsic parameters of different cameras. The strong point of the present approach is clearly seen in the minimization of the three constraints of a self-calibration system (a pair of images, 3D scene, any camera): The use of a single pair of images provides fewer equations, which minimizes the execution time of the program, the use of a 3D scene reduces the planarity constraints, and the use of any camera eliminates the constraints of cameras having constant parameters. The experiment results on synthetic and real data are presented to demonstrate the performance of the present approach in terms of accuracy, simplicity, stability, and convergence.
C1 [El Akkad, Nabil; Merras, Mostafa; Satori, Khalid] Sidi Mohamed Ben Abdellah Univ, Fac Sci, Dept Math & Comp Sci, LIIAN, Atlas, Fez, Morocco.
   [Saaidi, Abderrahim] Sidi Mohamed Ben Abdellah Univ, Polydisciplinary Fac Taza, Dept Math Phys & Comp Sci, LIMAO, Taza, Morocco.
C3 Sidi Mohamed Ben Abdellah University of Fez; Sidi Mohamed Ben Abdellah
   University of Fez
RP Saaidi, A (corresponding author), Sidi Mohamed Ben Abdellah Univ, Polydisciplinary Fac Taza, Dept Math Phys & Comp Sci, LIMAO, BP 1223, Taza, Morocco.
EM nabil_abdo80@yahoo.fr; saaidi.abde@yahoo.fr
RI satori, khalid/GSE-3077-2022; Saaidi, Abderrahim/R-1916-2019;
   Abderrahmani, Abdellatif EL/N-6052-2019; Merras, Mostafa/AAJ-4405-2020;
   AKKAD, Nabil EL/AAL-4049-2020
OI Merras, Mostafa/0000-0002-3020-726X; AKKAD, Nabil
   EL/0000-0003-0277-8003; SATORI, khalid/0000-0001-6055-4169; Saaidi,
   Abderrahim/0000-0003-1708-0468
CR Baataoui A., 2012, INT J COMPUT APPL, P29
   Bouda B, 2006, ICGST GVIP, P21
   Cao XC, 2006, COMPUT VIS IMAGE UND, V102, P227, DOI 10.1016/j.cviu.2006.01.004
   Chambon S, 2011, PATTERN RECOGN, V44, P2063, DOI 10.1016/j.patcog.2011.02.001
   El Akkad N, 2012, INT CONF MULTIMED, P161, DOI 10.1109/ICMCS.2012.6320196
   Gao YY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P537
   Gurdjos P, 2003, PROC CVPR IEEE, P491
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hemayed EE, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P351, DOI 10.1109/AVSS.2003.1217942
   Jiang ZT, 2011, COMM COM INF SC, V86, P452
   Jiang ZT, 2012, J COMPUT, V7, P774, DOI 10.4304/jcp.7.3.774-778
   Liu PJ, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P262, DOI 10.1109/CGI.2003.1214479
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manolis IAL, 2000, 3911 INRIA
   Mattoccia S, 2008, IEEE T IMAGE PROCESS, V17, P528, DOI 10.1109/TIP.2008.919362
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Mori M., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P196, DOI 10.1109/ISM.2010.36
   Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705
   Saaidi A., 2008, WSEAS Transactions on Computers Research, V3, P295
   Saaidi A., 2009, ICGST GVIP, P41
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Sturm P, 2002, IMAGE VISION COMPUT, V20, P415, DOI 10.1016/S0262-8856(02)00012-4
   Sturm P, 2000, IEEE T PATTERN ANAL, V22, P1199, DOI 10.1109/34.879804
   Torr PHS, 1997, INT J COMPUT VISION, V24, P271, DOI 10.1023/A:1007927408552
   Triggs B., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P89, DOI 10.1007/BFb0055661
   Wang GH, 2009, IEEE T CIRC SYST VID, V19, P1793, DOI 10.1109/TCSVT.2009.2031380
   Yanliang Shang, 2012, Information Technology Journal, V11, P376, DOI 10.3923/itj.2012.376.379
   Yue Zhao, 2012, Information Technology Journal, V11, P926, DOI 10.3923/itj.2012.926.930
   Zhang W., 2005, GVIP 05 C
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao Y., 2012, Information Technology Journal, V11, P276, DOI 10.3923/itj.2012.276.282
NR 31
TC 28
Z9 32
U1 3
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 519
EP 530
DI 10.1007/s00371-013-0877-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100005
DA 2024-07-18
ER

PT J
AU Chang, B
   Woo, S
   Ihm, I
AF Chang, Byungjoon
   Woo, Sangkyu
   Ihm, Insung
TI GPU-based parallel construction of compact visual hull meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Visual hull; Volumetric approach; Compact mesh; GPU algorithm; CUDA
   implementation; Marching cubes algorithm
AB Building a visual hull model from multiple two-dimensional images provides an effective way of understanding the three-dimensional geometries inherent in the images. In this paper, we present a GPU accelerated algorithm for volumetric visual hull reconstruction that aims to harness the full compute power of the many-core processor. From a set of binary silhouette images with respective camera parameters, our parallel algorithm directly outputs the triangular mesh of the resulting visual hull in the indexed face set format for a compact mesh representation. Unlike previous approaches, the presented method extracts a smooth silhouette contour on the fly from each binary image, which markedly reduces the bumpy artifacts on the visual hull surface due to a simple binary in/out classification. In addition, it applies several optimization techniques that allow an efficient CUDA implementation. We also demonstrate that the compact mesh construction scheme can easily be modified for also producing a time- and space-efficient GPU implementation of the marching cubes algorithm.
C1 [Chang, Byungjoon; Woo, Sangkyu; Ihm, Insung] Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Sogang University
RP Ihm, I (corresponding author), Sogang Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM jerrun@sogagn.ac.kr; coldnight.w@gmail.com; ihm@sogang.ac.kr
FU National Research Foundation of Korea (NRF); Ministry of Education,
   Science, and Technology [2012R1A1A2008958]; MCST/MKE/KEIT [KI001798]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education, Science, and Technology (Grant no. 2012R1A1A2008958), and
   by the strategic technology development program of MCST/MKE/KEIT
   (Development of Full 3D Reconstruction Technology for Broadcasting
   Communication Fusion (KI001798)).
CR [Anonymous], THESIS STANFORD U
   [Anonymous], 1893 UN RECH INRIA
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Erol A, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P234
   Franco J.S., 2003, P 14 BRIT MACHINE VI, P329, DOI [10.5244/C.17.32, DOI 10.5244/C.17.32]
   Harris M., 2007, GPU GEMS, V3, P851
   Ladikos Alexander, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563098
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Liang C, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P597
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Matusik W, 2001, SPRING EUROGRAP, P115
   NVIDIA, 2010, NVIDIA CUDA C PROGR
   Satish N., 2009, PROC 2009 IEEE INT S, P1
   Slabaugh G, 2001, SPRING EUROGRAP, P81
   Waizenegger W, 2009, IEEE IMAGE PROC, P4301, DOI 10.1109/ICIP.2009.5413661
   Zhang SJ, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P168, DOI 10.1109/VECIMS.2009.5068887
NR 16
TC 1
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 201
EP 211
DI 10.1007/s00371-013-0796-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900006
DA 2024-07-18
ER

PT J
AU Sfikas, K
   Theoharis, T
   Pratikakis, I
AF Sfikas, Konstantinos
   Theoharis, Theoharis
   Pratikakis, Ioannis
TI 3D object retrieval via range image queries in a bag-of-visual-words
   context
SO VISUAL COMPUTER
LA English
DT Article
DE 3D object retrieval; Range scans; Panoramic views; SIFT;
   Bag-of-visual-words model
ID RECOGNITION
AB 3D object retrieval based on range image queries that represent partial views of real 3D objects is presented. The complete 3D models of the database are described by a set of panoramic views, and a Bag-of-Visual-Words model is built using SIFT features extracted from them. To address the problem of partial matching, we suggest a histogram computation scheme, on the panoramic views, that represents local information by taking into account spatial context. Furthermore, a number of optimization techniques are applied throughout the process for enhancing the retrieval performance. Its superior performance is shown by evaluating it against state-of-the-art methods on standard datasets.
C1 [Sfikas, Konstantinos; Theoharis, Theoharis] Univ Athens, Comp Graph Lab, Dept Informat & Telecommun, Athens, Greece.
   [Theoharis, Theoharis] NTNU, IDI, Trondheim, Norway.
   [Pratikakis, Ioannis] Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
C3 National & Kapodistrian University of Athens; Norwegian University of
   Science & Technology (NTNU); Democritus University of Thrace
RP Sfikas, K (corresponding author), Univ Athens, Comp Graph Lab, Dept Informat & Telecommun, Athens, Greece.
EM ksfikas@di.uoa.gr; theotheo@di.uoa.gr; ipratika@ee.duth.gr
RI PRATIKAKIS, IOANNIS/AAD-3387-2019; Theoharis, Theoharis/AAN-2555-2020
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Sfikas,
   Konstantinos/0000-0002-9173-4557
FU European Union (European Social Fund, ESF); Greek national funds [MIS
   379516]
FX This research has been cofinanced by the European Union (European Social
   Fund, ESF) and Greek national funds through the Operational Program
   "Education and Lifelong Learning" of the National Strategic Reference
   Framework (NSRF), Research Funding Program THALES-3DOR (MIS 379516).
   Investing in knowledge society through the European Social Fund.
CR Adán A, 2011, PATTERN RECOGN LETT, V32, P1337, DOI 10.1016/j.patrec.2011.03.016
   Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], 2007, P INT C IMAGE PROCES
   [Anonymous], SMI
   [Anonymous], 2007, GRAPHICS VISUALIZATI
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Cheng E, 2010, I S BIOMED IMAGING, P197, DOI 10.1109/ISBI.2010.5490381
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Daras P, 2009, INT WORK CONTENT MUL, P115, DOI 10.1109/CBMI.2009.15
   DOUGHERTY ER, 1992, INTRO MORPHOLOGICAL
   Dutagaci Helin., 2009, Proceedings of Eurographics 3DOR, P69, DOI DOI 10.2312/3D0R/3D0R09/069-076
   Fang R, 2008, LECT NOTES COMPUT SC, V5358, P381, DOI 10.1007/978-3-540-89639-5_37
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Furuya T., 2009, P ACM INT C IM VID R, P1
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Giorgi Daniela., 2007, SHAPE RETRIEVAL CONT
   Hetzel G, 2001, PROC CVPR IEEE, P394
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mohamed W, 2012, VISUAL COMPUT, V28, P305, DOI 10.1007/s00371-011-0640-5
   Ohbuchi Ryutarou, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P63, DOI 10.1109/ICCVW.2009.5457716
   Ohbuchi R., 2003, P 5 ACM SIGMM INT WO, P39, DOI DOI 10.1145/973264.973272
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Ruiz-Correa S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1126
   Sfikas K., 2012, EUR WORKSH 3D OBJ RE, P9
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Shih JL, 2007, PATTERN RECOGN, V40, P283, DOI 10.1016/j.patcog.2006.04.034
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Stavropoulos G, 2010, IEEE T MULTIMEDIA, V12, P692, DOI 10.1109/TMM.2010.2053023
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Wahl E, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P474, DOI 10.1109/IM.2003.1240284
NR 43
TC 9
Z9 9
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1351
EP 1361
DI 10.1007/s00371-013-0876-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200010
DA 2024-07-18
ER

PT J
AU Musialski, P
   Cui, M
   Ye, JP
   Razdan, A
   Wonka, P
AF Musialski, Przemyslaw
   Cui, Ming
   Ye, Jieping
   Razdan, Anshuman
   Wonka, Peter
TI A framework for interactive image color editing
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Computational photography; Color manipulation;
   Interactive image editing; Recoloring
ID SEGMENTATION
AB We propose a new method for interactive image color replacement that creates smooth and naturally looking results with minimal user interaction. Our system expects as input a source image and rawly scribbled target color values and generates high quality results in interactive rates. To achieve this goal we introduce an algorithm that preserves pairwise distances of the signatures in the original image and simultaneously maps the color to the user defined target values. We propose efficient sub-sampling in order to reduce the computational load and adapt semi-supervised locally linear embedding to optimize the constraints in one objective function. We show the application of the algorithm on typical photographs and compare the results to other color replacement methods.
C1 [Musialski, Przemyslaw] Vienna Univ Technol, A-1040 Vienna, Austria.
   [Musialski, Przemyslaw; Cui, Ming; Ye, Jieping; Razdan, Anshuman; Wonka, Peter] Arizona State Univ, Tempe, AZ USA.
   [Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
C3 Technische Universitat Wien; Arizona State University; Arizona State
   University-Tempe; King Abdullah University of Science & Technology
RP Musialski, P (corresponding author), Vienna Univ Technol, A-1040 Vienna, Austria.
EM pm@cg.tuwien.ac.at; ming.cui@asu.edu; jieping.ye@asu.edu;
   arazdan@asu.edu; pwonka@gmail.com
RI Musialski, Przemyslaw/O-2617-2013
OI Musialski, Przemyslaw/0000-0001-6429-8190
FU Science Foundation Arizona; US Navy; NSF
FX This research was financially supported by Science Foundation Arizona,
   US Navy, and NSF. We would like tom thank Tom Ang (Fig. 14) and Norman
   Koren (Figs. 1, 5) for the permission to use their outstanding
   photographs.
CR ADOBE Inc, 2012, PHOT
   An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2010, ANN: a library for approximate nearest neighbor searching
   [Anonymous], 2003, FIELD GUIDE DIGITAL
   Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938
   Chang Y., 2005, ACM Trans. Appl. Perception, V2, P322
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li M.-T., 2010, 2010 2 INT C COMP EN, pV7
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu XP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409105
   Luan Q, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P465, DOI 10.1109/PG.2007.50
   Nadler B., 2005, Adv Neural Inf Process Syst, P955, DOI DOI 10.48550/ARXIV.MATH/0506090
   Nagel D., 2004, COL REPL PHOT CS
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1243980.1243983, 10.1145/1276377.1276444, 10.1145/1239451.1239505]
   Pitie A., 2007, 4 EUR C VIS MED PROD, DOI DOI 10.1049/CP:20070055
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667
   Shapira L, 2009, COMPUT GRAPH FORUM, V28, P629, DOI 10.1111/j.1467-8659.2009.01403.x
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Tai YW, 2005, PROC CVPR IEEE, P747
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wen CL, 2008, COMPUT GRAPH FORUM, V27, P1765, DOI 10.1111/j.1467-8659.2008.01321.x
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yang CK, 2008, IEEE COMPUT GRAPH, V28, P52, DOI 10.1109/MCG.2008.24
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
NR 40
TC 17
Z9 19
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1173
EP 1186
DI 10.1007/s00371-012-0761-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Miranda, FM
   Celes, W
AF Miranda, Fabio Markus
   Celes, Waldemar
TI Volume rendering of unstructured hexahedral meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Volume rendering; Hexahedral mesh; Unstructured mesh; Ray integral
AB Important engineering applications use unstructured hexahedral meshes for numerical simulations. Hexahedral cells, when compared to tetrahedral ones, tend to be more numerically stable and to require less mesh refinement. However, volume visualization of unstructured hexahedral meshes is challenging due to the trilinear variation of scalar fields inside the cells. The conventional solution consists in subdividing each hexahedral cell into five or six tetrahedra, approximating a trilinear variation by a nonadaptive piecewise linear function. This results in inaccurate images and increases the memory consumption. In this paper, we present an accurate ray-casting volume rendering algorithm for unstructured hexahedral meshes. In order to capture the trilinear variation along the ray, we propose the use of quadrature integration. A set of computational experiments demonstrates that our proposal produces accurate results, with reduced memory footprint. The entire algorithm is implemented on graphics cards, ensuring competitive performance. We also propose a faster approach that, as the tetrahedron subdivision scheme, also approximates the trilinear variation by a piecewise linear function, but in an adaptive and more accurate way, considering the points of minimum and maximum of the scalar function along the ray.
C1 [Miranda, Fabio Markus; Celes, Waldemar] Pontifical Catholic Univ Rio de Janeiro, Tecgraf PUC Rio Comp Sci Dept, Rio De Janeiro, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro
RP Miranda, FM (corresponding author), Pontifical Catholic Univ Rio de Janeiro, Tecgraf PUC Rio Comp Sci Dept, Rio De Janeiro, Brazil.
EM fmiranda@tecgraf.puc-rio.br; celes@tecgraf.puc-rio.br
OI Miranda, Fabio/0000-0001-8612-5805
FU CAPES (Brazilian National Research and Development Council); CNPq
   (Brazilian National Council for Scientific and Technological
   Development); Brazilian oil company, Petrobras
FX We thank CAPES (Brazilian National Research and Development Council) and
   CNPq (Brazilian National Council for Scientific and Technological
   Development) for the financial support to conduct this research. This
   work was done at the Tecgraf laboratory at PUC-Rio, which is mainly
   funded by the Brazilian oil company, Petrobras.
CR Bemardon F. F., 2006, Journal of Graphics Tools, V11, P1
   Carr H, 2006, IEEE T VIS COMPUT GR, V12, P231, DOI 10.1109/TVCG.2006.22
   Espinha R, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P273
   Garrity M. P., 1990, Computer Graphics, V24, P35, DOI 10.1145/99308.99316
   Hajjar J.E., 2008, IEEE PAC VIS S
   Marchesin S, 2009, IEEE T VIS COMPUT GR, V15, P1611, DOI 10.1109/TVCG.2009.149
   Marmitt G, 2006, P EUR IEEE VGTC S VI
   Max N., 2003, DATA VISUALIZATION, P157
   Miranda F. M., 2011, Proceedings of the 2011 24th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2011), P93, DOI 10.1109/SIBGRAPI.2011.3
   Moreland K, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P9
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Roettger S, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P23, DOI 10.1109/SWG.2002.1226506
   Röttger S, 2000, IEEE VISUAL, P109, DOI 10.1109/VISUAL.2000.885683
   Shirley P., 1990, Computer Graphics, V24, P63, DOI 10.1145/99308.99322
   Weiler M, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P71
   Weiler Manfred., 2003, VIS 03, P44, DOI DOI 10.1109/VISUAL.2003.1250390
   Williams P.L., 1992, PROC ACM WORKSHOP VO, P61
   Williams PL, 1998, IEEE T VIS COMPUT GR, V4, P37, DOI 10.1109/2945.675650
NR 18
TC 4
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 1005
EP 1014
DI 10.1007/s00371-012-0742-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900006
DA 2024-07-18
ER

PT J
AU Tian, J
   Jiang, WF
   Luo, T
   Cai, KY
   Peng, JL
   Wang, WC
AF Tian, Jiang
   Jiang, Wenfei
   Luo, Tao
   Cai, Kangying
   Peng, Jingliang
   Wang, Wencheng
TI Adaptive coding of generic 3D triangular meshes based on octree
   decomposition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Octree decomposition; Irregular mesh; Mesh compression; Geometry coding
ID GEOMETRIC COMPRESSION
AB In this paper, we present an adaptive-coding method for generic triangular meshes including both regular and irregular meshes. Though it is also based on iterative octree decomposition of the object space for the original mesh, as some prior arts, it has novelties in the following two aspects. First, it mathematically models the occupancy codes containing only a single-"1" bit for accurate initialization of the arithmetic coder at each octree level. Second, it adaptively prioritizes the bits in an occupancy code using a local surface smoothness measure that is based on triangle areas and therefore mitigates the effect of non-uniform vertex sampling over the surface. As a result, the proposed 3D mesh coder yields outstanding coding performance for both regular and irregular meshes and especially for the latter, as demonstrated by the experiments.
C1 [Tian, Jiang; Jiang, Wenfei; Luo, Tao; Cai, Kangying] Technicolor Res & Innovat, Beijing, Peoples R China.
   [Cai, Kangying; Wang, Wencheng] ISCAS, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Peng, Jingliang] Shandong Univ, Jinan 250100, Shandong, Peoples R China.
C3 Technicolor SA; Chinese Academy of Sciences; Institute of Software, CAS;
   Shandong University
RP Tian, J (corresponding author), Technicolor Res & Innovat, Beijing, Peoples R China.
EM jiang.tian@technicolor.com; wenfei.jiang@technicolor.com;
   tao.luo@technicolor.com; cai.kangying@technicolor.com;
   jingliap@gmail.com; whn@ios.ac.cn
RI Wang, Wencheng/A-3828-2009
CR Alliez P., 2003, In Advances in Multiresolution for Geometric Modelling, P3
   Bajaj CL, 1999, COMP GEOM-THEOR APPL, V14, P167, DOI 10.1016/S0925-7721(99)00026-7
   Botsch M., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P53
   Cai K., 2006, PROC ACM VIRTUAL REA, P83
   Devillers O, 2000, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2000.885711
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   Gumhold S., 2005, SIGGRAPH SKETCHES
   Hormann K., 2007, 35 ANN C COMP GRAPH
   Huang Y., 2006, EUROGRAPHICS S POINT, P103, DOI DOI 10.2312/SPBG/SPBG06/103-110
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   Kalaiah A, 2005, ACM T GRAPHIC, V24, P348, DOI 10.1145/1061347.1061356
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Laney D, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P470, DOI 10.1109/TDPVT.2002.1024102
   Ochotta T., 2004, EUROGRAPHICS S POINT, P103
   ONeill B., 1966, Elementary Differential Geometry
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Peng JL, 2010, COMPUT GRAPH FORUM, V29, P2029, DOI 10.1111/j.1467-8659.2010.01789.x
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Saupe D., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P333
   Schnabel R., 2006, S POINT BAS GRAPH 20, P111
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   WASCHBUSCH M, 2004, EUR S POINT BAS GRAP
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Wu J., 2005, Point-Based Graphics 2005 (IEEE Cat. No. 05EX1159), P25, DOI 10.1109/PBG.2005.194060
NR 27
TC 9
Z9 11
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 819
EP 827
DI 10.1007/s00371-012-0700-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500029
DA 2024-07-18
ER

PT J
AU Zhao, X
   Li, B
   Wang, L
   Kaufman, A
AF Zhao, Xin
   Li, Bo
   Wang, Lei
   Kaufman, Arie
TI RETRACTED: Texture-guided volumetric deformation and visualization using
   3D moving least squares (Retracted article. See vol. 30, pg. 1073, 2014)
SO VISUAL COMPUTER
LA English
DT Article; Retracted Publication
DE 3D MLS; Volumetric visualization; Texture-guided deformation
AB Examining and manipulating the large volumetric data attract great interest for various applications. For such purpose, we first extend the 2D moving least squares (MLS) technique into 3D, and propose a texture-guided deformation technique for creating visualization styles through interactive manipulations of volumetric models using 3D MLS. Our framework includes focus+context (F+C) visualization for simultaneously showing the entire model after magnification, and the cut-away or illustrative visualization for providing a better understanding of anatomical and biological structures. Both visualization styles are widely applied in the graphics areas. We present a mechanism for defining features using high-dimensional texture information, and design an interface for visualizing, selecting and extracting features/objects of interest. Methods of the interactive or automatic generation of 3D control points are proposed for the flexible and plausible deformation. We describe a GPU-based implementation to achieve real-time performance of the deformation techniques and the manipulation operators. Different from physical deformation models, our framework is goal-oriented and user-guided. We demonstrate the robustness and efficiency of our framework using various volumetric datasets.
C1 [Zhao, Xin; Li, Bo; Wang, Lei; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Zhao, X (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM xinzhao@cs.sunysb.edu; bli@cs.stonybrook.edu;
   leiwang1@cs.stonybrook.edu; ari@cs.stonybrook.edu
RI Wang, Lei/N-9874-2019
OI Wang, Lei/0000-0002-1931-7767
FU NSF [IIS0916235, CCF0702699, CNS0959979]; NIH [R01EB-7530]
FX This paper has been supported by NSF grants IIS0916235, CCF0702699 and
   CNS0959979 and NIH grant R01EB-7530.
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   Bier E. A., 1993, Computer Graphics Proceedings, P73, DOI 10.1145/166117.166126
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Bruckner S., Proceedings of the Seventh Joint Eurographics / IEEE VGTC Conference on Visualization, ser. EUROVIS'05. Aire-la-Ville, Switzerland, Switzerland: Eurographics Association, P69, DOI DOI 10.2312/VISSYM/EUROVIS05/069-076
   Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1077, DOI 10.1109/TVCG.2006.140
   Brunet T., 2006, EUROGRAPHICSIEEE VGT, P219
   Caban JJ, 2008, IEEE T VIS COMPUT GR, V14, P1364, DOI 10.1109/TVCG.2008.169
   Chen M, 2007, COMPUT GRAPH FORUM, V26, P824, DOI 10.1111/j.1467-8659.2007.01102.x
   Chen M., 2003, Eurographics/IEEE VGTC Workshop on Volume Graphics, P35, DOI DOI 10.1145/827051.827056
   Cohen M, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P32, DOI 10.1109/TPCG.2004.1314450
   Correa CD, 2007, IEEE T VIS COMPUT GR, V13, P1320, DOI 10.1109/TVCG.2007.70565
   Correa CD, 2010, COMPUT GRAPH-UK, V34, P370, DOI 10.1016/j.cag.2010.01.007
   Correa CarlosD., 2007, Sixth Eurographics / IEEE VGTC Workshop on Volume Graphics, P91
   Correa CarlosD., 2006, Fifth Eurographics / IEEE VGTC Workshop on Volume Graphics, P9, DOI DOI 10.2312/VG/VG06/009-016
   Cuno A., 2007, COMP GRAPH INT C, P1
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Ikits M., 2004, A focus and context interface for interactive volume rendering
   Keahey TA, 1996, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION '96, PROCEEDINGS, P38, DOI 10.1109/INFVIS.1996.559214
   Keahey TA, 1997, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P51, DOI 10.1109/INFVIS.1997.636786
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   Kurzion Y, 1995, SPRING COMP SCI, P21
   LaMar E, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P223, DOI 10.1109/PCCGA.2001.962877
   Lamping J., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P401
   LANCASTER P, 1981, MATH COMPUT, V37, P141, DOI 10.1090/S0025-5718-1981-0616367-1
   McGuffin MJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P401, DOI 10.1109/VISUAL.2003.1250400
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Tory M, 2005, IEEE T VIS COMPUT GR, V11, P71, DOI 10.1109/TVCG.2005.2
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Wang LJ, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P367
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P171, DOI 10.1109/TVCG.2010.34
   Zhou J., 2002, 3D DATA PROCESSING V, P87
   Zhu Yuanchen, 2007, 3D DEFORMATION USING
NR 32
TC 6
Z9 6
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2012
VL 28
IS 2
BP 193
EP 204
DI 10.1007/s00371-011-0635-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 881SW
UT WOS:000299510100006
DA 2024-07-18
ER

PT J
AU Berretti, S
   Ben Amor, B
   Daoudi, M
   del Bimbo, A
AF Berretti, Stefano
   Ben Amor, Boulbaba
   Daoudi, Mohamed
   del Bimbo, Alberto
TI 3D facial expression recognition using SIFT descriptors of automatically
   detected keypoints
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE 3D facial expression recognition; 3D facial expression retrieval; SIFT
   keypoints; Feature selection; SVM classification
ID FACE
AB Methods to recognize humans' facial expressions have been proposed mainly focusing on 2D still images and videos. In this paper, the problem of person-independent facial expression recognition is addressed using the 3D geometry information extracted from the 3D shape of the face. To this end, a completely automatic approach is proposed that relies on identifying a set of facial keypoints, computing SIFT feature descriptors of depth images of the face around sample points defined starting from the facial keypoints, and selecting the subset of features with maximum relevance. Training a Support Vector Machine (SVM) for each facial expression to be recognized, and combining them to form a multi-class classifier, an average recognition rate of 78.43% on the BU-3DFE database has been obtained. Comparison with competitor approaches using a common experimental setting on the BU-3DFE database shows that our solution is capable of obtaining state of the art results. The same 3D face representation framework and testing database have been also used to perform 3D facial expression retrieval (i.e., retrieve 3D scans with the same facial expression as shown by a target subject), with results proving the viability of the proposed solution.
C1 [Berretti, Stefano; del Bimbo, Alberto] Univ Florence, Dipartimento Sistemi & Informat, Florence, Italy.
   [Ben Amor, Boulbaba; Daoudi, Mohamed] TELECOM Lille 1, Inst TELECOM, LIFL, UMR 8022, Lille, France.
C3 University of Florence; Universite de Lille; IMT - Institut
   Mines-Telecom; IMT Atlantique
RP Berretti, S (corresponding author), Univ Florence, Dipartimento Sistemi & Informat, Florence, Italy.
EM stefano.berretti@unifi.it; alberto.delbimbo@unifi.it;
   boulbaba.benamor@telecom-lille1.eu; mohamed.daoudi@telecom-lille1.eu
RI Ben Amor, Boulbaba/K-7066-2018; Berretti, Stefano/U-9004-2019; Daoudi,
   Mohammed/H-5935-2013
OI Ben Amor, Boulbaba/0000-0002-4176-9305; Berretti,
   Stefano/0000-0003-1219-4386; DEL BIMBO, ALBERTO/0000-0002-1052-8322;
   Daoudi, Mohammed/0000-0003-4219-7860
CR [Anonymous], 1977, FACIAL ACTION CODING
   [Anonymous], P 1 COST 2101 WORKSH
   [Anonymous], 3DOR 10 EUR ASS
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], MPEG 4 FACIAL ANIMAT
   [Anonymous], 1987, ANTHROPOMETRIC FACIA
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Chalechale A., 2005, THESIS
   Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Ekman P., 1971, Nebraska symposium on motivation, V19, P207
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8
   Hao Tang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563052
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maalej Ahmed, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4129, DOI 10.1109/ICPR.2010.1003
   Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012
   MAYO M, 2009, IEEE INT C ADV VID S, P290
   Mian AS, 2008, INT J COMPUT VISION, V79, P1, DOI 10.1007/s11263-007-0085-5
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Mpiperis I, 2008, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2008.4518064
   Ohbuchi R., 2009, P WORKSH SEARCH 3D V
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Queirolo CC, 2010, IEEE T PATTERN ANAL, V32, P206, DOI 10.1109/TPAMI.2009.14
   Kassim SRA, 2006, IEEE IMAGE PROC, P661
   RODRIGUEZ JJ, 1990, IEEE T PATTERN ANAL, V12, P1138, DOI 10.1109/34.62603
   Samir C, 2009, INT J COMPUT VISION, V82, P80, DOI 10.1007/s11263-008-0187-8
   Soyel H, 2007, LECT NOTES COMPUT SC, V4633, P831
   Venkatesh YV, 2009, PATTERN RECOGN LETT, V30, P1128, DOI 10.1016/j.patrec.2009.04.007
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
NR 35
TC 109
Z9 117
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 1021
EP 1036
DI 10.1007/s00371-011-0611-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400007
DA 2024-07-18
ER

PT J
AU Jardim, E
   de Figueiredo, LH
AF Jardim, Eric
   de Figueiredo, Luiz Henrique
TI A fast hybrid method for apparent ridges
SO VISUAL COMPUTER
LA English
DT Article
DE Expressive lines; Non-photorealistic rendering
AB We propose a hybrid method for computing apparent ridges, expressive lines recently introduced by Judd et al. Unlike their original method, which works over the mesh entirely in object space, our method combines object-space and image-space computations and runs partially on the GPU, producing faster results in real time.
C1 [Jardim, Eric; de Figueiredo, Luiz Henrique] IMPA Inst Nacl Matemat Pura & Aplicada, Rio De Janeiro, Brazil.
C3 Instituto Nacional de Matematica Pura e Aplicada (IMPA)
RP de Figueiredo, LH (corresponding author), IMPA Inst Nacl Matemat Pura & Aplicada, Rio De Janeiro, Brazil.
EM ejardim@impa.br; lhf@impa.br
OI de Figueiredo, Luiz Henrique/0000-0001-5683-693X
CR Cole F, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1360612.1360657, 10.1145/1360612.1360687]
   Cole F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531334
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   DeCarlo D, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P63
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Gooch B., 2001, Non-photorealistic rendering
   HERTZMANN A, 1999, NONPHOTOREALISTIC RE, P7
   Interrante V, 1995, VISUALIZATION '95 - PROCEEDINGS, P52, DOI 10.1109/VISUAL.1995.480795
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Koenderink J. J., 1990, Solid shape
   Koenderink JJ, 1996, PERCEPTION, V25, P155, DOI 10.1068/p250155
   Lee Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239469
   Na KG, 2005, LECT NOTES COMPUT SC, V3767, P327
   Rusinkiewicz S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P486, DOI 10.1109/TDPVT.2004.1335277
   RUSINKIEWICZ S, 2010, TRIMESH2 LIB VERSION
   SOUSA MC, 2003, COMPUT GRAPH FORUM, V22, P327
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Wright RichardS., 2007, OPENGL SUPERBIBLE
   SUGGESTIVE CONTOURS
   APPARENT RIDGES LINE
NR 21
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 929
EP 937
DI 10.1007/s00371-011-0613-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200006
DA 2024-07-18
ER

PT J
AU Pinto, FD
   Freitas, CMD
AF Pinto, Francisco de Moura
   Dal Sasso Freitas, Carla Maria
TI Illustrating volume data sets and layered models with importance-aware
   composition
SO VISUAL COMPUTER
LA English
DT Article
DE Direct volume rendering; Importance; Composition; Layered models
AB Non-photorealistic (illustrative) rendering augments typical rendering models to selectively emphasize or de-emphasize specific structures of rendered objects. Illustrative techniques may affect not only the rendering style of specific portions of an object but also their visibility, ensuring that less important regions do not occlude more important ones. Cutaway views completely remove occluding, unimportant structures-possibly also removing valuable context information-while existing solutions for smooth reduction of occlusion based on importance lack precise visibility control, simplicity and generality. We introduce a new front-to-back fragment composition equation that directly takes into account a measure of sample importance and allows smooth and precise importance-based visibility control. We demonstrate the generality of our composition equation with several illustrative effects, which were obtained by using a set of importance measures calculated on the fly or defined by the user. The presented composition method is suitable for direct volume rendering as well as rendering of layered 3D models. We discuss both cases and show examples, though focusing mainly on illustration of volumetric data.
C1 [Pinto, Francisco de Moura; Dal Sasso Freitas, Carla Maria] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal do Rio Grande do Sul
RP Pinto, FD (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, Av Bento Goncalves 9500,Bldg 43425, Porto Alegre, RS, Brazil.
EM fmpinto@inf.ufrgs.br; carla@inf.ufrgs.br
RI /H-3333-2011
OI /0000-0003-1986-8435
CR Bruckner S, 2006, IEEE T VIS COMPUT GR, V12, P1559, DOI 10.1109/TVCG.2006.96
   Bruckner S, 2010, COMPUT GRAPH-UK, V34, P361, DOI 10.1016/j.cag.2010.04.003
   Bruckner S, 2009, COMPUT GRAPH FORUM, V28, P775, DOI 10.1111/j.1467-8659.2009.01474.x
   Correa CD, 2009, IEEE PAC VIS SYMP, P177, DOI 10.1109/PACIFICVIS.2009.4906854
   Csébfalvi B, 2001, COMPUT GRAPH FORUM, V20, pC452, DOI 10.1111/1467-8659.00538
   Everitt C, 2001, INTERACTIVE ORDER IN
   KRAUS M, 2005, P 16 IEEE VIS C
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Neumann L, 2000, COMPUT GRAPH FORUM, V19, pC351, DOI 10.1111/1467-8659.00427
   Pfister H, 2001, IEEE COMPUT GRAPH, V21, P16, DOI 10.1109/38.920623
   PINTO FDM, 2010, P 2010 22 BRAZ S COM
   Rautek P, 2008, COMPUT GRAPH FORUM, V27, P847, DOI 10.1111/j.1467-8659.2008.01216.x
   Rheingans P, 2001, IEEE T VIS COMPUT GR, V7, P253, DOI 10.1109/2945.942693
   Tenginakai S, 2001, IEEE VISUAL, P231, DOI 10.1109/VISUAL.2001.964516
   Treavett SMF, 2000, IEEE VISUAL, P203, DOI 10.1109/VISUAL.2000.885696
   Viola I, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P139, DOI 10.1109/VISUAL.2004.48
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   Viola I., 2005, Proceedings EG Workshop on Computational Aesthetics, Girona, Spain, May 2005, P209
NR 20
TC 6
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 875
EP 886
DI 10.1007/s00371-011-0606-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200002
DA 2024-07-18
ER

PT J
AU Xu, HX
   Cheng, ZQ
   Martin, RR
   Li, SK
AF Xu, Huaxun
   Cheng, Zhi-Quan
   Martin, Ralph R.
   Li, Sikun
TI 3D flow features visualization via fuzzy clustering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Feature visualization; 3D flow features; Fuzzy clustering; GPU
ID VORTEX; TOPOLOGY
AB A key approach to visualizing a flow field is to emphasize regions with significant behavior. However, it is difficult to give concrete criteria for classifying feature regions. In this paper, we use a novel framework in which fuzzy sets are used to determine flow features: Fuzzy relationships assess structural properties of features. A fuzzy c-means-like clustering algorithm is used to evaluate the importance of each voxel. Our approach can be readily modified with new fuzzy relationships describing other features of interest to users. We use a multi-resolution approach which displays structural features in greater detail, and represents the background by coarse-grained information. Experiments on synthetic and real datasets show that our framework can highlight significant aspects of the whole flow while avoiding occlusion and clutter. Interactive performance is achieved via a GPU implementation.
C1 [Cheng, Zhi-Quan] NUDT Univ, Comp Sch, PDL Lab, Changsha, Peoples R China.
   [Xu, Huaxun] Army Aviat Inst PLA, Beijing, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 National University of Defense Technology - China; Cardiff University
RP Cheng, ZQ (corresponding author), NUDT Univ, Comp Sch, PDL Lab, Changsha, Peoples R China.
EM xhxnudt@gmail.com; cheng.zhiquan@gmail.com; ralph@cs.cf.ac.uk;
   lisikun@263.net
RI Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
CR [Anonymous], 2005, SIAM C GEOM DES COMP
   Botchen R., 2008, INT S FLOW VIS
   Helgeland A, 2006, IEEE T VIS COMPUT GR, V12, P1535, DOI 10.1109/TVCG.2006.95
   HELMAN JL, 1991, IEEE COMPUT GRAPH, V11, P36, DOI 10.1109/38.79452
   Jaenicke H, 2007, IEEE T VIS COMPUT GR, V13, P1384, DOI 10.1109/TVCG.2007.70615
   JEONG J, 1995, J FLUID MECH, V285, P69, DOI 10.1017/S0022112095000462
   Krüger J, 2005, IEEE T VIS COMPUT GR, V11, P744, DOI 10.1109/TVCG.2005.87
   *NVIDA, 2010, NVID CUDA PROGR GUID
   PAL K, 1983, IEEE T PATTERN ANAL, V1, P69
   PARK SW, 2006, EUR IEEE VGTC S VIS, P131
   PEIKERT R, 2009, SPRING C COMP GRAPH, P43
   Petz C, 2009, COMPUT GRAPH FORUM, V28, P863, DOI 10.1111/j.1467-8659.2009.01463.x
   Post FH, 2003, COMPUT GRAPH FORUM, V22, P775, DOI 10.1111/j.1467-8659.2003.00723.x
   Salzbrunn T., 2008, Proceedings of Simulation and Visualization Conference, P75
   Salzbrunn T, 2006, IEEE T VIS COMPUT GR, V12, P1601, DOI 10.1109/TVCG.2006.104
   Schafhitzel T, 2008, COMPUT GRAPH FORUM, V27, P1023, DOI 10.1111/j.1467-8659.2008.01238.x
   Stegmaier S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P463
   SUJUDI D, 1995, P AIAA 12 COMP FLUID, P1695
   Timm H, 2004, FUZZY SET SYST, V147, P3, DOI 10.1016/j.fss.2003.11.009
   Weinkauf T, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P559
   WEINKAUF T, 2008, SIMULATION VISUALIZA, P237
   Weinkauf T, 2007, IEEE T VIS COMPUT GR, V13, P1759, DOI 10.1109/TVCG.2007.70545
   WU K, 2009, IEEE T VISUAL COMPUT, V99
NR 23
TC 3
Z9 3
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 441
EP 449
DI 10.1007/s00371-011-0577-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600004
DA 2024-07-18
ER

PT J
AU Grundhöfer, A
   Kurz, D
   Thiele, S
   Bimber, O
AF Grundhoefer, Anselm
   Kurz, Daniel
   Thiele, Sebastian
   Bimber, Oliver
TI Color invariant chroma keying and color spill neutralization for dynamic
   scenes and cameras
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2010)
CY JUN 08-12, 2010
CL Nanyang Technol Univ, Singapore, SINGAPORE
HO Nanyang Technol Univ
DE Chroma keying; Color spill; Image processing; Digitization and image
   capture
ID LIVE-ACTION
AB In this article we show how temporal backdrops that alternately change their color rapidly at recording rate can aid chroma keying by transforming color spill into a neutral background illumination. Since the chosen colors sum up to white, the chromatic (color) spill component is neutralized when integrating over both backdrop states. The ability to separate both states additionally allows to compute high-quality alpha mattes. Besides the neutralization of color spill, our method is invariant to foreground colors and supports applications with real-time demands. In this article, we explain different realizations of temporal backdrops and describe how keying and color spill neutralization are carried out, how artifacts resulting from rapid motion can be reduced, and how our approach can be implemented to be compatible with common real-time post-production pipelines.
C1 [Grundhoefer, Anselm; Kurz, Daniel; Thiele, Sebastian] Bauhaus Univ Weimar, D-99423 Weimar, Germany.
   [Bimber, Oliver] JKU Inst Comp Graph, A-4040 Linz, Austria.
C3 Bauhaus-Universitat Weimar; Johannes Kepler University Linz
RP Grundhöfer, A (corresponding author), Bauhaus Univ Weimar, Bauhausstr 11, D-99423 Weimar, Germany.
EM grundhoe@uni-weimar.de; daniel.kurz@uni-weimar.de;
   sebastian.thiele@uni-weimar.de; oliver.bimber@jku.at
CR Ben-Ezra M, 2000, PROC CVPR IEEE, P32, DOI 10.1109/CVPR.2000.855795
   CHAPLIN DJ, 1994, Patent No. 5313304
   CHOUDHURY B, 2008, I3D 08, P1
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Chuang YY, 2000, COMP GRAPH, P121, DOI 10.1145/344779.344844
   Debevec P, 2002, ACM T GRAPHIC, V21, P547
   DUPONT J, 2006, CRV 06, P33
   GRAHAM T, 2005, Patent No. 1499117
   Grau O, 2004, IEEE T CIRC SYST VID, V14, P370, DOI 10.1109/TCSVT.2004.823397
   GRUNDHOFER A, 2008, SIGGRAPH ASIA 08, P1
   Grundhofer Anselm., 2007, P 2007 6 IEEE ACM IN, P1
   Gvili R, 2003, PROC SPIE, V5006, P564, DOI 10.1117/12.474052
   KUECHLER M, 2005, 9 INT WORKSH IMM PRO, P23
   Matusik W, 2002, ACM T GRAPHIC, V21, P427, DOI 10.1145/566570.566599
   McGuire M., 2006, PRACTICAL REAL TIME, P235
   MCGUIRE M, 2005, SIGGRAPH 05, P567
   McGuire Morgan, 2006, SIGGRAPH 2006 SKETCH, P88, DOI [10.1145/1179849.1179959, DOI 10.1145/1179849.1179959]
   Moses R. A., 1987, TEMPORAL RESPONSIVEN
   Peers P., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P157
   Porter T., 1984, Proceedings of the 11th annual conference on Computer graphics and interactive techniques, P253, DOI 10.1145/800031.808606
   Sharma G., 2002, DIGITAL COLOR IMAGIN
   SMITH AR, 1996, SIGGRAPH 96, P259
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun J., 2007, P IEEE C COMP VIS PA
   VLAHOS P, 1991, Patent No. 5032901
   Vlahos Petro, 1977, US Patent, Patent No. [4,007,487, 4007487]
   Wang O, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P469, DOI 10.1109/PG.2007.52
   Wenger A, 2005, ACM T GRAPHIC, V24, P756, DOI 10.1145/1073204.1073258
   Whitesides T., 2004, SID Symp Digest Tech Papers, P133
   Zhu JY, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P402
   Zongker DE, 1999, COMP GRAPH, P205, DOI 10.1145/311535.311558
NR 31
TC 4
Z9 4
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1167
EP 1176
DI 10.1007/s00371-010-0464-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 635JD
UT WOS:000280650500003
DA 2024-07-18
ER

PT J
AU Kim, YJ
   Oh, YT
   Yoon, SH
   Kim, MS
   Elber, G
AF Kim, Yong-Joon
   Oh, Young-Taek
   Yoon, Seung-Hyun
   Kim, Myung-Soo
   Elber, Gershon
TI Precise Hausdorff distance computation for planar freeform curves using
   biarcs and depth buffer
SO VISUAL COMPUTER
LA English
DT Article
DE Hausdorff distance; Biarc approximation; Distance map; Depth buffer;
   Trimming; Lower bound
ID MINIMUM DISTANCE
AB We present a real-time algorithm for computing the precise Hausdorff Distance (HD) between two planar freeform curves. The algorithm is based on an effective technique that approximates each curve with a sequence of G (1) biarcs within an arbitrary error bound. The distance map for the union of arcs is then given as the lower envelope of trimmed truncated circular cones, which can be rendered efficiently to the graphics hardware depth buffer. By sampling the distance map along the other curve, we can estimate a lower bound for the HD and eliminate many redundant curve segments using the lower bound. For the remaining curve segments, we read the distance map and detect the pixel(s) with the maximum distance. Checking a small neighborhood of the maximum-distance pixel, we can reduce the computation to considerably smaller subproblems, where we employ a multivariate equation solver for an accurate solution to the original problem. We demonstrate the effectiveness of the proposed approach using several experimental results.
C1 [Kim, Yong-Joon; Oh, Young-Taek; Kim, Myung-Soo] Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151744, South Korea.
   [Yoon, Seung-Hyun] Dongguk Univ, Dept Game & Multimedia Engn, Seoul 100715, South Korea.
   [Elber, Gershon] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.
C3 Seoul National University (SNU); Dongguk University; Technion Israel
   Institute of Technology
RP Kim, MS (corresponding author), Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151744, South Korea.
EM mskim@snu.ac.kr
FU Israeli Ministry of Science [3-4642]; Israel Science Foundation
   [346/07]; MEST [K20717000006]; Korea Research Foundation
   [KRF-2008-313-D00923]
FX This work was supported in part by the Israeli Ministry of Science Grant
   No. 3-4642, in part by the Israel Science Foundation (grant No. 346/07),
   in part by KICOS through the Korean Israeli Binational Research Grant
   (K20717000006) provided by MEST in 2007, and also in part by the Korea
   Research Foundation under the Grant KRF-2008-313-D00923.
CR Aichholzer O, 2009, COMPUT AIDED DESIGN, V41, P339, DOI 10.1016/j.cad.2008.08.008
   ALLIEZ P, 2003, P INT MESH ROUNDT, P215
   Alt H., 2004, 20 EUROPEAN WORKSHOP, P233
   Alt H., 1999, HDB COMPUTATIONAL GE
   Alt H, 2008, INT J COMPUT GEOM AP, V18, P307, DOI 10.1142/S0218195908002647
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], ACM S VIRT REAL SOFT
   [Anonymous], 1985, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-1-4612-1098-6
   ATALLAH MJ, 1983, INFORM PROCESS LETT, V17, P207, DOI 10.1016/0020-0190(83)90042-X
   BARTON M, COMPUTER AI IN PRESS
   Chen XD, 2006, COMPUT AIDED DESIGN, V38, P1053, DOI 10.1016/j.cad.2006.04.012
   Chen XD, 2009, J COMPUT APPL MATH, V229, P294, DOI 10.1016/j.cam.2008.10.050
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Elber G., 2001, P 6 ACM S SOLID MODE, P1, DOI [10.1145/376957.376958, DOI 10.1145/376957.376958]
   Elberl G, 2008, LECT NOTES COMPUT SC, V4975, P191
   Ericson C., 2005, Real-time collision detection
   GILBERT EG, 1988, IEEE T ROBOTIC AUTOM, V4, P193, DOI 10.1109/56.2083
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   JOHNSON D, 2005, THESIS U UTAH
   JUTTLER B, 2000, MATH METHODS CAGD, P1
   Kim KJ, 2003, COMPUT AIDED DESIGN, V35, P871, DOI 10.1016/S0010-4485(02)00123-9
   Larsen E., 2000, IEEE INT C ROB AUT
   Lennerz C, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P60, DOI 10.1109/GMAP.2002.1027497
   Lin M.C., 2004, HDB DISCRETE COMPUTA, Vsecond, P787
   Lin M.C., 1998, PROC IMA C MATH SURF, P37
   LIN MC, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P1008, DOI 10.1109/ROBOT.1991.131723
   Llanas B, 2005, COMPUT OPTIM APPL, V30, P161, DOI 10.1007/s10589-005-4560-z
   MANOCHA D, 1998, P ACM S INT 3D GRAPH, P79
   QUINLAN S, 1994, IEEE INT CONF ROBOT, P3324, DOI 10.1109/ROBOT.1994.351059
   RABL M, 2009, P COMP KIN CK 2009, P141
   Rucklidge W., 1996, Lecture Notes in Computer Science, V1173
   Schneider P.J., 2002, GEOMETRIC TOOLS COMP
   Sír Z, 2006, COMPUT AIDED DESIGN, V38, P608, DOI 10.1016/j.cad.2006.02.003
   Sohn KA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P236, DOI 10.1109/PCCGA.2002.1167866
   TANG M, 2009, COMPUTER GRAPHICS AN
   *TECHN, IRIT 10 0 US MAN
NR 37
TC 34
Z9 39
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1007
EP 1016
DI 10.1007/s00371-010-0477-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800060
DA 2024-07-18
ER

PT J
AU Trescak, T
   Esteva, M
   Rodriguez, I
AF Trescak, Tomas
   Esteva, Marc
   Rodriguez, Inmaculada
TI A Virtual World Grammar for automatic generation of virtual worlds
SO VISUAL COMPUTER
LA English
DT Article
DE Shape grammars; Virtual institutions; 3D virtual worlds; Multi-agent
   systems; CAD
AB Hybrid systems such as those that combine 3D virtual worlds and organization based multiagent systems add new visual and communication features for multiuser applications. The design of such hybrid and dynamic systems is a challenging task. In this paper, we propose a system that can automatically generate a 3D virtual world (VW) from an organization based multiagent system (MAS) specification that establishes the activities participants can engage on. Both shape grammar and virtual world paradigms inspired us to propose a Virtual World Grammar (VWG) to support the generation process of a virtual world design. A VWG includes semantic information about both MAS specification and shape grammar elements. This information, along with heuristics and validations, guides the VW generation producing functional designs. To support the definition and execution of a Virtual World Grammar, we have developed a so named Virtual World Builder Toolkit (VWBT). We illustrate this process by generating a 3D visualization of a virtual institution from its specification.
C1 [Trescak, Tomas; Esteva, Marc] Spanish Council Sci Res, Artificial Intelligence Res Inst, Barcelona, Spain.
   [Rodriguez, Inmaculada] Univ Barcelona, Dept Appl Math, Barcelona, Spain.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Instituto
   de Investigacion en Inteligencia Artificial (IIIA); University of
   Barcelona
RP Trescak, T (corresponding author), Spanish Council Sci Res, Artificial Intelligence Res Inst, Barcelona, Spain.
EM ttrescak@iiia.csic.es; marc@iiia.csic.es; inma@maia.ub.es
RI Rodriguez, Inmaculada/H-9298-2015
OI Rodriguez, Inmaculada/0000-0001-5931-7713; Trescak,
   Tomas/0000-0002-2540-6002
FU EVE [TIN2009-14702-C02-01/TIN2009-14702-C02-02]; AT [CSD2007-0022]; EU;
   Catalan Gov. [2005-SGR-00093]; Marc Esteva's Ramon y Cajal
FX This work is partially funded by EVE
   (TIN2009-14702-C02-01/TIN2009-14702-C02-02) and AT (CONSOLIDER
   CSD2007-0022) projects, EU-FEDER funds, the Catalan Gov. (Grant
   2005-SGR-00093), and Marc Esteva's Ramon y Cajal contract.
CR ANCONA M, 2006, 8 C SIM SOC IT MAT A
   BOGDANOVYCH A, 2007, THESIS U TECHNOLOGY
   BROTA D, 2009, COMPUTER GRAPHICS VI, P151
   Duarte J.P., 2001, Customizing Mass Housing: A Discursive Grammar for Siza's Malagueira Houses
   Esteva M., 2004, Third International Joint Conference on Autonomous Agents and Multiagent Systems, V1, P236, DOI DOI 10.1109/AAMAS.2004.10060
   GEIGER C, 2000, VRST 00, P75
   MANSOURI H, 2009, WEB3D 2009 3D WEB TE, P101
   Rodriguez I., 2008, Wl 2008. 2008 IEEE/WIC/ACM International Conference on Web Intelligence. IAT 2008. 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology. Wl-IAT Workshop 2008 2008 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology Workshops, P196, DOI 10.1109/WIIAT.2008.320
   SOUTHEY F, 2001, ICCS 01, P333
   Stiny G., 1972, Information Processing 71 Proceedings of the IFIP Congress 1971. Volume 2, P1460
   Tanriverdi Vildan, 2001, P ACM S VIRTUAL REAL, P175
   TRESCAK T, 2009, CGIV 09
   TROYER OD, 2003, MMM, P279
   Wilson Jr, 2002, HUM FAC ER, P353
NR 14
TC 13
Z9 16
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 521
EP 531
DI 10.1007/s00371-010-0473-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800013
DA 2024-07-18
ER

PT J
AU Xia, T
   Wu, Q
   Chen, C
   Yu, YZ
AF Xia, Tian
   Wu, Qing
   Chen, Chun
   Yu, Yizhou
TI Lazy texture selection based on active learning
SO VISUAL COMPUTER
LA English
DT Article
DE Texture descriptors; Segmentation; Supervised classification; Graph cut;
   Scribbles
AB Interactive selection of desired textures and textured objects from a video is a challenging problem in video editing. In this paper, we present a scalable framework that accurately selects textured objects with only moderate user interaction. Our method applies the active learning methodology, and the user only needs to label minimal initial training data and subsequent query data. An active learning algorithm uses these labeled data to obtain an initial classifier and iteratively improves it until its performance becomes satisfactory. A revised graph-cut algorithm based on the trained classifier has also been developed to improve the spatial coherence of selected texture regions. We show that our system is responsive even with videos of a large number of frames, and it frees the user from extensive labeling work. A variety of operations, such as color editing, compositing, and texture cloning, can be then applied to the selected textures to achieve interesting editing effects.
C1 [Xia, Tian; Wu, Qing; Yu, Yizhou] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   [Chen, Chun] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Zhejiang University
RP Xia, T (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.
EM tianxia2@illinois.edu; qingwu1@illinois.edu; chenc@zju.edu.cn;
   yyz@illinois.edu
RI YU, YIZHOU/D-1603-2013; /F-3345-2010
OI /0000-0002-0470-5548
FU National Natural Science Foundation of China [60728204/F020404]
FX Four dynamic textures used in this paper, FLOWER, SEA PLANT, SMOKE, and
   BUBBLES, are from the DynTex database at Center for Mathematics and
   Computer Science (CWI), The Netherlands. This work was partially
   supported by National Natural Science Foundation of China
   (60728204/F020404).
CR Abe N., 1998, Fifteenth International Conference on Machine Learning, P1
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 1995, SIGGRAPH
   AVIDAN S, 2006, P EUR C COMP VIS ECC
   Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Brooks S., 2002, SIGGRAPH 2002 Proceedings, P653
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Chuang YY, 2001, PROC CVPR IEEE, P264
   DUNN D, 1994, IEEE T PATTERN ANAL, V16, P130, DOI 10.1109/34.273736
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gavish L., 2007, Principal-channels for one-sided object cutout
   Gleicher M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P183, DOI 10.1145/218380.218441
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Iyengar V., 2000, Sixth ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, P92
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   LI Y, 2008, P EUR S REND
   LUAN Q, 2007, EUR S REND
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Martin D., 2002, NEURAL INFORM PROCES
   Protiere A, 2007, IEEE T IMAGE PROCESS, V16, P1046, DOI 10.1109/TIP.2007.891796
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Revital Irony, 2005, EUR S REND
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   SCHAPIRE R, 2002, LECT NOTES STAT
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Wang J, 2007, INT CONF ACOUST SPEE, P601
NR 34
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2010
VL 26
IS 3
BP 157
EP 169
DI 10.1007/s00371-009-0359-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 558CW
UT WOS:000274719200001
DA 2024-07-18
ER

PT J
AU Chalmers, A
   Debattista, K
   Ramic-Brkic, B
AF Chalmers, Alan
   Debattista, Kurt
   Ramic-Brkic, Belma
TI Towards high-fidelity multi-sensory virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE Selective rendering; Multi-sensory virtual environments
AB Virtual environments are playing an increasingly important role for training people about real world situations, especially through the use of serious games. A key concern is thus the level of realism that virtual environments require in order to have an accurate match of what the user can expect in the real world with what they perceive in the virtual one. Failure to achieve the right level of realism runs the real risk that the user may adopt a different reaction strategy in the virtual world than would be desired in reality.
   High-fidelity, physically-based rendering has the potential to deliver the same perceptual quality of an image as if you were "there" in the real world scene being portrayed. However, our perception of an environment is not only what we see, but may be significantly influenced by other sensory inputs, including sound, smell, feel, and even taste. Computation and delivery of all sensory stimuli at interactive rates is a computationally complex problem. To achieve true physical accuracy for each of the senses individually for any complex scene in real-time is simply beyond the ability of current standard desktop computers. This paper discusses how human perception, and in particular any cross-modal effects in multi-sensory perception, can be exploited to selectively deliver high-fidelity virtual environments. Selective delivery enables those parts of a scene which the user is attending to, to be computed in high quality. The remainder of the scene is delivered in lower quality, at a significantly reduced computational cost, without the user being aware of this quality difference.
C1 [Chalmers, Alan; Debattista, Kurt; Ramic-Brkic, Belma] Univ Warwick, Int Digital Lab, WMG, Warwick, England.
C3 University of Warwick
RP Chalmers, A (corresponding author), Univ Warwick, Int Digital Lab, WMG, Warwick, England.
EM A.G.Chalmers@warwick.ac.uk
RI Ramic-Brkic, Belma/AHB-2625-2022
OI Ramic-Brkic, Belma/0000-0002-8205-0137
CR [Anonymous], 1998, THESIS NAVAL POSTGRA
   [Anonymous], 1890, PRINCIPLES PSYCHOL, DOI DOI 10.1037/10538-000
   Barfield W, 1995, PRESENCE-TELEOP VIRT, V5, P109, DOI 10.1162/pres.1996.5.1.109
   BARTZ D, 2008, EUROGRAPHICS 2008 ST
   Calvert GA, 2004, J PHYSIOL-PARIS, V98, P191, DOI 10.1016/j.jphysparis.2004.03.018
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   Chalmers A., 2007, INT J VIRTUAL REALIT, V6, P1
   CHALMERS A, 2009, SCCG 09, P15
   CHEN Y, 2006, ICAT 06
   DACHSBACHER C, 2007, SIGGRAPH 07, P61
   DEBATTISTA K, 2003, GRAPHITE, P13
   DEBATTISTA K, 2006, THESIS U BRISTOL BRI
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   ELLIS G, 2006, APGV 2006
   ELLIS G, 2006, SCCG 2006
   Hulusic V, 2008, WSCG 2008, FULL PAPERS, P41
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Longhurst P., 2006, P 4 INT C COMPUTER G, P21
   Mack Arien, 1998, Inattentional Blindness
   MARSLAND S, 2000, ANIMALS ANIMATS
   MASTOROPOULOU G, 2005, APGV 05, P9, DOI DOI 10.1145/1080402.1080404
   MASTOROPOULOU G, 2005, GRAPHITE 2005
   MASTOROPOULOU G, 2003, TPCG 04 THEORY PRACT, P128
   MASTOROPOULOU G, 2005, GRAPHITE 05, P363, DOI DOI 10.1145/1101389.1101462
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   NAKAMOTO T, 2008, IEEE COMPUTER GRAPHI
   NUNEZ D, 2003, AFRIGRAPH 2003, P101
   Pair J, 2006, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2006.23
   Pan M, 2007, COMPUT GRAPH FORUM, V26, P485, DOI 10.1111/j.1467-8659.2007.01071.x
   RAMIC B, 2007, SCCG 2007, P189
   Ramic-Brkic B., 2009, SPRING C COMP GRAPH, P175
   RIMELL S, 2002, ICMC02
   Ritschel T., 2008, ACM T GRAPHIC, V27, P1
   SUNDSTEDT V, 2005, SPRING C COMP GRAPH
   WALD I, STATE ART RAY TRACIN
   Ward G.J. R. Shakespeare., 1998, RENDERING RADIANCE A
   Washburn D A., 2003, Model. Simul. Mag, V2
   WHITTED T, 1980, SIGGRAPH 80, P14
   Winkler S, 2005, PROC SPIE, V5666, P139, DOI 10.1117/12.596852
   Yarbus A.L., 1967, EYE MOVEMENTS VISION, DOI [10.1007/978-1-4899-5379-7, DOI 10.1007/978-1-4899-5379-7]
   Yu YZ, 1999, COMP GRAPH, P215
   ZYBURA M, 1999, QUARTER PROJECT IND, V543
NR 43
TC 19
Z9 21
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1101
EP 1108
DI 10.1007/s00371-009-0389-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000006
DA 2024-07-18
ER

PT J
AU Koolwaaij, J
   Wibbels, M
   Böhm, S
   Luther, M
AF Koolwaaij, Johan
   Wibbels, Martin
   Boehm, Sebastian
   Luther, Marko
TI Living virtual history
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Mobile gaming; Pervasive computing; Context awareness
AB Mondial pervasive games feature the earth's surface as the game board with players traveling around the world like virtual pawns in the game. ContextKing is a game that makes extensive use of sensory inputs from the real world to create a whole new game experience within a user's social network. We discuss the management and utilization of context data, the principal game concept and its adoption and usage within the community.
C1 [Koolwaaij, Johan; Wibbels, Martin] INCA Expertise Grp, Enschede, Netherlands.
   [Boehm, Sebastian; Luther, Marko] DOCOMO Eurolabs, Smart & Secure Serv Res Grp, Munich, Germany.
C3 NTT Docomo
RP Koolwaaij, J (corresponding author), INCA Expertise Grp, Enschede, Netherlands.
EM johan.koolwaaij@novay.nl; martin.wibbels@novay.nl;
   boehm@docomolab-euro.com; luther@docomolab-euro.com
CR Baader F., 2003, DESCRIPTION LOGIC HD
   BOHM S, 2008, LNCS, V5318, P804
   BOHM S, 2008, P 1 INT DISCOTEC WOR
   Cameron L., 2004, The geocaching handbook
   Dey AK, 2001, PERS UBIQUIT COMPUT, V5, P4, DOI 10.1007/s007790170019
   Grau BC, 2008, J WEB SEMANT, V6, P309, DOI 10.1016/j.websem.2008.05.001
   Jahn S., 2006, Information Technology and Control, V35, P198
   Koolwaaij J, 2009, PROCEEDINGS OF THE IEEE VIRTUAL WORLDS FOR SERIOUS APPLICATIONS, P61, DOI 10.1109/VS-GAMES.2009.21
   LUTHER M, 2009, P 1 INT WORKSH STREA, V466
   Luther M, 2008, KNOWL ENG REV, V23, P7, DOI 10.1017/S0269888907001300
   NIEMI J, 2006, THESIS STOCKHOLM U
   Niemi J, 2005, P 4 DEC C CRIT COMP, P137
   NURMI P, 2006, P 3 INT C MOB UB SYS
   Sotamaa O., 2002, Computer Games and Digital Cultures, P35
   TEUBER K, 2005, SETTLERS CATAN GAME
   WESSEL M, 2007, P CONTEXT 07 WORKSH, V298
NR 16
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2009
VL 25
IS 12
BP 1055
EP 1062
DI 10.1007/s00371-009-0384-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 510LC
UT WOS:000271089000002
DA 2024-07-18
ER

PT J
AU Bonanni, U
   Montagnol, M
   Magnenat-Thalmann, N
AF Bonanni, Ugo
   Montagnol, Melanie
   Magnenat-Thalmann, Nadia
TI Multilayered visuo-haptic hair simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds
CY OCT 24-27, 2007
CL Hannover, GERMANY
SP Welfenlab, Gottfried Wilhelm Leibniz Univ, EuroGraphics, ACM SIGWEB, ACM SIGART
DE hair simulation; haptic rendering; multimodal perception; interactive
   modeling
AB Over the last fifteen years, research on hair simulation has made great advances in the domains of modeling, animation and rendering, and is now moving towards more innovative interaction modalities. The combination of visual and haptic interaction within a virtual hairstyling simulation framework represents an important concept evolving in this direction. Our visuo-haptic hair interaction framework consists of two layers which handle the response to the user's interaction at a local level (around the contact area), and at a global level (on the full hairstyle). Two distinct simulation models compute individual and collective hair behavior. Our multilayered approach can be used to efficiently address the specific requirements of haptics and vision. Haptic interaction with both models has been tested with virtual hairstyling tools.
C1 [Bonanni, Ugo; Montagnol, Melanie; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, CH-1227 Geneva, Switzerland.
C3 University of Geneva
RP Bonanni, U (corresponding author), Univ Geneva, MIRALab, Batiment A Route Drize 7, CH-1227 Geneva, Switzerland.
EM bonanni@miralab.unige.ch; montagnol@miralab.unige.ch;
   thalmann@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR ANJYO K, 1992, COMP GRAPH, V26, P111, DOI 10.1145/142920.134021
   [Anonymous], 2002, CHEM PHYS BEHAV HUMA
   Baltenneck F, 2001, J COSMET SCI, V52, P355
   Bando Y, 2003, COMPUT GRAPH FORUM, V22, P411, DOI 10.1111/1467-8659.00688
   Bertails F., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P207
   Bertails F, 2006, ACM T GRAPHIC, V25, P1180, DOI 10.1145/1141911.1142012
   BONANNI U, 2008, EUROGRAPHICS 2008 SH, P135
   Brenan K.E., 1995, Numerical Solution of Initial-Value Problems in Differential-Algebraic Equations
   Chang JohnnyT., 2002, P 2002 ACM SIGGRAPHE, P73
   CONTI F, 2005, CHAI OPENSOURCE LIB
   Daldegan A., 1993, Communicating with Virtual Worlds, P358
   GUPTA R, 2005, INT C CAD GRAPH, P273
   Gupta R, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P133
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   Hearle JWS, 2000, INT J BIOL MACROMOL, V27, P123, DOI 10.1016/S0141-8130(00)00116-1
   Nogueira ACS, 2006, PHOTOCH PHOTOBIO SCI, V5, P165, DOI 10.1039/b504574f
   Rosenblum R. E., 1991, Journal of Visualization and Computer Animation, V2, P141, DOI 10.1002/vis.4340020410
   Spillmann J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P63
   SWIFT JA, 1995, J COSMET SCI, V17, P245
   Volino P, 2006, IEEE T VIS COMPUT GR, V12, P131, DOI 10.1109/TVCG.2006.36
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Wei Xue., 2006, What is human hair? A light and scanning electron microscopy study
   Wortmann F.-J., 2006, International Journal of Cosmetic Science, V28, P61, DOI 10.1111/j.1467-2494.2006.00306.x
   Zuidema P, 2003, BIORHEOLOGY, V40, P431
NR 24
TC 3
Z9 4
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2008
VL 24
IS 10
BP 901
EP 910
DI 10.1007/s00371-008-0288-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 347IQ
UT WOS:000259134200006
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Aras, R
   Basarankut, B
   Çapin, T
   Özgüç, B
AF Aras, Rifat
   Basarankut, Barkin
   Capin, Tolga
   Oezguec, Buelent
TI 3D Hair sketching for real-time dynamic & key frame animations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE sketching; direct manipulation; key frame; hair animation
AB Physically based simulation of human hair is a well studied and well known problem. But the "pure" physically based representation of hair (and other animation elements) is not the only concern of the animators, who want to "control" the creation and animation phases of the content. This paper describes a sketch-based tool, with which a user can both create hair models with different styling parameters and produce animations of these created hair models using physically and key frame-based techniques. The model creation and animation production tasks are all performed with direct manipulation techniques in real-time.
C1 [Aras, Rifat; Basarankut, Barkin; Capin, Tolga; Oezguec, Buelent] Bilkent Univ, Dept Comp Engn, Ankara, Turkey.
C3 Ihsan Dogramaci Bilkent University
RP Aras, R (corresponding author), Bilkent Univ, Dept Comp Engn, Ankara, Turkey.
EM arifat@cs.bilkent.tr; barkin@cs.bilkent.tr; tcapin@cs.bilkent.tr;
   ozguc@cs.bilkent.tr
RI Capin, Tolga K/K-2683-2012; Çapın, Tolga Kurtuluş/G-6172-2018
CR [Anonymous], PROGRAMMING GUIDE OF
   Bando Y, 2003, COMPUT GRAPH FORUM, V22, P411, DOI 10.1111/1467-8659.00688
   Choe BW, 2005, IEEE T VIS COMPUT GR, V11, P160, DOI 10.1109/TVCG.2005.20
   FATTAL R, 2004, ACM SIGGRAPH 2004 PA, P441
   FU H, 2007, EUR WORKSH SKETCH BA
   Hadap S, 2001, COMPUT GRAPH FORUM, V20, pC329, DOI 10.1111/1467-8659.00525
   HERNANDEZ B, 2004, WSCG 2004 POSTER, P57
   KIM TY, 2002, P ACM SIGGRAPH, P620
   Koh CK, 2001, SPRING EUROGRAP, P127
   MALIK S, 2005, P EUR WORKSH SKETCH, P185
   OSHITA M, 2007, VIRTUAL WORLDS, V18, P583
   PETROVIC L, 2005, 0608 PIX
   Plante E, 2001, SPRING EUROGRAP, P139
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   TREUILLE A, 2003, ACM SIGGRAPH 2003 PA, P716
   Volino P, 2006, IEEE T VIS COMPUT GR, V12, P131, DOI 10.1109/TVCG.2006.36
   Wither J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P33, DOI 10.1109/SMI.2007.31
NR 17
TC 3
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 577
EP 585
DI 10.1007/s00371-008-0238-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800013
DA 2024-07-18
ER

PT J
AU Dong, WM
   Zhou, N
   Paul, JC
AF Dong, Weiming
   Zhou, Ning
   Paul, Jean-Claude
TI Perspective-aware texture analysis and synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE texture synthesis; perspectively featured texture; scale map
ID IMAGE; DEFORMATION
AB This paper presents a novel texture synthesis scheme for anisotropic 2D textures based on perspective feature analysis and energy optimization. Given an example texture, the synthesis process starts with analyzing the texel (TEXture ELement) scale variations to obtain the perspective map (scale map). Feature mask and simple user-assisted scale extraction operations including slant and tilt angles assignment and scale value editing are applied. The scale map represents the global variations of the texel scales in the sample texture. Then, we extend 2D texture optimization techniques to synthesize these kinds of perspectively featured textures. The non-parametric texture optimization approach is integrated with histogram matching, which forces the global statics of the texel scale variations of the synthesized texture to match those of the example. We also demonstrate that our method is well-suited for image completion of a perspectively featured texture region in a digital photo.
C1 [Dong, Weiming] CAS Inst Automat, LIAMA NLPR, Beijing, Peoples R China.
   [Zhou, Ning] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Tsinghua
   University
RP Dong, WM (corresponding author), CAS Inst Automat, LIAMA NLPR, Beijing, Peoples R China.
EM wmlake@gmail.com; zhoun03@mails.tsinghua.edu.cn;
   paul@mails.tsinghua.edu.cn
RI DONG, Weiming/AAG-7678-2020
OI DONG, Weiming/0000-0001-6502-145X
CR [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], COMBINING CUES SHAPE
   Clerc M, 2002, IEEE T PATTERN ANAL, V24, P536, DOI 10.1109/34.993560
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   CRIMINISI A, 2003, CVPR 03, V2
   Dischler JM, 2006, VISUAL COMPUT, V22, P926, DOI 10.1007/s00371-006-0077-4
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   Dong W., 2007, GI'07: Proceedings of Graphics Interface 2007', P249
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   KOPF J, 2007, SIGGRAPH 07, P2
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Lindeberg T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P683, DOI 10.1109/ICCV.1993.378146
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Norman JF, 2006, VISION RES, V46, P1057, DOI 10.1016/j.visres.2005.09.034
   PLANTIER J, 2001, P INT C IM PROC IEEE, P421
   Shen JB, 2007, VISUAL COMPUT, V23, P631, DOI 10.1007/s00371-007-0154-3
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Zelinka S, 2004, ACM T GRAPHIC, V23, P930, DOI 10.1145/1027411.1027413
   ZHANG J, 2003, SIGGRAPH 03, P295
NR 26
TC 4
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 515
EP 523
DI 10.1007/s00371-008-0232-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Agarwal, P
   Rajagopalan, S
   Prabhakaran, B
AF Agarwal, Parag
   Rajagopalan, Srinivas
   Prabhakaran, B.
TI Minimizing probable collision pairs searched in interactive animation
   authoring
SO VISUAL COMPUTER
LA English
DT Article
DE collision detection and prediction; bounding volume hierarchy; spatial
   hash table; spatial partitioning
AB Animation authoring involves an author's interaction with a scene, resulting in varying scene complexity for a given animation sequence. In such a varying environment, detection and prediction of collision in minimal time and with high accuracy is a challenge. This paper proposes using the bounding volume-based space subdivision mechanism to reduce search space for an object pair collision search. This data structure is enhanced using a direction-based spatial hash table, which predicts collision between static and dynamic objects. These techniques are shown to work in conjunction with existing search space reduction methods. The event of collision is accurately detected using known methods, such as kinetic data structures. Simulation results show that for a scene with 10000 objects with varying dynamic objects (10-90%), the method finds probable collision-pairs with 95-99% accuracy.
C1 [Agarwal, Parag; Rajagopalan, Srinivas; Prabhakaran, B.] Univ Texas Richardson, Dept Comp Sci, Richardson, TX 75083 USA.
C3 University of Texas System; University of Texas Dallas
RP Agarwal, P (corresponding author), Univ Texas Richardson, Dept Comp Sci, MS EC 31,POB 830688, Richardson, TX 75083 USA.
EM pxa016500@utdallas.edu; sxr035000@utdallas.edu; praba@utdallas.edu
CR Basch J, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P102
   BERGEN VDB, 2003, M KAUFMANN SERIES IN
   DONG JK, 1998, IEEE T VIS COMPUT GR, V4, P230
   Frank A. U., 1990, P 1 S DES IMPL LARG, P29
   GANTER MA, 1993, J MECH DESIGN, V115, P150, DOI 10.1115/1.2919312
   GOMEZ M, 2001, GAME PROGRAMMING GEM, V2, P388
   GOTTSCHALK S, SIGGRAPH 96, P171
   GOTTSCHALK S, 1998, THESIS U N CAROLINA
   Hubbard PM, 1996, ACM T GRAPHIC, V15, P179, DOI 10.1145/231731.231732
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Luque RodrigoG., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, P179
   PALMER IJ, 1995, COMPUT GRAPH FORUM, V14, P105, DOI 10.1111/1467-8659.1420105
   QUINLAN S, 1994, IEEE INT CONF ROBOT, P3324, DOI 10.1109/ROBOT.1994.351059
   SAMET H, 1989, SPATIAL DATA STRUCTU
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   THIBAULT WC, 1914, P 14 ANN C COMP GRAP
   Zachmann G, 1998, P IEEE VIRT REAL ANN, P90, DOI 10.1109/VRAIS.1998.658428
   ZACHMANN G, 2001, P 1 INT GAM TECHN C, P18
   ZYDA MJ, 1993, J VISUAL COMP ANIMAT, V4, P13, DOI 10.1002/vis.4340040104
NR 19
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 347
EP 359
DI 10.1007/s00371-007-0193-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200004
DA 2024-07-18
ER

PT J
AU Ersotelos, N
   Dong, F
AF Ersotelos, Nikolaos
   Dong, Feng
TI Building highly realistic facial modeling and animation: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE facial modeling; facial expression and animation; generic model
   adaptation; morphablemodeling; pseudo muscles; performance-driven
   animation; blend shapes
ID FACES; RECONSTRUCTION; CAPTURE; IMAGE
AB This paper provides a comprehensive survey on the techniques for human facial modeling and animation. The survey is carried out from two different perspectives: facial modeling, which concerns how to produce 3D face models, and facial animation, which regards how to synthesize dynamic facial expressions. To generate an individual face model, we can either perform individualization of a generic model or combine face models from an existing face collection. With respect to facial animation, we have further categorized the techniques into simulation-based, performance-driven and shape blend-based approaches. The strength and weakness of these techniques within each category are discussed, alongside with the applications of these techniques to various exploitations. In addition, a brief historical review of the technique evolution is provided. Limitations and future trend are discussed. Conclusions are drawn at the end of the paper.
C1 Brunel Univ, Dept Informat Syst & Comp, Uxbridge UB8 3PH, Middx, England.
C3 Brunel University
RP Ersotelos, N (corresponding author), Brunel Univ, Dept Informat Syst & Comp, Uxbridge UB8 3PH, Middx, England.
EM Nikolaos.Ersotelos@brunel.ac.uk
OI Ersotelos, Nikolaos/0000-0001-7699-9998
CR Abrantes GA, 1999, IEEE T CIRC SYST VID, V9, P290, DOI 10.1109/76.752096
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   [Anonymous], THESIS U UTAH SALT L
   [Anonymous], P ACM S VIRT REAL SO
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Blanz Volker., 1999, P 26 ANN C COMPUTER, P187, DOI DOI 10.1145/311535.311556
   CHAI JX, 2003, P 2003 ACM SIGGRAPH
   Debevec P., 1998, SIGGRAPH98, P189, DOI DOI 10.1145/280814.280864
   DECARLO D, 1998, P SIGGRAPH 98, P67, DOI DOI 10.1145/280814.280823
   Du YZ, 2003, PATTERN RECOGN LETT, V24, P2923, DOI 10.1016/S0167-8655(03)00153-3
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   GELDER A, 1998, J GRAPHICS TOOLS, V3, P21
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   Hassenzahl M, 2003, Berichte Des Ger Chapter Acm, V2003, P187
   Kähler K, 2003, ACM T GRAPHIC, V22, P554, DOI 10.1145/882262.882307
   Kahler K., 2002, Eurographics Symp. on Comp. Animation, P55, DOI DOI 10.1145/545261.545271
   KALRA P, 2004, HDB VIRTUAL HUMANS, pCH6
   KOCH A, 1996, P INT S FIELD PROGR, P151
   KSHIRSAGAR S, 2004, HDB VIRTUAL HUMANS, pCH10
   LEE W, 2004, HDB VIRTUAL HUMANS, pCH2
   Lee Y., 1995, SIGGRAPH, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]
   LITWINOWICZ P, 1994, P SIGGRAPH 94, P409
   Liu ZC, 2001, COMP GRAPH, P271
   Magnenat-Thalmann Nadia., 2004, HDB VIRTUAL HUMANS
   Marschner SR, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P262
   NOH J, 1998, 99705 INT MED SYST C
   Parke F., 1996, COMPUTER FACIAL ANIM
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Pighin F., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P143, DOI 10.1109/ICCV.1999.791210
   PIGHIN F, 1998, P SIGGRAPH 98, P75
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   Pratscher Michael., 2005, SCA 05, P329
   SEO H, 2003, P ACM SIGGRAPH EUR S, P120
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Tu PH, 2004, J COMPUT SCI TECH-CH, V19, P618, DOI 10.1007/BF02945587
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Ward K, 2007, IEEE T VIS COMPUT GR, V13, P213, DOI 10.1109/TVCG.2007.30
   Waters K., 1987, ACM SIGGRAPH Comput. Graph., V21, P17
   WILHELMS J, 1997, P SIGGRAPH 97, P173
   Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906
   YIN L, 2004, P 1I ANN C COMP GRAP, P360
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   ZHANG Q, 2004, P 2 INT 6 C COMP GRA, P173
   ZHANG Q, 2003, P ACM SIGGRAPH EUR S, P177
   Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9
NR 49
TC 48
Z9 57
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2008
VL 24
IS 1
BP 13
EP 30
DI 10.1007/s00371-007-0175-y
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 234GU
UT WOS:000251149500002
DA 2024-07-18
ER

PT J
AU Cui, M
   Wonka, P
   Razdan, A
   Hu, JX
AF Cui, Ming
   Wonka, Peter
   Razdan, Anshuman
   Hu, Jiuxiang
TI A new image registration scheme based on curvature scale space curve
   matching
SO VISUAL COMPUTER
LA English
DT Article
DE image registration; curve matching; curvature scale space
ID REPRESENTATION
AB We propose a new image registration scheme for remote sensing images. This scheme includes three steps in sequence. First, a segmentation process is performed on the input image pair. Then the boundaries of the segmented regions in two images are extracted and matched. These matched regions are called confidence regions. Finally, a non-linear optimization is performed in the matched regions only to obtain a global set of transform parameters. Experiments show that this scheme is more robust and converges faster than registration of the original image pair. We also develop a new curve-matching algorithm based on curvature scale space to facilitate the second step.
C1 Arizona State Univ, Dept Comp Sci & Engn, Tempe, AZ 85287 USA.
   Arizona State Univ, I3DEA, Mesa, AZ 85212 USA.
C3 Arizona State University; Arizona State University-Tempe; Arizona State
   University
RP Cui, M (corresponding author), Arizona State Univ, Dept Comp Sci & Engn, 878609, Tempe, AZ 85287 USA.
EM ming.cui@asu.edu
CR Abbasi S, 2000, IMAGE VISION COMPUT, V18, P199, DOI 10.1016/S0262-8856(99)00019-0
   AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681
   [Anonymous], P 16 INT C PATT REC
   [Anonymous], P IEEE INT C IMAGE P
   [Anonymous], PHOTOGRAMM ENG REM
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], MED IMAGE ANAL
   [Anonymous], P 14 INT C INF PROC
   [Anonymous], J COMPUT AIDED DESIG
   [Anonymous], GEOSIENCE REMOTE SEN
   [Anonymous], P INT GEOSCIENCES RE
   [Anonymous], PHYS MED BIOL
   [Anonymous], P 4 INT JOINT C PATT
   Bentoutou Y, 2005, IEEE T GEOSCI REMOTE, V43, P2127, DOI 10.1109/TGRS.2005.853187
   Brown L.G., 1992, ACM COMPUT SURV, V24, P325, DOI DOI 10.1145/146370.146374
   CHELLAPPA R, 1984, IEEE T PATTERN ANAL, V6, P102, DOI 10.1109/TPAMI.1984.4767482
   Cole-Rhodes AA, 2003, IEEE T IMAGE PROCESS, V12, P1495, DOI 10.1109/TIP.2003.819237
   Dai XL, 1999, IEEE T GEOSCI REMOTE, V37, P2351, DOI 10.1109/36.789634
   Ding LJ, 2000, PROC SPIE, V3979, P1235, DOI 10.1117/12.387631
   FLUSSER J, 1994, IEEE T GEOSCI REMOTE, V32, P382, DOI 10.1109/36.295052
   GOSHTASBY A, 1985, IEEE T PATTERN ANAL, V7, P738, DOI 10.1109/TPAMI.1985.4767734
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   LI H, 1995, IEEE T IMAGE PROCESS, V4, P320, DOI 10.1109/83.366480
   Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591
   MOKHTARIAN F, 1995, IEEE T PATTERN ANAL, V17, P539, DOI 10.1109/34.391387
   Pizer SM, 2003, INT J COMPUT VISION, V55, P85, DOI 10.1023/A:1026313132218
   Stone HS, 1999, IEEE T PATTERN ANAL, V21, P1074, DOI 10.1109/34.799911
   Vemuri BC, 2003, MED IMAGE ANAL, V7, P1, DOI 10.1016/S1361-8415(02)00063-4
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008
   Zheng Q, 1993, IEEE T IMAGE PROCESS, V2, P311, DOI 10.1109/83.236535
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 32
TC 27
Z9 32
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 607
EP 618
DI 10.1007/s00371-007-0164-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200006
DA 2024-07-18
ER

PT J
AU Jeong, SJ
   Kaufman, AE
AF Jeong, Seok-Jae
   Kaufman, Arie E.
TI Interactive wireless virtual colonoscopy
SO VISUAL COMPUTER
LA English
DT Article
DE wireless virtual colonoscopy; remote visualization; GPU-based volume
   rendering; interactive image streaming
ID COLORECTAL NEOPLASIA; CT COLONOGRAPHY; VISUALIZATION; ALGORITHM
AB We present an interactive virtual colon navigation system on a PDA that is a client-server system over a wireless network. For improving the quality of the rendering results on the PDA, the overall rendering speed, and the user interactivity, we propose three novel methods and adapt a GPU-based direct volume rendering technique. Using these proposed methods, our system can support approximately a two times faster navigation speed and 17 percent better PSNR than previous remote visualization methods with a 512x512x361 volumetric colon CT data using a PDA device over 802.11b wireless network.
C1 SUNY Stony Brook, Dept Comp Sci, Ctr Visual Comp, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Jeong, SJ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Ctr Visual Comp, Stony Brook, NY 11794 USA.
EM seokjae@gmail.com; ari@cs.sunysb.edu
CR Ang C. S., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P13, DOI 10.1109/VISUAL.1994.346342
   Beaulieu CF, 1999, RADIOLOGY, V212, P203, DOI 10.1148/radiology.212.1.r99jl17203
   Brachtl M, 2001, COMPUT GRAPH-UK, V25, P627, DOI 10.1016/S0097-8493(01)00091-7
   CAMPBELL G, 1986, 2 BIT PIXEL FULL COL, P215
   DIEPSTRATEN J, 2004, REMOTE LINE RENDERIN, P454
   Engel K, 2000, IEEE VISUAL, P449, DOI 10.1109/VISUAL.2000.885729
   ENGEL K, 1999, ADV INTELLIGENT COMP, P91
   ENGEL K, 2000, FRAMEWORK INTERACTIV, P167
   Fenlon HM, 1999, NEW ENGL J MED, V341, P1496, DOI 10.1056/NEJM199911113412003
   FLOYD RW, 1976, P SID, V17, P75
   HONG L, 1997, VIRTUAL VOYAGE INTER, P27
   *IND JPEG GROUP, 1998, LIB JPEG IMAG COMPR
   Kaufman AE, 2005, COMMUN ACM, V48, P37, DOI 10.1145/1042091.1042117
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Lamberti F., 2005, WSEAS Transactions on Information Science and Applications, V2, P258
   Lamberti F., 2003, ACCELERATED REMOTE G, P55
   LIPPERT L, 1996, COMPRESSION DOMAIN V, P95
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   *MICR CORP, 2006, WIND MOB 5 0 SDK POC
   *NVIDIA CORP, 2006, CG TOOLK 1 4 1
   Pickhardt PJ, 2003, NEW ENGL J MED, V349, P2191, DOI 10.1056/NEJMoa031618
   Quillet J., 2006, Proc. Int. Conf. 3D Web Technol, VWeb3D '06, P27, DOI [DOI 10.1145/1122591.1122595, 10.1145/1122591.1122595]
   RIZZO F, 2001, OVERLAP ADAPTIVE VEC, P401
   SCHALNAT GE, 2006, OFFICIAL PNG REFRENC
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   Stegmaier S, 2003, ISPA 2003: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, PTS 1 AND 2, P174
   Wan M., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P397, DOI 10.1109/VISUAL.1999.809914
   Yee J, 2001, RADIOLOGY, V219, P685, DOI 10.1148/radiology.219.3.r01jn40685
NR 28
TC 10
Z9 12
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 545
EP 557
DI 10.1007/s00371-007-0117-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200002
DA 2024-07-18
ER

PT J
AU Zhang, HT
   Kaufman, AE
AF Zhang, Haitao
   Kaufman, Arie E.
TI Point-and-edge model for edge-preserving splatting
SO VISUAL COMPUTER
LA English
DT Article
DE point-and-edge model; edge-preserving simplification; edge-preserving
   rendering; constrained splatting
AB We introduce the point-and-edge model for edge-preserving modeling and rendering. Besides a set of surface points, the point-and-edge model also includes edge points representing the sharp edges in the model. The surface points and the sharp edges are relatively independent of each other. We present a feedback algorithm to simplify the point-and-edge model with bounded error based on an edge-preserving clustering method. An efficient constrained splatting method is used to preserve the sharp edges in the rendering, regardless of the surface point density.
C1 SUNY Stony Brook, CVC, Stony Brook, NY 11794 USA.
   SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Zhang, HT (corresponding author), SUNY Stony Brook, CVC, Stony Brook, NY 11794 USA.
EM haitao@cs.sunysb.edu; ari@cs.sunysb.edu
RI Zhang, Haitao/IUP-7507-2023
CR ADAMS B, 2003, SIGGRAPH P, P651
   Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   [Anonymous], 2001, P IMR 2001 NEWP BEAC
   BALA K, 2003, SIGGRAPH 03, P631
   Botsch M, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P335, DOI 10.1109/PCCGA.2003.1238275
   BOTSCH M, 2004, S POINT BAS GRAPH, P25
   BOTSCH M, 2002, EGRW 02, P53
   Botsch Mario, 2005, P EUROGRAPHICSIEEE V, P17, DOI [DOI 10.2312/SPBG/SPBG05/017-024, 10.1109/PBG.2005.194059.6]
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   PAULY M, 2003, EUROGRAPHICS, P281
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Ren L, 2002, COMPUT GRAPH FORUM, V21, P461, DOI 10.1111/1467-8659.00606
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   TALTON JO, 2005, S POINT BAS GRAPH, P33
   Wicke M, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P160, DOI 10.1109/PCCGA.2004.1348346
   WU J, 2004, EUROGRAPHICS, P643
   WU J, 2005, S POINT BAS GRAPH, P25
   Zwicker M, 2004, PROC GRAPH INTERF, P247
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 26
TC 0
Z9 1
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 397
EP 408
DI 10.1007/s00371-007-0098-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800002
DA 2024-07-18
ER

PT J
AU Choi, JJ
   Lee, HJ
AF Choi, Jung-Ju
   Lee, Hwan-Jik
TI Rendering stylized highlights using projective textures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE stylized highlight; projective texture; principal direction and
   curvature
AB We present a method to render and control stylized highlights using projective textures for real-time applications. For each vertex of a given 3D model, we compute principal directions and their corresponding principal curvatures in the preprocessing step. We find the maximum specular intensity point on the surface of the model and compute the principal directions and curvatures at the point by bilinear interpolation. The position, orientation, and projection frustum of a texture projector are determined by the principal directions and curvatures at the point. Using a simple culling method, we reduce the number of triangles to compute the principal directions and curvatures. We can control the shape, size, position, and orientation of a stylized highlight in an easy, fast, and intuitive manner. We discuss the limitations of our method and also give approximate solutions for some limitations.
C1 Ajou Univ, Dept Digital Media, Suwon 443749, South Korea.
   Ajou Univ, Grad Sch Informat & Commun, Suwon 443749, South Korea.
C3 Ajou University; Ajou University
RP Choi, JJ (corresponding author), Ajou Univ, Dept Digital Media, Suwon 443749, South Korea.
EM jungju@ajou.ac.kr; fcnature@ajou.ac.kr
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Anjyo K, 2003, IEEE COMPUT GRAPH, V23, P54, DOI 10.1109/MCG.2003.1210865
   ANJYO KI, 2006, NPAR 06, P133
   Blinn JF, 1977, P 4 ANN C COMP GRAPH, P192, DOI [10.1145/563858.563893, DOI 10.1145/563858.563893]
   Cohen-Steiner D., 2003, SCG'03: Proceedings ofthe nineteenth annual symposium on Computational geometry, P312
   Correa W.T., 1998, INT C COMPUTER GRAPH, P435
   DEBEVEC PE, 1998, P 9 EUR REND WORKSH
   Decaudin Philippe, 1996, Research Report 2919
   DIEPSTRATEN J, 2004, P GI WORKSH METH WER
   GOOCH A, 1998, P 25 ANN C COMP GRAP, P447, DOI DOI 10.1145/280814.280950
   Lake A., 2000, Proceedings of the 1st International Symposium on Non-Photorealistic Animation and Rendering New York, NY, USA,, P13
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Porquet D., 2005, Proceedings of the 3rd International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia, GRAPHITE'05, P213, DOI DOI 10.1145/1101389.1101432
   SEGAL M, 1992, P SIGGRAPH, P249, DOI DOI 10.1145/133994.134071
   Winnemöller H, 2002, COMPUT GRAPH FORUM, V21, P309, DOI 10.1111/1467-8659.00590
   Yu YZ, 1999, COMPUT GRAPH-UK, V23, P245, DOI 10.1016/S0097-8493(99)00034-5
NR 16
TC 0
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 805
EP 813
DI 10.1007/s00371-006-0070-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000023
DA 2024-07-18
ER

PT J
AU Lim, CW
   Tan, TS
AF Lim, Chi-Wan
   Tan, Tiow-Seng
TI Surface reconstruction by layer peeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE surface reconstruction; mesh generation; geometric modeling; sampling;
   scattered data
ID ALGORITHM
AB Given an input point cloud P in R-3, this paper proposes a novel algorithm to identify surface neighbors of each point p is an element of P respecting the underlying surface S and then to construct a piecewise linear surface for P. The algorithm utilizes the simple k-nearest neighborhood in constructing local surfaces. It makes use of two concepts: a local convexity criterion to extract a set of surface neighbors for each point, and a global projection test to determine an order for the reconstruction. Our algorithm not only produces a topologically correct surface for well-sampled point sets, but also adapts well to handle under-sampled point sets. Furthermore, the computational cost of the algorithm increases almost linearly in the size of the point cloud. It, thus, scales well to deal with large input point sets.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
C3 National University of Singapore
RP Lim, CW (corresponding author), Natl Univ Singapore, Sch Comp, 3 Sci Dr 2, Singapore 117543, Singapore.
EM limchiwa@comp.nus.edu.sg; tants@comp.nus.edu.sg
CR ALEXA M, 2004, P EUR S POINT BAS GR, P150
   Alexander M, 2001, INTERNETWEEK, P21
   Amenta N, 2004, ACM T GRAPHIC, V23, P264, DOI 10.1145/1015706.1015713
   Amenta N., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P39, DOI 10.1145/276884.276889
   Amenta N., 2000, PROC 16 ACM ANN S CO, P213
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   ANDERSSON M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P167
   [Anonymous], P 28 ANN C COMP GRAP, DOI DOI 10.1145/383259.383266
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   ATTALI D, 1998, P 14 ANN S COMP GEOM, P39
   BERN M, 1993, DISCRETE COMPUT GEOM, V10, P47, DOI 10.1007/BF02573962
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   CHENG SW, 1999, P 15 ACM S COMP GEOM, P1
   Dey T.K., 2003, P 8 ACM S SOL MOD AP, P127, DOI DOI 10.1145/781606.781627
   Dey T.K., 2004, P 20 ANN S COMPUTATI, P330
   Dey T.K., 2001, P 17 ANN S COMPUTATI, P257, DOI [DOI 10.1145/378583.378682, 10.1145/378583.378682]
   Dey TK, 2003, DISCRETE COMPUT GEOM, V29, P419, DOI 10.1007/s00454-002-2838-9
   Funke S, 2002, SIAM PROC S, P781
   Giesen J., 2003, Proc. 19th Ann. Sympos. Comput. Geom, P329
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Levin D, 2004, MATH VISUAL, P37
   LINSEN L, 2003, P DAGST SEM 02151 TH
   Mederos B., 2005, Proc. Geometry Processing (Eurographics/ ACM SIGGRAPH), P53
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   MOUNT DM, 2005, ANN LIB APPROXIMATE
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   REVELLES J, 2000, P WSCG, P212
   SCHEIDEGGER CE, 2005, P S GEOM PROC, P63
   Xie H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P259, DOI 10.1109/VISUAL.2004.101
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
NR 31
TC 3
Z9 5
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 593
EP 603
DI 10.1007/s00371-006-0048-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000002
DA 2024-07-18
ER

PT J
AU Wang, JP
   Xu, K
   Zhou, K
   Lin, S
   Hu, SM
   Guo, BN
AF Wang, Jiaping
   Xu, Kun
   Zhou, Kun
   Lin, Stephen
   Hu, Shimin
   Guo, Baining
TI Spherical harmonics scaling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE signal processing; spherical harmonics; illumination; rendering
AB In this paper, we present a new SH operation, called spherical harmonics scaling, to shrink or expand a spherical function in the frequency domain. We show that this problem can be elegantly formulated as a linear transformation of SH projections, which is efficient to compute and easy to implement on a GPU. Spherical harmonics scaling is particularly useful for extrapolating visibility and radiance functions at a sample point to points closer to or farther from an occluder or light source. With SH scaling, we present applications to low-frequency shadowing for general deformable object, and to efficient approximation of spherical irradiance functions within a mid-range illumination environment.
C1 Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
   Chinese Acad Sci, Grad Sch, Chinese Acad Sci, Inst Comp Technol, Beijing 100864, Peoples R China.
   Tsinghua Univ, Beijing 100084, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Institute of Computing
   Technology, CAS; Tsinghua University
RP Zhou, K (corresponding author), Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
EM e_boris2002@hotmail.com; xu-k@mails.tsinghua.edu.cn;
   kunzhou@microsoft.com; stevelin@microsoft.com; shimin@tsinghua.edu.cn;
   bainguo@microsoft.com
RI Zhou, Kun/AAH-9290-2019; Zhou, Kun/ABF-4071-2020; Xu, Kun/K-7134-2012;
   Hu, Shi-Min/AAW-1952-2020
OI Zhou, Kun/0000-0003-2320-3655; 
CR Annen Thomas., 2004, EUROGRAPHICS SYMPOSI, P331
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   REN Z, 2006, SIGGRAPH 06
   Sloan Peter-Pike., 2002, Siggraph'02: Proceedings of the 29th annual conference on computer graphics and interactive techniques, P527
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Zhou K, 2005, ACM T GRAPHIC, V24, P1148, DOI 10.1145/1073204.1073325
NR 8
TC 9
Z9 11
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 713
EP 720
DI 10.1007/s00371-006-0057-8
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000014
DA 2024-07-18
ER

PT J
AU Shamir, A
   Shapira, L
   Cohen-Or, D
AF Shamir, A
   Shapira, L
   Cohen-Or, D
TI Mesh analysis using geodesic mean-shift
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 5th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY OCT 11-12, 2004
CL Seoul Natl Univ, Seoul, SOUTH KOREA
HO Seoul Natl Univ
DE mean-shift; meshes; segmentation; feature extraction
ID DENSITY-FUNCTION; DECOMPOSITION; SHAPE
AB In this paper, we introduce a versatile and robust method for analyzing the feature space associated with a given mesh surface. The method is based on the mean-shift operator, which was shown to be successful in image and video processing. Its strength lies in the fact that it works in a single joint space of geometry and attributes called the feature-space. The mean-shift procedure works as a gradient ascend finding maxima of an estimated probability density function in feature-space. Our method for using the mean-shift technique on surfaces solves several difficulties. First, meshes as opposed to images do not present a regular and uniform sampling of domain. Second, on surface meshes the shifting procedure must be constrained to stay on the surface and preserve geodesic distances. We define a special local geodesic parameterization scheme, and use it to generalize the mean-shift procedure to unstructured surface meshes. Our method can support piecewise linear attribute definitions as well as piecewise constant attributes.
C1 Tel Aviv Univ, Efi Arazi Sch Comp Sci, Interdisciplinary Ctr, IL-69978 Tel Aviv, Israel.
   Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
C3 Reichman University; Tel Aviv University; Tel Aviv University
RP Tel Aviv Univ, Efi Arazi Sch Comp Sci, Interdisciplinary Ctr, IL-69978 Tel Aviv, Israel.
EM arik@idc.ac.il; lior@liors.net; dcor@tau.ac.il
CR [Anonymous], 1987, 3 DIMENSIONAL MACHIN
   [Anonymous], STAT METH VID PROC W
   Arabie R., 1996, Clustering and Classification
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Collins R., 2003, Computer Vision and Pattern Recognition
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D., 2000, COMPUTER VISION PATT
   DEROSE T, 1998, ANN C SERIES, P85
   Erickson J., 2002, P 18 ANN S COMP GEOM, P244
   FLOATER M, 1995, COMPUT AIDED GEOM D, V14, P231
   Floater M.S., 2005, ADV MULTIRESOLUTION
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Garland M., 2001, P ACM S INT 3D GRAPH
   Gordon A.D., 1996, CLUSTERING CLASSIFIC, P65, DOI [10.1142/1930, DOI 10.1142/1930]
   Guralnik V., 2001, Workshop on Data Mining in Bioinformatics, P73
   INOUE K, 2001, COMPUTER AIDED DESIG, V33
   KALVIN AD, 1996, IEEE COMPUT GRAPH AP, V16
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   LEVY B, 1998, P SIGGRAPH 98, P343
   Mangan A.P., 1998, P IEEE VIS 1998 LAT
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   PULLA S, 2001, THESIS ARIZONA STATE
   Roberts SJ, 1997, PATTERN RECOGN, V30, P261, DOI 10.1016/S0031-3203(96)00079-9
   SANDER PV, 2003, P EUR ACM SIGGRAPH S, P146
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Shamir A, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P185, DOI 10.1109/VISUAL.2003.1250371
   SHAPIRA L, 2005, MATH FDN SCI VIS COM
   Sheffer A, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P291, DOI 10.1109/VISUAL.2002.1183787
   Sheffer A, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P61, DOI 10.1109/SMI.2002.1003529
   Sheffer Alla., 2000, P 9 INT MESHING ROUN, P161
   SHLAFMAN S, 2002, COMP GRAPH FOR P EUR, V21
   SORKINE O, 2002, P IEEE VISUALIZATION
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   YU B, 2003, P 8 INT C INT US INT, P204
   Zhuang XH, 1996, IEEE T IMAGE PROCESS, V5, P1293, DOI 10.1109/83.535841
   Zigelman G, 2002, IEEE T VIS COMPUT GR, V8, P198, DOI 10.1109/2945.998671
NR 43
TC 12
Z9 18
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 99
EP 108
DI 10.1007/s00371-006-0370-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 018GC
UT WOS:000235751200005
DA 2024-07-18
ER

PT J
AU Wu, FC
   Ma, WC
   Liang, RH
   Chen, BY
   Ouhyoung, M
AF Wu, FC
   Ma, WC
   Liang, RH
   Chen, BY
   Ouhyoung, M
TI Domain connected graph: the skeleton of a closed 3D shape for animation
SO VISUAL COMPUTER
LA English
DT Article
DE skeleton; medial axis transform; domain connected graph
ID REPRESENTATION; POINTS; SET
AB In previous research, three main approaches have been employed to solve the skeleton extraction problem: medial axis transform (MAT), generalized potential field and decomposition-based methods. These three approaches have been formulated using three different concepts, namely surface variation, inside energy distribution, and the connectivity of parts. By combining the above mentioned concepts, this paper creates a concise structure to represent the control skeleton of an arbitrary object.
   First, an algorithm is proposed to detect the end, connection and joint points of an arbitrary 3D object. These three points comprise the skeleton, and are the most important to consider when describing it. In order to maintain the stability of the point extraction algorithm, a prong-feature detection technique and a level iso-surfaces function-based on the repulsive force field was employed. A neighborhood relationship inherited from the surface able to describe the connection relationship of these positions was then defined. Based on this relationship, the skeleton was finally constructed and named domain connected graph (DCG). The DCG not only preserves the topology information of a 3D object, but is also less sensitive than MAT to the perturbation of shapes. Moreover, from the results of complicated 3D models, consisting of thousands of polygons, it is evident that the DCG conforms to human perception.
C1 Natl Taiwan Univ, Taipei 10764, Taiwan.
C3 National Taiwan University
RP Natl Taiwan Univ, Taipei 10764, Taiwan.
EM joyce@cmlab.csie.ntu.edu.tw; firebird@cmlab.csie.ntu.edu.tw;
   liang@mcu.edu.tw; robin@ntu.edu.tw; ming@csie.ntu.edu.tw
RI ; Chen, Bing-Yu/E-7498-2016
OI OUHYOUNG, MING/0000-0002-3038-6958; Liang,
   Rung-Huei/0000-0002-7294-8154; Chen, Bing-Yu/0000-0003-0169-7682
CR Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   Attali D, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P13, DOI 10.1109/ICIP.1996.560357
   BIASOTTI S, 2004, SURFACE TOPOLOGICAL, P87
   Bitter I, 2001, IEEE T VIS COMPUT GR, V7, P195, DOI 10.1109/2945.942688
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Borgefors G, 1997, PATTERN RECOGN LETT, V18, P465, DOI 10.1016/S0167-8655(97)00027-5
   Borgefors G, 1999, PATTERN RECOGN, V32, P1225, DOI 10.1016/S0031-3203(98)00082-X
   Bradshaw G, 2004, ACM T GRAPHIC, V23, P1, DOI 10.1145/966131.966132
   Capell S, 2002, ACM T GRAPHIC, V21, P586, DOI 10.1145/566570.566622
   Choi HI, 1997, PAC J MATH, V181, P57, DOI 10.2140/pjm.1997.181.57
   CHOI SW, 2001, LECT NOTES COMPUTER, V2191, P132
   Choi WP, 2003, PATTERN RECOGN, V36, P721, DOI 10.1016/S0031-3203(02)00098-5
   Chuang JH, 2000, IEEE T PATTERN ANAL, V22, P1241
   Culver T, 1999, P 5 ACM S SOL MOD AP, P179, DOI DOI 10.1145/304012.304030
   Foskey M., 2003, J. Comput. Inf. Sci. Eng., V3, P274, DOI DOI 10.1145/781606.781623
   Giblin P, 2004, IEEE T PATTERN ANAL, V26, P238, DOI 10.1109/TPAMI.2004.1262192
   Grigorishin T, 1998, PATTERN ANAL APPL, V1, P163, DOI 10.1007/BF01259366
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hubbard PM, 1996, ACM T GRAPHIC, V15, P179, DOI 10.1145/231731.231732
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Kimia BB, 2003, J PHYSIOL-PARIS, V97, P155, DOI 10.1016/j.jphysparis.2003.09.003
   KIMIA F, 1999, ACM S SOL MOD APPL, P130
   LEYMARIE F, 1992, IEEE T PATTERN ANAL, V14, P56, DOI 10.1109/34.107013
   Leymarie FF, 2004, INT C PATT RECOG, P123, DOI 10.1109/ICPR.2004.1334484
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   Ma WC, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P207
   MAYYA N, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P638, DOI 10.1109/CVPR.1994.323787
   Mortara M, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P245, DOI 10.1109/SMI.2002.1003552
   Nilsson F, 1997, GRAPH MODEL IM PROC, V59, P55, DOI 10.1006/gmip.1996.0412
   Ogniewicz R., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P63, DOI 10.1109/CVPR.1992.223226
   Ogniewicz R. L., 1995, 954 HARV ROB LAB
   Palenichka RM, 2002, INT C PATT RECOG, P143, DOI 10.1109/ICPR.2002.1044633
   PIZER SM, 1994, P IEEE C COMP VIS PA, P638
   SAVCHENKO VV, 1995, COMPUT GRAPH FORUM, V14, P181, DOI 10.1111/1467-8659.1440181
   Sheehy DJ, 1996, IEEE T VIS COMPUT GR, V2, P62, DOI 10.1109/2945.489387
   Sherbrooke E. C., 1995, Proceedings. Third Symposium on Solid Modeling and Applications, P187, DOI 10.1145/218013.218059
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   Siddiqi K, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P222, DOI 10.1109/ICCV.1998.710722
   Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653
   TAM R, 2002, EUR C COMP VIS, P672
   Verroust A, 2000, VISUAL COMPUT, V16, P15, DOI 10.1007/PL00007210
   Wade L, 2002, VISUAL COMPUT, V18, P97, DOI 10.1007/s003710100139
   Wolter F. E., 1993, Cut locus and medial axis in global shape interrogation and representation
   WU FC, 2004, ACM SIGGRAPH 2004 SK
   Wyvill G, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P2, DOI 10.1109/SMA.2001.923369
   Zhou Y, 1999, IEEE T VIS COMPUT GR, V5, P196, DOI 10.1109/2945.795212
   Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1158, DOI 10.1109/34.809109
NR 48
TC 25
Z9 36
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 117
EP 135
DI 10.1007/s00371-005-0357-4
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 018GC
UT WOS:000235751200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, BY
   Ono, Y
   Nishita, T
AF Chen, BY
   Ono, Y
   Nishita, T
TI Character animation creation using hand-drawn sketches
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE cel animation; nonphotorealistic rendering; 3D morphing; consistent mesh
   parameterization; sketches
AB To create a character animation, a 3D character model is often needed. However, since humanlike characters are not rigid bodies, to deform the character model to fit each animation frame is tedious work. Therefore, we propose an easy-to-use method for creating a set of consistent 3D character models from some hand-drawn sketches while keeping the projected silhouettes and features of the created models consistent with the input sketches. Since the character models possess vertexwise correspondences, they can be used for frame-consistent texture mapping or for making character animations. In our system, the user only needs to annotate the correspondence of the features among the input-vector-based sketches; the remaining processes are all performed automatically.
C1 Natl Taiwan Univ, Dept Informat Management, Taipei 10764, Taiwan.
   Univ Tokyo, Dept Complex Sci & Engn, Tokyo, Japan.
   Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10764, Taiwan.
C3 National Taiwan University; University of Tokyo; National Taiwan
   University
RP Natl Taiwan Univ, Dept Informat Management, Taipei 10764, Taiwan.
EM robin@ntul.edu.tw; ono@nis-lab.is.s.u-tokyo.ac.jp;
   nis@is.s.u-tokyo.ac.jp
RI Chen, Bing-Yu/E-7498-2016
OI Chen, Bing-Yu/0000-0003-0169-7682
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   [Anonymous], 1995, P 22 ANN C COMP GRAP, DOI DOI 10.1145/218380.218417
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Correa W.T., 1998, INT C COMPUTER GRAPH, P435
   Freitag L.A., 1995, P 4 INT MESHING ROUN, P47
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Kalnins RD, 2003, ACM T GRAPHIC, V22, P856, DOI 10.1145/882262.882355
   Kalnins RD, 2002, ACM T GRAPHIC, V21, P755, DOI 10.1145/566570.566648
   Kanai T, 2000, IEEE COMPUT GRAPH, V20, P62, DOI 10.1109/38.824544
   Karpenko O, 2002, COMPUT GRAPH FORUM, V21, P585, DOI 10.1111/1467-8659.t01-1-00709
   Kraevoy V, 2003, ACM T GRAPHIC, V22, P326, DOI 10.1145/882262.882271
   LI Y, 2003, P 2003 ACM SIGGRAPH, P309
   MARTIN D, 2000, P 1 INT S NONPH AN R, P75
   Michikawa T, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P60, DOI 10.1109/PCCGA.2001.962858
   Northrup J. D., 2000, Proceedings of the 1st International Symposium on Non-photorealistic Animation and Rendering, P31, DOI DOI 10.1145/340916.340920
   Ono Y, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P210, DOI 10.1109/CW.2004.1
   Petrovic L, 2000, COMP GRAPH, P511, DOI 10.1145/344779.345073
   Rademacher P, 1999, COMP GRAPH, P439, DOI 10.1145/311535.311612
   SINGH K., 1998, SIGGRAPH 98, P405, DOI DOI 10.1145/280814.280946
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
   TORRESANI L, 2003, P NIPS, P577
   Winnemöller H, 2002, COMPUT GRAPH FORUM, V21, P309, DOI 10.1111/1467-8659.00590
NR 22
TC 9
Z9 15
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 551
EP 558
DI 10.1007/s00371-005-0333-z
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ren, Z
   Hua, W
   Chen, L
   Bao, HJ
AF Ren, Z
   Hua, W
   Chen, L
   Bao, HJ
TI Intersection fields for interactive global illumination
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE global illumination; shading; hardware; photon tracing; global line
AB We present a novel visibility representation, Intersection Field (i-Field), to compute global illumination in interactive rates. The i-Field provides fast visibility and line-scene intersection queries. We factorize the direct illumination into local irradiance and visibility ratio. The latter is efficiently evaluated by querying the i-Field. The indirect illumination is simulated by photon tracing, which is also accelerated by the i-Field. By quickly detecting invalid portions, our approach can handle highly dynamic scenes, allowing light sources and scene geometries to be manipulated at interactive rates through rigid transformations and free deformations.
C1 Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM renzhong@cad.zju.edu.cn; huawei@cad.zju.edu.cn; chenlu@cad.zju.edu.cn;
   bao@cad.zju.edu.cn
CR ALEXANDER K, 1996, EGWR, P101
   ALEXANDER K, 1997, GRAPHITE, P49
   BENT L, 2004, EGWR, P123
   BRIAN E, 1992, ACM SIGGRAPH, P273
   BRUCE W, 2002, EGWR, P37
   BRUCE W, 1999, EGWR, V10, P235
   BRUCE W, 1997, ACM SIGGRAPH, P45
   CARSTEN B, 2003, COMPUT GRAPH FORUM, V22, P621
   CHRIS B, 1989, ACM SIGGRAPH, V23, P89
   CYRILLE D, 1999, EGWR, P235
   CYRILLE D, 2003, COMPUT GRAPH FORUM, V22, P55
   DAVID G, 1990, IEEE CG A, V10, P26
   FRANCESC C, 2004, COMPUT GRAPH FORUM, V23, P43
   FRANK S, 1999, EGWR, P225
   FREDO D, 1999, THESIS U J FOURIER G
   GENE G, 1998, IEEE CG A, V18, P32
   GEORGE D, 1997, ACM SIGGRAPH, P57
   GONZALO B, 2001, J VISUAL COMP ANIMAT, V12, P93
   GONZALO B, 1996, EGWR, P185
   GREGORY W, 1999, ACM TOG, V18, P361
   HECTOR Y, 2001, ACM TOG, V20, P39
   INGO W, 2001, COMPUT GRAPH FORUM, V20, P153
   JEFFRY N, 1996, IEEE T VISUALIZATION, V2, P283
   JENSEN HW, 1996, EUR REND WORKSH 1996, P21
   JOHANNES G, 2004, EGWR, P109
   KAROL M, 2001, ACM SIGGRAPH, P221
   KAVITA B, 1999, ACM TOG, V18, P213
   KIRILL D, 2002, EGWR, P25
   LARRY A, 1993, GRAPHITE, P155
   LASZLO K, 1998, EGWR, P247
   LASZLO K, 2003, EGWR, P64
   LASZLO K, 1999, COMPUT GRAPH FORUM, V18, P233
   MARC S, 2000, COMPUT GRAPH FORUM, V19, P13
   MARK S, 2000, EGWR, P377
   MARTIN I, 2003, IEEE T VISUALIZA JAN, P70
   MARYANN S, 2000, EGWR, P329
   PARAG T, 2002, GRAPHITE, P537
   PER C, 2004, EGWR, P132
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Pueyo X, 1997, J VISUAL COMP ANIMAT, V8, P221, DOI 10.1002/(SICI)1099-1778(199710/12)8:4<221::AID-VIS169>3.0.CO;2-F
   RAFAL M, 2002, ICCVG, P25
   RUI B, 1999, ACM S INT 3D GRAPH, P183
   Sbert M, 2004, COMPUT GRAPH FORUM, V23, P291, DOI 10.1111/j.1467-8659.2004.00760.x
   Sbert M, 1996, VISUAL COMPUT, V12, P47
   SETH T, 1996, EGWR, P257
   SHENCHANG C, 1991, GRAPHITE, P165
   SHENCHANG C, 1990, GRAPHITE, P135
   STEVE P, 1999, ACM S INT 3D GRAPH, P119
   TIMOTHY P, 2003, SIGGRAPH EUROGRAPHIC, P41
   TOMMER L, 2003, ACM SIGGRAPH, V22, P595
   VALDIMIR V, 2000, ACM TOG, V19, P122
   WILLIAM M, 1997, S INT 3D GRAPH, P7
   WILLIAM S, 2004, ACM TOG, V23, P742
   WOLFGANG S, 1997, EGWR, P93
   XAVIER G, 2001, COMPUT GRAPH FORUM, V20, P268
NR 55
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 569
EP 578
DI 10.1007/s00371-005-0329-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400009
DA 2024-07-18
ER

PT J
AU Jiménez, JR
   Pueyo, X
AF Jiménez, JR
   Pueyo, X
TI Interactive rendering of globally illuminated scenes including
   anisotropic and inhomogeneous participating media
SO VISUAL COMPUTER
LA English
DT Article
DE interactive rendering; global illumination; graphics hardware;
   participating media
AB Although new graphics hardware has accelerated the rendering process, the realistic simulation of scenes including participating media remains a difficult problem. Interactive results have been achieved for isotropic media as well as for single scattering. In this paper, we present an interactive global illumination algorithm for the simulation of scenes that include participating media, even anisotropic and/or inhomogeneous media. The position of the observer is important in order to render inhomogeneous media according to the transport equation. Previous work normally needed to be ray-based in order to compute this equation properly. Our approach is capable of achieving real time using two 3D textures on a simple desktop PC. For anisotropic participating media we combine density estimation techniques and graphics hardware capabilities.
C1 Univ Jaen, Dept Informat, GGGJ, Jaen 23071, Spain.
   Univ Girona, IIiA, GGG, Girona 17003, Spain.
C3 Universidad de Jaen; Universitat de Girona; Consejo Superior de
   Investigaciones Cientificas (CSIC); CSIC - Instituto de Investigacion en
   Inteligencia Artificial (IIIA)
RP Univ Jaen, Dept Informat, GGGJ, Campus Lagunillas, Jaen 23071, Spain.
EM rjimenez@ujaen.es; xavier@ima.udg.es
RI Pueyo, Xavier/L-3302-2017
OI Pueyo, Xavier/0000-0002-3622-583X; Jimenez-Perez,
   Juan-Roberto/0000-0002-1233-2294
CR [Anonymous], 2002, PROC ACM T GRAPH SIG, DOI DOI 10.1145/566570.566574
   Arvo JR, 1993, Global illumination, ACM SIGGRAPH courses, P1
   BHATE N, 1993, P ATARV 93 ADV TECHN, P43
   BHATE N, 1992, 3 EUR WORKSH REND, P227
   BIRI V, 2002, 13 EUR WORKSH REND P
   Blasi P., 1993, Computer Graphics Forum, V12, pC201, DOI 10.1111/1467-8659.1230201
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Cerezo E, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P481
   DOBASHI Y, 2002, SIGGRAPH EUR WORKSH, P99
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   Harris MJ, 2001, COMPUT GRAPH FORUM, V20, pC76, DOI 10.1111/1467-8659.00500
   JEMENEZ JR, 2003, EUR C GRAN SPAIN SEP
   Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925
   JENSEN HW, 1996, 7 EG REND WORKSH REN, P21
   JIMENEZ JR, 1999, MGFE MAT GEOMETRY FO
   Kipfer P., 2004, HWWS 04, P115, DOI [10.1145/1058129.1058146, DOI 10.1145/1058129.1058146]
   Lacroute P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P451, DOI 10.1145/192161.192283
   LAFORTUNE EP, 1996, 7 EUR WORKSH REND PO, P91
   LANGUENOU E, 1994, 5 EUR WORKSH REND DA, P69
   MA VCH, 2002, SIGGRAPH EUR WORKSH, P89
   Martín I, 1998, COMPUT GRAPH, V22, P601, DOI 10.1016/S0097-8493(98)00066-1
   MYSZKOWSKI K, 1997, 8 EUR WORKSH REND SA, P251
   PATTANAIK SN, 1993, J VISUAL COMP ANIMAT, V4, P133, DOI 10.1002/vis.4340040303
   PAULY M, 2000, 11 EUR WORKSH REND B, P11
   PEREZ F, 2000, P PAC GRAPH 2000 HON
   PEREZ F, 1997, EUR REND WORKSH 1997, P309
   PEREZ F, 2003, THESIS U POLITECNICA
   PURCELL TJ, 2003, SIGGRAPH EUR WORKSH, P41
   RUSHMEIER H, 1994, 5 EUR WORKSH REND, P35
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   Siegel R., 2001, Thermal Radiation Heat Transfer, V4th
   SILLION FX, 1995, IEEE T VIS COMPUT GR, V1, P240, DOI 10.1109/2945.466719
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   SOBIERAJSKI L, 1994, THESIS STATE U NEW Y
   STAM J, 1995, 6 EUR WORKSH REND DU, P41
   STAMMINGER M, 2000, 11 EUR WORKSH REND B, P377
   STURZLINGER W, 1997, 8 EG WORKSH REND REN, P93
   TOBLER R, 1997, 8 EUR WORKSH REND JU, P193
   VanGelder A, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P23, DOI 10.1109/SVV.1996.558039
   WARD G, 1996, MAT GEOMETRY FORMAT
NR 41
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2005
VL 21
IS 7
BP 449
EP 462
DI 10.1007/s00371-005-0300-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GE
UT WOS:000230991200002
DA 2024-07-18
ER

PT J
AU Cerezo, E
   Pérez, F
   Pueyo, X
   Seron, FJ
   Sillion, FX
AF Cerezo, E
   Pérez, F
   Pueyo, X
   Seron, FJ
   Sillion, FX
TI A survey on participating media rendering techniques
SO VISUAL COMPUTER
LA English
DT Review
DE three-dimensional graphics and realism; shading; radiosity; ray-tracing;
   participating medium; global illumination
ID GLOBAL ILLUMINATION; ATMOSPHERE; SCATTERING
AB Rendering participating media is important for a number of domains, ranging from commercial applications (entertainment, virtual reality) to simulation systems (driving, flying, and space simulators) and safety analyses (driving conditions, sign visibility). This article surveys global illumination algorithms for environments including participating media. It reviews both appearance-based and physically-based media methods, including the single-scattering and the more general multiple-scattering techniques. The objective of the survey is the characterization of all these methods: identification of their base techniques, assumptions, limitations, and range of utilization. It concludes with some reflections about the suitability of the methods depending on the specific application involved, and possible future research lines.
C1 Univ Zaragoza, Adv Comp Graph Grp, GIGA, Dept Comp Sci, E-50009 Zaragoza, Spain.
C3 University of Zaragoza
RP Univ Zaragoza, Adv Comp Graph Grp, GIGA, Dept Comp Sci, E-50009 Zaragoza, Spain.
EM ecerezo@unizar.es; fredericpcx@terra.es; xavier.pueyo@ima.udg.es;
   seron@unizar.es; francois.sillion@imag.fr
RI Pueyo, Xavier/L-3302-2017; Cerezo, Eva/L-6095-2014; Seron Arbeloa,
   Francisco Jose/L-3146-2014
OI Pueyo, Xavier/0000-0002-3622-583X; Cerezo, Eva/0000-0003-4424-0770;
   Seron Arbeloa, Francisco Jose/0000-0003-1683-4694
CR Adabala N, 2000, J VISUAL COMP ANIMAT, V11, P279, DOI 10.1002/1099-1778(200012)11:5<279::AID-VIS234>3.0.CO;2-P
   AKELEY K, 2001, CS448A U STANF
   Akenine-Moller T., 2003, Real-Time Rendering
   AKENINEMOLLER T, 2002, PRACTICAL PARALLEL R
   [Anonymous], 1994, Radiosity and global illumination
   Arques D., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P143
   Bhate N., 1992, Proceedings of the Third Eurographics Workshop on Rendering, P227
   BHATE N., 1993, Advanced Techniques in Animation, Rendering and Visualization, P43
   BIRI V, 2002, RENDERING TECHNIQUES, P9
   BLASI P, 1993, P EUROGRAPHICS 93, V12, pC201
   Blasi P., 1994, P EUROGRAPHICS WORKS, P173
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Bohren C.F., 1993, Absorption and Scattering of Light by Small Particles, p530 pp
   BOHREN CF, 1987, AM J PHYS, V55, P524, DOI 10.1119/1.15109
   CEREZO E, 2002, ADV MODELLING ANIMAT
   CHANDRASEKHAR S, 1960, RADIATIVE TRANSFER
   CHRISTENSEN PH, 1995, THESIS U WASHINGTON
   DEVLIN K, 2002, P EUR 2002 STAT ART
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   DOBASHI Y., 2002, GRAPHICS HARDWARE, P99
   DUMONT E, 1998, LECT NOTES COMPUTATI
   DUTRE P., 1993, Proceedings of Compugraphics '93, P128
   Ebert D. S., 1990, Computer Graphics, V24, P357, DOI 10.1145/97880.97918
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster I, 1995, DESIGNING BUILDING P
   Gardner G. Y., 1985, Computer Graphics, V19, P297, DOI 10.1145/325165.325248
   GEIST R, 2004, EUR S REND NORRK SWE, P355
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   Hanrahan P., 1993, Computer Graphics Proceedings, P165, DOI 10.1145/166117.166139
   HANRAHAN P, 1991, COMP GRAPH, V25, P197
   Harris MJ, 2001, COMPUT GRAPH FORUM, V20, pC76, DOI 10.1111/1467-8659.00500
   HARRIS MJ, 2002, P GAM DEV C 2002
   Inakage M., 1989, New Advances in Computer Graphics. Proceedings of CG International '89, P533
   IRWIN J, 1996, P 1996 EUR UK C, P103
   Jackel D, 1997, COMPUT GRAPH FORUM, V16, P201, DOI 10.1111/1467-8659.00180
   Jensen H. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P311, DOI 10.1145/280814.280925
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 1999, SPRING EUROGRAP, P273
   JENSEN HW, 2001, ANN C SERIES, P399
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kaneda K., 1991, Visual Computer, V7, P247, DOI 10.1007/BF01905690
   KATO T, 2002, RENDERING TECHNIQUES, P7
   KLASSEN RV, 1987, ACM T GRAPHIC, V6, P215, DOI 10.1145/35068.35071
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   Lafortune E. P., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P91
   LANGE T, 1998, 6781998 U DORTM FACH
   LANGUENOU E, 1994, 5 EUR WORKSH REND DA, P69
   LANGUENOU E, 1994, THESIS U RENNES
   LATHROP KD, 1968, NUCL SCI ENG, V32, P357, DOI 10.13182/NSE68-4
   LECOCQ P, 2000, AFIG 00
   LECOCQ P, 2002, P DRIV SIM C 2002 PA
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Max N. L., 1986, Computer Graphics, V20, P117, DOI 10.1145/15886.15899
   MAX NL, 1994, 5 EUR WORKSH REND, P87
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Nishida Toshisada, 1994, P373, DOI 10.1145/192161.192261
   Nishita T., 1993, Computer Graphics Proceedings, P175, DOI 10.1145/166117.166140
   Nishita T, 1997, COMPUT GRAPH FORUM, V16, pC357, DOI 10.1111/1467-8659.00173
   NISHITA T, 1996, P 4 PAC C COMP GRAPH, P66
   Nishita T., 1996, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, P379
   Nishita Tomoyuki., 1987, COMPUTER GRAPHICS P, V21, P303, DOI [10.1145/37401.37437, DOI 10.1145/37401.37437]
   PATMORE C, 1993, GRAPHICS DESIGN VISU, P59
   PATTANAIK SN, 1993, J VISUAL COMP ANIMAT, V4, P133, DOI 10.1002/vis.4340040303
   Pauly M, 2000, SPRING COMP SCI, P11
   Pérez F, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P425
   Perez F., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P309
   Pérez F, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P71, DOI 10.1109/PCCGA.2000.883883
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   PREETHAM AJ, 1999, ANN C SERIES, P91
   Premoze S, 2001, COMPUT GRAPH FORUM, V20, P189, DOI 10.1111/1467-8659.00548
   PREMOZE S, 2003, P EUR S REND, P52
   Premoze Simon., 2004, EUROGRAPHICS SYMPOSI, P363, DOI DOI 10.2312/EGWR/EGSR04/363-374
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   RILEY K, 2004, EUR S REND, P375
   ROYSAM B, 1993, P IEEE C IND APPL DE, P661
   RUSHMEIER H, 1994, 5 EUR WORKSH REND, P35
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   RUSHMEIER HE, 1988, THESIS CORNELL U ITH
   SAKAS G, 1990, EUROGRAPHICS 90, P519
   Sakas G., 1993, Visual Computer, V9, P200, DOI 10.1007/BF01901724
   SAKAS G, 1991, P EUR 91 AMST
   SAKAS G, 1992, P VIS 92 BOST
   Schaufler G., 1995, Proc. GI Workshop on Modeling, Virtual Worlds, P129
   Siegel R., 1992, Thermal Radiation Heat Transfer
   Sillion F, 1995, SPRING COMP SCI, P196
   SILLION FX, 1995, IEEE T VIS COMPUT GR, V1, P240, DOI 10.1109/2945.466719
   SMITS B, 1994, ANN C SERIES, P435
   SOBIERAJSKI L, 1994, THESIS STATE U NEW Y
   Stam J, 1995, SPRING COMP SCI, P41
   STAM J, 1994, GRAPH INTER, P51
   STAM J, 1993, ANN C SERIES, P369
   STAM J, 1995, ANN C SERIES, P129
   STAM J, 1995, THESIS U TORONTO
   Tadamura K., 1993, Computer Graphics Forum, V12, pC189, DOI 10.1111/1467-8659.1230189
   UHL F, 1997, GOVT AEROSPACE SIMUL, V29, P427
   van der Hulst H.C., 1981, LIGHT SCATTERING SMA
   VEACH E, 1997, COMPUTER GRAPHICS, V31, P65
   VEACH E, 1995, ANN C SERIES, P419
   WALD I, 2003, REALTIME RAY TRACING
   WATT M, 1999, COMPUT GRAPH, V24, P377
   Westover L., 1990, Computer Graphics, V24, P367, DOI 10.1145/97880.97919
   WILKIE A, 2004, EUR S REND NOORK SWE, P387
   Willis P. J., 1987, Computer Graphics Forum, V6, P35, DOI 10.1111/j.1467-8659.1987.tb00343.x
   Yaeger L., 1986, Computer Graphics, V20, P85, DOI 10.1145/15886.15895
NR 107
TC 100
Z9 117
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2005
VL 21
IS 5
BP 303
EP 328
DI 10.1007/s00371-005-0287-1
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 937OJ
UT WOS:000229935100003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Baranoski, GVG
   Krishnaswamy, A
   Kimmel, B
AF Baranoski, GVG
   Krishnaswamy, A
   Kimmel, B
TI Increasing the predictability of tissue subsurface scattering
   simulations
SO VISUAL COMPUTER
LA English
DT Article
DE biophysically-based rendering; phase function
ID LIGHT-SCATTERING; MODEL; SKIN; TRANSMISSION; REFLECTANCE; PHEOMELANIN;
   RADIATION; EUMELANIN; TEXTURE; MELANIN
AB Models of light interaction with matter usually rely on subsurface scattering approximations based on the use of phase functions - notably, the Henyey-Greenstein phase function and its variations. In this paper, we challenge the generalized use of these approximations, especially for organic materials, and propose the application of a data-oriented approach whenever reliable measured data is available. Our research is supported by comparisons involving the original measured data that motivated the use of phase functions in algorithmic simulations of tissue subsurface scattering. We hope that this investigation will help strengthen the biophysical basis required for the predictable rendering of organic materials.
C1 Univ Waterloo, Sch Comp Sci, Nat Phenomena Simulat Grp, Waterloo, ON N2L 3G1, Canada.
C3 University of Waterloo
RP Baranoski, GVG (corresponding author), Univ Waterloo, Sch Comp Sci, Nat Phenomena Simulat Grp, Waterloo, ON N2L 3G1, Canada.
RI Baranoski, Gladimir/A-1944-2008
CR ANDERSON RR, 1981, J INVEST DERMATOL, V77, P13, DOI 10.1111/1523-1747.ep12479191
   [Anonymous], ACM SIGGRAPH 2001 C
   [Anonymous], WAVE PROPAGATION SCA
   Arvo J., 1995, THESIS YALE U
   Baranoski G., 2004, LIGHT INTERACTION PL
   BRULS WAG, 1984, PHOTOCHEM PHOTOBIOL, V40, P485, DOI 10.1111/j.1751-1097.1984.tb04622.x
   BRULS WAG, 1982, PHOTOCHEM PHOTOBIOL, V36, P709, DOI 10.1111/j.1751-1097.1982.tb09493.x
   BRULS WAG, 1984, PHOTOCHEM PHOTOBIOL, V40, P231, DOI 10.1111/j.1751-1097.1984.tb04581.x
   Chedekel MR, 1995, MELANIN: ITS ROLE IN HUMAN PHOTOPROTECTION, P11
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Dunn A, 1996, IEEE J SEL TOP QUANT, V2, P898, DOI 10.1109/2944.577313
   FLOCK ST, 1989, IEEE T BIO-MED ENG, V36, P1162, DOI 10.1109/TBME.1989.1173624
   FURUTSU K, 1980, J OPT SOC AM, V70, P360, DOI 10.1364/JOSA.70.000360
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   Govaerts YM, 1996, APPL OPTICS, V35, P6585, DOI 10.1364/AO.35.006585
   GREENBERG DP, 1997, ANN C SERIES, P477
   HANRAHAN P, 1993, ANN C SERIES, P165
   Henyey LG, 1941, ASTROPHYS J, V93, P70, DOI 10.1086/144246
   Imai FH, 1996, J IMAGING SCI TECHN, V40, P422
   Jacques S. L., 1987, Lasers in the Life Sciences, V1, P309
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   KATTAWAR GW, 1975, J QUANT SPECTROSC RA, V15, P839, DOI 10.1016/0022-4073(75)90095-3
   Krishnaswamy A, 2004, COMPUT GRAPH FORUM, V23, P331, DOI 10.1111/j.1467-8659.2004.00764.x
   Krishnaswamy A., 2004, J GRAPHICS TOOLS, V9, P31
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Lalonde P, 1997, COMPUT GRAPH FORUM, V16, pC293, DOI 10.1111/1467-8659.00166
   Li ZJ, 1996, IEEE T GEOSCI REMOTE, V34, P264, DOI 10.1109/36.481911
   MA QL, 1990, IEEE T GEOSCI REMOTE, V28, P865, DOI 10.1109/TGRS.1990.1238684
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Mourant JR, 1998, APPL OPTICS, V37, P3586, DOI 10.1364/AO.37.003586
   Ng CSL, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P249, DOI 10.1109/CGI.2001.934681
   Nicodemus F. E., 1992, Geometrical Considerations and Nomenclature for Reflectance, P94
   Parsad D, 2003, BRIT J DERMATOL, V149, P624, DOI 10.1046/j.1365-2133.2003.05440.x
   PEDERSEN GD, 1976, BIOPHYS J, V16, P199, DOI 10.1016/S0006-3495(76)85681-0
   PHARR M, 2000, ANN C SERIES, P75
   Prahl S. A., 2017, Proceedings of the SPIE, V10305, DOI 10.1117/12.2283590
   Prahl S.A., 1988, LIGHT TRANSPORT TISS
   REYNOLDS LO, 1980, J OPT SOC AM, V70, P1206, DOI 10.1364/JOSA.70.001206
   Sardar DK, 1998, LASER MED SCI, V13, P106, DOI 10.1007/s101030050062
   Shimada M, 2001, PHYS MED BIOL, V46, P2385, DOI 10.1088/0031-9155/46/9/308
   Stam J, 2001, SPRING EUROGRAP, P39
   STEINKE JM, 1988, J OPT SOC AM A, V5, P813, DOI 10.1364/JOSAA.5.000813
   THODY AJ, 1991, J INVEST DERMATOL, V97, P340, DOI 10.1111/1523-1747.ep12480680
   Tsumura N, 2003, ACM T GRAPHIC, V22, P770, DOI 10.1145/882262.882344
   Tuchin V., 2000, TISSUE OPTICS
   TUCKER CJ, 1977, APPL OPTICS, V16, P635, DOI 10.1364/AO.16.000635
   UESUGI A, 1971, J QUANT SPECTROSC RA, V11, P797, DOI 10.1016/0022-4073(71)90056-2
   Van de Hulst H. C., 1980, Multiple Light Scattering
   Van de Hulst H.C., 1981, LIGHT SCATTERING SMA
   VANGEMERT MJC, 1989, IEEE T BIO-MED ENG, V36, P1146, DOI 10.1109/10.42108
   VRHEL MJ, 1994, COLOR RES APPL, V19, P4, DOI 10.1111/j.1520-6378.1994.tb00053.x
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   WILSON BC, 1983, MED PHYS, V10, P824, DOI 10.1118/1.595361
   WITT AN, 1977, ASTROPHYS J SUPPL S, V35, P1, DOI 10.1086/190463
   YOON G, 1989, APPL OPTICS, V28, P2250, DOI 10.1364/AO.28.002250
   YOON G, 1987, IEEE J QUANTUM ELECT, V23, P1721, DOI 10.1109/JQE.1987.1073224
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 64
TC 7
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2005
VL 21
IS 4
BP 265
EP 278
DI 10.1007/s00371-005-0288-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 928MD
UT WOS:000229274900005
DA 2024-07-18
ER

PT J
AU Serón, FJ
   Gutiérrez, D
   Magallón, J
   Ferragut, L
   Asensio, MI
AF Serón, FJ
   Gutiérrez, D
   Magallón, J
   Ferragut, L
   Asensio, MI
TI The evolution of a WILDLAND forest FIRE FRONT
SO VISUAL COMPUTER
LA English
DT Article
DE fire; natural combustion; simulation; physics-based modeling; finite
   element method
ID NUMERICAL-SIMULATION; MODEL; SPREAD; LANDSCAPES
AB The rate of the spread and shape of a forest fire front is a problem that has not been thoughtfully studied from a computer graphics perspective. Here, using physically based computer graphics modeling, we propose a model for the simulation of wildland fires over 3D complex terrain. The model is based on conservation laws of energy and species, which includes radiation convection, reaction and natural convection, and takes into account the endothermic and exothermic phases of this kind of phenomenon. As an application, a simulation of a wildland fire in the Ebro basin of Spain is presented. The results are visualized on synthetic imagery, obtained by using the digital model of the studied terrain plus its corresponding images acquired by the Spot 4 and LandSat TM satellites.
C1 Univ Zaragoza, Zaragoza 50018, Spain.
   Ctr Politecn Super, Inst Invest Ingn Aragon, Grp Informat Graf, Zaragoza, Spain.
   Univ Salamanca, Dept Matemat Aplicada, E-37008 Salamanca, Spain.
C3 University of Zaragoza; University of Salamanca
RP Univ Zaragoza, C Maria de Luna 1, Zaragoza 50018, Spain.
EM seron@unizar.es; mas@gugu.usal.es
RI Asensio, Isabel/K-1425-2014; Ferragut, Luis/K-1972-2014; Magallón,
   Juan/L-3098-2014; Asensio, Isabel/JCD-9476-2023; Seron Arbeloa,
   Francisco Jose/L-3146-2014
OI Asensio, Isabel/0000-0002-8713-5594; Ferragut, Luis/0000-0001-5730-4281;
   Magallón, Juan/0000-0002-3355-0055; Asensio, Isabel/0000-0002-8713-5594;
   Seron Arbeloa, Francisco Jose/0000-0003-1683-4694
CR Ahrens J, 1997, VISUALIZATION '97 - PROCEEDINGS, P451, DOI 10.1109/VISUAL.1997.663919
   Albini F., 1976, COMPUTER BASED MODEL
   Albini F., 1976, INT30 USDA FOR SERV
   Anderson H.E., 1983, INT305 USDA FOR SERV
   Anderson H. E., 1982, INT122 USDA FOR SERV
   Andrews PL, 1989, INT260 USDA FOR SERV, DOI [10.2737/int-gtr-260, DOI 10.2737/INT-GTR-260]
   [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 1972, MATH MODEL PREDICTIN
   ANTONOVSKI AY, 1992, SYSTEMS ANAL GLOBAL, P373
   ASENSIO MI, 1998, THESIS U SALAMANCA
   BAKER WL, 1993, OIKOS, V66, P66, DOI 10.2307/3545196
   BARAFF D, 2003, ACM SIGGRAPH COMP GR, P862
   Bebernes J., 1989, MATH PROBLEMS COMBUS
   BOSSERT JE, 1998, 2 S FIR FOR MET AM M
   BUKOWSKI R, 1997, P SIGGRAPH 1997 ACM
   Burgan R.E., 1984, INT167 USDA FOR SERV
   CANDEL S, 1996, COMPUTATIONAL METHOD
   CATCHPOLE WR, 1985, 2185 U NEW S WAL FAC
   CERIMELE MM, 1991, COMPUT GRAPH, V15, P231, DOI 10.1016/0097-8493(91)90076-T
   CHIBA N, 1994, J VISUAL COMP ANIMAT, V5, P37, DOI 10.1002/vis.4340050104
   CHOU YH, 1992, INT J GEOGR INF SYST, V6, P123, DOI 10.1080/02693799208901900
   CHUVIESCA E, 1996, FUNDAMENTOS TELEDETE
   COX G, 1992, FIRE SCI TECHNOLOGY
   Cox G., 1995, Combustion Fundamentals of Fire
   DEVLIN K, 2001, AFRIGRAPH 2001 ACM S
   DIEZ ELG, 1994, J APPL METEOROL, V33, P519, DOI 10.1175/1520-0450(1994)033<0519:AOFMFT>2.0.CO;2
   FALERO JEM, 1996, MAPPING, V29, P51
   Ferragut L, 1996, COMPUTATIONAL FLUID DYNAMICS '96, P111
   FINNEY MA, 1993, P 12 INT C FIR FOR M
   FRANDSEN WH, 1971, COMBUST FLAME, V16, P9, DOI 10.1016/S0010-2180(71)80005-6
   GARDNER G, 1992, FRACTAL ELLIPSOID FI
   GARDNER RH, 1999, SPATIAL MODELING FOR
   Godunov S.K., 1979, Resolution numerique des problemes multi-dimensionnels de la dynamique des gaz
   GOKTEIN TG, 2004, P SIGGRAPH 2004 COMP
   GOVINDARAJAN J, 1999, IEEE VISUALIZATION 9
   GREEN DG, 1983, ECOL MODEL, V20, P33, DOI 10.1016/0304-3800(83)90030-3
   GUTIERREZ D, 2001, 13 C INT ING GRAF CI
   GWYNFOR DR, 1988, INT J NUMER METH ENG, V25, P625
   Hargrove WW, 2000, ECOL MODEL, V135, P243, DOI 10.1016/S0304-3800(00)00368-9
   Henderson TC, 2000, COMPUT SCI ENG, V2, P64, DOI 10.1109/5992.825750
   INAKAGE M, 1989, P COMP GRAPH INT 89
   INAKAGE M, 1991, SIGGRAPH 1991 COURSE, V27, P6
   IZBECKI S, 1989, INT88352COA US FOR S
   Keane RE, 1996, TREE PHYSIOL, V16, P319
   Kessell S. R., 1976, Environmental Management, V1, P39, DOI 10.1007/BF01867398
   KOURTZ PH, 1971, FOREST SCI, V17, P163
   KRAJEWSKI S, 1996, UNDERSTADING CONTOUR
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   MACKAY G, 1984, J PHYS A-MATH GEN, V17, pL757, DOI 10.1088/0305-4470/17/14/006
   MAGALLON J, 2000, 12 C INT ING GRAF CI
   Martin LD, 1997, J APPL METEOROL, V36, P705, DOI 10.1175/1520-0450-36.6.705
   Montenegro R, 1997, NONLINEAR ANAL-THEOR, V30, P2873, DOI 10.1016/S0362-546X(97)00341-6
   Neff M, 1999, PROC GRAPH INTERF, P193
   OHTSUKI T, 1986, J PHYS A-MATH GEN, V19, pL281, DOI 10.1088/0305-4470/19/5/012
   OREGAN WG, 1976, FOREST SCI, V22, P61
   PEACOCK RD, 1993, 1299 NIST
   PERLIN K, 1998, NOISE HYPERTEXTURE A, pCH9
   Perry CH, 1994, SYNTHESIZING FLAMES
   RASMUSSEN N, 2003, P SIGGRAPH 2003 COMP
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI 10.1145/964967.801167
   REISNER JM, 1994, J ATMOS SCI, V51, P117, DOI 10.1175/1520-0469(1994)051<0117:TFLFNF>2.0.CO;2
   RICHARDS GA, 1993, COMBUST SCI TECHNOL, V94, P57, DOI 10.1080/00102209308935304
   RICHARDS GD, 1988, INT J NUMER METH ENG, V25, P625, DOI 10.1002/nme.1620250222
   Rothermel R.C., 1983, How to predict the spread and intensity of forest and range fires, V143
   ROTHERMEL RC, 1984, INT167 USDA FOR SERV
   RUSHMEIER H, 1994, P 5 EUR WORKSH REND
   RUSHMEIER H, 1995, IEEE COMPUT GRAPH AP
   RZEZNIK MJ, 1999, INT C FIR RES ENG IC
   Sims K., 1990, Computer Graphics, V24, P405, DOI 10.1145/97880.97923
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   Stam J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P129, DOI 10.1145/218380.218430
   STAM J, 1994, GRAPH INTER, P51
   THALMMAN N, 1987, IMAGE SYNTHESIS
   TURNER MG, 1994, NAT AREA J, V14, P3
   Vasconcelos M. J., 1992, International Journal of Wildland Fire, V2, P87, DOI 10.1071/WF9920087
   WADE C, 1999, NEW ENG TOOL EVALUAT
   Wagner C. E. van, 1977, Canadian Journal of Forest Research, V7, P23, DOI 10.1139/x77-004
   Weber R. O., 1991, International Journal of Wildland Fire, V1, P245, DOI 10.1071/WF9910245
   Welch S, 1997, P 5 INT S FIR SAF SC
   Williams FA, 1985, COMBUSTION THEORY
   Yngve GD, 2000, COMP GRAPH, P29, DOI 10.1145/344779.344801
   Zel'dovich Y., 1985, Mathematical theory of combustion and explosions
NR 83
TC 14
Z9 17
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2005
VL 21
IS 3
BP 152
EP 169
DI 10.1007/s00371-004-0278-7
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 921UV
UT WOS:000228791700003
DA 2024-07-18
ER

PT J
AU Metoyer, RA
   Hodgins, JK
AF Metoyer, RA
   Hodgins, JK
TI Reactive pedestrian path following from examples
SO VISUAL COMPUTER
LA English
DT Article
DE animation; pedestrian simulation; reactive control
ID MOTION; LOCOMOTION; MODEL
AB Architectural and urban planning applications require animations of people to present an accurate and compelling view of a new environment. Ideally, these animations would be easy for a non-programmer to construct, just as buildings and streets can be modeled by an architect or artist using commercial modeling software. In this paper, we explore an approach for generating reactive path following based on the user's examples of the desired behavior. The examples are used to build a model of the desired reactive behavior. The model is combined with reactive control methods to produce natural 2D pedestrian trajectories. The system then automatically generates 3D pedestrian locomotion using a motion-graph approach. We discuss the accuracy of the learned model of pedestrian motion and show that simple direction primitives can be recorded and used to build natural, reactive, path-following behaviors.
C1 Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97330 USA.
   Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
C3 Oregon State University; Carnegie Mellon University
RP Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97330 USA.
EM metoyer@cs.orst.edu; jkh@cs.cmu.edu
CR [Anonymous], 2000, THESIS U TOKYO
   [Anonymous], 2012, Robot Motion Planning
   [Anonymous], ANN C SERIES
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Arkin R.C., 1987, IEEE C ROBOTICS AUTO, P264
   Ashida K, 2001, COMP ANIM CONF PROC, P84, DOI 10.1109/CA.2001.982380
   BARNES C, 2000, P 2000 AAAI SPRING S
   BATTY M, 1998, CTR ADV SPATIAL ANAL, V4
   Blue VJ, 2000, FROM ANIM ANIMAT, P437
   Blumberg B, 2002, ACM T GRAPHIC, V21, P417, DOI 10.1145/566570.566597
   BLUMBERG BM, 1995, ANN C SERIES, P47
   *BOST DYN, 2000, PEOPL
   Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   Dijkstra J., 2000, Proceedings on the 5th international conference, P101
   Farenc N, 2000, APPL ARTIF INTELL, V14, P69, DOI 10.1080/088395100117160
   Funge J, 1999, COMP GRAPH, P29, DOI 10.1145/311535.311538
   GIPPS PG, 1985, MATH COMPUT SIMULAT, V27, P95, DOI 10.1016/0378-4754(85)90027-8
   Goldenstein S, 1999, VISUAL COMPUT, V15, P349, DOI 10.1007/s003710050184
   Helbing D., 1992, Complex Systems, V6, P391
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   HENDERSON LF, 1974, TRANSPORT RES, V8, P509, DOI 10.1016/0041-1647(74)90027-6
   KHATIB O, 1986, INT J ROBOT RES, V5, P90, DOI 10.1177/027836498600500106
   Ko HS, 1996, PRESENCE-TELEOP VIRT, V5, P367, DOI 10.1162/pres.1996.5.4.367
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Krogh B. H., 1986, Proceedings 1986 IEEE International Conference on Robotics and Automation (Cat. No.86CH2282-2), P1664
   KUFFNER JJ, 1998, CAPTECH, P171
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   LOVAS GG, 1993, MODELLING AND SIMULATION 1993, P469
   Lyons D., 1986, Proceedings 1986 IEEE International Conference on Robotics and Automation (Cat. No.86CH2282-2), P1749
   *MASS SOFTW, 2004, MASS
   METOYER R, 2002, THESIS GEORGIA I TEC
   Metoyer RA, 2000, PROC GRAPH INTERF, P61
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   Musse S.R., 1997, COMPUTER ANIMATION S, P39, DOI [10.1007/978-3-7091-6874-5_3, 10.1007/978-3-7091-6874-53, DOI 10.1007/978-3-7091-6874-5_3]
   Musse SR, 1999, SPRING COMP SCI, P23
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   NOSER H, 1995, COMPUT GRAPH, V19, P7, DOI 10.1016/0097-8493(94)00117-H
   Quinn M.J., 2003, P 2 INT C PEDESTRIAN, P63
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Schadschneider A, 2002, PEDESTRIAN AND EVACUATION DYNAMICS, P75
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   SCHODL A, 2002, P 1 ACM S COMP AN, P121
   SCHODL A, 2000, ADV NEURAL INFORMATI, V13
   SUN H, 2001, ANN C SERIES, P261
   TECCHIA F, 2001, P GAM TECHN C 2001
   TU X, 1994, ANN C SERIES, P43
   *VISARC, 1999, PROF VIS SERV BUILD
   WEBBER B, 1995, P INT WORKSH HUM INT
NR 48
TC 28
Z9 32
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2004
VL 20
IS 10
BP 635
EP 649
DI 10.1007/s00371-004-0265-z
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 876TH
UT WOS:000225520000003
DA 2024-07-18
ER

PT J
AU Moradoff, S
   Lischinski, D
AF Moradoff, S
   Lischinski, D
TI Constrained synthesis of textural motion for articulated characters
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY FEB 12-14, 2003
CL Tel Aviv Univ, Tel Aviv, ISRAEL
HO Tel Aviv Univ
DE animation; articulated characters; constraints; motion
   editing/synthesis/reuse; textural motion
AB Obtaining high-quality, realistic motions of articulated characters is both time consuming and expensive, necessitating the development of easy-to-use and effective tools for motion editing and reuse. We propose a new simple technique for generating constrained variations of different lengths from an existing captured or otherwise animated motion. Our technique is applicable to textural motions, such as walking or dancing, where the motion sequence can be decomposed into shorter motion segments without an obvious temporal ordering among them. Inspired by previous work on texture synthesis and video textures, our method essentially produces a reordering of these shorter segments. Discontinuities are eliminated by carefully choosing the transition points and applying local adaptive smoothing in their vicinity, if necessary. The user is able to control the synthesis process by specifying a small number of simple constraints.
C1 Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91905 Jerusalem, Israel.
C3 Hebrew University of Jerusalem
RP Moradoff, S (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91905 Jerusalem, Israel.
EM danix@cs.huji.ac.il
OI Lischinski, Dani/0000-0002-6191-0361
CR [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], P 22 ANN C COMP GRAP
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Bowden R., 2000, CVPRW
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Bruderlin Armin., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '95, P97, DOI DOI 10.1145/218380.218421
   COHEN MF, 1992, COMP GRAPH, V26, P293
   DEBONET JS, 1997, P SIGGRAPH 97, P361, DOI DOI 10.1145/258734.258882
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   GLEICHER, 1998, P SIGGRAPH 98 ORL 19, P33
   Gleicher M, 1998, J VISUAL COMP ANIMAT, V9, P65, DOI 10.1002/(SICI)1099-1778(199804/06)9:2<65::AID-VIS176>3.0.CO;2-Z
   Gleicher M., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/253284.253321
   Gleicher M, 2001, GRAPH MODELS, V63, P107, DOI 10.1006/gmod.2001.0549
   Gleicher M., 2001, P 2001 S INTERACTIVE, P195
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Lee J, 1999, COMP GRAPH, P39
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Li YZ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P483, DOI 10.1109/PCCGA.2002.1167909
   Perlin Ken., 1996, SIGGRAPH 96, P205
   Pullen K, 2002, ACM T GRAPHIC, V21, P501
   Pullen K, 2000, COMP ANIM CONF PROC, P36, DOI 10.1109/CA.2000.889031
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Tanco LM, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P137, DOI 10.1109/HUMO.2000.897383
   Unuma M., 1995, P 22 ANN C COMPUTER, P91, DOI DOI 10.1145/218380.218419
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   ZHAO T, 2002, P ACCV 2002 MELB AUS
   2001, ALIAS WAVEFRONT MAYA
NR 30
TC 1
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2004
VL 20
IS 4
BP 253
EP 265
DI 10.1007/s00371-003-0231-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 834XR
UT WOS:000222444700005
DA 2024-07-18
ER

PT J
AU Wu, EH
   Zheng, X
AF Wu, EH
   Zheng, X
TI Composition of novel views through an efficient image warping
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 19th Computer Graphics International Conference
CY JUL 03-06, 2001
CL CITY UNIV HONG KONG, KOWLOON, PEOPLES R CHINA
SP Comp Graph Soc, IEEE Hong Kong Comp Chapter, KC Wong Educ Fda, Hong Kong Pei Hua Educ Fdn
HO CITY UNIV HONG KONG
DE image-based rendering; 3D warping; forward warping; inverse warping
AB This paper presents an efficient inverse warping algorithm for generating novel views from multiple reference images taken from different viewpoints. The method proceeds in three steps: preprocess for edge-pixel extraction, inverse warping from the primary image, and hole filling from remaining reference images. In using advantages of epipolar-line features and the depth discontinuity in the reference images, the inverse warping can be efficiently applied in segments, accelerating the rendering process substantially. Two acceleration techniques are proposed at the last stage for hole filling. By using the algorithm proposed, we may navigate a virtual environment at an interactive rate.
C1 Univ Macao, Fac Sci & Technol, Macao, Peoples R China.
   Chinese Acad Sci, Comp Sci Lab, Inst Software, Beijing, Peoples R China.
   Zhongxinguoan Informat Technol Co, Beijing, Peoples R China.
C3 University of Macau; Chinese Academy of Sciences; Institute of Software,
   CAS
RP Wu, EH (corresponding author), Univ Macao, Fac Sci & Technol, Macao, Peoples R China.
CR [Anonymous], P 1997 S INT 3D GRAP
   Chen S.E., 1993, P 20 ANN C COMP GRAP
   Chen Shenchang Eric, 1995, P 22 ANN C COMP GRAP
   LAVEAU S, 1994, P IAPR INT C PATT RE, V1
   Liu X., 2000, P ACM S VIRT REAL SO
   Ma S., 1998, Computer Vision
   MARCATO RW, 1998, OPTIMIZING INVERSE W
   McMillan L., 1995, P 22 ANN C COMP GRAP
   McMillan L, 1997, IMAGE BASED APPROACH
   SCHAUFLER G, 1999, REND TECHN 969 P 10
   Shade Jonathan, 1998, P 25 ANN C COMP GRAP
   TANG RX, 1990, COMPUTER GRAPHICS
NR 12
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2003
VL 19
IS 5
BP 319
EP 328
DI 10.1007/s00371-002-0183-x
PG 10
WC Computer Science, Software Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 726KP
UT WOS:000185598500005
DA 2024-07-18
ER

PT J
AU Attene, M
   Biasotti, S
   Spagnuolo, M
AF Attene, M
   Biasotti, S
   Spagnuolo, M
TI Shape understanding by contour-driven retiling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on Shape Modeling and Applications (SMI
   2001)
CY MAY 07-11, 2001
CL GENOA, ITALY
SP Consiglio Nazl Ricerche
DE shape analysis; Reeb graph; remeshing; computational topology
ID EXTRACTION
AB Given a triangle mesh representing a closed manifold surface of arbitrary genus, a method is proposed to automatically extract the Reeb graph of the manifold with respect to the height function. The method is based on a slicing strategy that traces contours while inserting them directly in the mesh as constraints. Critical areas, which identify isolated and non-isolated critical points of the surface, are recognized and coded in the extended Reeb graph (ERG). The remeshing strategy guarantees that topological features are correctly maintained in the graph, and the tiling of ERG nodes reproduces the original shape at a minimal, but topologically correct, geometric level.
C1 CNR, Ist Matemat Applicata & Tecnol Informaz, I-16149 Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP CNR, Ist Matemat Applicata & Tecnol Informaz, Via Marini 6, I-16149 Genoa, Italy.
EM attene@ima.ge.cnr.it; silvia@ima.ge.cnr.it; spagnuolo@ima.ge.cnr.it
RI Spagnuolo, Michela/ABA-1927-2021; Spagnuolo, Michela/F-5068-2013;
   Biasotti, Silvia/G-8602-2012
OI Spagnuolo, Michela/0000-0002-5682-6990; Spagnuolo,
   Michela/0000-0002-5682-6990; Biasotti, Silvia/0000-0002-9992-825X
CR [Anonymous], P ACM S SOL MOD APP
   [Anonymous], 1963, MORSE THEORY AM 51, DOI [10.1515/9781400881802, DOI 10.1515/9781400881802]
   Attene M, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P142, DOI 10.1109/SMA.2001.923385
   Axen Ulrike., 1998, Mathematical Visualization, P223
   Bajaj CL, 1998, COMPUT GRAPH-UK, V22, P3, DOI 10.1016/S0097-8493(97)00079-4
   Bajaj CL, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P39, DOI 10.1109/SVV.1996.558041
   BANCHOFF TF, 1970, AM MATH MON, V77, P475, DOI 10.2307/2317380
   Barequet G, 1998, COMP GEOM-THEOR APPL, V10, P155, DOI 10.1016/S0925-7721(98)00005-4
   Biasotti S, 2000, LECT NOTES COMPUT SC, V1953, P185
   BIASOTTI S, 2001, GEOMETRIC MODELLING
   Biasotti S., 2000, PROC SPRING C COMPUT, P174
   BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0
   Carr H, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P918
   Dey T.K., 1999, Contemporary Mathematics, V223, P109
   Edelsbrunner H., 2001, Symp. on Computational Geometry, P70, DOI 10.1007/s0045400329265
   ENGELKING R, 1992, SIGMA SERIES PURE MA, V4
   Falcidieno B, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P646, DOI 10.1109/CGI.1998.694323
   FOMENKO A, 1997, TOPOLOGICAL MODELLIN
   Guillemin V, 1974, DIFFERENTIAL TOPOLOG
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Jun CS, 2001, COMPUT AIDED DESIGN, V33, P825, DOI 10.1016/S0010-4485(01)00098-7
   Kulkarni P, 1996, COMPUT AIDED DESIGN, V28, P683, DOI 10.1016/0010-4485(95)00083-6
   Livnat Y, 1996, IEEE T VIS COMPUT GR, V2, P73, DOI 10.1109/2945.489388
   Mantyla M., 1988, INTRO SOLID MODELLIN
   NACKMAN LR, 1984, IEEE T PATTERN ANAL, V6, P442, DOI 10.1109/TPAMI.1984.4767549
   NACKMAN LR, 1985, IEEE T PATTERN ANAL, V7, P187, DOI 10.1109/TPAMI.1985.4767643
   PENTLAND AP, 1986, ARTIF INTELL, V28, P293, DOI 10.1016/0004-3702(86)90052-4
   PFALTZ JL, 1990, GEOGRAPH ANAL, V8, P77
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Shattuck DW, 2001, IEEE T MED IMAGING, V20, P1167, DOI 10.1109/42.963819
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   TAKAHASHI S, 1995, COMPUT GRAPH FORUM, V14, pC181, DOI 10.1111/j.1467-8659.1995.cgf143_0181.x
   VANKREVELD M, 1994, LECT NOTES COMPUTER, V884, P113
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Wood ZJ, 2000, IEEE VISUAL, P275, DOI 10.1109/VISUAL.2000.885705
NR 36
TC 43
Z9 47
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 127
EP 138
DI 10.1007/s00371-002-0182-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 691AU
UT WOS:000183583300006
DA 2024-07-18
ER

PT J
AU Jiang, L
   Cai, LL
   Wu, W
   Zhou, Z
AF Jiang, Ling
   Cai, Liangliang
   Wu, Wei
   Zhou, Zhong
TI Mirror world: creating digital twins of the space and persons from video
   streamings
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Mirror world; 3D visualization; Human analysis; Digital twins
ID POSE
AB Creating digital twins of scenes has been widely studied in smart city applications. The development of 3D virtual games attracts many researchers. However, most products have high costs, neglect 3D recovery of the moving people, and lack structured analysis. In this paper, we present a mirror world based on PTZ cameras, which creates digital twins of the space and persons from video streamings, including real-time video registration for PTZ cameras and 3D recovery of the human pose and shape. Our goal is to quickly build a digital space to represent and visualize the physical world, leveraging the two tasks to improve the performance of structured scene understanding. We use the scene images from PTZ cameras to create the 3D scene model and propose an image edge alignment method to optimize the texture mismatching during real-time video registration. Then we propose a human analysis network for 3D recovery of the human pose and shape and add refinement to improve performance on two datasets. We place the 3D human bodies in the right position in the mirror world and utilize joint data from specific scenarios to drive the optimization of the system. Consequently, the mirror world provides 3D visualization and structured analysis of the scene with low cost, thereby enhancing users' spatial understanding.
C1 [Jiang, Ling; Cai, Liangliang; Wu, Wei; Zhou, Zhong] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Zhou, Zhong] Zhongguancun Lab, Beijing, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory
RP Zhou, Z (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Zhou, Z (corresponding author), Zhongguancun Lab, Beijing, Peoples R China.
EM jiangjiang@buaa.edu.cn; caill0429@buaa.edu.cn; wuwei@buaa.edu.cn;
   zz@buaa.edu.cn
OI Zhou, Zhong/0000-0002-5825-7517
FU National Key Research and Development Program of China [62272018];
   Natural Science Foundation of China
FX This work is supported by the Natural Science Foundation of China under
   Grant No. 62272018.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Brau E, 2016, INT CONF 3D VISION, P582, DOI 10.1109/3DV.2016.84
   Chen D., 2022, VISUAL COMPUT, V39, P1
   Decamp P., 2010, P INT C MULTIMEDIA M
   Everingham M., 2010, BMVC, V2, P5
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hildebrandt D, 2014, GEOINFORMATICA, V18, P537, DOI 10.1007/s10707-013-0189-8
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hu Y., 2015, Video driven pedestrian visualization with characteristic appearances, P183, DOI [10.1145/2821592.2821614, DOI 10.1145/2821592.2821614]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jun Chen, 2000, GeoInformatica, V4, P375, DOI 10.1023/A:1026513912425
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kim K., 2009, IEEE INT S MIXED AUG
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   LENZ RK, 1988, IEEE T PATTERN ANAL, V10, P713, DOI 10.1109/34.6781
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Li MX, 1996, IEEE T PATTERN ANAL, V18, P1105, DOI 10.1109/34.544080
   Li SC, 2020, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR42600.2020.00621
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Louwsma J, 2006, GEOINFORMATICA, V10, P531, DOI 10.1007/s10707-006-0345-5
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Milosavljevic A, 2016, INT J GEOGR INF SCI, V30, P2089, DOI 10.1080/13658816.2016.1161197
   Neumann U, 2003, P IEEE VIRT REAL ANN, P61, DOI 10.1109/VR.2003.1191122
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Pan CW, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P65, DOI 10.1109/CW.2016.17
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Sawhney H. S., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P157
   SEGAL M, 1992, COMP GRAPH, V26, P249, DOI 10.1145/142920.134071
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Takeuchi Y., 2013, P 19 ACM S VIRTUAL R, P189, DOI [10.1145/2503713.2503742, DOI 10.1145/2503713.2503742]
   Tan Jun Kai Vince, 2017, Indirect deep structured learning for 3D human body shape and pose prediction, V3
   Tanin E, 2006, GEOINFORMATICA, V10, P91, DOI 10.1007/s10707-005-4887-8
   Tao F, 2019, NATURE, V573, P490, DOI 10.1038/d41586-019-02849-1
   Tekin B, 2016, Arxiv, DOI arXiv:1605.05180
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Tripathi S., 2020, ARXIV
   Wache H., 2020, HAW INT C SYST SCI H
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang K., 2022, Vis. Comput, V39, P1
   Wu JZ, 2020, VISUAL COMPUT, V36, P1401, DOI 10.1007/s00371-019-01740-4
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yang Y., 2015, IEEE INT C IMAGE PRO
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17
   Zhou Y, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI [10.1145/3281505.3281513, 10.1109/TSMC.2018.2858843]
NR 60
TC 0
Z9 0
U1 5
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 28
PY 2023
DI 10.1007/s00371-023-03193-2
EA DEC 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DL6J0
UT WOS:001132234000001
DA 2024-07-18
ER

PT J
AU Ying, WY
   Dong, TY
   Fan, J
AF Ying, Wen-yuan
   Dong, Tian-yang
   Fan, Jing
TI Edge-priority-extraction network using re-parameterization for real-time
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Super-resolution; Real-time inference; Edge information;
   Re-parameterization
ID IMAGE; ACCURATE
AB Recently, super-resolution (SR) has achieved superior performance with the development of deep learning. However, previous methods usually require considerable computational resources with a large model size, which hinders practical applications. To achieve real-time inference and high quality for SR, this paper presents an edge-priority-extraction network, which is constructed with our proposed edge-priority blocks (EPB). The EPB utilizes multiple branches with edge information to further improve the network representation. Moreover, it can be re-parameterized for efficient inference. For more effective utilization of edge information, this paper also proposes the mix-priority filter with edge extraction of horizontal and vertical priorities to improve the network performance. The filters can adaptively extract the edge information with multi-direction derivatives. The experimental results show that our models can use less computational cost to meet the real-time demand and have a better SR performance than the recent real-time SR models.
C1 [Ying, Wen-yuan; Dong, Tian-yang; Fan, Jing] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
C3 Zhejiang University of Technology
RP Dong, TY (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM dty@zjut.edu.cn
RI fan, jing/KHX-6210-2024
OI Ying, Wenyuan/0000-0003-4390-7828
FU National Natural Science Foundation of China [62072405]; National
   Natural Science Foundation of China [LGF20F020017]; Zhejiang Provincial
   Natural Science Foundation of China
FX This research was supported by the National Natural Science Foundation
   of China under Grant No. 62072405 and the Zhejiang Provincial Natural
   Science Foundation of China under Grant No. LGF20F020017.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Amaranageswarao G, 2020, APPL INTELL, V50, P2177, DOI 10.1007/s10489-020-01670-y
   Arora S, 2018, PR MACH LEARN RES, V80
   BAMBERGER RH, 1992, IEEE T SIGNAL PROCES, V40, P882, DOI 10.1109/78.127960
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Chu XX, 2021, INT C PATT RECOG, P59, DOI 10.1109/ICPR48806.2021.9413080
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Ding XH, 2021, PROC CVPR IEEE, P10881, DOI 10.1109/CVPR46437.2021.01074
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong X, 2017, ADV NEUR IN, V30
   Frankle J., 2018, ARXIV180303635
   Guo YW, 2016, ADV NEUR IN, V29
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kingma D. P., 2014, arXiv
   Li W., 2020, ADV NEURAL INF PROCE, V33, P20343
   Li Y., 2020, PROCEEDING EUROPEAN, P608
   Liang YD, 2016, NEUROCOMPUTING, V194, P340, DOI 10.1016/j.neucom.2016.02.046
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu N, 2020, AAAI CONF ARTIF INTE, V34, P4876
   Liu Z., 2018, INT C LEARN REPR
   Lu XB, 2024, VISUAL COMPUT, V40, P41, DOI 10.1007/s00371-022-02764-z
   Ma XL, 2020, AAAI CONF ARTIF INTE, V34, P5117
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Paris S, 2015, COMMUN ACM, V58, P81, DOI 10.1145/2723694
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang SZ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3132093
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu J, 2023, IEEE SIGNAL PROC LET, V30, P733, DOI 10.1109/LSP.2023.3286811
   Wu YS, 2022, LECT NOTES COMPUT SC, V13679, P92, DOI 10.1007/978-3-031-19800-7_6
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Yifan Gong, 2020, GLSVLSI '20. Proceedings of the 2020 Great Lakes Symposium on VLSI, P119, DOI 10.1145/3386263.3407650
   Yu JH, 2018, Arxiv, DOI arXiv:1808.08718
   Zagoruyko S, 2018, Arxiv, DOI arXiv:1706.00388
   Zeng C, 2022, APPL INTELL, V52, P17797, DOI 10.1007/s10489-022-03454-y
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhan Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4801, DOI 10.1109/ICCV48922.2021.00478
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou FQ, 2018, NEUROCOMPUTING, V290, P34, DOI 10.1016/j.neucom.2018.02.027
   Zhou L, 2022, EUROPEAN C COMPUTER, P256
   Zhu XY, 2022, IEEE T CIRC SYST VID, V32, P1273, DOI 10.1109/TCSVT.2021.3078436
   Zhuang ZW, 2018, ADV NEUR IN, V31
NR 56
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 21
PY 2023
DI 10.1007/s00371-023-03197-y
EA DEC 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CW7N9
UT WOS:001128338300003
DA 2024-07-18
ER

PT J
AU Parseh, MJ
   Rahmanimanesh, M
   Keshavarzi, P
   Azimifar, Z
AF Parseh, Mohammad Javad
   Rahmanimanesh, Mohammad
   Keshavarzi, Parviz
   Azimifar, Zohreh
TI Scene representation using a new two-branch neural network model
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Scene classification; Semantic scene representation; Scene graph;
   Feature fusion
ID CLASSIFICATION; GRAPH; SEMANTICS; FEATURES; CNN
AB Scene classification and recognition have always been one of the most challenging tasks of scene understanding due to the inherent ambiguity in visual scenes. The core of scene classification and recognition tasks is scene representation. Deep learning advances in computer vision, especially deep CNNs, have significantly improved scene representation in the last decade. Deep convolutional features extracted from deep CNNs provide discriminative representations of the images and are widely used in various computer vision tasks, such as scene classification. Deep convolutional features capture the appearance characteristics of the image and the spatial information about different image regions. Meanwhile, the semantic and context information obtained from high-level concepts about scene images, such as objects and their relationships, can significantly contribute to identifying scene images. Therefore, in this paper, we divide visual scenes into two categories, object-based and layout-based. Object-based scenes are scenes that have scene-specific objects and, based on those objects, can be described and identified. In contrast, the layout-based scenes do not have scene-specific objects and are described and identified based on the appearance and layout of the image. This paper proposes a new neural network model for representing and classifying visual scenes, which we call G-CNN (GNN-CNN). The proposed model includes two modules, feature extraction and feature fusion, and the feature extraction module composes of visual and semantic branches. The visual branch is responsible for extracting deep CNN features from the image, and the semantic branch is responsible for extracting semantic GNN features from the scene graph corresponding to the image. The feature fusion module is a novel two-stream neural network that fuses the CNN and GNN feature vectors to produce a comprehensive representation of the scene image. Finally, a fully-connected classifier classified the obtained comprehensive feature vector into one of the pre-defined categories. The proposed model has been evaluated on three benchmark scene datasets, UIUC Sports, MIT67, and SUN397, and obtained classification accuracy of 99.91%, 96.01%, and 85.32%, respectively. In addition, a new dataset named Scene40, which has been introduced in our previous paper, is also used for further evaluation of the proposed method. The comparison results based on classification accuracy criteria show that the proposed model can outperform the best previous methods on three benchmark scene datasets.
C1 [Parseh, Mohammad Javad] Jahrom Univ, Dept Comp Engn & Informat Technol, Jahrom, Iran.
   [Rahmanimanesh, Mohammad; Keshavarzi, Parviz] Semnan Univ, Dept Elect & Comp Engn, Semnan, Iran.
   [Azimifar, Zohreh] Shiraz Univ, Dept Comp Sci & Engn, Shiraz, Iran.
C3 Semnan University; Shiraz University
RP Parseh, MJ (corresponding author), Jahrom Univ, Dept Comp Engn & Informat Technol, Jahrom, Iran.
EM parseh@jahromu.ac.ir
RI Parseh, Mohammad Javad/KHD-5651-2024; Keshavarzi, Parviz/M-2641-2017
OI Parseh, Mohammad Javad/0000-0003-0109-3133; 
CR Abadal S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3477141
   [Anonymous], 2014, P AS C COMP VIS
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bai S, 2017, INT J PATTERN RECOGN, V31, DOI 10.1142/S0218001417550138
   Bai S, 2017, EXPERT SYST APPL, V71, P279, DOI 10.1016/j.eswa.2016.10.038
   Cangea C, 2018, Arxiv, DOI arXiv:1811.01287
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chami I, 2022, Arxiv, DOI [arXiv:2005.03675, DOI 10.48550/ARXIV.2005.03675]
   Chang XJ, 2022, Arxiv, DOI [arXiv:2104.01111, 10.48550/arXiv.2104.01111]
   Chang X, 2021, IET INTELL TRANSP SY, V15, P423, DOI 10.1049/itr2.12035
   Chen BH, 2018, PATTERN RECOGN, V76, P339, DOI 10.1016/j.patcog.2017.10.039
   Chen GW, 2020, IEEE T IMAGE PROCESS, V29, P5877, DOI 10.1109/TIP.2020.2986599
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cong YR, 2022, Arxiv, DOI arXiv:2201.11460
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Dixit M, 2016, ADV NEUR IN, V29
   Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916
   Donggeun Yoo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P71, DOI 10.1109/CVPRW.2015.7301274
   Durand T, 2016, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2016.513
   Feng ZP, 2023, NEUROCOMPUTING, V533, P104, DOI 10.1016/j.neucom.2023.02.057
   Gamage B.M.S.V., 2021, arXiv
   Gao BB, 2015, Arxiv, DOI [arXiv:1504.05277, DOI 10.48550/ARXIV.1504.05277]
   Gao HY, 2019, PR MACH LEARN RES, V97
   Garcia V., 2017, arXiv
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   github, GitHub Repository Link (ConceptNet NumberBatch)
   GitHub Repository Link (RelTR), ABOUT US
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gu JY, 2018, LECT NOTES COMPUT SC, V11216, P392, DOI 10.1007/978-3-030-01258-8_24
   Guo S, 2017, IEEE T IMAGE PROCESS, V26, P808, DOI 10.1109/TIP.2016.2629443
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hare JS, 2006, PROC SPIE, V6073, DOI 10.1117/12.647755
   Hayat M, 2016, IEEE T IMAGE PROCESS, V25, P4829, DOI 10.1109/TIP.2016.2599292
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kampffmeyer M., 2019, CVPR, P11487
   Kawamoto T, 2018, ADV NEUR IN, V31
   Khan SH, 2017, IEEE I CONF COMP VIS, P5639, DOI 10.1109/ICCV.2017.601
   Khan SH, 2016, IEEE T IMAGE PROCESS, V25, P3372, DOI 10.1109/TIP.2016.2567076
   Krishnamurthy Jayant., 2013, Transactions of the Association for Computational Linguistics, V1, P193, DOI DOI 10.1162/TACLA00220
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Labinghisa BA, 2022, MULTIMED TOOLS APPL, V81, P28405, DOI 10.1007/s11042-022-12481-3
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Lee J, 2019, PR MACH LEARN RES, V97
   Li J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091366
   Li L.-J., 2007, 2007 IEEE 11 INT C C, P1
   Li YB, 2019, PATTERN RECOGN, V90, P436, DOI 10.1016/j.patcog.2019.02.005
   Li YS, 2017, IEEE I CONF COMP VIS, P5757, DOI 10.1109/ICCV.2017.613
   Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Limin Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P30, DOI 10.1109/CVPRW.2015.7301333
   Lin CW, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108530
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2335, DOI 10.1109/TPAMI.2017.2651061
   Liu SP, 2019, NEUROCOMPUTING, V338, P191, DOI 10.1016/j.neucom.2019.01.090
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3267271
   Liu YF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3133956
   Liu Y, 2023, IEEE T PATTERN ANAL, V45, P11624, DOI 10.1109/TPAMI.2023.3284038
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P5573, DOI 10.1109/TIP.2021.3086590
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P3168, DOI 10.1109/TIP.2019.2957930
   Liu Y, 2018, AAAI CONF ARTIF INTE, P7178
   López-Cifuentes A, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107256
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lv GR, 2023, VISUAL COMPUT, V39, P1629, DOI 10.1007/s00371-022-02433-1
   Maheshwari P, 2021, AAAI CONF ARTIF INTE, V35, P2328
   Messina N, 2020, INT J MULTIMED INF R, V9, P113, DOI 10.1007/s13735-019-00178-7
   Muller-Budack E, 2021, INT J MULTIMED INF R, V10, P111, DOI 10.1007/s13735-021-00207-4
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Narasimhan M, 2018, ADV NEUR IN, V31
   Norcliffe-Brown W, 2018, ADV NEUR IN, V31
   Parseh MJ, 2022, INT J MULTIMED INF R, V11, P619, DOI 10.1007/s13735-022-00246-5
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Qiu J., 2021, P IEEECVF C COMPUTER, P8322
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Schroeder B., 2019, P IEEECVF INT C COMP
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Seong H, 2020, IEEE ACCESS, V8, P82066, DOI 10.1109/ACCESS.2020.2989863
   Shi J, 2019, IEEE ACCESS, V7, P45230, DOI 10.1109/ACCESS.2019.2908448
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song XH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4523
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Song XH, 2016, PATTERN RECOGN, V59, P98, DOI 10.1016/j.patcog.2016.01.019
   Sorkhi AG, 2020, MULTIMED TOOLS APPL, V79, P18033, DOI 10.1007/s11042-019-08264-y
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Sun N, 2019, IEEE T CIRC SYST VID, V29, P1715, DOI 10.1109/TCSVT.2018.2848543
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang PJ, 2017, NEUROCOMPUTING, V225, P188, DOI 10.1016/j.neucom.2016.11.023
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang C, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108589
   Wang C, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P889, DOI 10.1145/3132847.3132967
   Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339
   Wang Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181062
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Wang ZX, 2018, Arxiv, DOI arXiv:1807.00504
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xia SF, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8101072
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie GS, 2017, IEEE T CIRC SYST VID, V27, P1263, DOI 10.1109/TCSVT.2015.2511543
   Xie L, 2018, PATTERN RECOGN, V82, P118, DOI 10.1016/j.patcog.2018.04.025
   Xie LX, 2017, INT J COMPUT VISION, V123, P226, DOI 10.1007/s11263-016-0970-x
   Xu K., 2018, arXiv, DOI DOI 10.48550/ARXIV.1810.00826
   Xu Pengfei, 2020, EasyChair Preprint
   Yan H., 2023, arXiv
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Ying R, 2018, ADV NEUR IN, V31
   Yuan Y, 2015, IEEE T NEUR NET LEAR, V26, P2222, DOI 10.1109/TNNLS.2014.2359471
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng DL, 2021, Arxiv, DOI arXiv:2101.10531
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P1793, DOI 10.1109/TGRS.2015.2488681
   Zhang MH, 2018, ADV NEUR IN, V31
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou L, 2013, PATTERN RECOGN, V46, P424, DOI 10.1016/j.patcog.2012.07.017
   Zhu GM, 2022, Arxiv, DOI [arXiv:2201.00443, 10.48550/arXiv.2201.00443]
   Zhu YY, 2023, IEEE T IND INFORM, V19, P1248, DOI 10.1109/TII.2022.3179243
   Ziwei Zhang, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P249, DOI 10.1109/TKDE.2020.2981333
   Zuo Z, 2015, PATTERN RECOGN, V48, P3004, DOI 10.1016/j.patcog.2015.02.003
NR 149
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 1
PY 2023
DI 10.1007/s00371-023-03162-9
EA DEC 2023
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z4NY8
UT WOS:001111873100001
DA 2024-07-18
ER

PT J
AU Zhang, GW
   Kong, YY
   Li, WZ
   Tang, XC
   Zhang, WD
   Chen, J
   Wang, L
AF Zhang, Guowei
   Kong, Yangyang
   Li, Wuzhi
   Tang, Xincheng
   Zhang, Weidong
   Chen, Jing
   Wang, Li
TI Lightweight deep learning model for logistics parcel detection
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Object detection; Logistics parcel detection;
   Lightweight; SFYOLOv5
ID NETWORK
AB Logistics parcel detection technology is critical for unmanned sorting. YOLOv5, the state-of-the-art (SOTA) object detection model, is a classic network widely used in engineering. However, on the premise of fast and accurate detection of logistics parcels, it confronts certain challenges of high computing load and parameter quantity. To address these issues, this paper proposes a two-scale lightweight deep learning model named SFYOLOv5. In this article, a lightweight feature extraction module called Pruned-Shuffle-Block (PSB) is proposed. Meanwhile, a double-layer pyramid structure for the medium and large target detection is created in accordance with the image size distribution of logistics parcels. With this structure, the Floating Point of Operations (FLOPs) and parameters in feature extraction were significantly reduced. In addition, a down sampling module named Focus for Downsampling (FFD) is designed and attention modules are introduced to extract high-level semantic information in logistics parcels. These modules not only compensate for the loss caused by down sampling but also improve the mean Average Precision (mAP). Finally, the comparison experiment is performed by using the self-built logistics parcel dataset. The results show that the mAP of the model reaches 99.1%, the number of model parameters decreased by 92.14%, and the FLOPs decreased by 89.57% compared with the existing SOTA model. This model can be used in logistics parcel intelligent sorting.
C1 [Zhang, Guowei; Kong, Yangyang; Li, Wuzhi; Tang, Xincheng; Zhang, Weidong; Chen, Jing] Xiamen Univ Technol, Sch Mech & Automot Engn, Fujian Key Lab Green Intelligent Cleaning Technol, Xiamen 361024, Peoples R China.
   [Wang, Li] Shunfeng Technol Co Ltd, Res & Dev Dept, Xuefu Rd, Shenzhen 518000, Guangdong Provi, Peoples R China.
C3 Xiamen University of Technology
RP Wang, L (corresponding author), Shunfeng Technol Co Ltd, Res & Dev Dept, Xuefu Rd, Shenzhen 518000, Guangdong Provi, Peoples R China.
EM 2019000050@s.xmut.edu.cn; 2022031306@s.xmut.edu.cn; zgwzw1986@126.com
RI Zhang, Guowei/HJH-0318-2022
OI Zhang, Guowei/0000-0002-6371-5455; Zhang, Guowei/0000-0002-1290-5590
FU Natural Science Foundation of Fujian Province [2020J05236]; Fujian
   Science and Technology Plan STS Project [2021T3069]; Xiamen Key
   Laboratory Of Intelligent Manufacturing Equipment and Scientific
   Research Start-up Project of Xiamen University of Technology [YKJ20006R]
FX This study was funded by the Natural Science Foundation of Fujian
   Province (2020J05236), the Fujian Science and Technology Plan STS
   Project (2021T3069), Xiamen Key Laboratory Of Intelligent Manufacturing
   Equipment and Scientific Research Start-up Project of Xiamen University
   of Technology (YKJ20006R).
CR Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Chen CL, 2021, J SUPERCOMPUT, V77, P7791, DOI 10.1007/s11227-020-03558-7
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chen ZY, 2022, AGRONOMY-BASEL, V12, DOI 10.3390/agronomy12020365
   Clevert D., 2016, ARXIV151107289
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu JM, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13163059
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jung HK, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12147255
   Li H., 2018, P BRIT MACH VIS C
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma Ningning, 2018, P EUR C COMP VIS ECC, DOI [10.1007/978-3-030-01264-9_8, DOI 10.1007/978-3-030-01264-9_8]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Saavedra D, 2021, NEURAL COMPUT APPL, V33, P7803, DOI 10.1007/s00521-020-05521-2
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shu X., 2022, IEEE T CIRCUITS SYST, DOI [10.48550/arXiv.2112.10992, DOI 10.48550/ARXIV.2112.10992]
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang PF, 2023, VISUAL COMPUT, V39, P5185, DOI 10.1007/s00371-022-02653-5
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xu XY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031215
   Xue J, 2022, SOFT COMPUT, V26, P10879, DOI 10.1007/s00500-022-07106-8
   Yao X, 2023, VISUAL COMPUT, V39, P5469, DOI 10.1007/s00371-022-02673-1
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
NR 33
TC 0
Z9 0
U1 10
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2751
EP 2759
DI 10.1007/s00371-023-02982-z
EA JUL 2023
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001027416100002
DA 2024-07-18
ER

PT J
AU He, WT
   Ren, JF
   Bai, RB
AF He, Wentao
   Ren, Jianfeng
   Bai, Ruibin
TI Data augmentation by morphological mixup for solving Raven's progressive
   matrices
SO VISUAL COMPUTER
LA English
DT Article
DE Raven's progressive matrices; Data augmentation; Image mixup; Visual
   analogical reasoning
ID IMAGE; NETWORK
AB Raven's progressive matrix (RPM) is one kind of visual abstract reasoning tasks, which tests the ability of extracting reasoning rules from limited samples and applying them to an unknown setting. It is frequently used in evaluating human intelligence. Recent advances of RPM-like datasets and solution models partially address the challenges of visually understanding the RPM questions and logically reasoning the missing answers. This paper tackles the challenges of the poor generalization performance due to insufficient samples in RPM datasets. To address the problem of insufficient data for precisely conducting relational reasoning in RPMs, we propose an effective scheme, namely candidate answer morphological mixup (CAM-Mix). CAM-Mix serves as a data augmentation strategy by gray-scale image morphological mixup, which regularizes various solution methods and overcomes the model overfitting problem. Compared with existing methods, a more accurate decision boundary could be defined by creating new negative candidate answers semantically similar to the correct answers. Experimental results show that the proposed data augmentation method on state-of-the-art RPM solution models can provide significant and consistent performance improvements on various RPM-like datasets compared with state-of-the-art solution models and other data augmentation strategies.
C1 [He, Wentao; Ren, Jianfeng; Bai, Ruibin] Univ Nottingham Ningbo China, Sch Comp Sci, 199 Taikang East Rd, Ningbo 315100, Peoples R China.
   [Ren, Jianfeng; Bai, Ruibin] Univ Nottingham Ningbo China, Nottingham Ningbo China Beacons Excellence Res & I, 199 Taikang East Rd, Ningbo 315100, Peoples R China.
C3 University of Nottingham Ningbo China; University of Nottingham Ningbo
   China
RP Ren, JF (corresponding author), Univ Nottingham Ningbo China, Sch Comp Sci, 199 Taikang East Rd, Ningbo 315100, Peoples R China.; Ren, JF (corresponding author), Univ Nottingham Ningbo China, Nottingham Ningbo China Beacons Excellence Res & I, 199 Taikang East Rd, Ningbo 315100, Peoples R China.
EM jianfeng.ren@nottingham.edu.cn
RI Ren, Jianfeng/D-4160-2017
OI He, Wentao/0000-0002-6319-1639
FU National Natural Science Foundation of China [72071116]; Ningbo
   Municipal Science and Technology Bureau [2019B10026, 2022Z173]
FX This research was supported in part by the National Natural Science
   Foundation of China under Grant No. 72071116 and in part & nbsp;by the
   Ningbo Municipal Science and Technology Bureau under GrantNos.
   2019B10026 and 2022Z173.
CR Ametefe DS, 2023, VISUAL COMPUT, V39, P1703, DOI 10.1007/s00371-022-02437-x
   Amizadeh S, 2020, PR MACH LEARN RES, V119
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barrett DGT, 2018, PR MACH LEARN RES, V80
   Ding RX, 2022, INT CONF ACOUST SPEE, P1840, DOI 10.1109/ICASSP43922.2022.9747106
   Dvornik N, 2021, IEEE T PATTERN ANAL, V43, P2014, DOI 10.1109/TPAMI.2019.2961896
   Ebadi MJ, 2021, INT J INTERACT MULTI, V6, P189, DOI 10.9781/ijimai.2020.12.002
   Guo HY, 2019, AAAI CONF ARTIF INTE, P3714
   He, 2021, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He W., 2023, P 37 AAAI C ART INT, V37, P22
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Hu S, 2021, AAAI CONF ARTIF INTE, V35, P1567
   Inoue H., 2018, arXiv
   Khan MJ, 2022, VISUAL COMPUT, V38, P509, DOI 10.1007/s00371-020-02031-z
   Kong W., 2023, P IEEE INT C AC SPEE
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang DJ, 2018, IEEE ACCESS, V6, P58774, DOI 10.1109/ACCESS.2018.2872698
   Liu SY, 2020, NEUROCOMPUTING, V401, P123, DOI 10.1016/j.neucom.2020.02.094
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Mai ZJ, 2022, IEEE T NEUR NET LEAR, V33, P3050, DOI 10.1109/TNNLS.2020.3049011
   MARAGOS P, 1989, IEEE T PATTERN ANAL, V11, P586, DOI 10.1109/34.24793
   Nazari K, 2022, J SCI FOOD AGR, V102, P6907, DOI 10.1002/jsfa.12052
   Ren JF, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107709
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Song X., 2023, P 37 AAAI C ART INT, V37, P2303, DOI DOI 10.1609/AAAI.V37I2.25325
   Song X., 2023, P IEEE INT C AC SPEE
   Summers C, 2019, IEEE WINT CONF APPL, P1262, DOI 10.1109/WACV.2019.00139
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang SH, 2023, EXPERT SYST APPL, V225, DOI 10.1016/j.eswa.2023.120094
   Wang XH, 2019, PATTERN RECOGN, V88, P331, DOI 10.1016/j.patcog.2018.11.030
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Zhang C., 2019, Advances in neural information processing systems, P1075
   Zhang C, 2019, PROC CVPR IEEE, P5312, DOI 10.1109/CVPR.2019.00546
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang JL, 2023, IEEE T IMAGE PROCESS, V32, P3000, DOI 10.1109/TIP.2023.3266161
   Zhang JL, 2022, INT CONF ACOUST SPEE, P1960, DOI 10.1109/ICASSP43922.2022.9746921
   Zheng K., 2019, P ADV NEUR INF PROC, P5842
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou F, 2019, VISUAL COMPUT, V35, P1583, DOI 10.1007/s00371-018-1559-x
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuo T, 2021, IEEE T IMAGE PROCESS, V30, P8332, DOI 10.1109/TIP.2021.3114987
NR 44
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2457
EP 2470
DI 10.1007/s00371-023-02930-x
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001024240700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lian, YF
   Shi, X
   Shen, SC
   Hua, J
AF Lian, Yuanfeng
   Shi, Xu
   Shen, ShaoChen
   Hua, Jing
TI Multitask learning for image translation and salient object detection
   from multimodal remote sensing images
SO VISUAL COMPUTER
LA English
DT Article
DE Multitask learning; Image translation; Salient object detection; Remote
   sensing image; Context-aware learning
AB This paper presents a novel and efficient multitask learning framework for image translation and saliency detection from remote sensing images, which mainly contains the image translation network-weight sharing attention GAN (WSA-GAN) and the salient object detection network-boundary guidance network (BGNet). WSA-GAN can be used to generate a large number of synthetic infrared remote sensing images (IRIs) or optical remote sensing images (ORIs) from the corresponding complementary modality images. Then, a new multimodal context-aware learning is proposed for feature extraction and to coordinate the entanglement of latent features in the multimodal context of ORIs and IRIs. Since convolutional neural networks do not perform well when the object has directional variance, our framework introduces the attention-aware CapsNet (AACNet) to alleviate the problem and enhance the feature expressiveness. In addition, knowledge distillation strategy is introduced in AACNet to reduce the model complexity. Finally, the multiscale feature learning network and the boundary-aware block are designed to generate more accurate saliency detection results with clear boundaries. Experimental results demonstrate that the presented image translation and salient object detection networks outperform other approaches.
C1 [Lian, Yuanfeng; Shi, Xu; Shen, ShaoChen] China Univ Petr, Dept Comp Sci & Technol, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
   [Hua, Jing] Wayne State Univ, Detroit, MI USA.
C3 China University of Petroleum; Wayne State University
RP Lian, YF (corresponding author), China Univ Petr, Dept Comp Sci & Technol, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
EM lianyuanfeng@cup.edu.cn; 2020215940@student.cup.edu.cn
OI Lian, Yuanfeng/0000-0002-1801-2507
FU NSFC [61972353]; NSF [IIS-1816511, OAC-1910469]; Strategic Cooperation
   Technology Projects [ZLZX2020-05]
FX This research was funded by NSFC 61972353, NSF IIS-1816511, OAC-1910469
   and Strategic Cooperation Technology Projects of CNPC and CUPB:
   ZLZX2020-05.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen TY, 2022, KNOWL-BASED SYST, V248, DOI 10.1016/j.knosys.2022.108901
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Das DK, 2022, VISUAL COMPUT, V38, P3803, DOI 10.1007/s00371-021-02222-2
   Dean J., 2015, NIPS DEEP LEARNING R
   Feng Y., 2022, IEEE T MULTIMEDIA
   Gatys L. A., 2015, arXiv
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Hasanov A, 2019, J AMB INTEL SMART EN, V11, P403, DOI 10.3233/AIS-190534
   Heo Young-Jin, 2021, Journal of Multimedia Information System, V8, P85, DOI 10.33851/JMIS.2021.8.2.85
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hinton G.E., 2018, INT C LEARN REPR
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu XW, 2021, IEEE T CIRC SYST VID, V31, P1079, DOI 10.1109/TCSVT.2020.2995220
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Janakiramaiah B., 2021, SOFT COMPUT, P1
   Jia BH, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030884
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kaji S, 2019, RADIOL PHYS TECHNOL, V12, P235, DOI 10.1007/s12194-019-00520-y
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Lan J., 2022, VISUAL COMPUT, P1
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Li JZ, 2022, ARRAY-NY, V15, DOI 10.1016/j.array.2022.100205
   Li RY, 2021, J INFRARED MILLIM W, V40, P530, DOI 10.11972/j.issn.1001-9014.2021.04.012
   Li TP, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107372
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu MY, 2016, ADV NEUR IN, V29
   Liu MY, 2017, ADV NEUR IN, V30
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P3688, DOI 10.1109/TPAMI.2021.3053577
   Mazzia V, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-93977-0
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Park H.J., 2022, ARXIV
   Sabour S, 2017, ADV NEUR IN, V30
   Tang H, 2019, IEEE INT CONF AUTOMA, P192, DOI 10.1109/fg.2019.8756586
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang Q, 2019, IEEE T GEOSCI REMOTE, V57, P911, DOI 10.1109/TGRS.2018.2862899
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yu YT, 2021, INT J APPL EARTH OBS, V104, DOI 10.1016/j.jag.2021.102548
   Yu YT, 2019, IEEE GEOSCI REMOTE S, V16, P1894, DOI 10.1109/LGRS.2019.2912582
   Yuan YC, 2018, IEEE T IMAGE PROCESS, V27, P1311, DOI 10.1109/TIP.2017.2762422
   Zhang LB, 2019, INT J REMOTE SENS, V40, P8270, DOI 10.1080/01431161.2019.1608384
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 3
Z9 3
U1 2
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1395
EP 1414
DI 10.1007/s00371-023-02857-3
EA MAY 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000981343000001
DA 2024-07-18
ER

PT J
AU Ma, DC
   Xian, Y
   Li, B
   Li, SP
   Zhang, DQ
AF Ma, Decao
   Xian, Yong
   Li, Bing
   Li, Shaopeng
   Zhang, Daqiao
TI Visible-to-infrared image translation based on an improved CGAN
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial networks; Infrared image generation; ConvNeXt;
   Gradient vector loss
AB This study proposes an Infrared (IR) generative adversarial network (IR-GAN) to generate high-quality IR images using visible images, based on a conditional generative adversarial network. IR-GAN improves texture loss and edge distortion during infrared image generation and includes a novel generator implementing a U-Net architecture based on ConvNeXt (UConvNeXt). This approach enhances the utilization of underlying and deep features in the image during the upsampling process using two types of skip connections, thereby improving texture information. IR-GAN also adds gradient vector loss to generator training, which effectively improves the edge extraction capabilities of the generator. In addition, a multi-scale PatchGAN was included in IR-GAN to enrich local and global image features. Results produced by the proposed model were compared to those of the Pix2Pix and ThermalGAN architectures applied to the IVFG dataset and assessed using five evaluation metrics. Our method produced a structural similarity index measure (SSIM) 10.1% higher than that of Pix2Pix and 12.4% higher than ThermalGAN for the IVFG dataset.
C1 [Ma, Decao; Xian, Yong; Li, Bing; Li, Shaopeng; Zhang, Daqiao] High Tech Inst Xian, Xian 710025, Peoples R China.
   [Li, Shaopeng] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Rocket Force University of Engineering; Tsinghua University
RP Ma, DC (corresponding author), High Tech Inst Xian, Xian 710025, Peoples R China.
EM madecaoedu@163.com
RI li, bing/HTS-1845-2023; Li, Shaopeng/AAM-9713-2020; Ma,
   Decao/KOZ-8038-2024
FU National Natural Science Foundation of China [62103432]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62103432.
CR Abbasi F, 2022, IEEE T WIREL COMMUN, V21, P7582, DOI 10.1109/TWC.2022.3159807
   Aslahishahri M, 2021, IEEE INT CONF COMP V, P1312, DOI 10.1109/ICCVW54120.2021.00152
   Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   BENYOSEF N, 1983, APPL OPTICS, V22, P190, DOI 10.1364/AO.22.000190
   Bi FK, 2022, VISUAL COMPUT, V38, P2581, DOI 10.1007/s00371-021-02133-2
   Chandaliya PK, 2021, 2021 INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2021), DOI 10.1109/IJCB52358.2021.9484329
   Chen CLP, 2014, IEEE T GEOSCI REMOTE, V52, P574, DOI 10.1109/TGRS.2013.2242477
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dion D, 2016, PROC SPIE, V9979, DOI 10.1117/12.2240641
   Goodfellow I. J., 2014, ARXIV
   Gupta S., 2013, International Journal Of Advanced Research In Computer Engineering Technology (IJARCET), V2
   Han T, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20205948
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jacobs P. A., 2006, THERMAL INFRARED CHA, V70
   Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46
   LI B, 2021, COMPOS PART B-ENG
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li LD, 2022, VISUAL COMPUT, V38, P3979, DOI 10.1007/s00371-021-02244-w
   Li YC, 2022, FIRE SAFETY J, V132, DOI 10.1016/j.firesaf.2022.103629
   Liu HJ, 2020, VISUAL COMPUT, V36, P2105, DOI 10.1007/s00371-020-01913-6
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Maheepala M, 2020, IEEE SENS J, V20, P3971, DOI 10.1109/JSEN.2020.2964380
   Manu CM, 2022, VISUAL COMPUT, DOI 10.1007/s00371-077-07516-9
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mizginov V., 2021, INT ARCH PHOTOGRAMME, P155, DOI DOI 10.5194/ISPRS-ARCHIVES-XLIV-2-W1-2021-155-2021
   Mozaffari M. H., 2022, P CAN C ART INT, DOI [10.21428/594757db.7c1cd4-1, DOI 10.21428/594757DB.7C1CD4-1]
   Özkanoglu MA, 2022, PATTERN RECOGN LETT, V155, P69, DOI 10.1016/j.patrec.2022.01.026
   Rao J, 2022, VIRTUAL PHYS PROTOTY, V17, P1047, DOI 10.1080/17452759.2022.2086142
   Razakarivony S, 2016, J VIS COMMUN IMAGE R, V34, P187, DOI 10.1016/j.jvcir.2015.11.002
   Reisfeld E, 2023, VISUAL COMPUT, V39, P2811, DOI 10.1007/s00371-022-02494-2
   Ross V, 2011, PROC SPIE, V8014, DOI 10.1117/12.883548
   Schonfeld E., 2020, 2020 IEEECVF C COMPU, P8204, DOI 10.1109/CVPR42600.2020.00823
   Siddique N, 2021, IEEE ACCESS, V9, P82031, DOI 10.1109/ACCESS.2021.3086020
   Sobel I., 1990, An Isotropic 3x3 Image Gradient Operator, P376, DOI [10.13140/RG.2.1.1912.4965, DOI 10.13140/RG.2.1.1912.4965]
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Thompson DR, 2018, REMOTE SENS ENVIRON, V216, P355, DOI 10.1016/j.rse.2018.07.003
   Uddin MS, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13163257
   Wang LC, 2022, VEHICLE SYST DYN, V60, P1788, DOI 10.1080/00423114.2021.1874428
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xu J., 2022, PROC CVPR IEEE, P1
   Yangyang Ma, 2021, 2021 International Conference on Control, Automation and Information Sciences (ICCAIS), P1029, DOI 10.1109/ICCAIS52680.2021.9624500
   Yilmaz A, 2003, IMAGE VISION COMPUT, V21, P623, DOI 10.1016/S0262-8856(03)00059-3
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SQ, 2022, VISUAL COMPUT, V38, P2125, DOI 10.1007/s00371-021-02272-6
   Zheng L., 2009, MIPPR 2009 MULTISPEC, V7494
NR 47
TC 2
Z9 2
U1 7
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1289
EP 1298
DI 10.1007/s00371-023-02847-5
EA APR 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000963563800001
DA 2024-07-18
ER

PT J
AU Hu, P
   Wang, CJ
   Li, DQ
   Zhao, X
AF Hu, Peng
   Wang, Chenjun
   Li, Dequan
   Zhao, Xin
TI An improved hybrid multiscale fusion algorithm based on NSST for
   infrared-visible images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Image fusion; Multiscale decomposition; Morphological; Support value
   transform; Shearlet transform
ID PERFORMANCE; TRANSFORM; NETWORK
AB The key to improving the fusion quality of infrared-visible images is effectively extracting and fusing complementary information such as bright-dark information and saliency details. For this purpose, an improved hybrid multiscale fusion algorithm inspired by non-subsampled shearlet transform (NSST) is proposed. In this algorithm, firstly, the support value transform (SVT) is used instead of the non-subsampled pyramid as the frequency separator to decompose an image into a set of high-frequency support value images and one low-frequency approximate background. These support value images mainly contain the saliency details from the source image. And then, the shearlet transform of NSST is retained to further extract the saliency edges from these support value images. Secondly, to extract the bright-dark details from the low-frequency approximate background, a morphological multiscale top-bottom hat decomposition is constructed. Finally, the extracted information is combined by different rules and the fused image is reconstructed by the corresponding inverse transforms. Experimental results have shown the proposed algorithm has obvious advantages in retaining saliency details and improving image contrast over those state-of-the-art algorithms.
C1 [Hu, Peng; Wang, Chenjun; Li, Dequan; Zhao, Xin] Anhui Univ Sci & Technol, State Key Lab Min Response & Disaster Prevent & Co, Huainan 232001, Peoples R China.
   [Hu, Peng; Wang, Chenjun; Li, Dequan; Zhao, Xin] Anhui Univ Sci & Technol, Sch Artificial Intelligence, Huainan 232001, Peoples R China.
C3 Anhui University of Science & Technology; Anhui University of Science &
   Technology
RP Hu, P (corresponding author), Anhui Univ Sci & Technol, State Key Lab Min Response & Disaster Prevent & Co, Huainan 232001, Peoples R China.; Hu, P (corresponding author), Anhui Univ Sci & Technol, Sch Artificial Intelligence, Huainan 232001, Peoples R China.
EM aust_hp@163.com
FU Natural Science Research Project of Anhui Educational Committee
   [2022AH050801]; University-level key projects of Anhui University of
   science and technology [QNZD2021-02]; Anhui Provincial Natural Science
   Foundation [2208085ME128]; Scientific Research Foundation for Highlevel
   Talents of Anhui University of Science and Technology [13210679];
   Huainan Science and Technology Planning Project [2021005]
FX AcknowledgementsWe sincerely thank the reviewers and editors for
   carefully checking our manuscript and providing many suggestions. This
   work is supported by the Natural Science Research Project of Anhui
   Educational Committee (No. 2022AH050801), University-level key projects
   of Anhui University of science and technology (No. QNZD2021-02), Anhui
   Provincial Natural Science Foundation (No. 2208085ME128), Scientific
   Research Foundation for Highlevel Talents of Anhui University of Science
   and Technology (No. 13210679), Huainan Science and Technology Planning
   Project (No. 2021005).
CR Averbuch A., 2020, arXiv, DOI [10.48550/arXiv.2008.11595, DOI 10.48550/ARXIV.2008.11595]
   Averbuch A, 2021, SIGNAL PROCESS-IMAGE, V97, DOI 10.1016/j.image.2021.116334
   BAMBERGER RH, 1992, IEEE T SIGNAL PROCES, V40, P882, DOI 10.1109/78.127960
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Candès E, 2006, MULTISCALE MODEL SIM, V5, P861, DOI 10.1137/05064182X
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Cheng BY, 2018, NEUROCOMPUTING, V310, P135, DOI 10.1016/j.neucom.2018.05.028
   Deepika T, 2020, Arxiv, DOI [arXiv:2007.11488, DOI 10.48550/ARXIV.2007.11488]
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Guanqiu Qi, 2019, International Journal of Simulation and Process Modelling, V14, P559
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He G., 2019, IEEE GEOSCI REMOTE S, V99, P1
   Hu P, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102977
   Hu P, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102994
   Kong Zhuo, 1994, J PHYS C SER, V2021, DOI DOI 10.1088/1742-6596/2232/1/012018
   KUTYNIOK G., 2007, J WAVELET THEORY APP, V1, P1
   Kwon HJ, 2021, CHEMOSENSORS, V9, DOI 10.3390/chemosensors9040075
   Li H, 2016, INFRARED PHYS TECHN, V74, P28, DOI 10.1016/j.infrared.2015.11.002
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2011, INFORM FUSION, V12, P74, DOI 10.1016/j.inffus.2010.03.002
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Ma JY, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103016
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2
   Sun CQ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122162
   Tan Z, 2021, IEEE T GEOSCI REMOTE, V60
   Wang XT, 2020, ARAB J SCI ENG, V45, P3245, DOI 10.1007/s13369-020-04351-7
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yan XA, 2019, IEEE ACCESS, V7, P123436, DOI 10.1109/ACCESS.2019.2937751
   Zhao C, 2019, INT J WAVELETS MULTI, V17, DOI 10.1142/S0219691319500450
   Zheng S, 2007, IEEE T IMAGE PROCESS, V16, P1831, DOI 10.1109/TIP.2007.896687
   Zheng ZF, 2019, IEEE ACCESS, V7, P118472, DOI 10.1109/ACCESS.2019.2936295
   Zhu P, 2017, OPT REV, V24, P370, DOI 10.1007/s10043-017-0331-1
NR 45
TC 2
Z9 2
U1 5
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 APR 3
PY 2023
DI 10.1007/s00371-023-02844-8
EA APR 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C5WF3
UT WOS:000962608500002
DA 2024-07-18
ER

PT J
AU Wei, HY
   Zhang, QQ
   Qin, YG
   Li, X
   Qian, YR
AF Wei, Hongyang
   Zhang, Qianqian
   Qin, Yugang
   Li, Xiang
   Qian, Yurong
TI YOLOF-F: you only look one-level feature fusion for traffic sign
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Traffic sign detection; Multi-scale detection; Single-level detector;
   Feature fusion; Corner pooling
AB This paper proposes a detector that focuses on multi-scale detection problems and effectively enhances the detection performance to solve the problem that is hard to detect minor traffic signs. This detector, called YOLOF-F (you only look one-level feature fusion), is a single-stage detector that extracts multi-scale feature information from a single layer of fusion feature. First, we propose FFM (feature fusion module) to fuse different scales. Next, we offer a new encoder CDE (corner dilated encoder) to enhance the angular point information in the feature map, improve position regression accuracy, and maintain a faster detection speed. Finally, YOLOF-F achieved 74.57% and 77.23% of the AP on the GTSDB and CTSD datasets and reached 32 FPS. Extensive experiments validate that YOLOF-F is faster and more effective than most traffic sign detection methods.
C1 [Wei, Hongyang; Zhang, Qianqian; Qin, Yugang; Li, Xiang; Qian, Yurong] Xinjiang Univ, Sch Software, Urumqi 830000, Peoples R China.
   [Wei, Hongyang; Zhang, Qianqian; Qin, Yugang; Li, Xiang; Qian, Yurong] Key Lab Signal Detect & Proc, Urumqi 83000, Peoples R China.
   [Wei, Hongyang; Zhang, Qianqian; Qin, Yugang; Li, Xiang; Qian, Yurong] Xinjiang Univ, Key Lab Software Engn, Urumqi 83000, Peoples R China.
C3 Xinjiang University; Xinjiang University
RP Qian, YR (corresponding author), Xinjiang Univ, Sch Software, Urumqi 830000, Peoples R China.; Qian, YR (corresponding author), Key Lab Signal Detect & Proc, Urumqi 83000, Peoples R China.; Qian, YR (corresponding author), Xinjiang Univ, Key Lab Software Engn, Urumqi 83000, Peoples R China.
EM weihy@stu.xju.edu.cn; zhangqianqian@stu.xju.edu.cn;
   qinyugang1998@gmail.com; lixiang@stu.xju.edu.cn; qyr@xju.edu.cn
FU National Science Foundation of China [U1803261]; National Natural
   Science Foundation of China [61966035]; Xinjiang Uygur Autonomous Region
   Innovation Team [XJEDU2017T002]; international cooperation project of
   China-region Science and Technology Department "Data-driven China-Russia
   cloud computing sharing platform construction" [2020E01023]; research on
   depth learning labeling method based on multi-feature fusion
   [2020D01A34]; research on Video Information Processing Technology Based
   on Public Safety [U1803261]
FX This work was supported by the National Science Foundation of China
   under Grant U1803261 and funded by the National Natural Science
   Foundation of China (61966035), Xinjiang Uygur Autonomous Region
   Innovation Team (XJEDU2017T002), the international cooperation project
   of China-region Science and Technology Department "Data-driven
   China-Russia cloud computing sharing platform construction"
   (2020E01023), research on depth learning labeling method based on
   multi-feature fusion (2020D01A34), and research on Video Information
   Processing Technology Based on Public Safety(U1803261).
CR Bi ZQ, 2021, INT J MACH LEARN CYB, V12, P3069, DOI 10.1007/s13042-020-01185-5
   Cao JH, 2021, IEEE ACCESS, V9, P3579, DOI 10.1109/ACCESS.2020.3047414
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Houben S, 2013, IEEE INT C INTELL TR, P7, DOI 10.1109/ITSC.2013.6728595
   Jin YM, 2020, IEEE ACCESS, V8, P38931, DOI 10.1109/ACCESS.2020.2975828
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li Y., 2020, C INT TRANSP SYST IT, P1
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu F, 2021, INT J PATTERN RECOGN, V35, DOI 10.1142/S021800142152008X
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Rajendran Shehan P., 2019, 2019 International Conference on Communication and Electronics Systems (ICCES), P784, DOI 10.1109/ICCES45898.2019.9002557
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren K, 2021, J REAL-TIME IMAGE PR, V18, P2181, DOI 10.1007/s11554-021-01102-1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   VAILLANT R, 1994, IEE P-VIS IMAGE SIGN, V141, P245, DOI 10.1049/ip-vis:19941301
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan JX, 2021, J SIGNAL PROCESS SYS, V93, P899, DOI 10.1007/s11265-020-01614-2
   Wei Hongyang, 2022, APPL INTELL, P1
   XiongFei Liu, 2020, IOP Conference Series: Materials Science and Engineering, V787, DOI 10.1088/1757-899X/787/1/012034
   Zhang JM, 2017, ALGORITHMS, V10, DOI 10.3390/a10040127
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu XZ, 2021, Arxiv, DOI arXiv:2010.04159
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 41
TC 6
Z9 6
U1 2
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 747
EP 760
DI 10.1007/s00371-023-02813-1
EA MAR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000952181900001
DA 2024-07-18
ER

PT J
AU Liu, ZK
   He, K
   Zhang, DZ
   Wang, L
AF Liu, Zikang
   He, Kai
   Zhang, Dazhuang
   Wang, Lei
TI Local feature guidance framework for robust 3D point cloud registration
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point cloud registration; Local feature; Internal correlation;
   Spatial correspondence; Deep learning
ID GO-ICP; HISTOGRAMS
AB 3D point cloud registration is a basic task in computer vision. In recent years, various of learning-based methods have been proposed to solve this problem. These methods effectively overcome the original problem of over-reliance on initial conditions and enhance the ability of obtaining the corresponding relationship. However, few methods pay enough attention to local features and tend to cause some mismatches. Therefore, this paper proposes two networks to extract local features sufficiently. To obtain a more accurate correspondence relationship between the point clouds, we propose a feature weight allocation network (FWANet), in which the expression ability of feature is enhanced using the proposed significant feature extraction module. Besides that, we utilize an interference elimination module to remove the interference points and enhance the internal correlation of point clouds. We also propose a spatial structural generation network (SSGNet), which fully utilizes the spatial location information to determine the spatial correspondence and generate a reliable connection after concatenating multi-dimensional features. At last, a complete feature space can be effectively captured after combining our FWANet with SSGNet together. We conducted multiple experiments on ModelNet40 datasets and achieved excellent results. Experimental results on four types of data demonstrate the superiority of our algorithm against the state-of-the-art ones. Our code will be available at as soon as the paper is accepted.
C1 [Liu, Zikang; He, Kai; Zhang, Dazhuang; Wang, Lei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP He, K (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM hekai@tju.edu.cn
FU National Natural Science Foundation of China [62171314]
FX This study was supported by the National Natural Science Foundation of
   China (No. 62171314), and the recipient of the support was Kai He
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Campbell D, 2019, PROC CVPR IEEE, P11788, DOI 10.1109/CVPR.2019.01207
   Campbell D, 2016, PROC CVPR IEEE, P5685, DOI 10.1109/CVPR.2016.613
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Deschaud JE, 2018, IEEE INT CONF ROBOT, P2480
   Dym N, 2019, IEEE I CONF COMP VIS, P1628, DOI 10.1109/ICCV.2019.00171
   Feng RT, 2021, IEEE GEOSC REM SEN M, V9, P120, DOI 10.1109/MGRS.2021.3081763
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Han J, 2015, CHIN AUTOM CONGR, P746, DOI 10.1109/CAC.2015.7382597
   Han L, 2019, IEEE T ROBOT, V35, P498, DOI 10.1109/TRO.2018.2882730
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Kurobe A, 2020, IEEE ROBOT AUTOM LET, V5, P3960, DOI 10.1109/LRA.2020.2970946
   Li DS, 2022, APPL INTELL, V52, P9638, DOI 10.1007/s10489-021-03055-1
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Liu YL, 2018, LECT NOTES COMPUT SC, V11216, P460, DOI 10.1007/978-3-030-01258-8_28
   Lu WX, 2019, IEEE I CONF COMP VIS, P12, DOI 10.1109/ICCV.2019.00010
   Lu WX, 2019, PROC CVPR IEEE, P6382, DOI 10.1109/CVPR.2019.00655
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   Mafarja MM, 2017, NEUROCOMPUTING, V260, P302, DOI 10.1016/j.neucom.2017.04.053
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mozaffari M. Hamed, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P421, DOI 10.1007/978-3-030-64559-5_33
   Mozaffari MH, 2020, METHODS, V179, P26, DOI 10.1016/j.ymeth.2020.05.011
   Papazov C, 2012, INT J ROBOT RES, V31, P538, DOI 10.1177/0278364911436019
   Paszke A, 2019, ADV NEUR IN, V32
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sarode V., 2019, IEEE INT C COMPUTER
   Segal Aleksandr, 2009, ROBOTICS SCI SYSTEMS, V2
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y., 2019, Advances in Neural Information Processing Systems, V32
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184
   Yoo H, 2020, J MECH SCI TECHNOL, V34, P2667, DOI 10.1007/s12206-020-0540-6
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 45
TC 0
Z9 0
U1 2
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6459
EP 6472
DI 10.1007/s00371-022-02739-0
EA DEC 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000895050300003
DA 2024-07-18
ER

PT J
AU Zhang, XK
   Liu, WB
   Xing, WW
   Wei, X
AF Zhang, Xiukun
   Liu, Weibin
   Xing, Weiwei
   Wei, Xiang
TI GCAENet: global-class context with advanced edge network for single
   human parsing
SO VISUAL COMPUTER
LA English
DT Article
DE Human parsing; GCAENet; Global-class context; Edge information; Hybrid
   loss
AB In this paper, we propose an effective single human parsing framework, called global-class context with advanced edge network (GCAENet), which explores the human parsing task in terms of both contextual information and edge information. Since rich contextual information is crucial for pixel-level classification tasks, e.g., human parsing, some researches of human parsing have adopted atrous spatial pyramid pooling and pyramid pooling module to exploit context information. However, these methods focus only on global contextual information and not adequate attention to class contextual information. Hence, we propose an integrated approach, where a global-class context module is introduced to join the global context and the class context. Furthermore, for the problems of the boundary confusion between adjacent parts and intra-class semantic inconsistency in parsing results, we propose advanced edge module based on the edge perceiving module from Context Embedding with Edge Perceiving framework to attain more refined edge prediction map and provide guidance edge information for parsing task. In addition, we also utilize cross-entropy and Lovasz-Softmax double loss as parsing supervise. Experimental results demonstrate the proposed GCAENet achieves state-of-the-art accuracy on LIP and ATR datasets.
C1 [Zhang, Xiukun; Liu, Weibin] Beijing Jiaotong Univ, Inst informat Sci, Beijing 100044, Peoples R China.
   [Xing, Weiwei; Wei, Xiang] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Liu, WB (corresponding author), Beijing Jiaotong Univ, Inst informat Sci, Beijing 100044, Peoples R China.
EM wbliu@bjtu.edu.cn
FU Beijing Natural Science Foundation; National Natural Science Foundation
   of China;  [4212025];  [61876018];  [61906014];  [61976017]
FX AcknowledgementsThis research is partially supported by the Beijing
   Natural Science Foundation (No. 4212025) and the National Natural
   Science Foundation of China (Nos. 61876018, No. 61906014, No. 61976017).
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Berman M., 2018, P IEEE C COMPUTER VI
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Dong J, 2013, IEEE I CONF COMP VIS, P3408, DOI 10.1109/ICCV.2013.423
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/cvpr.2019.00763, 10.1109/CVPR.2019.00763]
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Jaccard P., 1901, B SOC VAUD SCI NAT, V37, P547, DOI DOI 10.5169/SEALS-266450
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2017, IEEE T PATTERN ANAL, V39, P115, DOI 10.1109/TPAMI.2016.2537339
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   LIU S, 2015, PROC CVPR IEEE, P1419, DOI DOI 10.1109/CVPR.2015.7298748
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Luo XH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P654, DOI 10.1145/3240508.3240634
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Simo-Serra E, 2015, LECT NOTES COMPUT SC, V9005, P64, DOI 10.1007/978-3-319-16811-1_5
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang Y, 2017, SPRING SER CHALLENGE, P273, DOI 10.1007/978-3-319-57021-1_9
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang SY, 2021, IEEE T CIRC SYST VID, V31, P1016, DOI 10.1109/TCSVT.2020.2990531
   Zhang XM, 2020, NEUROCOMPUTING, V402, P375, DOI 10.1016/j.neucom.2020.03.096
   Zhang Z., 2020, P IEEE C COMPUTER VI
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204
   Zheng CX, 2018, VISUAL COMPUT, V34, P735, DOI 10.1007/s00371-017-1411-8
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 44
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6379
EP 6394
DI 10.1007/s00371-022-02735-4
EA DEC 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000895050300001
DA 2024-07-18
ER

PT J
AU Gadipudi, N
   Elamvazuthi, I
   Izhar, LI
   Tiwari, L
   Hebbalaguppe, R
   Lu, CK
   Doss, ASA
AF Gadipudi, Nivesh
   Elamvazuthi, Irraivan
   Izhar, Lila Iznita
   Tiwari, Lokender
   Hebbalaguppe, Ramya
   Lu, Cheng-Kai
   Doss, Arockia Selvakumar Arockia
TI A review on monocular tracking and mapping: from model-based to
   data-driven methods
SO VISUAL COMPUTER
LA English
DT Review
DE Visual odometry; Visual simultaneous localization and mapping; Camera
   pose estimation; Filtering; Bundle adjustment; Deep learning
ID VISUAL ODOMETRY; SIMULTANEOUS LOCALIZATION; SLAM; VISION; CAMERA; MAP;
   OPTIMIZATION; MOTION; STEREO; FILTER
AB Visual odometry and visual simultaneous localization and mapping aid in tracking the position of a camera and mapping the surroundings using images. It is an important part of robotic perception. Tracking and mapping using a monocular camera is cost-effective, requires less calibration effort, and is easy to deploy across a wide range of applications. This paper provides an extensive review of the developments for the first two decades of the twenty-first century. Astounding results from early methods based on filtering have intrigued the community to extend these algorithms using other forms of techniques like bundle adjustment and deep learning. This article starts by introducing the basic sensor systems and analyzing the evolution of monocular tracking and mapping algorithms through bibliometric data. Then, it covers the overview of filtering and bundle adjustment methods, followed by recent advancements in methods using deep learning with the mathematical constraints applied on the networks. Finally, the popular benchmarks available for developing and evaluating these algorithms are presented along with a comparative study on a different class of algorithms. It is anticipated that this article will serve as the latest introductory tool and further ignite the interest of the community to solve current and future impediments.
C1 [Gadipudi, Nivesh; Elamvazuthi, Irraivan; Izhar, Lila Iznita] Univ Teknol Petronas, Dept Elect & Elect Engn, Smart Assist & Rehabil Technol SMART Res Grp, Bandar Seri Iskandar 32610, Malaysia.
   [Tiwari, Lokender; Hebbalaguppe, Ramya] TCS Res, New Delhi, India.
   [Hebbalaguppe, Ramya] Indian Inst Technol Delhi, New Delhi, India.
   [Lu, Cheng-Kai] Natl Taiwan Normal Univ, Dept Elect Engn, Taipei, Taiwan.
   [Doss, Arockia Selvakumar Arockia] Vellore Inst Technol, Sch Mech Engn, Design & Automat Res Grp, Chennai 600127, Tamil Nadu, India.
C3 Universiti Teknologi Petronas; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Delhi; National
   Taiwan Normal University; Vellore Institute of Technology (VIT); VIT
   Chennai
RP Elamvazuthi, I (corresponding author), Univ Teknol Petronas, Dept Elect & Elect Engn, Smart Assist & Rehabil Technol SMART Res Grp, Bandar Seri Iskandar 32610, Malaysia.
EM nivesh_18001319@utp.edu.my; elamvazuthi@utp.edu.my;
   lila.izhar@utp.edu.my; tiwarilokender@gmail.com;
   ramya.hebbalaguppe@tcs.com; cklu@ntnu.edu.tw;
   arockia.selvalcumar@vit.ac.in
RI Arockia Doss, Arockia Selvakumar/M-2457-2017; Elamvazuthi,
   Irraivan/J-2508-2013
OI Arockia Doss, Arockia Selvakumar/0000-0002-7810-6994; Elamvazuthi,
   Irraivan/0000-0002-4721-9400; Gadipudi, Nivesh/0000-0001-8403-7270
FU YUTP Grant [015LC0-243]
FX The authors are grateful to the sponsors who provided YUTP Grant
   (015LC0-243) for this project.
CR Abadi Martin, 2016, arXiv
   Almalioglu Y, 2019, IEEE INT CONF ROBOT, P5474, DOI [10.1109/icra.2019.8793512, 10.1109/ICRA.2019.8793512]
   Altwaijry H., 2016, BMVC
   Angeli A, 2008, IEEE INT CONF ROBOT, P1842, DOI 10.1109/ROBOT.2008.4543475
   Aqel MOA, 2016, SPRINGERPLUS, V5, DOI 10.1186/s40064-016-3573-7
   Armangué X, 2003, IMAGE VISION COMPUT, V21, P205, DOI 10.1016/S0262-8856(02)00154-3
   Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144
   Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9_46
   Barath D, 2022, PROC CVPR IEEE, P15723, DOI 10.1109/CVPR52688.2022.01529
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Bian J.W., 2019, ARXIV
   Blanco-Claraco JL, 2014, INT J ROBOT RES, V33, P207, DOI 10.1177/0278364913507326
   Blöchliger F, 2018, IEEE INT CONF ROBOT, P3818
   Brachmann E, 2019, IEEE I CONF COMP VIS, P7524, DOI 10.1109/ICCV.2019.00762
   Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489
   Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Cadena C, 2010, ROBOT AUTON SYST, V58, P1207, DOI 10.1016/j.robot.2010.08.003
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cai Q., 2021, ARXIV
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638
   Çelik K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1566, DOI 10.1109/IROS.2009.5354050
   Chen L, 2021, GEO-SPAT INF SCI, V24, P58, DOI 10.1080/10095020.2020.1843376
   Chen Tianqi, 2015, ARXIV
   Cheng JH, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9040202
   Civera J, 2008, IEEE T ROBOT, V24, P932, DOI 10.1109/TRO.2008.2003276
   Clark R., 2018, ARXIV
   Clemente LauraA., 2007, INROBOTICS SCI SYSTE, V2, P2
   Concha A, 2015, IEEE INT C INT ROBOT, P5686, DOI 10.1109/IROS.2015.7354184
   Costante G, 2018, IEEE ROBOT AUTOM LET, V3, P1735, DOI 10.1109/LRA.2018.2803211
   Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717
   Davison AJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1403
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Debeunne C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072068
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   DeTone Daniel, 2016, ARXIV160603798
   Ding Y., 2022, P IEEECVF C COMPUTER, P12766
   Dissanayake G, 2002, AUTON ROBOT, V12, P267, DOI 10.1023/A:1015217631658
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong YC, 2021, IEEE T INTELL TRANSP, V22, P36, DOI 10.1109/TITS.2019.2952159
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Engel J., 2016, ARXIV, DOI DOI 10.1177/2F0278364911400640
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forster C, 2017, IEEE T ROBOT, V33, P249, DOI 10.1109/TRO.2016.2623335
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810
   Gadipudi N, 2022, NEURAL COMPUT APPL, V34, P18823, DOI 10.1007/s00521-022-07484-y
   Gadipudi N, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21238155
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geyer J., 2020, A2D2 AUDI AUTONOMOUS
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grasa OG, 2014, IEEE T MED IMAGING, V33, P135, DOI 10.1109/TMI.2013.2282997
   Guivant JE, 2001, IEEE T ROBOTIC AUTOM, V17, P242, DOI 10.1109/70.938382
   Guizilini V, 2013, INT J ROBOT RES, V32, P526, DOI 10.1177/0278364912472245
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   He B, 2015, SENSORS-BASEL, V15, P19852, DOI 10.3390/s150819852
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   Helmick DM, 2004, AEROSP CONF PROC, P772
   Ho T., 2015, 2015 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2015.7156568
   Holmes SA, 2009, IEEE T PATTERN ANAL, V31, P1251, DOI 10.1109/TPAMI.2008.189
   Hoseini SA, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7110305
   Huang AS, 2010, INT J ROBOT RES, V29, P1595, DOI 10.1177/0278364910384295
   Huang JS, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017), P2360, DOI 10.1109/ROBIO.2017.8324772
   Huang SD, 2007, IEEE T ROBOT, V23, P1036, DOI 10.1109/TRO.2007.903811
   Jaderberg M, 2015, ARXIV
   Jiang WC, 2017, J VIS COMMUN IMAGE R, V48, P419, DOI 10.1016/j.jvcir.2017.03.015
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kirsanov P, 2019, IEEE INT C INT ROBOT, P2470, DOI [10.1109/IROS40897.2019.8967921, 10.1109/iros40897.2019.8967921]
   Klein G., 2007, IEEE and ACM Intl. Sym. on Mixed and Augmented Reality (ISMAR), P225, DOI DOI 10.1109/ISMAR.2007.4538852
   Konda Kishore, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P486
   Koumis AS, 2019, IEEE INT C INT ROBOT, P265, DOI [10.1109/IROS40897.2019.8967919, 10.1109/iros40897.2019.8967919]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo XY, 2020, IEEE COMPUT SOC CONF, P160, DOI 10.1109/CVPRW50498.2020.00026
   Kwon J, 2010, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2010.5539789
   Lee SH, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013029
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li RH, 2018, COGN COMPUT, V10, P875, DOI 10.1007/s12559-018-9591-8
   Li SK, 2019, IEEE I CONF COMP VIS, P2851, DOI 10.1109/ICCV.2019.00294
   Li W., 2018, ARXIV
   Li YL, 2015, NEUROCOMPUTING, V149, P736, DOI 10.1016/j.neucom.2014.08.003
   Liu JG, 2014, NEUROCOMPUTING, V145, P269, DOI 10.1016/j.neucom.2014.05.034
   Liu Q, 2019, IEEE ACCESS, V7, P18076, DOI 10.1109/ACCESS.2019.2896988
   Liu XT, 2020, IEEE T MED IMAGING, V39, P1438, DOI 10.1109/TMI.2019.2950936
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   Liu YL, 2022, IEEE T INTELL TRANSP, V23, P5387, DOI 10.1109/TITS.2021.3053412
   Loo SY, 2019, IEEE INT CONF ROBOT, P5218, DOI [10.1109/ICRA.2019.8794425, 10.1109/icra.2019.8794425]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Y, 2021, AAAI CONF ARTIF INTE, V35, P2260
   Lui V, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.116
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Mahmoud N, 2017, INT J COMPUT ASS RAD, V12, P1, DOI 10.1007/s11548-016-1444-x
   Mahon I, 2008, IEEE T ROBOT, V24, P1002, DOI 10.1109/TRO.2008.2004888
   Maity S, 2017, IEEE INT CONF COMP V, P2408, DOI 10.1109/ICCVW.2017.284
   Majdik AL, 2017, INT J ROBOT RES, V36, P269, DOI 10.1177/0278364917702237
   Marchand É, 2002, VISUAL COMPUT, V18, P1, DOI 10.1007/s003710100122
   Martins PF, 2020, IEEE INT CONF AUTON, P306, DOI 10.1109/ICARSC49921.2020.9096104
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Memon AR, 2020, ROBOT AUTON SYST, V126, DOI 10.1016/j.robot.2020.103470
   Merrill N., 2018, ARXIV
   Milz S, 2018, IEEE COMPUT SOC CONF, P360, DOI 10.1109/CVPRW.2018.00062
   Montemerlo M, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P593
   Montemerlo M., 2003, P INT JOINT C ARTIFI, P1151, DOI [10.5555/1630659.1630824, DOI 10.5555/1630659.1630824]
   Muller P, 2017, IEEE WINT CONF APPL, P624, DOI 10.1109/WACV.2017.75
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Nistér D, 2004, PROC CVPR IEEE, P652
   Nourani-Vatani N, 2009, IEEE INT CONF ROBOT, P1411
   Ono Y, 2018, ADV NEUR IN, V31
   Pan J, 2022, ISPRS J PHOTOGRAMM, V183, P439, DOI 10.1016/j.isprsjprs.2021.11.007
   Pandey G, 2011, INT J ROBOT RES, V30, P1543, DOI 10.1177/0278364911400640
   Pandey T, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041313
   Paszke A, 2019, ADV NEUR IN, V32
   Paz LM, 2008, IEEE T ROBOT, V24, P946, DOI 10.1109/TRO.2008.2004637
   Persson M, 2018, LECT NOTES COMPUT SC, V11208, P334, DOI 10.1007/978-3-030-01225-0_20
   Piniés P, 2008, IEEE T ROBOT, V24, P1094, DOI 10.1109/TRO.2008.2004636
   Prasad V, 2019, IEEE WINT CONF APPL, P2087, DOI 10.1109/WACV.2019.00226
   Pumarola A., 2017, P 2017 IEEE INT C RO, P4503, DOI DOI 10.1109/ICRA.2017.7989522
   Pupilli Mark., 2005, BMVC
   Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640
   Ranftl R, 2018, LECT NOTES COMPUT SC, V11205, P292, DOI 10.1007/978-3-030-01246-5_18
   Richter S.R., 2021, ARXIV
   Roberts R, 2008, IEEE INT CONF ROBOT, P47, DOI 10.1109/ROBOT.2008.4543185
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Saputra MRU, 2019, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2019.00035
   Saputra MRU, 2019, IEEE INT CONF ROBOT, P3549, DOI [10.1109/icra.2019.8793581, 10.1109/ICRA.2019.8793581]
   Saputra MRU, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3177853
   Sarlin PE, 2021, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR46437.2021.00326
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Shah S., 2017, Field and service robotics, DOI 10.1007/978-3-319-67361-5_40
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911
   Song SY, 2014, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR.2014.203
   Strasdat H, 2010, IEEE INT CONF ROBOT, P2657, DOI 10.1109/ROBOT.2010.5509636
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Sun QY, 2019, CHAOS, V29, DOI 10.1063/1.5120605
   Sünderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI
   Taheri H, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104032
   Taketomi T, 2017, IPSJ Trans. Comput. Vis. Appl, V9, P1, DOI [10.1186/s41074-017-0027-2, DOI 10.1186/S41074-017-0027-2]
   Tang C., 2018, ARXIV
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tiwari Lokender, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P437, DOI 10.1007/978-3-030-58621-8_26
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Tseng KK, 2021, ENTERP INF SYST-UK, V15, P1162, DOI 10.1080/17517575.2019.1698772
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Nguyen T, 2018, IEEE ROBOT AUTOM LET, V3, P2346, DOI 10.1109/LRA.2018.2809549
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Valada A, 2018, IEEE INT CONF ROBOT, P6939, DOI 10.1109/ICRA.2018.8462979
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wang AJ, 2020, IEEE T IMAGE PROCESS, V29, P4130, DOI 10.1109/TIP.2020.2968751
   Wang HJ, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/6367273
   Wang S, 2018, INT J ROBOT RES, V37, P513, DOI 10.1177/0278364917734298
   Wang WS, 2020, IEEE INT C INT ROBOT, P4909, DOI 10.1109/IROS45743.2020.9341801
   Wang XW, 2020, IEEE ACCESS, V8, P175220, DOI 10.1109/ACCESS.2020.3025557
   Wei YM, 2013, J ZHEJIANG U-SCI C, V14, P486, DOI 10.1631/jzus.CIDE1302
   Woodman O., 2007, An introduction to inertial navigation
   Xu S., 2021 DIGITAL IMAGE C, P1
   Xue F, 2019, PROC CVPR IEEE, P8567, DOI 10.1109/CVPR.2019.00877
   Yan K, 2019, IEEE ACCESS, V7, P147523, DOI 10.1109/ACCESS.2019.2946387
   Yang AL, 2017, COMM COM INF SC, V761, P410, DOI 10.1007/978-981-10-6370-1_41
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yang SA, 2019, INT CONF MACH LEARN, P1, DOI 10.1109/icmlc48188.2019.8949230
   Yang Y, 2019, COMPLEXITY, DOI 10.1155/2019/8176489
   Yi K., 2016, ARXIV
   Yin XC, 2017, IEEE I CONF COMP VIS, P5871, DOI 10.1109/ICCV.2017.625
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Younes G., 2016, ARXIV
   Younes G, 2017, ROBOT AUTON SYST, V98, P67, DOI 10.1016/j.robot.2017.09.010
   Yousif K., 2015, Intell. Ind. Syst., V1, P289, DOI [DOI 10.1007/S40903-015-0032-7, 10.1007/s40903-015-0032-7]
   Yu K, 2020, VISUAL COMPUT, V36, P2051, DOI 10.1007/s00371-020-01911-8
   Yun DS, 2014, I C INF COMM TECH CO, P609, DOI 10.1109/ICTC.2014.6983225
   Zaffar M, 2018, NASA ESA CONF, P285, DOI 10.1109/AHS.2018.8541483
   Zhai GY, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107187
   Zhang JN, 2020, INT J MACH LEARN CYB, V11, P615, DOI 10.1007/s13042-019-01020-6
   Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561
   Zhao CQ, 2021, IEEE T NEUR NET LEAR, V32, P5392, DOI 10.1109/TNNLS.2020.3044181
   Zhou DF, 2020, IEEE T INTELL TRANSP, V21, P791, DOI 10.1109/TITS.2019.2900330
   Zhou HZ, 2020, INT J COMPUT VISION, V128, P756, DOI 10.1007/s11263-019-01221-0
   Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu R, 2022, NEUROCOMPUTING, V467, P22, DOI 10.1016/j.neucom.2021.09.029
   Zou Y., 2020, ECCV
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zubizarreta J, 2020, IEEE T ROBOT, V36, P1363, DOI 10.1109/TRO.2020.2991614
NR 199
TC 1
Z9 1
U1 11
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5897
EP 5924
DI 10.1007/s00371-022-02702-z
EA NOV 2022
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000884954800001
DA 2024-07-18
ER

PT J
AU Guo, ZH
   Han, DZ
AF Guo, Zihan
   Han, Dezhi
TI Multi-modal co-attention relation networks for visual question answering
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Visual question answering; Co-attention; Visual object
   relation reasoning
ID SCHEME
AB The current mainstream visual question answering (VQA) models only model the object-level visual representations but ignore the relationships between visual objects. To solve this problem, we propose a Multi-Modal Co-Attention Relation Network (MCARN) that combines co-attention and visual object relation reasoning. MCARN can model visual representations at both object-level and relation-level, and stacking its visual relation reasoning module can further improve the accuracy of the model on Number questions. Inspired by MCARN, we propose two models, RGF-CA and Cos-Sin+CA, which combine co-attention with the relative geometry features of visual objects, and achieve excellent comprehensive performance and higher accuracy on Other questions respectively. Extensive experiments and ablation studies based on the benchmark dataset VQA 2.0 prove the effectiveness of our models, and also verify the synergy of co-attention and visual object relation reasoning in VQA task.
C1 [Guo, Zihan; Han, Dezhi] Shanghai Maritime Univ, Coll Informat Engn, 1550 Haigang Ave, Shanghai 201306, Peoples R China.
C3 Shanghai Maritime University
RP Guo, ZH (corresponding author), Shanghai Maritime Univ, Coll Informat Engn, 1550 Haigang Ave, Shanghai 201306, Peoples R China.
EM guo_zihan11@163.com; dzhan@shmtu.edu.cn
OI , Zihan/0000-0001-6193-1341
FU National Natural Science Foundation of China [61873160, 61672338];
   Natural Science Foundation of Shanghai [21ZR1426500]
FX This research is supported by the National Natural Science Foundation of
   China under Grant 61873160 and Grant 61672338, and the Natural Science
   Foundation of Shanghai under Grant 21ZR1426500. We thank all the
   reviewers for their constructive comments and helpful suggestions.
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Chen K, 2015, ARXIV
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Chung Junyoung, 2014, ARXIV14123555
   Cui MM, 2020, IEEE T VEH TECHNOL, V69, P15815, DOI 10.1109/TVT.2020.3036631
   Cui MM, 2019, IEEE INTERNET THINGS, V6, P9076, DOI 10.1109/JIOT.2019.2927497
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo ZH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20236758
   Han DZ, 2022, IEEE T IND INFORM, V18, P3530, DOI 10.1109/TII.2021.3114621
   Han DZ, 2021, SOFT COMPUT, V25, P5411, DOI 10.1007/s00500-020-05539-7
   Han DZ, 2022, IEEE T DEPEND SECURE, V19, P316, DOI 10.1109/TDSC.2020.2977646
   HE K, 2016, CVPR, V1, P770
   He SR, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174897
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hosseinabad SH, 2021, VISUAL COMPUT, V37, P119, DOI 10.1007/s00371-019-01786-4
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lao MR, 2018, IEEE ACCESS, V6, P31516, DOI 10.1109/ACCESS.2018.2844789
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li HZ, 2022, IEEE INTERNET THINGS, V9, P4704, DOI 10.1109/JIOT.2021.3107846
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H., 2018, CMSA 2018
   Liu Y, 2019, APPL SOFT COMPUT, V82, DOI 10.1016/j.asoc.2019.105584
   Lu JS, 2016, ADV NEUR IN, V29
   Lu P, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1880, DOI 10.1145/3219819.3220036
   Malinowski M, 2014, ADV NEUR IN, V27
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Peng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1202, DOI 10.1145/3343031.3350925
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Rahman T, 2021, IEEE COMPUT SOC CONF, P1653, DOI 10.1109/CVPRW53098.2021.00181
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Ren FJ, 2020, IEEE ACCESS, V8, P50626, DOI 10.1109/ACCESS.2020.2980024
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santoro A, 2017, ADV NEUR IN, V30
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang P, 2017, PROC CVPR IEEE, P3909, DOI 10.1109/CVPR.2017.416
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan F, 2022, VISUAL COMPUT, V38, P3097, DOI 10.1007/s00371-022-02524-z
   Yang C, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9111882
   Yang ZQ, 2020, IEEE IMAGE PROC, P1411, DOI 10.1109/ICIP40778.2020.9190771
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z., 2019, arXiv
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang Y., 2018, ICLR, V1
   Zhang Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4383
NR 63
TC 6
Z9 6
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5783
EP 5795
DI 10.1007/s00371-022-02695-9
EA OCT 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000875523700001
DA 2024-07-18
ER

PT J
AU He, YQ
   Xia, GH
   Feng, HC
   Wang, Z
AF He, Yunqian
   Xia, Guihua
   Feng, Hongchao
   Wang, Zhe
TI PCTP: point cloud transformer pooling block for points set abstraction
   structure
SO VISUAL COMPUTER
LA English
DT Article
DE Self-attention; Transformer; Point cloud; Pooling
AB Point cloud is a simple but accurate form of data in the 3D domain, and its disorder brings the challenge of feature representation. The transformer structure which has been successfully used in natural language processing helps to establish connections between discrete points in the point cloud data. In this work, by focusing on adapting the self-attention mechanism to point cloud data, we propose a point cloud transformer pooling (PCTP) method combined with the typical set abstraction (SA) structure. In the proposed PCTP, we use the transformer structure to fuse non-local features while pooling local features. The SA structure is widely used in various point cloud networks for various tasks, so we apply the PCTP module to multiple baselines containing SA-like structures. The preliminary experimental results show that the proposed PCTP can significantly improve multiple tasks with a small additional computational cost.
C1 [He, Yunqian; Xia, Guihua; Feng, Hongchao; Wang, Zhe] Harbin Engn Univ, Coll Intelligent Syst Sci & Engn, Harbin 150001, Peoples R China.
   [He, Yunqian; Xia, Guihua] Harbin Engn Univ, Minist Educ, Key Lab Intelligent Technol & Applicat Marine Equ, Harbin 150001, Peoples R China.
   [He, Yunqian; Xia, Guihua] Heilongjiang Prov Key Lab Environm Intelligent Pe, Harbin 150001, Peoples R China.
C3 Harbin Engineering University; Harbin Engineering University
RP Xia, GH (corresponding author), Harbin Engn Univ, Coll Intelligent Syst Sci & Engn, Harbin 150001, Peoples R China.; Xia, GH (corresponding author), Harbin Engn Univ, Minist Educ, Key Lab Intelligent Technol & Applicat Marine Equ, Harbin 150001, Peoples R China.; Xia, GH (corresponding author), Heilongjiang Prov Key Lab Environm Intelligent Pe, Harbin 150001, Peoples R China.
EM xiaguihua@hrbeu.edu.cn
RI guihua, xia/ABG-2340-2021
FU National Key R&D Program of China [2019YFE0105400]; Development Project
   of Ship Situational Intelligent Awareness System [MC-201920-X01]
FX This work was supported in part by the National Key R&D Program of China
   (2019YFE0105400), and in part by the Development Project of Ship
   Situational Intelligent Awareness System (MC-201920-X01). Our code is
   released through https://github.com/He-Yunqian/PCTP.git.
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Atzmon M., 2018, ARXIV
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Chen X., 2017, PROC CVPR IEEE, V1, P3, DOI DOI 10.1109/CVPR.2017.691
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fan HH, 2023, IEEE T PATTERN ANAL, V45, P2181, DOI 10.1109/TPAMI.2022.3161735
   Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398
   Geiger A., 2012, CVPR
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He YQ, 2021, NEUROCOMPUTING, V459, P201, DOI 10.1016/j.neucom.2021.06.046
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Kuang HW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030704
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YY, 2018, ADV NEUR IN, V31
   Liang Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13326, DOI 10.1109/CVPR42600.2020.01334
   Liu Q, 2022, VISUAL COMPUT, V38, P3341, DOI 10.1007/s00371-022-02550-x
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu ZJ, 2019, ADV NEUR IN, V32
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, arXiv
   Rosa S., 2020, 2020 P IEEECVF C COM, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang Z., 2018, ARXIV
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Ye MS, 2020, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR42600.2020.00170
   Yi HW, 2020, IEEE INT CONF ROBOT, P2274, DOI [10.1109/icra40945.2020.9196556, 10.1109/ICRA40945.2020.9196556]
   Zarzar J., 2019, ARXIV
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhao H., 2020, ARXIV
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu X., 2020, arXiv
NR 53
TC 2
Z9 2
U1 3
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5669
EP 5681
DI 10.1007/s00371-022-02688-8
EA OCT 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000875798600001
DA 2024-07-18
ER

PT J
AU Jiang, ZY
   Zhang, ZX
   Yu, YY
   Liu, RS
AF Jiang, Zhiying
   Zhang, Zengxi
   Yu, Yiyao
   Liu, Risheng
TI Bilevel modeling investigated generative adversarial framework for image
   restoration
SO VISUAL COMPUTER
LA English
DT Article
DE Image restoration; Single image deraining; Image deconvolution; Bilevel
   optimization
ID REMOVAL; RAIN
AB Generative adversarial network (GAN), which is developed on the alternative improvement of the generator and discriminator to obtain the optimal network, has become a problem of great interest for computer vision. In image restoration, domain knowledge and heuristic image prior are significant for low-level tasks, while the effectiveness of GAN relies on the empirical designing of networks and the massive training data only, ignoring the intrinsic principle owned by the task. Therefore, the restored results obtained by GAN usually suffer from information loss, and the structure may not be preserved well. To alleviate this issue, we develop a bilevel modeling investigated generative adversarial framework to incorporate task-specific domain knowledge with discriminative prior for adaptive image restoration. In our method, bilevel optimization is investigated to establish our basic iterative mechanism. The lower layer in the paradigm introduces domain knowledge to reveal the task essences in a physics model. At the same time, the upper layer exploits the data-driven discriminative prior to guiding the estimated one. Within the interaction between the lower and upper layers, the upper network realizes the correction of the intermediate results obtained by the lower heuristic prior guidance. We apply it to two low-level image restoration tasks, image deconvolution and single image deraining. Extensive experiments prove that the proposed method performs favorably against the state-of-the-art methods quantitatively and qualitatively.
C1 [Jiang, Zhiying; Zhang, Zengxi] Dalian Univ Technol, Sch Software Technol, Dalian, Peoples R China.
   [Yu, Yiyao; Liu, Risheng] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Jiang, Zhiying; Zhang, Zengxi; Liu, Risheng] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Liu, RS (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Liu, RS (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM rsliu@dlut.edu.cn
RI Zhang, Zengxi/JXX-6151-2024
OI Zhang, Zengxi/0000-0001-8581-3905; Liu, Risheng/0000-0002-9554-0565
FU National Natural Science Foundation of China [61922019, 61672125]
FX This study was funded by the National Natural Science Foundation of
   China under Grant (Nos. 61922019 and 61672125).
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Deng LJ, 2018, APPL MATH MODEL, V59, P662, DOI 10.1016/j.apm.2018.03.001
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Ding XH, 2016, MULTIMED TOOLS APPL, V75, P2697, DOI 10.1007/s11042-015-2657-7
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Fortunato HE, 2014, VISUAL COMPUT, V30, P661, DOI 10.1007/s00371-014-0966-x
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Jiang ZY, 2022, IEEE T SYST MAN CY-S, V52, P6383, DOI 10.1109/TSMC.2022.3144141
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liu J., 2022, P IEEECVF C COMPUTER, P5802
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu RS, 2022, IEEE T IMAGE PROCESS, V31, P4922, DOI 10.1109/TIP.2022.3190209
   Liu RS, 2020, IEEE T NEUR NET LEAR, V31, P1653, DOI 10.1109/TNNLS.2019.2921597
   Liu RS, 2020, IEEE T PATTERN ANAL, V42, P3027, DOI 10.1109/TPAMI.2019.2920591
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan XG, 2022, IEEE T PATTERN ANAL, V44, P7474, DOI 10.1109/TPAMI.2021.3115428
   Tran P, 2021, PROC CVPR IEEE, P11951, DOI 10.1109/CVPR46437.2021.01178
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Radford A., 2015, ARXIV
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Sonderby C. K., 2017, ICLR
   Sun LB, 2013, IEEE INT CONF COMPUT
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang H, 2021, PROC CVPR IEEE, P14786, DOI 10.1109/CVPR46437.2021.01455
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang JW, 2017, PROC CVPR IEEE, P6969, DOI 10.1109/CVPR.2017.737
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhao J, 2016, ARXIV
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 64
TC 4
Z9 4
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5563
EP 5575
DI 10.1007/s00371-022-02681-1
EA OCT 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000869195300001
DA 2024-07-18
ER

PT J
AU Kong, LX
   Yang, T
   Xie, LSQ
   Xu, D
   He, KJ
AF Kong, Lingxiang
   Yang, Tao
   Xie, Lisiqi
   Xu, Dan
   He, Kangjian
TI Cascade connection-based channel attention network for bidirectional
   medical image registration
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Medical image registration; Unsupervised registration;
   Channel attention network
ID DEFORMABLE REGISTRATION; LEARNING FRAMEWORK; YOUNG; MR
AB Medical image registration is an essential task in researching and applying medical images. Doctors can observe and extract relevant pathological features to quickly analyze the disease by registered images to diagnose the infection. After more than ten years of research and development, medical image registration has achieved good research results in traditional and deep learning methods. However, most existing methods only focus on unidirectional medical image registration research and rarely consider bidirectional medical image registration research. This paper proposes a new, unsupervised bidirectional medical image registration method based on this aspect. This method guarantees the registration effect in the forward and reverses directions and adds a cascade connection-based channel attention network to the registration model to enable better automatic learning of the registration model, optimizes feature weights, and extracts essential information from images to improve registration performance. We verified the effectiveness of our method by conducting experiments on large-scale 3D brain MRI images and achieved a comparable registration speed and effect with most existing medical image registration methods.
C1 [Kong, Lingxiang; Xie, Lisiqi; Xu, Dan; He, Kangjian] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
   [Yang, Tao] Yunnan Leg Technol Co Ltd, Kunming 650041, Yunnan, Peoples R China.
C3 Yunnan University
RP Xu, D; He, KJ (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
EM Danxu@ynu.edu.cn; hekj@ynu.edu.cn
RI Xu, Dan/KPA-7396-2024; He, Kangjian/CAG-0300-2022
OI Xu, Dan/0000-0003-4602-3550; He, Kangjian/0000-0001-6207-9728
FU Yunnan Province Research and development of key technologies for
   clinical medicine of "heart brain treatment" Project [202203AC100052];
   National Natural Science Foundation of China [62202416, 62162068];
   Yunnan Province Ten Thousand Talents Program; Yunling Scholars Special
   Project [YNWR-YLXZ-2018-022]; Yunnan Provincial Science and Technology
   Department-Yunnan University "Double First-Class" Construction Joint
   Fund Project [2019FY003012]
FX This work was supported in part by the Yunnan Province Research and
   development of key technologies for clinical medicine of "heart brain
   treatment" Project under Grant No.202203AC100052, in part by the
   National Natural Science Foundation of China under Grant 62202416, Grant
   62162068, in part by the Yunnan Province Ten Thousand Talents Program
   and Yunling Scholars Special Project under Grant YNWR-YLXZ-2018-022, in
   part by the Yunnan Provincial Science and Technology Department-Yunnan
   University "Double First-Class" Construction Joint Fund Project under
   Grant No.2019FY003012.
CR Ahmad S, 2018, VISUAL COMPUT, V34, P21, DOI 10.1007/s00371-016-1307-z
   Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   Andersen D, 2016, VISUAL COMPUT, V32, P1481, DOI 10.1007/s00371-015-1135-6
   Anzid H, 2023, VISUAL COMPUT, V39, P1667, DOI 10.1007/s00371-022-02435-z
   Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007
   Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004
   Avants BB, 2011, NEUROIMAGE, V54, P2033, DOI 10.1016/j.neuroimage.2010.09.025
   Balakrishnan G, 2019, IEEE T MED IMAGING, V38, P1788, DOI 10.1109/TMI.2019.2897538
   Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964
   Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cui M, 2007, VISUAL COMPUT, V23, P607, DOI 10.1007/s00371-007-0164-1
   Dalca AV, 2018, LECT NOTES COMPUT SC, V11070, P729, DOI 10.1007/978-3-030-00928-1_82
   de Vos BD, 2019, MED IMAGE ANAL, V52, P128, DOI 10.1016/j.media.2018.11.010
   de Vos BD, 2017, LECT NOTES COMPUT SC, V10553, P204, DOI 10.1007/978-3-319-67558-9_24
   Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021
   Gan R, 2008, MED IMAGE ANAL, V12, P452, DOI 10.1016/j.media.2008.01.004
   Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006
   He ZQ, 2023, APPL INTELL, V53, P2936, DOI 10.1007/s10489-022-03659-1
   Hellier P., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P590
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia F, 2021, IEEE INT CONF COMP V, P354, DOI 10.1109/ICCVW54120.2021.00044
   Li H., 2017, ARXIV
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Liu YY, 2022, SIGNAL PROCESS, V193, DOI 10.1016/j.sigpro.2021.108418
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498
   Mok Tony C. W., 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12263), P211, DOI 10.1007/978-3-030-59716-0_21
   Mok TCW, 2020, PROC CVPR IEEE, P4643, DOI 10.1109/CVPR42600.2020.00470
   Ou YM, 2011, MED IMAGE ANAL, V15, P622, DOI 10.1016/j.media.2010.07.002
   Reaungamornrat S, 2016, IEEE T MED IMAGING, V35, P2413, DOI 10.1109/TMI.2016.2576360
   Rohe Marc-Michel, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P266, DOI 10.1007/978-3-319-66182-7_31
   Rueckert D, 1999, IEEE T MED IMAGING, V18, P712, DOI 10.1109/42.796284
   Rueckert Daniel, 2006, Med Image Comput Comput Assist Interv, V9, P702
   Shattuck DW, 2008, NEUROIMAGE, V39, P1064, DOI 10.1016/j.neuroimage.2007.09.031
   Soleimani M, 2022, MED BIOL ENG COMPUT, V60, P1015, DOI 10.1007/s11517-022-02515-1
   Sparks BF, 2002, NEUROLOGY, V59, P184, DOI 10.1212/WNL.59.2.184
   Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4
   Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040
   Wu GR, 2012, LECT NOTES COMPUT SC, V7511, P90, DOI 10.1007/978-3-642-33418-4_12
   Yang X, 2017, NEUROIMAGE, V158, P378, DOI 10.1016/j.neuroimage.2017.07.008
   Zhang GY, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103172
   Zhou CC, 2021, COMPUT BIOL MED, V138, DOI 10.1016/j.compbiomed.2021.104923
NR 43
TC 3
Z9 3
U1 25
U2 98
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5527
EP 5545
DI 10.1007/s00371-022-02678-w
EA OCT 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000862743400001
DA 2024-07-18
ER

PT J
AU Hu, FX
   Cao, HJ
   Chen, SY
   Sun, YA
   Su, QT
AF Hu, Fangxu
   Cao, Hongjiao
   Chen, Siyu
   Sun, Yehan
   Su, Qingtang
TI A robust and secure blind color image watermarking scheme based on
   contourlet transform and Schur decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Blind watermarking; Contourlet transform; Schur decomposition; Chaos
   encryption
ID QUANTIZATION INDEX MODULATION; ALGORITHM
AB In order to protect the copyright of color digital images in the current Internet environment, this paper proposes a blind color image watermarking scheme based on nonsubsampled contourlet transform (NSCT) and Schur decomposition. In the stage of watermark embedding, NSCT is applied to acquire low-frequency coefficients of host images, and non-overlapping coefficient blocks of sized 4 x 4 are selected from the layered low-frequency coefficients by using Hash pseudo-random algorithm based on MD5. The watermark information encrypted by Lorenz chaotic mapping is embedded into the maximum eigenvalue of the matrix by quantization index modulation. In the extraction process, watermark information is extracted from the attacked watermarked image without using any original information. The simulation results show that all values of peak signal-to-noise ratio of the proposed watermarking scheme are more than 40 dB under the payload of 1/16 bit/pixel, and the values of Structural Similarity Index Metric are more than 0.97; when subjected to various common attacks, the mean of normalized cross-correlation is above 0.96, and the bit error rate is about 0.05; the key space is more than 2(624); the running time is about 1.5 s. Compared with other schemes, on the premise of ensuring invisibility and embedding capacity, the proposed scheme has been significantly improved in terms of security and robustness.
C1 [Hu, Fangxu; Cao, Hongjiao; Chen, Siyu; Sun, Yehan; Su, Qingtang] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
C3 Ludong University
RP Su, QT (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM sdytsqt@163.com
RI Chen, Siqi/IZE-8631-2023
FU National Natural Science Foundations of China [61771231, 62171209,
   61772253, 61873117, 61872170, 61803253]; Key Project of Shandong
   Provincial Natural Science Foundation [ZR2020KF023]
FX The work was supported by the National Natural Science Foundations of
   China (Nos. 61771231, 62171209, 61772253, 61873117, 61872170 and
   61803253), and the Key Project of Shandong Provincial Natural Science
   Foundation (No. ZR2020KF023).
CR Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen HY, 2012, J ZHEJIANG U-SCI C, V13, P573, DOI 10.1631/jzus.C1100338
   Chen SY, 2022, VISUAL COMPUT, V38, P2189, DOI 10.1007/s00371-021-02277-1
   Chen Y, 2021, SIGNAL PROCESS, V185, DOI 10.1016/j.sigpro.2021.108088
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Golea N., 2010, IEEE INT C COMPUTER, P1
   Hsu LY, 2017, J VIS COMMUN IMAGE R, V46, P33, DOI 10.1016/j.jvcir.2017.03.009
   Hu HT, 2020, INFORM SCIENCES, V519, P161, DOI 10.1016/j.ins.2020.01.019
   Hu K, 2022, VISUAL COMPUT, V38, P2153, DOI 10.1007/s00371-021-02275-3
   Ko HJ, 2020, INFORM SCIENCES, V517, P128, DOI 10.1016/j.ins.2019.11.005
   Li YM, 2021, INFORM SCIENCES, V551, P205, DOI 10.1016/j.ins.2020.11.020
   Liu DC, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116292
   Liu DC, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114540
   Liu DC, 2021, VISUAL COMPUT, V37, P2355, DOI 10.1007/s00371-020-01991-6
   Liu P, 2022, MULTIMED TOOLS APPL, V81, P2637, DOI 10.1007/s11042-021-11532-5
   LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Martinsson PG, 2019, ACM T MATH SOFTWARE, V45, DOI 10.1145/3242670
   Mellimi S, 2021, PATTERN RECOGN LETT, V151, P222, DOI 10.1016/j.patrec.2021.08.015
   Molina-Garcia J, 2020, SIGNAL PROCESS-IMAGE, V81, DOI 10.1016/j.image.2019.115725
   Pan JS, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104049
   Shen YX, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114414
   Singh SP, 2018, J VIS COMMUN IMAGE R, V53, P86, DOI 10.1016/j.jvcir.2018.03.006
   Sinhal R, 2021, PATTERN RECOGN LETT, V145, P171, DOI 10.1016/j.patrec.2021.02.011
   Su QT, 2022, INT J INTELL SYST, V37, P4747, DOI 10.1002/int.22738
   Su QT, 2020, SOFT COMPUT, V24, P445, DOI 10.1007/s00500-019-03924-5
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Su QT, 2017, AEU-INT J ELECTRON C, V78, P64, DOI 10.1016/j.aeue.2017.05.025
   University of Granada Computer Vision Group, 2002, CVG UGR IM DAT
   University of Southern California, 1997, SIGN IN PROC I USC S
   Wei J, 2010, P 2 INT C E BUS INF
   Xiao-Long Liu, 2018, IEEE Transactions on Circuits and Systems for Video Technology, V28, P1047, DOI 10.1109/TCSVT.2016.2633878
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
   Zhang XT, 2021, INT J INTELL SYST, V36, P4321, DOI 10.1002/int.22461
   Zhang XT, 2020, OPTIK, V219, DOI 10.1016/j.ijleo.2020.165272
NR 36
TC 6
Z9 6
U1 6
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4573
EP 4592
DI 10.1007/s00371-022-02610-2
EA JUL 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000829278800001
DA 2024-07-18
ER

PT J
AU Saranya, MS
   Geetha, P
AF Saranya, M. S.
   Geetha, P.
TI A deep learning-based feature extraction of cloth data using modified
   grab cut segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Piecewise feature extraction; Modified GrabCut; Cloth retrieval;
   RetrieveNet; Bottleneck attention model (BAM)
ID RETRIEVAL
AB In recent years, e-commerce and online shopping have grown tremendously. The development of an image-based cloth retrieval system has become a major research topic in order to effectively and accurately recover required garments from a big database. In this paper, a piecewise feature extraction (PWF) framework is proposed to retrieve the cross-domain clothes. This feature selection strategy is presented to extract optimal features in order to enhance the detection rate while simultaneously simplifying image retrieval computation. First, the user-input query images are collected from online or snapshots. Further clutter backgrounds and different skin textures, of the image, are eliminated via modified GrabCut segmentation. Followed by, the PWF model retrieves the required cloth from the shop domain. In PWF cloth and sleeve size attributes are identified in a separate block with Hough transform and fuzzy rule. Consequently, high-level features are extracted by using a novel RetrieveNet. BAM (Bottleneck attention module) of the RetrieveNet emphasizes the network to focus on the domain features of the input cloth. Finally, the L2 norm is employed to determine the degree of similarity between the shop and query cloths. Based on the similarity, related clothes are retrieved. DARN and Deepfashion2, public datasets are exploited to validate the proposed model, showing that the top-50 image retrieval accuracy is 69.54% and 47.29%, respectively. The efficiency of the proposed clothes retrieval approach is demonstrated by the outcomes of our experiments.
C1 [Saranya, M. S.; Geetha, P.] Anna Univ, Coll Engn, Dept Comp Sci & Engn, Chennai 600025, Tamil Nadu, India.
C3 Anna University; Anna University Chennai
RP Saranya, MS (corresponding author), Anna Univ, Coll Engn, Dept Comp Sci & Engn, Chennai 600025, Tamil Nadu, India.
EM saranyasivaraman5@gmail.com; geethaplanisamy@gmail.com
RI P, Geetha/ADG-6534-2022
OI P, Geetha/0000-0002-5637-3856
CR Ak KE, 2018, PATTERN RECOGN LETT, V112, P212, DOI 10.1016/j.patrec.2018.07.019
   Bhatnagar A., 2018, ARXIV PREPRINT ARXIV
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Cheng SL, 2019, VISUAL COMPUT, V35, P1255, DOI 10.1007/s00371-018-1583-x
   Deng LL, 2018, WIRELESS PERS COMMUN, V102, P599, DOI 10.1007/s11277-017-5050-1
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Hou YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12127, DOI 10.1109/ICCV48922.2021.01193
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jianlong Fu, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P420, DOI 10.1007/978-3-642-37444-9_33
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kinli F, 2019, IEEE INT CONF COMP V, P3109, DOI 10.1109/ICCVW.2019.00376
   Kuang ZH, 2019, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2019.00316
   Lang YN, 2020, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR42600.2020.00267
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li JP, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON OPTO-ELECTRONIC INFORMATION PROCESSING (ICOIP), P1, DOI 10.1109/OPTIP.2017.8030687
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Li ZM, 2016, INT C PATT RECOG, P2912, DOI 10.1109/ICPR.2016.7900079
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3755, DOI 10.1145/3474085.3478327
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Miao YW, 2020, IEEE ACCESS, V8, P142669, DOI 10.1109/ACCESS.2020.3013631
   Pradhan J, 2020, VISUAL COMPUT, V36, P1847, DOI 10.1007/s00371-019-01773-9
   Sharma V, 2021, IEEE WINT CONF APPL, P1347, DOI 10.1109/WACV48630.2021.00139
   Stephen O, 2019, INT CONF ADV COMMUN, P408, DOI 10.23919/icact.2019.8701958
   Su HB, 2021, IEEE T CIRC SYST VID, V31, P3254, DOI 10.1109/TCSVT.2020.3034981
   Valle D., 2018, IEEE INT JOINT C NEU, P1, DOI DOI 10.1109/EPIM.2018.8756397
   Xia YZ, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P804, DOI 10.1109/FSKD.2017.8393378
   Zhang HJ, 2020, NEURAL COMPUT APPL, V32, P4519, DOI [10.1007/s00521-018-3579-x, 10.1007/s00521-018-3691-y]
   Zhao HR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1054
NR 29
TC 2
Z9 2
U1 2
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4195
EP 4211
DI 10.1007/s00371-022-02584-1
EA JUL 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825908300003
DA 2024-07-18
ER

PT J
AU Chen, J
   Fu, ZP
   Huang, J
   Hu, XR
   Peng, T
AF Chen, Jia
   Fu, Zhenpeng
   Huang, Jin
   Hu, Xinrong
   Peng, Tao
TI Boosting vision transformer for low-resolution borehole image stitching
   through algebraic multigrid
SO VISUAL COMPUTER
LA English
DT Article
DE Borehole image stitching; Low-resolution; Vision transformer; Algebraic
   multigrid; Affine transform
ID ALGORITHM
AB In geotechnical engineering, borehole image stitching technology is needed to detect cracks in borehole walls and quicksand, etc., which helps prevent natural disasters. However, traditional manual stitching of low-resolution borehole images requires a lot of manual work by experts, which leads to very expensive costs. In this paper, We propose Vision Transformer for low-resolution borehole image mosaic framework integrating algebraic multigrid (AMG), which saves a lot of manual costs and gradually moves towards automation. Then, we design AMG enhanced Vision Transformer stitching model for borehole image, which overcomes the problems of stitching seam and black image blocks caused by image distortion and blur. Finally, we collect a large number of real data sets from different engineering sites, such as natural gas tunnel, oil exploration and mining ore detection, so that our proposed method can better adapt to different geotechnical environments. Experiments show that our method has better monotonicity, and the result of the stitching image is more complete than the existing results based on SIFT, APAP and COTR.
C1 [Chen, Jia; Fu, Zhenpeng; Huang, Jin; Hu, Xinrong; Peng, Tao] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
C3 Wuhan Textile University
RP Huang, J (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Peoples R China.
EM derick0320@foxmail.com
RI Hu, Xinrong/HGA-1351-2022
CR Adel E, 2015, INT J ADV COMPUT SC, V6, P55
   Ahmad RM, 2020, AIPR 2020: 2020 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN RECOGNITION, P180, DOI 10.1145/3430199.3430243
   [Anonymous], 2011, PROC CVPR IEEE
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   CHANG CH, 2014, PROC CVPR IEEE, P3254, DOI DOI 10.1109/CVPR.2014.422
   Chen KL, 2014, CHIN CONTR CONF, P7292, DOI 10.1109/ChiCC.2014.6896208
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dong Q., 2010, STITCHING ALGORITHM
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   He K, 2017, AER ADV ENG RES, V130, P617
   Huang Y., 2019, INT C PIONEERING COM, P120
   Jiang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6187, DOI 10.1109/ICCV48922.2021.00615
   Jung SY, 2005, P SOC PHOTO-OPT INS, V5674, P444, DOI 10.1117/12.586753
   Kimmel R, 2003, SIAM J SCI COMPUT, V24, P1218, DOI 10.1137/S1064827501389229
   Lei F, 2014, C IND ELECT APPL, P79, DOI 10.1109/ICIEA.2014.6931135
   LI H, 1995, IEEE T IMAGE PROCESS, V4, P320, DOI 10.1109/83.366480
   Liao TL, 2020, IEEE T IMAGE PROCESS, V29, P724, DOI 10.1109/TIP.2019.2934344
   Malik NUR, 2019, IEEE I C SIGNAL IMAG, P214, DOI [10.1109/ICSIPA45851.2019.8977732, 10.1109/icsipa45851.2019.8977732]
   Moyou M, 2020, IEEE T IMAGE PROCESS, V29, P3374, DOI 10.1109/TIP.2019.2959722
   Ruge J.W., 1987, Multigrid Methods, P73, DOI DOI 10.1007/S10444-014-9395-7
   Sarwinda D, 2021, PROCEDIA COMPUT SCI, V179, P423, DOI 10.1016/j.procs.2021.01.025
   Singh SK, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, COMMUNICATION, COMPUTER, AND OPTIMIZATION TECHNIQUES (ICEECCOT), P399
   Song HW, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040296
   Stuben K., 2001, Multigrid, P413
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Vaswani A, 2017, ADV NEUR IN, V30
   Xing J., 2007, 2 INT C INN COMP INF, P453
   Xu QB, 2010, APPL MATH SER B, V25, P367, DOI 10.1007/s11766-010-2302-1
   Xu Y., 2010, 2010 6 INT C WIRELES, DOI 10.1109/ICMSS.2010.5576775
   Yang ZQ, 2018, IEEE ACCESS, V6, P38544, DOI 10.1109/ACCESS.2018.2853100
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang DY, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE 2020), P2255, DOI 10.1109/ICMCCE51767.2020.00489
   Zhang X, 2016, AER ADV ENG RES, V81, P460
   Zhong LH, 2017, IOP CONF SER-MAT SCI, V281, DOI 10.1088/1757-899X/281/1/012061
   Zhu F.K., 2018, MODULAR MACHINE TOOL
NR 35
TC 4
Z9 4
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3191
EP 3203
DI 10.1007/s00371-022-02564-5
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000825226300004
DA 2024-07-18
ER

PT J
AU Yang, X
   Yang, CL
   Chen, WJ
AF Yang, Xin
   Yang, Chunling
   Chen, Wenjun
TI A hybrid sampling and gradient attention network for compressed image
   sensing
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Compressed image sensing; Hybrid sampling; Block
   permutation; Gradient attention
ID RECONSTRUCTION NETWORK
AB Block compressed sensing (BCS), which is widely used in compressed image sensing (CIS), brings the advantages of lower complexity for sampling and reconstruction. But it will result in redundant information sampling in the smooth block and insufficient information sampling in the texture block, which makes the reconstruction quality of image details poor. To address this problem, we propose a novel Hybrid Sampling and Gradient Attention Network for CIS, dubbed HSGANet. In HSGANet, a new linear sampling strategy, called hybrid BCS (HBCS) sampling is proposed to realize BCS sampling with balanced information entropy for sampling blocks. Specifically, HBCS is composed of the proposed block permutation sampling (BPS) and BCS sampling, where the BPS is used to increase the proportion of texture block information in the image measurement, which is realized by BCS sampling following image block permutation. Furthermore, a selection algorithm is developed to achieve optimal image information balanced permutation. To match HBCS, we design the initial reconstruction fusion sub-network and the deep reconstruction sub-network which is constructed by cascading GA-Blocks with gradient attention sub-network. In each phase, the gradient attention sub-network can achieve pixel-level adaptive fusion of the gradient map obtained by minimizing measurement errors of BPS and BCS. Extensive experimental results show that our HSGANet has a great improvement in reconstruction accuracy than the state-of-the-art methods with a comparable running speed and model complexity.
C1 [Yang, Xin; Yang, Chunling; Chen, Wenjun] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
C3 South China University of Technology
RP Yang, CL (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
EM eexyang@mail.scut.edu.cn; eeclyang@scut.edu.cn;
   eecwjjun@mail.scut.edu.cn
OI Yang, Xin/0000-0003-3431-5118
FU Key Program of Natural Science Foundation of Guangdong Province
   [2017A030311028]; Natural Science Foundation of Guangdong Province
   [2019A1515011949]
FX This work is supported by the Key Program of Natural Science Foundation
   of Guangdong Province (2017A030311028) and the Natural Science
   Foundation of Guangdong Province (2019A1515011949)
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Canh TN, 2021, IEEE T COMPUT IMAG, V7, P86, DOI 10.1109/TCI.2020.3034433
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Du J, 2019, NEUROCOMPUTING, V328, P105, DOI 10.1016/j.neucom.2018.04.084
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Kingma D. P., 2014, arXiv
   KULKARNI K, 2016, PROC CVPR IEEE, P449, DOI DOI 10.1109/CVPR.2016.55
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Mardani M., 2018, ARXIV PREPRINT ARXIV
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mousavi A, 2017, INT CONF ACOUST SPEE, P2272, DOI 10.1109/ICASSP.2017.7952561
   Pei HQ, 2020, IEEE IMAGE PROC, P2870, DOI [10.1109/icip40778.2020.9191043, 10.1109/ICIP40778.2020.9191043]
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Su YM, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115989
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Canh TN, 2019, IEEE IMAGE PROC, P2105, DOI [10.1109/ICIP.2019.8803165, 10.1109/icip.2019.8803165]
   Tian JP, 2022, VISUAL COMPUT, V38, P4193, DOI 10.1007/s00371-021-02288-y
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   You D., 2021, 2021 IEEE INT C MULT, P1
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
NR 40
TC 0
Z9 0
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4213
EP 4226
DI 10.1007/s00371-022-02585-0
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000819903900003
DA 2024-07-18
ER

PT J
AU Liu, AQ
   Li, SM
   Chang, YL
AF Liu, Anqi
   Li, Sumei
   Chang, Yongli
TI Cross-resolution feature attention network for image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Super-resolution; Convolutional neural network; Cross-resolution;
   Resolution-wise attention; Progressive upsampling
AB In recent years, single image super-resolution based on convolutional neural network (CNN) has been extensively researched. However, most CNN-based methods only focus on mining features at a single resolution or processing features at different resolutions in series, which will cause the loss of broad context or spatial details. To address these issues, we design a crossresolution feature attention network, which can progressively reconstruct images at different scale factors. Specifically, the reconstruction of each scale factor contains cascaded cross-resolution residual block (CRRB) and a resolution-wise attention block (RAB). CRRB can extract features at different resolutions in parallel rather than in series, which enriches not only global contextual information but also spatial details. RAB can adaptively capture the importance of features from different resolutions, making the feature fusion more effective. We have tested our network on Set5, Set14, BSD100, Urban100 and Manga109 dataset for x2, x4 and x8 SR. Experimental results on the five datasets show that our proposed method achieves 0.24 dB, 0.09 dB and 0.21dB higher PSNR than that of MS-LapSRN, MGBP and RMUN for x8 SR, respectively, and also achieves favorable performance against the state-of-the-art methods on x2 and x4 SR.
C1 [Liu, Anqi; Li, Sumei; Chang, Yongli] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Li, SM (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM liuanqi@tju.edu.cn; lisumei@tju.edu.cn; chang_yli@163.com
OI Li, Sumei/0000-0002-4793-3161
FU National Natural Science Foundation of China [61971306]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61971306.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2019, ARXIV PREPRINT ARXIV
   Anwar S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3390462
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chudasama V, 2022, VISUAL COMPUT, V38, P3643, DOI 10.1007/s00371-021-02193-4
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2021, INFORM SCIENCES, V546, P769, DOI 10.1016/j.ins.2020.08.114
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang ZQ, 2020, IEEE T BROADCAST, V66, P814, DOI 10.1109/TBC.2020.2977513
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AQ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1610, DOI 10.1109/ICASSP39728.2021.9414535
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Michelini PN, 2019, AAAI CONF ARTIF INTE, P4642
   Pang YW, 2019, IEEE T INF FOREN SEC, V14, P3322, DOI 10.1109/TIFS.2019.2916592
   Pfster H., 2021, P IEEE CVF INT C COM, P4278
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xie W., 2021, P IEEE CVF INT C COM, P4308
   Xu Y, 2020, IEEE T NEUR NET LEAR, V31, P4747, DOI 10.1109/TNNLS.2019.2957527
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang W, 2018, IEEE CONF COMPUT
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhao XL, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103075
   Zhou Y, 2022, IEEE T CYBERNETICS, V52, P5855, DOI 10.1109/TCYB.2020.3044374
NR 50
TC 6
Z9 6
U1 4
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3837
EP 3849
DI 10.1007/s00371-022-02519-w
EA JUN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000814494400003
DA 2024-07-18
ER

PT J
AU Ohkawara, M
   Fujishiro, I
AF Ohkawara, Masaru
   Fujishiro, Issei
TI Illumination-aware group portrait compositor
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Digital image compositing; Visual coherence;
   Perception
ID IMAGE
AB We present a novel compositing framework for full-length human figures that maintains their surface details and appends the localized nature of light and shadow, thereby synthesizing composite results with high visual coherence. The framework is extended from the compositing pipeline proposed in our previous study so that it deploys five stages for photometric information estimation, as well as for 3D reconstruction, global illumination simulation, lighting transfer, and compositing. Based on the interpretation that a sense of coexistence can be achieved through visual coherence, we demonstrate that the proposed framework functions properly as a group portrait compositor. The composite results that the proposed framework composed the images separately rendered 3D human models compared favorably with the results which rendered multiple avatars together. Based on this empirical evaluation, the proposed framework is expected as a new means of fostering a sense of coexistence in remote societies and of efficiently generating highly photorealistic cyberworlds.
C1 [Ohkawara, Masaru; Fujishiro, Issei] Keio Univ, Yokohama, Kanagawa, Japan.
C3 Keio University
RP Ohkawara, M (corresponding author), Keio Univ, Yokohama, Kanagawa, Japan.
EM masaru.ohkawara@fj.ics.keio.ac.jp
OI Ohkawara, Masaru/0000-0001-5816-9237; Fujishiro,
   Issei/0000-0002-8898-730X
FU JSPS KAKENHI [21H04916, 20K20481]; Grants-in-Aid for Scientific Research
   [21H04916, 20K20481] Funding Source: KAKEN
FX This work has been financially supported in part by JSPS KAKENHI
   Grants-in-Aid for Scientific Research (A) No. 21H04916 and Challenging
   Research (Pioneering) No. 20K20481.
CR Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Autodesk, 2021, AOVS ARN MAYA US GUI
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Bluff R., 2020, ACM SIGGRAPH 2020 PR, DOI [10.1145/3368850.3383439, DOI 10.1145/3368850.3383439]
   Cao XC, 2005, VISUAL COMPUT, V21, P639, DOI 10.1007/s00371-005-0335-x
   Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864
   Du H, 2013, VISUAL COMPUT, V29, P217, DOI 10.1007/s00371-012-0722-z
   He T., 2021, P IEEE CVF INT C COM, P11
   Hinton G, 2012, LECT 6D SEPARATE ADA
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kanamori Y, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275104
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Li Q, 2010, VISUAL COMPUT, V26, P41, DOI 10.1007/s00371-009-0375-8
   Mikhailov A., 2019, Google AI Blog
   Ohkawara M, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P17, DOI 10.1109/CW52790.2021.00011
   Pandey R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459872
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Philip J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323013
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Porter T., 1984, Computers & Graphics, V18, P253
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Riklin-Raviv T., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P566, DOI 10.1109/CVPR.1999.784968
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Shu ZX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3095816
   Spataro J., 2020, FUTURE WORK THE GOOD
   Sun TC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323008
   Susanna R., 2020, STORY MICROSOFT INNO
   Tingbo H., 2020, BLOG POSTGOOGLE BLOG
   Tsai YH, 2017, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2017.299
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xu D, 2022, VISUAL COMPUT, V38, P4251, DOI 10.1007/s00371-021-02292-2
   Yun-Ta T., 2020, BLOG POSTGOOGLE BLOG
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
NR 39
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4009
EP 4018
DI 10.1007/s00371-022-02508-z
EA MAY 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000800836300001
PM 35615421
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Mulmule, PV
   Kanphade, RD
   Dhane, DM
AF Mulmule, Pallavi V.
   Kanphade, Rajendra D.
   Dhane, Dhiraj M.
TI Artificial intelligence-assisted cervical dysplasia detection using
   papanicolaou smear images
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Cervical cancer; Dysplasia; Pap-stained images; Random forest
   segmentation; Feature extraction and selection; Artificial neural
   network
ID CANCER; CLASSIFICATION; SEGMENTATION; CELLS
AB Cervical dysplasia is a cancerous condition, and it is essential to correctly identify them from Pap smear images using machine intelligence. Regular screening and early diagnosis is the most vital step for detecting dysplastic stage, so as to treat them effectively. However, the manual inspection of Papanicolaou test screened under microscope is laborious, subjective and time-consuming task. Therefore, the objective of this research was to develop an artificial intelligence-enabled assistive tool to detect the cervical dysplasia cancer. Here, the pixel-based segmentation to classification mapping approach is introduced which is the two-step classification, i.e. cell segmentation and cell classification. In cell segmentation stage, the novel filter to feature map approach is used. Total 112 filtered images were generated from each original cell images. The feature vector was then created for every original pixel using filtered images. In Dysplasia cancer classification stage, the 163 features consisting the edge detector, texture, noise, membrane detector and colour features are considered. Three classifiers, namely artificial neural network (ANN), support vector machine (SVM) and random forest (RF), are used to detect and diagnose the dysplasia stage cancer. These classifiers are evaluated for performance using seven different performance measures. For cell segmentation approach, the RF reported accuracy of 99.07% and it outperformed in terms of accuracy over ANN and SVM classifiers. Finally, the cervical dysplasia is accurately identified with 97.5% accuracy using ANN as compared to SVM and RF.
C1 [Mulmule, Pallavi V.] DY Patil Inst Technol, Dept E & TC, Pune 411018, Maharashtra, India.
   [Kanphade, Rajendra D.] JSPMs Jayawantrao Sawant Coll Engn, Pune 411028, Maharashtra, India.
   [Dhane, Dhiraj M.] TSSMs, Padmabhooshan Vasantdada Patil Inst Technol, Pune 411021, Maharashtra, India.
RP Dhane, DM (corresponding author), TSSMs, Padmabhooshan Vasantdada Patil Inst Technol, Pune 411021, Maharashtra, India.
EM dhirajdhane@ieee.org
RI Dhane, Dhiraj/HPF-6406-2023; Dhane, Dhiraj/GSE-3473-2022; Dhane, Dhiraj
   Manohar/ABG-2997-2021
OI Dhane, Dhiraj Manohar/0000-0003-2188-4389
CR Alpaydin E., 2009, INTRO MACHINE LEARNI
   Alyafeai Z, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112951
   [Anonymous], 2015, SPRINGER 4 INT C SOF
   [Anonymous], 2020, CERVICAL CANC OVERVI
   Arbyn M, 2020, LANCET GLOB HEALTH, V8, pE191, DOI 10.1016/S2214-109X(19)30482-6
   Arganda-Carreras I, 2017, BIOINFORMATICS, V33, P2424, DOI 10.1093/bioinformatics/btx180
   Arya M, 2020, MULTIMED TOOLS APPL, V79, P24157, DOI 10.1007/s11042-020-09206-9
   Ashok B, 2016, INT J ENG RES APPL, V6, P94
   Bhatt AR, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.348
   Bora K, 2017, COMPUT METH PROG BIO, V138, P31, DOI 10.1016/j.cmpb.2016.10.001
   Burger W., 2009, Principles of Digital Image Processing, DOI DOI 10.1007/978-1-84800-191-6
   Cloppet F, 2008, INT C PATT RECOG, P789
   de Sanjose S., 2015, Tropical Hemato-Oncology, P143
   Devi M.A., 2018, J KING SAUD UNIV-COM, V34, P1352
   Gençtav A, 2012, PATTERN RECOGN, V45, P4151, DOI 10.1016/j.patcog.2012.05.006
   Guven M, 2014, BIOMED ENG ONLINE, V13, DOI 10.1186/1475-925X-13-159
   Hemalatha K., 2018, DATA ENG INTELLIGENT, P83
   HILL T, 1994, INT J FORECASTING, V10, P5, DOI 10.1016/0169-2070(94)90045-0
   I.A. for Researchon Cancer, 2018, POPULATION FACT SHEE
   Jantzen J., 2005, P NISIS 2005 NATURE, P1
   Jantzen J, PAP SMEAR DTUHERLEV
   Juneja A, 2003, Indian J Cancer, V40, P15
   Lakshmi G. Anna, 2017, International Journal of Image, Graphics and Signal Processing, V9, P39, DOI 10.5815/ijigsp.2017.11.05
   Lin HM, 2019, IEEE ACCESS, V7, P71541, DOI 10.1109/ACCESS.2019.2919390
   Livingston F., 2005, ECE591Q Machine Learning Journal Paper, P1
   Lu Z., REAL CERVICAL CYTOLO
   Marinakis Y, 2009, COMPUT BIOL MED, V39, P69, DOI 10.1016/j.compbiomed.2008.11.006
   Mittra I, 2010, INT J CANCER, V126, P976, DOI 10.1002/ijc.24840
   Mulmule P.V, 2021, 2021 ASIAN C INNOVAT, P1
   Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565
   Phoulady HA, 2016, IEEE IMAGE PROC, P2658, DOI 10.1109/ICIP.2016.7532841
   Rafael C.G, 2010, DIGITAL IMAGE PROCES
   Sajeena TA, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING AND NETWORK COMMUNICATIONS (COCONET), P663, DOI 10.1109/CoCoNet.2015.7411260
   Siegel R, 2013, CA-CANCER J CLIN, V63, P11, DOI 10.3322/caac.21166
   Solar M, 2019, INT CONF EDEMOC EGOV, P213, DOI [10.1109/ICEDEG.2019.8734400, 10.1109/icedeg.2019.8734400]
   Song YY, 2017, IEEE T MED IMAGING, V36, P288, DOI 10.1109/TMI.2016.2606380
   Song YY, 2014, IEEE ENG MED BIO, P2903, DOI 10.1109/EMBC.2014.6944230
   Sudharshini S., 2012, J Acad Med Sci, V2, P124, DOI [10.4103/2249-13.4855.141132, DOI 10.4103/2249-13.4855.141132, DOI 10.4103/2249-4855.141132]
   Tan SY, 2015, SINGAP MED J, V56, P586, DOI 10.11622/smedj.2015155
   Teeyapan K, 2015, PROCEEDINGS 5TH IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING (ICCSCE 2015), P514, DOI 10.1109/ICCSCE.2015.7482239
   Thulaseedharan JV, 2012, ASIAN PAC J CANCER P, V13, P2991, DOI 10.7314/APJCP.2012.13.6.2991
   Vu M, 2018, CURR PROB CANCER, V42, P457, DOI 10.1016/j.currproblcancer.2018.06.003
   William Wasswa, 2019, Informatics in Medicine Unlocked, V14, P23, DOI 10.1016/j.imu.2019.02.001
   Wu M, 2018, BIOSCIENCE REP, V38, DOI 10.1042/BSR20181769
   Zhang L, 2017, IEEE J BIOMED HEALTH, V21, P1633, DOI 10.1109/JBHI.2017.2705583
NR 45
TC 7
Z9 7
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2381
EP 2392
DI 10.1007/s00371-022-02463-9
EA APR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000781743000001
DA 2024-07-18
ER

PT J
AU Yang, ZY
   Zhang, HM
   Liang, NY
   Li, ZN
   Sun, WJ
AF Yang, Zuyuan
   Zhang, Huimin
   Liang, Naiyao
   Li, Zhenni
   Sun, Weijun
TI Semi-supervised multi-view clustering by label relaxation based
   non-negative matrix factorization
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-view clustering; Semi-supervised learning; Non-negative matrix
   factorization; Label relaxation
AB Semi-supervised multi-view clustering in the subspace has attracted sustained attention. The existing methods often project the samples with the same label into the same point in the low dimensional space. This hard constraint-based method magnifies the dimension reduction error, restricting the subsequent clustering performance. To relax the labeled data during projection, we propose a novel method called label relaxation-based semi-supervised non-negative matrix factorization (LRSNMF). In our method, we first employ the Spearman correlation coefficient to measure the similarity between samples. Based on this, we design a new relaxed non-negative label matrix for better subspace learning, instead of the binary matrix. Also, we derive an updated algorithm based on an alternative iteration rule to solve the proposed model. Finally, the experimental results on three real-world datasets (i.e., MSRC, ORL1, and ORL2) with six evaluation indexes (i.e., accuracy, NMI, purity, F-score, precision, and recall) show the advantages of our LRSNMF, with comparison to the state-of-the-art methods.
C1 [Yang, Zuyuan; Zhang, Huimin; Liang, Naiyao; Li, Zhenni; Sun, Weijun] Guangdong Univ Technol, Guangdong Key Lab IoT Informat Technol, Sch Automat, Guangzhou 510006, Peoples R China.
   [Yang, Zuyuan] Ante Laser Co Ltd, Guangzhou 510006, Peoples R China.
   [Sun, Weijun] Guangdong Hong Kong Macao Joint Lab Smart Discret, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology
RP Liang, NY (corresponding author), Guangdong Univ Technol, Guangdong Key Lab IoT Informat Technol, Sch Automat, Guangzhou 510006, Peoples R China.
EM zuyuangdut@aliyun.com; huimingdut@aliyun.com; naiyaogdut@aliyun.com;
   lizhenni2012@gmail.com; gdutswj@gdut.edu.cn
RI sun, weijun/GZB-2595-2022
OI Liang, Naiyao/0000-0003-0946-7494
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010154002, 2019B010118001, 2019B010121001]; National Natural
   Science Foundation of China [61803096, 61801133, U191140003]; Guangzhou
   Science and Technology Program Project [202002030289]; Guangdong Natural
   Science Foundation [2022A1515010688]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province under Grants 2019B010154002,
   2019B010118001, and 2019B010121001; in part by the National Natural
   Science Foundation of China under Grants 61803096, 61801133, and
   U191140003; in part by the Guangzhou Science and Technology Program
   Project under Grant 202002030289; in part by the Guangdong Natural
   Science Foundation under Grant 2022A1515010688.
CR Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cai H, 2020, INFORM SCIENCES, V536, P171, DOI 10.1016/j.ins.2020.05.073
   Dai WT, 2022, VISUAL COMPUT, V38, P1181, DOI 10.1007/s00371-021-02137-y
   Das Bhattacharjee S, 2018, IEEE T MULTIMEDIA, V20, P2761, DOI 10.1109/TMM.2018.2814338
   Ding C, 2006, P 12 ACM SIGKDD INT, P126, DOI 10.1145/1150402.1150420
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P1006, DOI 10.1109/TNNLS.2017.2648880
   Goldberger J, 2008, PATTERN RECOGN LETT, V29, P1632, DOI 10.1016/j.patrec.2008.04.003
   Huang L, 2019, PATTERN RECOGN, V86, P344, DOI 10.1016/j.patcog.2018.09.016
   Huang SD, 2018, KNOWL-BASED SYST, V158, P1, DOI 10.1016/j.knosys.2018.05.017
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Jiang Y, 2014, MACH VISION APPL, V25, P1635, DOI 10.1007/s00138-013-0556-3
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li GP, 2017, NEUROCOMPUTING, V237, P1, DOI 10.1016/j.neucom.2016.04.028
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Liang NY, 2020, KNOWL-BASED SYST, V190, DOI 10.1016/j.knosys.2019.105185
   LIU J, 2013, INT SYMP ASYNCHRON C, P1, DOI DOI 10.1109/ASYNC.2013.29
   Liu J, 2015, IEEE T NEUR NET LEAR, V26, P1233, DOI 10.1109/TNNLS.2014.2335234
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107207
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Ou WH, 2016, NEUROCOMPUTING, V204, P116, DOI 10.1016/j.neucom.2015.09.133
   PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203
   Peng JY, 2019, INT J MACH LEARN CYB, V10, P879, DOI 10.1007/s13042-017-0766-5
   Peng SY, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107683
   Qian B, 2018, 2018 17TH INTERNATIONAL SYMPOSIUM ON DISTRIBUTED COMPUTING AND APPLICATIONS FOR BUSINESS ENGINEERING AND SCIENCE (DCABES), P103, DOI 10.1109/DCABES.2018.00036
   Qiu YH, 2021, VISUAL COMPUT, V37, P2253, DOI 10.1007/s00371-020-01984-5
   Shao WX, 2016, IEEE DATA MINING, P1203, DOI [10.1109/ICDM.2016.134, 10.1109/ICDM.2016.0160]
   Shi SJ, 2020, NEUROCOMPUTING, V399, P369, DOI 10.1016/j.neucom.2020.02.071
   Tan Y, 2016, INT C CLOUD COMP BIG, P1, DOI 10.1109/CCBD.2016.012
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang H, 2016, IEEE DATA MINING, P1245, DOI [10.1109/ICDM.2016.34, 10.1109/ICDM.2016.0167]
   Wang J, 2016, LECT NOTES COMPUT SC, V9948, P435, DOI 10.1007/978-3-319-46672-9_49
   Wang J, 2018, IEEE T CYBERNETICS, V48, P2620, DOI 10.1109/TCYB.2017.2747400
   Wang R, 2020, IEEE T KNOWL DATA EN, V32, P2014, DOI 10.1109/TKDE.2019.2913377
   Wang S, 2022, VISUAL COMPUT, V38, P2539, DOI 10.1007/s00371-021-02129-y
   Wang XM, 2019, IEEE T CYBERNETICS, V49, P3333, DOI 10.1109/TCYB.2018.2842052
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Xing ZW, 2021, ENG APPL ARTIF INTEL, V103, DOI 10.1016/j.engappai.2021.104289
   Yang ZY, 2021, IEEE T CYBERNETICS, V51, P3249, DOI 10.1109/TCYB.2020.2984552
   Yang ZY, 2020, IEEE T SYST MAN CY-S, V50, P2524, DOI 10.1109/TSMC.2018.2820084
   Yang ZY, 2017, IEEE T NEUR NET LEAR, V28, P948, DOI 10.1109/TNNLS.2016.2517096
   Yang ZY, 2011, IEEE T IMAGE PROCESS, V20, P1112, DOI 10.1109/TIP.2010.2081678
   Yi YG, 2015, NEUROCOMPUTING, V149, P1021, DOI 10.1016/j.neucom.2014.07.031
   Zeng S, 2018, IEEE T FUZZY SYST, V26, P1671, DOI 10.1109/TFUZZ.2017.2743679
   Zhan K, 2019, IEEE T KNOWL DATA EN, V31, P1984, DOI 10.1109/TKDE.2018.2872061
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang WY, 2016, PHYSICA A, V451, P440, DOI 10.1016/j.physa.2016.01.056
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zong LL, 2017, NEURAL NETWORKS, V88, P74, DOI 10.1016/j.neunet.2017.02.003
NR 52
TC 5
Z9 5
U1 3
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1409
EP 1422
DI 10.1007/s00371-022-02419-z
EA MAR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000777227800001
DA 2024-07-18
ER

PT J
AU Lian, YF
   Gu, DR
   Hua, J
AF Lian, Yuanfeng
   Gu, Dingru
   Hua, Jing
TI SORCNet: robust non-rigid shape correspondence with enhanced descriptors
   by Shared Optimized Res-CapsuleNet
SO VISUAL COMPUTER
LA English
DT Article
DE Shape correspondence; Shape descriptors; Optimized residual networks;
   Capsule networks
AB 3D non-rigid shape correspondence, as an important research topic in 3D shape analysis, is useful but challenging in computer graphics, computer vision, and pattern recognition. Despite recent success of several deep neural networks for shape correspondence, those networks cannot achieve robust results on non-rigid objects due to their local deformation complexity. This paper presents a novel and efficient shape correspondence network-Shared Optimized Res-CapsuleNet (SORCNet)-that learns point features based on enhanced descriptors to solve dense correspondence between non-rigid 3D shapes. To further improve the iterative efficiency and accuracy of the model, we design an optimized residual network structure, based on the stochastic gradient descent algorithm with momentum and weight decay (SGDW). Moreover, as the convolutional neural network does not perform well when the shape has directional variance, we present a shared capsule network structure with dual routings, which correlates the hierarchical geometric relationships of the semantic parts well to extract more informative point features. We proved that the primary capsule has a greater influence on feature extraction than the routing and decoder parts. The entire network, SORCNet, is integrated and trained/tested by taking the descriptors and Laplacian eigenbases of two shapes as input. The experiments on public datasets, such as FAUST, SCAPE, TOSCA and KIDS, demonstrate the better effectiveness, accuracy, and adaptability of our method than those of the state of the art in 3D shape correspondence.
C1 [Lian, Yuanfeng; Gu, Dingru] China Univ Petr, Dept Comp Sci & Technol, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
   [Hua, Jing] Wayne State Univ, Detroit, MI USA.
C3 China University of Petroleum; Wayne State University
RP Gu, DR (corresponding author), China Univ Petr, Dept Comp Sci & Technol, Beijing Key Lab Petr Data Min, Beijing, Peoples R China.
EM 2605658935@qq.com; lianyuanfeng@cup.edu.cn
FU NSF [NSFC 61972353];  [IIS-1816511];  [OAC-1910469]
FX This work was partially supported by the grants: NSFC 61972353, NSF
   IIS-1816511 and OAC-1910469.
CR Abadi Martin, 2016, arXiv
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2017, Capsule Network Performance on Complex Data
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Biasotti S, 2016, COMPUT GRAPH FORUM, V35, P87, DOI 10.1111/cgf.12734
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Bronstein MM, 2006, NUMER LINEAR ALGEBR, V13, P149, DOI 10.1002/nla.475
   Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236
   Chen Z., 2018, ARXIV PREPRINT ARXIV
   Cheraghian A, 2019, IEEE WINT CONF APPL, P1194, DOI 10.1109/WACV.2019.00132
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Corman É, 2015, LECT NOTES COMPUT SC, V8928, P283, DOI 10.1007/978-3-319-16220-1_20
   Cortes C, 2017, PR MACH LEARN RES, V70
   Dai GX, 2016, PATTERN RECOGN LETT, V83, P330, DOI 10.1016/j.patrec.2016.04.005
   Domhan T, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3460
   Duarte K, 2018, ADV NEUR IN, V31
   Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097
   Ginzburg D., 2019, ARXIV PREPRINT ARXIV
   Groueix T, 2018, LECT NOTES COMPUT SC, V11206, P235, DOI 10.1007/978-3-030-01216-8_15
   Halimi O., 2018, ARXIV PREPRINT ARXIV
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Hinton G.E., 2018, INT C LEARN REPR
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Jia BH, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030884
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kingma D. P., 2014, arXiv
   Leung FHF, 2003, IEEE T NEURAL NETWOR, V14, P79, DOI 10.1109/TNN.2002.804317
   Li H., 2018, P BRIT MACH VIS C
   Li PJ, 2017, MULTIMED TOOLS APPL, V76, P10207, DOI 10.1007/s11042-016-3606-9
   Lin A., 2018, ARXIV PREPRINT ARXIV
   Lipman Y, 2011, ADV MATH, V227, P1047, DOI 10.1016/j.aim.2011.01.020
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Loshchilov Ilya, 2017, ARXIV171105101
   Ma L, 2003, NEUROCOMPUTING, V51, P361, DOI 10.1016/S0925-2312(02)00597-0
   Mangasarian OL., 1994, Nonlinear Programming, DOI [DOI 10.1137/1.9781611971255, 10.1137/1.9781611971255]
   Marin R, 2020, COMPUT GRAPH FORUM, V39, P160, DOI 10.1111/cgf.13751
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Mateus D, 2008, PROC CVPR IEEE, P1523
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Ovsjanikov Maks, 2016, SIGGRAPH ASIA 2016 C, P9
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Roufosse JM, 2019, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2019.00170
   Sabour S, 2017, ADV NEUR IN, V30
   Sansoni G, 2009, SENSORS-BASEL, V9, P568, DOI 10.3390/s90100568
   Shtern A, 2014, AXIOMS, V3, DOI 10.3390/axioms3030300
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tay Yi, 2020, ARXIV PREPRINT ARXIV
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   UMEHARA M., 2017, Differential Geometry of Curves and Surfaces
   Wang D, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23142
   Wang HY, 2018, LECT NOTES COMPUT SC, V11212, P3, DOI 10.1007/978-3-030-01237-3_1
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
   Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976
NR 59
TC 3
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 749
EP 763
DI 10.1007/s00371-021-02372-3
EA JAN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741630200001
DA 2024-07-18
ER

PT J
AU Wu, CM
   Huang, CC
   Zhang, JJ
AF Wu, Chengmao
   Huang, Congcong
   Zhang, Jiajia
TI Intuitionistic fuzzy information-driven total Bregman divergence fuzzy
   clustering with multiple local information constraints for image
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Image segmentation; Intuitionistic fuzzy clustering; Total Bregman
   divergence; Local similarity measure; Intuitionistic fuzzy weighted
   local information factor
ID C-MEANS ALGORITHM; SET; FUZZINESS; NEGATION
AB Aiming at the shortcoming of existing robust intuitionistic fuzzy clustering and its variant algorithms in the presence of high noise, we will explore a novel intuitionistic fuzzy clustering-related segmentation algorithm with strong robustness. To enhance the anti-noise robustness and segmentation accuracy of intuitionistic fuzzy clustering-related algorithms, an intuitionistic fuzzy information-driven total Bregman divergence fuzzy clustering with multiple local information constraints is proposed in this paper. In this algorithm, a modified total Bregman divergence with spatial information constraints is firstly constructed to replace existing squared Euclidean distance in intuitionistic fuzzy clustering, which can further enhance the ability of noise suppression. By introducing an intuitionistic fuzzy weighted local information into the objective function of intuitionistic fuzzy c-means clustering, its noise sensitivity can be reduced and image details can be preserved effectively. Combining neighborhood spatial information, neighborhood gray information with the normalized variance of neighborhood pixels, a local similarity measure between the current pixel and its neighborhood pixels is constructed to better describe the influence degree of neighborhood pixels on the current pixel, and it is adaptively embedded into modified total Bregman divergence-based intuitionistic fuzzy clustering. In the end, a robust total Bregman divergence-based fuzzy weighted local information clustering algorithm motivated by intuitionistic fuzzy information of the image is obtained to solve the problem of robust image segmentation, and its convergence is strictly proved by the Zangwill theorem. Many experimental results show that the segmentation accuracy ACC of the proposed algorithm can be as high as more than 90%, and the misclassification rate ME can be as low as less than 28%. Therefore, the proposed algorithm has better segmentation performance and robustness than the existing state-of-the-art intuitionistic fuzzy clustering-related segmentation algorithm in the presence of noise.
C1 [Wu, Chengmao; Huang, Congcong; Zhang, Jiajia] Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Huang, CC (corresponding author), Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
EM wuchengmao123@sohu.com; huangcongcong_1234@163.com; zhang_jj97@163.com
OI Wu, Chengmao/0000-0002-5881-4723; Huang, Congcong/0000-0002-2971-8978;
   Zhang, JiajiaZ/0000-0002-3144-4179
FU National Natural Science Foundation of China [61671377, 51709228];
   Shaanxi Natural Science Foundation of China [2016JM8034, 2017JM6107,
   2018JM4018]; School of Electronic Engineering, Xi'an University of Posts
   & Telecommunications, Xi'an, China
FX This work was supported by the National Natural Science Foundation of
   China (61671377, 51709228) and the Shaanxi Natural Science Foundation of
   China (2016JM8034, 2017JM6107, 2018JM4018). Besides, the authors would
   like to thank the School of Electronic Engineering, Xi'an University of
   Posts & Telecommunications, Xi'an, China, for financial support.
CR Ahmed MN, 2002, IEEE T MED IMAGING, V21, P193, DOI 10.1109/42.996338
   Alipour S, 2014, MACH VISION APPL, V25, P1469, DOI 10.1007/s00138-014-0606-5
   [Anonymous], 1969, Nonlinear Programming, A Unified Approach
   Arora J, 2020, EAI ENDORSED TRANS S, V7, DOI 10.4108/eai.13-7-2018.159622
   Arora J, 2018, J INTELL FUZZY SYST, V35, P5255, DOI 10.3233/JIFS-169809
   Atanassov K. T., 1986, Fuzzy Sets and Systems, V20, P87, DOI 10.1016/S0165-0114(86)80034-3
   Bai XZ, 2019, IEEE J BIOMED HEALTH, V23, P2039, DOI 10.1109/JBHI.2018.2884208
   Balafar MA, 2010, ARTIF INTELL REV, V33, P261, DOI 10.1007/s10462-010-9155-0
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Benaichouche AN, 2013, DIGIT SIGNAL PROCESS, V23, P1390, DOI 10.1016/j.dsp.2013.07.005
   Bezdek J. C., 1981, Pattern recognition with fuzzy objective function algorithms
   Bezdek J. C., 1973, Journal of Cybernetics, V3, P58, DOI 10.1080/01969727308546047
   BEZDEK JC, 1980, IEEE T PATTERN ANAL, V2, P1, DOI 10.1109/TPAMI.1980.4766964
   Bharill Neha, 2016, IEEE Transactions on Big Data, V2, P339, DOI 10.1109/TBDATA.2016.2622288
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI [10.1016/0041-5553(67)90040-7, DOI 10.1016/0041-5553(67)90040-7]
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Carata SV, 2016, INT CONF COMM, P61, DOI 10.1109/ICComm.2016.7528317
   Chaira T, 2014, INT J COMPUT INT SYS, V7, P360, DOI 10.1080/18756891.2013.865830
   Chaira T, 2011, APPL SOFT COMPUT, V11, P1711, DOI 10.1016/j.asoc.2010.05.005
   Chen MY, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106237
   Chen SC, 2004, IEEE T SYST MAN CY B, V34, P1907, DOI 10.1109/TSMCB.2004.831165
   Chen XJ, 2019, IET IMAGE PROCESS, V13, P607, DOI 10.1049/iet-ipr.2018.5597
   Chen X, 2016, IEEE SYS MAN CYBERN, P2547, DOI 10.1109/SMC.2016.7844622
   COLEMAN GB, 1979, P IEEE, V67, P773, DOI 10.1109/PROC.1979.11327
   de Groen P., 1996, NIEUW ARCH WISK, V14, P237
   Despotovic I, 2013, IEEE SIGNAL PROC LET, V20, P295, DOI 10.1109/LSP.2013.2244080
   Fuchs P, 2021, NEUROCOMPUTING, V453, P85, DOI 10.1016/j.neucom.2021.04.094
   Gharib RR., 2018, MACH LEARN DATA MIN, DOI [10.5772/intechopen.74514, DOI 10.5772/INTECHOPEN.74514]
   Giordana N, 1997, IEEE T PATTERN ANAL, V19, P465, DOI 10.1109/34.589206
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Guo YH, 2013, CIRC SYST SIGNAL PR, V32, P1699, DOI 10.1007/s00034-012-9531-x
   Hasnat MA, 2016, STAT COMPUT, V26, P861, DOI 10.1007/s11222-015-9576-3
   Hua XQ, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20040256
   Hua XQ, 2018, DIGIT SIGNAL PROCESS, V75, P232, DOI 10.1016/j.dsp.2018.01.008
   Huang CW, 2015, SOFT COMPUT, V19, P459, DOI 10.1007/s00500-014-1264-2
   Isidoro JMGP, 2021, MEASUREMENT, V180, DOI 10.1016/j.measurement.2021.109477
   Kaur Prabhjot, 2017, International Journal of Information Technology, V9, P345, DOI 10.1007/s41870-017-0039-2
   Kaur Prabhjot, 2012, WSEAS Transactions on Computers, V11, P65
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Krishnapuram R., 1993, IEEE Transactions on Fuzzy Systems, V1, P98, DOI 10.1109/91.227387
   Kumar Dhirendra, 2019, Multimedia Tools and Applications, V78, P12663, DOI 10.1007/s11042-018-5954-0
   Kumar D, 2020, SOFT COMPUT, V24, P4003, DOI 10.1007/s00500-019-04169-y
   Kumar S, 2016, 2016 SECOND INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE & COMMUNICATION TECHNOLOGY (CICT), P6, DOI 10.1109/CICT.2016.12
   Li CM, 2011, IEEE T IMAGE PROCESS, V20, P2007, DOI 10.1109/TIP.2011.2146190
   Lin KP, 2014, IEEE T FUZZY SYST, V22, P1074, DOI 10.1109/TFUZZ.2013.2280141
   Liu MZ, 2012, IEEE T PATTERN ANAL, V34, P2407, DOI 10.1109/TPAMI.2012.44
   Liu Meizhu, 2011, TOTAL BREGMAN DIVERG
   Luo J, 2017, ADV INTELL SYST, V541, P365, DOI 10.1007/978-3-319-49568-2_52
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mújica-Vargas D, 2020, J INTELL FUZZY SYST, V39, P1097, DOI 10.3233/JIFS-192005
   Mujica-Vargas D, 2020, INT J FUZZY SYST, V22, P901, DOI 10.1007/s40815-020-00824-x
   Olabarriaga SD, 2001, MED IMAGE ANAL, V5, P127, DOI 10.1016/S1361-8415(00)00041-4
   Pelekis Nikos, 2008, International Journal of Business Intelligence and Data Mining, V3, P45, DOI 10.1504/IJBIDM.2008.017975
   Qiu CY, 2013, PATTERN RECOGN LETT, V34, P1329, DOI 10.1016/j.patrec.2013.04.021
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shivhare P., 2015, INT J ENG ADV TECHNO, V4, P153
   Szilágyi L, 2003, P ANN INT IEEE EMBS, V25, P724, DOI 10.1109/IEMBS.2003.1279866
   Szmidt E, 2000, FUZZY SET SYST, V114, P505, DOI 10.1016/S0165-0114(98)00244-9
   Tang YC, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00510
   Tang YC, 2019, ROBOT CIM-INT MANUF, V59, P36, DOI 10.1016/j.rcim.2019.03.001
   Tsai DM, 2011, PATTERN RECOGN, V44, P1750, DOI 10.1016/j.patcog.2011.02.009
   Vemuri BC, 2011, IEEE T MED IMAGING, V30, P475, DOI 10.1109/TMI.2010.2086464
   Verma H, 2016, APPL SOFT COMPUT, V46, P543, DOI 10.1016/j.asoc.2015.12.022
   Verma H, 2015, INT J ARTIF INTELL T, V24, DOI 10.1142/S0218213015500165
   Vlachos IK, 2006, INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR MODELLING, CONTROL & AUTOMATION JOINTLY WITH INTERNATIONAL CONFERENCE ON INTELLIGENT AGENTS, WEB TECHNOLOGIES & INTERNET COMMERCE, VOL 1, PROCEEDINGS, P2
   Wang L, 2010, J NEUROSCI METH, V188, P316, DOI 10.1016/j.jneumeth.2010.03.004
   Wang Zhao, 2014, Application Research of Computers, V31, P2864, DOI 10.3969/j.issn.1001-3695.2014.09.073
   Wu CM, 2020, APPL SOFT COMPUT, V94, DOI 10.1016/j.asoc.2020.106468
   Wu CM, 2020, DIGIT SIGNAL PROCESS, V97, DOI 10.1016/j.dsp.2019.102615
   [吴成茂 Wu Chengmao], 2019, [兵工学报, Acta Armamentarii], V40, P1890
   Xu ZS, 2010, J SYST ENG ELECTRON, V21, P580, DOI 10.3969/j.issn.1004-4132.2010.04.009
   YAGER RR, 1979, INT J GEN SYST, V5, P221, DOI 10.1080/03081077908547452
   YAGER RR, 1980, INFORM CONTROL, V44, P236, DOI 10.1016/S0019-9958(80)90156-4
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang QZ, 2018, CIV ENG J-STAVEB OBZ, V27, P513, DOI 10.14311/CEJ.2018.04.0041
   Zhao F, 2020, IEEE T FUZZY SYST, V28, P1023, DOI 10.1109/TFUZZ.2020.2973121
   Zhao F, 2019, IEEE T FUZZY SYST, V27, P387, DOI 10.1109/TFUZZ.2018.2852289
   Zhao F, 2018, NEUROCOMPUTING, V312, P296, DOI 10.1016/j.neucom.2018.05.116
   Zhao F, 2014, EXPERT SYST APPL, V41, P4083, DOI 10.1016/j.eswa.2014.01.003
NR 79
TC 5
Z9 5
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 149
EP 181
DI 10.1007/s00371-021-02319-8
EA NOV 2021
PG 33
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000717889400001
DA 2024-07-18
ER

PT J
AU Cohen, M
   Sato, R
   Noji, R
   Iida, T
   Tokumitsu, Y
AF Cohen, Michael
   Sato, Rintaro
   Noji, Ryota
   Iida, Takato
   Tokumitsu, Yoshiki
TI Directional selectivity in panoramic and pantophonic interfaces:
   Flashdark, Narrowcasting for Stereoscopic Photospherical Cinemagraphy,
   <i>Akabeko</i> Ensemble
SO VISUAL COMPUTER
LA English
DT Article
DE Cinemagraphy; Entertainment computing; IoT; Location-based entertainment
   (lbe); Narrowcasting; Omnistereo; omnidirectional stereoscopic ("ods");
   Panoramic interfaces; Pantophony; Smart lighting; Transparency atlas;
   Virtual reality
ID VIDEO
AB We investigate the potential of interactive user interfaces with omnidirectional horizontal selection. Three novel multimodal interfaces have been developed, exploring different ways of displaying and controlling spaces that encourage panoramic and pantophonic experience by featuring selection of objects distributed around the subjective equator. Besides being explicitly omnidirectional, at least regarding azimuthal orientation, they are all stereoscopic "omni-stereo." "Flashdark" allows the experience of actively darkening a physical area, inverting the usual polarity of lighting control by using a smartphone affordance with intuitive operation as a virtual "anti-light." "Narrowcasting for Stereoscopic Photospherical Cinemagraphy" uses a visibility atlas to selectively animate articulated sectors of a photographically captured binocular scene. "Akabeko Ensemble" is visual music, using a keyboard controller to trigger pantophonic display of a helical chorus by an annular speaker array.
C1 [Cohen, Michael; Sato, Rintaro; Noji, Ryota; Iida, Takato; Tokumitsu, Yoshiki] Univ Aizu, Spatial Media Grp, Comp Arts Lab, Aizu Wakamatsu, Fukushima 9658580, Japan.
C3 University of Aizu
RP Cohen, M (corresponding author), Univ Aizu, Spatial Media Grp, Comp Arts Lab, Aizu Wakamatsu, Fukushima 9658580, Japan.
EM mcohen@u-aizu.ac.jp
RI Cohen, Michael/AAG-8852-2020
OI Cohen, Michael/0000-0001-8941-1575
CR Cohen M, 2003, INT J HUM-COMPUT INT, V15, P297, DOI 10.1207/S15327590IJHC1502_7
   Cohen M., 1998, CYBERWORLDS, V19, P289, DOI [10.1007/978-4-431-67941-7_19, DOI 10.1007/978-4-431-67941-7_19]
   Cohen M., 2007, SPRINGER LECT NOTES, V4813, DOI 10.1007/978-3-540-76702-2
   Cohen M., 2017, SCPA SCI PHONE APPS, V3, P1, DOI [10.1186/s41070-017-0017-x, DOI 10.1186/ISSNS41070-017-0017-X.1]
   Cohen M., 2019, ICAT EGVE, DOI 10.2312/egve.20191303
   Cohen M., 2020, The Technology of Binaural Understanding, P351, DOI [10.1007/978-3-030-00386-9_13, DOI 10.1007/978-3-030-00386-9_13]
   Cohen M, 2007, 2007 CIT: 7TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY, PROCEEDINGS, P517, DOI 10.1109/CIT.2007.140
   Cohen M, 2015, LECT NOTES COMPUT SC, V9189, P274, DOI 10.1007/978-3-319-20804-6_25
   Fafard DB, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281540
   Ford B., 2002, REVOLUTIONARY QUICKT
   Geyer C, 2003, VISUAL COMPUT, V19, P405, DOI 10.1007/s00371-003-0204-4
   Glassner A.S., 2012, GRAPHICS GEMS, VIII, P366, DOI [10.1016/B978-0-08-050755-2.50079-8, DOI 10.1016/B978-0-08-050755-2.50079-8]
   Herder J, 2002, J NEW MUSIC RES, V31, P269, DOI 10.1076/jnmr.31.3.269.14180
   Hoelzl I, 2014, VISUAL STUD, V29, P261, DOI 10.1080/1472586X.2014.941559
   Holynski Aleksander, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P5806, DOI 10.1109/CVPR46437.2021.00575
   Huhtamo E, 2013, Illusions in Motion. Media Archaeology of the Moving Panorama and Related Spectacles
   Jenrungrot V., 2020, P ANN C NEUR INF PRO, P20925
   Jung J, 2017, VISUAL COMPUT, V33, P737, DOI 10.1007/s00371-017-1368-7
   Kang Inhye, 2017, [Journal of History of Modern Art, 현대미술사연구], V41, P139, DOI 10.17057/kahoma.2017..41.006
   Kim H, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P532, DOI 10.1109/VRW52623.2021.00148
   Kitchens SA., 1998, QUICKTIME VR BOOK CR
   Liu F., 2008, P 16 ACM INT C MULTI, P329
   Liu XX, 2017, T GIS, V21, P897, DOI 10.1111/tgis.12247
   Marrinan T, 2021, IEEE T VIS COMPUT GR, V27, P2587, DOI 10.1109/TVCG.2021.3067780
   Nielsen F, 2005, VISUAL COMPUT, V21, P92, DOI 10.1007/s00371-004-0273-z
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   Ranaweera R, 2015, PRESENCE-TELEOP VIRT, V24, P220, DOI 10.1162/PRES_a_00232
   Rav-Acha A, 2005, PROC CVPR IEEE, P58
   Richardt C, 2013, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2013.166
   Ritter KA, 2022, VIRTUAL REAL-LONDON, V26, P571, DOI 10.1007/s10055-021-00502-9
   Rui Yong., 2001, Proceedings of the SIGCHI conference on Human factors in computing systems, P450
   Ryskeldiev B, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3132813
   Seo D, 2014, MULTIMEDIA SYST, V20, P707, DOI 10.1007/s00530-013-0333-1
   Shepard RN., 1984, PSYCHOL MUSIC, P343
   Smith NG, 2013, INT SYMP IMAGE SIG, P552
   Spors S, 2013, P IEEE, V101, P1920, DOI 10.1109/JPROC.2013.2264784
   Streitberger A., 2014, HETEROGENEOUS OBJECT, P59
   Tokumitsu Y., 2020, ETLTC, V77, DOI 10.1051/shsconf/20207704005
   Villegas J., 2011, PRINCIPLES APPL SPAT, P463, DOI [10.1142/9789814299312_0036, DOI 10.1142/9789814299312_0036]
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
NR 40
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 3125
EP 3137
DI 10.1007/s00371-021-02293-1
EA OCT 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000706056900003
DA 2024-07-18
ER

PT J
AU Yang, SD
   Chen, HY
   Xu, FC
   Li, Y
   Yuan, JM
AF Yang, Shuaidong
   Chen, Haiyun
   Xu, Fancheng
   Li, Yang
   Yuan, Jiemin
TI High-performance UAVs visual tracking based on siamese network
SO VISUAL COMPUTER
LA English
DT Article
DE Object tracking; Unmanned aerial vehicles; Intersection over union;
   Siamese network; Attention
ID ROBUST
AB In the process of unmanned aerial vehicles (UAVs) object tracking, the tracking object is lost due to many problems such as occlusion and fast motion. In this paper, based on the SiamRPN algorithm, a UAV object tracking algorithm that optimizes the semantic information of cyberspace, channel feature information and strengthens the selection of bounding boxes, is proposed. Since the traditional SiamRPN method does not consider remote context information, the calculation and selection of bounding boxes need to be improved. Therefore, (1) we design a convolutional attention module to enhance the weighting of feature spatial location and feature channels. (2) We also add a multi-spectral channel attention module to the search branch of the network to further solve remote dependency problems of the network and effectively understand different UAVs tracking scenes. Finally, we use the distance intersection over union to predict the bounding box, and the accurate prediction bounding box is regressed. The experimental results show that the algorithm has strong robustness and accuracy in many scenes.
C1 [Yang, Shuaidong; Chen, Haiyun; Xu, Fancheng; Li, Yang; Yuan, Jiemin] Southwest Petr Univ, Sch Elect Engn & Informat, Chengdu, Peoples R China.
C3 Southwest Petroleum University
RP Yang, SD (corresponding author), Southwest Petr Univ, Sch Elect Engn & Informat, Chengdu, Peoples R China.
EM yangshauidong@foxmail.com
OI Yang, Shuaidong/0000-0002-8133-4525
FU Smart grid and intelligent control Nanchong city Key Laboratory Platform
   Construction [19SXHZ0011]
FX This research was supported by the Smart grid and intelligent control
   Nanchong city Key Laboratory Platform Construction (phase I),
   Grant/Award Number: 19SXHZ0011.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cheng H, 2017, IEEE INT C INT ROBOT, P1732, DOI 10.1109/IROS.2017.8205986
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fu CH, 2014, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2014.6907659
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gong KC, 2021, VISUAL COMPUT, V37, P371, DOI 10.1007/s00371-020-01805-9
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu H., 2015, P IEEE C COMP VIS PA, P211
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li YM, 2021, IEEE T MULTIMEDIA, V23, P810, DOI 10.1109/TMM.2020.2990064
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   QILONG W, 2020, 2020 IEEE CVF C COMP
   Qin Z, 2020, ARXIV PREPRINT ARXIV
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian B, 2011, IEEE INT C INTELL TR, P1103, DOI 10.1109/ITSC.2011.6083125
   Tian CW, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.106949
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang XC, 2017, IEEE T IMAGE PROCESS, V26, P4765, DOI 10.1109/TIP.2017.2723239
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yang M., 2021, VISUAL COMPUT
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yin YJ, 2016, IEEE T INSTRUM MEAS, V65, P510, DOI 10.1109/TIM.2015.2509318
   Yu HY, 2020, INT J COMPUT VISION, V128, P1141, DOI 10.1007/s11263-019-01266-1
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang T., 2017, IEEE IPCCC, P4335
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 69
TC 8
Z9 8
U1 0
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2107
EP 2123
DI 10.1007/s00371-021-02271-7
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000684532700001
DA 2024-07-18
ER

PT J
AU Ben Jabra, S
   Zagrouba, E
AF Ben Jabra, Saoussen
   Zagrouba, Ezzeddine
TI Robust anaglyph 3D video watermarking based on cyan mosaic generation
   and DCT insertion in Krawtchouk moments
SO VISUAL COMPUTER
LA English
DT Article
DE 3D anaglyph video; Watermarking; Cyan mosaic; Krawtchouk moments; DCT
   embedding; Collusion attack
ID COLLUSION; SCHEME
AB The need to protect 3D video content against pirating has become very important due to the widespread expansion of the internet and the proliferation of 3D video content consumption over networks. Several 3D video watermarking approaches have been proposed to resolve this problem, but they have been lacking efficiency to resist malicious attacks, especially the collusion attack. In fact, this attack has not been considered by most existing work. This paper proposes a novel approach for 3D anaglyph watermarking based on cyan mosaic as an embedding target. First, cyan mosaic is generated using the cyan channel extracted from the original 3D anaglyph video. Then, a signature is embedded into the obtained mosaic using Krawtchouk moments and a DCT based technique. This maximizes the compromise between invisibility and robustness against usual and malicious attacks. Finally, the marked 3D anaglyph video is obtained by combining the marked cyan video and the original red video. The experimental results illustrate good robustness against compression, collusion and several other additional attacks, such as geometric manipulation and temporal attacks. Besides, the suggested technique provides high invisibility thanks to DCT based embedding.
C1 [Ben Jabra, Saoussen] Univ Sousse, LR16ES06 Lab Rech Informat Modelisat & Traitement, Ecole Natl Ingn Sousse ENISo, BP 264 Sousse Erriadh, Sousse 4023, Tunisia.
   [Zagrouba, Ezzeddine] Univ Tunis El Manar, Inst Super Informat, LR16ES06 Lab Rech Informat Modelisat & Traitement, 2 Rue Abou Raihane Bayrouni, Ariana 2080, Tunisia.
C3 Universite de Sousse; Universite de Tunis-El-Manar
RP Ben Jabra, S (corresponding author), Univ Sousse, LR16ES06 Lab Rech Informat Modelisat & Traitement, Ecole Natl Ingn Sousse ENISo, BP 264 Sousse Erriadh, Sousse 4023, Tunisia.
EM saoussen.bj@gmail.com; ezzeddine.zagrouba@uvt.tn
RI Zagrouba, Ezzeddine/D-7896-2014
OI Zagrouba, Ezzeddine/0000-0002-2574-9080
CR Agilandeeswari L, 2016, MULTIMED TOOLS APPL, V75, P8745, DOI 10.1007/s11042-015-2789-9
   Asikuzzaman M, 2018, IEEE T CIRC SYST VID, V28, P2131, DOI 10.1109/TCSVT.2017.2712162
   Barhoumi W, 2013, SIGNAL IMAGE VIDEO P, V7, P843, DOI 10.1007/s11760-011-0273-1
   Bayoudh Ines, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P158
   Bayoudh I, 2018, MULTIMED TOOLS APPL, V77, P14361, DOI 10.1007/s11042-017-5033-y
   Bayoudh I, 2017, LECT NOTES COMPUT SC, V10617, P493, DOI 10.1007/978-3-319-70353-4_42
   Bhatnagar G, 2011, J VISUAL-JAPAN, V14, P85, DOI 10.1007/s12650-010-0067-5
   Devi HS, 2016, CONT ENG SCI, V9, P1575, DOI [10.12988/ces.2016.69156, DOI 10.12988/CES.2016.69156]
   Dhaou D, 2019, LECT NOTES COMPUT SC, V11679, P96, DOI 10.1007/978-3-030-29891-3_9
   Dhaou D, 2019, 3D RES, V10, DOI 10.1007/s13319-019-0231-1
   Dhaou D, 2018, VISAPP: PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL 4: VISAPP, P501, DOI 10.5220/0006619305010510
   Dhaou D, 2019, 3D RES, V10, DOI 10.1007/s13319-019-0223-1
   Gupta G, 2018, MICROSYST TECHNOL, V24, P2539, DOI 10.1007/s00542-017-3689-x
   Houmansadr A, 2007, LECT NOTES COMPUT SC, V4437, P343
   Kerbiche A, 2018, MULTIMED TOOLS APPL, V77, P26769, DOI 10.1007/s11042-018-5888-6
   Koubaa M, 2012, MULTIMED TOOLS APPL, V56, P281, DOI 10.1007/s11042-010-0626-8
   Lynda S., 2020, VISUAL COMPUT, V3, P1
   Manaf AA., 2016, INT J APPL ENG RES, V11, P3484
   Munoz-Ramirez DO, 2015, P EUR WORKSH URB DAT, P1, DOI [10.1109/ICEEE.2015.7357955, DOI 10.2312/UDMV.20151341]
   Patel R., 2015, INT J ENG TECH RES I, V3, P55
   Prathap I, 2014, COMPUT ELECTR ENG, V40, P51, DOI 10.1016/j.compeleceng.2013.11.005
   Salih JW, 2015, INT J ADV SCI TECHNO, V82, P11, DOI [10.14257/ijast.2015.82.02, DOI 10.14257/ijast.2015.82.02]
   Venkataramana A, 2007, ICCTA 2007: INTERNATIONAL CONFERENCE ON COMPUTING: THEORY AND APPLICATIONS, PROCEEDINGS, P676
   Waleed Jumana, 2013, International Journal of Advancements in Computing Technology, V5, P163
   Wang C., 2015, INT J IMAGE PROCESS, V9, P156
   Zadokar SR, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P483, DOI 10.1109/ICACCI.2013.6637219
   Zeng L, 2014, MACH VISION APPL, V25, P1271, DOI 10.1007/s00138-013-0551-8
   Zhang G., 2021, VISUAL COMPUT, P1
   Zhang L, 2007, IET INFORM SECUR, V1, P97, DOI 10.1049/iet-ifs:20060105
NR 29
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3611
EP 3625
DI 10.1007/s00371-021-02191-6
EA JUN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000660360700002
DA 2024-07-18
ER

PT J
AU Ahmad, N
   Asghar, S
   Gillani, SA
AF Ahmad, Nouman
   Asghar, Sohail
   Gillani, Saira Andleeb
TI Transfer learning-assisted multi-resolution breast cancer
   histopathological images classification
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Convolution neural network; Transfer learning; Breast
   cancer; Histopathological images
ID CONVOLUTIONAL NEURAL-NETWORKS; DEEP
AB Breast cancer is one of the leading death cause among women nowadays. Several methods have been proposed for the detection of breast cancer. Various machine learning-based automatic diagnosis systems have been developed known as Computer Aided Diagnostics (CAD) systems. Initial CAD systems were based on machine learning algorithms however due to the automatic feature extraction ability of convolutional neural networks (CNN)-based deep learning models are widely adopted. Deep learning is widely used in various fields. Healthcare is one of the essential field that deep learning has transformed. Another common issue faced by the patients is difference of opinion among different pathologists and medical practitioners. Such human errors often lead to misleading or delayed judgment, which may be fatal to human life. To improve decision consistency, efficiency, and error reduction, researchers in the field of healthcare are using deep learning-based approaches and achieved state of art results. In this study, a deep learning (DL) and transfer learning-based approach is proposed to classify histopathological images for breast cancer diagnosis. In this study, we have adopted patch selection approach to classify breast histopathological images on small number of training images using transfer learning without losing the performance. Initially, patches are extracted from Whole Slide Images and fed into the CNN for features extraction. Based on these features, the discriminative patches are selected and then fed to Efficient-Net architecture pre-trained on ImageNet dataset. Features extracted from Efficient-Net architecture are also used to train a SVM classifier. The proposed model outperforms the baseline methods in terms of multiple performance measures.
C1 [Ahmad, Nouman; Asghar, Sohail] COMSATS Univ Islamabad CUI, Dept Comp Sci, Islamabad 45550, Pakistan.
   [Gillani, Saira Andleeb] Bahria Univ, Dept Comp Sci, Lahore Campus, Lahore, Pakistan.
C3 COMSATS University Islamabad (CUI)
RP Ahmad, N (corresponding author), COMSATS Univ Islamabad CUI, Dept Comp Sci, Islamabad 45550, Pakistan.
EM noumanahmad3777@outlook.com
RI Ahmad, Nouman/KIB-5267-2024
OI Ahmad, Nouman/0000-0003-0202-9205
CR Abdullah-Al Nahid, 2018, BIOMED RES INT, V2018, DOI 10.1155/2018/2362108
   Abu Samah A, 2017, IEEE I C SIGNAL IMAG, P102, DOI 10.1109/ICSIPA.2017.8120587
   Akhtar Z, 2016, J ELECTR COMPUT ENG, V2016, DOI 10.1155/2016/4721849
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Araújo T, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177544
   Bardou D, 2018, IEEE ACCESS, V6, P24680, DOI 10.1109/ACCESS.2018.2831280
   Benhammou Y., 2018, proceedings of the international conference on learning and optimization algorithms: theory and applications, P1
   Benhammou Y, 2020, NEUROCOMPUTING, V375, P9, DOI 10.1016/j.neucom.2019.09.044
   Chan A, 2016, ROY SOC OPEN SCI, V3, DOI 10.1098/rsos.160558
   Das K, 2018, I S BIOMED IMAGING, P578, DOI 10.1109/ISBI.2018.8363642
   de Matos J, 2019, IEEE IJCNN
   Deniz E, 2018, HEALTH INF SCI SYST, V6, DOI 10.1007/s13755-018-0057-x
   Gandomkar Z, 2018, ARTIF INTELL MED, V88, P14, DOI 10.1016/j.artmed.2018.04.005
   Guo Y, 2020, NEURAL COMPUT APPL
   Guo YB, 2020, NEUROCOMPUTING, V379, P305, DOI 10.1016/j.neucom.2019.10.091
   Guo YB, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-019-2940-0
   Han ZY, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04075-z
   Jiang Y, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0207258
   Kahya M.A., 2017, J. Appl. Math. Bioinf., V7, P49
   King DB, 2015, ACS SYM SER, V1214, P1
   Kumar K., 2018, P 2018 4 INT C RECEN, P1, DOI DOI 10.1109/RAIT.2018.8389034
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lyu CQ, 2018, BMC GENOMICS, V19, DOI 10.1186/s12864-018-5283-8
   Motlagh NH, 2018, BIORXIV
   Nahid AA, 2018, INFORMATION, V9, DOI 10.3390/info9010019
   Nawaz MA, 2018, INT J COMPUT SCI NET, V18, P152
   Nejad EM, 2015, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON IMAGING, SIGNAL PROCESSING AND COMMUNICATION, P50, DOI 10.1145/3132300.3132331
   Orenstein EC, 2017, IEEE WINT CONF APPL, P1082, DOI 10.1109/WACV.2017.125
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382
   Perez, 2017, ENVIRON TECHNOL
   Razavian A. S., 2014, Workshop on IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014), P806, DOI [10.1109/cvprw.2014.131, DOI 10.1109/CVPRW.2014.131]
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sanchez-Morillo Daniel, 2018, Bioinformatics and Biomedical Engineering. 6th International Work-Conference, IWBBIO 2018. Proceedings: LNB 10814, P276, DOI 10.1007/978-3-319-78759-6_26
   Shallu, 2018, ICT EXPRESS, V4, P247, DOI 10.1016/j.icte.2018.10.007
   Sharma S, 2020, J DIGIT IMAGING, V33, P632, DOI 10.1007/s10278-019-00307-y
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516044442, 10.1146/annurev-bioeng-071516-044442]
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Singh J, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13395-9
   Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264
   Sun JM, 2017, 2017 IEEE CONFERENCE ON BIG DATA AND ANALYTICS (ICBDA), P43, DOI 10.1109/ICBDAA.2017.8284105
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tan MX, 2019, PR MACH LEARN RES, V97
   Uludag U, 2004, PATTERN RECOGN, V37, P1533, DOI 10.1016/j.patcog.2003.11.012
   Wang P, 2021, BIOMED SIGNAL PROCES, V65, DOI 10.1016/j.bspc.2020.102341
   Wang Zheng-peng, 2012, Computer Engineering, V38, P38, DOI 10.3969/j.issn.1000-3428.2012.10.010
NR 46
TC 44
Z9 45
U1 1
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2751
EP 2770
DI 10.1007/s00371-021-02153-y
EA MAY 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000650092900001
DA 2024-07-18
ER

PT J
AU Ghosh, M
   Roy, SS
   Mukherjee, H
   Obaidullah, SM
   Santosh, KC
   Roy, K
AF Ghosh, Mridul
   Roy, Sayan Saha
   Mukherjee, Himadri
   Obaidullah, Sk Md
   Santosh, K. C.
   Roy, Kaushik
TI Understanding movie poster: transfer-deep learning approach for
   graphic-rich text recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Transfer learning; CNN; Movie title
ID SCRIPT IDENTIFICATION; IMAGE
AB Graphic-rich texts are common in posters. In a movie poster, information, such as movie title, tag lines, and names of the actors, director, and production house, is available. Graphic-rich texts in movie titles represent not only sentiments but also their genre. Understanding the poster requires graphic-rich text recognition. Prior to that, one requires text localization, so background and foreground graffiti can be well segmented. In this paper, we propose a transfer learning-based approach for graphic-rich text localization, which was tuned by introducing reverse augmentation and rotated/inclined rectangle drawing technique. A convolution neural network-based model is then applied to identify their corresponding scripts. In our experiments, on a newly developed dataset (available upon request) that is composed of movie posters with multiple scripts of 1154 images, we achieved an average accuracy of 99.30%. Our results outperformed previously developed tools that are relying on handcrafted features.
C1 [Ghosh, Mridul] Shyampur Siddheswari Mahavidyalaya, Dept Comp Sci, Howrah, India.
   [Roy, Sayan Saha] Calcutta Univ, Dept Radio Phys & Elect, Kolkata, India.
   [Mukherjee, Himadri; Roy, Kaushik] West Bengal State Univ, Dept Comp Sci, Kolkata, India.
   [Obaidullah, Sk Md] Aliah Univ, Dept Comp Sci & Engn, Kolkata, India.
   [Santosh, K. C.] Univ South Dakota, Dept Comp Sci, Vermillion, SD 57069 USA.
C3 University of Calcutta; West Bengal State University; Aliah University;
   University of South Dakota
RP Roy, K (corresponding author), West Bengal State Univ, Dept Comp Sci, Kolkata, India.
EM mridulxyz@gmail.com; sayansaharoy97@gmail.com; himadrim027@gmail.com;
   sk.obaidullah@gmail.com; santosh.kc@usd.edu; kaushik.mrg@gmail.com
RI GHOSH, DR. MRIDUL/AEY-8327-2022; GHOSH, MRIDUL/ABA-8687-2021; Santosh,
   K.C./H-1363-2012; Roy, Kaushik/O-7021-2019
OI GHOSH, DR. MRIDUL/0000-0002-4777-2492; GHOSH,
   MRIDUL/0000-0002-4777-2492; Roy, Kaushik/0000-0002-3360-7576; Sk, Md
   Obaidullah/0000-0002-5207-3709; Saha Roy, Sayan/0000-0002-1245-8161
CR Agarwal Megha, 2010, International Journal of Signal and Imaging Systems Engineering, V3, P246, DOI 10.1504/IJSISE.2010.038020
   Aksoy S, 2005, IEEE T GEOSCI REMOTE, V43, P581, DOI 10.1109/TGRS.2004.839547
   Banashree NP, 2007, PROC WRLD ACAD SCI E, V20, P46
   Bhunia AK, 2019, PATTERN RECOGN, V85, P172, DOI 10.1016/j.patcog.2018.07.034
   Bouguelia MR, 2018, INT J MACH LEARN CYB, V9, P1307, DOI 10.1007/s13042-017-0645-0
   Busch A, 2005, IEEE T PATTERN ANAL, V27, P1720, DOI 10.1109/TPAMI.2005.227
   Cai LL, 2017, VISUAL COMPUT, V33, P163, DOI 10.1007/s00371-015-1167-y
   Deguillaume F, 2002, PROC SPIE, V4675, P313, DOI 10.1117/12.465289
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Ghosh M, 2019, PROC INT CONF DOC, P86, DOI 10.1109/ICDARW.2019.00020
   Gomez L, 2017, PATTERN RECOGN, V67, P85, DOI 10.1016/j.patcog.2017.01.032
   Gómez L, 2016, PROCEEDINGS OF 12TH IAPR WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, (DAS 2016), P192, DOI 10.1109/DAS.2016.64
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hochberg J, 1997, IEEE T PATTERN ANAL, V19, P176, DOI 10.1109/34.574802
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang L, 2021, VISUAL COMPUT, V37, P1221, DOI 10.1007/s00371-020-01995-2
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jolliffe I., 2011, International Encyclopedia of Statistical Science, P1094, DOI [DOI 10.1007/978-3-642-04898-2_455, 10.1007/978-3-642-04898-2_455]
   Khmag A, 2018, VISUAL COMPUT, V34, P575, DOI 10.1007/s00371-017-1362-0
   Köksoy O, 2006, APPL MATH COMPUT, V175, P1716, DOI 10.1016/j.amc.2005.09.016
   Lu SJ, 2015, INT J DOC ANAL RECOG, V18, P125, DOI 10.1007/s10032-015-0237-z
   Luisier F, 2011, IEEE T IMAGE PROCESS, V20, P696, DOI 10.1109/TIP.2010.2073477
   Ma H., 2003, ACM T ASIAN LANG INF, V2, P193, DOI [10.1145/979872.979875, DOI 10.1145/979872.979875]
   Nayef Nibal, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1582, DOI 10.1109/ICDAR.2019.00254
   Obaidullah SM, 2018, MULTIMED TOOLS APPL, V77, P1643, DOI 10.1007/s11042-017-4373-y
   Obaidullah SM, 2016, ADV INTELL SYST, V396, P37, DOI 10.1007/978-81-322-2653-6_3
   Obaidullah SM, 2014, APPL COMPUT INTELL S, V2014, DOI 10.1155/2014/896128
   Pal U, 2003, PROC INT CONF DOC, P880
   Rimey R.D., 1992, P 1992 DARPA IM UND, P927
   Santosh, 1999, WILEY ENCY ELECT ELE, P1
   Shi BG, 2016, PATTERN RECOGN, V52, P448, DOI 10.1016/j.patcog.2015.11.005
   Shi CZ, 2013, PATTERN RECOGN LETT, V34, P107, DOI 10.1016/j.patrec.2012.09.019
   Shivakumara P, 2015, COMPUT VIS IMAGE UND, V130, P35, DOI 10.1016/j.cviu.2014.09.003
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Wang Guo-de, 2012, Computer Engineering, V38, P199, DOI 10.3969/j.issn.1000-3428.2012.11.061
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Zhang DX, 2018, CSEE J POWER ENERGY, V4, P362, DOI 10.17775/CSEEJPES.2018.00520
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
NR 44
TC 12
Z9 12
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1645
EP 1664
DI 10.1007/s00371-021-02094-6
EA MAR 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000633290700001
DA 2024-07-18
ER

PT J
AU Yang, SY
   Cao, N
   Guo, B
   Li, G
AF Yang, Shuyuan
   Cao, Ning
   Guo, Bin
   Li, Gang
TI Depth map super-resolution based on edge-guided joint trilateral
   upsampling
SO VISUAL COMPUTER
LA English
DT Article
DE Depth image; Image super-resolution; Edge-guided; Joint trilateral
   upsampling
ID PHOTOGRAPHY; ENHANCEMENT; FLASH
AB Depth image super-resolution (DISR) is a significant yet challenging task. In this paper, we propose a novel edge-guided framework for color-guided DISR aiming at reducing the artifacts caused by the introduced color image. Considering the different view synthesis characteristics of texture and smooth regions in depth image, we propose that the edge and smooth regions of depth map should be reconstructed in different ways. In our framework, a novel joint trilateral filter is built firstly, which has two different modes: one for the pixels on the edges and the other for the pixels in the smooth regions. Secondly, in each filtering iteration during the whole upsampling process, we use the edge map updated by the upsampled depth map as a guidance to decide when to change the filter mode. Benefiting from the strategy, the high-resolution depth map reconstructed has less texture copying and contains sharp and smooth edges. Experimental results demonstrate the effectiveness of our approach over prior depth map upsampling works.
C1 [Yang, Shuyuan; Cao, Ning; Guo, Bin] Hohai Univ, Sch Comp & Informat, Nanjing 210098, Peoples R China.
   [Yang, Shuyuan; Guo, Bin] Xinjiang Agr Univ, Coll Comp & Informat Engn, Urumqi 830052, Peoples R China.
   [Li, Gang] Minist Water Resources Peoples Republ China, Informat Ctr, Beijing 100032, Peoples R China.
C3 Hohai University; Xinjiang Agricultural University
RP Yang, SY (corresponding author), Hohai Univ, Sch Comp & Informat, Nanjing 210098, Peoples R China.; Yang, SY (corresponding author), Xinjiang Agr Univ, Coll Comp & Informat Engn, Urumqi 830052, Peoples R China.
EM jsjysy@xjau.edu.cn
OI Yang, Shuyuan/0000-0003-2077-1817
FU National Natural Science Foundation of China [41830110]
FX This study was funded by the National Natural Science Foundation of
   China under grant No.41830110.
CR Alexiadis DS, 2013, 2013 IEEE 11TH IVMSP WORKSHOP: 3D IMAGE/VIDEO TECHNOLOGIES AND APPLICATIONS (IVMSP 2013)
   [Anonymous], 2008, WORKSH MULT MULT SEN
   Diebel J., 2005, P 18 INT C NEUR INF, V18, P291
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Eichhardt I, 2017, MACH VISION APPL, V28, P267, DOI 10.1007/s00138-017-0831-9
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Ferstl D, 2015, IEEE I CONF COMP VIS, P513, DOI 10.1109/ICCV.2015.66
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Gilboa G, 2004, IEEE T PATTERN ANAL, V26, P1020, DOI 10.1109/TPAMI.2004.47
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Lo KH, 2018, IEEE T CYBERNETICS, V48, P371, DOI 10.1109/TCYB.2016.2637661
   Lu XK, 2018, MULTIMED TOOLS APPL, V77, P15521, DOI 10.1007/s11042-017-5131-x
   Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Meng-Lin Chiang, 2018, 2018 IEEE 4th International Conference on Computer and Communications (ICCC). Proceedings, P1445, DOI 10.1109/CompComm.2018.8780777
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ren YN, 2018, KSII T INTERNET INF, V12, P3217, DOI 10.3837/tiis.2018.07.013
   Riemens A.K., 2009, Proceedings of SPIE, V7257, p72570M
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Xiang XZ, 2016, PROCEEDINGS OF THE 2016 12TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P1403, DOI 10.1109/WCICA.2016.7578282
   Xie J, 2016, IEEE T IMAGE PROCESS, V25, P428, DOI 10.1109/TIP.2015.2501749
   Yang H, 2020, VISUAL COMPUT, V36, P1411, DOI 10.1007/s00371-019-01748-w
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P834, DOI 10.1109/TPAMI.2014.2353642
   Yuan L, 2017, J VIS COMMUN IMAGE R, V46, P280, DOI 10.1016/j.jvcir.2017.04.012
   Zhang Z, 2017, IEEE INT CONF AUTOMA, P238, DOI 10.1109/FG.2017.38
   Zhou DS, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181074
NR 36
TC 2
Z9 2
U1 4
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 883
EP 895
DI 10.1007/s00371-021-02057-x
EA FEB 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000616439100001
DA 2024-07-18
ER

PT J
AU Lin, CJ
   Cheng, LY
   Yang, CW
AF Lin, Chiuhsiang Joe
   Cheng, Lai-Yu
   Yang, Chih-Wei
TI An investigation of the influence of age on eye fatigue and hand
   operation performance in a virtual environment
SO VISUAL COMPUTER
LA English
DT Article
DE Hand movement performance; Depth perception; Visual fatigue;
   Fitts&#8217; law; Three-dimensional (3D)
ID DISTANCE PERCEPTION; DEPTH; INFORMATION; REALITY
AB Only a few studies in the literature have focused on the effects of age on fatigue susceptibility and hand operation performance in virtual environments, and even less research has been carried out focusing on older adults. This study aimed to assess the hand movement performance in a virtual environment with two group measures, including visual fatigue and depth perception. A total of 10 adults (5 young adults and 5 older adults) participated in this study. This study was conducted in the following order: pre-CFF measurement, performance of a Fitts' Law task with stereoscopic viewing, post-CFF measurement every 10 min (during task), and recording of task movement time and error rate after each condition of the task. This study found significant effects of age and task index of difficulty on hand movement. Compared to the older adults, the young adults appeared to have better hand movement performance. Regarding eye fatigue, eye fatigue was significantly higher at binocular parallax of 9 cm than at 6 cm and 3 cm. Surprisingly, age had a significant effect on hand movement performance but not on visual fatigue. This study suggests that the operation time should be less than 20 min and that parallax should be 6 cm (visual angle = 1.38 degrees) to prevent visual fatigue when movement tasks are performed in a virtual environment. These data highlight the potential for age-related differences in hand movement performance during the performance of tasks in which fast and accurate selection are required in combination with the manipulation of 3D objects in a virtual environment, and the study provides directions for further exploration.
C1 [Lin, Chiuhsiang Joe] Natl Taiwan Univ Sci & Technol, Dept Ind Management, 43,Sec 4,Keelung Rd, Taipei 10607, Taiwan.
   [Cheng, Lai-Yu] Chihlee Univ Technol, Dept Mkt & Logist Management, 313,Sec 1,Wenhua Rd, New Taipei 22050, Taiwan.
   [Yang, Chih-Wei] Chihlee Univ Technol, Dept Informat Management, 313,Sec 1,Wenhua Rd, New Taipei 22050, Taiwan.
C3 National Taiwan University of Science & Technology
RP Cheng, LY (corresponding author), Chihlee Univ Technol, Dept Mkt & Logist Management, 313,Sec 1,Wenhua Rd, New Taipei 22050, Taiwan.
EM k6842051@ms34.hinet.net
FU National Science Council, Taiwan, ROC
FX We gratefully acknowledge the support of this study by a grant from the
   National Science Council, Taiwan, ROC. The authors would like to thank
   Mr. Wei-Chieh Len for assistance with the data processing.
CR Argelaguet F, 2013, COMPUT GRAPH-UK, V37, P121, DOI 10.1016/j.cag.2012.12.003
   Armbrüster C, 2008, CYBERPSYCHOL BEHAV, V11, P9, DOI 10.1089/cpb.2007.9935
   Bergstrom JCR, 2013, INT J HUM-COMPUT INT, V29, P541, DOI 10.1080/10447318.2012.728493
   Bowman D.A., 2005, 3D User Interfaces: Theory and Practice
   Bystrom KE, 1999, PRESENCE-TELEOP VIRT, V8, P241, DOI 10.1162/105474699566107
   Carmeli E, 2003, J GERONTOL A-BIOL, V58, P146, DOI 10.1093/gerona/58.2.M146
   Chambon M, 2009, J EXP SOC PSYCHOL, V45, P283, DOI 10.1016/j.jesp.2008.08.023
   Chen HJ, 2019, INT J IND ERGONOM, V72, P45, DOI 10.1016/j.ergon.2019.04.004
   Creem-Regehr SH, 2010, WIRES COGN SCI, V1, P800, DOI 10.1002/wcs.82
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Dillon TW, 1996, COMPUT HUM BEHAV, V12, P49, DOI 10.1016/0747-5632(95)00018-6
   Emoto M, 2005, J DISP TECHNOL, V1, P328, DOI 10.1109/JDT.2005.858938
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Friedland H, 2017, COGN TECHNOL WORK, V19, P571, DOI 10.1007/s10111-017-0442-2
   Harm DL, 2008, VISUAL COMPUT, V24, P995, DOI 10.1007/s00371-008-0277-1
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hsu SH, 1999, INT J IND ERGONOM, V23, P461, DOI 10.1016/S0169-8141(98)00013-4
   IJsselsteijn WA, 2005, 3D VIDEOCOMMUNICATION: ALGORITHMS, CONCEPTS AND REAL-TIME SYSTEMS IN HUMAN CENTRED COMMUNICATION, P219
   Interrante V, 2006, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2006.52
   Kock N, 2018, COGN TECHNOL WORK, V20, P489, DOI 10.1007/s10111-018-0479-x
   Lee CH, 2009, VISUAL COMPUT, V25, P1009, DOI 10.1007/s00371-009-0356-y
   Lee YH, 2013, HUM MOVEMENT SCI, V32, P511, DOI 10.1016/j.humov.2012.02.001
   Liu CL, 2009, LECT NOTES COMPUT SC, V5610, P474, DOI 10.1007/978-3-642-02574-7_53
   McGee JS, 2000, CYBERPSYCHOL BEHAV, V3, P469, DOI 10.1089/10949310050078931
   Mon-Williams M, 2000, ERGONOMICS, V43, P391, DOI 10.1080/001401300184486
   MORGAN M, 1994, J GERONTOL, V49, pM133, DOI 10.1093/geronj/49.3.M133
   Murata A, 2001, HUM MOVEMENT SCI, V20, P791, DOI 10.1016/S0167-9457(01)00058-6
   Murgia Alessio, 2009, International Journal of Virtual Reality, V8, P67
   Naceri A., 2010, International Journal On Advances in Intelligent Systems, V3, P51
   Naceri A, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P107, DOI 10.1109/VR.2012.6180905
   Piryankova IV, 2013, DISPLAYS, V34, P153, DOI 10.1016/j.displa.2013.01.001
   Plumert Jodie., 2005, ACM Transactions on Applied Perception, V2, P216, DOI DOI 10.1145/1077399.1077402
   ROLLAND JP, 1995, PRESENCE-TELEOP VIRT, V4, P24, DOI 10.1162/pres.1995.4.1.24
   Seigle D., 2009, VERITAS VISUS, V4, P69
   Siriaraya P, 2012, COMPUT HUM BEHAV, V28, P1873, DOI 10.1016/j.chb.2012.05.005
   Sjölinder M, 2005, INT J HUM-COMPUT ST, V63, P537, DOI 10.1016/j.ijhcs.2005.04.024
   Soukoreff RW, 2004, INT J HUM-COMPUT ST, V61, P751, DOI 10.1016/j.ijhcs.2004.09.001
   Speranza F, 2006, PROC SPIE, V6055, DOI 10.1117/12.640865
   Swan JE, 2007, IEEE T VIS COMPUT GR, V13, P429, DOI 10.1109/TVCG.2007.1035
   Swenson HA, 1932, J GEN PSYCHOL, V7, P360, DOI 10.1080/00221309.1932.9918473
   Ukai K, 2008, DISPLAYS, V29, P106, DOI 10.1016/j.displa.2007.09.004
   Viguier A, 2001, PERCEPTION, V30, P115, DOI 10.1068/p3119
   Willemsen P, 2008, PRESENCE-TELEOP VIRT, V17, P91, DOI 10.1162/pres.17.1.91
   Xia PJ, 2013, VISUAL COMPUT, V29, P433, DOI 10.1007/s00371-012-0748-2
   Yano S, 2004, DISPLAYS, V25, P141, DOI 10.1016/j.displa.2004.09.002
NR 45
TC 3
Z9 3
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2301
EP 2313
DI 10.1007/s00371-020-01987-2
EA NOV 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA TT5FX
UT WOS:000587645300001
DA 2024-07-18
ER

PT J
AU Alhasson, HF
   Willcocks, CG
   Alharbi, SS
   Kasim, A
   Obara, B
AF Alhasson, Haifa F.
   Willcocks, Chris G.
   Alharbi, Shuaa S.
   Kasim, Adetayo
   Obara, Boguslaw
TI The relationship between curvilinear structure enhancement and ridge
   detection methods
SO VISUAL COMPUTER
LA English
DT Article
DE Curvilinear structures; Ridge detection; Curvilinear enhancement;
   Skeletonisation; Object detection; Image analysis
ID MODIFIED HAUSDORFF DISTANCE; FAST PARALLEL ALGORITHM; MATHEMATICAL
   MORPHOLOGY; ORIENTATION RESPONSES; VESSEL SEGMENTATION; IMAGE-ANALYSIS;
   DIGITAL IMAGE; SKELETON; 3D; EXTRACTION
AB Curvilinear structure detection and quantification is a large research area with many imaging applications in fields such as biology, medicine, and engineering. Curvilinear enhancement is often used as a pre-processing stage for ridge detection, but there has been little investigation into the relationship between enhancement and ridge detection. In this paper, we thoroughly evaluate the pair-wise combinations of different curvilinear enhancement and ridge detection methods across two highly varied datasets, as well as samples of three other datasets. In particular, we present the approaches complementing one another and the gained insights, which will aid researchers in designing generic ridge detectors.
C1 [Alhasson, Haifa F.; Willcocks, Chris G.; Alharbi, Shuaa S.; Obara, Boguslaw] Univ Durham, Dept Comp Sci, Durham, England.
   [Kasim, Adetayo] Univ Durham, Dept Anthropol, Durham, England.
   [Alhasson, Haifa F.; Alharbi, Shuaa S.] Qassim Univ, Dept Informat Technol, Coll Comp, Buraydah, Saudi Arabia.
C3 Durham University; Durham University; Qassim University
RP Obara, B (corresponding author), Univ Durham, Dept Comp Sci, Durham, England.
EM boguslaw.obara@durham.ac.uk
RI alharbi, shuaa s/ABE-3036-2020; Willcocks, Chris G/F-9253-2015
OI Willcocks, Chris G/0000-0001-6821-3924; Alhasson, Haifa
   F./0000-0002-6503-2826
FU Saudi Arabian Ministry of Education Doctoral Scholarship; Qassim
   University in Saudi Arabia
FX Haifa Alhasson and Shuaa Alharbi are supported by the Saudi Arabian
   Ministry of Education Doctoral Scholarship and Qassim University in
   Saudi Arabia.
CR Alharbi SS, 2019, SIGNAL IMAGE VIDEO P, V13, P941, DOI 10.1007/s11760-019-01431-6
   Annunziata R, 2015, LECT NOTES COMPUT SC, V9351, P588, DOI 10.1007/978-3-319-24574-4_70
   [Anonymous], 2003, DICTA, DOI [10.1177/0734242X0302100404, DOI 10.1177/0734242X0302100404]
   [Anonymous], 1992, Nieuw Arch. Wiskd.
   [Anonymous], 2006, IEEE INT S VOR DIAGR
   ARCELLI C, 1988, COMPUT VISION GRAPH, V43, P361, DOI 10.1016/0734-189X(88)90089-8
   Aylward SR, 2003, INT J COMPUT VISION, V55, P123, DOI 10.1023/A:1026126900358
   Bankhead P, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0032435
   Bas E, 2011, NEUROINFORMATICS, V9, P181, DOI 10.1007/s12021-011-9105-2
   Ben Boudaoud L, 2015, 3RD INTERNATIONAL CONFERENCE ON CONTROL, ENGINEERING & INFORMATION TECHNOLOGY (CEIT 2015)
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x
   BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0
   Borgefors G., 1993, Proceedings of the 8th Scandinavian Conference on Image Analysis, P105
   Budai A, 2013, INT J BIOMED IMAGING, V2013, DOI 10.1155/2013/154860
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1522, DOI 10.1109/83.862630
   Chatzis V, 2000, IEEE T IMAGE PROCESS, V9, P1798, DOI 10.1109/83.869190
   CHEN YS, 1988, PATTERN RECOGN LETT, V7, P99, DOI 10.1016/0167-8655(88)90124-9
   Chung DH, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P927, DOI 10.1109/ICIP.2000.899868
   CONN AR, 1991, SIAM J NUMER ANAL, V28, P545, DOI 10.1137/0728030
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   Corson F, 2008, THESIS
   Dash J, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND NETWORKS (CINE), P166, DOI 10.1109/CINE.2015.39
   Do Carmo Manfredo P, 2016, Dover Books on Mathematics, Vsecond
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   FREEMAN WT, 1990, THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION, P406
   Goldber D. E., 1988, Machine Learning, V3, P95, DOI 10.1023/A:1022602019183
   Grun G, 2012, DEV VERTEBRATE RETIN
   Guangliang Cheng, 2017, IEEE Transactions on Geoscience and Remote Sensing, V55, P3322, DOI 10.1109/TGRS.2017.2669341
   HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941
   Hassouna MS, 2009, IEEE T PATTERN ANAL, V31, P2257, DOI 10.1109/TPAMI.2008.271
   Hassouna MS, 2005, PROC CVPR IEEE, P458
   Hesselink WH, 2008, IEEE T PATTERN ANAL, V30, P2204, DOI 10.1109/TPAMI.2008.21
   Holm S, 2017, J MED IMAGING, V4, DOI 10.1117/1.JMI.4.1.014503
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Jacob M, 2004, IEEE T PATTERN ANAL, V26, P1007, DOI 10.1109/TPAMI.2004.44
   Jerman T, 2016, IEEE T MED IMAGING, V35, P2107, DOI 10.1109/TMI.2016.2550102
   Jiang XY, 2003, IEEE T PATTERN ANAL, V25, P131, DOI 10.1109/TPAMI.2003.1159954
   Joshi P, 2020, VISUAL COMPUT, V36, P71, DOI 10.1007/s00371-018-1587-6
   KLETTE R, 1987, IMAGE VISION COMPUT, V5, P287, DOI 10.1016/0262-8856(87)90005-9
   Kovesi P., 1999, Videre, V1
   Kwon OK, 2001, PATTERN RECOGN, V34, P2005, DOI 10.1016/S0031-3203(00)00132-1
   Lin KH, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P477, DOI 10.1109/ISIMP.2001.925437
   Lindeberg T, 1996, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.1996.517113
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Ling Y, 2012, VISUAL COMPUT, V28, P733, DOI 10.1007/s00371-012-0691-2
   López AM, 1999, IEEE T PATTERN ANAL, V21, P327, DOI 10.1109/34.761263
   Lopez-Molina C, 2015, SIGNAL PROCESS, V116, P55, DOI 10.1016/j.sigpro.2015.03.024
   Lu Y, 2001, PROC INT CONF DOC, P921, DOI 10.1109/ICDAR.2001.953920
   Maio D, 1997, IEEE T PATTERN ANAL, V19, P27, DOI 10.1109/34.566808
   MARAGOS PA, 1986, IEEE T ACOUST SPEECH, V34, P1228, DOI 10.1109/TASSP.1986.1164959
   McCall R.B., 1975, TECH REP
   Meijering E, 2004, CYTOM PART A, V58A, P167, DOI 10.1002/cyto.a.20022
   Mendonça AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955
   Merveille O, 2018, IEEE T PATTERN ANAL, V40, P304, DOI 10.1109/TPAMI.2017.2672972
   Merveille O, 2017, IMAGE PROCESS ON LIN, V7, P246, DOI 10.5201/ipol.2017.207
   Miri Mohammad Saleh, 2009, 2009 IEEE International Conference on Signal and Image Processing Applications (ICSIPA 2009), P90, DOI 10.1109/ICSIPA.2009.5478726
   Mount DM, 1999, PATTERN RECOGN, V32, P17, DOI 10.1016/S0031-3203(98)00086-7
   Niemeijer M, 2004, PROC SPIE, V5370, P648, DOI 10.1117/12.535349
   Nixon Mark S, 2012, FEATURE EXTRACTION I, DOI DOI 10.1016/B978-0-12-396549-3.00007-0
   Obara B, 2012, IEEE T IMAGE PROCESS, V21, P2572, DOI 10.1109/TIP.2012.2185938
   Olson CF, 1997, IEEE T IMAGE PROCESS, V6, P103, DOI 10.1109/83.552100
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   PERONA P, 1992, IMAGE VISION COMPUT, V10, P663, DOI 10.1016/0262-8856(92)90011-Q
   Piuze E, 2011, COMPUT GRAPH FORUM, V30, P247, DOI 10.1111/j.1467-8659.2011.01856.x
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Saha PK, 2016, PATTERN RECOGN LETT, V76, P3, DOI 10.1016/j.patrec.2015.04.006
   Sarangi P.P., 2017, P INT C COMPUTER VIS, P229, DOI [10.1007/978-981-10-2107-7_21, DOI 10.1007/978-981-10-2107-7_21]
   Sato Y, 1997, LECT NOTES COMPUT SC, V1205, P213, DOI 10.1007/BFb0029240
   Sazak Ç, 2019, PATTERN RECOGN, V88, P739, DOI 10.1016/j.patcog.2018.10.011
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Sheet D, 2010, IEEE T CONSUM ELECTR, V56, P2475, DOI 10.1109/TCE.2010.5681130
   Shen W, 2017, IEEE T IMAGE PROCESS, V26, P5298, DOI 10.1109/TIP.2017.2735182
   Shen W, 2016, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2016.31
   Shen W, 2016, PATTERN RECOGN, V52, P306, DOI 10.1016/j.patcog.2015.10.015
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Shui PL, 2012, PATTERN RECOGN, V45, P806, DOI 10.1016/j.patcog.2011.07.020
   Siddiqi K, 1999, INT J COMPUT VISION, V35, P13, DOI 10.1023/A:1008102926703
   Siddiqi K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P828, DOI 10.1109/ICCV.1999.790307
   Sironi A, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2462363
   Sironi A, 2015, IEEE I CONF COMP VIS, P316, DOI 10.1109/ICCV.2015.44
   Sluimer I, 2006, IEEE T MED IMAGING, V25, P385, DOI 10.1109/TMI.2005.862753
   Smistad E, 2012, THESIS
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930
   Stosic T, 2006, IEEE T MED IMAGING, V25, P1101, DOI 10.1109/TMI.2006.879316
   Strokina N, 2013, LECT NOTES COMPUT SC, V7944, P22
   Takacs B, 1998, PATTERN RECOGN, V31, P1873, DOI 10.1016/S0031-3203(98)00076-4
   Vala Hetal J., 2013, INT J ADV RES COMPUT, V2, P387, DOI DOI 10.1007/S11548-009-0389-8
   Van Wijk J. J., 2002, P VISSYM
   Vincent L, 1993, IEEE T IMAGE PROCESS, V2, P176, DOI 10.1109/83.217222
   Wade L, 2002, VISUAL COMPUT, V18, P97, DOI 10.1007/s003710100139
   WANG DCC, 1983, COMPUT VISION GRAPH, V24, P363, DOI 10.1016/0734-189X(83)90061-0
   Wang G, 2020, COMPUT ELECTRON AGR, V168, DOI 10.1016/j.compag.2019.105102
   Willcocks CG, 2017, IEEE T PATTERN ANAL, V39, P1757, DOI 10.1109/TPAMI.2016.2613866
   Willcocks CG, 2012, VISUAL COMPUT, V28, P775, DOI 10.1007/s00371-012-0688-x
   Yim PJ, 2000, IEEE T MED IMAGING, V19, P568, DOI 10.1109/42.870662
   Yu CB, 2009, INTERDISCIP SCI, V1, P280, DOI 10.1007/s12539-009-0046-5
   Zana F, 2001, IEEE T IMAGE PROCESS, V10, P1010, DOI 10.1109/83.931095
   ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023
   Zhao CJ, 2005, PATTERN RECOGN LETT, V26, P581, DOI 10.1016/j.patrec.2004.09.022
   Zhou Y, 1998, VISUAL COMPUT, V14, P303, DOI 10.1007/s003710050142
   Zhu H, 1999, COMPUT VIS IMAGE UND, V73, P281, DOI 10.1006/cviu.1998.0723
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 106
TC 2
Z9 3
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2263
EP 2283
DI 10.1007/s00371-020-01985-4
EA OCT 2020
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000582644100001
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Villegas-Cortez, J
   Benavides-Alvarez, C
   Avilés-Cruz, C
   Román-Alonso, G
   de Vega, FF
   Chávez, F
   Cordero-Sánchez, S'
AF Villegas-Cortez, Juan
   Benavides-Alvarez, Cesar
   Aviles-Cruz, Carlos
   Roman-Alonso, Graciela
   de Vega, Francisco Fernandez
   Chavez, Francisco
   Cordero-Sanchez, Salomon
TI Interest points reduction using evolutionary algorithms and CBIR for
   face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-objective; Face recognition; Parallel algorithms; CBIR; Genetic
   algorithm
ID IMAGE OPERATORS; ENVIRONMENT; DESIGN
AB Face recognition has become a fundamental biometric tool that ensures identification of people. Besides a high computational cost, it constitutes an open problem for identifying faces under ideal conditions as well as those under general conditions. Though the advent of high memory and inexpensive computer technologies has made the implementation of face recognition possible in several devices and authentication systems, achieving 100% face recognition in real time is still a challenging task. This paper implements an evolutionary computer genetic algorithm for optimizing the number of interest points on faces, intended to get a quick and precise facial recognition using local analysis texture technique applied to CBIR methodology. Our approach was evaluated using different databases, getting an efficient facial recognition of up to 100% considering only seven interest points from a total of 54 cited in the literature. The interest points reduction was possible through a parallel implementation of our approach using a 54-processor cluster that executes the similar task up to 300% more faster.
C1 [Villegas-Cortez, Juan] Univ Autonoma Metropolitana, Dept Sistemas, Av San Pablo Xalpa 180, Mexico City 02200, DF, Mexico.
   [Benavides-Alvarez, Cesar; Aviles-Cruz, Carlos] Univ Autonoma Metropolitana, Dept Elect, Av San Pablo Xalpa 180, Mexico City 02200, DF, Mexico.
   [Roman-Alonso, Graciela] Univ Autonoma Metropolitana, Dept Ingn Elect, San Rafael Atlixco 186, Mexico City 09340, DF, Mexico.
   [de Vega, Francisco Fernandez; Chavez, Francisco] Univ Extremadura, Dept Comp Sci, C Santa Teresa Jornet 38, Merida 06800, Spain.
   [Cordero-Sanchez, Salomon] Univ Autonoma Metropolitana, Dept Quim, San Rafael Atlixco 186, Mexico City 09340, DF, Mexico.
C3 Universidad Autonoma Metropolitana - Mexico; Universidad Autonoma
   Metropolitana - Mexico; Universidad Autonoma Metropolitana - Mexico;
   Universidad de Extremadura; Universidad Autonoma Metropolitana - Mexico
RP Benavides-Alvarez, C (corresponding author), Univ Autonoma Metropolitana, Dept Elect, Av San Pablo Xalpa 180, Mexico City 02200, DF, Mexico.
EM juanvc@azc.uam.mx; caviles@azc.uam.mx
RI de la O, Francisco Chavez/H-3971-2011; Cortez, Juan
   Villegas/O-8143-2019; Cordero Sanchez, Salomon/A-6137-2018; Villegas
   Cortez, Juan/F-6421-2010
OI Cordero Sanchez, Salomon/0000-0002-7366-089X; Villegas Cortez,
   Juan/0000-0001-8918-1044
FU Fundacion Carolina, Spain; Grant "Evolucion de descriptores estadisticos
   de textura de superficie para implementacion en clasificacion de
   imagenes digitales" [UAM-CBI EL006-18]; European Regional Development
   Fund (FEDER) [TIN2014-56494-C4-4-P, TIN2017-85727-C4-4-P]; Junta de
   Extremadura Project [IB16035]; CONACyT
FX This work has been supported by Fundacion Carolina, Spain, under the
   scholarship program 2016-2017. This work has been developed under Grant
   "Evolucion de descriptores estadisticos de textura de superficie para
   implementacion en clasificacion de imagenes digitales," UAM-CBI
   EL006-18. The authors would like to thank Spanish Ministry of Economy,
   Industry and Competitiveness and European Regional Development Fund
   (FEDER) under Projects TIN2014-56494-C4-4-P (Ephemec) and
   TIN2017-85727-C4-4-P (DeepBio); Junta de Extremadura Project IB16035
   Regional Government of Extremadura, Consejeria of Economy and
   Infrastructure, FEDER. Cesar Benavides thanks the CONACyT for the
   scholarship support.
CR [Anonymous], 2018, ABS180406655 CORR
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Ben Fredj H, 2020, J APPL MATH COMPUT, V63, P479, DOI 10.1007/s12190-020-01326-7
   Benavente R, 1998, 24 COMP VIS CTR
   Benavides C, 2016, IEEE LAT AM T, V14, P2418, DOI 10.1109/TLA.2016.7530440
   Benavides C, 2015, P MAEB 2015 MER SPAI, P733
   Chaudhry S, 2017, APPL SOFT COMPUT, V53, P168, DOI 10.1016/j.asoc.2016.12.035
   Chávez F, 2018, INT J HIGH PERFORM C, V32, P706, DOI 10.1177/1094342016678302
   Chávez F, 2016, LECT NOTES COMPUT SC, V9598, P91, DOI 10.1007/978-3-319-31153-1_7
   Clemente E, 2015, APPL SOFT COMPUT, V32, P250, DOI 10.1016/j.asoc.2015.03.011
   Eiben A.E., 2015, NAT COMP SER, P223, DOI 10.1007/978-3-662-44874-8_15
   Ekenel HK, 2009, LECT NOTES COMPUT SC, V5558, P299, DOI 10.1007/978-3-642-01793-3_31
   Serrano-Talamantes JF, 2013, EXPERT SYST APPL, V40, P2398, DOI 10.1016/j.eswa.2012.10.064
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Krisshna NLA, 2014, APPL SOFT COMPUT, V22, P141, DOI 10.1016/j.asoc.2014.05.007
   Li H, 2014, ANAL FACE RECOGNITIO, P3045
   Liu C., COMMUNICATIONS COMPU
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Milborrow S., 2010, Pattern Recognition Association of South Africa, V201, P1
   Milborrow S, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P380
   Olague G, 2011, IMAGE VISION COMPUT, V29, P484, DOI 10.1016/j.imavis.2011.03.004
   Perez CB, 2013, INTELL DATA ANAL, V17, P561, DOI 10.3233/IDA-130594
   Raghuwanshi G., 2012, INT J COMPUT APPL, V43, P8, DOI DOI 10.5120/6186-8665
   Ramakrishnan S, 2016, FACE RECOGNITION SEM, DOI 10.5772/61471
   Sengupta S, 2016, IEEE WINT CONF APPL
   Srinivasan A, 2014, NOVEL APPROACH FACIA, P155
   Sumana IJ, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P11, DOI 10.1109/MMSP.2008.4665041
   Torres RD, 2009, PATTERN RECOGN, V42, P283, DOI 10.1016/j.patcog.2008.04.010
   Trujillo L, 2008, EVOL COMPUT, V16, P483, DOI 10.1162/evco.2008.16.4.483
   Ugail H, 2018, VISUAL COMPUT, V34, P1243, DOI 10.1007/s00371-018-1494-x
   Zhao TZ, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P495, DOI 10.1109/CISP.2008.90
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 36
TC 3
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1883
EP 1897
DI 10.1007/s00371-020-01949-8
EA SEP 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000570982200001
DA 2024-07-18
ER

PT J
AU Meng, XY
   Zhang, GL
   Jia, SM
   Li, XZ
   Zhang, XY
AF Meng, Xiaoyan
   Zhang, Guoliang
   Jia, Songmin
   Li, Xiuzhi
   Zhang, Xiangyin
TI Auxiliary criterion conversion via spatiotemporal semantic encoding and
   feature entropy for action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Spatiotemporal semantic feature; Feature entropy;
   Bag-of-visual-words model; Text-based relevance analysis
ID DESCRIPTORS
AB Video-based action recognition in realistic scenes is a core technology for human-computer interaction and smart surveillance. Although the trajectory features with the bag of visual words have confirmed promising performance, spatiotemporal interactive information cannot be effectively encoded which is valuable for classification. To address this issue, we propose a spatiotemporal semantic feature (ST-SF) and implement the conversion of it to the auxiliary criterion based on the information entropy theory. First, we present a text-based relevance analysis method to estimate the textual labels of objects most relevant to actions, which are employed to train the more targeted detectors based on the deep network. False detections are optimized by the inter-frame cooperativity and dynamic programming to construct the valid tubes. Then, we design the ST-SF to encode the interactive information, and the concept and calculation of feature entropy are defined based on the spatial distribution of ST-SFs on the training set. Finally, we achieve a two-stage classification strategy using the resulting decision gains. Experimental results on three publicly available datasets demonstrate that our method is robust and improves upon the state-of-the-art algorithms.
C1 [Meng, Xiaoyan; Zhang, Guoliang; Jia, Songmin; Li, Xiuzhi; Zhang, Xiangyin] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Meng, Xiaoyan; Zhang, Guoliang; Jia, Songmin; Li, Xiuzhi; Zhang, Xiangyin] Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Meng, Xiaoyan; Zhang, Guoliang; Jia, Songmin; Li, Xiuzhi] Minist Educ, Engn Res Ctr Digital Community, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Zhang, GL (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.; Zhang, GL (corresponding author), Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.; Zhang, GL (corresponding author), Minist Educ, Engn Res Ctr Digital Community, Beijing 100124, Peoples R China.
EM zhangglmxy@foxmail.com
RI li, xiu/GXV-1745-2022
FU BJUT United Grand Scientific Research Program on Intelligent
   Manufacturing [040000546317552]; National Natural Science Foundation of
   China [61175087, 61703012]
FX This research is financially supported by the 2017 BJUT United Grand
   Scientific Research Program on Intelligent Manufacturing (No.
   040000546317552) and the National Natural Science Foundation of China
   (Nos. 61175087, 61703012).
CR [Anonymous], 2017, IEEE C COMP VIS PATT
   Biswas P, 2016, ABAC'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL WORKSHOP ON ATTRIBUTE BASED ACCESS CONTROL, P1, DOI 10.1145/2875491.2875498
   Cao C., 2016, INT JOINT C ART INT, V1, P3
   Cao CQ, 2018, IEEE T CYBERNETICS, V48, P1095, DOI 10.1109/TCYB.2017.2756840
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eum S., 2018, IEEE INT C IM PROC, P1
   Gammulle H, 2017, IEEE WINT CONF APPL, P177, DOI 10.1109/WACV.2017.27
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Iqbal U, 2017, IEEE INT CONF AUTOMA, P438, DOI 10.1109/FG.2017.61
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Jung HJ, 2017, PATTERN RECOGN LETT, V85, P21, DOI 10.1016/j.patrec.2016.11.012
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Jingen Liu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1996, DOI [10.1109/ICINIS.2009.13, 10.1109/CVPRW.2009.5206744]
   Ma L, 2015, IEEE I CONF COMP VIS, P3128, DOI 10.1109/ICCV.2015.358
   Ma M, 2018, PATTERN RECOGN, V76, P506, DOI 10.1016/j.patcog.2017.11.026
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Pan Y, 2019, AAAI CONF ARTIF INTE, P4683
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Prest A, 2013, IEEE T PATTERN ANAL, V35, P835, DOI 10.1109/TPAMI.2012.175
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   SHANNON CE, 1951, BELL SYST TECH J, V30, P50, DOI 10.1002/j.1538-7305.1951.tb01366.x
   Shao L, 2016, INT J COMPUT VISION, V118, P115, DOI 10.1007/s11263-015-0861-6
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang H., 2009, BMVC
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang L., 2017, ARXIV160508140
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yi Y, 2017, EXPERT SYST APPL, V75, P44, DOI 10.1016/j.eswa.2017.01.008
   Yi Y, 2018, VISUAL COMPUT, V34, P391, DOI 10.1007/s00371-016-1345-6
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zhang GL, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.1.013021
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 52
TC 2
Z9 2
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1673
EP 1690
DI 10.1007/s00371-020-01931-4
EA AUG 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000554423100002
DA 2024-07-18
ER

PT J
AU Ataee, Z
   Mohseni, H
AF Ataee, Zivar
   Mohseni, Hadis
TI Structured dictionary learning using mixed-norms and group-sparsity
   constraint
SO VISUAL COMPUTER
LA English
DT Article
DE Supervised dictionary learning; Sparse representation; Structured
   sparsity; Mixed norms; Classification
ID DISCRIMINATIVE DICTIONARY; K-SVD; FACE RECOGNITION; REPRESENTATION
AB Recently, sparse representation and dictionary learning have shown significant performance in machine vision. In particular, several supervised dictionary learning methods have been proposed for classification aim and increasing its accuracy. Among them, structured dictionary learning is an interesting approach which captures the discriminative properties of each class and common features among all classes in class-specific sub-dictionaries and a distinct shared sub-dictionary, respectively. It extracts the structural information that exists in samples of each class to increase the classification accuracy. Therefore, in this paper, a group-based structured dictionary learning method is proposed that captures structural information in each class and learns class-specific and shared sub-dictionaries based on mixed l2,1 norm. Also, mixed l2,1 norm is used for acquiring the sparse coefficients of data samples based on the learned sub-dictionaries. Then, classification is done by finding the class with (1) minimum reconstruction error or (2) maximum number of nonzero groups based on l1,0 norm. The proposed method is evaluated by conducting experiments on Extended YaleB, AR and CMU-PIE face databases and the USPS handwritten digits database. The experimental results demonstrate the effectiveness of the proposed method in data representation and classification.
C1 [Ataee, Zivar; Mohseni, Hadis] Shahid Bahonar Univ Kerman, Dept Comp Engn, Kerman, Iran.
C3 Shahid Bahonar University of Kerman (SBUK)
RP Mohseni, H (corresponding author), Shahid Bahonar Univ Kerman, Dept Comp Engn, Kerman, Iran.
EM ataeezivar2015@eng.uk.ac.ir; hmohseni@uk.ac.ir
RI Mohseni, Hadis/IYJ-3609-2023
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Akhtar N, 2017, PROC CVPR IEEE, P3919, DOI 10.1109/CVPR.2017.417
   Bengio S., P 22 INT C NEUR INF
   Bryt O, 2008, J VIS COMMUN IMAGE R, V19, P270, DOI 10.1016/j.jvcir.2008.03.001
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Chi Y.T., 2013, IEEE C COMP VIS PATT
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Elhamifar E., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1873, DOI 10.1109/CVPR.2011.5995664
   Fan BJ, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.013007
   Fan CD, 2019, VISUAL COMPUT, V35, P1797, DOI 10.1007/s00371-018-1603-x
   Foroughi H, 2018, IEEE T IMAGE PROCESS, V27, P806, DOI 10.1109/TIP.2017.2766446
   Gu GH, 2016, ARTIF INTELL REV, V46, P431, DOI 10.1007/s10462-016-9470-1
   Huang JZ, 2011, J MACH LEARN RES, V12, P3371
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jacob L., 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Le Pennec E, 2005, IEEE T IMAGE PROCESS, V14, P423, DOI 10.1109/TIP.2005.843753
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Liu HD, 2014, PATTERN RECOGN, V47, P1835, DOI 10.1016/j.patcog.2013.11.007
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4042, DOI 10.1109/TIP.2017.2713940
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Martinez A. M., 1998, THE AR FACE DATABASE
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964
   Sim T., 2002, CMU POSE ILLUMINATIO, V4, P53
   Sprechmann P, 2010, INT CONF ACOUST SPEE, P2042, DOI 10.1109/ICASSP.2010.5494985
   Sun YB, 2014, IEEE T IMAGE PROCESS, V23, P3816, DOI 10.1109/TIP.2014.2331760
   Sun YP, 2018, NEURAL COMPUT APPL, V30, P1265, DOI 10.1007/s00521-016-2764-z
   Suo YM, 2014, IEEE IMAGE PROC, P150, DOI 10.1109/ICIP.2014.7025029
   Wang CP, 2018, APPL INTELL, V48, P156, DOI 10.1007/s10489-017-0956-6
   Wang DH, 2014, PATTERN RECOGN, V47, P885, DOI 10.1016/j.patcog.2013.08.004
   Wang XD, 2020, IEEE T CIRC SYST VID, V30, P59, DOI 10.1109/TCSVT.2018.2886600
   Wen ZD, 2017, IEEE T CYBERNETICS, V47, P3758, DOI 10.1109/TCYB.2016.2581861
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Xu Y, 2017, IEEE ACCESS, V5, P8502, DOI 10.1109/ACCESS.2017.2695239
   Xu Y, 2015, COMPUT VIS IMAGE UND, V136, P59, DOI 10.1016/j.cviu.2015.01.006
   Yang M, 2014, PROC CVPR IEEE, P4138, DOI 10.1109/CVPR.2014.527
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Zhang Q.Z.Q., 2010, PROC CVPR IEEE, DOI [10.1109/CVPR.2010.5539989, DOI 10.1109/CVPR.2010.5539989]
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zhao L, 2016, VISUAL COMPUT, V32, P1165, DOI 10.1007/s00371-015-1169-9
NR 46
TC 1
Z9 1
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1679
EP 1692
DI 10.1007/s00371-019-01766-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700008
DA 2024-07-18
ER

PT J
AU Chen, YT
   Liu, LW
   Tao, JJ
   Xia, RL
   Zhang, Q
   Yang, K
   Xiong, J
   Chen, X
AF Chen, Yuantao
   Liu, Linwu
   Tao, Jiajun
   Xia, Runlong
   Zhang, Qian
   Yang, Kai
   Xiong, Jie
   Chen, Xi
TI The improved image inpainting algorithm via encoder and similarity
   constraint
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Image inpainting; Residual network;
   Contextual loss function
ID SUPERRESOLUTION
AB Existing image inpainting algorithms based on neural network models are affected by structural distortions and blurred textures on visible connectivity. As a result, overfitting and overlearning phenomena can easily emerge during the image inpainting procedure. Image inpainting refers to the repairing of missing parts of an image, given an image that is broken or incomplete. After the repairing operation is complete, there are obvious signs of repair in damaged areas, semantic discontinuities, and unclearness. This paper proposes an improved image inpainting method based on a new encoder combined with a context loss function. In order to obtain clear repaired images and ensure that the semantic features of images are fully learned, a generative network based on the fusion model of squeeze-and-excitation networks deep residual learning has been proposed to improve the application of network features in order to obtain clear images and reduce network parameters. At the same time, a discriminative network based on the squeeze-and-excitation residual Network has been proposed to strengthen the capability of the discriminative network. In order to make the generated image more realistic, so that the restored image will be more similar to the original image, a joint context-awareness loss training method (contextual perception loss network) has also been proposed to generate the similarity of the local features of the network constraint, with the result that the repaired image is closer to the original picture and more realistic. The experimental results can demonstrate that the proposed algorithm demonstrates better adaptive capability than the comparison algorithms on a number of image categories. In addition, the processing results of the image inpainting procedure were also superior to those of five state-of-the-art algorithms.
C1 [Chen, Yuantao; Liu, Linwu; Tao, Jiajun; Chen, Xi] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.
   [Chen, Yuantao; Liu, Linwu; Tao, Jiajun; Chen, Xi] Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Tran, Changsha 410114, Hunan, Peoples R China.
   [Xia, Runlong] Hunan Inst Sci & Tech Informat, Changsha 411105, Hunan, Peoples R China.
   [Zhang, Qian; Yang, Kai] Hunan ZOOMLION Intelligent Technol Corp Ltd, Dept Elect Prod, Changsha 410005, Hunan, Peoples R China.
   [Xiong, Jie] Yangtze Univ, Elect & Informat Sch, Jingzhou 434023, Peoples R China.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology; Yangtze University
RP Chen, YT (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.; Chen, YT (corresponding author), Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Tran, Changsha 410114, Hunan, Peoples R China.
EM chenyt@csust.edu.cn; liulinwu@stu.csust.edu.cn;
   taojiajun@stu.csust.edu.cn; xiarunlong@vip.qq.com;
   zhangqian@zoomlion.com; yangkai@zoomlion.com; xiongjie@yangtzeu.edu.cn;
   chentianjun@163.com
RI Chen, Yuantao/AAC-7165-2019; Tao, Jiajun/ISA-2359-2023
OI Chen, Yuantao/0000-0003-2277-1765; Tao, Jiajun/0000-0002-3201-3004
FU National Natural Science Foundation of China [61972056, 61772454,
   61402053, 61981340416]; Natural Science Foundation of Hunan Province of
   China [2020JJ4623]; Scientific Research Fund of Hunan Provincial
   Education Department [17A007, 19C0028, 19B005]; Changsha Science and
   Technology Planning [KQ1703018, KQ1706064, KQ1703018-01, KQ1703018-04];
   Junior Faculty Development Program Project of Changsha University of
   Science and Technology [2019QJCZ011]; "Double First-class" International
   Cooperation and Development Scientific Research Project of Changsha
   University of Science and Technology [2019IC34]; Practical Innovation
   and Entrepreneurship Ability Improvement Plan for Professional Degree
   Postgraduate of Changsha University of Science and Technology
   [SJCX202072]; Postgraduate Training Innovation Base Construction Project
   of Hunan Province [2019-248-51]
FX This study was funded by the National Natural Science Foundation of
   China [61972056, 61772454, 61402053, 61981340416], the Natural Science
   Foundation of Hunan Province of China [2020JJ4623], the Scientific
   Research Fund of Hunan Provincial Education Department [17A007, 19C0028,
   19B005], the Changsha Science and Technology Planning [KQ1703018,
   KQ1706064, KQ1703018-01, KQ1703018-04], the Junior Faculty Development
   Program Project of Changsha University of Science and Technology
   [2019QJCZ011], the "Double First-class" International Cooperation and
   Development Scientific Research Project of Changsha University of
   Science and Technology [2019IC34], the Practical Innovation and
   Entrepreneurship Ability Improvement Plan for Professional Degree
   Postgraduate of Changsha University of Science and Technology
   [SJCX202072], and the Postgraduate Training Innovation Base Construction
   Project of Hunan Province [2019-248-51].
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Altantawy DA, 2020, VISUAL COMPUT, V36, P333, DOI 10.1007/s00371-018-1611-x
   [Anonymous], 2015, ADV NEURAL INFORM PR
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Beckouche S, 2013, ASTRON ASTROPHYS, V556, DOI 10.1051/0004-6361/201220752
   Chen YB, 2020, J CONTEMP CHINA, V29, P1, DOI [10.1080/10670564.2019.1621526, 10.1080/01932691.2020.1791172, 10.1007/s12652-020-02066-z]
   Chen YT, 2020, WIREL COMMUN MOB COM, V2020, DOI 10.1155/2020/8822777
   Chen YT, 2019, CLUSTER COMPUT, V22, pS7435, DOI 10.1007/s10586-018-1772-4
   Chen YT, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.5533
   Chen YT, 2019, J AMB INTEL HUM COMP, V10, P4855, DOI 10.1007/s12652-018-01171-4
   Chen YT, 2019, IEEE ACCESS, V7, P58791, DOI 10.1109/ACCESS.2019.2911892
   Chen YT, 2019, CLUSTER COMPUT, V22, pS7665, DOI 10.1007/s10586-018-2368-8
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Gu K, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030758
   HaCohen Y., 2010, ICCP, P1
   Haouchine N, 2020, VISUAL COMPUT, V36, P211, DOI 10.1007/s00371-018-1600-0
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu GL, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P389, DOI 10.1109/BigMM.2017.43
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Li WJ, 2020, IEEE INTERNET THINGS, V7, P5882, DOI 10.1109/JIOT.2019.2949352
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liao N, 2020, J INTELL FUZZY SYST
   Liao ZF, 2020, IEEE ACCESS, V8, P90630, DOI 10.1109/ACCESS.2020.2994328
   Liu BW, 2019, VISUAL COMPUT, V35, P85, DOI 10.1007/s00371-017-1454-x
   Liu Y, 2020, VISUAL COMPUT, V36, P827, DOI 10.1007/s00371-019-01656-z
   Liu Yuan, 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2017.2785278
   Liu Y, 2020, NEUROCOMPUTING, V406, P106, DOI 10.1016/j.neucom.2020.04.017
   Luo YJ, 2020, J REAL-TIME IMAGE PR, V17, P125, DOI 10.1007/s11554-019-00917-3
   Mikaeli E, 2020, VISUAL COMPUT, V36, P1573, DOI 10.1007/s00371-019-01756-w
   Naderahmadian Y, 2016, IEEE T SIGNAL PROCES, V64, P592, DOI 10.1109/TSP.2015.2486743
   Nie G., 2019, 2019 IEEE INT C COMP, P3283
   Pan N, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00512-8
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sheng GQ, 2019, J SEISM EXPLOR, V28, P593
   Wang C, 2018, VISUAL COMPUT, V34, P67, DOI 10.1007/s00371-016-1312-2
   Wang J, 2019, MATH BIOSCI ENG, V16, P5851, DOI 10.3934/mbe.2019292
   Yang CW, 2019, VISUAL COMPUT, V35, P695, DOI 10.1007/s00371-018-1504-z
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang H, 2020, VISUAL COMPUT, V36, P1411, DOI 10.1007/s00371-019-01748-w
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yin B, 2020, IEEE T IND INFORM, V16, P2520, DOI 10.1109/TII.2019.2933534
   Yu F, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/8034196
   Yu F, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/5212601
   Yu F, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/5859273
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang JM, 2020, IEEE ACCESS, V8, P29742, DOI 10.1109/ACCESS.2020.2972338
   Zhang JM, 2020, ANN TELECOMMUN, V75, P369, DOI 10.1007/s12243-019-00731-9
   Zhang JM, 2019, IEEE ACCESS, V7, P83873, DOI 10.1109/ACCESS.2019.2924944
   Zhang JY, 2020, J INTERNET TECHNOL, V21, P1, DOI 10.3966/160792642020012101001
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhou LY, 2020, IEEE ACCESS, V8, P30436, DOI 10.1109/ACCESS.2020.2972269
NR 55
TC 91
Z9 92
U1 8
U2 77
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1691
EP 1705
DI 10.1007/s00371-020-01932-3
EA JUL 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000551707100001
DA 2024-07-18
ER

PT J
AU Yu, K
   Ahn, J
   Lee, J
   Kim, M
   Han, J
AF Yu, Kihyun
   Ahn, JeongHyeon
   Lee, Jongmin
   Kim, Myungho
   Han, JungHyun
TI Collaborative SLAM and AR-guided navigation for floor layout inspection
SO VISUAL COMPUTER
LA English
DT Article
DE SLAM; Augmented reality; Inspection; Discrepancy check
AB This paper presents how visual SLAM and AR interfaces are integrated for inspecting the floor layouts, which is a standard stage in building a semiconductor fabrication plant (commonly called a fab). The proposed system supports multiple inspectors working in a large fab floor through client-server communications. The inspectors/clients scan the fab floor using smartphones to construct the floor map collaboratively, and then, its discrepancy from the given floor plan is checked by the server. The inspection results are broadcast to all clients and visualized via AR to help the inspectors move to unscanned areas. The experiments made with the proposed system show its accuracy and efficiency. By solving a real-world industrial problem, the proposed system proves the benefits that can be brought by integrating collaborative SLAM technique and AR-guided navigation interface.
C1 [Yu, Kihyun; Ahn, JeongHyeon; Lee, Jongmin; Han, JungHyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Kim, Myungho] Iaan Co Ltd, Bucheon, South Korea.
C3 Korea University
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM blackapple1202@korea.ac.kr; miru3137@korea.ac.kr; leejong32@korea.ac.kr;
   delmaroo@iaansoft.co.kr; jhan@korea.ac.kr
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) - Korea government (MSIT) [2020-0-00861]; National
   Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF-2017M3C4A7066316]
FX This work was supported by Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) (No. 2020-0-00861) and National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIT) (No.
   NRF-2017M3C4A7066316).
CR Barbieri L, 2019, LECT NOTES COMPUT SC, V11614, P99, DOI 10.1007/978-3-030-25999-0_9
   Bruno F, 2019, INT J ADV MANUF TECH, V105, P875, DOI 10.1007/s00170-019-04254-4
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Dünser A, 2012, COMPUT GRAPH-UK, V36, P1084, DOI 10.1016/j.cag.2012.10.001
   Egodagamage R, 2018, COMPUT GRAPH-UK, V71, P113, DOI 10.1016/j.cag.2018.01.002
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Forster C, 2013, IEEE INT C INT ROBOT, P3963, DOI 10.1109/IROS.2013.6696923
   Georgel P., 2007, P 2007 6 IEEE ACM IN, P1, DOI DOI 10.1109/ISMAR.2007.4538834
   Georgel P, 2009, INT SYM MIX AUGMENT, P187, DOI 10.1109/ISMAR.2009.5336468
   Golodetz S, 2018, IEEE T VIS COMPUT GR, V24, P2895, DOI 10.1109/TVCG.2018.2868533
   Hough P.V., 1962, U.S. Patent, Patent No. 3069654
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Kahn S, 2013, COMPUT IND, V64, P1115, DOI 10.1016/j.compind.2013.04.004
   Koch C, 2014, AUTOMAT CONSTR, V48, P18, DOI 10.1016/j.autcon.2014.08.009
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Makita K., 2016, SERVICEOLOGY DESIGNI, P109
   Marder-Eppstein Eitan, 2016, ACM SIGGRAPH 2016 Real-Time Live! 2016, P25, DOI DOI 10.1145/2933540.2933550
   Mohanarajah G, 2015, IEEE T AUTOM SCI ENG, V12, P423, DOI 10.1109/TASE.2015.2408456
   Mulloni A., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P229, DOI 10.1109/ISMAR.2011.6092390
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Palmarini R, 2018, ROBOT CIM-INT MANUF, V49, P215, DOI 10.1016/j.rcim.2017.06.002
   Perla R, 2016, ADJUNCT PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P355, DOI 10.1109/ISMAR-Adjunct.2016.0119
   Polvi J, 2018, IEEE T VIS COMPUT GR, V24, P2118, DOI 10.1109/TVCG.2017.2709746
   Riazuelo L, 2014, ROBOT AUTON SYST, V62, P401, DOI 10.1016/j.robot.2013.11.007
   Schmuck Patrik, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3863, DOI 10.1109/ICRA.2017.7989445
   Strasdat H., 2010, ROBOT SCI SYST 6, V2, P7
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Takata S, 2004, CIRP ANN-MANUF TECHN, V53, P643, DOI 10.1016/S0007-8506(07)60033-X
   Verizon Media, 2019, GOOGL MAPS AR NAV RO
   Wasenmüller O, 2016, INT SYM MIX AUGMENT, P125, DOI 10.1109/ISMAR.2016.15
   Webel S, 2007, 2007 6 IEEE ACM INT, P281
NR 34
TC 7
Z9 7
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2051
EP 2063
DI 10.1007/s00371-020-01911-8
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000549317900002
OA Bronze
DA 2024-07-18
ER

PT J
AU Srisamosorn, V
   Kuwahara, N
   Yamashita, A
   Ogata, T
   Shirafuji, S
   Ota, J
AF Srisamosorn, Veerachart
   Kuwahara, Noriaki
   Yamashita, Atsushi
   Ogata, Taiki
   Shirafuji, Shouhei
   Ota, Jun
TI Human position and head direction tracking in fisheye camera using
   randomized ferns and fisheye histograms of oriented gradients
SO VISUAL COMPUTER
LA English
DT Article
DE Human tracking; Fisheye camera; Video surveillance; Head direction
   estimation
ID HUMAN ACTIVITY RECOGNITION; MODEL
AB This paper proposes a system for tracking human position and head direction using fisheye camera mounted to the ceiling. This is believed to be the first system to estimate head direction from ceiling-mounted fisheye camera. Fisheye histograms of oriented gradients descriptor is developed as a substitute to the histograms of oriented gradients descriptor which has been widely used for human detection in perspective camera. Human body and head are detected by the proposed descriptor and tracked to extract head area for direction estimation. Direction estimation using randomized ferns is adapted to work with fisheye images by using the proposed descriptor, guided by the direction of movement. With experiments on available dataset and new dataset with ground truth, the direction can be estimated with average error below 40 circle, with head position error half of the head size.
C1 [Srisamosorn, Veerachart; Yamashita, Atsushi] Univ Tokyo, Sch Engn, Dept Precis Engn, Tokyo, Japan.
   [Kuwahara, Noriaki] Kyoto Inst Technol, Grad Sch Sci & Technol, Kyoto, Japan.
   [Ogata, Taiki] Tokyo Inst Technol, Sch Comp, Dept Comp Sci, Tokyo, Japan.
   [Shirafuji, Shouhei; Ota, Jun] Univ Tokyo, Sch Engn, Ctr Engn RACE, Res Artifacts, Tokyo, Japan.
C3 University of Tokyo; Kyoto Institute of Technology; Tokyo Institute of
   Technology; University of Tokyo
RP Srisamosorn, V (corresponding author), Univ Tokyo, Sch Engn, Dept Precis Engn, Tokyo, Japan.
EM veera.sr@race.u-tokyo.ac.jp
RI Jun, Ota/CAG-2441-2022
OI Jun, Ota/0000-0002-4738-2275; Shirafuji, Shouhei/0000-0002-3102-5118
FU JSPS KAKENHI [15H01698]; Grants-in-Aid for Scientific Research
   [15H01698] Funding Source: KAKEN
FX This work was partially supported by JSPS KAKENHI (Grant Number
   15H01698).
CR [Anonymous], J COSMOL ASTROPART P
   [Anonymous], 12 IAPR C MACH VIS A
   [Anonymous], THESIS
   [Anonymous], FERNS PLANAR OBJECT
   [Anonymous], PIROPO DAT
   [Anonymous], 9 WORKSH VIS COMP WC
   Benfold B., 2009, P 20 BRIT MACH VIS C
   Benfold B, 2011, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2011.6126516
   Bensebaa A, 2018, VISUAL COMPUT, V34, P1109, DOI 10.1007/s00371-018-1520-z
   Chamveha I, 2013, COMPUT VIS IMAGE UND, V117, P1502, DOI 10.1016/j.cviu.2013.06.005
   Chen C, 2012, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2012.6247845
   Chiang A.-T., 2014, ICME WORKSH, P1
   Cinaroglu I, 2014, SIG PROCESS COMMUN, P2275, DOI 10.1109/SIU.2014.6830719
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Delibasis KK, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P470
   Demiroz B. E., 2012, 2012_5th_International Symposium_on_Communications,_Control_and_Signal_Processing, P1
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Hulens Dries, 2016, VISIGRAPP 2016. 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. Proceedings: VISAPP 2016, P538
   Jalal A, 2017, PATTERN RECOGN, V61, P295, DOI 10.1016/j.patcog.2016.08.003
   Jalal A, 2013, INDOOR BUILT ENVIRON, V22, P271, DOI 10.1177/1420326X12469714
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153
   Krams O, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Liu BZ, 2017, SIGNAL PROCESS-IMAGE, V54, P1, DOI 10.1016/j.image.2017.02.008
   Meinel L, 2014, I SYMP CONSUM ELECTR, P398
   Özuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Prisacariu V., 2009, fastHOG-a real-time GPU implementation of HOG
   Rehder E, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2292, DOI 10.1109/ITSC.2014.6958057
   Saito Mamoru, 2010, 2010 IEEE International Conference on Systems, Man and Cybernetics (SMC 2010), P243, DOI 10.1109/ICSMC.2010.5642246
   Srisamosorn V, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417727357
   Tang Y, 2011, PROCEEDINGS OF 2011 INTERNATIONAL SYMPOSIUM - ACCOUNTING INFORMATION SYSTEM AND CORPORATE GOVERNANCE, P96
   Tasson D., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P65, DOI 10.1109/CVPRW.2015.7301368
   Nguyen VT, 2016, I C INF COMM TECH CO, P840, DOI 10.1109/ICTC.2016.7763311
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Yan Y, 2016, IEEE T PATTERN ANAL, V38, P1070, DOI 10.1109/TPAMI.2015.2477843
   Yoshimoto H, 2003, PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, P247, DOI 10.1109/MFI-2003.2003.1232665
   Zhou ZN, 2008, IEEE T CIRC SYST VID, V18, P1489, DOI 10.1109/TCSVT.2008.2005612
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 39
TC 8
Z9 8
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1443
EP 1456
DI 10.1007/s00371-019-01749-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000011
DA 2024-07-18
ER

PT J
AU Mohammadi, S
   Maleki, R
AF Mohammadi, Shahram
   Maleki, Reza
TI Air-writing recognition system for Persian numbers with a novel
   classifier
SO VISUAL COMPUTER
LA English
DT Article
DE Kinect; Air-writing; Slope variations detection; Analytical
   classification; Persian number
ID HAND GESTURE RECOGNITION; LONGEST COMMON SUBSEQUENCE
AB Air-writing through hand or fingertip is a functional and attractive mechanism. Since there is usually no pen-up and pen-down in air-writing, trajectory of numbers and words in the air-writing will be a connected series of characters (ligature Stroke). Identification of legitimate characters such as digits or letters inside a ligature Stroke is one of the most important challenges faced in this area. By solving these challenges, there will be more uses in future. In this work, the color and depth images of the Kinect sensor are used to identify the user's air-writing, which includes the digits and numbers of the Persian language. To extract a feature vector from the trajectory, we propose a simple but very effective method, called slope variations detection, which is robust to variations of size, translation, and rotation of the trajectory. Also, a novel analytical classifier is proposed to map a vector to a character. This classifier has higher speed and accuracy than traditional classifiers, such as SVM, HMM, and K Nearest Neighbors. Experimental results show that the average recognition rate for digits and numbers of Persian language is 98% which is quite acceptable for a practical system.
C1 [Mohammadi, Shahram; Maleki, Reza] Univ Zanjan, Dept Engn, Zanjan, Iran.
C3 University Zanjan
RP Mohammadi, S (corresponding author), Univ Zanjan, Dept Engn, Zanjan, Iran.
EM shahram@znu.ac.ir; r.maleki@znu.ac.ir
CR AGGARWAL V, 2001, INPHARMA        1027, P13
   Beg S., 2013, J INFORM DISPLAY, V14, DOI [10.1080/15980316.2013.860928, DOI 10.1080/15980316.2013.860928]
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P403, DOI 10.1109/THMS.2015.2492598
   Dan Z., 2013, COMMUN COMPUT PHYS, V363, DOI [10.1007/978-3-642-37149-3_25, DOI 10.1007/978-3-642-37149-3_25]
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Feng Ziyong., 2012, P 4 INT C INTERNET M, P70, DOI DOI 10.1145/2382336.2382356
   Froba B., 2004, P 6 IEEE INT C AUT F, DOI [10.1109/AFGR.2004.1301514, DOI 10.1109/AFGR.2004.1301514]
   Fu ZJ, 2019, IEEE T MOBILE COMPUT, V18, P473, DOI 10.1109/TMC.2018.2831709
   Han J., 2013, IEEE Transactions on Cybernetics, V43, P5, DOI DOI 10.1109/TCYB.2013.2265378
   Jin XJ, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P120, DOI 10.1109/ACPR.2013.7
   Kalra S, 2016, PROCEEDINGS ON 2016 2ND INTERNATIONAL CONFERENCE ON NEXT GENERATION COMPUTING TECHNOLOGIES (NGCT), P456, DOI 10.1109/NGCT.2016.7877459
   Kane L, 2017, IEEE T HUM-MACH SYST, V47, P1077, DOI 10.1109/THMS.2017.2706695
   Kumar P., 2017, 2017 15 IAPR INT C M, DOI DOI 10.23919/MVA.2017.7986825
   Liu F., 2017, 29 CHIN CONTR DEC C, DOI [10.1109/CCDC.2017.7978867, DOI 10.1109/CCDC.2017.7978867]
   Murata T, 2014, INT J DISTRIB SENS N, DOI 10.1155/2014/278460
   Nyirarugira C, 2015, SIGNAL PROCESS-IMAGE, V30, P178, DOI 10.1016/j.image.2014.10.008
   Patil S, 2016, J SENSORS, V2016, DOI 10.1155/2016/3692876
   Plamondon R, 2000, IEEE T PATTERN ANAL, V22, P63, DOI 10.1109/34.824821
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rautaray SiddharthS., 2012, Int J UbiComp, V3, P21
   Ren Y, 2009, IEEE I C EMBED SOFTW, P344, DOI 10.1109/ICESS.2009.21
   Saini R, 2018, EXPERT SYST APPL, V93, P169, DOI 10.1016/j.eswa.2017.10.021
   Sen P, 2011, ICT&A 2011/DEMSET 2011: INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES AND APPLICATIONS / INTERNATIONAL CONFERENCE ON DESIGN AND MODELING IN SCIENCE, EDUCATION, AND TECHNOLOGY, P19
   Singha J, 2015, ARTIF INTELL REV, V43, P1, DOI [10.1007/s10462-012-9356-9, DOI 10.1007/S10462-012-9356-9]
   Singha J, 2017, MULTIMEDIA SYST, V23, P499, DOI 10.1007/s00530-016-0510-0
   Songbin X., 2016, 2016 IEEE INT C SYST, DOI [10.1109/SMC.2016.7844452, DOI 10.1109/SMC.2016.7844452]
   Stern H, 2013, PATTERN RECOGN LETT, V34, P1980, DOI 10.1016/j.patrec.2013.02.007
   Trigueiros P, 2014, COMM COM INF SC, V449, P162, DOI 10.1007/978-3-662-44440-5_10
   Wang S, 2009, INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL SCIENCES AND OPTIMIZATION, VOL 1, PROCEEDINGS, P274, DOI 10.1109/CSO.2009.201
   Wu C.H., 2013, IEEE 17 INT S CONS E, DOI [10.1109/ISCE.2013.6570227, DOI 10.1109/ISCE.2013.6570227]
   Zeng W, 2018, MULTIMED TOOLS APPL, V77, P28185, DOI 10.1007/s11042-018-5998-1
   Zhang D., 2001, LECT NOTES COMPUTER, V2195, DOI [10.1007/3-540-45453-5_111, DOI 10.1007/3-540-45453-5_111]
   Zhang X, 2013, IEEE MULTIMEDIA, V20, P85, DOI 10.1109/MMUL.2013.50
NR 33
TC 8
Z9 8
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1001
EP 1015
DI 10.1007/s00371-019-01717-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100010
DA 2024-07-18
ER

PT J
AU Gui, Y
   Zeng, G
AF Gui, Yan
   Zeng, Guang
TI Joint learning of visual and spatial features for edit propagation from
   a single image
SO VISUAL COMPUTER
LA English
DT Article
DE Image editing; Edit propagation; Deep neural network; Fully connected
   conditional random field
AB In this paper, we regard edit propagation as a multi-class classification problem and deep neural network (DNN) is used to solve the problem. We design a shallow and fully convolutional DNN that can be trained end-to-end. To achieve this, our method uses combinations of low-level visual features, which are extracted from the input image, and spatial features, which are computed through transforming user interactions, as input of the DNN, which efficiently performs a joint learning of visual and spatial features. We then train the DNN on many of such combinations in order to build a DNN-based pixel-level classifier. Our DNN is also equipped with patch-by-patch training and whole image estimation, speeding up learning and inference. Finally, we improve classification accuracy of the DNN by employing a fully connected conditional random field. Experimental results show that our method can respond to user interactions well and generate precise results compared with the state-of-art edit propagation approaches. Furthermore, we demonstrate our method on various applications.
C1 [Gui, Yan] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha, Hunan, Peoples R China.
   [Zeng, Guang] Changsha Univ Sci & Technol, Changsha, Hunan, Peoples R China.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology
RP Gui, Y (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha, Hunan, Peoples R China.
EM guiyan@csust.edu.cn
FU National Natural Science Foundations of P. R. China [61402053, 61602059,
   61772087, 61802031]; Scientific Research Fund of Education Department of
   Hunan Province [16C0046, 16A008]
FX We would like to thank Prof. Yiyu Cai and Dr. Zhifeng Xie for
   proofreading our paper. We would also like to thank the reviewers for
   their valuable comments. This study was funded by the National Natural
   Science Foundations of P. R. China (Grant Nos. 61402053; 61602059;
   61772087; 61802031) and the Scientific Research Fund of Education
   Department of Hunan Province (Grant Nos. 16C0046; 16A008).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Bie XH, 2011, COMPUT GRAPH FORUM, V30, P2041, DOI 10.1111/j.1467-8659.2011.02059.x
   Cambra AB, 2018, VISUAL COMPUT, V34, P1493, DOI 10.1007/s00371-017-1422-5
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XW, 2014, PROC CVPR IEEE, pCP5, DOI 10.1109/CVPR.2014.365
   Chen XW, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366151
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Dalmau O, 2010, COMPUT GRAPH FORUM, V29, P2372, DOI 10.1111/j.1467-8659.2010.01751.x
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Endo Y, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.12822
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   [桂彦 Gui Yan], 2018, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V30, P1284
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Kingma D. P., 2014, arXiv
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li Y, 2008, COMPUT GRAPH FORUM, V27, P1255, DOI 10.1111/j.1467-8659.2008.01264.x
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luan Q., 2007, P 18 EUR C CREND TEC, P309
   Ma LQ, 2012, COMPUT GRAPH-UK, V36, P1005, DOI 10.1016/j.cag.2012.08.001
   Musialski P, 2013, VISUAL COMPUT, V29, P1173, DOI 10.1007/s00371-012-0761-5
   Pellacini F, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1243980.1243983, 10.1145/1276377.1276444, 10.1145/1239451.1239505]
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1135, DOI 10.1109/TVCG.2010.125
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508404
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yatagawa T, 2015, VISUAL COMPUT, V31, P1101, DOI 10.1007/s00371-015-1094-y
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
NR 43
TC 57
Z9 57
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 469
EP 482
DI 10.1007/s00371-019-01633-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500003
DA 2024-07-18
ER

PT J
AU Yatagawa, T
   Todo, H
   Yamaguchi, Y
   Morishima, S
AF Yatagawa, Tatsuya
   Todo, Hideki
   Yamaguchi, Yasushi
   Morishima, Shigeo
TI Data compression for measured heterogeneous subsurface scattering via
   scattering profile blending
SO VISUAL COMPUTER
LA English
DT Article
DE Subsurface scattering; Data compression; BSSRDF; Mobiles
ID DIFFUSION; ALGORITHM; MODEL
AB Subsurface scattering involves the complicated behavior of light beneath the surfaces of translucent objects that includes scattering and absorption inside the object's volume. Physically accurate numerical representation of subsurface scattering requires a large number of parameters because of the complex nature of this phenomenon. The large amount of data restricts the use of the data on memory-limited devices such as video game consoles and mobile phones. To address this problem, this paper proposes an efficient data compression method for heterogeneous subsurface scattering. The key insight of this study is that heterogeneous materials often comprise a limited number of base materials, and the size of the subsurface scattering data can be significantly reduced by parameterizing only a few base materials. In the proposed compression method, we represent the scattering property of a base material using a function referred to as the base scattering profile. A small subset of the base materials is assigned to each surface position, and the local scattering property near the position is described using a linear combination of the base scattering profiles in the log scale. The proposed method reduces the data by a factor of approximately 30 compared to a state-of-the-art method, without significant loss of visual quality in the rendered graphics. In addition, the compressed data can also be used as bidirectional scattering surface reflectance distribution functions (BSSRDF) without incurring much computational overhead. These practical aspects of the proposed method also facilitate the use of higher-resolution BSSRDFs in devices with large memory capacity.
C1 [Yatagawa, Tatsuya] Waseda Univ, Grad Sch Adv Sci & Engn, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan.
   [Todo, Hideki] Chuo Gakuin Univ, Fac Liberal Arts, 451 Kujike, Abiko, Chiba 2701196, Japan.
   [Yamaguchi, Yasushi] Univ Tokyo, Grad Sch Arts & Sci, Meguro Ku, 3-8-1 Komaba, Tokyo 1530041, Japan.
   [Morishima, Shigeo] Waseda Univ, Waseda Res Inst Sci & Engn, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan.
C3 Waseda University; University of Tokyo; Waseda University
RP Yatagawa, T (corresponding author), Waseda Univ, Grad Sch Adv Sci & Engn, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan.
EM tatsy@acm.org; todo@fla.cgu.ac.jp; yama@graco.c.u-tokyo.ac.jp;
   shigeo@waseda.jp
RI Yamaguchi, Yasushi/S-5779-2019; Yamaguchi, Yasushi/IZP-8277-2023;
   Yatagawa, Tatsuya/HKV-3976-2023
OI Yatagawa, Tatsuya/0000-0003-4653-2435; Morishima,
   Shigeo/0000-0001-8859-6539; YAMAGUCHI, Yasushi/0000-0003-0790-4144
FU Japan's Society for the Promotion of Science [JP16J02280]; JSPS KAKENHI
   [JP16H02818, JP15H05924]; JST ACCEL Grant [JPM-JAC1602]; Waseda
   Institute of Advanced Science and Engineering
FX Tatsuya Yatagawa was supported by a Research Fellowship for Young
   Researchers from Japan's Society for the Promotion of Science
   (Grant-in-Aid for JSPS Fellows JP16J02280). Hideki Todo was supported by
   JSPS KAKENHI Grant No. JP15H05924. Yasushi Yamaguchi was supported by
   JSPS KAKENHI Grant No. JP16H02818. Shigeo Morishima was supported by JST
   ACCEL Grant No. JPM-JAC1602, and a Grant-in-Aid from the Waseda
   Institute of Advanced Science and Engineering.
CR [Anonymous], ACM SIGGRAPH ASIA TE
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Borlum J., 2011, Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics, P7
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Chandrasekhar S., 1950, RAD TRANSFER
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   CONN AR, 1991, SIAM J NUMER ANAL, V28, P545, DOI 10.1137/0728030
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dal Corso A, 2017, VISUAL COMPUT, V33, P371, DOI 10.1007/s00371-016-1207-2
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Frederickx R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073681
   Frisvad JR, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682629
   Goesele M, 2004, ACM T GRAPHIC, V23, P835, DOI 10.1145/1015706.1015807
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jimenez J., 2010, GPU PRO ADV RENDERIN, P335
   Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529
   Kaplanyan A., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'10, P99, DOI [10.1145/1730804.1730821, 10.1145/1730804.1730821.24, DOI 10.1145/1730804.1730821.24]
   Koa MD, 2014, VISUAL COMPUT, V30, P821, DOI 10.1007/s00371-014-0952-3
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Mertens T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P130
   Nicodemus F., 1977, GEOMETRICAL CONSIDER
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Shah MA, 2009, IEEE COMPUT GRAPH, V29, P66, DOI 10.1109/MCG.2009.11
   Song Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531337
   Stam J, 1995, SPRING COMP SCI, P41
   Wang JP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330520
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 29
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 541
EP 558
DI 10.1007/s00371-018-01626-x
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500008
OA hybrid
DA 2024-07-18
ER

PT J
AU Huang, YB
   Qiu, CY
   Yuan, K
AF Huang, Yibin
   Qiu, Congying
   Yuan, Kui
TI Surface defect saliency of magnetic tile
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Surface defect; Convolutional network
ID OBJECT DETECTION; REGION DETECTION; IMAGES; MODEL; STRIP
AB Computer vision builds a connection between image processing and industrials, bringing modern perception to the automated manufacture of magnetic tiles. In this article, we propose a real-time model called MCuePush U-Net, specifically designed for saliency detection of surface defect. Our model consists of three main components: MCue, U-Net and Push network. MCue generates three-channel resized inputs, including one MCue saliency image and two raw images; U-Net learns the most informative regions, and essentially it is a deep hierarchical structured convolutional network; Push network defines the specific location of predicted surface defects with bounding boxes, constructed by two fully connected layers and one output layer. We show that the model exceeds the state of the art in saliency detection of magnetic tiles, in which it both effectively and explicitly maps multiple surface defects from low-contrast images. The proposed model significantly reduces time cost of machinery from 0.5 s per image to 0.07 s and enhances detection accuracy for image-based defect examinations.
C1 [Huang, Yibin] Univ Chinese Acad Sci, Inst Automat, Chinese Acad Sci, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
   [Qiu, Congying] Columbia Univ, Dept Civil Engn & Engn Mech, New York, NY 10027 USA.
   [Yuan, Kui] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Institute of Automation, CAS; Columbia University; Chinese Academy
   of Sciences; Institute of Automation, CAS
RP Huang, YB (corresponding author), Univ Chinese Acad Sci, Inst Automat, Chinese Acad Sci, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM huangyibin2014@ia.ac.cn
OI huang, yi bin/0000-0001-8517-1068
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aiger D, 2010, PROC CVPR IEEE, P295, DOI 10.1109/CVPR.2010.5540198
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   [Anonymous], 2013, MATH PROBL ENG, DOI DOI 10.1155/2013/785383
   [Anonymous], 2012, The 2012 International Joint Conference on Neural Networks (IJCNN)
   [Anonymous], 2018, MAR GEORESOUR GEOTEC
   Bai XL, 2014, IEEE T IND INFORM, V10, P2135, DOI 10.1109/TII.2014.2359416
   Ben Gharsallah M, 2015, ADV MATER SCI ENG, V2015, DOI 10.1155/2015/871602
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cha YJ, 2017, COMPUT-AIDED CIV INF, V32, P361, DOI 10.1111/mice.12263
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gorji S, 2017, PROC CVPR IEEE, P3472, DOI 10.1109/CVPR.2017.370
   Guan SQ, 2015, ISIJ INT, V55, P1950, DOI 10.2355/isijinternational.ISIJINT-2015-041
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   He SF, 2017, IEEE I CONF COMP VIS, P1059, DOI 10.1109/ICCV.2017.120
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Kapoor A, 2016, VISUAL COMPUT, V33, P1
   Li XQ, 2014, NDT&E INT, V62, P6, DOI 10.1016/j.ndteint.2013.10.006
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ramanishka Vasili., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition CVPR, V1, P7
   Ren RX, 2018, IEEE T CYBERNETICS, V48, P929, DOI 10.1109/TCYB.2017.2668395
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rudinac Maja, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P404, DOI 10.1109/ICPR.2010.107
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Simon M, 2017, IEEE I CONF COMP VIS, P4970, DOI 10.1109/ICCV.2017.531
   Song KC, 2014, ISIJ INT, V54, P2598, DOI 10.2355/isijinternational.54.2598
   Soukup D, 2014, LECT NOTES COMPUT SC, V8887, P668, DOI 10.1007/978-3-319-14249-4_64
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Tavakoli HR, 2011, LECT NOTES COMPUT SC, V6688, P666, DOI 10.1007/978-3-642-21227-7_62
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Xie LF, 2016, APPL SURF SCI, V375, P118, DOI 10.1016/j.apsusc.2016.03.013
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang BL, 2016, VISUAL COMPUT, V32, P1415, DOI 10.1007/s00371-015-1129-4
   Yang C, 2018, 2018 IEEE 23 INT C D, V1, P1
   Yang CL, 2017, OPT LASER TECHNOL, V90, P7, DOI 10.1016/j.optlastec.2016.08.016
   Yang CL, 2016, NDT&E INT, V83, P78, DOI 10.1016/j.ndteint.2016.04.006
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang Z, 2017, VISUAL COMPUT, V33, P1403, DOI 10.1007/s00371-016-1287-z
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang M., 2017, ARXIV171008149
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 58
TC 183
Z9 200
U1 20
U2 122
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 85
EP 96
DI 10.1007/s00371-018-1588-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800008
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Moulin, M
   Dutré, P
AF Moulin, Matthias
   Dutre, Philip
TI On the use of local ray termination for efficiently constructing
   qualitative BSPs, BIHs and (S)BVHs
SO VISUAL COMPUTER
LA English
DT Article
DE Graphics data structures; Acceleration data structures; Ray tracing; Ray
   termination
ID TRAVERSAL ORDER
AB Acceleration data structures (ADSs) exploit spatial coherence by distributing a scene's geometric primitives into spatial groups, effectively reducing the cost of ray tracing queries. The most effective ADSs are hierarchical, adaptive tree structures such as BSPs, BIHs and (S)BVHs. The de facto standard cost metric for building these structures is the Surface Area Heuristic (SAH), which assumes a scene-exterior isotropic ray distribution of non-terminating rays. Despite its restrictive assumptions, the SAH remains competitive against many fundamentally different cost metrics targeting more common ray distributions. Our goal is not to radically change and replace the SAH, but to adapt it by introducing the concept of local ray termination in the context of voxel partitioning during the ADS construction and voxel traversal order during ADS traversal. We develop heuristics to approximate local ray termination efficiently without additional preprocessing or ray (sub)sampling. Our heuristics are used for approximating the visibility probabilities in the Ray Termination Surface Area Heuristic (RTSAH) for constructing BSPs, BIHs and (S)BVHs for accelerating closest-hit ray queries and for approximating the hit probabilities in the Shadow Ray Distribution Heuristic for constructing dedicated BVHs for accelerating any-hit ray queries. The main aim of our paper is to analyze the potential of including local ray termination into the SAH. The results indicate rendering performance close to the references (SAH and NodeSATO) on average due to small and/or compensating gains in the number of ray-triangle intersection tests and ADS node traversal steps. Furthermore, prerendering build times are higher for the RTSAH due to triangle clipping.
C1 [Moulin, Matthias; Dutre, Philip] Katholieke Univ Leuven, Dept Comp Sci, Leuven, Belgium.
C3 KU Leuven
RP Moulin, M (corresponding author), Katholieke Univ Leuven, Dept Comp Sci, Leuven, Belgium.
EM matthias.moulin@gmail.com
RI Dutré, Philip LMJ/A-8716-2014; Moulin, Matthias/X-6281-2018
OI Moulin, Matthias/0000-0002-0616-8483; Dutre, Philip/0000-0002-9344-2523
CR Aila T., 2013, P 5 HIGH PERFORMANCE, P101
   [Anonymous], RR3204 INRIA
   [Anonymous], 2005, ACM SIGGRAPH 2005 CO
   [Anonymous], EUR 2009 EUR ASS
   [Anonymous], FINAL ANSWERS
   [Anonymous], EUR S REND EXP ID IM
   [Anonymous], EUR S REND EXP ID IM
   [Anonymous], 1994, Radiosity and global illumination
   [Anonymous], ACM SIGGRAPH COURSE
   [Anonymous], LIGHTING CHALLENGES
   [Anonymous], P 25 SPRING C COMP G
   [Anonymous], 2004, OPERATIONS RES APPL
   [Anonymous], P MIN STOCH SEARCH A
   [Anonymous], P 4 ACM SIGGRAPH EUR
   [Anonymous], THESIS
   [Anonymous], 2016, J COMPUTER GRAPHICS
   [Anonymous], RAY TRACING NEWS
   Appel A., 1968, P AFIPS FALL JOINT C, P37, DOI [DOI 10.1145/1468075.1468082, 10.1145/1468075.1468082]
   BENTLEY JL, 1979, COMPUT SURV, V11, P397, DOI 10.1145/356789.356797
   Bittner J, 2013, COMPUT GRAPH FORUM, V32, P85, DOI 10.1111/cgf.12000
   Cauchy A., 1841, C.R. Acad. Sci., V13, P1060
   Cauchy A., 1850, Mem. Acad. Sci, V22, P3
   Choi B, 2012, COMPUT GRAPH-UK, V36, P38, DOI 10.1016/j.cag.2011.11.007
   Cohen Michael F., 1993, Radiosity and realistic image synthesis
   Dammertz H, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P155, DOI 10.1109/RT.2008.4634636
   Ernst M, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P73, DOI 10.1109/RT.2007.4342593
   Ganestam P., 2015, J COMPUTER GRAPHICS, V4, P23
   Ganestam P, 2016, COMPUT GRAPH FORUM, V35, P285, DOI 10.1111/cgf.12831
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Hapala M, 2011, COMPUT GRAPH FORUM, V30, P199, DOI 10.1111/j.1467-8659.2010.01844.x
   Havran V, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P209
   Havran  V., 2000, THESIS
   Havran V, 1999, P SCCG 99 SPRING C C, P171
   Havran V, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P71
   Hottel H.C., 1931, T AM SOC MECH ENG, V53, P265
   Hunt W, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P3, DOI 10.1109/RT.2008.4634613
   Hunt W, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P81
   Hurley JamesT., 2002, P GRAPHICON
   Ize T, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P159, DOI 10.1109/RT.2008.4634637
   Ize T, 2011, COMPUT GRAPH FORUM, V30, P297, DOI 10.1111/j.1467-8659.2011.01861.x
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kay T. L., 1986, Computer Graphics, V20, P269, DOI 10.1145/15886.15916
   Kensler A, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P73, DOI 10.1109/RT.2008.4634624
   MacDonald J. D., 1990, Visual Computer, V6, P153, DOI 10.1007/BF01911006
   Markov AA, 2006, SCI CONTEXT, V19, P591, DOI 10.1017/S0269889706001074
   Markov A.A, 1913, Bulletin de l'Academie Imperiale des Sciences de St-Petersbourg, V7, P153
   McGuire M., 2017, Computer Graphics Archive
   Nah JH, 2014, COMPUT GRAPH FORUM, V33, P167, DOI 10.1111/cgf.12341
   Ooi B. C., 1987, P IEEE COMPSAC C
   Pharr M., 2010, PHYS BASED RENDERING
   Popov S, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P89
   Popov Stefan., 2009, P C HIGH PERFORMANCE, P15, DOI DOI 10.1145/1572769.1572772
   Reinhard E., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P41
   Rubin S. M., 1980, Computer Graphics, V14, P110, DOI 10.1145/965105.807479
   Ruppert D, 2011, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-1-4419-7787-8
   Shevtsov M, 2007, COMPUT GRAPH FORUM, V26, P395, DOI 10.1111/j.1467-8659.2007.01062.x
   Silvennoinen A, 2014, COMPUT GRAPH FORUM, V33, P235, DOI 10.1111/cgf.12271
   Stich M., 2009, P C HIGH PERF GRAPH, P7, DOI DOI 10.1145/1572769.1572771
   Vinkler M, 2012, COMPUT GRAPH-UK, V36, P283, DOI 10.1016/j.cag.2012.02.013
   Wachter C., 2006, Proceedings of the Eurographics Symposium on Rendering, P139, DOI DOI 10.2312/EGWR/EGSR06/139-149
   Wald I, 2004, COMPUT GRAPH FORUM, V23, P595, DOI 10.1111/j.1467-8659.2004.00791.x
   Wald I, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P33, DOI 10.1109/RT.2007.4342588
   Wald I, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P61
   Wald I, 2009, COMPUT GRAPH FORUM, V28, P1691, DOI 10.1111/j.1467-8659.2008.01313.x
   Walter B, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P81, DOI 10.1109/RT.2008.4634626
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Woop S., 2006, SIGGRAPHEUROGRAPHICS, P67
NR 67
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1809
EP 1826
DI 10.1007/s00371-018-1575-x
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300010
DA 2024-07-18
ER

PT J
AU Wang, YC
   Zhang, Q
   Lin, F
   Goh, CK
   Seah, HS
AF Wang, Yan Chao
   Zhang, Qian
   Lin, Feng
   Goh, Chi Keong
   Seah, Hock Soon
TI PolarViz: a discriminating visualization and visual analytics tool for
   high-dimensional data
SO VISUAL COMPUTER
LA English
DT Article
DE PolarViz; Customized radial distortion; Focus plus context;
   Discriminating visualization; High-dimensional data analytics; Turbine
   performance assessment
ID RADVIZ; REDUCTION; FIT
AB Visual analytics tools are of paramount importance in handling high-dimensional datasets such as those in our turbine performance assessment. Conventional tools such as RadViz have been used in 2D exploratory data analysis. However, with the increase in dataset size and dimensionality, the clumping of projected data points toward the origin in RadViz causes low space utilization, which largely degenerates the visibility of the feature characteristics. In this study, to better evaluate the hidden patterns in the center region, we propose a new focus + context distortion approach, termed PolarViz, to manipulate the radial distribution of data points. We derive radial equalization to automatically spread out the frequency, and radial specification to shape the distribution based on user's requirement. Computational experiments have been conducted on two datasets including a benchmark dataset and a turbine performance simulation data. The performance of the proposed algorithm as well as other methods for solving the clumping problem in both data space and image space are illustrated and compared, and the pros and cons are analyzed. Moreover, a user study was conducted to assess the performance of the proposed method.
C1 [Wang, Yan Chao] Nanyang Technol Univ, Rolls Royce NTU Corp Lab, Sch Comp Sci & Engn, Singapore, Singapore.
   [Zhang, Qian] Nanyang Technol Univ, Rolls Royce NTU Corp Lab, Singapore, Singapore.
   [Lin, Feng; Seah, Hock Soon] Nanyang Technol Univ, SCSE, Singapore, Singapore.
   [Goh, Chi Keong] Rolls Royce Singapore Pte Ltd, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University; Rolls-Royce Holding Group
RP Lin, F (corresponding author), Nanyang Technol Univ, SCSE, Singapore, Singapore.
EM YWANG054@e.ntu.edu.sg; ASFLIN@ntu.edu.sg
RI Seah, Hock Soon/AAK-9900-2020
OI Seah, Hock Soon/0000-0003-2699-7147; Goh, Chi Keong/0000-0002-4250-7307;
   Zhang, Qian/0000-0001-8187-4970
FU National Research Foundation (NRF) Singapore under the Corp
   Lab@UniversityScheme
FX This work was conducted within the Rolls-Royce@NTU Corporate Lab with
   support from the National Research Foundation (NRF) Singapore under the
   Corp Lab@UniversityScheme.The work is largely extended from our CGI 2017
   paper [38].
CR Albuquerque G., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P19, DOI 10.1109/VAST.2010.5652433
   [Anonymous], 1994, ACM Transactions on Computer-Human Interaction, DOI [10.1145/180171.180173, DOI 10.1145/180171.180173]
   [Anonymous], 2014, ENG VERS 1 8A
   Artero AO, 2004, XVII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P340, DOI 10.1109/SIBGRA.2004.1352979
   BASU A, 1995, PATTERN RECOGN LETT, V16, P433, DOI 10.1016/0167-8655(94)00115-J
   Bertini E, 2004, IEEE INFOR VIS, P622, DOI 10.1109/IV.2004.1320207
   Chen KK, 2006, ACM T INFORM SYST, V24, P245, DOI 10.1145/1148020.1148024
   Daniels K, 2012, INFORM VISUAL, V11, P273, DOI 10.1177/1473871612439357
   Draper GM, 2009, IEEE T VIS COMPUT GR, V15, P759, DOI 10.1109/TVCG.2009.23
   Ellis G, 2007, IEEE T VIS COMPUT GR, V13, P1216, DOI 10.1109/TVCG.2007.70535
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Heer J, 2012, COMMUN ACM, V55, P45, DOI 10.1145/2133806.2133821
   Hoffman P, 1997, VISUALIZATION '97 - PROCEEDINGS, P437, DOI 10.1109/VISUAL.1997.663916
   Hughes Ciaran, 2008, IET Irish Signals and Systems Conference. ISSC 2008, P162, DOI 10.1049/cp:20080656
   Ibrahim A, 2016, IEEE C EVOL COMPUTAT, P736, DOI 10.1109/CEC.2016.7743865
   Inselberg A., 2009, Parallel Coordinates. Visual Multidimensional Geometry and its Applications, DOI DOI 10.1007/978-0-387-68628-8
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   KADMON N., 1978, CARTOGR J, V15, P36, DOI [DOI 10.1179/CAJ.1978.15.1.36, 10.1179/caj.1978.15.1.36.]
   Kohonen T., 1998, Neurocomputing, V21, P1, DOI 10.1016/S0925-2312(98)00030-7
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Liu SX, 2014, VISUAL COMPUT, V30, P1373, DOI 10.1007/s00371-013-0892-3
   Liu SS, 2017, IEEE T VIS COMPUT GR, V23, P1249, DOI 10.1109/TVCG.2016.2640960
   Mackinlay J. D., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P173, DOI 10.1145/108844.108870
   Nováková L, 2009, INFORMATION VISUALIZATION, IV 2009, PROCEEDINGS, P104, DOI 10.1109/IV.2009.103
   Packer JF, 2017, VISUAL COMPUT, V33, P1291, DOI 10.1007/s00371-016-1217-0
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Ono JHP, 2015, SIBGRAPI, P165, DOI 10.1109/SIBGRAPI.2015.38
   Pryke A, 2007, LECT NOTES COMPUT SC, V4403, P361
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rubio-Sánchez M, 2016, IEEE T VIS COMPUT GR, V22, P619, DOI 10.1109/TVCG.2015.2467324
   Russell A., 2012, Proceedings of the 2012 16th International Conference on Information Visualisation (IV), P229, DOI 10.1109/IV.2012.46
   Russo AC, 2014, PSYCHOL INJ LAW, V7, P178, DOI 10.1007/s12207-014-9190-2
   Sarkar M., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P83, DOI 10.1145/142750.142763
   Sharko J, 2008, IEEE T VIS COMPUT GR, V14, P1444, DOI 10.1109/TVCG.2008.173
   Spence R., 1982, Behaviour and Information Technology, V1, P43, DOI 10.1080/01449298208914435
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Wang YC, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095155
   Zhou FF, 2015, IEEE PAC VIS SYMP, P111, DOI 10.1109/PACIFICVIS.2015.7156365
NR 39
TC 12
Z9 13
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1567
EP 1582
DI 10.1007/s00371-018-1558-y
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000006
DA 2024-07-18
ER

PT J
AU Yang, H
   Zhang, ZB
   Guan, YJ
AF Yang, Hang
   Zhang, Zhongbo
   Guan, Yujing
TI Rolling bilateral filter-based text image deblurring
SO VISUAL COMPUTER
LA English
DT Article
DE Rolling bilateral filter; Text image deblurring; Texture removal; Edge
   preserving
ID RESTORATION
AB Although many competitive approaches have been developed for image deblurring, the priors which rely on natural image are less effective for text images which have special properties, lack of heavy-tailed gradient, and clean background. Considering the specific structure of text images, in this work, we present an effective yet simple deblurring method based on rolling bilateral filtering, an improved rolling guidance filtering specifically. The rolling bilateral filter can remove texture and preserve image structures. Thus, it is appropriate for processing the document image whose background regions are uniform. According to this property, we propose an efficient iterative algorithm to estimate the image and point spread function. Experimental results demonstrate that our method outperforms the traditional deblurring algorithms designed for typical natural images and text images.
C1 [Yang, Hang] Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Jilin, Peoples R China.
   [Zhang, Zhongbo; Guan, Yujing] Jilin Univ, Dept Math, Changchun 130012, Jilin, Peoples R China.
C3 Chinese Academy of Sciences; Changchun Institute of Optics, Fine
   Mechanics & Physics, CAS; Jilin University
RP Yang, H (corresponding author), Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Jilin, Peoples R China.
EM yanghang09@mails.jlu.edu.cn
CR [Anonymous], [No title captured]
   [Anonymous], P CVPR
   Banerjee J, 2009, PROC CVPR IEEE, P517, DOI 10.1109/CVPRW.2009.5206601
   Brown MS, 2004, IEEE T PATTERN ANAL, V26, P1295, DOI 10.1109/TPAMI.2004.87
   Buades A, 2010, IEEE T IMAGE PROCESS, V19, P1978, DOI 10.1109/TIP.2010.2046605
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P1302, DOI 10.1109/TIP.2015.2400217
   Chen XG, 2011, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2011.5995568
   Cho HJ, 2012, LECT NOTES COMPUT SC, V7576, P524, DOI 10.1007/978-3-642-33715-4_38
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Donaldson K, 2005, PROC CVPR IEEE, P1188
   Dong JX, 2017, IEEE I CONF COMP VIS, P2497, DOI 10.1109/ICCV.2017.271
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fang HZ, 2017, SIGNAL PROCESS, V138, P182, DOI 10.1016/j.sigpro.2017.01.021
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gong D, 2017, IEEE I CONF COMP VIS, P1670, DOI 10.1109/ICCV.2017.184
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu Z, 2012, LECT NOTES COMPUT SC, V7576, P59, DOI 10.1007/978-3-642-33715-4_5
   Jiang XL, 2017, NEUROCOMPUTING, V242, P1, DOI 10.1016/j.neucom.2017.01.080
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li J, 2016, J VIS COMMUN IMAGE R, V40, P14, DOI 10.1016/j.jvcir.2016.06.003
   Li TH, 2002, IEEE T IMAGE PROCESS, V11, P847, DOI 10.1109/TIP.2002.801127
   Likforman-Sulem L., 2009, ICDAR
   Liu SG, 2015, VISUAL COMPUT, V31, P733, DOI 10.1007/s00371-014-0998-2
   Lou YF, 2011, J MATH IMAGING VIS, V39, P1, DOI 10.1007/s10851-010-0220-8
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan J, 2016, OXID MED CELL LONGEV, V2016, P1
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Song CW, 2016, NEUROCOMPUTING, V197, P95, DOI 10.1016/j.neucom.2016.02.053
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu YQ, 2016, NEUROCOMPUTING, V171, P1185, DOI 10.1016/j.neucom.2015.07.049
   Yan Y, 2017, PROCEEDINGS OF THE 9TH (2017) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, P34
   Yang H, 2017, SIGNAL PROCESS, V138, P16, DOI 10.1016/j.sigpro.2017.03.006
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85
NR 47
TC 7
Z9 8
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1627
EP 1640
DI 10.1007/s00371-018-1562-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000010
DA 2024-07-18
ER

PT J
AU Yuan, Q
   Li, JX
   Zhang, LW
   Wu, ZF
   Liu, GY
AF Yuan, Quan
   Li, Junxia
   Zhang, Lingwei
   Wu, Zhefu
   Liu, Guangyu
TI Blind motion deblurring with cycle generative adversarial networks
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Blind deblurring; Motion deblurring; Cycle consistency
ID IMAGE; MODELS
AB Blind motion deblurring is one of the most basic and challenging problems in image processing and computer vision. It aims to recover a sharp image from its blurred version knowing nothing about the blurring process. Many existing methods use the maximum a posteriori or expectation maximization framework to tackle this problem, but they cannot handle well the natural images with high-frequency features. Most recently, deep neural networks have been emerging as a powerful tool for image deblurring. In this paper, we show that encoder-decoder architecture gives better results for image deblurring tasks. In addition, we propose a novel end-to-end learning model that refines the generative adversarial network by many novel strategies to tackle the problem of image deblurring. Experimental results show that our model can capture high-frequency features well, and achieve the competitive performance.
C1 [Yuan, Quan; Li, Junxia; Zhang, Lingwei; Wu, Zhefu; Liu, Guangyu] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.
   [Li, Junxia] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology
RP Li, JX (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.; Li, JX (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
EM junxiali99@163.com
RI yuan, quan/GZM-5597-2022; zhang, ying/JQX-1479-2023
FU National Natural Science Foundation of China (NSFC) [61702272, 61773219,
   61771249, 61802199]; Startup Foundation for Introducing Talent of NUIST
   [2243141701034]
FX We thank Guangcan Liu and Yubao Sun for their helpful discussions and
   advices. This work was supported by the National Natural Science
   Foundation of China (NSFC) under Grant Nos. 61702272, 61773219, 61771249
   and 61802199, and the Startup Foundation for Introducing Talent of NUIST
   (2243141701034).
CR [Anonymous], 2017, ARXIV170306029
   Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743
   Cambra AB, 2018, VISUAL COMPUT, V34, P1493, DOI 10.1007/s00371-017-1422-5
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chandramouli P, 2018, IEEE T IMAGE PROCESS, V27, P1723, DOI 10.1109/TIP.2017.2775062
   Chollet F, 2015, KERAS
   Duda RO., 2012, Pattern classificatio
   Fan Q, 2018, VISUAL COMPUT, V34, P1145, DOI 10.1007/s00371-018-1546-2
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   Guo Shi, 2018, ARXIV180704686
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hobbs JB, 2018, J AM COLL RADIOL, V15, P34, DOI 10.1016/j.jacr.2017.08.034
   Ineichen P, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10030435
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khmag A, 2018, VISUAL COMPUT, V34, P1661, DOI 10.1007/s00371-017-1439-9
   Kingma D. P., 2014, arXiv
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Kotera J, 2018, IEEE IMAGE PROC, P2860, DOI 10.1109/ICIP.2018.8451661
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kupyn O., 2017, P IEEE C COMP VIS PA, P2261
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Liu GC, 2014, IEEE T IMAGE PROCESS, V23, P5047, DOI 10.1109/TIP.2014.2362055
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529
   Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Pan Jinshan, 2018, ARXIV180800605
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sanchez-Lengeling B, 2018, SCIENCE, V361, P360, DOI 10.1126/science.aat2663
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sheikh HR, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P709
   Sun LB, 2013, IEEE INT CONF COMPUT
   Sun ZZ, 2018, IEEE T CIRC SYST VID, V28, P193, DOI 10.1109/TCSVT.2016.2605045
   Tofighi M, 2018, IEEE SIGNAL PROC LET, V25, P273, DOI 10.1109/LSP.2017.2782570
   Wang RX, 2018, IEEE T IMAGE PROCESS, V27, P2897, DOI 10.1109/TIP.2018.2815084
   Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu XY, 2018, IEEE T IMAGE PROCESS, V27, P194, DOI 10.1109/TIP.2017.2753658
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu ZB, 2018, VISUAL COMPUT, V34, P1691, DOI 10.1007/s00371-017-1443-0
   Zhang HL, 2011, MEDIAT INFLAMM, V2011, DOI 10.1155/2011/949072
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 14
Z9 14
U1 5
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1591
EP 1601
DI 10.1007/s00371-019-01762-y
EA OCT 2019
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ2RH
UT WOS:000544084600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, W
   Gao, W
AF Wang, Wei
   Gao, Wei
TI Efficient multi-plane extraction from massive 3D points for modeling
   large-scale urban scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Plane fitting; 3D reconstruction; Piecewise planar assumption; Markov
   Random Field
AB In modeling large-scale urban scenes, extracting reliable dominant planes from initial 3D points plays an important role for inferring the complete scene structures. However, traditional local and global methods are frequently prone to missing many real planes and also appear powerless when massive 3D points are present. To solve these problems, the paper presents an efficient multi-plane extraction method based on scene structure priors. The proposed method first explores the potential relations between the planes by detecting 2D line segments in the projection map produced from initial 3D points (i.e., simplify 3D model to 2D model), including: (1) multi-line detection in regions by the guidance of scene structure priors; (2) multi-line detection between regions under the Markov Random Field framework incorporating scene structure priors. Then, according to the resulting plane relations, a rapid multi-plane generation is carried out instead of the time-consuming plane fitting over 3D points. Experimental results confirm that the proposed method can efficiently produce sufficient and reliable dominant planes from a vast number of noisy 3D points (only about 8s on 2000K 3D points) and can be applied for modeling large-scale urban scenes.
C1 [Wang, Wei] Zhoukou Normal Univ, Sch Network Engn, Zhoukou 466000, Peoples R China.
   [Gao, Wei] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Gao, Wei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Zhoukou Normal University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Wang, W (corresponding author), Zhoukou Normal Univ, Sch Network Engn, Zhoukou 466000, Peoples R China.
EM wangwei@zknu.cn; wgao@nlpr.ia.ac.cn
RI Zhang, Chi/JSK-0744-2023; Gao, Wei/A-4473-2015
OI Gao, Wei/0000-0003-2257-5684
FU National Key R&D Program of China [2016YFB0502002]; National Laboratory
   of Pattern Recognition [201700004]; National Natural Science Foundation
   of China [61472419]; Natural Science Foundation of Henan Province
   [162300410347]; College Key Research Project of Henan Province
   [17A520018, 17A520019]; Zhoukou Normal University [zknuc2015103,
   zknub2201705]
FX This work is supported in part by the National Key R&D Program of China
   (2016YFB0502002), and in part by the Open Project Program of the
   National Laboratory of Pattern Recognition (201700004), the National
   Natural Science Foundation of China (61472419), the Natural Science
   Foundation of Henan Province (162300410347), the College Key Research
   Project of Henan Province (17A520018, 17A520019), the School-Based
   Project of Zhoukou Normal University (zknuc2015103, zknub2201705)
CR [Anonymous], 2005, P IEEE INT C IM PROC
   [Anonymous], 2011, P 24 INT C NEUR INF
   Bódis-Szomorú A, 2014, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2014.67
   Chauve AL, 2010, PROC CVPR IEEE, P1261, DOI 10.1109/CVPR.2010.5539824
   Chin TJ, 2012, IEEE T PATTERN ANAL, V34, P625, DOI 10.1109/TPAMI.2011.169
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu JJ, 2016, SYST CONTROL LETT, V93, P1, DOI 10.1016/j.sysconle.2016.03.006
   Furukawa Y., 2010, P COMP VIS PATT REC, P1422
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Isack H, 2010, INT J COMPUT VISION, V97, P123
   Jeong J, 2012, CASES'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON COMPILERS, ARCHITECTURES AND SYNTHESIS FOR EMBEDDED SYSTEMS, P191
   Jin Yu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2041, DOI 10.1109/CVPR.2011.5995608
   Lazic N, 2009, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2009.5459302
   Michailidis GT, 2017, VISUAL COMPUT, V33, P1347, DOI 10.1007/s00371-016-1230-3
   Micusík B, 2010, INT J COMPUT VISION, V89, P106, DOI 10.1007/s11263-010-0327-9
   MONSZPART A., 2015, ACM SIGGRAPH, V34, P103
   Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310
   Sinha SN, 2009, IEEE I CONF COMP VIS, P1881, DOI 10.1109/ICCV.2009.5459417
   Thakoor N., 2008, P COMP VIS PATT REC, P1
   Toldo R, 2008, LECT NOTES COMPUT SC, V5302, P537, DOI 10.1007/978-3-540-88682-2_41
   Verleysen C, 2016, PROC CVPR IEEE, P3327, DOI 10.1109/CVPR.2016.362
   Vincent E, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P182, DOI 10.1109/ISPA.2001.938625
   Wolberg G, 2018, VISUAL COMPUT, V34, P605, DOI 10.1007/s00371-017-1365-x
NR 24
TC 2
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 625
EP 638
DI 10.1007/s00371-018-1492-z
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900002
DA 2024-07-18
ER

PT J
AU Yang, CW
   Feng, HJ
   Xu, ZH
   Li, Q
   Chen, YT
AF Yang, Chenwei
   Feng, Huajun
   Xu, Zhihai
   Li, Qi
   Chen, Yueting
TI Correction of overexposure utilizing haze removal model and image fusion
   technique
SO VISUAL COMPUTER
LA English
DT Article
DE Overexposure; Image restoration; Dark channel prior; Weighted least
   squares filter; Image fusion
ID CONTRAST ENHANCEMENT; OVER-EXPOSURE; COLOR
AB This paper presents an efficient method for overexposure correction utilizing haze removal model and image fusion technique, which draws on the experience of HDR technique. Assuming an OE image can be modeled as a normal exposure image added up with a layer of asymmetrical colorful haze, its submerged information in OE regions is enhanced by an improved haze removal model based on dark channel prior. The enhancement result possesses better visualization in OE regions and color distortion to a certain extent. With the image fusion technique based on weighted least squares filters and global contrast-based saliency, the texture obtained in OE regions is utilized to restore the overexposure. The advantages of the selected image fusion technique are validated in the paper. In the experiments, the proposed method is compared with conventional methods to corroborate the performance. Both the subjective visualization and quantitative indicators show that the result is effective in correcting the overexposure without increasing pseudo-information and oversaturation.
C1 [Yang, Chenwei; Feng, Huajun; Xu, Zhihai; Li, Qi; Chen, Yueting] Zhejiang Univ, State Key Lab Modem Opt Instrumentat, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Yang, CW (corresponding author), Zhejiang Univ, State Key Lab Modem Opt Instrumentat, Hangzhou 310027, Zhejiang, Peoples R China.
EM 470427422@qq.com
OI Yang, Chenwei/0000-0001-7201-7205
CR Abebe MA, 2018, COMPUT VIS IMAGE UND, V168, P3, DOI 10.1016/j.cviu.2017.05.011
   Aggarwal M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P10, DOI 10.1109/ICCV.2001.937583
   [Anonymous], TIP
   [Anonymous], 1995, Studies in Optics
   [Anonymous], 18 IEEE INT S CONS E
   [Anonymous], TPAMI
   [Anonymous], TPAMI
   [Anonymous], TIP
   Arora S, 2015, 2015 3rd International Conference on Information and Communication Technology (ICoICT), P207, DOI 10.1109/ICoICT.2015.7231423
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Guo D, 2010, PROC CVPR IEEE, P515, DOI 10.1109/CVPR.2010.5540170
   Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167
   Hou LK, 2013, SIAM J IMAGING SCI, V6, P2213, DOI 10.1137/120888302
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Lee DH, 2014, IEEE T CONSUM ELECTR, V60, P173, DOI 10.1109/TCE.2014.6851990
   Masood SZ, 2009, COMPUT GRAPH FORUM, V28, P1861, DOI 10.1111/j.1467-8659.2009.01564.x
   Panetta KA, 2008, IEEE T SYST MAN CY B, V38, P174, DOI 10.1109/TSMCB.2007.909440
   Park D, 2013, INT CONF ACOUST SPEE, P2469, DOI 10.1109/ICASSP.2013.6638099
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shen C. T., 2012, SIGGRAPH ASIA 2012 T, P1
   Tan R, 2008, IEEE C COMPUTER VISI, P1
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tumblin J, 2005, PROC CVPR IEEE, P103
   Yoon YJ, 2014, SENSORS-BASEL, V14, P17159, DOI 10.3390/s140917159
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 30
TC 7
Z9 7
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 695
EP 705
DI 10.1007/s00371-018-1504-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900007
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Yan, FH
   Zhou, Z
AF Zhou, Yao
   Yan, Feihu
   Zhou, Zhong
TI Handling pure camera rotation in semi-dense monocular SLAM
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Semi-dense visual SLAM; Rotation-only camera motion; Direct method
AB In this paper, we present a method for semi-dense monocular simultaneous localization and mapping (SLAM) that is capable of dealing with pure camera rotation motion which brings forward a severe challenge for current direct (featureless) monocular SLAM approaches. A probabilistic depth map model built on Bayesian estimation is combined with the main framework of the state-of-the-art direct method LSD-SLAM. Using this model, both rotation-only and general camera motions could be tracked, and a consistent depth map could be built in real-time. Experimental results demonstrate the outstanding performance of the proposed system.
C1 [Zhou, Yao; Yan, Feihu; Zhou, Zhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Yan, FH (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM yfhmail@163.com; zz@buaa.edu.cn
FU National 863 Program of China [2015AA016403]; Natural Science Foundation
   of China [61472020, 61572061, 61602223]
FX This work is supported by the National 863 Program of China under Grant
   No. 2015AA016403 and the Natural Science Foundation of China under Grant
   Nos. 61472020, 61572061, 61602223.
CR [Anonymous], 2014, IEEE INT C ROB AUT
   [Anonymous], INT C COMP VIS
   [Anonymous], 2015, IEEE RSJ INT C INT R
   [Anonymous], 2015, IEEE RSJ INT C INT R
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Engel Jakob, 2014, EUR C COMP VIS
   Gauglitz S., 2012, IEEE INT S MIX AUGM
   Gruber L., 2010, IEEE INT S MIX AUG R
   Herrera D., 2014, INT C 3D VIS TOK JAP
   Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Murray David, 2007, IEEE ACM INT S MIX A
   Ott R, 2007, VISUAL COMPUT, V23, P843, DOI 10.1007/s00371-007-0159-y
   Pirchheim C, 2013, INT SYM MIX AUGMENT, P229, DOI 10.1109/ISMAR.2013.6671783
   Pizzoli M., 2014, IEEE INT C ROB AUT
   Reif R, 2008, VISUAL COMPUT, V24, P987, DOI 10.1007/s00371-008-0271-7
   Strasdat H, 2012, IMAGE VISION COMPUT, V30, P65, DOI 10.1016/j.imavis.2012.02.009
   Sturm J., 2012, IEEE RSJ INT C INT R
   Triggs B., 2000, LECT NOTES COMPUTER, V1883
   Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006
   Wang SD, 2010, VISUAL COMPUT, V26, P445, DOI 10.1007/s00371-010-0436-z
   Zhou Y., 2016, 6 INT C VIRT REAL VI
NR 23
TC 7
Z9 9
U1 4
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 123
EP 132
DI 10.1007/s00371-017-1435-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900011
DA 2024-07-18
ER

PT J
AU Shen, QQ
   Sheng, Y
   Chen, CK
   Zhang, GX
   Ugail, H
AF Shen, Qiqi
   Sheng, Yun
   Chen, Congkun
   Zhang, Guixu
   Ugail, Hassan
TI A PDE patch-based spectral method for progressive mesh compression and
   mesh denoising
SO VISUAL COMPUTER
LA English
DT Article
DE Spectral method; Mesh processing; Patchwise PDE; Progressive mesh
   compression; Mesh denoising
ID SURFACES
AB The development of the patchwise partial differential equation (PDE) framework a few years ago has paved the way for the PDE method to be used in mesh signal processing. In this paper, we, for the first time, extend the use of the PDE method to progressive mesh compression and mesh denoising. We, meanwhile, upgrade the existing patchwise PDE method in patch merging, mesh partitioning, and boundary extraction to accommodate mesh signal processing. In our new method, an arbitrary mesh model is partitioned into patches, each of which can be represented by a small set of coefficients of its PDE spectral solution. Since low-frequency components contribute more to the reconstructed mesh than high-frequency ones, we can achieve progressive mesh compression and mesh denoising by manipulating the frequency terms of the PDE solution. Experimental results demonstrate the feasibility of our method in both progressive mesh compression and mesh denoising.
C1 [Shen, Qiqi; Sheng, Yun; Chen, Congkun; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Ugail, Hassan] Univ Bradford, Ctr Visual Comp, Bradford BD7 1DP, W Yorkshire, England.
C3 East China Normal University; University of Bradford
RP Sheng, Y (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
EM ysheng@cs.ecnu.edu.cn
FU National Natural Science Foundation of China [61202291]
FX The authors would like to thank the reviewers for their constructive
   comments. The authors would also like to thank Xuequan Lu for his quick
   and responsible assistance during the revision of this paper. This work
   has been supported by the National Natural Science Foundation of China
   (61202291).
CR Ahmat N, 2011, INT J PHARMACEUT, V405, P113, DOI 10.1016/j.ijpharm.2010.12.006
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   Bloor MIG, 1995, J AIRCRAFT, V32, P1269, DOI 10.2514/3.46874
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Castro Gabriela Gonzalez, 2007, Journal of Multimedia, V2, P15, DOI 10.4304/jmm.2.6.15-25
   Chen CK, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1683
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Dekanski CW, 1997, J PROPUL POWER, V13, P398, DOI 10.2514/2.5177
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Du H, 2000, COMPUT GRAPH FORUM, V19, pC261, DOI 10.1111/1467-8659.00418
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Karni Z., 2001, Graphics Interface, V1, P1
   Karypis George, 1998, A Software Package for Partitioning Unstructured Graphs, Partitioning Meshes, and Computing Fill-Reducing Orderings of Sparse Matrices, P38
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kim B, 2005, COMPUT GRAPH FORUM, V24, P295, DOI 10.1111/j.1467-8659.2005.00854.x
   Kubiesa S, 2004, VISUAL COMPUT, V20, P682, DOI 10.1007/s00371-004-0261-3
   Lee H, 2012, VISUAL COMPUT, V28, P137, DOI 10.1007/s00371-011-0602-y
   Lu XQ, 2017, COMPUT AIDED GEOM D, V54, P49, DOI 10.1016/j.cagd.2017.02.011
   Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Pang MY, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P97, DOI 10.1109/CW.2010.13
   Pauly M, 2001, COMP GRAPH, P379, DOI 10.1145/383259.383301
   Peng JL, 2005, ACM T GRAPHIC, V24, P609, DOI 10.1145/1073204.1073237
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Sheng Y, 2011, MATH COMPUT MODEL, V54, P1536, DOI 10.1016/j.mcm.2011.04.025
   Sheng Y, 2010, VISUAL COMPUT, V26, P975, DOI 10.1007/s00371-010-0456-8
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Taubin G., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P123, DOI 10.1145/280814.280834
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Ugail H., 2007, INT J SHAPE MODELING, V13, P201, DOI DOI 10.1142/S0218654307001007
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   You LH, 2003, COMPUTING, V71, P353, DOI 10.1007/s00607-003-0028-0
   Zhang H, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P502
   Zhang HJ, 2002, COMPUT GRAPH-UK, V26, P89, DOI 10.1016/S0097-8493(01)00160-1
   Zhang JJ, 2004, COMPUT GRAPH FORUM, V23, P311, DOI 10.1111/j.1467-8659.2004.00762.x
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
NR 43
TC 8
Z9 9
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1563
EP 1577
DI 10.1007/s00371-017-1431-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700008
DA 2024-07-18
ER

PT J
AU Ugail, H
   Al-dahoud, A
AF Ugail, Hassan
   Al-dahoud, Ahmad
TI Is gender encoded in the smile? A computational framework for the
   analysis of the smile driven dynamic face for gender recognition
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY SEP 20-22, 2017
CL Univ Chester, Chester, ENGLAND
SP Eurograph Assoc, Int Federat Informat Proc, ACM SIGGRAPH
HO Univ Chester
DE Smile dynamics; Gender recognition; Machine learning; k-Nearest
   neighbour
ID EMOTION; WOMEN; MEN
AB Automatic gender classification has become a topic of great interest to the visual computing research community in recent times. This is due to the fact that computer-based automatic gender recognition has multiple applications including, but not limited to, face perception, age, ethnicity, identity analysis, video surveillance and smart human computer interaction. In this paper, we discuss a machine learning approach for efficient identification of gender purely from the dynamics of a person's smile. Thus, we show that the complex dynamics of a smile on someone's face bear much relation to the person's gender. To do this, we first formulate a computational framework that captures the dynamic characteristics of a smile. Our dynamic framework measures changes in the face during a smile using a set of spatial features on the overall face, the area of the mouth, the geometric flow around prominent parts of the face and a set of intrinsic features based on the dynamic geometry of the face. This enables us to extract 210 distinct dynamic smile parameters which form as the contributing features for machine learning. For machine classification, we have utilised both the Support Vector Machine and the k-Nearest Neighbour algorithms. To verify the accuracy of our approach, we have tested our algorithms on two databases, namely the CK+ and the MUG, consisting of a total of 109 subjects. As a result, using the k-NN algorithm, along with tenfold cross validation, for example, we achieve an accurate gender classification rate of over 85%. Hence, through the methodology we present here, we establish proof of the existence of strong indicators of gender dimorphism, purely in the dynamics of a person's smile.
C1 [Ugail, Hassan; Al-dahoud, Ahmad] Univ Bradford, Fac Engn & Informat, Ctr Visual Comp, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Bradford
RP Ugail, H (corresponding author), Univ Bradford, Fac Engn & Informat, Ctr Visual Comp, Bradford BD7 1DP, W Yorkshire, England.
EM h.ugail@bradford.ac.uk
CR Abel EL, 2010, PSYCHOL SCI, V21, P542, DOI 10.1177/0956797610363775
   Aifanti N., 2010, P 11 INT WORKSH IM A, DOI DOI 10.1371/JOURNAL.PONE.0009715
   Al-dahoud A., 2016, P INT C ADV COMP INT, P421
   ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   An XH, 2008, ADVANCES IN MATRIX THEORY AND ITS APPLICATIONS, VOL II, P1, DOI 10.1145/1399504.1360639
   [Anonymous], INT J ADV RES ELECT
   Antipov G, 2017, PATTERN RECOGN, V72, P15, DOI 10.1016/j.patcog.2017.06.031
   Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   BRITON NJ, 1995, J NONVERBAL BEHAV, V19, P49, DOI 10.1007/BF02173412
   Brody L.R., 2016, Handbook of Emotions, P369
   Bukar AM, 2017, IET COMPUT VIS, V11, P650, DOI 10.1049/iet-cvi.2016.0486
   Cashdan E, 1998, J NONVERBAL BEHAV, V22, P209, DOI 10.1023/A:1022967721884
   Dantcheva A, 2017, IEEE T INF FOREN SEC, V12, P719, DOI 10.1109/TIFS.2016.2632070
   DEUTSCH FM, 1987, PSYCHOL WOMEN QUART, V11, P341, DOI 10.1111/j.1471-6402.1987.tb00908.x
   DIMBERG U, 1990, BIOL PSYCHOL, V30, P151, DOI 10.1016/0301-0511(90)90024-Q
   DIMBERG U, 1990, BIOL PSYCHOL, V31, P137, DOI 10.1016/0301-0511(90)90013-M
   Ekman P., 2009, TELLING LIES CLUES D
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Hess U, 2004, EMOTION, V4, P378, DOI 10.1037/1528-3542.4.4.378
   Kalam S., 2014, INT J COMPUT APPL, V85, P32, DOI DOI 10.5120/14855-3222
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lian HC, 2006, LECT NOTES COMPUT SC, V3972, P202
   Liebart M., 2004, PERIO, V1, P17
   Lu L, 2009, INT CONF ACOUST SPEE, P1065, DOI 10.1109/ICASSP.2009.4959771
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mozaffari Saeed, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1192, DOI 10.1109/ICPR.2010.297
   Rai Preeti, 2014, 2014 9th International Conference on Industrial and Information Systems (ICIIS), P1, DOI 10.1109/ICIINFS.2014.7036569
   RUBIN LR, 1974, PLAST RECONSTR SURG, V53, P384, DOI 10.1097/00006534-197404000-00002
   Saleem MA, 2016, PROCEEDINGS OF 2016 FUTURE TECHNOLOGIES CONFERENCE (FTC), P116, DOI 10.1109/FTC.2016.7821598
   Simon RW, 2004, AM J SOCIOL, V109, P1137, DOI 10.1086/382111
   Surakka V, 1998, INT J PSYCHOPHYSIOL, V29, P23, DOI 10.1016/S0167-8760(97)00088-3
   Van Boxtel Anton., 2010, P OF MEASURING BEHAV, P104, DOI DOI 10.1145/1931344.1931382
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
NR 33
TC 13
Z9 14
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1243
EP 1254
DI 10.1007/s00371-018-1494-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000009
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Bensebaa, A
   Larabi, S
AF Bensebaa, Amina
   Larabi, Slimane
TI Direction estimation of moving pedestrian groups for intelligent
   vehicles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Directional areas; Pedestrian group; Intelligent vehicles; Video
   surveillance; Motion analysis; Silhouette
AB In this paper, we consider direction estimation of pedestrian group for video surveillance and intelligent vehicles applications. A theoretical study of the position of vanishing points in image plane associated with all directions in the scene leads to the definition of the notion of directional areas. Image plane is divided along the x-axis into a set of bounded areas; each one is associated with a specific direction. The pedestrian direction is inferred directly depending on the belonging area of the vanishing points computed from video sequence. Top and bottom points of walking pedestrian define two parallel lines in 3D. The vanishing point is estimated from video sequence and from the direction of the pedestrian. The obtained results demonstrate the efficacy and robustness of the proposed method and confirm the improvement with respect to state-of-the-art approaches.
C1 [Bensebaa, Amina; Larabi, Slimane] USTHB Univ, Dept Comp Sci, Bab Ezzouar, Algeria.
C3 University Science & Technology Houari Boumediene
RP Larabi, S (corresponding author), USTHB Univ, Dept Comp Sci, Bab Ezzouar, Algeria.
EM slarabi@usthb.dz
RI Larabi, Slimane/AAD-7871-2020
OI larabi, slimane/0000-0001-8994-5980
CR [Anonymous], IEEE INT VEH S GOLD
   [Anonymous], 2016, P ICCAD
   Baltieri D, 2012, LECT NOTES COMPUT SC, V7576, P270, DOI 10.1007/978-3-642-33715-4_20
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Cheng Chen, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P5, DOI 10.1109/AVSS.2011.6027284
   Cucchiara R., 2001, IEEE INT TRANSP SYST
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Flohr F., 2014, IEEE INT VEH S IV DE
   Guan JZ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16050654
   Junejo IN, 2011, MACH VISION APPL, V22, P137, DOI 10.1007/s00138-009-0210-2
   Liu H, 2015, IEEE IMAGE PROC, P1568, DOI 10.1109/ICIP.2015.7351064
   Lv F., 2002, IEEE INT C PATT REC, P281
   Raman R, 2016, IEEE ACCESS, V4, P5788, DOI 10.1109/ACCESS.2016.2608844
   Raza M, 2018, NEUROCOMPUTING, V272, P647, DOI 10.1016/j.neucom.2017.07.029
   Ricci E, 2015, IEEE I CONF COMP VIS, P4660, DOI 10.1109/ICCV.2015.529
   Shimizu H., 2004, IEEE INT VEH S GOLD
   Tao J., C IEEE INT C COMP VI, P230
   Varadarajan J, 2018, INT J COMPUT VISION, V126, P410, DOI 10.1007/s11263-017-1026-6
   Yu Shiqi, 2006, ICPR
   Zhao Guangzhe, 2012, Journal of Electronics (China), V29, P72, DOI 10.1007/s11767-012-0814-y
NR 20
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1109
EP 1118
DI 10.1007/s00371-018-1520-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400032
DA 2024-07-18
ER

PT J
AU Zhang, XL
   Liu, SG
AF Zhang, Xiaoli
   Liu, Shiguang
TI Contrast preserving image decolorization combining global features and
   local semantic features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Image decolorization; Convolution neural network; Local semantic
   features; Global features
ID COLOR
AB Image decolorization known as the process to transform a color image to a grayscale one is widely used in single-channel image processing, black and white printing, etc. It is a dimension reduction process which inevitably suffers from information loss. The general goal of image decolorization is to preserve the color contrast of the color image. Traditional image decolorization methods are generally divided into local methods and global methods. However, local methods are not accurate enough to process local pixel blocks which may tend to cause local artifacts. While global methods cannot deal well in local color blocks, which are usually time-consuming, too. Therefore, this paper presents a way to combine the local semantic features and the global features. The traditional image decolorization method uses the low-level features of an image. Instead, in this paper, the convolution neural network is used to learn the global features and local semantic features of an image which can better preserve the contrast in both local color blocks and adjacent pixels of the color image. Finally, the global features and the local semantic features are combined to decolorize the image. Experiments indicate that our method outperforms the state of the arts in terms of contrast preservation.
C1 [Zhang, Xiaoli] Tianjin Univ, Sch Software, Tianjin, Peoples R China.
   [Liu, Shiguang] Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
C3 Tianjin University; Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
EM lsg@tju.edu.cn
FU Natural Science Foundation of China [61672375, 61170118]
FX This study was funded by the Natural Science Foundation of China (Grant
   Numbers 61672375 and 61170118).
CR Ancuti CO, 2011, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2011.5995414
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2010, P AS C COMP VIS
   [Anonymous], 2016, PROC IEEE INT C UBIQ
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Hou X.G., 2017, THESIS JILIN U CHANG, P1
   HUNTER RS, 1958, J OPT SOC AM, V48, P985, DOI 10.1364/JOSA.48.000985
   Ji Z. P., 2016, VISUAL COMPUT, V32, P1
   Kim Y. J., 2009, ACM SIGGRAPH ASIA
   Lin Z. C., 2008, MSRTR2008189
   Liu QG, 2015, IEEE T IMAGE PROCESS, V24, P2889, DOI 10.1109/TIP.2015.2423615
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   LU C., 2012, SIGGRAPH ASIA 2012 T, P1
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   Qin L, 2012, 2012 IEEE FIFTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P1, DOI [10.1109/ICACI.2012.6463111, 10.1109/ICCH.2012.6724460]
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Song YX, 2013, SPRINGER SER MATER S, V186, P1, DOI 10.1007/978-1-4614-8121-8_1
   Sowmya V., 2016, SIGNAL IMAGE VIDEO P, V11, P1
   Wu TR, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.4.043004
   Wyszecki G., 1968, PHYS TODAY, V21, P83, DOI DOI 10.1063/1.3035025
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhang H. C., 2017, INT C IM GRAPH, P560
NR 26
TC 22
Z9 23
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1099
EP 1108
DI 10.1007/s00371-018-1524-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400031
DA 2024-07-18
ER

PT J
AU Oh, YJ
   Shin, Y
   Lee, IK
AF Oh, Young Jin
   Shin, Yeonbi
   Lee, In-Kwon
TI Efficient oriented particle arrangements for position-based dynamics
   simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Position-based dynamics; Oriented particle; Soft body simulation
ID SEGMENTATION; APPROXIMATION
AB We propose two methods to improve the arrangement of oriented particles for position-based dynamics simulation. The first method, within object particle arrangement, segments a target mesh and places a single ellipsoidal particle in each segment. Because the number of oriented particles for simulation is smaller than the number used in a conventional arrangement method which randomly places spherical particles on the target mesh's surface, we can calculate simulation results more quickly. The second method, on surface particle arrangement, which arranges ellipsoidal particles on the surface of the target mesh, behaves similarly to the conventional method. However, we improve the conventional method by optimizing the position and radiuses of the particles to solve the problem of the protrusion of particles from the mesh surface, which produces inaccurate collision handling results. Based on the results of various experiments, we show that simulations using the oriented particle structures constructed by the proposed methods are more efficient and accurate than those conducted using the conventional method. In addition, we compare the soft body simulation characteristics that appear based on the two proposed methods.
C1 [Oh, Young Jin; Lee, In-Kwon] Yonsei Univ, Dept Comp Sci, Seoul, South Korea.
   [Shin, Yeonbi] LG Elect, H&A Air Solut Seoul Lab, Seoul, South Korea.
C3 Yonsei University; LG Electronics
RP Lee, IK (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul, South Korea.
EM skrcjstk@gmail.com; yeonbi.shin@lge.com; iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022
OI Lee, In-Kwon/0000-0002-1534-1882
CR Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Bender J., 2013, EUROGRAPHICS 2013 State of the Art Reports
   Bischoff S, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P480, DOI 10.1109/TDPVT.2002.1024103
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Lu L, 2007, COMPUT GRAPH FORUM, V26, P329, DOI 10.1111/j.1467-8659.2007.01055.x
   Macklin M, 2016, P 9 INT C MOT GAM, P49, DOI [10.1145/2994258.2994272, DOI 10.1145/2994258.2994272]
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964987
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Muller M., 2011, WORKSH VIRT REAL INT, DOI [10.2312/PE/vriphys/vriphys11/083-091, DOI 10.2312/PE/VRIPHYS/VRIPHYS11/083-091]
   Muller Matthias, 2016, P 9 INT C MOT GAM MI, P31, DOI DOI 10.1145/2994258.2994260
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shlens J., 2005, ARXIV
   Simari P. D., 2005, Graphics Interface, P161
   Svanberg K, 2001, SIAM J OPTIMIZ, V12, P555
   Wang R, 2006, VISUAL COMPUT, V22, P612, DOI 10.1007/s00371-006-0052-0
   Yaz I. O, 2016, CGAL USER REFERENCE
   Zabih R, 2004, PROC CVPR IEEE, P437
NR 19
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 507
EP 516
DI 10.1007/s00371-017-1356-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400005
DA 2024-07-18
ER

PT J
AU Max, N
   Duff, T
   Mildenhall, B
   Yan, YJ
AF Max, Nelson
   Duff, Tom
   Mildenhall, Ben
   Yan, Yajie
TI Approximations for the distribution of microflake normals
SO VISUAL COMPUTER
LA English
DT Article
DE Microflake; Sparse voxel octree; Volume rendering; Distribution of
   normals
AB Scenes in computer animation can have extreme complexity, especially when high resolution objects are placed in the distance and occupy only a few pixels. A useful technique for level of detail in these cases is to use a sparse voxel octree containing both hard surfaces and a participating medium consisting of microflakes. In this paper, we discuss three different methods for approximating the distribution of normals of the microflakes, which is needed to compute extinction, inscattering of attenuated direct illumination, and multiple scattering in the participating medium. Specifically, we consider (a) k means approximation with k weighted representatives, (b) expansion in spherical harmonics, and (c) the distribution of the normals of a specific ellipsoid. We compare their image quality, data size, and computation time.
C1 [Max, Nelson; Duff, Tom] Pixar Animat Studios, Emeryville, CA 94608 USA.
   [Max, Nelson] Univ Calif Davis, Davis, CA 95616 USA.
   [Mildenhall, Ben] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Yan, Yajie] Washington Univ, St Louis, MO USA.
C3 University of California System; University of California Davis;
   University of California System; University of California Berkeley;
   Washington University (WUSTL)
RP Max, N (corresponding author), Pixar Animat Studios, Emeryville, CA 94608 USA.; Max, N (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM max@cs.ucdavis.edu; td@pixar.com; bmild@berkeley.edu; yajieyan@wustl.edu
RI Yan, Yajie/ABP-7888-2022
OI Yan, Yajie/0000-0002-2508-850X
FU University of California, Davis
FX We thank Mark Meyer, Tony DeRose, Eric Heitz, Wojciech Jarosz, Derek
   Nowrouzezahrai and Ted Kim for helpful discussions, and the SIGGRAPH,
   Pacific Graphics, and Visual Computer reviewers for useful suggestions.
   Nelson Max thanks the University of California, Davis for sabbatical
   salary.
CR [Anonymous], 2014, THESIS
   [Anonymous], 2011, ACM T GRAPH
   Bruneton E, 2012, IEEE T VIS COMPUT GR, V18, P242, DOI 10.1109/TVCG.2011.81
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   Cook Robert L, 1987, ACM_SIGGRAPH_Computer_Graphics, V21, P95
   Crow F. C., 1982, Computer Graphics, V16, P9, DOI 10.1145/965145.801253
   Green R., 2003, ARCH GAME DEV C, V56
   Heitz E., 2015, SGGX MICROFLAKE DI S
   Heitz E., 2012, EUR ACM SIGGRAPH S H, DOI [10.2312/EGGH/HPG12/125-134, DOI 10.2312/EGGH/HPG12/125-134]
   Heitz E., 2014, J. Comput. Graph. Tech, V3, P32
   Heitz E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766988
   Heitz Eric, 2013, P ACM SIGGRAPH S INT, P129
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Jarosz W, 2009, COMPUT GRAPH FORUM, V28, P577, DOI 10.1111/j.1467-8659.2009.01398.x
   Jensen H. W., 2012, REALISTIC IMAGE SYNT
   Joy K. I., 1988, TUTORIAL COMPUTER GR, P2
   Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003
   Kaplanyan, 2010, ADV REAL TIME RENDER
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Max N., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P239
   Moon JT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360630
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   OLANO M, 2010, LEAN MAPPING, P181, DOI [10.1145/1730804.1730834, DOI 10.1145/1730804.1730834]
   Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448
   RENSE WA, 1950, J OPT SOC AM, V40, P55, DOI 10.1364/JOSA.40.000055
   Sloan P. P., EXP MAT PRES 2008 GA
   Toksvig M., 2005, Journal of Graphics Tools, V10, P65
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   Walter B., 2007, EUROGRAPHICS C RENDE
   [No title captured]
NR 32
TC 1
Z9 1
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2018
VL 34
IS 3
BP 443
EP 457
DI 10.1007/s00371-017-1352-2
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW2CZ
UT WOS:000425110900011
DA 2024-07-18
ER

PT J
AU Namane, R
   Miguet, S
   Oulebsir, FB
AF Namane, Rachid
   Miguet, Serge
   Oulebsir, Fatima Boumghar
TI A fast voxelization algorithm for trilinearly interpolated isosurfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Voxelized isosurface; Dividing cubes; Trilinear interpolant; Incremental
   algorithm
ID MARCHING CUBES
AB In this work, we propose a new method for a fast incremental voxelization of isosurfaces obtained by the trilinear interpolation of 3D data. Our objective consists in the fast generation of subvoxelized isosurfaces extracted by a point-based technique similar to the Dividing Cubes algorithm. Our technique involves neither an exhaustive scan search process nor a graph-based search approach when generating isosurface points. Instead an optimized incremental approach is adopted here for a rapid isosurface extraction. With a sufficient sampling subdivision criteria around critical points, the extracted isosurface is both correct and topologically consistent with respect to the piecewise trilinear interpolant. Furthermore, the discretization scheme used in our method ensures obtaining thin - one voxel width - isosurfaces as compared to the one given by the Dividing Cubes algorithm. The resultant subvoxelized isosurfaces are efficiently tested against all possible configurations of the trilinear interpolant and real-world datasets.
C1 [Namane, Rachid] UMBB, Inst Elect & Elect Engn, Boumerdes, Algeria.
   [Namane, Rachid; Oulebsir, Fatima Boumghar] USTHB, Comp Sci & Elect Fac, LRPE Lab, ParIMed Team, Algiers, Algeria.
   [Miguet, Serge] Lyon 2 Univ, LIRIS Lab, Lyon, France.
C3 Universite de M'hammed Bougara Boumerdes; University Science &
   Technology Houari Boumediene; Institut National des Sciences Appliquees
   de Lyon - INSA Lyon
RP Namane, R (corresponding author), UMBB, Inst Elect & Elect Engn, Boumerdes, Algeria.; Namane, R (corresponding author), USTHB, Comp Sci & Elect Fac, LRPE Lab, ParIMed Team, Algiers, Algeria.
EM rdnamane@gmail.com; serge.miguet@univ-lyon2.fr; fboumghar@usthb.dz
RI Cherifi, Hocine/X-9376-2019
OI Cherifi, Hocine/0000-0001-9124-4921; Oulebsir Boumghar,
   Fatima/0000-0002-3403-7026
FU Algerian Ministry of Higher Education and Research
FX These results were obtained during an 18 months internship in LIRIS
   Laboratory, Lyon2 University of France. Therefore, we would like to
   express our gratitude to all the members of the LIRIS-M2DisCo team for
   their valuable feedback and guidance that helped us significantly
   throughout this work. The work was funded by Algerian Ministry of Higher
   Education and Research.
CR BOUMGHAR F, 1996, LECT NOTES COMPUTER, P269
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Carr H, 2010, IEEE T VIS COMPUT GR, V16, P533, DOI 10.1109/TVCG.2009.10
   Chernyaev E.V., 1995, 9517 CERN CN
   CLINE HE, 1988, MED PHYS, V15, P320, DOI 10.1118/1.596225
   Co CS, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P325, DOI 10.1109/PCCGA.2003.1238274
   Custodio L, 2013, COMPUT GRAPH-UK, V37, P840, DOI 10.1016/j.cag.2013.04.004
   JI GF, 2008, P IEEE PAC VIS S, P25
   Jonas A, 1997, PATTERN RECOGN, V30, P1803, DOI 10.1016/S0031-3203(97)00011-3
   Kebaili A., 2000, Machine Graphics & Vision, V9, P281
   KONG TY, 1989, COMPUT VISION GRAPH, V48, P357, DOI 10.1016/0734-189X(89)90147-3
   Liu XW, 2002, P I MECH ENG B-J ENG, V216, P459, DOI 10.1243/0954405021519979
   Livnat Y., P VISUALIZATION, V04, P457
   Lopes A, 2003, IEEE T VIS COMPUT GR, V9, P16, DOI 10.1109/TVCG.2003.1175094
   Lorensen W. E., COMPUT GRAPH, V21, P163
   NATARAJAN BK, 1994, VISUAL COMPUT, V11, P52, DOI 10.1007/BF01900699
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Nielson GM, 2003, IEEE T VIS COMPUT GR, V9, P283, DOI 10.1109/TVCG.2003.1207437
   Sohn BS, 2009, KSII T INTERNET INF, V3, P667, DOI 10.3837/tiis.2009.06.006
   Sreevalsan-Nair J., INT C COMPUT GRAPH T
   Theisel H, 2002, COMPUT GRAPH FORUM, V21, P19, DOI 10.1111/1467-8659.00563
   Zhang HT, 2006, IEEE T VIS COMPUT GR, V12, P1267, DOI 10.1109/TVCG.2006.153
NR 22
TC 2
Z9 2
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 5
EP 20
DI 10.1007/s00371-016-1306-0
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, Z
   Xiong, HL
AF Yang, Zhen
   Xiong, Huilin
TI Computing object-based saliency via locality-constrained linear coding
   and conditional random fields
SO VISUAL COMPUTER
LA English
DT Article
DE Top-down model; Locality-constrained linear coding; Conditional random
   field; Object-based saliency
ID RECOGNITION; ATTENTION; FRAMEWORK
AB Predicting object location using a top-down saliency model has grown increasingly popular in recent years. In this work, we combine locality-constrained linear coding (LLC) with a conditional random field (CRF), and construct a top-down saliency model to generate a specific object-based saliency map. During the training phase, we use the LLC codes as the latent variables of the CRF model, and meanwhile learn a class-specific codebook by CRF modulation. In the testing phase, we use this top-down model to distinguish specific objects from a cluttered background. Finally, we evaluate the experimental results on the MSRA-B, Garz-02, Weizmann Horse, and Plane datasets by applying the developed object-based saliency model. The performance shows that our approach can not only improve the precision but also dramatically reduce the computational complexity.
C1 [Yang, Zhen; Xiong, Huilin] Shanghai Jiao Tong Univ, Dept Automat, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Yang, Z (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM yangzhen5771@sjtu.edu.cn; hlxiong@sjtu.edu.cn
FU National Natural Foundation of China [61375008]
FX This work was supported by the National Natural Foundation of China
   under Grant no. 61375008. We thank LetPub (http://www.letpub.com) for
   its linguistic assistance during the preparation of this manuscript.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aldavert D, 2010, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR.2010.5540098
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2004, P C COMP VIS PATT RE, DOI DOI 10.1109/CVPR.2004.314
   Bergbauer J., 2013, P SOC PHOTO-OPT INS, V8651
   Bertelli L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2153, DOI 10.1109/CVPR.2011.5995597
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   Elazary L, 2008, J VISION, V8, DOI 10.1167/8.3.3
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   Khan FS, 2009, IEEE I CONF COMP VIS, P979, DOI 10.1109/ICCV.2009.5459362
   Kocak A., 2014, P BRIT MACH VIS C
   Lafferty John, 2001, INT C MACH LEARN ICM
   Liu T., 2007, LEARNING DETECT SALI
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Mannan SK, 2009, CURR BIOL, V19, pR247, DOI 10.1016/j.cub.2009.02.020
   Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54
   Qi W, 2017, VISUAL COMPUT, V33, P209, DOI 10.1007/s00371-015-1176-x
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Shi YJ, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.061113
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Teuber HL, 1955, ANNU REV PSYCHOL, V6, P267, DOI 10.1146/annurev.ps.06.020155.001411
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709
   Wangjiang Zhu Y. W. J. S., 2014, SALIENCY OPTIMIZATIO
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Xu M, 2015, VISUAL COMPUT, V31, P355, DOI 10.1007/s00371-014-0930-9
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   YANG JM, 2012, PROC CVPR IEEE, P2296, DOI [DOI 10.1109/CVPR.2012.6247940, 10.1109/CVPR.2012.6247940]
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhang J., 2015, 2015 IEEE INT C COMP
   Zhang WJ, 2016, VISUAL COMPUT, V32, P275, DOI 10.1007/s00371-015-1065-3
   Zhong GY, 2016, VISUAL COMPUT, V32, P611, DOI 10.1007/s00371-015-1077-z
NR 44
TC 3
Z9 3
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1403
EP 1413
DI 10.1007/s00371-016-1287-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100004
DA 2024-07-18
ER

PT J
AU Argudo, O
   Andujar, C
   Chica, A
   Guérin, E
   Digne, J
   Peytavie, A
   Galin, E
AF Argudo, Oscar
   Andujar, Carlos
   Chica, Antonio
   Guerin, Eric
   Digne, Julie
   Peytavie, Adrien
   Galin, Eric
TI Coherent multi-layer landscape synthesis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Coherent multi-layer landscapes; Dictionary matching; Example-based
   modeling
ID TERRAIN GENERATION
AB We present an efficient method for generating coherent multi-layer landscapes. We use a dictionary built from exemplars to synthesize high-resolution fully featured terrains from input low-resolution elevation data. Our example-based method consists in analyzing real-world terrain examples and learning the procedural rules directly from these inputs. We take into account not only the elevation of the terrain, but also additional layers such as the slope, orientation, drainage area, the density and distribution of vegetation, and the soil type. By increasing the variety of terrain exemplars, our method allows the user to synthesize and control different types of landscapes and biomes, such as temperate or rain forests, arid deserts and mountains.
C1 [Argudo, Oscar; Andujar, Carlos; Chica, Antonio] Univ Politecn Cataluna, Comp Sci Dept, ViRVIG, Barcelona, Spain.
   [Guerin, Eric] Univ Lyon, LIRIS, CNRS, INSA Lyon, F-69621 Villeurbanne, France.
   [Galin, Eric] Univ Lyon 2, LIRIS, CNRS, F-69676 Villeurbanne, France.
   [Digne, Julie; Peytavie, Adrien] Univ Lyon 1, LIRIS, CNRS, F-69622 Villeurbanne, France.
C3 Universitat Politecnica de Catalunya; Institut National des Sciences
   Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS); Centre National de la Recherche Scientifique
   (CNRS); Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Universite Claude Bernard
   Lyon 1
RP Argudo, O (corresponding author), Univ Politecn Cataluna, Comp Sci Dept, ViRVIG, Barcelona, Spain.
EM oargudo@cs.upc.edu
RI Chica, Antonio/K-7979-2014; Peytavie, Adrien/X-2253-2019; Galin,
   Eric/X-1938-2019; Guérin, Eric/X-2241-2019; Andujar, Carlos/K-3692-2014
OI Galin, Eric/0000-0002-5946-4112; Guérin, Eric/0000-0002-2189-2728;
   Digne, Julie/0000-0003-0905-0840; Argudo, Oscar/0000-0003-3943-1839;
   Peytavie, Adrien/0000-0002-6994-9164; Andujar,
   Carlos/0000-0002-8480-4713
FU Spanish Ministry of Economy and Competitiveness; FEDER
   [TIN2014-52211-C2-1-R]; Spanish Ministry of Education, Culture and
   Sports [FPU13/01079]; Fonds National pour la Societe Numerique; project
   HDW [ANR-16-CE33-0001]
FX This work has been partially funded by the Spanish Ministry of Economy
   and Competitiveness and FEDER Grant TIN2014-52211-C2-1-R, the Spanish
   Ministry of Education, Culture and Sports Grant FPU13/01079. This work
   is part of the project PAPAYA, funded by the Fonds National pour la
   Societe Numerique, and project HDW ANR-16-CE33-0001.
CR Alsweis M., 2005, EUR WORKSH NAT PHEN
   Chiba N, 1998, J VISUAL COMP ANIMAT, V9, P185, DOI 10.1002/(SICI)1099-1778(1998100)9:4<185::AID-VIS178>3.0.CO;2-2
   Cordonnier G, 2016, COMPUT GRAPH FORUM, V35, P165, DOI 10.1111/cgf.12820
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Deussen Oliver., 2006, Digital Design of Nature: Computer Generated Plants and Organics
   Emilien A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766975
   Emilien A, 2012, VISUAL COMPUT, V28, P809, DOI 10.1007/s00371-012-0699-7
   Gain J, 2015, COMPUT GRAPH FORUM, V34, P105, DOI 10.1111/cgf.12545
   Gain J., 2009, P 2009 S INT 3D GRAP, V1, P31, DOI [10.1145/1507149.1507155, DOI 10.1145/1507149.1507155]
   Génevaux JD, 2015, COMPUT GRAPH FORUM, V34, P198, DOI 10.1111/cgf.12530
   Génevaux JD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461996
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P177, DOI 10.1111/cgf.12821
   Han C., 2008, ACM SIGGRAPH
   Hnaidi H, 2010, COMPUT GRAPH FORUM, V29, P2179, DOI 10.1111/j.1467-8659.2010.01806.x
   Kelley A. D., 1988, Computer Graphics, V22, P263, DOI 10.1145/378456.378519
   Lane B, 2002, PROC GRAPH INTERF, P69
   Mech R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P397, DOI 10.1145/237170.237279
   Musgrave F. K., 1989, Computer Graphics, V23, P41, DOI 10.1145/74334.74337
   Nagashima K, 1997, VISUAL COMPUT, V13, P456, DOI 10.1007/s003710050117
   Prusinkiewicz P., 1993, Proceedings Graphics Interface '93, P174
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wei L.Y., 2009, EUROGRAPHICS STATE A
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
NR 26
TC 18
Z9 18
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 1005
EP 1015
DI 10.1007/s00371-017-1393-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800030
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bi, L
   Kim, J
   Kumar, A
   Fulham, M
   Feng, DG
AF Bi, Lei
   Kim, Jinman
   Kumar, Ashnil
   Fulham, Michael
   Feng, Dagan
TI Stacked fully convolutional networks with multi-channel learning:
   application to medical image segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th International Conference on Computer Graphics (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Fully convolutional networks (FCNs); Segmentation; Regions of interest
   (ROI)
ID TUMOR SEGMENTATION; NEURAL-NETWORKS; DEEP; MODEL; DIAGNOSIS
AB The automated segmentation of regions of interest (ROIs) in medical imaging is the fundamental requirement for the derivation of high-level semantics for image analysis in clinical decision support systems. Traditional segmentation approaches such as region-based depend heavily upon hand-crafted features and a priori knowledge of the user. As such, these methods are difficult to adopt within a clinical environment. Recently, methods based on fully convolutional networks (FCN) have achieved great success in the segmentation of general images. FCNs leverage a large labeled dataset to hierarchically learn the features that best correspond to the shallow appearance as well as the deep semantics of the images. However, when applied to medical images, FCNs usually produce coarse ROI detection and poor boundary definitions primarily due to the limited number of labeled training data and limited constraints of label agreement among neighboring similar pixels. In this paper, we propose a new stacked FCN architecture with multi-channel learning (SFCN-ML). We embed the FCN in a stacked architecture to learn the foreground ROI features and background non-ROI features separately and then integrate these different channels to produce the final segmentation result. In contrast to traditional FCN methods, our SFCN-ML architecture enables the visual attributes and semantics derived from both the fore- and background channels to be iteratively learned and inferred. We conducted extensive experiments on three public datasets with a variety of visual challenges. Our results show that our SFCN-ML is more effective and robust than a routine FCN and its variants, and other state-of-the-art methods.
C1 [Bi, Lei; Kim, Jinman; Kumar, Ashnil; Fulham, Michael; Feng, Dagan] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
   [Fulham, Michael] Royal Prince Alfred Hosp, Dept Mol Imaging, Sydney, NSW, Australia.
   [Fulham, Michael] Univ Sydney, Sydney Med Sch, Sydney, NSW, Australia.
   [Feng, Dagan] Shanghai Jiao Tong Univ, Med X Res Inst, Shanghai, Peoples R China.
C3 University of Sydney; University of Sydney; NSW Health; Royal Prince
   Alfred Hospital; University of Sydney; Shanghai Jiao Tong University
RP Kim, J (corresponding author), Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
EM jinman.kim@sydney.edu.au
RI Kim, Jin/AAS-5810-2021; Kim, Jin Man/HJO-8987-2023
OI Kim, Jin/0000-0002-7667-9588; Feng, Dagan/0000-0002-3381-214X; kim,
   jinman/0000-0001-5960-1060; Kumar, Ashnil/0000-0001-6236-7324; , Lei
   Bi/0000-0001-9759-0200; Fulham, Michael/0000-0003-0602-6319
CR Ahn E, 2017, IEEE J BIOMED HEALTH, V21, P1685, DOI 10.1109/JBHI.2017.2653179
   [Anonymous], ACTIVE CONTOUR SEGME
   [Anonymous], CLIN DECISION SUPPOR
   [Anonymous], PATTERN RECOGNIT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], COMPUT MED IMAGING G
   [Anonymous], 2014, BMVC
   [Anonymous], 2015, P ICLR
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   BenTaieb Aicha, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P460, DOI 10.1007/978-3-319-46723-8_53
   Bi L, 2012, INT C DIG IM COMP TE, P1
   Bi L, 2014, LECT NOTES COMPUT SC, V8673, P569, DOI 10.1007/978-3-319-10404-1_71
   Bi L, 2013, IEEE ENG MED BIO, P5453, DOI 10.1109/EMBC.2013.6610783
   Celebi ME, 2013, SKIN RES TECHNOL, V19, pE252, DOI 10.1111/j.1600-0846.2012.00636.x
   Cha KH, 2016, MED PHYS, V43, P1882, DOI 10.1118/1.4944498
   Chen H, 2017, MED IMAGE ANAL, V36, P135, DOI 10.1016/j.media.2016.11.004
   Chen XJ, 2012, IEEE T IMAGE PROCESS, V21, P2035, DOI 10.1109/TIP.2012.2186306
   Ciresan D., 2012, ADV NEURAL INFORM PR, V25, P2843
   Corrado G., 2012, P 25 INT C NEUR INF, P1223
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doi K, 2005, BRIT J RADIOL, V78, pS3, DOI 10.1259/bjr/82933343
   Farag A, 2017, IEEE T IMAGE PROCESS, V26, P386, DOI 10.1109/TIP.2016.2624198
   Fu HZ, 2016, I S BIOMED IMAGING, P698, DOI 10.1109/ISBI.2016.7493362
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Hogeweg L, 2012, MED IMAGE ANAL, V16, P1490, DOI 10.1016/j.media.2012.06.009
   Hu P., 2016, Int. J. Comput. Assist. Radiol. Surg., P1
   Isgum I, 2009, IEEE T MED IMAGING, V28, P1000, DOI 10.1109/TMI.2008.2011480
   Kumar A, 2017, IEEE J BIOMED HEALTH, V21, P31, DOI 10.1109/JBHI.2016.2635663
   Lartizien C, 2014, IEEE J BIOMED HEALTH, V18, P946, DOI 10.1109/JBHI.2013.2283658
   Li BN, 2011, COMPUT BIOL MED, V41, P1, DOI 10.1016/j.compbiomed.2010.10.007
   Li CY, 2013, IEEE J BIOMED HEALTH, V17, P92, DOI 10.1109/TITB.2012.2227273
   Li CY, 2012, COMPUT METH PROG BIO, V107, P164, DOI 10.1016/j.cmpb.2011.07.005
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li X, 2013, IEEE I CONF COMP VIS, P3328, DOI 10.1109/ICCV.2013.413
   Lin L, 2016, IEEE T CYBERNETICS, V46, P2796, DOI 10.1109/TCYB.2015.2489719
   Lu F, 2017, INT J COMPUT ASS RAD, V12, P171, DOI 10.1007/s11548-016-1467-3
   Nock R, 2004, IEEE T PATTERN ANAL, V26, P1452, DOI 10.1109/TPAMI.2004.110
   O'Neill GT, 2012, VISUAL COMPUT, V28, P205, DOI 10.1007/s00371-011-0636-1
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Qi Dou, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P149, DOI 10.1007/978-3-319-46723-8_18
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth HR, 2015, LECT NOTES COMPUT SC, V9349, P556, DOI 10.1007/978-3-319-24553-9_68
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Shotton J, 2008, PROC CVPR IEEE, P1245
   Silveira M, 2009, IEEE J-STSP, V3, P35, DOI 10.1109/JSTSP.2008.2011119
   Sirinukunwattana K, 2015, IEEE T MED IMAGING, V34, P2366, DOI 10.1109/TMI.2015.2433900
   Somkantha K, 2011, IEEE T BIO-MED ENG, V58, P567, DOI 10.1109/TBME.2010.2091129
   Song Y, 2013, LECT NOTES COMPUT SC, V8149, P284, DOI 10.1007/978-3-642-40811-3_36
   Song Y, 2012, IEEE T MED IMAGING, V31, P1061, DOI 10.1109/TMI.2012.2185057
   van Rikxoort EM, 2010, MED IMAGE ANAL, V14, P39, DOI 10.1016/j.media.2009.10.001
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wong A, 2011, IEEE T INF TECHNOL B, V15, P929, DOI 10.1109/TITB.2011.2157829
   Yan Xu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P496, DOI 10.1007/978-3-319-46723-8_57
   Zhang Y, 2008, MEDIVIS 2008: FIFTH INTERNATIONAL CONFERENCE BIOMEDICAL VISUALIZATION - INFORMATION VISUALIZATION IN MEDICAL AND BIOMEDICAL INFORMATICS, PROCEEDINGS, P71, DOI 10.1109/MediVis.2008.12
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 58
TC 41
Z9 44
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 1061
EP 1071
DI 10.1007/s00371-017-1379-4
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800035
DA 2024-07-18
ER

PT J
AU Hristova, H
   Le Meur, O
   Cozot, R
   Bouatouch, K
AF Hristova, Hristina
   Le Meur, Olivier
   Cozot, Remi
   Bouatouch, Kadi
TI High-dynamic-range image recovery from flash and non-flash image pairs
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE HDR; Image enhancement; Flash/non-flash photography
ID VISIBILITY; APPEARANCE
AB In this paper, we propose a novel method for creating HDR images from only two images-flash and non-flash images. Our method consists of two main steps, namely brightness gamma correction and bi-local chromatic adaptation transform (CAT). The brightness gamma correction performs series of increases and decreases of the non-flash brightness and yields multiple images with various exposure values. The bi-local CAT enhances the quality of each computed image by recovering missing details, using information from the flash image. The final multi-exposure images are then merged together to compute an HDR image. An evaluation shows that our HDR images, obtained by using only two LDR images, are close to HDR images, obtained by combining five manually taken multi-exposure images. Our method does not require the usage of a tripod and it is suitable for images of non-still objects, such as people, candle flames.
C1 [Hristova, Hristina; Cozot, Remi; Bouatouch, Kadi] Univ Rennes 1, Rennes, France.
   [Le Meur, Olivier] Univ Rennes 1, Image Proc, Rennes, France.
C3 Universite de Rennes; Universite de Rennes
RP Hristova, H (corresponding author), Univ Rennes 1, Rennes, France.
EM hristina.hristova@irisa.fr
OI Hristova, Hristina/0000-0003-2894-6933
CR Aguerrebere C., 2014, 2014 IEEE International Conference on Computational Photography, P1, DOI [DOI 10.1109/ICCPHOT.2014.6831807, 10.1109/ICCPHOT. 2014.6831807]
   [Anonymous], COMPUTER GRAPHICS FO
   [Anonymous], 2013, Colour Appearance Models
   [Anonymous], 2015, EXPRESSIVE INT S COM
   [Anonymous], 12 AS C COMP VIS ACC
   Bist C., 2016, P GRAPHICS INTERFACE, P57
   Debevec P.E., 2008, Recovering High Dynamic Range Radiance Maps from Photographs, P31
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   Gallo O., 2009, P IEEE INT C COMP PH, P1
   Granados M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508410
   Gryaditskaya Y, 2015, COMPUT GRAPH FORUM, V34, P119, DOI 10.1111/cgf.12684
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Kalantari NK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508402
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Li Z., 2016, ICASSP
   Mann S., 1995, P SOC IM SCI TECHN 4
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Matsuoka R, 2013, INT CONF ACOUST SPEE, P1612, DOI 10.1109/ICASSP.2013.6637924
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Nayar SK, 2002, LECT NOTES COMPUT SC, V2353, P636
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Orozco R.R., 2016, HIGH DYNAMIC RANGE V, P121, DOI [10.1016/B978-0-08-100412-8., DOI 10.1016/B978-0-08-100412-8]
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Sidibe Desire, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2240
   Tocci MD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964936
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
NR 31
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 725
EP 735
DI 10.1007/s00371-017-1399-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800005
DA 2024-07-18
ER

PT J
AU Sbert, M
   Havran, V
AF Sbert, Mateu
   Havran, Vlastimil
TI Adaptive multiple importance sampling for general functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Global illumination; Rendering equation analysis; Multiple importance
   sampling; Monte Carlo
AB We propose a mathematical expression for the optimal distribution of the number of samples in multiple importance sampling (MIS) and also give heuristics that work well in practice. The MIS balance heuristic is based on weighting several sampling techniques into a single estimator, and it is equal to Monte Carlo integration using a mixture of distributions. The MIS balance heuristic has been used since its invention almost exclusively with an equal number of samples from each technique. We introduce the sampling costs and adapt the formulae to work well with them. We also show the relationship between the MIS balance heuristic and the linear combination of these techniques, and that MIS balance heuristic minimum variance is always less or equal than the minimum variance of the independent techniques. Finally, we give one-dimensional and two-dimensional function examples, including an environment map illumination computation with occlusion.
C1 [Sbert, Mateu] Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
   [Sbert, Mateu] Girona Univ, Inst Informat & Applicat, Girona, Spain.
   [Havran, Vlastimil] Czech Tech Univ, Fac Elect Engn, Prague, Czech Republic.
C3 Tianjin University; Universitat de Girona; Czech Technical University
   Prague
RP Sbert, M (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.; Sbert, M (corresponding author), Girona Univ, Inst Informat & Applicat, Girona, Spain.
EM mateu@ima.udg.edu
RI Havran, Vlastimil/B-4530-2014; Sbert, Mateu/G-6711-2011
OI Havran, Vlastimil/0000-0002-3329-8814; Sbert, Mateu/0000-0003-2164-6858
FU Czech Science Foundation [GA14-19213S]; Spanish Government
   [TIN2016-75866-C3-3-R]
FX This work has been partially funded by Czech Science Foundation research
   program GA14-19213S and by Grant TIN2016-75866-C3-3-R from the Spanish
   Government.
CR [Anonymous], 1997, ROBUST MONTE CARLO M
   [Anonymous], SIMULATION MONTE CAR
   [Anonymous], 2003, Handbook of Means and Their Inequalities
   Bekaert P., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P125
   Cornuet JM, 2012, SCAND J STAT, V39, P798, DOI 10.1111/j.1467-9469.2011.00756.x
   Csonka F., 2001, TR18620119 VIENN U T
   Douc R, 2007, ANN STAT, V35, P420, DOI 10.1214/009053606000001154
   Douc R., 2007, ESAIM: Probability and Statistics, V11, P427
   Elvira V, 2015, INT CONF ACOUST SPEE, P4075, DOI 10.1109/ICASSP.2015.7178737
   GRAYBILL FA, 1959, BIOMETRICS, V15, P543, DOI 10.2307/2527652
   Hachisuka T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601138
   Hardy G, 1952, INEQUALITIES
   Havran V., 2014, P 13 ACM SIGGRAPH IN, P141
   Kalos M., 1986, M CARLO METHODS BASI
   KOROVKIN PP, 1975, INEQUALITIES
   Lafortune E. P., 1994, Using the modified Phong reflectance model for physically based rendering
   Lai YC, 2015, VISUAL COMPUT, V31, P83, DOI 10.1007/s00371-013-0908-z
   Lu H, 2013, COMPUT GRAPH FORUM, V32, P131, DOI 10.1111/cgf.12220
   Marin J.M., 2012, ARXIV12112548
   Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533
   Sbert M, 2016, COMPUT GRAPH FORUM, V35, P451, DOI 10.1111/cgf.13042
   Tokuyoshi Y., 2010, ACM SIGGRAPH ASIA 20
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419, DOI 10.1145/218380.218498
NR 23
TC 16
Z9 16
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 845
EP 855
DI 10.1007/s00371-017-1398-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800016
DA 2024-07-18
ER

PT J
AU Dingli, A
   Giordimaina, A
AF Dingli, Alexiei
   Giordimaina, Andreas
TI Webcam-based detection of emotional states
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Webcam-based; Video-based; Heart rate monitoring;
   Non-contact; Adaptive gaming; Effective computing; User experience;
   Human-computer interaction
ID EXPERIENCE
AB Game designers have to deal with the complex task of monitoring the emotional state of players in games. There are different elements with the game, which effect the player's emotional status. Since the game play experience occurs almost unconsciously, traditional methods such as think aloud may disrupt the playing experience, thus skewing the results obtained. Other methods include fitting cables and electrodes to the player to monitor biological information. Although such devices can offer significant accurate results, they are not commonly found and may cause discomfort while playing games. Because of this, we propose a webcam-based heart rate monitoring method that can be used to predict the player's emotional state. We first analyzed the change in heart rate with respect to the players emotional state. This allowed us to find a correlation between emotional states, such as frustration, fun, challenge, and boredom. The second objective was to create a webcam-based method to monitor the heart rate. This was performed by extracting the RGB channels from the face region and then retrieving the underlying components using a dimensionality-reduction method. The results obtained from the webcam-based method were far from perfect, but this was expected, since we were performing the tests under realistic conditions. The last objective was to predict the player's emotional state using the heart rate obtained from the webcam-based method. The accuracy of the prediction was up to 76 %, which exceeded our initial aim. This system will be implemented in Unity 3D to make its integration and adoption easier.
C1 [Dingli, Alexiei; Giordimaina, Andreas] Univ Malta, Dept Intelligent Comp Syst, Msida, Malta.
C3 University of Malta
RP Dingli, A (corresponding author), Univ Malta, Dept Intelligent Comp Syst, Msida, Malta.
EM alexieid@gmail.com; andreas.giordimaina@um.edu.mt
OI Dingli, Alexiei/0000-0002-5951-8299
CR Bahreini K, 2016, INTERACT LEARN ENVIR, V24, P590, DOI 10.1080/10494820.2014.908927
   Bousefsaf F, 2013, BIOMED SIGNAL PROCES, V8, P568, DOI 10.1016/j.bspc.2013.05.010
   Burelli P, 2011, LECT NOTES COMPUT SC, V6815, P25, DOI 10.1007/978-3-642-22571-0_3
   Chen J, 2007, COMMUN ACM, V50, P31, DOI 10.1145/1232743.1232769
   Csikszentmihalyi M., 1991, FLOW PSYCHOL OPTIMAL, V41
   Drachen A., 2010, Proceedings of the 5th ACM SIGGRAPH Symposium on Video Games, P49, DOI DOI 10.1145/1836135.1836143
   Ganglbauer Eva, 2009, WORKSH US EXP EV MET
   Hudlicka Eva., 2009, Proceedings of the 4th International Conference on Foundations of Digital Games, P299
   Kwon S, 2012, IEEE ENG MED BIO, P2174, DOI 10.1109/EMBC.2012.6346392
   Lan Wei, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P281, DOI 10.1007/978-3-642-37444-9_22
   Lee D, 2015, IEEE ENG MED BIO, P2758, DOI 10.1109/EMBC.2015.7318963
   Lewandowska M., 2011, 2011 Federated Conference on Computer Science and Information Systems (FedCSIS), P405
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Mandryk RL, 2006, BEHAV INFORM TECHNOL, V25, P141, DOI 10.1080/01449290500331156
   Martinez H.P., 2009, 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, P1, DOI [DOI 10.1109/ACII.2009.5349592, 10.1109/ETFA.2009.5347014]
   Nacke L., 2009, Proceedings of DiGRA 2009: Breaking New Ground: Innovation in Games, Play, Practice and Theory
   Pedersen C, 2010, IEEE T COMP INTEL AI, V2, P54, DOI 10.1109/TCIAIG.2010.2043950
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Pursche T, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P544, DOI 10.1109/ICCE.2012.6161965
   Schell J., 2014, The Art of Game Design: A book of lenses
   Shaker N, 2011, LECT NOTES COMPUT SC, V6975, P547, DOI 10.1007/978-3-642-24571-8_68
   Sweetser P, 2005, COMPUTERS ENTERTAINM, V3, P3, DOI [10.1145/1077246.1077253, DOI 10.1145/1077246.1077253]
   TAYLOR R, 1990, J DIAGN MED SONOG, V6, P35, DOI 10.1177/875647939000600106
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Wong T. L., 2015, THESIS
   Yannakakis GN, 2008, INT J HUM-COMPUT ST, V66, P741, DOI 10.1016/j.ijhcs.2008.06.004
   Yannakakis GN, 2011, LECT NOTES COMPUT SC, V6974, P437, DOI 10.1007/978-3-642-24600-5_47
   Yu SJ, 2015, IEEE SYS MAN CYBERN, P1041, DOI 10.1109/SMC.2015.188
   Zaunseder S, 2014, 2014 IEEE 34TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO), P286, DOI 10.1109/ELNANO.2014.6873915
NR 29
TC 5
Z9 6
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 459
EP 469
DI 10.1007/s00371-016-1309-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4JZ
UT WOS:000398767100006
DA 2024-07-18
ER

PT J
AU Vourvopoulos, A
   Badia, SBI
   Liarokapis, F
AF Vourvopoulos, Athanasios
   Bermudez I Badia, Sergi
   Liarokapis, Fotis
TI EEG correlates of video game experience and user profile in
   motor-imagery-based brain-computer interaction
SO VISUAL COMPUTER
LA English
DT Article
DE Brain-computer interfaces; Motor-imagery; EEG; Gaming experience
ID VIRTUAL-REALITY; INTERFACE; COMMUNICATION; ENGAGEMENT; HANDEDNESS;
   PEOPLE; GENDER; ALPHA; TASK; BCI
AB Through the use of brain-computer interfaces (BCIs), neurogames have become increasingly more advanced by incorporating immersive virtual environments and 3D worlds. However, training both the user and the system requires long and repetitive trials resulting in fatigue and low performance. Moreover, many users are unable to voluntarily modulate the amplitude of their brain activity to control the neurofeedback loop. In this study, we are focusing on the effect that gaming experience has in brain activity modulation as an attempt to systematically identify the elements that contribute to high BCI control and to be utilized in neurogame design. Based on the current literature, we argue that experienced gamers could have better performance in BCI training due to enhanced sensorimotor learning derived from gaming. To investigate this, two experimental studies were conducted with 20 participants overall, undergoing 3 BCI sessions, resulting in 88 EEG datasets. Results indicate (a) an effect from both demographic and gaming experience data to the activity patterns of EEG rhythms, and (b) increased gaming experience might not increase significantly performance, but it could provide faster learning for 'Hardcore' gamers.
C1 [Vourvopoulos, Athanasios; Bermudez I Badia, Sergi] Univ Madeira, Fac Ciencias Exatas & Engn, Campus Univ Penteada, P-9020105 Funchal, Portugal.
   [Vourvopoulos, Athanasios; Bermudez I Badia, Sergi] Polo Cient & Tecnol Madeira, Madeira Interact Technol Inst, P-9020105 Funchal, Portugal.
   [Liarokapis, Fotis] Masaryk Univ, Human Comp Interact Lab, Fac Informat, Brno, Czech Republic.
C3 Universidade da Madeira; Masaryk University Brno
RP Vourvopoulos, A (corresponding author), Univ Madeira, Fac Ciencias Exatas & Engn, Campus Univ Penteada, P-9020105 Funchal, Portugal.; Vourvopoulos, A (corresponding author), Polo Cient & Tecnol Madeira, Madeira Interact Technol Inst, P-9020105 Funchal, Portugal.
EM athanasios.vourvopoulos@m-iti.org; sergi.bermudez@m-iti.org;
   liarokap@fi.muni.cz
RI Badia, Sergi Bermúdez i/C-8681-2018; Liarokapis, Fotis/AAQ-9498-2021;
   Liarokapis, Fotis/AAD-4444-2019; Vourvopoulos, Athanasios/F-3872-2017
OI Badia, Sergi Bermúdez i/0000-0003-4452-0414; Liarokapis,
   Fotis/0000-0003-3617-2261; Liarokapis, Fotis/0000-0003-3617-2261;
   Vourvopoulos, Athanasios/0000-0001-9676-8599
FU European Commission through the RehabNet project-Neuroscience-Based
   Interactive Systems for Motor Rehabilitation-EC [303891 RehabNet
   FP7-PEOPLE-2011-CIG]; Fundacao para a Ciencia e Tecnologia (Portuguese
   Foundation for Science and Technology) [SFRH/BD/97117/2013]; LARSyS
   (Laboratorio de Robotica e Sistemas em Engenharia e Ciencia)
   [UID/EEA/50009/2013]; Fundação para a Ciência e a Tecnologia
   [SFRH/BD/97117/2013] Funding Source: FCT
FX This work was supported by the European Commission through the RehabNet
   project-Neuroscience-Based Interactive Systems for Motor
   Rehabilitation-EC (303891 RehabNet FP7-PEOPLE-2011-CIG), by the Fundacao
   para a Ciencia e Tecnologia (Portuguese Foundation for Science and
   Technology) through SFRH/BD/97117/2013, and LARSyS (Laboratorio de
   Robotica e Sistemas em Engenharia e Ciencia) through UID/EEA/50009/2013.
   Authors would also like to thank the members of NeuroRehab Lab at the
   University of Madeira and the HCI Lab at Masaryk University for their
   support and inspiration.
CR Adams E., 2002, CASUAL CORE STAT MEC
   Ahn M, 2014, SENSORS-BASEL, V14, P14601, DOI 10.3390/s140814601
   Allison BZ, 2008, CLIN NEUROPHYSIOL, V119, P399, DOI 10.1016/j.clinph.2007.09.121
   Allison BZ, 2010, HUM-COMPUT INT-SPRIN, P35, DOI 10.1007/978-1-84996-272-8_3
   [Anonymous], GAM VIRT WORLDS SER
   [Anonymous], 1958, ELECTROEN CLIN NEURO
   [Anonymous], 2012, BERL BRAIN COMP INT
   [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   [Anonymous], ADV COMPUTER ENTERTA
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Berka C, 2007, AVIAT SPACE ENVIR MD, V78, pB231
   Blum T, 2012, INT SYM MIX AUGMENT, P271, DOI 10.1109/ISMAR.2012.6402569
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Feng J, 2007, PSYCHOL SCI, V18, P850, DOI 10.1111/j.1467-9280.2007.01990.x
   Friedman D., 2015, Handbook of Digital Games and Entertainment Technologies, P1
   Friedman D, 2007, PRESENCE-TELEOP VIRT, V16, P100, DOI 10.1162/pres.16.1.100
   Friedman D, 2010, HUM-COMPUT INTER-US, V25, P67, DOI 10.1080/07370020903586688
   Friedrich EVC, 2013, CLIN NEUROPHYSIOL, V124, P916, DOI 10.1016/j.clinph.2012.11.010
   GALIN D, 1982, BRAIN LANG, V16, P19, DOI 10.1016/0093-934X(82)90070-0
   Garry MI, 2004, J NEUROPHYSIOL, V91, P1570, DOI 10.1152/jn.00595.2003
   GLASS A, 1984, BIOL PSYCHOL, V19, P169, DOI 10.1016/0301-0511(84)90035-8
   Gozli DG, 2014, HUM MOVEMENT SCI, V38, P152, DOI 10.1016/j.humov.2014.09.004
   Granek JA, 2010, CORTEX, V46, P1165, DOI 10.1016/j.cortex.2009.10.009
   Green CS, 2003, NATURE, V423, P534, DOI 10.1038/nature01647
   Guger C, 2003, IEEE T NEUR SYS REH, V11, P145, DOI 10.1109/TNSRE.2003.814481
   Guger C, 2009, NEUROSCI LETT, V462, P94, DOI 10.1016/j.neulet.2009.06.045
   Jolliffe I., 2014, WILEY STATSREF STAT
   Kalcher J, 1996, MED BIOL ENG COMPUT, V34, P382, DOI 10.1007/BF02520010
   Klimesch W, 1999, BRAIN RES REV, V29, P169, DOI 10.1016/S0165-0173(98)00056-3
   Krepki R, 2007, MULTIMED TOOLS APPL, V33, P73, DOI 10.1007/s11042-006-0094-3
   Lan ZR, 2016, VISUAL COMPUT, V32, P347, DOI 10.1007/s00371-015-1183-y
   Lardon MT, 1996, BIOL PSYCHOL, V44, P19, DOI 10.1016/S0301-0511(96)05198-8
   Lécuyer A, 2008, COMPUTER, V41, P66, DOI 10.1109/MC.2008.410
   Lotte F, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00568
   Marshall D, 2013, IEEE T COMP INTEL AI, V5, P82, DOI 10.1109/TCIAIG.2013.2263555
   Neuper C, 1999, J CLIN NEUROPHYSIOL, V16, P373, DOI 10.1097/00004691-199907000-00010
   OLDFIELD RC, 1971, NEUROPSYCHOLOGIA, V9, P97, DOI 10.1016/0028-3932(71)90067-4
   Pfurtscheller G, 2008, COMPUTER, V41, P58, DOI 10.1109/MC.2008.432
   Pineda JA, 2003, IEEE T NEUR SYS REH, V11, P181, DOI 10.1109/TNSRE.2003.814445
   POPE AT, 1995, BIOL PSYCHOL, V40, P187, DOI 10.1016/0301-0511(95)05116-3
   Renard Y, 2010, PRESENCE-VIRTUAL AUG, V19, P35, DOI 10.1162/pres.19.1.35
   Roberts R, 2008, J SPORT EXERCISE PSY, V30, P200, DOI 10.1123/jsep.30.2.200
   Schomer DL, 2011, Niedermeyer's Electroencephalography: Basic Principles, Clinical Applications, and Related Fields, V7e
   Skola F, 2016, VISUAL COMPUT, V32, P761, DOI 10.1007/s00371-016-1246-8
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater M, 2000, PRESENCE-TELEOP VIRT, V9, P413, DOI 10.1162/105474600566925
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Tan D, 2010, HUM-COMPUT INT-SPRIN, P3, DOI 10.1007/978-1-84996-272-8_1
   Taylor R.M., 2001, Proceedings of the ACM Symposium on Virtual Reality Software and Technology - VRST '01, P55, DOI [10.1145/505008.505019, DOI 10.1145/505008.505019, 10.1145/505008.505019., DOI 10.1145/505008.5050192]
   van de Laar B, 2013, IEEE T COMP INTEL AI, V5, P176, DOI 10.1109/TCIAIG.2013.2253778
   Vidaurre C, 2010, BRAIN TOPOGR, V23, P194, DOI 10.1007/s10548-009-0121-6
   Vourvopoulos A., 2015, 7 INT C GAM VIRT WOR
   Vourvopoulos A, 2013, IEEE 15 INT C E HLTH
   Vukovic A., 2010, P 2010 3 INT S APPL, P1
   Wang YJ, 2008, IEEE ENG MED BIOL, V27, P64, DOI 10.1109/MEMB.2008.923958
   Wolpaw JR, 2002, CLIN NEUROPHYSIOL, V113, P767, DOI 10.1016/S1388-2457(02)00057-3
NR 56
TC 31
Z9 34
U1 2
U2 67
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 533
EP 546
DI 10.1007/s00371-016-1304-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4JZ
UT WOS:000398767100010
DA 2024-07-18
ER

PT J
AU Wu, XS
   Sun, JD
AF Wu, Xiaosheng
   Sun, Junding
TI Joint-scale LBP: a new feature descriptor for texture classification
SO VISUAL COMPUTER
LA English
DT Article
DE Texture classification; Local binary pattern (LBP); Joint-scale local
   binary pattern (JLBP); Complete JLBP (CJLBP)
ID LOCAL BINARY PATTERN; ROTATION-INVARIANT; RECOGNITION
AB This paper presents a simple, efficient, yet robust approach, named joint-scale local binary pattern (JLBP), for texture classification. In the proposed approach, the joint-scale strategy is developed firstly, and the neighborhoods of different scales are fused together by a simple arithmetic operation. And then, the descriptor is extracted from the mutual integration of the local patches based on the conventional local binary pattern (LBP). The proposed scheme can not only describe the micro-textures of a local structure, but also the macro-textures of a larger area because of the joint of multiple scales. Further, motivated by the completed local binary pattern (CLBP) scheme, the completed JLBP (CJLBP) is presented to enhance its power. The proposed descriptor is evaluated in relation to other recent LBP-based patterns and non-LBP methods on popular benchmark texture databases, Outex, CURet and UIUC. Generally, the experimental results show that the new method performs better than the state-of-the-art techniques.
C1 [Wu, Xiaosheng; Sun, Junding] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454003, Henan, Peoples R China.
C3 Henan Polytechnic University
RP Sun, JD (corresponding author), Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454003, Henan, Peoples R China.
EM wuxs@hpu.edu.cn; sunjd@hpu.edu.cn
FU NSFC [61572173]; basic and advanced technology research project of Henan
   Province [132300410462, 112300410281]; research team of HPU [T2014-3]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions that we refer to in this paper. We
   would also like to thank Dr. Guo and Dr. Zhao as well as the MVG group
   for sharing their codes. This work is sponsored by the NSFC (No.
   61572173) and the basic and advanced technology research project of
   Henan Province (Nos. 132300410462, 112300410281), the research team of
   HPU (No. T2014-3).
CR [Anonymous], P WORKSH FAC REAL LI
   Brahnam S., 2014, Local Binary Patterns - New Variants and Applications
   Chen J, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.122
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Davarzani R, 2015, SIGNAL PROCESS, V111, P274, DOI 10.1016/j.sigpro.2014.11.005
   Guo ZH, 2013, NEUROCOMPUTING, V116, P182, DOI 10.1016/j.neucom.2011.11.038
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Hafiane A., 2015, PATTERN RECOGN
   Han J, 2007, IMAGE VISION COMPUT, V25, P1474, DOI 10.1016/j.imavis.2006.12.015
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hussain S. U., 2012, BR MACH VIS C
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li CR, 2014, PATTERN RECOGN, V47, P313, DOI 10.1016/j.patcog.2013.05.003
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Li Z, 2012, IEEE T IMAGE PROCESS, V21, P2130, DOI 10.1109/TIP.2011.2173697
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828
   Liao S, 2007, LECT NOTES COMPUT SC, V4844, P672
   Liu L, 2014, IEEE T IMAGE PROCESS, V23, P3071, DOI 10.1109/TIP.2014.2325777
   Maani R, 2013, IEEE T IMAGE PROCESS, V22, P2409, DOI 10.1109/TIP.2013.2249081
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Murala S, 2014, IEEE J BIOMED HEALTH, V18, P929, DOI 10.1109/JBHI.2013.2288522
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Qian XM, 2011, PATTERN RECOGN, V44, P2502, DOI 10.1016/j.patcog.2011.03.029
   Ren JF, 2015, PATTERN RECOGN, V48, P3180, DOI 10.1016/j.patcog.2015.02.001
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, IEEE T IMAGE PROCESS, V22, P4049, DOI 10.1109/TIP.2013.2268976
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Shu YC, 2014, NEUROCOMPUTING, V144, P378, DOI 10.1016/j.neucom.2014.04.035
   Song TC, 2014, IEEE SIGNAL PROC LET, V21, P93, DOI 10.1109/LSP.2013.2293335
   Sun JD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-23
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Nguyen TN, 2015, VISUAL COMPUT, V31, P391, DOI 10.1007/s00371-014-0934-5
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Verma M., 2015, NEUROCOMPUTING
   Wu XS, 2015, SENSORS-BASEL, V15, P6399, DOI 10.3390/s150306399
   Zhao Y, 2013, NEUROCOMPUTING, V106, P68, DOI 10.1016/j.neucom.2012.10.017
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
   Zhu C, 2013, PATTERN RECOGN, V46, P1949, DOI 10.1016/j.patcog.2013.01.003
NR 41
TC 33
Z9 34
U1 0
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 317
EP 329
DI 10.1007/s00371-015-1202-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300006
DA 2024-07-18
ER

PT J
AU Koch, S
   Kasten, J
   Wiebel, A
   Scheuermann, G
   Hlawitschka, M
AF Koch, Stefan
   Kasten, Jens
   Wiebel, Alexander
   Scheuermann, Gerik
   Hlawitschka, Mario
TI 2D Vector field approximation using linear neighborhoods
SO VISUAL COMPUTER
LA English
DT Article
DE I.6.6. Flow visualization, simulation output analysis; I.6.9.b Flow
   visualization, computing methodologies; I.3.8 Computer graphics,
   applications, computer applications; J.2 Physical sciences and
   engineering, physics
ID FLOW VISUALIZATION; FEATURE-EXTRACTION; TOPOLOGY; COMPRESSION
AB We present a vector field approximation for two-dimensional vector fields that preserves their topology and significantly reduces the memory footprint. This approximation is based on a segmentation. The flow within each segmentation region is approximated by an affine linear function. The implementation is driven by four aims: (1) the approximation preserves the original topology; (2) the maximal approximation error is below a user-defined threshold in all regions; (3) the number of regions is as small as possible; and (4) each point has the minimal approximation error. The generation of an optimal solution is computationally infeasible. We discuss this problem and provide a greedy strategy to efficiently compute a sensible segmentation that considers the four aims. Finally, we use the region-wise affine linear approximation to compute a simplified grid for the vector field.
C1 [Koch, Stefan; Kasten, Jens; Scheuermann, Gerik; Hlawitschka, Mario] Univ Leipzig, Inst Comp Sci, Augustuspl 10, D-04109 Leipzig, Germany.
   [Wiebel, Alexander] Coburg Univ Appl Sci, Friedrich Streib Str 2, D-96450 Coburg, Germany.
C3 Leipzig University; Hochschule Coburg
RP Koch, S (corresponding author), Univ Leipzig, Inst Comp Sci, Augustuspl 10, D-04109 Leipzig, Germany.
EM skoch@informatik.uni-leipzig.de
RI Hlawitschka, Mario/AAU-8942-2020
OI Wiebel, Alexander/0000-0002-6583-3092
FU European Social Fund [100098251]
FX We thank Markus Rutten, Guillaume Daviller, and Bernd Noack for
   providing the simulation datasets. Special thanks go to the FAnToM
   development group for providing the visualization software. We also
   thank Roxana Bujack and Sebastian Volke for the fruitful discussions. S.
   Koch and J. Kasten were supported by the European Social Fund (Appl. No.
   100098251).
CR [Anonymous], 2009, PROC INT S VISION MO
   [Anonymous], 2004, DIFFERENTIAL EQUATIO
   Chen CK, 2011, COMPUT GRAPH FORUM, V30, P1941, DOI 10.1111/j.1467-8659.2011.02064.x
   Chen JL, 2003, PROC SPIE, V5009, P79, DOI 10.1117/12.477521
   Dey TK, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P281, DOI 10.1109/PG.2007.34
   Garcke H, 2000, IEEE VISUAL, P351, DOI 10.1109/VISUAL.2000.885715
   Griebel M, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P35, DOI 10.1109/VISUAL.2004.32
   Haller G, 2001, PHYS FLUIDS, V13, P3365, DOI 10.1063/1.1403336
   Heckel B., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P19, DOI 10.1109/VISUAL.1999.809863
   HELMAN J, 1989, COMPUTER, V22, P27, DOI 10.1109/2.35197
   JEONG J, 1995, J FLUID MECH, V285, P69, DOI 10.1017/S0022112095000462
   Kenwright DN, 1999, IEEE T VIS COMPUT GR, V5, P135, DOI 10.1109/2945.773805
   Koch S, 2013, IEEE PAC VIS SYMP, P249, DOI 10.1109/PacificVis.2013.6596152
   Kuhn Alexander., 2011, VMV 2011 VISION MODE, P191
   Laramee RS, 2007, MATH VIS, P1, DOI 10.1007/978-3-540-70823-0_1
   Li HY, 2006, IEEE T VIS COMPUT GR, V12, P289, DOI 10.1109/TVCG.2006.54
   Lodha SK, 2000, IEEE VISUAL, P343, DOI 10.1109/VISUAL.2000.885714
   Lu KW, 2013, IEEE PAC VIS SYMP, P257, DOI 10.1109/PacificVis.2013.6596153
   Marchesin S, 2010, IEEE T VIS COMPUT GR, V16, P1578, DOI 10.1109/TVCG.2010.212
   MCKENZIE A, 2005, P EUR IEEE VGTC S VI, P29
   McLoughlin T, 2010, COMPUT GRAPH FORUM, V29, P1807, DOI 10.1111/j.1467-8659.2010.01650.x
   Peikert R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P263, DOI 10.1109/VISUAL.1999.809896
   Post FH, 2003, COMPUT GRAPH FORUM, V22, P775, DOI 10.1111/j.1467-8659.2003.00723.x
   ROMKEDAR V, 1990, J FLUID MECH, V214, P347, DOI 10.1017/S0022112090000167
   Salzbrunn T., 2008, Proceedings of Simulation and Visualization Conference, P75
   Schneider D, 2010, COMPUT GRAPH FORUM, V29, P1153, DOI 10.1111/j.1467-8659.2009.01672.x
   Stalling D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P249, DOI 10.1145/218380.218448
   Sujudi D, 1995, 12 COMP FLUID DYN C
   Telea A., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P35, DOI 10.1109/VISUAL.1999.809865
   Theisel H, 2003, COMPUT GRAPH FORUM, V22, P333, DOI 10.1111/1467-8659.00680
   Tricoche X, 2002, COMPUT GRAPH-UK, V26, P249, DOI 10.1016/S0097-8493(02)00056-0
   Tricoche X, 2000, MATHEMATICS OF SURFACES IX, P99
   Wiebel A., 2012, TOPOL METHOD NONL AN, P177
NR 33
TC 11
Z9 11
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1563
EP 1578
DI 10.1007/s00371-015-1140-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200006
DA 2024-07-18
ER

PT J
AU Samavati, F
   Runions, A
AF Samavati, Faramarz
   Runions, Adam
TI Interactive 3D content modeling for Digital Earth
SO VISUAL COMPUTER
LA English
DT Article
DE Digital Earth; Computer graphics; Interactive modeling; Sketch-based
   modeling; Subdivision surfaces; Image-based modeling
ID TERRAIN SYNTHESIS
AB Digital Earth is a global reference model for integrating, processing and visualizing geospatial datasets. In this reference model, various data-types, including Digital Elevation Models (DEM) and imagery (orthophotos), are universally and openly available for the entire globe. However, 3D content such as detailed terrains with features, man-made structures, 3D water bodies and 3D vegetation are not commonly available in Digital Earth. In this paper, we present an interactive system for the rapid creation and integration of these types of 3D content to augment Digital Earth. The inputs to our system include available data sources, such as DEM and imagery information depicting landscapes and urban environments. The proposed system employs sketch-based and image-assisted tools to support interactive creation of textured 3D content. For adding terrain features visible in orthophotos, and also the basin of water bodies, we use a multiscale least square surface fitting to generate an adaptive triangular subdivision. For modeling forests and vegetation, we use image-based techniques and take advantage of visible regions and colors of forests in orthophotos. For 3D man-made structures, starting from a single photograph, we provide a simple image-assisted sketching tool to extract these objects, correct for perspective distortion and place them into desired locations.
C1 [Samavati, Faramarz] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Runions, Adam] Univ Calgary, Dept Comp Sci, Graph Jungle Comp Graph Lab, Calgary, AB, Canada.
C3 University of Calgary; University of Calgary
RP Samavati, F (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
EM samavati@ucalgary.ca
RI Runions, Adam/Z-6049-2019
OI Runions, Adam/0000-0002-7758-7423
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
FX We would like to deeply thank Kaveh Ketabchi for his numerous
   contributions to this paper. This work was supported in part by the
   Natural Sciences and Engineering Research Council (NSERC) of Canada.
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   [Anonymous], 1998, AUST SURV, DOI DOI 10.1080/00050348.1998.10558728
   [Anonymous], 1973, PATTERN CLASSIFICATI
   [Anonymous], INT J DIGITAL EARTH
   [Anonymous], 2013, P INT S SKETCH BAS I, DOI DOI 10.1145/2487381.2487382
   [Anonymous], 2000, P 1 INT C DISCR GLOB
   [Anonymous], INT C CYB CW 2015
   [Anonymous], SURVEY PROCEDURAL MO
   [Anonymous], 2000, Computational Geometry Algorithms and Applications
   [Anonymous], P DMACH 2011
   [Anonymous], Proceedings of the 4th International Conference on Foundations of Digital Games. FDG'09. 2009, DOI DOI 10.1145/1536513.1536532
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bernhardt A., 2011, Proceedings of the 2011 24th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2011), P64, DOI 10.1109/SIBGRAPI.2011.28
   Bolstad P., 2005, GIS FUNDAMENTALS 1 T, V2nd
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Brosz J, 2007, COMM COM INF SC, V4, P58
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Do Carmo M.P., 1976, DIFFERENTIAL GEOMETR, V2
   Gain J., 2009, P 2009 S INT 3D GRAP, V1, P31, DOI [10.1145/1507149.1507155, DOI 10.1145/1507149.1507155]
   Habbecke M, 2012, COMPUT GRAPH FORUM, V31, P641, DOI 10.1111/j.1467-8659.2012.03043.x
   Hammes J., 2001, Digital Earth Moving. First International Symposium, DEM 2001. Proceedings (Lecture Notes in Computer Science Vol.2181), P98
   Hnaidi H, 2010, COMPUT GRAPH FORUM, V29, P2179, DOI 10.1111/j.1467-8659.2010.01806.x
   Irschara A., 2007, Computer Vision, P1
   Jenny H, 2011, CARTOGR J, V48, P11, DOI 10.1179/1743277411Y.0000000002
   Jiang NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618459
   Kelly T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944854
   Kosecká J, 2005, COMPUT VIS IMAGE UND, V100, P274, DOI 10.1016/j.cviu.2005.04.005
   Kun Gao, 2005, Mathematics of Surfaces XI 11th IMA International Conference. Proceedings (Lecture Notes in Computer Science Vol. 3604), P219
   Lafarge F, 2011, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2011.6126353
   Lane B, 2002, PROC GRAPH INTERF, P69
   Lee SC, 2003, FIRST IEEE INTERNATIONAL WORKSHOP ON HIGHER-LEVEL KNOWLEDGE IN 3D MODELING AND MOTION ANALYSIS, PROCEEDINGS, P58
   Lee SC, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P148, DOI 10.1109/ACV.2002.1182173
   Lee SC, 2002, INT C PATT RECOG, P107, DOI 10.1109/ICPR.2002.1047411
   Lee SC, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P170, DOI 10.1109/WACV.2000.895419
   Li Z., 2010, Digital terrain modeling: principles and methodology
   Longay S., 2012, The proceedings of the Eurographics Symposium on Sketch-Based Interfaces and Modeling, P107, DOI [DOI 10.2312/SBM/SBM12/107-120, 10.2312/SBM/SBM12/107-120]
   Loop CT, 1987, THESIS
   Losasso F, 2004, ACM T GRAPHIC, V23, P769, DOI 10.1145/1015706.1015799
   Maidment D.R., 2002, ARC HYDRO GIS WATER
   Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077
   Olsen L., 2010, Proc. of GI, P225
   Olsen L, 2011, IEEE COMPUT GRAPH, V31, P24, DOI 10.1109/MCG.2011.84
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Pakdel HR, 2004, LECT NOTES COMPUT SC, V3045, P237
   Poullis C, 2009, IEEE T VIS COMPUT GR, V15, P654, DOI 10.1109/TVCG.2008.189
   Pusch Richard, 2010, Proceedings of the Shape Modeling International (SMI 2010), P256, DOI 10.1109/SMI.2010.39
   Santoro F, 2013, INT J DIGIT EARTH, V6, P94, DOI 10.1080/17538947.2011.642902
   Schmidt Ryan, 2009, P 6 EUR S SKETCH BAS, P133, DOI DOI 10.1145/1572741.1572765
   SEVERN A., 2006, Proceedings of the 3rd Eurographics Workshop on Sketch-Based Interfaces and Modeling, P75
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112
   Stachniak S., 2005, Computer Graphics and Artificial Intelligence, V1, P64
   Tan P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409061
   Tasse FP, 2014, COMPUT GRAPH-UK, V45, P101, DOI 10.1016/j.cag.2014.09.001
   Wang WP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330513
   Xiao JX, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618460
   Xue TF, 2012, IEEE T IMAGE PROCESS, V21, P4180, DOI 10.1109/TIP.2012.2200494
   Yang L., 2009, P 17 ACM SIGSPATIAL, P131, DOI [DOI 10.1145/1653771.1653792, 10.1145/1653771.1653792]
   Zhou H, 2007, IEEE T VIS COMPUT GR, V13, P834, DOI 10.1109/TVCG.2007.1027
   Zlatanova S, 2004, COMPUT GEOSCI-UK, V30, P419, DOI 10.1016/j.cageo.2003.06.004
   Zorin D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P259, DOI 10.1145/258734.258863
NR 63
TC 9
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1293
EP 1309
DI 10.1007/s00371-016-1227-y
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800008
DA 2024-07-18
ER

PT J
AU Andersen, TG
   Falster, V
   Frisvad, JR
   Christensen, NJ
AF Andersen, Tobias Gronbeck
   Falster, Viggo
   Frisvad, Jeppe Revall
   Christensen, Niels Jorgen
TI Hybrid fur rendering: combining volumetric fur with explicit hair
   strands
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Fur; Hair strand; Rasterization; Ray marching; Photorealistic rendering;
   Order-independent transparency; Shell volume
ID SCATTERING; SIMULATION
AB Hair is typically modeled and rendered using either explicitly defined hair strand geometry or a volume texture of hair densities. Taken each on their own, these two hair representations have difficulties in the case of animal fur as it consists of very dense and thin undercoat hairs in combination with coarse guard hairs. Explicit hair strand geometry is not well-suited for the undercoat hairs, while volume textures are not well-suited for the guard hairs. To efficiently model and render both guard hairs and undercoat hairs, we present a hybrid technique that combines rasterization of explicitly defined guard hairs with ray marching of a prismatic shell volume with dynamic resolution. The latter is the key to practical combination of the two techniques, and it also enables a high degree of detail in the undercoat. We demonstrate that our hybrid technique creates a more detailed and soft fur appearance as compared with renderings that only use explicitly defined hair strands. Finally, our rasterization approach is based on order-independent transparency and renders high-quality fur images in seconds.
C1 [Andersen, Tobias Gronbeck; Falster, Viggo; Frisvad, Jeppe Revall; Christensen, Niels Jorgen] Tech Univ Denmark, Lyngby, Denmark.
C3 Technical University of Denmark
RP Frisvad, JR (corresponding author), Tech Univ Denmark, Lyngby, Denmark.
EM jerf@dtu.dk
RI Christensen, Niels Jørgen/I-5585-2013; Frisvad, Jeppe Revall/I-4679-2013
OI Frisvad, Jeppe Revall/0000-0002-0603-3669; Falster,
   Viggo/0000-0002-2748-6069; Christensen, Niels Jorgen/0000-0001-8258-0579
CR Ando M, 1995, LECT NOTES COMPUT SC, V1024, P463
   Angelidis A, 2009, VISUAL COMPUT, V25, P255, DOI 10.1007/s00371-008-0218-z
   Barringer R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366181
   Bruderlin A., 2003, PHOTOREALISTIC HAIR
   Csuri C., 1979, COMPUT GRAPHICS-US, V13, P289
   d'Eon E, 2011, COMPUT GRAPH FORUM, V30, P1181, DOI 10.1111/j.1467-8659.2011.01976.x
   Goldman D. B., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P127, DOI 10.1145/258734.258807
   Heitz Eric, 2012, EGGH HPG 12
   Hiebert B., 2006, ACM SIGGRAPH COURSES
   Kajiya J. T., 1989, Computer Graphics, V23, P271, DOI 10.1145/74334.74361
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   KASZOWSKI S, 1970, Journal of Mammalogy, V51, P27, DOI 10.2307/1378528
   Kniss J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P109, DOI 10.1109/VISUAL.2002.1183764
   LeBlanc A. M., 1991, Journal of Visualization and Computer Animation, V2, P92, DOI 10.1002/vis.4340020305
   Lee J, 2010, COMPUT ANIMAT VIRT W, V21, P311, DOI 10.1002/cav.361
   Lengyel J., 2001, P 2001 S INTERACTIVE, P227, DOI [10.1145/364338.364407, DOI 10.1145/364338.364407]
   Lengyel JE, 2000, SPRING COMP SCI, P243
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   Marschner SR, 2003, ACM T GRAPHIC, V22, P780, DOI 10.1145/882262.882345
   MCEWAN I, 2012, J GRAPH TOOLS, V16, P85
   Miller G. S. P., 1988, Proceedings of Graphics Interface '88, P138
   Moon JT, 2006, ACM T GRAPHIC, V25, P1067, DOI 10.1145/1141911.1141995
   Moon JT, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360630
   Neulander I, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P233
   Neulander I., 2001, ACM SIGGRAPH 2001 SK, P190
   Neulander I., 2010, ACM SIGGRAPH 2010 TA, P2
   Neulander Ivan., 2004, SIGGRAPH '04, P43
   NEYRET F, 1995, GRAPH INTER, P83
   Neyret F., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P215
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Qin H, 2014, IEEE T VIS COMPUT GR, V20, P1178, DOI 10.1109/TVCG.2013.270
   Ren Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778792
   Sadeghi I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778793
   VanGelder A, 1997, PROC GRAPH INTERF, P181
   Xu Kun, 2011, ACM T, V30, P1
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Yang G, 2008, IEEE COMPUT GRAPH, V28, P85, DOI 10.1109/MCG.2008.73
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
   Yu Xuan., 2012, Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'12, P111, DOI DOI 10.1145/2159616.2159635
   Yuksel Cem, 2010, ACM SIGGRAPH 2010 CO
   Zinke A., 2004, VISION MODELING VISU, P191
   Zinke A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360631
   [No title captured]
NR 43
TC 8
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 739
EP 749
DI 10.1007/s00371-016-1252-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gutschke, M
   Vais, A
   Wolter, FE
AF Gutschke, Martin
   Vais, Alexander
   Wolter, Franz-Erich
TI Differential geometric methods for examining the dynamics of slow-fast
   vector fields
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Differential geometry; Dynamical system; Slow-fast vector field; Jump
   set; Hit set; DAE system; Geodesic polar coordinates; Cut locus
ID EQUATIONS; CANARDS
AB In this work we present computational methods for examining dynamical systems. We focus on those systems being characterized by slow-fast vector fields or corresponding differential algebraic equations that commonly occur in physical applications. In the latter ones scientists usually consider a manifold of admissible physical states and a vector field describing the time evolution of the physical system. The manifold is typically implicitly defined within a higher-dimensional space by a system of equations. Certain physical systems, such as relaxation oscillators, perform sudden jumps in their state evolution when they are forced into an unstable state. The main contribution of the present work is to model the dynamical evolution incorporating the jumping behavior from a perspective of computational geometry which not only provides a qualitative analysis but also produces quantitative results. We use geodesic polar coordinates (GPC) to numerically obtain explicit parametrizations of the implicitly defined manifold and of the relevant jump and hit sets. Moreover, to deal with the possibly high co-dimension of the considered implicitly defined manifold we sketch how GPC in combination with the cut locus concept can be used to numerically obtain an essentially injective global parametrization. This allows us to parametrize and visualize the dynamical evolution of the system including the aforementioned jump phenomena. As main tools we use homotopy approaches in conjunction with concepts from differential geometry. We discuss how to numerically realize and how to apply them to several examples from mechanics, electrical engineering and biology.
C1 [Gutschke, Martin; Vais, Alexander; Wolter, Franz-Erich] Leibniz Univ Hannover, Welfenlab, Hannover, Germany.
C3 Leibniz University Hannover
RP Gutschke, M (corresponding author), Leibniz Univ Hannover, Welfenlab, Hannover, Germany.
EM gutschke@welfenlab.de; vais@welfenlab.de; few@welfenlab.de
RI Wolter, Franz-Erich/JAC-5956-2023; Wolter, Franz-Erich/AAV-3008-2020
OI Wolter, Franz-Erich/0000-0002-2293-5494; Wolter,
   Franz-Erich/0000-0002-2293-5494
FU German Research Foundation (DFG)
FX The research in this article was partially supported by the German
   Research Foundation (DFG), Project "Differentialgeometrische Methoden
   zur Analyse nichtlinearer elektronischer Schaltungen und deren
   Visualisierung".
CR Allgower E., 2003, CLASSICS APPL MATH, DOI [10.1137/1.9780898719154, DOI 10.1137/1.9780898719154]
   [Anonymous], 2010, DIFFERENTIAL TOPOLOG
   [Anonymous], 1997, INTRO MODERN THEORY
   Dey TK, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P125, DOI 10.1145/1542362.1542390
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   Golub G. H., 1965, SIAM J. Numer. Anal., V2, P205
   Guckenheimer J, 2000, INT J BIFURCAT CHAOS, V10, P2669, DOI 10.1142/S0218127400001742
   GUCKENHEIMER J, 2003, GLOBAL ANAL PERIADIC, P261
   Guckenheimer J, 2005, MOSC MATH J, V5, P91, DOI 10.17323/1609-4514-2005-5-1-91-103
   Gutschke M., 2004, THESIS LEIBNIZ U HAN
   Gutschke M., 2013, P NASAGEM WORKSH COM
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Mathis W., 2009, ITG FACHBERICHT ISTE, P30
   Mémoli F, 2004, J COMPUT PHYS, V195, P263, DOI 10.1016/j.jcp.2003.10.007
   NASS H, 2007, THESIS LEIBNIZ U HAN
   Nass H., 2007, P NASAGEM 26 OCT 200
   Poincaré H, 1905, T AM MATH SOC, V6, P237
   RAUSCH T, 1997, MATH SURFACES, V7, P43
   Rausch T., 1999, THESIS U HANNOVER
   Sinclair R, 2002, EXP MATH, V11, P1
   STEWART I, 1981, B MATH BIOL, V43, P279
   Tchizawa K., 1984, YOKOHAMA MATH J, V32, P203
   Thielhelm H, 2012, VISUAL COMPUT, V28, P529, DOI 10.1007/s00371-012-0681-4
   Thiessen T., 2011, EUR C CIRC THEOR DES
   Thiessen T., 2012, 20 EDITION NONLINEAR
   Thiessen T, 2010, INTERNATIONAL CONFERENCE ON SIGNALS AND ELECTRONIC SYSTEMS (ICSES '10): CONFERENCE PROCEEDINGS, P209
   Thiessen T, 2011, COMPEL, V30, P1307, DOI 10.1108/03321641111133217
   Thom R, 1975, STRUCTURAL STABILITY
   Wolter F.-E., 1992, Computer-Aided Geometric Design, V9, P241, DOI 10.1016/0167-8396(92)90033-L
   Wolter F.-E., 1979, ARCH MATH, V1, P92
   Wolter F.-E., 1993, 922 MIT DES LAB MIT
   WOLTER FE, 1992, ENG COMPUT, V8, P61, DOI 10.1007/BF01200103
   WOLTER FE, 1985, THESIS TU BERLIN
   Wolter FE, 2011, LECT NOTES APPL COMP, V57, P211
   ZANGWILL WI, 1981, PATHWAYS SOLUTIONS F
   Zeeman E.C., 2000, CATASTROPHE THEORY S
NR 37
TC 4
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 169
EP 186
DI 10.1007/s00371-014-1036-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800007
DA 2024-07-18
ER

PT J
AU Liu, SX
   Cui, WW
   Wu, YC
   Liu, MC
AF Liu, Shixia
   Cui, Weiwei
   Wu, Yingcai
   Liu, Mengchen
TI A survey on information visualization: recent advances and challenges
SO VISUAL COMPUTER
LA English
DT Article
DE Information visualization; Interactive techniques; Large datasets
ID HIGH-DIMENSIONAL DATA; INTERACTIVE VISUAL ANALYSIS; GRAPH VISUALIZATION;
   DOCUMENT COLLECTIONS; SOCIAL NETWORKS; FOCUS REGIONS; TIME-SERIES;
   DESIGN; TEXT; UNCERTAINTY
AB Information visualization (InfoVis), the study of transforming data, information, and knowledge into interactive visual representations, is very important to users because it provides mental models of information. The boom in big data analytics has triggered broad use of InfoVis in a variety of domains, ranging from finance to sports to politics. In this paper, we present a comprehensive survey and key insights into this fast-rising area. The research on InfoVis is organized into a taxonomy that contains four main categories, namely empirical methodologies, user interactions, visualization frameworks, and applications, which are each described in terms of their major goals, fundamental principles, recent trends, and state-of-the-art approaches. At the conclusion of this survey, we identify existing technical challenges and propose directions for future research.
C1 [Liu, Shixia; Cui, Weiwei; Wu, Yingcai; Liu, Mengchen] Microsoft Res Asia, Beijing, Peoples R China.
C3 Microsoft; Microsoft Research Asia
RP Liu, SX (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM Shixia.Liu@microsoft.com; weiwei.cui@microsoft.com;
   yingcai.wu@microsoft.com; v-meli@microsoft.com
RI Liu, Shi-Xia/C-5574-2016
OI Liu, Shi-Xia/0000-0001-6104-4320
CR Afzal S, 2012, IEEE T VIS COMPUT GR, V18, P2556, DOI 10.1109/TVCG.2012.264
   Albuquerque G, 2011, IEEE T VIS COMPUT GR, V17, P2317, DOI 10.1109/TVCG.2011.237
   Alper B, 2011, IEEE T VIS COMPUT GR, V17, P2325, DOI 10.1109/TVCG.2011.234
   Alper B, 2011, IEEE T VIS COMPUT GR, V17, P2259, DOI 10.1109/TVCG.2011.186
   Alsakran J, 2011, IEEE PAC VIS SYMP, P131, DOI 10.1109/PACIFICVIS.2011.5742382
   Alsakran J, 2012, IEEE COMPUT GRAPH, V32, P34, DOI 10.1109/MCG.2011.91
   Angus D, 2012, IEEE T VIS COMPUT GR, V18, P988, DOI 10.1109/TVCG.2011.100
   [Anonymous], 2006, J SOCIAL STRUCTURE
   [Anonymous], 2005, Proceedings of the SIGCHI conference on Human factors in computing systems
   [Anonymous], 2004, P WORK C ADV VIS INT, DOI DOI 10.1145/989863.989880
   [Anonymous], 2010, P 16 ACM SIGKDD INT
   Archambault D, 2011, IEEE T VIS COMPUT GR, V17, P539, DOI 10.1109/TVCG.2010.78
   Baudel T, 2012, IEEE T VIS COMPUT GR, V18, P2593, DOI 10.1109/TVCG.2012.205
   Bertini E, 2011, IEEE T VIS COMPUT GR, V17, P2203, DOI 10.1109/TVCG.2011.229
   Bezerianos A, 2012, IEEE T VIS COMPUT GR, V18, P2516, DOI 10.1109/TVCG.2012.251
   Block F, 2012, IEEE T VIS COMPUT GR, V18, P2789, DOI 10.1109/TVCG.2012.272
   Borgo R, 2012, IEEE T VIS COMPUT GR, V18, P2759, DOI 10.1109/TVCG.2012.197
   Borkin MA, 2011, IEEE T VIS COMPUT GR, V17, P2479, DOI 10.1109/TVCG.2011.192
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Bostock M, 2009, IEEE T VIS COMPUT GR, V15, P1121, DOI 10.1109/TVCG.2009.174
   Boukhelifa N, 2012, IEEE T VIS COMPUT GR, V18, P2769, DOI 10.1109/TVCG.2012.220
   Brandes U, 2011, IEEE T VIS COMPUT GR, V17, P2283, DOI 10.1109/TVCG.2011.169
   Burch M, 2011, IEEE T VIS COMPUT GR, V17, P2440, DOI 10.1109/TVCG.2011.193
   Cao N, 2012, IEEE T VIS COMPUT GR, V18, P2649, DOI 10.1109/TVCG.2012.291
   Cao N, 2011, IEEE T VIS COMPUT GR, V17, P2581, DOI 10.1109/TVCG.2011.188
   Cao N, 2010, IEEE T VIS COMPUT GR, V16, P1172, DOI 10.1109/TVCG.2010.154
   Caserta P, 2011, IEEE T VIS COMPUT GR, V17, P913, DOI 10.1109/TVCG.2010.110
   Chen M, 2010, IEEE T VIS COMPUT GR, V16, P1206, DOI 10.1109/TVCG.2010.132
   Chen T, 2012, COMPUT GRAPH-UK, V36, P241, DOI 10.1016/j.cag.2012.02.010
   Claessen JHT, 2011, IEEE T VIS COMPUT GR, V17, P2310, DOI 10.1109/TVCG.2011.201
   Correa Carlos D., 2009, Proceedings of the 2009 IEEE Symposium on Visual Analytics Science and Technology. VAST 2009. Held co-jointly with VisWeek 2009, P51, DOI 10.1109/VAST.2009.5332611
   Correa CD, 2012, IEEE T VIS COMPUT GR, V18, P106, DOI 10.1109/TVCG.2010.260
   Cui WW, 2008, IEEE T VIS COMPUT GR, V14, P1277, DOI 10.1109/TVCG.2008.135
   Cui WW, 2011, IEEE T VIS COMPUT GR, V17, P2412, DOI 10.1109/TVCG.2011.239
   Cui WW, 2010, IEEE COMPUT GRAPH, V30, P42, DOI 10.1109/MCG.2010.102
   Dasgupta A, 2012, COMPUT GRAPH FORUM, V31, P1015, DOI 10.1111/j.1467-8659.2012.03094.x
   Dasgupta A, 2011, IEEE T VIS COMPUT GR, V17, P2241, DOI 10.1109/TVCG.2011.163
   Dinkla K, 2012, IEEE T VIS COMPUT GR, V18, P2457, DOI 10.1109/TVCG.2012.208
   Dörk M, 2012, IEEE T VIS COMPUT GR, V18, P2709, DOI 10.1109/TVCG.2012.252
   ElHakim R, 2010, VISUAL COMPUT, V26, P1071, DOI 10.1007/s00371-010-0451-0
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1141, DOI 10.1109/TVCG.2008.153
   Ersoy O, 2011, IEEE T VIS COMPUT GR, V17, P2364, DOI 10.1109/TVCG.2011.233
   Fekete JD, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P167, DOI 10.1109/INFVIS.2004.64
   Feng KC, 2012, IEEE T VIS COMPUT GR, V18, P1330, DOI 10.1109/TVCG.2011.128
   Ferreira N, 2011, IEEE T VIS COMPUT GR, V17, P2374, DOI 10.1109/TVCG.2011.176
   Fink M, 2012, IEEE T VIS COMPUT GR, V18, P2583, DOI 10.1109/TVCG.2012.193
   Fisher D, 2010, IEEE T VIS COMPUT GR, V16, P1157, DOI 10.1109/TVCG.2010.222
   Gao Z. J., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P1056, DOI 10.1109/ICDM.2011.148
   Geisler G., 1998, MAKING INFORM MORE A
   Geng Z, 2011, IEEE T VIS COMPUT GR, V17, P2572, DOI 10.1109/TVCG.2011.166
   Gomez SR, 2012, IEEE T VIS COMPUT GR, V18, P2411, DOI 10.1109/TVCG.2012.214
   Gou L, 2011, IEEE T VIS COMPUT GR, V17, P2449, DOI 10.1109/TVCG.2011.247
   Hadlak S, 2011, IEEE T VIS COMPUT GR, V17, P2334, DOI 10.1109/TVCG.2011.213
   Haroz S, 2012, IEEE T VIS COMPUT GR, V18, P2402, DOI 10.1109/TVCG.2012.233
   Haunert JH, 2011, IEEE T VIS COMPUT GR, V17, P2555, DOI 10.1109/TVCG.2011.191
   Havre S, 2002, IEEE T VIS COMPUT GR, V8, P9, DOI 10.1109/2945.981848
   Healey CG, 2012, IEEE T VIS COMPUT GR, V18, P1744, DOI 10.1109/TVCG.2012.23
   Heer J, 2010, IEEE T VIS COMPUT GR, V16, P1149, DOI 10.1109/TVCG.2010.144
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Heine C, 2011, IEEE T VIS COMPUT GR, V17, P1599, DOI 10.1109/TVCG.2010.270
   Hofmann H, 2012, IEEE T VIS COMPUT GR, V18, P2441, DOI 10.1109/TVCG.2012.230
   Holme P, 2011, J COMPUT SCI TECH-CH, V26, P829, DOI 10.1007/s11390-011-0182-3
   Holten D, 2009, COMPUT GRAPH FORUM, V28, P983, DOI 10.1111/j.1467-8659.2009.01450.x
   Hsiao JPL, 2011, VISUAL COMPUT, V27, P633, DOI 10.1007/s00371-011-0576-9
   Hu Mengdie, 2012, P SIGCHI C HUM FACT, P2751, DOI [10.1145/2207676.2208672, DOI 10.1145/2207676.2208672]
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2213, DOI 10.1109/TVCG.2011.175
   Hullman J, 2011, IEEE T VIS COMPUT GR, V17, P2231, DOI 10.1109/TVCG.2011.255
   Hurter C, 2012, COMPUT GRAPH FORUM, V31, P865, DOI 10.1111/j.1467-8659.2012.03079.x
   Hurter C, 2011, IEEE T VIS COMPUT GR, V17, P2600, DOI 10.1109/TVCG.2011.223
   Isenberg P, 2012, IEEE T VIS COMPUT GR, V18, P689, DOI 10.1109/TVCG.2011.287
   Isenberg P, 2011, IEEE T VIS COMPUT GR, V17, P2469, DOI 10.1109/TVCG.2011.160
   Jenny B, 2012, IEEE T VIS COMPUT GR, V18, P2575, DOI 10.1109/TVCG.2012.192
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Keim D.A., 2006, Challenges in Visual Data Analysis, P9
   Keim DA, 1996, IEEE T KNOWL DATA EN, V8, P923, DOI 10.1109/69.553159
   Keim DA, 2007, IEEE CONF VIS ANAL, P115, DOI 10.1109/VAST.2007.4389004
   Khoury M, 2012, COMPUT GRAPH FORUM, V31, P975, DOI 10.1111/j.1467-8659.2012.03090.x
   Kim SH, 2012, IEEE T VIS COMPUT GR, V18, P2421, DOI 10.1109/TVCG.2012.215
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Ko S, 2012, COMPUT GRAPH FORUM, V31, P1245, DOI 10.1111/j.1467-8659.2012.03117.x
   Koh K, 2010, IEEE T VIS COMPUT GR, V16, P1190, DOI 10.1109/TVCG.2010.175
   Kong N, 2012, IEEE T VIS COMPUT GR, V18, P2631, DOI 10.1109/TVCG.2012.229
   Krstajic M, 2011, IEEE T VIS COMPUT GR, V17, P2432, DOI 10.1109/TVCG.2011.179
   Kwon BC, 2012, IEEE T VIS COMPUT GR, V18, P1992, DOI 10.1109/TVCG.2012.89
   Lam H, 2012, IEEE T VIS COMPUT GR, V18, P1520, DOI 10.1109/TVCG.2011.279
   Landge AG, 2012, IEEE T VIS COMPUT GR, V18, P2467, DOI 10.1109/TVCG.2012.286
   Lee B, 2010, IEEE T VIS COMPUT GR, V16, P1182, DOI 10.1109/TVCG.2010.194
   Lee H, 2012, COMPUT GRAPH FORUM, V31, P1155, DOI 10.1111/j.1467-8659.2012.03108.x
   Lee J.H., 2013, IEEE T VIS IN PRESS
   Lei Shi, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P99, DOI 10.1109/VAST.2010.5652931
   Lex A, 2011, IEEE T VIS COMPUT GR, V17, P2291, DOI 10.1109/TVCG.2011.250
   Liu S., 2013, IEEE Transactions on Visualization and Computer Graphics, V19
   Liu S., 2009, P 18 ACM C INF KNOWL, P543
   Liu SX, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P183
   Liu SX, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089101
   Liu Z., 2013, IEEE TRANS VIS COMPU
   Lloyd D, 2011, IEEE T VIS COMPUT GR, V17, P2498, DOI 10.1109/TVCG.2011.209
   Luo DN, 2012, IEEE T VIS COMPUT GR, V18, P93, DOI 10.1109/TVCG.2010.225
   Ma J, 2012, IEEE T VIS COMPUT GR, V18, P2799, DOI 10.1109/TVCG.2012.244
   MacEachren AM, 2012, IEEE T VIS COMPUT GR, V18, P2496, DOI 10.1109/TVCG.2012.279
   Maciejewski R, 2011, IEEE T VIS COMPUT GR, V17, P440, DOI 10.1109/TVCG.2010.82
   Maguire E, 2012, IEEE T VIS COMPUT GR, V18, P2603, DOI 10.1109/TVCG.2012.271
   Marriott K, 2012, IEEE T VIS COMPUT GR, V18, P2477, DOI 10.1109/TVCG.2012.245
   Mashima D, 2012, IEEE T VIS COMPUT GR, V18, P1424, DOI 10.1109/TVCG.2011.288
   Micallef L, 2012, IEEE T VIS COMPUT GR, V18, P2536, DOI 10.1109/TVCG.2012.199
   Munzner T, 2003, ACM T GRAPHIC, V22, P453, DOI 10.1145/882262.882291
   Nocaj A, 2012, IEEE T VIS COMPUT GR, V18, P2546, DOI 10.1109/TVCG.2012.250
   Oelke D, 2012, IEEE T VIS COMPUT GR, V18, P662, DOI 10.1109/TVCG.2011.266
   Oesterling P, 2011, IEEE T VIS COMPUT GR, V17, P1547, DOI 10.1109/TVCG.2011.27
   Paiva JGS, 2011, IEEE T VIS COMPUT GR, V17, P2459, DOI 10.1109/TVCG.2011.212
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P1229, DOI 10.1109/TVCG.2008.138
   Pileggi H, 2012, IEEE T VIS COMPUT GR, V18, P2819, DOI 10.1109/TVCG.2012.263
   Pilhöfer A, 2012, IEEE T VIS COMPUT GR, V18, P2506, DOI 10.1109/TVCG.2012.207
   Pretorius J, 2011, IEEE T VIS COMPUT GR, V17, P2402, DOI 10.1109/TVCG.2011.253
   Purchase HC, 2012, IEEE T VIS COMPUT GR, V18, P81, DOI 10.1109/TVCG.2010.269
   Rodgers J, 2011, IEEE T VIS COMPUT GR, V17, P2489, DOI 10.1109/TVCG.2011.196
   Scheepens R, 2011, IEEE T VIS COMPUT GR, V17, P2518, DOI 10.1109/TVCG.2011.181
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2729, DOI 10.1109/TVCG.2012.255
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Selassie D, 2011, IEEE T VIS COMPUT GR, V17, P2354, DOI 10.1109/TVCG.2011.190
   Shi CL, 2012, IEEE T VIS COMPUT GR, V18, P2669, DOI 10.1109/TVCG.2012.253
   Shi L, 2009, IEEE PAC VIS SYMP, P41, DOI 10.1109/PACIFICVIS.2009.4906836
   Shiravi H, 2012, IEEE T VIS COMPUT GR, V18, P1313, DOI 10.1109/TVCG.2011.144
   Slingsby A, 2011, IEEE T VIS COMPUT GR, V17, P2545, DOI 10.1109/TVCG.2011.197
   Song Y., 2010, AAAI
   Song YQ, 2013, IEEE T KNOWL DATA EN, V25, P1227, DOI 10.1109/TKDE.2012.45
   Steinberger M, 2011, IEEE T VIS COMPUT GR, V17, P2249, DOI 10.1109/TVCG.2011.183
   Streit M, 2012, IEEE T VIS COMPUT GR, V18, P998, DOI 10.1109/TVCG.2011.108
   SUGIYAMA K, 1981, IEEE T SYST MAN CYB, V11, P109, DOI 10.1109/TSMC.1981.4308636
   Talbot J, 2012, IEEE T VIS COMPUT GR, V18, P2613, DOI 10.1109/TVCG.2012.196
   Tan L, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2011.89
   Tanahashi Y, 2012, IEEE T VIS COMPUT GR, V18, P2679, DOI 10.1109/TVCG.2012.212
   Tatu A, 2011, IEEE T VIS COMPUT GR, V17, P584, DOI 10.1109/TVCG.2010.242
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2719, DOI 10.1109/TVCG.2012.237
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Trimm D, 2012, IEEE T VIS COMPUT GR, V18, P2809, DOI 10.1109/TVCG.2012.288
   Tu Y, 2008, IEEE T VIS COMPUT GR, V14, P1157, DOI 10.1109/TVCG.2008.114
   Turkay C, 2012, IEEE T VIS COMPUT GR, V18, P2621, DOI 10.1109/TVCG.2012.256
   Turkay C, 2011, IEEE T VIS COMPUT GR, V17, P2591, DOI 10.1109/TVCG.2011.178
   van Ham F, 2009, IEEE T VIS COMPUT GR, V15, P1169, DOI 10.1109/TVCG.2009.165
   van Zudilova-Seinstra E., 2009, TRENDS INTERACTIVE V
   Vande Moere A, 2012, IEEE T VIS COMPUT GR, V18, P2739, DOI 10.1109/TVCG.2012.221
   Verbeek K, 2011, IEEE T VIS COMPUT GR, V17, P2536, DOI 10.1109/TVCG.2011.202
   von Landesberger T, 2011, COMPUT GRAPH FORUM, V30, P1719, DOI 10.1111/j.1467-8659.2011.01898.x
   Walny J, 2012, IEEE T VIS COMPUT GR, V18, P2779, DOI 10.1109/TVCG.2012.275
   Walny J, 2011, IEEE T VIS COMPUT GR, V17, P2508, DOI 10.1109/TVCG.2011.251
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P2528, DOI 10.1109/TVCG.2011.205
   Wattenberg M, 2008, IEEE T VIS COMPUT GR, V14, P1221, DOI 10.1109/TVCG.2008.172
   Weaver C, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P159, DOI 10.1109/INFVIS.2004.12
   WEHREND S, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P139, DOI 10.1109/VISUAL.1990.146375
   Wickham H, 2011, IEEE T VIS COMPUT GR, V17, P2223, DOI 10.1109/TVCG.2011.227
   Wongsuphasawat K, 2012, IEEE T VIS COMPUT GR, V18, P2659, DOI 10.1109/TVCG.2012.225
   Wood J, 2012, IEEE T VIS COMPUT GR, V18, P2749, DOI 10.1109/TVCG.2012.262
   Wood J, 2011, IEEE T VIS COMPUT GR, V17, P2384, DOI 10.1109/TVCG.2011.174
   Wood M., 1994, VISUALIZATION MODERN, V2, P13
   Wu YC, 2013, IEEE T VIS COMPUT GR, V19, P278, DOI 10.1109/TVCG.2012.114
   Wu YC, 2012, IEEE T VIS COMPUT GR, V18, P2526, DOI 10.1109/TVCG.2012.285
   Wu YC, 2011, COMPUT GRAPH FORUM, V30, P741, DOI 10.1111/j.1467-8659.2011.01923.x
   Wu YC, 2010, IEEE T VIS COMPUT GR, V16, P1109, DOI 10.1109/TVCG.2010.183
   Xu K, 2012, IEEE T VIS COMPUT GR, V18, P2449, DOI 10.1109/TVCG.2012.189
   Xu P., 2013, IEEE T VIS COMPUT GR, V19, P93
   Yang J, 2013, IEEE T VIS COMPUT GR, V19, P1034, DOI 10.1109/TVCG.2012.172
   Yee KP, 2001, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2001, PROCEEDINGS, P43
   Yi JS, 2007, IEEE T VIS COMPUT GR, V13, P1224, DOI 10.1109/TVCG.2007.70515
   Yuan XR, 2012, IEEE T VIS COMPUT GR, V18, P2699, DOI 10.1109/TVCG.2012.236
   Zhang J, 2010, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, P1079
   Zhao J, 2012, IEEE T VIS COMPUT GR, V18, P2639, DOI 10.1109/TVCG.2012.226
   Zinsmaier M, 2012, IEEE T VIS COMPUT GR, V18, P2486, DOI 10.1109/TVCG.2012.238
NR 169
TC 226
Z9 256
U1 5
U2 169
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2014
VL 30
IS 12
BP 1373
EP 1393
DI 10.1007/s00371-013-0892-3
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS8DX
UT WOS:000344481400006
DA 2024-07-18
ER

PT J
AU Junejo, IN
   Junejo, KN
   Al Aghbari, Z
AF Junejo, Imran N.
   Junejo, Khurrum Nazir
   Al Aghbari, Zaher
TI Silhouette-based human action recognition using SAX-Shapes
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Computer vision; Time series shapelets; Symbolic
   Aggregate approXimation
ID REPRESENTATION
AB Human action recognition is an important problem in Computer Vision. Although most of the existing solutions provide good accuracy results, the methods are often overly complex and computationally expensive, hindering practical applications. In this regard, we introduce the combination of time-series representation for the silhouette and Symbolic Aggregate approXimation (SAX), which we refer to as SAX-Shapes, to address the problem of human action recognition. Given an action sequence, the extracted silhouettes of an actor from every frame are transformed into time series. Each of these time series is then efficiently converted into the symbolic vector: SAX. The set of all these SAX vectors (SAX-Shape) represents the action. We propose a rotation invariant distance function to be used by a random forest algorithm to perform the human action recognition. Requiring only silhouettes of actors, the proposed method is validated on two public datasets. It has an accuracy comparable to the related works and it performs well even in varying rotation.
C1 [Junejo, Imran N.; Al Aghbari, Zaher] Univ Sharjah, Sharjah, U Arab Emirates.
   [Junejo, Khurrum Nazir] Natl Univ Comp & Emerging Sci, Dept Comp Sci, Karachi, Pakistan.
C3 University of Sharjah
RP Junejo, IN (corresponding author), Univ Sharjah, Sharjah, U Arab Emirates.
EM ijunejo@sharjah.ac.ae
RI Junejo, Imran/ABA-2975-2020
FU University of Sharjah [120227]
FX This research is funded by University of Sharjah (Project 120227).
CR Abdelkader MF, 2011, COMPUT VIS IMAGE UND, V115, P439, DOI 10.1016/j.cviu.2010.10.006
   Adamek T, 2004, IEEE T CIRC SYST VID, V14, P742, DOI 10.1109/TCSVT.2004.826776
   AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
   Al Aghbari Z, 2009, J ADV COMPUT INTELL, V13, P109, DOI 10.20965/jaciii.2009.p0109
   Ali S., 2007, P ICCV, P1
   [Anonymous], P ICCV
   [Anonymous], P INT C PATT REC
   [Anonymous], EUR C COMP VIS 2010
   [Anonymous], ICCV WORKSH
   [Anonymous], P SIAM INT C DAT MIN
   [Anonymous], P ICCV
   [Anonymous], 7 EUR C COMP VIS ECC
   [Anonymous], P CVPR
   [Anonymous], INT C DAT ENG
   [Anonymous], 2006, P 32 INT C VER LARG
   Attalla E, 2005, PATTERN RECOGN, V38, P2229, DOI 10.1016/j.patcog.2005.02.009
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779
   Breiman L., 2001, Mach. Learn., V45, P5
   Cardone A., 2003, Transactions of the ASME. Journal of Computing and Information Science in Engineering, V3, P109, DOI 10.1115/1.1577356
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Faloutsos C., 1994, SIGMOD Record, V23, P419, DOI 10.1145/191843.191925
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI [DOI 10.1109/TEC.1961.5219197, 10.1109/TEC.1961.5219197]
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Grundmann M., 2008, P ICPR, P1
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hsiao Pei-Chi., 2008, Pattern Recognition, P1
   Ikizler N, 2007, LECT NOTES COMPUT SC, V4814, P271
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Junejo IN, 2012, J VIS COMMUN IMAGE R, V23, P853, DOI 10.1016/j.jvcir.2012.05.001
   Keogh E, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P289, DOI 10.1109/ICDM.2001.989531
   Keogh E, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P226, DOI 10.1109/ICDM.2005.79
   Keogh E. J., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P285, DOI 10.1145/347090.347153
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li R., 2007, P ICCV, P1
   Lin J., 2003, 8THACM SIGMOD WORKSH, DOI [10.1145/882082. 882086, DOI 10.1145/882082.882086]
   Lin J, 2007, DATA MIN KNOWL DISC, V15, P107, DOI 10.1007/s10618-007-0064-z
   Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154
   Niebles J.C., 2006, P BMVC
   Popivanov I, 2002, PROC INT CONF DATA, P212, DOI 10.1109/ICDE.2002.994711
   Syeda-Mahmood T, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P64, DOI 10.1109/EVENT.2001.938868
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Wongun Choi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3273, DOI 10.1109/CVPR.2011.5995707
   Ye LX, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P947
   Yilmaz A, 2005, PROC CVPR IEEE, P984
   Zhang DS, 2004, PATTERN RECOGN, V37, P1, DOI 10.1016/j.patcog.2003.07.008
   Zunic J, 2006, IEEE T IMAGE PROCESS, V15, P3478, DOI 10.1109/TIP.2006.877527
NR 49
TC 20
Z9 23
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 259
EP 269
DI 10.1007/s00371-013-0842-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100002
DA 2024-07-18
ER

PT J
AU Wang, ZJ
   Ben Salah, M
   Zhang, H
AF Wang, Zhijie
   Ben Salah, Mohamed
   Zhang, Hong
TI Object joint detection and tracking using adaptive multiple motion
   models
SO VISUAL COMPUTER
LA English
DT Article
DE Joint detection and tracking; Multi-motion model; Object kinematic state
ID PARTICLE FILTER; TARGETS
AB This paper deals with the problem of detecting objects that may switch between different motion models. In order to accurately detect these moving objects taking into account possible changing motion models, we propose an adaptive multi-motion model in the joint detection and tracking (JDT) framework. The proposed technique differs from the existing JDT-based methods mainly in two ways. First we express the solution in the JDT framework via a formulation in the multiple motion model setting. Second, we introduce a new motion model prediction function which exploits the correlation between the motion model and object kinematic state. Experiments on both synthetic and real videos demonstrate that the JDT method employing the proposed adaptive multi-motion model can detect objects more accurately than the existing peer methods when objects change their motion models.
C1 [Wang, Zhijie; Ben Salah, Mohamed; Zhang, Hong] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
C3 University of Alberta
RP Ben Salah, M (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM zhijie@ualberta.ca; mbensala@ualberta.ca; hzhang@ualberta.ca
CR Arulampalam M.S., 2004, EURASIP J APPL SIGNA, V2004
   Bar-Shalom Y, 2001, Estimation with Applications to Tracking and Navigation
   Bi SZ, 2008, PROG ELECTROMAGN RES, V87, P15, DOI 10.2528/PIER08091501
   Black MJ, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P16, DOI 10.1109/AFGR.1998.670919
   BLOM HAP, 1988, IEEE T AUTOMAT CONTR, V33, P780, DOI 10.1109/9.1299
   Boers Y, 2003, IEE P-RADAR SON NAV, V150, P344, DOI 10.1049/ip-rsn:20030741
   Corder G. W., 2014, Nonparametric statistics: a step-by-step approach, DOI DOI 10.1002/9781118165881
   Czyz J, 2005, INT CONF ACOUST SPEE, P217
   Czyz J, 2007, IMAGE VISION COMPUT, V25, P1271, DOI 10.1016/j.imavis.2006.07.027
   Du SC, 2007, J ZHEJIANG UNIV-SC A, V8, P1277, DOI 10.1631/jzus.2007.A1277
   Hao Sun, 2010, Proceedings of the 2010 International Conference on Information and Automation (ICIA 2010), P1936, DOI 10.1109/ICINFA.2010.5512013
   Isard M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P107, DOI 10.1109/ICCV.1998.710707
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   Jaeggli T, 2009, INT J COMPUT VISION, V83, P121, DOI 10.1007/s11263-008-0158-0
   Jixu Chen, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2655, DOI 10.1109/CVPRW.2009.5206580
   Mazor E, 1998, IEEE T AERO ELEC SYS, V34, P103, DOI 10.1109/7.640267
   McGinnity S, 2000, IEEE T AERO ELEC SYS, V36, P1006, DOI 10.1109/7.869522
   Nandakumaran N, 2008, IEEE T AERO ELEC SYS, V44, P1326, DOI 10.1109/TAES.2008.4667712
   Ng W, 2005, AEROSP CONF PROC, P2126
   Punithakumar K, 2008, IEEE T AERO ELEC SYS, V44, P87, DOI 10.1109/TAES.2008.4516991
   Ristic B, 2003, SIGNAL PROCESS, V83, P1223, DOI 10.1016/S0165-1684(03)00042-2
   Ristic B., 2003, Beyond the Kalman Filter: Particle Filters for Tracking Applications
   Russell S., 2009, Artificial intelligence
   Rutten MG, 2005, 2005 7th International Conference on Information Fusion (FUSION), Vols 1 and 2, P169
   Wang ZJ, 2009, ICAPR 2009: SEVENTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, PROCEEDINGS, P14, DOI 10.1109/ICAPR.2009.52
   Zhang Hong., 2008, IEEE SIGNAL PROC MAG, V25, P198
NR 26
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 173
EP 187
DI 10.1007/s00371-013-0793-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900004
DA 2024-07-18
ER

PT J
AU Zhao, FK
   Liu, XG
AF Zhao, Fukai
   Liu, Xinguo
TI 3D gradient enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Gradient; Enhancement; Rendering
AB Enhancement can exaggerate visual details in both image processing and 3D rendering. In this paper, we adapt the gradient enhancement technique from image processing to 3D rendering through differentiating the rendering result with respect to the image space coordinates under point lighting. In this way, we can achieve 3D enhancement taking into account the gradient of geometry, projection transform, visibility and highlight. We also propose a tunable shape descriptor for users to achieve rendering results in different enhancement extent. Moreover, we extend this method to the environment lighting with some simplifications. Finally, we demonstrate that our method can handle the grazing angle area and the edges of sharp slope better than the previous method due to the gradient of the projection transform.
C1 [Zhao, Fukai; Liu, Xinguo] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Zhao, FK (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM zhaofukai@cad.zju.edu.cn; xgliu@cad.zju.edu.cn
FU NSFC [60970074]; Fok Ying-Tong Education Foundation; Fundamental
   Research Funds for the Central Universities
FX Many thanks to Romain Vergne et al. for sharing their OpenGL Shader
   source code, the anonymous reviewers for their valuable comments and
   Ming Zeng and Bo Jiang for proof reading. The testing scenes are
   courtesy of Stanford 3D Scanning Repository (Armadillo and Dragon). This
   work was partially supported by NSFC (No. 60970074), Fok Ying-Tong
   Education Foundation and the Fundamental Research Funds for the Central
   Universities.
CR Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814
   Belkin M, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P278, DOI 10.1145/1377676.1377725
   Ben-Artzi A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356686
   Cignoni P, 2005, COMPUT GRAPH-UK, V29, P125, DOI 10.1016/j.cag.2004.11.012
   Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Decarlo D, 2007, INT S NONPH AN REND
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Gonzalez R.C., 2007, DIGITAL IMAGE PROCES, V1
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   Goodwin T, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P53
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Ihrke M., 2009, P SPIE, V49
   Judd T, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239470
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kindlmann G, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P513, DOI 10.1109/VISUAL.2003.1250414
   KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321
   Kolomenkin M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409110
   Lee CH, 2006, IEEE T VIS COMPUT GR, V12, P197, DOI 10.1109/TVCG.2006.30
   Lee Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239469
   Miller G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P319, DOI 10.1145/192161.192244
   Nienhaus M, 2004, PROC GRAPH INTERF, P49
   Ohtake Y, 2004, ACM T GRAPHIC, V23, P609, DOI 10.1145/1015706.1015768
   Pharr M., 2004, GPU GEMS AMBIENT OCC
   Ritschel T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360689
   Rusinkiewicz S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P486, DOI 10.1109/TDPVT.2004.1335277
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Vergne R., 2010, ACM SIGGRAPH S INT 3, P143
   Vergne R., 2008, Proceedings of the 6th international symposium on Nonphotorealistic animation and rendering, P23, DOI [10. 1145/1377980. 1377987, DOI 10.1145/1377980.1377987]
   Vergne R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185590
   Vergne R, 2011, IEEE T VIS COMPUT GR, V17, P1071, DOI 10.1109/TVCG.2010.252
   Vergne R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531331
   Zhang L., 2009, P S INT 3D GRAPH GAM
   Zhang X, 2010, VISUAL COMPUT, V26, P985, DOI 10.1007/s00371-010-0431-4
NR 34
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2014
VL 30
IS 1
BP 113
EP 126
DI 10.1007/s00371-013-0787-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 291AR
UT WOS:000329800500009
DA 2024-07-18
ER

PT J
AU Kim, ST
   Hong, JM
AF Kim, Sun-Tae
   Hong, Jeong-Mo
TI Visual simulation of turbulent fluids using MLS interpolation profiles
SO VISUAL COMPUTER
LA English
DT Article
DE Moving least-squares; Turbulent smoke; Simulation control; Fluid
   simulation
ID CUBIC-POLYNOMIAL INTERPOLATION; HYPERBOLIC-EQUATIONS; UNIVERSAL SOLVER;
   ANIMATION
AB A detailed description of turbulent fluids based on numerical simulation is an important research topic required by many visual effects. We propose a novel method to simulate fluids with turbulent small-scale details. By inserting diffusive derivatives and divergence-free constraints to moving least-squares (MLS) fitting, we upgrade the velocity interpolation method for existing fluid solvers to enhance the subgrid accuracy. The time-step restriction of asymptotic property of diffusive derivatives is resolved by means of coupling to the constrained interpolation profile (CIP) advection framework. The proposed constrained moving least-squares interpolation profile (CMIP) method provides intuitive control over turbulence through the adjustment of one parameter as though controlling the Reynolds number with an inviscid model. The proposed method generates improved visuals of the highly turbulent fluid and is complementary to existing techniques that are currently being used.
C1 [Kim, Sun-Tae; Hong, Jeong-Mo] Dongguk Univ Seoul, Seoul 100715, South Korea.
C3 Dongguk University
RP Hong, JM (corresponding author), Dongguk Univ Seoul, 30 Pildong Ro 1 Gil 26 Pildong 3, Seoul 100715, South Korea.
EM stkim@atelierj.pro; jmhong@atelierj.pro
FU Ministry of Culture, Sports and Tourism (MCST); Korea Creative Content
   Agency (KOCCA) in the Culture Technology (CT) Research & Development
   Program
FX This Research is supported by Ministry of Culture, Sports and Tourism
   (MCST) and Korea Creative Content Agency (KOCCA) in the Culture
   Technology (CT) Research & Development Program 2012.
CR Bridson R, 2007, ACM T GRAPHIC, V26, P26, DOI DOI 10.1145/1239451.1239497
   Chorin A. J., 1997, Journal of Computational Physics, V135, P118, DOI 10.1006/jcph.1997.5716
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Feldman BE, 2005, ACM T GRAPHIC, V24, P904, DOI 10.1145/1073204.1073281
   Foster N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P181, DOI 10.1145/258734.258838
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Fresch U., 1996, TURBULENCE LEGACY AN
   Gao Y, 2009, COMPUT GRAPH FORUM, V28, P1845, DOI 10.1111/j.1467-8659.2009.01562.x
   Greenwood S., 2004, Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P287
   Hong JM, 2008, COMPUT ANIMAT VIRT W, V19, P469, DOI 10.1002/cav.236
   Hong JM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360647
   Hong JM, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239498
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Huerta A, 2004, COMPUT METHOD APPL M, V193, P1119, DOI 10.1016/j.cma.2003.12.010
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P467, DOI 10.1111/j.1467-8659.2008.01144.x
   Kim D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618466
   Kim J., 2006, Proc ACM SIGGRAPH/Eurograph Symp Comp Anim, SCA '06, P335
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Lentine M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778851
   Lorenz E., 1996, ESSENCE CHAOS
   Losasso F, 2008, IEEE T VIS COMPUT GR, V14, P797, DOI 10.1109/TVCG.2008.37
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Nayroles B., 1992, COMPUT MECH, V10, P307, DOI [DOI 10.1007/BF00364252, 10.1007/BF00364252]
   Nealen A., 2004, An As-Short-As-Possible Introduction to the Least Squares, Weighted Least Squares and Moving Least Squares Methods for Scattered Data Approximation and Interpolation
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Pfaff T., 2012, ACM SIGGRAPH 2012 PA
   Schechter H., 2008, P 2008 ACM EUR S COM
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Shen C., 2004, ACM T GRAPHICS SIGGR, V31, P321
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thürey N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P191
   YABE T, 1991, COMPUT PHYS COMMUN, V66, P219, DOI 10.1016/0010-4655(91)90071-R
   YABE T, 1991, COMPUT PHYS COMMUN, V66, P233, DOI 10.1016/0010-4655(91)90072-S
NR 36
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1293
EP 1302
DI 10.1007/s00371-012-0770-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200006
DA 2024-07-18
ER

PT J
AU Zhu, C
   Leow, WK
AF Zhu, Chen
   Leow, Wee Kheng
TI Textured mesh surface reconstruction of large buildings with multi-view
   stereo
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Building reconstruction; Multi-view stereo; Mesh surface reconstruction
AB There are three main approaches for reconstructing 3D models of buildings. Laser scanning is accurate but expensive and limited by the laser's range. Structure-from-motion (SfM) and multi-view stereo (MVS) recover 3D point clouds from multiple views of a building. MVS methods, especially patch-based MVS, can achieve higher density than do SfM methods. Sophisticated algorithms need to be applied to the point clouds to construct mesh surfaces. The recovered point clouds can be sparse in areas that lack features for accurate reconstruction, making recovery of complete surfaces difficult. Moreover, segmentation of the building's surfaces from surrounding surfaces almost always requires some form of manual inputs, diminishing the ease of practical application of automatic 3D reconstruction algorithms. This paper presents an alternative approach for reconstructing textured mesh surfaces from point cloud recovered by patch-based MVS method. To a good first approximation, a building's surfaces can be modeled by planes or curve surfaces which are fitted to the point cloud. 3D points are resampled on the fitted surfaces in an orderly pattern, whose colors are obtained from the input images. This approach is simple, inexpensive, and effective for reconstructing textured mesh surfaces of large buildings. Test results show that the reconstructed 3D models are sufficiently accurate and realistic for 3D visualization in various applications.
C1 [Zhu, Chen; Leow, Wee Kheng] Natl Univ Singapore, Dept Comp Sci, Singapore 117417, Singapore.
C3 National University of Singapore
RP Leow, WK (corresponding author), Natl Univ Singapore, Dept Comp Sci, 13 Comp Dr, Singapore 117417, Singapore.
EM zhuchen@comp.nus.edu.sg; leowwk@comp.nus.edu.sg
CR Allen PK, 2003, IEEE COMPUT GRAPH, V23, P32, DOI 10.1109/MCG.2003.1242380
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Blais F, 2004, J ELECTRON IMAGING, V13, P231, DOI 10.1117/1.1631921
   CORNELIS N, 2006, P CVPR
   Cornelis N, 2008, INT J COMPUT VISION, V78, P121, DOI 10.1007/s11263-007-0081-9
   Dellaert F., 2000, P IEEE CVPR
   Früh C, 2003, IEEE COMPUT GRAPH, V23, P52, DOI 10.1109/MCG.2003.1242382
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Yasutaka., PATCH BASED MULTIVIE
   Furukawa Yasutaka, 2010, P CVPR
   Guidi G, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P565, DOI 10.1109/3DIM.2005.2
   Havlena M., 2009, P CVPR
   Havlena M., 2010, P ECCV
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Jancosek M., 2011, P CVPR
   Labatut P., 2007, P CVPR
   Liang Y., 2013, 3D RECONSTRUCTION AR
   Micusik B., 2009, P CVPR
   Moons T, 2008, FOUND TRENDS COMPUT, V4, P287, DOI 10.1561/0600000007
   Snavely N., 2006, P SIGGRAPH
   Snavely N., BUNDLER STRUCTURE MO
   Xiao J., 2009, P SIGGRAPH AS
NR 22
TC 7
Z9 7
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 609
EP 615
DI 10.1007/s00371-013-0827-z
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400015
DA 2024-07-18
ER

PT J
AU Zhu, XQ
   Jin, XG
   Liu, SJ
   Zhao, HL
AF Zhu, Xiaoqiang
   Jin, Xiaogang
   Liu, Shengjun
   Zhao, Hanli
TI Analytical solutions for sketch-based convolution surface modeling on
   the GPU
SO VISUAL COMPUTER
LA English
DT Article
DE Convolution surface; Closed-form solution; Planar polygon skeleton;
   Sketch-based modeling; CUDA
AB Convolution surfaces are attractive for modeling objects of complex evolving topology. This paper presents some novel analytical convolution solutions for planar polygon skeletons with both finite-support and infinite-support kernel functions. We convert the double integral over a planar polygon into a simple integral along the contour of the polygon based on Green's theorem, which reduces the computational cost and allows for efficient parallel computation on the GPU. For finite support kernel functions, a skeleton clipping algorithm is presented to compute the valid skeletons. The analytical solutions are integrated into a prototype modeling system on the GPU (Graphics Processing Unit). Our modeling system supports point, polyline and planar polygon skeletons. Complex objects with arbitrary genus can be modeled easily in an interactive way. Resulting convolution surfaces with high quality are rendered with interactive ray casting.
C1 [Zhu, Xiaoqiang; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liu, Shengjun] Cent S Univ, Sch Math Sci & Comp Technol, Changsha 410083, Peoples R China.
   [Zhao, Hanli] Wenzhou Univ, Coll Phys & Elect Informat Engn, Wenzhou 325035, Peoples R China.
C3 Zhejiang University; Central South University; Wenzhou University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM zhuxiaoqiang@zjucadcg.cn; jin@cad.zju.edu.cn; shjliu.cg@gmail.com;
   hanlizhao@gmail.com
RI Liu, Shengjun/AAI-8456-2020
FU National Key Basic Research Foundation of China [2009CB320801];
   NSFC-MSRA Joint Funding [60970159]; National Natural Science Foundation
   of China [60933007, 61173119]; Zhejiang Provincial Natural Science
   Foundation of China [Z1110154]
FX Xiaogang Jin was supported by the National Key Basic Research Foundation
   of China (Grant No. 2009CB320801), the NSFC-MSRA Joint Funding (Grant
   No. 60970159), the National Natural Science Foundation of China (Grant
   No. 60933007), and the Zhejiang Provincial Natural Science Foundation of
   China (Grant No. Z1110154). Shengjun Liu was supported by the National
   Natural Science Foundation of China (Grant No. 61173119).
CR ALEXE A., 2004, P AFRIGRAPH, P25
   ALEXE A, 2005, PACIFIC GRAPHICS
   Angelidis A., 2002, P ACM S SOLID MODELI, P45
   [Anonymous], J GRAPH TOOLS
   Bernhardt A., 2008, EUROGRAPHICS WORKSH, P57
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   David F., 1997, PROCEDURAL ELEMENTS
   Eyiyurekli M., 2009, Proceedings o f the 6th Eurographics Symposium on Sketch-Based Interfaces and Modeling, SBIM '09, P45, DOI DOI 10.1145/1572741.1572750
   Gourmel O, 2010, COMPUT GRAPH FORUM, V29, P281, DOI 10.1111/j.1467-8659.2009.01597.x
   Hubert E, 2012, GRAPH MODELS, V74, P1, DOI 10.1016/j.gmod.2011.07.001
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Jin XG, 2002, VISUAL COMPUT, V18, P530, DOI 10.1007/s00371-002-0161-3
   Jin XG, 2002, COMPUT GRAPH-UK, V26, P437, DOI 10.1016/S0097-8493(02)00087-0
   Jin XG, 2009, VISUAL COMPUT, V25, P279, DOI 10.1007/s00371-008-0267-3
   Kanamori Y, 2008, COMPUT GRAPH FORUM, V27, P351, DOI 10.1111/j.1467-8659.2008.01132.x
   Karpenko O, 2002, COMPUT GRAPH FORUM, V21, P585, DOI 10.1111/1467-8659.t01-1-00709
   Karpenko OA, 2006, ACM T GRAPHIC, V25, P589, DOI 10.1145/1141911.1141928
   Knoll A, 2009, COMPUT GRAPH FORUM, V28, P26, DOI 10.1111/j.1467-8659.2008.01189.x
   Kravtsov D, 2010, COMPUT GRAPH FORUM, V29, P128, DOI 10.1111/j.1467-8659.2009.01582.x
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   McCormack J, 1998, COMPUT GRAPH FORUM, V17, P113, DOI 10.1111/1467-8659.00232
   Schmidt R., 2005, EUROGRAPHICS WORKSHO, P53
   Schmidt R., 2005, SIGGRAPH TECHNICAL S
   Sherstyuk A, 1999, VISUAL COMPUT, V15, P171, DOI 10.1007/s003710050170
   Sherstyuk A, 1999, COMPUT GRAPH FORUM, V18, P139, DOI 10.1111/1467-8659.00364
   Tai CL, 2004, COMPUT GRAPH FORUM, V23, P71, DOI 10.1111/j.1467-8659.2004.00006.x
   Wilfred K., 2002, ADV CALCULUS
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   Wyvill B., 1996, SIGGRAPH COURSES
   Wyvill Brian., 2005, P 1 EUROGRAPHICS C C, P67
NR 30
TC 13
Z9 14
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1115
EP 1125
DI 10.1007/s00371-011-0662-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300005
DA 2024-07-18
ER

PT J
AU Dey, TK
   Ranjan, P
   Wang, YS
AF Dey, Tamal K.
   Ranjan, Pawas
   Wang, Yusu
TI Eigen deformation of 3D models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Laplace operator; Eigenvectors; Eigenspace; Deformation; Animation
AB Recent advances in mesh deformations have been dominated by two techniques: one uses an intermediate structure like a cage which transfers the user intended moves to the mesh, the other lets the user to impart the moves to the mesh directly. The former one lets the user deform the model in real-time and also preserve the shape with sophisticated techniques like Green Coordinates. The direct techniques on the other hand free the user from the burden of creating an appropriate cage though they take more computing time to solve larger non-linear optimizations. It would be ideal to develop a cage-free technique that provides real-time deformation while respecting the local geometry. Using a simple eigen-framework, we devise such a technique. Our framework creates an implicit skeleton automatically. The user only specifies the motion in a simple and intuitive manner, and our algorithm computes a deformation whose quality is similar to that of the cage-based scheme with Green Coordinates.
C1 [Dey, Tamal K.; Ranjan, Pawas; Wang, Yusu] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Ranjan, P (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM tamaldey@cse.ohio-state.edu; ranjan@cse.ohio-state.edu;
   yusu@cse.ohio-state.edu
RI Dey, Tamal Krishna/D-4989-2012
FU Division of Computing and Communication Foundations; Direct For Computer
   & Info Scie & Enginr [0747082] Funding Source: National Science
   Foundation
CR Baran I., 2007, P SIGGRAPH 07, P72
   Belkin M, 2008, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SGG'08), P278, DOI 10.1145/1377676.1377725
   Ben-Chen M., 2009, P SIGGRAPH 09, P34
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch M., 2006, SGP 06, P11
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Dey TK, 2010, PROC APPL MATH, V135, P650
   Du H., 2004, SM 04, P25
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Hildebrandt K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019638
   Hildebrandt K, 2011, COMPUT GRAPH FORUM, V30, P1513, DOI 10.1111/j.1467-8659.2011.02025.x
   Jacobson A., 2011, P SIGGRAPH 11, P78
   Joshi P., 2007, P SIGGRAPH 07, P71
   Ju T, 2005, Proceedings of the 3rd Symposium on Geometry Processing, P181, DOI DOI 10.2312/SGP/SGP05/181
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kobayashi K.G., 2003, Proceedings of the eighth ACM symposium on Solid modeling and applications. SM '03, P226, DOI DOI 10.1145/781606.781641.2
   Langer T., 2006, S GEOMETRY PROCESSIN, P81
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Lipman Y., 2008, P SIGGRAPH 08, P78
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rong G, 2008, CASA, P17
   Rong GD, 2008, VISUAL COMPUT, V24, P787, DOI 10.1007/s00371-008-0260-x
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Sorkine O, 2006, COMPUT GRAPH FORUM, V25, P789, DOI 10.1111/j.1467-8659.2006.00999.x
   Warren J, 1996, ADV COMPUT MATH, V6, P97, DOI 10.1007/BF02127699
   Weber O., COMPUT GRAPH FORUM, P265
   Yoshizawa Shin., 2003, P 8 ACM S SOLID MODE, P247, DOI DOI 10.1145/781606.781643
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 35
TC 7
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 585
EP 595
DI 10.1007/s00371-012-0705-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500007
DA 2024-07-18
ER

PT J
AU Li, MF
   Tong, RF
AF Li, Mengfei
   Tong, Ruofeng
TI All-hexahedral mesh generation via inside-out advancing front based on
   harmonic fields
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Hexahedral mesh; Harmonic field; Advancing front
ID PARAMETERIZATION; ALGORITHM
AB The generation of hexahedral meshes is an open problem that has undergone significant research. This paper deals with a novel inside-out advancing front method to generate unstructured all-hexahedral meshes for given volumes. Two orthogonal harmonic fields, principal and radial harmonic fields, are generated to guide the inside-out advancing front process based on a few user interactions. Starting from an initial hexahedral mesh inside the given volume, we advance the boundary quadrilateral mesh along the streamlines of radial field and construct layers of hexahedral elements. To ensure high quality and uniform size of the hexahedral mesh, quadrilateral elements are decomposed in such a way that no non-hexahedral element is produced. For complex volume with branch structures, we segment the complex volume into simple sub-volumes that are suitable for our method. Experimental results show that our method generates high quality all-hexahedral meshes for the given volumes.
C1 [Li, Mengfei; Tong, Ruofeng] Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Tong, RF (corresponding author), Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM trf@zju.edu.cn
CR Alliez P, 2005, ACM T GRAPHIC, V24, P617, DOI 10.1145/1073204.1073238
   [Anonymous], 1995, INT MESH ROUNDT
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Baker TJ, 2005, PROG AEROSP SCI, V41, P29, DOI 10.1016/j.paerosci.2005.02.002
   Benhabiles H, 2010, VISUAL COMPUT, V26, P1451, DOI 10.1007/s00371-010-0494-2
   BLACKER TD, 1993, ENG COMPUT, V9, P83, DOI 10.1007/BF01199047
   Canann S., 1992, PLASTERING NEW APPRO
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cifuentes A., 1992, Finite Elem. Anal. Des., V12, P313, DOI DOI 10.1016/0168-874X(92)90040-J
   Du Q, 2003, INT J NUMER METH ENG, V56, P1355, DOI 10.1002/nme.616
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Golovinskiy A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409098
   Gregson J, 2011, COMPUT GRAPH FORUM, V30, P1407, DOI 10.1111/j.1467-8659.2011.02015.x
   Jiazhi Xia, 2010, Proceedings of the Shape Modeling International (SMI 2010), P3, DOI 10.1109/SMI.2010.10
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Knupp PM, 2003, FINITE ELEM ANAL DES, V39, P217, DOI 10.1016/S0168-874X(02)00070-7
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lai YK, 2010, IEEE T VIS COMPUT GR, V16, P95, DOI 10.1109/TVCG.2009.59
   Lai YK, 2009, COMPUT AIDED GEOM D, V26, P665, DOI 10.1016/j.cagd.2008.09.007
   Levy B., 2010, ACM T GRAPHIC, V29, DOI [10.1145/1833349.1778856, DOI 10.1145/1833349.1778856]
   Liao SH, 2009, COMPUT GRAPH-UK, V33, P424, DOI 10.1016/j.cag.2009.03.018
   Maréchal L, 2009, PROCEEDINGS OF THE 18TH INTERNATIONAL MESHING ROUNDTABLE, P65, DOI 10.1007/978-3-642-04319-2_5
   Martin T, 2009, COMPUT AIDED GEOM D, V26, P648, DOI 10.1016/j.cagd.2008.09.008
   Nieser M, 2011, COMPUT GRAPH FORUM, V30, P1397, DOI 10.1111/j.1467-8659.2011.02014.x
   Owen S. J., 1998, P 7 INT MESHING ROUN, P239
   Schneiders R, 1996, ENG COMPUT, V12, P168, DOI 10.1007/BF01198732
   Schneiders R, 1996, 5 INT MESH ROUNDT, P205
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shepherd JF, 2008, ENG COMPUT-GERMANY, V24, P195, DOI 10.1007/s00366-008-0091-4
   Shepherd JF, 2009, ENG COMPUT-GERMANY, V25, P97, DOI 10.1007/s00366-008-0108-z
   Shewchuk J. R., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P86, DOI 10.1145/276884.276894
   Si H., 2006, 14 WEIERSTR I APPL A
   Staten ML, 2010, INT J NUMER METH ENG, V81, P135, DOI 10.1002/nme.2679
   Tautges TJ, 1996, INT J NUMER METH ENG, V39, P3327
   Vyas V, 2009, PROCEEDINGS OF THE 18TH INTERNATIONAL MESHING ROUNDTABLE, P377, DOI 10.1007/978-3-642-04319-2_22
   Wang Y., 2003, COMMUN INF SYST, V3, P191, DOI DOI 10.4310/CIS.2003.V3.N3.A4
NR 37
TC 3
Z9 5
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 839
EP 847
DI 10.1007/s00371-012-0707-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500031
DA 2024-07-18
ER

PT J
AU Hollemeersch, CF
   Pieters, B
   Lambert, P
   Van de Walle, R
AF Hollemeersch, Charles-Frederik
   Pieters, Bart
   Lambert, Peter
   Van de Walle, Rik
TI A new approach to combine texture compression and filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Texture compression; Texture filtering; DCT
ID STANDARD
AB Texture mapping has been widely used to improve the quality of 3D rendered images. To reduce the storage and bandwidth impact of texture mapping, compression systems are commonly used. To further increase the quality of the rendered images, texture filtering is also often adopted. These two techniques are generally considered to be independent. First, a decompression step is executed to gather texture samples, which is then followed by a separate filtering step. We have investigated a system based on linear transforms that merges both phases together. This allows more efficient decompression and filtering at higher compression ratios. This paper formally presents our approach for any linear transformation, how the commonly used discrete cosine transform can be adapted to this new approach, and how this method can be implemented in real time on current-generation graphics cards using shaders. Through reuse of the existing hardware filtering, fast magnification and minification filtering is achieved. Our implementation provides fully anisotropically filtered samples four to six times faster than an implementation using two separate phases for decompression and filtering. Additionally, our transform-based compression also provides increased and variable compression ratios over standard hardware compression systems at a comparable or better quality level.
C1 [Hollemeersch, Charles-Frederik; Pieters, Bart; Lambert, Peter; Van de Walle, Rik] Multimedia Lab, B-9050 Ledeberg Ghent, Belgium.
RP Hollemeersch, CF (corresponding author), Multimedia Lab, Gaston Crommenlaan 8,Bus 201, B-9050 Ledeberg Ghent, Belgium.
EM charlesfrederik.hollemeersch@ugent.be; bart.pieters@ugent.be;
   peter.lambert@ugent.be; rik.vandewalle@ugent.be
RI Lambert, Peter/D-7776-2016
OI Lambert, Peter/0000-0001-5313-4158
FU Ghent University; Interdisciplinary Institute for Broadband Technology
   (IBBT); Institute for the Promotion of Innovation by Science and
   Technology in Flanders (IWT); Fund for Scientific Research-Flanders
   (FWOFlanders); Belgian Federal Science Policy Office (BFSPO); European
   Union
FX The research activities that have been described in this paper were
   funded by Ghent University, the Interdisciplinary Institute for
   Broadband Technology (IBBT), the Institute for the Promotion of
   Innovation by Science and Technology in Flanders (IWT), the Fund for
   Scientific Research-Flanders (FWOFlanders), the Belgian Federal Science
   Policy Office (BFSPO), and the European Union.
CR [Anonymous], 2003, HWWS'03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware
   Blow J., 2001, GAME DEV MAGAZINE
   Brown P., 2007, TEXTURE COMPRESSION
   Brown S., 2007, SQUISH DXT COMPRESSI
   Chen CH, 2002, VISUAL COMPUT, V18, P29, DOI 10.1007/s003710100130
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   Dugad R, 2001, IEEE T CIRC SYST VID, V11, P461, DOI 10.1109/76.915353
   Goodwin M.M., 1998, SPRINGER INT SERIES
   Heckbert P., 1989, THESIS CS DIVISION U
   Hou H. S., 1992, Journal of Visual Communication and Image Representation, V3, P73, DOI 10.1016/1047-3203(92)90031-N
   Iourcha K., 1999, U.S. Patent, Patent No. 5956431
   Leung C.S., 2006, SHADER X4, P251
   Malvar H. S., 2003, JVTI014
   Martucci S.A., 1995, P 1995 INT C IM PROC, V2, P2244
   McCormack J, 1999, COMP GRAPH, P243, DOI 10.1145/311535.311562
   Munkberg J, 2006, ACM T GRAPHIC, V25, P698, DOI 10.1145/1141911.1141944
   Poynton C.A., 2003, DIGITAL VIDEO HDTV A, P87
   Rao K., 2000, TRANSFORM DATA COMPR, P1
   Roimela K., 2008, P I3D, P207, DOI 10.1145/1342250.1342282
   Segal M., 2006, The OpenGL Graphics System: A Speci cation
   Stachera J., 2006, WSCG 06 C P
   Van Waveren J. M. P., 2007, REAL TIME YCOCG DXT
   van Waveren J.P., 2008, REAL TIME NORMAL MAP
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Williams L., 1983, COMPUT GRAPH, V17, P1, DOI DOI 10.1145/964967.801126
NR 26
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 371
EP 385
DI 10.1007/s00371-011-0621-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300004
DA 2024-07-18
ER

PT J
AU Li, Z
   Jin, Y
   Jin, XG
   Ma, LZ
AF Li, Zhong
   Jin, Yao
   Jin, Xiaogang
   Ma, Lizhuang
TI Approximate straightest path computation and its application in
   parameterization
SO VISUAL COMPUTER
LA English
DT Article
DE Optimal cutting plane; Straightest path; Mesh parameterization; Vertex
   stretch; Measured boundary
ID SURFACE
AB This paper proposes an approaching method to compute the straightest path between two vertices on meshes. An initial cutting plane is first constructed using the normal information of the source and destination vertices. Then an optimal cutting plane is iteratively created by comparing with previous path distance. Our study shows that the final straightest path based on this optimal cutting plane is more accurate and insensitive to the mesh boundary. Furthermore, we apply the straightest path result to compute the measured boundary in the parameter domain for mesh parameterization, and we obtain a new computing formula for vertex stretch in the planar parameterization. Experimental results show that our parameterization method can effectively reduce distortions.
C1 [Li, Zhong; Jin, Yao] Zhejiang Sci Tech Univ, Dept Math & Sci, Hangzhou 310018, Peoples R China.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
C3 Zhejiang Sci-Tech University; Zhejiang University; Shanghai Jiao Tong
   University
RP Li, Z (corresponding author), Zhejiang Sci Tech Univ, Dept Math & Sci, Hangzhou 310018, Peoples R China.
EM lizhongzju@hotmail.com
FU National Natural Science Foundation of China [60903143, 51075421];
   Natural Science Foundation of Zhejiang Province of China [Y1090141,
   Y1110504]; Qianjiang Talent Project of Zhejiang Province of China
   [QJD0902006]; Zhejiang Provincial Natural Science Foundation of China
   [Z1110154]; Science and Technology Plan of Zhejiang Province
   [2010C13G2010068]
FX The authors gratefully appreciate the anonymous referees for the useful
   comments and suggestions which helped us improve this paper
   considerably. This research was supported by the National Natural
   Science Foundation of China under Grant Nos. 60903143 and 51075421;
   Natural Science Foundation of Zhejiang Province of China under Grant
   Nos. Y1090141 and Y1110504 and Qianjiang Talent Project of Zhejiang
   Province of China under Grant No. QJD0902006. Xiaogang Jin is supported
   by the Zhejiang Provincial Natural Science Foundation of China (Grant
   No. Z1110154) and the Science and Technology Plan of Zhejiang Province
   (2010C13G2010068).
CR Ben-Chen M, 2008, COMPUT GRAPH FORUM, V27, P449, DOI 10.1111/j.1467-8659.2008.01142.x
   CHEN JD, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P360, DOI 10.1145/98524.98601
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   He Y, 2009, COMPUT GRAPH-UK, V33, P369, DOI 10.1016/j.cag.2009.03.024
   Hormann K., 2008, ACM SIGGRAPH ASIA 2008 courses, p12:1, DOI DOI 10.1145/1508044.1508091
   Kanai T, 2001, COMPUT AIDED DESIGN, V33, P801, DOI 10.1016/S0010-4485(01)00097-5
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Lee HY, 2005, IEEE MULTIMEDIA, V12, P27, DOI 10.1109/MMUL.2005.5
   Lee SY, 2006, LECT NOTES COMPUT SC, V4077, P609
   Lee Y, 2002, COMPUT GRAPH-UK, V26, P677, DOI 10.1016/S0097-8493(02)00123-1
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   MITCHELL JSB, 1987, SIAM J COMPUT, V16, P647, DOI 10.1137/0216045
   Peyré G, 2005, PROG NONLINEAR DIFFE, V63, P157
   POLTHIER K, 1998, MATH VISUALIZATION, P366
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Sheffer A, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000011
   Springborn B, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360676
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Verma V., 2009, P ACM GIS, P227
   Xin SQ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559761
   Yoshizawa S, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P200
NR 25
TC 3
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 63
EP 74
DI 10.1007/s00371-011-0600-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000006
DA 2024-07-18
ER

PT J
AU Hsiao, JPL
   Healey, CG
AF Hsiao, Joe Ping-Lin
   Healey, Christopher G.
TI Visualizing combinatorial auctions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Combinatorial auction; Ecommerce; Perception; Visualization
AB We propose a novel scheme to visualize combinatorial auctions; auctions that involve the simultaneous sale of multiple items. Buyers bid on complementary sets of items, or bundles, where the utility of securing all the items in the bundle is more than the sum of the utility of the individual items. Our visualizations use concentric rings divided into arcs to visualize the bundles in an auction. The arcs' positions and overlaps allow viewers to identify and follow bidding strategies. Properties of color, texture, and motion are used to represent different attributes of the auction, including active bundles, prices bid for each bundle, winning bids, and bidders' interests. Keyframe animations are used to show changes in an auction over time. We demonstrate our visualization technique on a standard testbed dataset generated by researchers to evaluate combinatorial auction bid strategies, and on recent Federal Communications Commission (FCC) auctions designed to allocate wireless spectrum licenses to cell phone service providers.
C1 [Hsiao, Joe Ping-Lin; Healey, Christopher G.] N Carolina State Univ, Dept Comp Sci, Raleigh, NC 27695 USA.
C3 North Carolina State University
RP Healey, CG (corresponding author), N Carolina State Univ, Dept Comp Sci, Raleigh, NC 27695 USA.
EM healey@csc.ncsu.edu
RI Healey, Christopher/ABH-9682-2020
CR Bartram L., 2002, Information Visualization, V1, P66, DOI 10.1057/palgrave/ivs/9500005
   CIE, 1978, CIE PUBLICATION S2, V15
   CRAMTON P, 2002, HDB TELECOMMUNICATIO
   Goeree J., 2007, EXPT COMP FLEXIBLE T
   Golumbic MC, 2005, Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications, P41
   Healey CG, 1996, IEEE VISUAL, P263, DOI 10.1109/VISUAL.1996.568118
   Healey CG, 1999, IEEE T VIS COMPUT GR, V5, P145, DOI 10.1109/2945.773807
   Huber DE, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P527
   Kosara R, 2002, IEEE COMPUT GRAPH, V22, P22, DOI 10.1109/38.974515
   Ono C, 2003, IKE'03: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE ENGINEERING, VOLS 1 AND 2, P445
   RASSENTI SJ, 1982, BELL J ECON, V13, P402, DOI 10.2307/3003463
   Song JJ, 2005, TRANSPORT RES B-METH, V39, P914, DOI 10.1016/j.trb.2004.11.003
   Venn J., 1880, Dublin Philosophical Magazine and Journal of Science, V10, P1, DOI DOI 10.1080/14786448008626877
   WARE C, 1988, IEEE COMPUT GRAPH, V8, P41, DOI 10.1109/38.7760
   Ware C., 2020, INFORM VISUALIZATION
   Wurman P. R., 2004, ELECTRON COMMER R A, V3, P329
   ZUREL E, 2001, P 3 ACM C EL COMM, P125
NR 17
TC 2
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 633
EP 643
DI 10.1007/s00371-011-0576-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600022
DA 2024-07-18
ER

PT J
AU Johan, H
   Li, B
   Wei, YM
   Iskandarsyah
AF Johan, Henry
   Li, Bo
   Wei, Yuanmin
   Iskandarsyah
TI 3D model alignment based on minimum projection area
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE 3D model alignment; Minimum projection area; 3D model retrieval
AB 3D model alignment is an important step for applications such as 3D model retrieval and 3D model recognition. In this paper, we propose a novel Minimum Projection Area-based (MPA) alignment method for pose normalization. Our method finds three principal axes to align a model: the first principal axis gives the minimum projection area when we perform an orthographic projection of the model in the direction parallel to this axis, the second axis is perpendicular to the first axis and gives the minimum projection area, and the third axis is the cross product of the first two axes. We devise an optimization method based on Particle Swarm Optimization to efficiently find the axis with minimum projection area. For application in retrieval, we further perform axis ordering and orientation in order to align similar models in similar poses. We have tested MPA on several standard databases which include rigid/non-rigid and open/watertight models. Experimental results demonstrate that MPA has a good performance in finding alignment axes which are parallel to the ideal canonical coordinate frame of models and aligning similar models in similar poses under different conditions such as model variations, noise, and initial poses. In addition, it achieves a better 3D model retrieval performance than several commonly used approaches such as CPCA, NPCA, and PCA.
C1 [Johan, Henry; Li, Bo; Wei, Yuanmin; Iskandarsyah] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
C3 Nanyang Technological University
RP Johan, H (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM henryjohan@ntu.edu.sg
CR [Anonymous], UUCS2007015 UTR U DE
   [Anonymous], 2005, COMPUTER AIDED DESIG
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Eberhart R. C., 1999, Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), P1927, DOI 10.1109/CEC.1999.785508
   Fang R, 2008, LECT NOTES COMPUT SC, V5358, P381, DOI 10.1007/978-3-540-89639-5_37
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Loop C, 1987, THESIS U UTAH
   Martinek M, 2009, COMPUT GRAPH-UK, V33, P291, DOI 10.1016/j.cag.2009.03.023
   Napoléon T, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/367181
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Podolak J., 2006, ACM T GRAPH, V25, P2
   PU JT, 2005, P ASME IDETC CIE 200, P301
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Tedjokusumo J, 2007, LECT NOTES COMPUT SC, V4351, P74
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   VRANIC DV, 2004, THESIS U LEIPZIG
   Yamauchi H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P265
NR 23
TC 14
Z9 17
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 565
EP 574
DI 10.1007/s00371-011-0590-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600015
DA 2024-07-18
ER

PT J
AU Wang, C
   Wan, TR
   Palmer, IJ
AF Wang, C.
   Wan, T. R.
   Palmer, I. J.
TI Urban flood risk analysis for determining optimal flood protection
   levels based on digital terrain model and flood spreading model
SO VISUAL COMPUTER
LA English
DT Article
DE Urban flood risk analysis; Digital terrain model; Flood spreading model;
   Active contour model
ID ACTIVE CONTOURS
AB The objective of the paper is to present a new risk-analysis approach for the assessment of optimal flood protection levels in urban flood risk management, which is based on an active contour method. Although the active contour method is a very popular research topic, there has been no attempt made on deriving a model for simulating flooding and inundating to date, as far as we are aware. We have developed a flooding prototype system, which consists of two main parts: a digital terrain model and a flood simulation model. The digital terrain model is constructed using real world measurement data of GIS, in terms of digital elevation data and satellite image data. A pyramidal data arrangement structure is used for dealing with the requirements of terrain details with different resolutions. A new flooding model has been developed, which is useful for urban flood simulation. It consists of a flooding image spatial segmentation based on an active contour model, a water level calculation process, a standard gradient descent method for energy minimisation. When testing the 3D flood simulation system, the simulation results are very close to the real flood situation, and this method has faster speed and greater accuracy of simulating the inundation area in comparison to the conventional 2D flood simulation models.
C1 [Wang, C.; Wan, T. R.; Palmer, I. J.] Univ Bradford, Sch Comp Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Bradford
RP Wang, C (corresponding author), Univ Bradford, Sch Comp Informat & Media, Bradford BD7 1DP, W Yorkshire, England.
EM c.wang4@bradford.ac.uk; t.wan@bradford.ac.uk; i.j.palmer@bradford.ac.uk
OI Wang, Chen/0000-0001-9099-5483
CR AHUJA N, 1984, COMPUT VISION GRAPH, V26, P207, DOI 10.1016/0734-189X(84)90183-X
   BENDER J, 2004, P 17 INT C COMP AN S
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   CHEN HH, 1988, COMPUT VISION GRAPH, V43, P409, DOI 10.1016/0734-189X(88)90092-8
   Debevec P, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P234, DOI 10.1109/IM.2003.1240255
   Douglas J.F., 2001, FLUID MECH-SOV RES, VFourth
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   Gianinetto M, 2006, IEEE T GEOSCI REMOTE, V44, P236, DOI 10.1109/TGRS.2005.859952
   HAILE A, 2005, ISPRS WG 3 3 3 4 5 3
   HAKIM S, 2002, P ISPRS COMM 5 S, P143
   *HC, 2004, 16 HC ENV FOOD RUR A
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   LINDSTROM P, 1996, P SIGGRAPH 1996 AUG
   LIU YL, 2005, P GEOSC REM SENS S 2, V6, P4395
   Sequeira V, 2001, PROC SPIE, V4309, P126
   Ulrich T., 2000, CONTINUOUS LOD TERRA
   Wang C, 2007, IEEE INT CONF INF VI, P607
NR 18
TC 8
Z9 9
U1 0
U2 44
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1369
EP 1381
DI 10.1007/s00371-009-0414-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700004
DA 2024-07-18
ER

PT J
AU Kanezaki, A
   Harada, T
   Kuniyoshi, Y
AF Kanezaki, Asako
   Harada, Tatsuya
   Kuniyoshi, Yasuo
TI Partial matching of real textured 3D objects using color cubic
   higher-order local auto-correlation features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE Real object retrieval; Shape and texture features; Partial matching;
   Real-time 3D processing; Object detection
ID RECOGNITION; SEARCH
AB In recent years, the need for retrieving real 3D objects has grown significantly. However, various important considerations must be taken into account to solve the real 3D object retrieval problem. Three-dimensional models obtained without the use of special equipment such as engineered environments or multi-camera systems are often incomplete. Therefore, the ability to perform partial matching is essential. Moreover, the time required for the matching process must be relatively short, since the operation will need to be performed repeatedly to deal with the dynamic nature of day-to-day human environments. Furthermore, real models often include rich texture information, which can compensate for the limited shape information. Thus, the descriptors of the 3D models have to consider both shape and texture patterns. In this paper, we present new 3D shape features which take into account the object's texture. The additive property of these features enables efficient partial matching between query data and 3D models in a database. In the experiments, we compare these features with conventional features, namely Spin-Image, Textured Spin-Image, and CHLAC features using a dataset of real textured objects. Furthermore, we demonstrate the retrieval performance of these features on a real color 3D scene.
C1 [Kanezaki, Asako] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.
   [Harada, Tatsuya] Univ Tokyo, Intelligent Cooperat Syst Lab, Tokyo, Japan.
   [Kuniyoshi, Yasuo] Univ Tokyo, Sch Informat Sci & Technol, Dept Mechanoinformat, Tokyo, Japan.
C3 University of Tokyo; University of Tokyo; University of Tokyo
RP Kanezaki, A (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.
EM kanezaki@isi.imi.i.u-tokyo.ac.jp; harada@isi.imi.i.u-tokyo.ac.jp;
   kuniyosh@isi.imi.i.u-tokyo.ac.jp
RI Kanezaki, Asako/A-8515-2017
OI Kanezaki, Asako/0000-0003-3217-1405
CR [Anonymous], P S GEOM PROC
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   CORTELAZZO GM, 2006, P 3 INT S 3DPVT
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Demirci MF, 2006, INT J COMPUT VISION, V69, P203, DOI 10.1007/s11263-006-6993-y
   Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016
   Funkhouser T., 2006, P S GEOM PROC
   Johnson AE, 1998, GRAPH MODEL IM PROC, V60, P261, DOI 10.1006/gmip.1998.0474
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kobayashi T, 2004, INT C PATT RECOG, P741, DOI 10.1109/ICPR.2004.1333879
   MADEMLIS A, 2007, P ICIP
   Nene SA, 1996, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.1996.517172
   Ohbuchi R., 2008, P IEEE INT C SHAP MO
   Park S, 2006, VISUAL COMPUT, V22, P168, DOI 10.1007/s00371-006-0374-y
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   ZAHARIA T, 2002, P ICME
   ZHANG J, 2005, RETRIEVING ARTICULAT, P285
NR 20
TC 13
Z9 13
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1269
EP 1281
DI 10.1007/s00371-010-0521-3
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200004
DA 2024-07-18
ER

PT J
AU Tatsuma, A
   Aono, M
AF Tatsuma, Atsushi
   Aono, Masaki
TI Multi-Fourier spectra descriptor and augmentation with spectral
   clustering for 3D shape retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Shape Modeling and Applications
CY JUN 04-06, 2008
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH, EUROGRAPHICS, Comp Graph Soc
DE Shape retrieval; Feature vector; Spectral clustering
ID SURFACES
AB We propose a new method of similarity search for 3D shape models, given an arbitrary 3D shape as a query. The method features the high search performance enabled in part by our unique feature vector called Multi-Fourier Spectra Descriptor (MFSD), and in part by augmenting the feature vector with spectral clustering. The MFSD is composed of four independent Fourier spectra with periphery enhancement. It allows us to faithfully capture the inherent characteristics of an arbitrary 3D shape object regardless of the dimension, orientation, and original location of the object when it is first defined. Given a 3D shape database, the augmentation with spectral clustering is done first by computing the p-minimum spanning tree of the whole data set, where p is a number usually much less than m, the size of the whole 3D shape data set. We then define the affinity matrix, which is a square matrix of size m by m, where each element of the matrix denotes the distance between two shape objects. The distance is computed in advance by traversing the p-minimum spanning tree. The eigenvalue decomposition is then applied to the affinity matrix to reduce dimensionality of the matrix, followed by grouping into k clusters. The cluster information is kept for augmenting the search performance when a query is given. With a series of benchmark data sets, we will demonstrate that our approach outperforms previously known methods for 3D shape retrieval.
C1 [Tatsuma, Atsushi; Aono, Masaki] Toyohashi Univ Technol, Dept Informat & Comp Sci, Aichi, Japan.
C3 Toyohashi University of Technology
RP Aono, M (corresponding author), Toyohashi Univ Technol, Dept Informat & Comp Sci, 1-1 Tempakucho, Aichi, Japan.
EM aono@ics.tut.ac.jp
CR [Anonymous], UUCS2006030
   [Anonymous], 2001, P ADV NEURAL INFORM
   [Anonymous], 1239 U MONTR DEP INF
   [Anonymous], UUCS2007015
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bengio Y, 2004, NEURAL COMPUT, V16, P2197, DOI 10.1162/0899766041732396
   Biasotti S, 2008, PATTERN RECOGN, V41, P2855, DOI 10.1016/j.patcog.2008.02.003
   BRATLEY P, ALGORITHM 738 PROGRA
   Bustos B., 2004, Proceedings. IEEE Sixth International Symposium on Multimedia Software, P514
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Bustos B, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1303, DOI 10.1109/ICME.2004.1394465
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   DARAS P, 2006, SHREC2006, P17
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   He Xiaofei., 2004, ACM MULTIMEDIA, P17
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Levina E., 2005, ADV NEURAL INFORM PR, V17
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   MAKADIA A, 2006, SHREC2006, P32
   Min P., 2004, THESIS PRINCETON U, V2004
   *MPEG 7 VID GROUP, 2000, N3397 ISOIEC MPEG7 V
   Novoselov VV, 2003, GENE EXPR PATTERNS, V3, P225, DOI 10.1016/S1567-133X(02)00077-7
   Ohbuchi R, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P265, DOI 10.1109/PCCGA.2002.1167870
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   OHBUCHI R, 2006, P ACM MIR 2006 SANT
   OHBUCHI R, 2003, P 5 ACM SIGMM WORKSH
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   PU J, 2005, J COMPUTER AIDED DES, V2, P717
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Vranic DV, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P963
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   VRANIC DV, 2004, THESIS U LEIPZIG
   YANG L, 2005, PATTERN RECOGNIT LET, V26
   ZHANG H., 2007, P EUROGRAPHICS STATE, P1, DOI DOI 10.1109/IPDPS.2007.370248
   Zhang J, 2005, LECT NOTES COMPUT SC, V3757, P285, DOI 10.1007/11585978_19
NR 48
TC 43
Z9 49
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2009
VL 25
IS 8
BP 785
EP 804
DI 10.1007/s00371-008-0304-2
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 465NI
UT WOS:000267593700006
DA 2024-07-18
ER

PT J
AU Grimm, C
   Ju, T
   Phan, L
   Hughes, J
AF Grimm, Cindy
   Ju, Tao
   Phan, Ly
   Hughes, John
TI Adaptive smooth surface fitting with manifolds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Manifolds; Surface fitting; Analytic surface
ID RECONSTRUCTION
AB We present a smooth, everywhere C (k) , analytic surface representation for closed surfaces of arbitrary topology. We demonstrate fitting this representation to meshes of varying resolutions and sampling quality. The fitting process is adaptive and provides controls for both the average and the maximum allowable error. The representation is suitable for applications which require consistent parameterizations across different surfaces.
C1 [Grimm, Cindy; Ju, Tao; Phan, Ly] Washington Univ, St Louis, MO USA.
   [Hughes, John] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
C3 Washington University (WUSTL); Brown University
RP Grimm, C (corresponding author), Washington Univ, St Louis, MO USA.
EM cmg@cs.wustl.edu; taoju@cse.wustl.edu; faanly@cs.wustl.edu;
   jfh@cs.brown.edu
OI Grimm, Cindy/0000-0002-1711-7112
CR Erickson J, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1038
   FERGUSON H, 1993, COMPUT AIDED GEOM D, V10, P315, DOI 10.1016/0167-8396(93)90044-4
   GRIMM C, 2005, GRAPHITE
   GRIMM C, 2003, MATH SURFACES, V10, P14
   GRIMM C, 1995, COMPUT GRAPH, V29
   Grimm C. M., 2004, International Journal of Shape Modeling, V10, P51, DOI 10.1142/S0218654304000602
   Grimm CM, 2002, J BIOMECH ENG-T ASME, V124, P136, DOI 10.1115/1.1431266
   GU X, 2003, S GEOM PROC, P127
   Gu X., 2005, SPM 05, P27
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   He Y, 2006, LECT NOTES COMPUT SC, V4077, P409
   Jeong WK, 2002, GRAPH MODELS, V64, P78, DOI 10.1006/gmod.2002.0572
   KRISHNAMURTHY V, 1996, SIGGRAPH 96 C P, P313
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   LI WC, 2006, SGP 06, P191
   Litke N, 2001, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2001.964527
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MARAI GE, 2007, IEEE T VIS COMPUT GR
   Marinov M, 2005, GRAPH MODELS, V67, P452, DOI 10.1016/j.gmod.2005.01.003
   Navau JC, 2000, COMPUT AIDED GEOM D, V17, P643, DOI 10.1016/S0167-8396(00)00020-0
   Rockwood A., 1999, International Journal of Shape Modeling, V5, P135, DOI 10.1142/S0218654399000149
   Saba S, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P256, DOI 10.1109/SMI.2005.32
   Sander PedroV., 2002, EGRW 02, P87
   Shi XQ, 2004, COMPUT AIDED GEOM D, V21, P893, DOI 10.1016/j.cagd.2004.08.001
   Wallner J, 1996, CURVES SURFACES APPL, P445
   Wang X, 2007, SENSORS-BASEL, V7, P251, DOI 10.3390/s7030251
   Ying LX, 2004, ACM T GRAPHIC, V23, P271, DOI 10.1145/1015706.1015714
   ZHENG J, 2005, GRAPHITE, P405
   GEO GEOMAGIC COMMERC
NR 29
TC 3
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 589
EP 597
DI 10.1007/s00371-009-0334-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300024
DA 2024-07-18
ER

PT J
AU Yu, X
   Yu, JY
   McMillan, L
AF Yu, Xuan
   Yu, Jingyi
   McMillan, Leonard
TI Towards multi-perspective rasterization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Multi-perspective rendering; Graphics hardware; Real-time rendering;
   Rasterization
AB We present a novel framework for real-time multi-perspective rendering. While most existing approaches are based on ray-tracing, we present an alternative approach by emulating multi-perspective rasterization on the classical perspective graphics pipeline. To render a general multi-perspective camera, we first decompose the camera into piecewise linear primitive cameras called the general linear cameras or GLCs. We derive the closed-form projection equations for GLCs and show how to rasterize triangles onto GLCs via a two-pass rendering algorithm. In the first pass, we compute the GLC projection coefficients of each scene triangle using a vertex shader. The linear raster on the graphics hardware then interpolates these coefficients at each pixel. Finally, we use these interpolated coefficients to compute the projected pixel coordinates using a fragment shader. In the second pass, we move the pixels to their actual projected positions. To avoid holes, we treat neighboring pixels as triangles and re-render them onto the GLC image plane. We demonstrate our real-time multi-perspective rendering framework in a wide range of applications including synthesizing panoramic and omnidirectional views, rendering reflections on curved mirrors, and creating multi-perspective faux animations. Compared with the GPU-based ray tracing methods, our rasterization approach scales better with scene complexity and it can render scenes with a large number of triangles at interactive frame rates.
C1 [Yu, Xuan; Yu, Jingyi] Univ Delaware, Newark, DE 19716 USA.
   [McMillan, Leonard] Univ N Carolina, Chapel Hill, NC USA.
C3 University of Delaware; University of North Carolina; University of
   North Carolina Chapel Hill
RP Yu, JY (corresponding author), Univ Delaware, Newark, DE 19716 USA.
EM xuanyu@udel.edu; jingyiyu@udel.edu
RI McMillan, Leonard/IQS-6191-2023
OI McMillan, Leonard/0000-0002-8453-0847
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Agrawala M, 2000, SPRING COMP SCI, P125
   [Anonymous], SIGGRAPH 97
   ASEEM A, 2006, SIGGRAPH 06 ACM SIGG, P853
   CARR NA, 2002, P ACM SIGGRAPH EUROG
   Diefenbach P. J., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P59, DOI 10.1145/253284.253308
   Durand F., 2002, NPAR '02: Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering, P111
   GASCUEL JD, 2008, SI3D 08, P107
   GLASSNER AS, 2000, MSRTR200005
   Gupta R, 1997, IEEE T PATTERN ANAL, V19, P963, DOI 10.1109/34.615446
   Hanson AJ, 1998, VISUALIZATION '98, PROCEEDINGS, P327, DOI 10.1109/VISUAL.1998.745320
   HOU X, 2006, SIGGRAPH 06 ACM SIGG, P79
   Kitamura Y, 2001, COMP GRAPH, P231, DOI 10.1145/383259.383285
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lindholm E, 2001, COMP GRAPH, P149, DOI 10.1145/383259.383274
   Mei CH, 2005, COMPUT GRAPH FORUM, V24, P335, DOI 10.1111/j.1467-8659.2005.00858.x
   Ofek E., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P333, DOI 10.1145/280814.280929
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   POPESCU V, 2006, 3DPVT 06, P121, DOI DOI 10.1109/3DPVT.2006.26
   PURCELL T., 2005, SIGGRAPH '05: ACM SIGGRAPH 2005 Courses, P258
   RADEMACHER P, 1998, COMPUT GRAPH, V32, P199
   Román A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50
   ROMAN A, 2003, IEEE T PATTERN ANAL, V25, P741
   Seitz SM, 2003, IEEE COMPUT GRAPH, V23, P16, DOI 10.1109/MCG.2003.1242377
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   SIMON A, 2004, VR 04, P67
   Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1
   Wyman Chris., 2006, I3D 06, P153
   YU J, 2004, 8 EUR C COMP VIS ECC
   YU J, 2004, FRAMEWORK MULTIPERSP
   Zomet A, 2003, IEEE T PATTERN ANAL, V25, P741, DOI 10.1109/TPAMI.2003.1201823
NR 31
TC 3
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 549
EP 557
DI 10.1007/s00371-009-0335-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300019
DA 2024-07-18
ER

PT J
AU Reiterer, B
   Concolato, C
   Lachner, J
   Le Feuvre, J
   Moissinac, JC
   Lenzi, S
   Chessa, S
   Ferrá, EF
   Menaya, JJG
   Hellwagner, H
AF Reiterer, Bernhard
   Concolato, Cyril
   Lachner, Janine
   Le Feuvre, Jean
   Moissinac, Jean-Claude
   Lenzi, Stefano
   Chessa, Stefano
   Fernandez Ferra, Enrique
   Gonzalez Menaya, Juan Jose
   Hellwagner, Hermann
TI User-centric universal multimedia access in home networks
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE Universal Multimedia Access; multimedia adaptation; UPnP AV; context
   awareness; content sharing
AB Much research is currently being conducted towards Universal Multimedia Access, aiming at removing barriers that arise when multimedia content is to be consumed with more and more heterogeneous devices and over diverse networks. We argue that users should be put at the center of the research work to enable user-centric multimedia access.
   In this paper we present the requirements for a user-centric multimedia access system in a networked home environment. These requirements are easy access to available content repositories, context awareness, content adaptation and session migration. After showing the limits of state-of-the-art technologies, we present the architecture of a system which allows unified access to the home network content, automatically delivered to rendering devices close to the user, adapted according to the rendering device constraints, and which is also capable of session mobility.
C1 [Reiterer, Bernhard; Lachner, Janine; Hellwagner, Hermann] Klagenfurt Univ, Inst Informat Technol, A-9020 Klagenfurt, Austria.
   [Concolato, Cyril; Le Feuvre, Jean; Moissinac, Jean-Claude] TELECOM ParisTech, F-75013 Paris, France.
   [Lenzi, Stefano; Chessa, Stefano] Univ Pisa, I-56124 Pisa, Italy.
   [Lenzi, Stefano; Chessa, Stefano] CNR, Ist Sci & Tecnol Informat, I-56124 Pisa, Italy.
   [Fernandez Ferra, Enrique; Gonzalez Menaya, Juan Jose] Telefon Invest & Desarrollo, Madrid 28043, Spain.
C3 University of Klagenfurt; IMT - Institut Mines-Telecom; Institut
   Polytechnique de Paris; Telecom Paris; University of Pisa; Consiglio
   Nazionale delle Ricerche (CNR); Istituto di Scienza e Tecnologie
   dell'Informazione "Alessandro Faedo" (ISTI-CNR); Telefonica SA
RP Reiterer, B (corresponding author), Klagenfurt Univ, Inst Informat Technol, Univ Str 65-67, A-9020 Klagenfurt, Austria.
EM reiterer@itec.uni-klu.ac.at; concolato@enst.fr;
   janine@itec.uni-klu.ac.at; lefeuvre@enst.fr; moissinac@enst.fr;
   stefano.lenzi@isti.cnr.it; stefano.chessa@isti.cnr.it; eff@tid.es;
   juanjo@tid.es; hellwagn@itec.uni-klu.ac.at
RI Moissinac, Jean-Claude/B-2015-2017; Chessa, Stefano/A-6837-2012
OI Moissinac, Jean-Claude/0000-0001-7878-1116; Hellwagner,
   Hermann/0000-0003-1114-2584; Chessa, Stefano/0000-0002-1248-9478; Lenzi,
   Stefano/0000-0003-4465-8384
CR CHOU LD, 2004, P 2 IEEE INT WORKSH, P2
   Ciavarella C, 2004, PERS UBIQUIT COMPUT, V8, P82, DOI 10.1007/s00779-004-0265-z
   *DANAE, EU RES PROJ
   De Keukelaere F, 2005, ICOMP '05: Proceedings of the 2005 International Conference on Internet Computing, P287
   *EP, EU RES PROJ
   *INT CORP, 2003, OV UPNP AV ARCH 2003
   INT NOE, EU RES PROJ
   *ISO IEC, 2004, INF TECHN MULT FRA 7
   Jakab M, 2007, 15TH EUROMICRO INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND NETWORK-BASED PROCESSING, PROCEEDINGS, P363, DOI 10.1109/PDP.2007.52
   KATSIKIAN S, 2007, MOBILE TELEPORTER VI
   Kropfberger M, 2007, WEBIST 2007: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, VOL WIA, P83
   Lachner J, 2007, SECOND INTERNATIONAL WORKSHOP ON SEMANTIC MEDIA ADAPTATION AND PERSONALIZATION, PROCEEDINGS, P159, DOI 10.1109/SMAP.2007.35
   Le Feuvre J., 2007, P 15 ACM INT C MULT, P1009, DOI [10.1145/1291233.1291452, DOI 10.1145/1291233.1291452]
   Pereira F, 2003, IEEE SIGNAL PROC MAG, V20, P63, DOI 10.1109/MSP.2003.1184340
   SCHOFFMANN K, 2005, THESIS KLAGENFURT U
   Schulzrinne H., 2003, 3550 IETF RFC
   *SOCRADES, EU RES PROJ
   *SODA, ITEA PROJ
   UPNP FORUM MEDIASERV
NR 19
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 837
EP 845
DI 10.1007/s00371-008-0265-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800039
DA 2024-07-18
ER

PT J
AU Gehling, MB
   Hofsetz, C
   Musse, SR
AF Gehling, Mauricio Bammann
   Hofsetz, Christian
   Musse, Soraia Raupp
TI Normalpaint: an interactive tool for painting normal maps
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE normal maps; 3D painting; painting systems
AB This paper presents a novel 3D painting system that allows interactive painting on normal maps. The process of creating a highly detailed model and later extracting normal maps is slow and prone to artifacts. We propose an interactive framework where the user paints directly on normal maps while visualizing the results as they would appear in the target rendering system.
RP Hofsetz, C (corresponding author), Av Unisinos 950, BR-9302200 Sao Leopoldo, RS, Brazil.
EM mbg3dmind@gmail.com; chofsetz@acm.org; soraia.musse@pucrs.br
RI Musse, Soraia Raupp/AAS-3787-2021; Musse, Soraia Raupp R/G-4801-2012
OI Musse, Soraia Raupp/0000-0002-3278-217X
CR AGRAWALA M, 1995, SI3D 95 P 1995 S INT
   BLINN JF, 1978, SIGGRAPH 78, P286
   Cignoni P, 1998, VISUALIZATION '98, PROCEEDINGS, P59, DOI 10.1109/VISUAL.1998.745285
   COHEN J, 1998, SIGGRAPH 98, P115
   Cook RL, 1984, SIGGRAPH 84, P223
   DAILY J, 1995, CHI 95 C COMP, P296
   FOURNIER A, 1992, GRAPH INT 92 WORKSH, P45
   GREGORY AD, 2000, INTOUCH INTERACTIVE
   HANRAHAN P, 1990, SIGGRAPH COMPUT GRAP, V24, P215
   HECKBERT PS, 1986, IEEE COMPUT GRAPH, V6, P56, DOI 10.1109/MCG.1986.276672
   Hernandez B, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P578, DOI 10.1109/CGI.2004.1309267
   Jain R., 1995, MACHINE VISION
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   WILLIAMS L, 1990, SI3D 90 P 1990 S INT, P225
NR 15
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 897
EP 904
DI 10.1007/s00371-007-0132-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600030
DA 2024-07-18
ER

PT J
AU Ogáyar, CJ
   Rueda, AJ
   Segura, RJ
   Feito, FR
AF Ogayar, C. J.
   Rueda, A. J.
   Segura, R. J.
   Feito, F. R.
TI Fast and simple hardware accelerated voxelizations using simplicial
   coverings
SO VISUAL COMPUTER
LA English
DT Article
DE voxelization algorithms; graphics hardware; volume graphics
ID VOLUME; ALGORITHMS
AB Voxelization of solids, that is the representation of a solid by a set of voxels that approximates it, is an operation with important applications in fields like solid modeling, physical simulation or volume graphics. Moreover, the new generation of affordable 3D raster displays has renewed the interest on fast voxelization algorithms, as the scan-conversion of a solid is a basic operation on these devices.
   In this paper a hardware accelerated method for computing a voxelization of a polyhedron is presented. The algorithm is simple, efficient, robust and handles any kind of polyhedron (self-intersecting, with or without holes, manifold or non-manifold). Three different implementations are described in detail. The first is a conventional implementation in the CPU, the second is a hardware accelerated implementation that uses standard OpenGL primitives, and the third exploits the capabilities of modern GPUs by using vertex programs.
C1 Univ Jaen, Escuela Politecn Super, Dept Informat, Jaen 23071, Spain.
C3 Universidad de Jaen
RP Ogáyar, CJ (corresponding author), Univ Jaen, Escuela Politecn Super, Dept Informat, Campus Las Lagunillas,Edif A3, Jaen 23071, Spain.
EM cogayar@ujaen.es; ajrueda@ujaen.es; rsegura@ujaen.es; ffeito@ujaen.es
RI Rueda-Ruiz, Antonio Jesús/AAY-5298-2021; Ogayar-Anguita,
   Carlos-Javier/K-2166-2017; Segura, Rafael/J-3622-2012; Feito,
   Francisco/M-1672-2014
OI Rueda-Ruiz, Antonio Jesús/0000-0001-7692-454X; Ogayar-Anguita,
   Carlos-Javier/0000-0003-0958-990X; Segura, Rafael/0000-0002-3075-6963;
   Feito, Francisco/0000-0001-8230-6529
CR Andres E, 1997, COMPUT GRAPH FORUM, V16, pC3, DOI 10.1111/1467-8659.00137
   [Anonymous], 1999, OpenGL programming guide: the official guide to learning OpenGL
   Blundell B.G., 2000, Volumetric three-dimensional display systems
   Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   CARUCCI F, 2004, GPU GEMS, V2, P47
   Chen M, 2000, COMPUT GRAPH FORUM, V19, P281, DOI 10.1111/1467-8659.00464
   Ebert D, 1999, COMMUN ACM, V42, P100, DOI 10.1145/310930.310979
   Fang SF, 2000, COMPUT GRAPH-UK, V24, P433, DOI 10.1016/S0097-8493(00)00038-8
   FEITO F, 1995, COMPUT GRAPH, V19, P595, DOI 10.1016/0097-8493(95)00037-D
   Feito FR, 1997, COMPUT GRAPH, V21, P23, DOI 10.1016/S0097-8493(96)00067-2
   FERNANDO R, 2003, CG TUTOTRIAL DEFINIT
   Fernando R., 2004, Programming Techniques, Tips and Tricks for Real- Time Graphics
   Foley J. D., 1994, Introduction to Computer Graphics", V55
   FRISKEN FS, 1998, IEEE S VOLUME VISUAL, P23
   Haumont D., 2002, J GRAPHICS TOOLS, V7, P27, DOI DOI 10.1080/10867651.2002.10487563
   Huang J, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P119, DOI 10.1109/SVV.1998.729593
   Jones MW, 1996, COMPUT GRAPH FORUM, V15, P311, DOI 10.1111/1467-8659.1550311
   KARABASSI E, 2002, ACM J GRAPH TOOLS, V4, P114
   KAUFMAN A, 1993, COMPUTER, V26, P51, DOI 10.1109/MC.1993.274942
   KAUFMAN A, 1988, COMPUT GRAPH, V12, P213, DOI 10.1016/0097-8493(88)90032-5
   KAUFMAN A, 1986, P 1986 WORKSH INT 3D, P45
   KAUFMAN A, 1987, IEEE COMPUT GRAPH, V0021, P00171
   LEE YT, 1982, COMMUN ACM, V25, P635, DOI 10.1145/358628.358643
   Li W., 2005, GPU GEMS 2 CHAPTER 4, P747
   PASSALI G, 2004, P COMPUTER GRAPHICS, P274
   Pastoor S, 1997, DISPLAYS, V17, P100, DOI 10.1016/S0141-9382(96)01040-2
   Pharr M., 2005, GPU GEMS
   RUEDA AJ, 2005, P WSCG 2004, P227
   RUEDA AJ, 2004, GRAPH MODELS, V26, P805
   Sramek M, 1999, IEEE T VIS COMPUT GR, V5, P251, DOI 10.1109/2945.795216
   SRAMEK M, 1999, P INT WORKSHOP VOLUM, P119
   WANG SW, 1994, IEEE COMPUT GRAPH, V14, P26, DOI 10.1109/38.310721
NR 32
TC 8
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 535
EP 543
DI 10.1007/s00371-007-0097-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200001
DA 2024-07-18
ER

PT J
AU Ma, WC
   Hsiao, CT
   Lee, KY
   Chuang, YY
   Chen, BY
AF Ma, Wan-Chun
   Hsiao, Chun-Tse
   Lee, Ken-Yi
   Chuang, Yung-Yu
   Chen, Bing-Yu
TI Real-time triple product relighting using spherical local-frame
   parameterization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE all-frequency relighting; precomputed radiance transfer; local frame;
   spherical wavelets; real-time rendering
AB This paper addresses the problem of real-time rendering for objects with complex materials under varying all-frequency illumination and changing view. Our approach extends the triple product algorithm by using local-frame parameterization, spherical wavelets, per-pixel shading and visibility textures. Storing BRDFs with local-frame parameterization allows us to handle complex BRDFs and incorporate bump mapping more easily. In addition, it greatly reduces the data size compared to storing BRDFs with respect to the global frame. The use of spherical wavelets avoids uneven sampling and energy normalization of cubical parameterization. Finally, we use per-pixel shading and visibility textures to remove the need for fine tessellations of meshes and shift most computation from vertex shaders to more powerful pixel shaders. The resulting system can render scenes with realistic shadow effects, complex BRDFs, bump mapping and spatially-varying BRDFs under varying complex illumination and changing view at real-time frame rates on modern graphics hardware.
C1 Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Commun & Multimedia Lab, Taipei, Taiwan.
C3 National Taiwan University
RP Ma, WC (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Commun & Multimedia Lab, Taipei, Taiwan.
EM alexma98@gmail.com
RI ; Chen, Bing-Yu/E-7498-2016
OI Chuang, Yung-Yu/0000-0002-1383-0017; Chen, Bing-Yu/0000-0003-0169-7682
CR Bonneau G.-P., 1999, IEEE Visualization, P279
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   KAUTZ J, 2002, P 13 EUR WORKSH REND, P291
   LEHTINEN J, 2003, P 2003 S INT 3D GRAP, P59
   LIU X, 2004, P EUR S REND, P337
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Nielson GM, 1997, VISUALIZATION '97 - PROCEEDINGS, P143, DOI 10.1109/VISUAL.1997.663871
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   SCHRODER P, 1995, P EGWR 1995, P252
   Schruder P., 1995, Proc. 22nd Ann. Conf. Comput. Graphics Interactive Techniques (SIGGRAPH'95), P161
   Sloan P., 2006, Proceedings of the 2006 symposium on Interactive 3D, P23
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   TSAI YT, 2006, IN PRESS P SIGGRAPH
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
   WANG R, IN PRESS ACM T GRAPH
   WANG R, 2004, P EUR S REND, P345
   Wang Z, 2004, COMPUT VIS IMAGE UND, V96, P327, DOI 10.1016/j.cviu.2004.03.017
   [No title captured]
NR 22
TC 7
Z9 11
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 682
EP 692
DI 10.1007/s00371-006-0064-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000011
DA 2024-07-18
ER

PT J
AU Nieda, T
   Pasko, A
   Kunii, TL
AF Nieda, T
   Pasko, A
   Kunii, TL
TI Detection and classification of topological evolution for linear
   metamorphosis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Multimedia Modelling (MMM 2004)
CY JAN 05-07, 2004
CL Brisbane, AUSTRALIA
SP Queensland Univ Technol, Ctr Informat Technol Innovat, Deakin Univ, Sch Informat Technol, Apple Comp, Int Federat Automat Control, Emu Design
DE critical point classification; Morse theory; shape metamorphosis;
   topological evolution
AB The advantage of functional methods for shape metamorphosis is the automatic generation of intermediate shapes possible between the key shapes of different topology types. However, functional methods have a serious problem: shape interpolation is applied without topological information and thereby the time values of topological changes are not known. Thus, it is difficult to identify the time intervals for key frames of shape metamorphosis animation that faithfully visualize the topological evolution. Moreover, information on the types of topological changes is missing. To overcome the problem, we apply topological analysis to functional linear shape metamorphosis and classify the type of topological evolution by using a Hessian matrix. Our method is based on Morse theory and analyzes how the critical points appear. We classify the detected critical points into maximum point, minimum point, and saddle point types. Using the types of critical points, we can define the topological information for shape metamorphosis. We illustrate these methods using shape metamorphosis in 2D and 3D spaces.
C1 Hosei Univ, Grad Sch Comp & Informat Sci, Koganei, Tokyo 1848584, Japan.
   Kanazawa Inst Technol, IT Inst, Shibuya Ku, Tokyo 1500001, Japan.
C3 Hosei University
RP Hosei Univ, Grad Sch Comp & Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.
EM i04t0013@cis.k.hosei.ac.jp; pasko@k.hosei.ac.jp; tosi@kunii.com
RI Pasko, Alexander/H-9344-2017
OI Pasko, Alexander/0000-0002-4785-7066
CR Attene M, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P142, DOI 10.1109/SMA.2001.923385
   DeCarlo D, 1996, PROC GRAPH INTERF, P194
   Galin E, 1996, COMPUT GRAPH FORUM, V15, pC143, DOI 10.1111/1467-8659.1530143
   Hart J.C., 1998, Proceeding of SIGGRAPH on Implicit Surfaces, P69
   Kanonchayos P, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P465, DOI 10.1109/CW.2002.1180914
   Lazarus F, 1998, VISUAL COMPUT, V14, P373, DOI 10.1007/s003710050149
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Stander BT, 1997, Proc. SIGGRAPH, P279, DOI DOI 10.1145/258734.258868
   Takahashi S, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2001.962859
   Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580
   WU ST, 1999, 4 INT WORKSH IMPL SU, P73
NR 11
TC 6
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2006
VL 22
IS 5
BP 346
EP 356
DI 10.1007/s00371-006-0011-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 041PQ
UT WOS:000237468500007
DA 2024-07-18
ER

PT J
AU Diaz-Gutierrez, P
   Gopi, M
AF Diaz-Gutierrez, P
   Gopi, M
TI Quadrilateral and tetrahedral mesh stripification using 2-factor
   partitioning of the dual graph
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE graph matching; 2-factor; quadrilateral stripification; tetrahedral
   stripification
AB In order to find a 2-factor of a graph, there exists a O(n(1.5)) deterministic algorithm [7] and a O(n(3)) randomized algorithm [14]. In this paper, we propose novel O(nlog(3)n log log n) algorithms to find a 2-factor, if one exists, of a graph in which all n vertices have degree 4 or less. Such graphs are actually dual graphs of quadrilateral and tetrahedral meshes. A 2-factor of such graphs implicitly defines a linear ordering of the mesh primitives in the form of strips. Further, by introducing a few additional primitives, we reduce the number of tetrahedral strips to represent the entire tetrahedral mesh and represent the entire quad surface using a single quad strip.
C1 Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA.
C3 University of California System; University of California Irvine
RP Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA.
EM pablo@ics.uci.edu; gopi@ics.uci.edu
CR [Anonymous], 1891, Acta Mathematica
   ASANO T, 1986, J ACM, V33, P290, DOI 10.1145/5383.5387
   Bogomjakov A, 2002, COMPUT GRAPH FORUM, V21, P137, DOI 10.1111/1467-8659.00573
   BOSE P, 1995, INT S ALG COMP, P372
   Conn H.E., 1990, P 28 ALLERTON C COMM, P788
   DIAZGUTIERREZ P, 2005, P INT C COMP GRAPH
   Evans F, 1996, IEEE VISUAL, P319, DOI 10.1109/VISUAL.1996.568125
   Gibbons A., 1985, Algorithmic graph theory
   Gopi M, 2004, COMPUT GRAPH FORUM, V23, P371, DOI 10.1111/j.1467-8659.2004.00768.x
   HEIGHWAY EA, 1983, IEEE T MAGN, V19, P2535, DOI 10.1109/TMAG.1983.1062810
   KING D, 2001, HPL2000121
   MALLON PN, 2004, COMPRESSION FLY REND
   MUKHOPADHYAY A, 2003, 15 CAN C COMP GEOM
   Pajarola R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P299, DOI 10.1109/VISUAL.1999.809901
   Pandurangan G, 2005, INFORM PROCESS LETT, V95, P321, DOI 10.1016/j.ipl.2005.04.001
   Pascucci V., 2004, JOINT EUROGRAPHICS I
   PENG J, 2005, TECHNOLOGIES 3D MESH
   SOMMER O, 2000, P VIS DAT EXPL AN C, P124
   Szymczak A., 1999, 5 S SOLID MODELING, P54
   TAUBIN G, 2002, INT WORKSH VIS MATH
   Thorup M., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P343, DOI 10.1145/335305.335345
   UENG SK, 2003, VOLUME GRAPHICS, P95
   VANECEK P, 2004, QUADRILATERAL MESHES
   XIANG X, 1999, P S INT 3D GRAPH, P71
NR 24
TC 1
Z9 4
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 689
EP 697
DI 10.1007/s00371-005-0336-9
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400021
DA 2024-07-18
ER

PT J
AU Garcia, A
   Shen, HW
AF Garcia, A
   Shen, HW
TI GPU-based 3D wavelet reconstruction with tileboarding
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE tileboards; wavelets
AB In this paper, we present a GPU-based algorithm for reconstructing 3D wavelets using fragment programs. To minimize the data transfer and fragment processing overhead, we propose a novel scheme that uses tileboards as a primary layout to organize 3D wavelet coefficients. By accessing the tileboards with correct texture coordinates, Haar and Daubechies wavelets can be evaluated by the GPU in real time. The tileboard also serves as input to the rendering programs. We demonstrate how the tileboards allow us to efficiently cull unnecessary data, and we extend our work to render large volumes with multiple resolution levels.
C1 Ohio State Univ, Dreese Labs 395, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Ohio State Univ, Dreese Labs 395, 2015 Neil Av, Columbus, OH 43210 USA.
EM agarcia@cse.ohio-state.edu; hwshen@cse.ohio-state.edu
RI Shen, Han-wei/A-4710-2012
CR Chui C. K., 1992, An Introduction to Wavelets, DOI 10.2307/2153134
   FERNANDO R, 2004, TUTORIAL EUROGRAPHIC
   FORAN J, 1994, WORKSH VOL VIS
   Guthe S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P53, DOI 10.1109/VISUAL.2002.1183757
   Hopf M., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P471, DOI 10.1109/VISUAL.1999.809934
   Hopf M, 2000, SPRING COMP SCI, P93
   HOPF M, 1999, WORKSH VIS MOD VIS V, P317
   IHM I, 1999, COMPUT GRAPH FORUM, V18, P249
   Kraus Martin., 2002, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, HWWS '02, P7
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   Lefohn A., 2003, IEEE Visualiza- tion
   Mark WR, 2003, ACM T GRAPHIC, V22, P896, DOI 10.1145/882262.882362
   MUAKI S, 1992, VIS 92, P21
   Ning P., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P11, DOI 10.1109/VISUAL.1993.398845
   Rodler F. F., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P108, DOI 10.1109/PCCGA.1999.803354
   ROST R.J., 2004, OPENGL SHADING LANGU
   SCHNEIDER J, 2003, P IEEE VIS
   STOLLNIT EJ, 1996, WAVELETS COMPUTER GR
   Strang G., 1996, Wavelets and Filter Banks
   WANG J, 2002, ACM WORKSH GEN PURP
   Westermann Rudiger., 1994, Proceedings of the 1994 Symposium on Volume Visualization, VVS '94, P51
NR 21
TC 18
Z9 22
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 755
EP 763
DI 10.1007/s00371-005-0332-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400028
DA 2024-07-18
ER

PT J
AU Leifman, G
   Meir, R
   Tal, A
AF Leifman, G
   Meir, R
   Tal, A
TI Semantic-oriented 3d shape retrieval using relevance feedback
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE 3D retrieval; search engine; relevance feedback
AB Shape-based retrieval of 3D models has become an important challenge in computer graphics. Object similarity, however, is a subjective matter, dependent on the human viewer, since objects have semantics and are not mere geometric entities. Relevance feedback aims at addressing the subjectivity of similarity. This paper presents a novel relevance feedback algorithm that is based on supervised as well as unsupervised feature extraction techniques. It also proposes a novel signature for 3D models, the sphere projection. A Web search engine that realizes the signature and the relevance feedback algorithm is presented. We show that the proposed approach produces good results and outperforms previous techniques.
C1 Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM gleifman@techunix.technion.ac.il; rmeir@ee.technion.ac.il;
   ayellet@ee.technion.ac.il
CR Atmosukarto I, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P334, DOI 10.1109/MMMC.2005.39
   BANG H, 2002, ICIP
   Bespalov D., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P208, DOI DOI 10.1145/781606.781638
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   DELFINADO CJA, 1995, COMPUT AIDED GEOM D, V12, P771, DOI 10.1016/0167-8396(95)00016-Y
   Duda R., 1973, Pattern Classification and Scene Analysis
   ELAD M, 2001, EG MULTIMEDIA, V39, P97
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Ishikawa Y., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P218
   IYER N, 2004, TMCE
   Jarvelin Kalervo, 2000, P 23 ANN INT ACM SIG
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kazhdan M., 2003, ALGORITHMICA, V38
   KONKE S, 2002, P DAGST SEM SCI VIS, P14
   Korfhage R., 1997, INFORMATION STORAGE
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Peng J, 1999, COMPUT VIS IMAGE UND, V75, P150, DOI 10.1006/cviu.1999.0770
   Rui Y., 1998, STORAGE RETRIEVAL IM, P25
   Santini S, 2000, IEEE MULTIMEDIA, V7, P26, DOI 10.1109/93.879766
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf B., 2002, Learning with Kernels
   Shilane P., 2004, Shape Modeling International
   Tieu K, 2000, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2000.855824
   VANRIJSBERGEN CJ, 1979, INFORMATION RETRIEVA
   Veltkamp RC, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P188, DOI 10.1109/SMA.2001.923389
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   Vranic DV, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P177, DOI 10.1109/ICME.2002.1035747
   ZHOU X, 2001, P IEEE COMP VIS PATT
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 33
TC 48
Z9 57
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 865
EP 875
DI 10.1007/s00371-005-0341-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Oh, S
   Kim, H
   Magnenat-Thalmann, N
   Wohn, K
AF Oh, S
   Kim, H
   Magnenat-Thalmann, N
   Wohn, K
TI Generating unified model for dressed virtual humans
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE dressed virtual human modeling; shape fitting; multiple correspondence
   mapping
AB Scenes with crowds of dressed virtual humans are getting more attention and importance in 3D games and virtual reality applications. Crowd scenes, which include huge amounts of virtual humans, require complex computation for animation and rendering. In this research, new methods are proposed to generate efficient virtual human models by unifying a body and a garment into an animatable model, which has skinning parameters for the common skeleton-driven animation. The generated model has controlled complexity in geometry and semantic information. The unified model is constructed by using the correspondence between the body and the garment meshes. To establish the correspondence, two opposite optimization methods are proposed and compared: the first is to fit the body onto the garment and the second is to fit the garment onto the body. The innovative aspect of our method lies in supporting multiple correspondences between body and cloth parts. This enables us to handle the skirt model which is difficult to be processed by using previous works, due to its topological differences to the body model.
C1 Korea Adv Inst Sci & Technol, Dept EECS, Virtual Real Lab, Seoul, South Korea.
   Univ Geneva, MIRALab, CH-1211 Geneva, Switzerland.
C3 Korea Advanced Institute of Science & Technology (KAIST); University of
   Geneva
RP Korea Adv Inst Sci & Technol, Dept EECS, Virtual Real Lab, Seoul, South Korea.
EM redmong@vr.kaist.ac.kr; kim@miralab.unige.ch; thalmann@miralab.unige.ch;
   wohn@vr.kaist.ac.kr
RI Wohn, Kwangyun/C-2013-2011; Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960; Kim, HyungSeok/0000-0003-4816-2992
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   [Anonymous], P ACM SIGGRAPH
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   CORDIER E, 2004, PACIFIC GRAPHICS, P257
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   KRISHNAMURTHY V, 1996, P SIGGRAPH 96, P313, DOI DOI 10.1145/237170.237270
   Lee AWF, 1999, COMP GRAPH, P343, DOI 10.1145/311535.311586
   MARSCHNER SR, 2000, P 11 EUR WORKSH REND, P231
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   NOH JY, 2002, P SIGGRAPH 02, P277
   OH SW, 2004, P 5 KOR ISR BIN C GE, P115
   Praun E, 2001, COMP GRAPH, P179, DOI 10.1145/383259.383277
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
NR 17
TC 5
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 522
EP 531
DI 10.1007/s00371-005-0339-6
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400004
DA 2024-07-18
ER

PT J
AU Lin, IC
   Ouhyoung, M
AF Lin, IC
   Ouhyoung, M
TI Mirror MoCap: Automatic and efficient capture of dense 3D facial motion
   parameters from video
SO VISUAL COMPUTER
LA English
DT Article
DE facial animation; motion capture; facial animation parameters; automatic
   tracking
ID FACES; ALGORITHMS; FRAMEWORK; ANIMATION; TRACKING
AB In this paper, we present an automatic and efficient approach to the capture of dense facial motion parameters, which extends our previous work of 3D reconstruction from mirror-reflected multiview video. To narrow search space and rapidly generate 3D candidate position lists, we apply mirrored-epipolar bands. For automatic tracking, we utilize spatial proximity of facial surfaces and temporal coherence to find the best trajectories and rectify statuses of missing and false tracking. More than 300 markers on a subject's face are tracked from video at a process speed of 9.2 frames per second (fps) on a regular PC. The estimated 3D facial motion trajectories have been applied to our facial animation system and can be used for facial motion analysis.
C1 Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 300, Taiwan.
   Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Yang Ming Chiao Tung University; National Taiwan University
RP Natl Chiao Tung Univ, Dept Comp & Informat Sci, 1001 Ta Hsueh Rd, Hsinchu 300, Taiwan.
EM ichenlin@cis.nctu.edu.tw; ming@csie.ntu.edu.tw
OI Lin, I-Chen/0000-0001-9924-4723; OUHYOUNG, MING/0000-0002-3038-6958
CR Ahlberg J, 2002, EURASIP J APPL SIG P, V2002, P566, DOI 10.1155/S1110865702203078
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Basu S, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P46, DOI 10.1109/NAMW.1997.609851
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   BLANZ V, 1999, P SIGGRAPH, P353
   Bozic S.M., 1979, DIGITAL KALMAN FILTE
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   BUCKLEY K, 2000, P RAD 2000
   CASTANON DA, 1990, IEEE T AERO ELEC SYS, V26, P405, DOI 10.1109/7.53448
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   Davis J, 2005, IEEE T PATTERN ANAL, V27, P296, DOI 10.1109/TPAMI.2005.37
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Goto T, 2001, IEEE SIGNAL PROC MAG, V18, P17, DOI 10.1109/79.924885
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   Haralick R.M., 1992, COMPUTER ROBOTIC VIS, V1
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   KALBERER GA, 2004, P COMP AN 2001 SEUL, P18
   KALBERER GA, 2001, P COMP AN 2001 SEOUL, P18
   Kshirsagar S, 2003, COMPUT GRAPH FORUM, V22, P631, DOI 10.1111/1467-8659.t01-2-00711
   KURATATE T, 1998, P INT C AUD VIS SPEE, P185
   Lin IC, 2002, IEEE COMPUT GRAPH, V22, P72, DOI 10.1109/MCG.2002.1046631
   Pandzic IS, 1999, VISUAL COMPUT, V15, P330, DOI 10.1007/s003710050182
   PATTERSON EC, 1991, P COMP AN 91, P31
   Pighin F., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P143, DOI 10.1109/ICCV.1999.791210
   PIGHIN F, 1998, P SIGGRAPH 98, P75
   Tu PH, 2004, J COMPUT SCI TECH-CH, V19, P618, DOI 10.1007/BF02945587
   WENG JY, 1989, IEEE T PATTERN ANAL, V11, P451, DOI 10.1109/34.24779
   WOLF JK, 1989, IEEE T AERO ELEC SYS, V25, P287, DOI 10.1109/7.18692
   Yeasin M, 2004, IEEE T MULTIMEDIA, V6, P398, DOI 10.1109/TMM.2004.827514
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
NR 30
TC 17
Z9 43
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2005
VL 21
IS 6
BP 355
EP 372
DI 10.1007/s00371-005-0291-5
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 952GD
UT WOS:000230991100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liou, CY
   Kuo, YT
AF Liou, CY
   Kuo, YT
TI Conformal self-organizing map for a genus-zero manifold
SO VISUAL COMPUTER
LA English
DT Article
DE conformality; surface reconstruction; conformal mapping; deformation
   measure; self-organizing map
ID NETWORK; PARAMETERIZATION; ORGANIZATION; MODELS
AB This paper presents the implementation of a surface mesh on a genus-zero manifold with 3D scattered data of sculpture surfaces using the conformal self-organizing map (CSM). It starts with a regular mesh on a sphere and gradually shapes the regular mesh to match its object's surface by using the CSM. It can drape a uniform mesh on an object with a high degree of conformality. It accomplishes the surface reconstruction and also defines a conformal mapping from a sphere to the object's manifold.
C1 Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Taiwan University
RP Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM cyliou@csie.ntu.edu.tw
RI Liou, Cheng-Yuan/HDM-0177-2022
OI LIOU, CHENG-YUAN/0000-0001-7479-1413
CR AMENTA N, 1999, P SIGGRAPH, P415
   [Anonymous], ELEMENTARY DIFFERENT
   Barhak J, 2001, IEEE T VIS COMPUT GR, V7, P1, DOI 10.1109/2945.910817
   BAUER HU, 1992, IEEE T NEURAL NETWOR, V3, P570, DOI 10.1109/72.143371
   Chen SW, 1996, IEEE T NEURAL NETWOR, V7, P374, DOI 10.1109/72.485673
   CHOI DI, 1994, IEEE T NEURAL NETWOR, V5, P561, DOI 10.1109/72.298226
   Churchill RV, 1984, COMPLEX VARIABLES AP
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Driscoll TA, 1996, ACM T MATH SOFTWARE, V22, P168, DOI 10.1145/229473.229475
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Gotsman C, 2003, ACM T GRAPHIC, V22, P358, DOI 10.1145/882262.882276
   Gu X., 2002, Commun. Inf. Syst., V2, P121, DOI DOI 10.4310/CIS.2002.V2.N2.A2
   Gu X., 2004, IEEE Trans. Med. Imaging, V23, P1, DOI DOI 10.1109/TMI.2004.831226
   HOPPE H, 2002, 11 INT MESH ROOUNDT, P141
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Ivrissimtzis IP, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P78
   Kenner H., 1976, Geodesic Math and How to Use it
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liou CY, 1999, NEURAL NETWORKS, V12, P893, DOI 10.1016/S0893-6080(99)00034-9
   Liou CY, 2000, ARTIF INTELL, V116, P265, DOI 10.1016/S0004-3702(99)00093-4
   Ohmori K, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P126, DOI 10.1109/SMA.2001.923383
   Ritter H, 1999, KOHONEN MAPS, P97, DOI 10.1016/B978-044450270-4/50007-3
   Surazhsky V, 2003, P EUR S GEOM PROC, P17
   Tai WP, 2000, VISUAL COMPUT, V16, P91, DOI 10.1007/s003710050199
   TAI WP, 1997, THESIS NATL TAIWAN U
   Varady T, 1997, COMPUT AIDED DESIGN, V29, P255, DOI 10.1016/S0010-4485(96)00054-1
   YU Y, 1999, P IEEE VISUALIZATION, P61
NR 28
TC 8
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2005
VL 21
IS 5
BP 340
EP 353
DI 10.1007/s00371-005-0290-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 937OJ
UT WOS:000229935100005
DA 2024-07-18
ER

PT J
AU Lipus, B
   Guid, N
AF Lipus, B
   Guid, N
TI A new implicit blending technique for volumetric modelling
SO VISUAL COMPUTER
LA English
DT Article
DE computer graphics; volumetric modelling; implicit surface; blending
   function
AB Current implicit blending techniques are mostly designed for use in surface modelling, where only boundaries of the object defined by the implicit primitives are important. In contrast, in volumetric implicit modelling the interior of the object is also significant, which requires different and more suitable techniques for combining implicit primitives. In this paper, we first discuss irregularities that occur using the current techniques. Then, a new technique for blending implicit primitives, especially appropriate in volumetric modelling (e.g., cloud modelling), is introduced. It overcomes these abnormalities and gives us better results than current techniques.
C1 Univ Maribor, Fac Elect & Comp Engn, SLO-2000 Maribor, Slovenia.
C3 University of Maribor
RP Univ Maribor, Fac Elect & Comp Engn, Smetanova 17, SLO-2000 Maribor, Slovenia.
EM bogdan.lipus@uni-mb.si; guid@uni-mb.si
RI Lipus, Bogdan/Z-6203-2019
OI Lipus, Bogdan/0000-0001-6529-4263
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   BLOOMENTHAL J, 1990, 23 SIGGRAPH
   Dobashi Y, 1998, PACIFIC GRAPHICS '98, PROCEEDINGS, P53, DOI 10.1109/PCCGA.1998.731998
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   EBERT D, 1997, SIGGRAPH 97 VIS P AU, P147
   EBERT DS, 1998, TEXTURING MODELIN PR
   Elinas P., 2000, Journal of Graphics Tools, V5, P33, DOI 10.1080/10867651.2000.10487531
   Harris MJ, 2001, COMPUT GRAPH FORUM, V20, pC76, DOI 10.1111/1467-8659.00500
   Nishimura H., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P718
   Nishita T., 1993, Computer Graphics Proceedings, P175, DOI 10.1145/166117.166140
   NISHITA T, 1996, COMPUT GRAPH, V30, P379
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Wyvill B., 1989, Visual Computer, V5, P75, DOI 10.1007/BF01901483
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 18
TC 7
Z9 7
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 83
EP 91
DI 10.1007/s00371-004-0272-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300006
DA 2024-07-18
ER

PT J
AU Morey, J
   Sedig, K
AF Morey, J
   Sedig, K
TI Adjusting degree of visual complexity: an interactive approach for
   exploring four-dimensional polytopes
SO VISUAL COMPUTER
LA English
DT Article
DE mathematical visualization; high-dimensional geometry; interactive
   techniques; computer-aided visual reasoning; dynamic exploration
AB Few mathematical visualization tools support integrated, flexible interaction with complex, 4D mathematical concepts. This paper presents a solution to exploring uniform 4D polytopes through a mathematical visualization tool by introducing an approach for adjusting the degree of visual complexity of these complicated geometric structures. This approach introduces a number of interactive techniques: contextualizing, filtering, focus+scoping, and stacking-unstacking. Although these techniques can be effectively used in isolation, their integrated application provides highly specified and sophisticated interaction with polytopes, helping users make sense of these challenging mathematical structures. Exploring complicated structures from other domains such as chemistry and biology may benefit from this approach.
C1 Univ Western Ontario, Dept Comp Sci, Cognit Engn Lab, London, ON N6A 3K7, Canada.
   Univ Western Ontario, Fac Informat & Media Studies, Cognit Engn Lab, London, ON N6A 3K7, Canada.
C3 Western University (University of Western Ontario); Western University
   (University of Western Ontario)
RP Univ Western Ontario, Dept Comp Sci, Cognit Engn Lab, London, ON N6A 3K7, Canada.
EM jmorey@uwo.ca; sedig@uwo.ca
CR [Anonymous], MATH ART MATH VISUAL
   [Anonymous], P CHI 96
   [Anonymous], ACM T COMPUTER HUMAN
   [Anonymous], 1995, GRADUATE TEXTS MATH
   Arcavi A., 2000, International Journal of Computers for Mathematical Learning, V5, P25, DOI 10.1023/A:1009841817245
   CARD SK, 1999, READINGS INFORMATION
   Coxeter HSM., 1991, REGULAR COMPLEX POLY
   Cross R. A., 1994, Proceedings. Visualization '94 (Cat. No.94CH35707), P156, DOI 10.1109/VISUAL.1994.346324
   Gawrilow Ewgenij., 2001, Proceedings of the 17th annual symposium on Computational geometry, P222
   Hanson A. J., 1995, Proceedings. Visualization '95 (Cat. No.95CB35835), P126, DOI 10.1109/VISUAL.1995.480804
   Hanson A. J., 1992, Proceedings. Visualization '92 (Cat. No.92CH3201-1), P84, DOI 10.1109/VISUAL.1992.235222
   Hanson A. J., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P196, DOI 10.1109/VISUAL.1993.398869
   Hanson A. J., 1991, Proceedings Visualization '91 (Cat. No.91CH3046-0), P321, DOI 10.1109/VISUAL.1991.175821
   HANSON AJ, 1995, GRAPHICS GEMS, V5, P55
   Jackiw N., 1995, The Geometer's Sketchpad
   Keller P.R., 1993, Visual Cues: Practical Data Visualization
   McMullen P., 2002, ABSTRACT REGULAR POL
   Morey J, 2001, FIFTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P483, DOI 10.1109/IV.2001.942100
   Palais R.S., 1999, Notices of the AMS, V46, P647
   Phillips M., 1993, NOT AM MATH SOC, V40, P985
   Polthier K, 2002, MATH VISUAL, P29
   Scharein R.G., 1998, Interactive topological drawing
   Sedig K., 2003, Information Visualization, V2, P142, DOI 10.1057/palgrave.ivs.9500047
   SPENCE R, 2001, INFORMATION VISUALIZ
   Strothotte C., 1997, Seeing between the Pixels: Pictures in Interactive Systems
   Strothotte T., 1998, Computational Visualization: Graphics, Abstraction, and Interactivity
   Stylianou D.A., 2002, Journal of Mathematical Behavior, V21, P303, DOI [10.1016/S0732-3123(02)00131-1, DOI 10.1016/S0732-3123(02)00131-1]
   Tufte ER, 1997, Beautiful Evidence
   WESTHOVENS R, 1995, CLIN RHEUMATOL, V14, P19, DOI 10.1007/BF02215853
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 46
TC 4
Z9 4
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 565
EP 585
DI 10.1007/s00371-004-0259-x
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600005
DA 2024-07-18
ER

PT J
AU You, LH
   Zhang, JJ
   Comninos, P
AF You, LH
   Zhang, JJ
   Comninos, P
TI Blending surface generation using a fast and accurate analytical
   solution of a fourth-order PDE with three shape control parameters
SO VISUAL COMPUTER
LA English
DT Article
DE surface blending; fourth-order partial differential equations; fast and
   accurate PDE solution; vector-valued shape parameters; force function
ID FINITE-ELEMENT METHODS
AB In this paper, we propose to use a fourth-order partial differential equation (PDE) to solve a class of surface-blending problems. This equation has three vector-valued shape control parameters. It incorporates all the previously published forms of fourth-order PDEs for surface blending and can generate a larger class of blending surfaces than existing equations. To apply the proposed PDE to the solution of various blending problems, we have developed a fast and accurate resolution method. Our method modifies Navier's solution for the elastic bending deformation of thin plates by making it satisfy the boundary conditions exactly. A comparison between our method, the closed-form solution method, and other existing analytical methods indicates that the developed method is able to generate blending surfaces almost as quickly and accurately as the closed-form solution method, far more efficiently and accurately than the numerical methods and other existing analytical methods. Having investigated the effects that the vector-valued shape parameters and the force function of the proposed equation have on the blending surface, we have found that they have a significant influence on its shape. They provide flexible user handles that surface designers can use to adjust the blending surface to acquire the desired shape. The developed method was employed in the investigation of surface-blending problems where the primary surfaces were expressed in parametric, implicit, and explicit forms.
C1 Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth Media Sch, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth Media Sch, Poole BH12 5BB, Dorset, England.
EM jzhang@bournemouth.ac.uk
CR Bloomfield J., 1990, Australian Journal of Science and Medicine in Sport, V22, P4
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P324, DOI 10.1016/0010-4485(90)90083-O
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Bloor MIG, 2000, PHYS REV E, V61, P4218, DOI 10.1103/PhysRevE.61.4218
   Bloor MIG, 1996, COMPUT AIDED DESIGN, V28, P145, DOI 10.1016/0010-4485(95)00060-7
   BLOOR MIG, 1995, COMPUT AIDED GEOM D, V12, P381, DOI 10.1016/0167-8396(94)00021-J
   BROWN JM, 1990, ADV DESIGN AUTOMATIO, V1, P265
   CHENG SY, 1990, ADV DESIGN AUTOMATIO, V1, P257
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   DU H, 2000, P EUROGRAPHICS 2000, V19, P61
   Li ZC, 1998, J COMPUT MATH, V16, P457
   Li ZC, 1999, J COMPUT APPL MATH, V110, P241, DOI 10.1016/S0377-0427(99)00231-9
   Li ZC, 1999, J COMPUT APPL MATH, V110, P155, DOI 10.1016/S0377-0427(99)00208-3
   Mimis AP, 2001, J PROPUL POWER, V17, P492, DOI 10.2514/2.5787
   Rossignac J. R., 1984, Computers in Mechanical Engineering, V3, P65
   Ugail H, 1999, COMPUT GRAPH-UK, V23, P525, DOI 10.1016/S0097-8493(99)00071-0
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   VIDA J, 1994, COMPUT AIDED DESIGN, V26, P341, DOI 10.1016/0010-4485(94)90023-X
   Whitaker RT, 1998, P 3 INT WORKSH IMPL, P19
   You L. H., 1999, P 7 INT C CENTR EUR, P485
   You LH, 2000, COMPUT METHOD APPL M, V190, P853, DOI 10.1016/S0045-7825(99)00448-X
   You LH, 2001, FIFTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P404, DOI 10.1109/IV.2001.942089
   YOU LH, 2003, IN PRESS J MAT PROCE
   Zhang JJ, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P250, DOI 10.1109/SMA.2001.923396
   Zhang XL, 2002, ALCHERINGA, V26, P1, DOI 10.1080/03115510208619239
   Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900
NR 26
TC 14
Z9 15
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 199
EP 214
DI 10.1007/s00371-004-0241-7
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900008
DA 2024-07-18
ER

PT J
AU Ju, T
   Schaefer, S
   Warren, J
AF Ju, T
   Schaefer, S
   Warren, J
TI Convex contouring of volumetric data
SO VISUAL COMPUTER
LA English
DT Article
DE contour; polygonization; implicit modeling
AB In this paper, we present a fast, table-driven isosurface extraction technique on volumetric data. Unlike Marching Cubes or other cell-based algorithms, the proposed polygonization generates convex negative space inside individual cells, enabling fast collision detection on the triangulated isosurface. In our implementation, we are able to perform over 2 million point classifications per second. The algorithm is driven by an automatically constructed lookup table that stores compact decision trees by sign configurations. The decision trees determine triangulations dynamically by values at cell corners. Using the same technique, we can perform fast, crack-free multiresolution contouring on nested grids of volumetric data. The method can also be extended to extract isosurfaces on arbitrary convex, space-filling polyhedra.
C1 Rice Univ, Dept Comp Sci, Houston, TX 77251 USA.
C3 Rice University
RP Rice Univ, Dept Comp Sci, 6100 Main,MS-132, Houston, TX 77251 USA.
EM jutao@rice.edu; sschaefe@rice.edu; jwarren@rice.edu
CR BAKER HH, 1989, INT J COMPUT VISION, V3, P51, DOI 10.1007/BF00054838
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   BOADA I, 2001, 0102RR IIA U GIR
   DUURST MJ, 1988, COMPUT GRAPH, V22, P72
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Muller H., 1993, Visual Computer, V9, P182, DOI 10.1007/BF01901723
   NIELSON GM, 1991, VISUALIZATION 91, P83
   NING P, 1993, IEEE COMPUT GRAPH, V13, P33, DOI 10.1109/38.252552
   Poston T, 1998, COMPUT GRAPH FORUM, V17, pC137, DOI 10.1111/1467-8659.00261
   Shekhar R, 1996, IEEE VISUAL, P335, DOI 10.1109/VISUAL.1996.568127
   VANGELDER A, 1994, ACM T GRAPHIC, V13, P337, DOI 10.1145/195826.195828
   WALLIN A, 1991, IEEE COMPUT GRAPH, V11, P28, DOI 10.1109/38.103391
   Westermann R, 1999, VISUAL COMPUT, V15, P100, DOI 10.1007/s003710050165
   [No title captured]
NR 14
TC 5
Z9 7
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 513
EP 525
DI 10.1007/s00371-003-0216-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600008
DA 2024-07-18
ER

PT J
AU Wang, JX
   Zou, YS
   Alfarraj, O
   Sharma, PK
   Said, W
   Wang, J
AF Wang, Jianxin
   Zou, Yongsong
   Alfarraj, Osama
   Sharma, Pradip Kumar
   Said, Wael
   Wang, Jin
TI Image super-resolution method based on the interactive fusion of
   transformer and CNN features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Super-resolution; Transformer; Multi-head shift transposed attention;
   Feed-forward network
ID NETWORK
AB Recently, Transformer has achieved outstanding performance in the field of computer vision, where the ability to capture global context is crucial for image super-resolution (SR) reconstruction. Unlike convolutional neural networks (CNNs), Transformers lack a local mechanism for information exchange within local regions. To address this problem, we propose a U-Net network (TCSR) based on Transformer and CNN feature interaction fusion, which has skip-connections for local and global semantic feature learning. The TCSR takes the transformer blocks as the basic module of the U-Net architecture and gradually extracts multi-scale feature information while modeling global long-range dependencies. First, we propose an efficient multi-head shift transposed attention to improve the internal structure of the transformer and thus recover sufficient texture details. In addition, a feature enhancement module is inserted in the skip-connection positions to capture local structural information at different levels. Finally, to further exploit the contextual information from features, we use a locally enhanced feed-forward layer to replace the feed-forward network in each Transformer, which incorporates local feature representation into the global context. Powered by these designs, TCSR has the ability to capture both local and global dependencies for image HR reconstruction. Extensive experiments showed that compared with other state-of-the-art SR algorithms, our proposed method could effectively recover the details of the image, producing significant improvements in both visual effect and image quality.
C1 [Wang, Jianxin; Zou, Yongsong; Wang, Jin] Changsha Univ Sci & Technol, Sch Hydraul & Environm Engn, Changsha 410114, Peoples R China.
   [Wang, Jianxin; Wang, Jin] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
   [Alfarraj, Osama] King Saud Univ, Community Coll, Dept Comp Sci, Riyadh 11437, Saudi Arabia.
   [Sharma, Pradip Kumar] Univ Aberdeen, Dept Comp Sci, Aberdeen AB24 3FX, Scotland.
   [Said, Wael] Zagazig Univ, Fac Comp & Informat, Dept Comp Sci, Zagazig 44511, Egypt.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology; King Saud University; University of Aberdeen;
   Egyptian Knowledge Bank (EKB); Zagazig University
RP Wang, J (corresponding author), Changsha Univ Sci & Technol, Sch Hydraul & Environm Engn, Changsha 410114, Peoples R China.; Wang, J (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
EM jinwang@csust.edu.cn
RI Said, wael/HZK-3491-2023; Wang, Jin/AAI-7009-2020
OI Wang, Jin/0000-0001-5473-8738; Sharma, Pradip Kumar/0000-0001-6620-9083
FU This work was supported by the Scientific Research Fund of the Hunan
   Provincial Education Department (Grant No. 22C0171), the Traffic Science
   and Technology Project of Hunan Province (Grant No. 202042), the
   Research Foundation of the Education Bureau of Hu [22C0171]; Scientific
   Research Fund of the Hunan Provincial Education Department [202042];
   Traffic Science and Technology Project of Hunan Province [21B0287,
   RSP2023R102]; Research Foundation of the Education Bureau of Hunan
   Province; King Saud University, Riyadh, Saudi Arabia
FX This work was supported by the Scientific Research Fund of the Hunan
   Provincial Education Department (Grant No. 22C0171), the Traffic Science
   and Technology Project of Hunan Province (Grant No. 202042), the
   Research Foundation of the Education Bureau of Hunan Province (Grant No.
   21B0287), and the Researchers Support Project (Grant No. RSP2023R102),
   King Saud University, Riyadh, Saudi Arabia.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cao JZ, 2022, LECT NOTES COMPUT SC, V13678, P393, DOI 10.1007/978-3-031-19797-0_23
   Cao JZ, 2022, LECT NOTES COMPUT SC, V13678, P325, DOI 10.1007/978-3-031-19797-0_19
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen HY, 2021, Arxiv, DOI [arXiv:2104.09497, DOI 10.48550/ARXIV.2104.09497]
   Chen Xiangyu, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P22367, DOI 10.1109/CVPR52729.2023.02142
   Chen Y., 2023, Vis Comput, P1
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fang J., 2022, P IEEE CVF C COMP VI, P1103
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2021, INFORM SCIENCES, V546, P769, DOI 10.1016/j.ins.2020.08.114
   Hui Z, 2020, NEUROCOMPUTING, V404, P50, DOI 10.1016/j.neucom.2020.05.008
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lei Ba J., 2016, arXiv
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Li Z, 2022, IEEE T IMAGE PROCESS, V31, P2647, DOI 10.1109/TIP.2022.3160072
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang JY, 2022, Arxiv, DOI arXiv:2201.12288
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Niu B., 2020, EUR C COMP VIS, P191, DOI [10.1007/978-3-030-58610-2_47, DOI 10.1007/978-3-030-58610-2_12]
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J., 2021, KSII Trans. Internet Inf. Syst, V15, P25
   Wang JX, 2024, VISUAL COMPUT, V40, P2655, DOI 10.1007/s00371-023-02968-x
   Wang J, 2021, IEEE ACCESS, V9, P15992, DOI 10.1109/ACCESS.2021.3052946
   Wang W, 2019, INT J COMPUT INT SYS, V12, P1592, DOI 10.2991/ijcis.d.191209.001
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang JM, 2020, ANN TELECOMMUN, V75, P369, DOI 10.1007/s12243-019-00731-9
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu LL, 2019, NEUROCOMPUTING, V345, P58, DOI 10.1016/j.neucom.2018.12.077
NR 54
TC 0
Z9 0
U1 10
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 3
PY 2023
DI 10.1007/s00371-023-03138-9
EA NOV 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6SC3
UT WOS:001092896800001
DA 2024-07-18
ER

PT J
AU Wang, SY
   Kang, ZL
   Chen, L
   Guo, YJ
   Zhao, YC
   Chai, YF
AF Wang, Shenyi
   Kang, Zhilong
   Chen, Lei
   Guo, Yanju
   Zhao, Yuchen
   Chai, Yuanfei
TI Partial point cloud registration algorithm based on deep learning and
   non-corresponding point estimation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Partial point cloud registration; Deep learning; Global feature;
   Correspondence; Feature interaction; Non-corresponding point estimation
ID FRAMEWORK
AB For the limitations of global feature-based deep learning point cloud registration algorithms in partial point cloud registration, this paper proposes a partial point cloud registration algorithm NcPE-PNLK combining global features and correspondence. The NcPE-PNLK algorithm introduces a feature interaction module to complete the information interaction between two point clouds in the feature extraction stage, which can improve the credibility of the feature. Moreover, the algorithm predicts the correspondence through the non-corresponding point estimation module, which reduces the influence of non-overlapping regions on the global feature and effectively solves the problem of performance degradation of the global feature registration algorithms in partial point cloud registration. We test the registration performance of NcPE-PNLK on synthetic scene dataset and real dataset in this paper. The experimental results show that NcPE-PNLK can effectively reduce the impact of non-overlapping regions in the registration process and achieve better performance compared with the global feature-based registration algorithms. In addition, compared with the correspondence-based registration algorithms, the NcPE-PNLK algorithm does not need to calculate correspondence precisely, which can achieve high-precision partial point cloud registration with guaranteed efficiency.
C1 [Wang, Shenyi; Kang, Zhilong; Guo, Yanju; Zhao, Yuchen; Chai, Yuanfei] Hebei Univ Technol, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Chen, Lei] Tianjin Univ Commerce, Sch Informat Engn, Tianjin, Peoples R China.
C3 Hebei University of Technology; Tianjin University of Commerce
RP Guo, YJ (corresponding author), Hebei Univ Technol, Sch Elect & Informat Engn, Tianjin, Peoples R China.
EM guoyanju@hebut.edu.cn
FU Science and Technology Project of Hebei Education Department; Tianjin
   Research Program of Application Foundation [ZD2018045, 15JCYBJC17100];
   Advanced Technology of China [ZD2018045, 15JCYBJC17100]
FX This study was supported by the Science and Technology Project of Hebei
   Education Department and Tianjin Research Program of Application
   Foundation and Advanced Technology of China under Grant numbers
   ZD2018045 and 15JCYBJC17100.
CR Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Cao AQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13209, DOI 10.1109/ICCV48922.2021.01298
   Cattaneo D, 2022, IEEE T ROBOT, V38, P2074, DOI 10.1109/TRO.2022.3150683
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Elbaz G, 2017, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2017.265
   Favre K, 2021, INT C PATT RECOG, P7072, DOI 10.1109/ICPR48806.2021.9412379
   Förstner W, 2017, IEEE INT CONF COMP V, P2165, DOI 10.1109/ICCVW.2017.253
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Hitchcox T, 2020, IEEE INT C INT ROBOT, P4615, DOI 10.1109/IROS45743.2020.9340944
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Levoy M., 2005, Figshare
   Li DS, 2022, APPL INTELL, V52, P9638, DOI 10.1007/s10489-021-03055-1
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Li XQ, 2021, PROC CVPR IEEE, P12758, DOI 10.1109/CVPR46437.2021.01257
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Liu WX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15273, DOI 10.1109/ICCV48922.2021.01501
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Min T, 2021, IEEE ROBOT AUTOM LET, V6, P7270, DOI 10.1109/LRA.2021.3097268
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Quan SW, 2018, INFORM SCIENCES, V444, P153, DOI 10.1016/j.ins.2018.02.070
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Salvi J, 2007, IMAGE VISION COMPUT, V25, P578, DOI 10.1016/j.imavis.2006.05.012
   Sarode V, 2019, Arxiv, DOI arXiv:1908.07906
   Sarode V, 2020, INT CONF 3D VISION, P1029, DOI 10.1109/3DV50981.2020.00113
   Shi JL, 2018, VISUAL COMPUT, V34, P377, DOI 10.1007/s00371-016-1339-4
   Tong GF, 2024, VISUAL COMPUT, V40, P831, DOI 10.1007/s00371-023-02819-9
   Vaswani A, 2017, ADV NEUR IN, V30
   Villena Martinez V., 2020, Appl. Sci, V512, P295, DOI [10.1016/j.ins.2019.04.020, DOI 10.1016/J.INS.2019.04.020]
   Wang YH, 2019, 33 C NEURAL INFORM P, V32
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wentao Yuan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P733, DOI 10.1007/978-3-030-58558-7_43
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiaoshui Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11363, DOI 10.1109/CVPR42600.2020.01138
   Xie ZX, 2010, IMAGE VISION COMPUT, V28, P563, DOI 10.1016/j.imavis.2009.09.006
   Xiong FG, 2020, IEEE ACCESS, V8, P100120, DOI 10.1109/ACCESS.2020.2995369
   Xu H, 2022, AAAI CONF ARTIF INTE, P2848
   Yew ZJ, 2022, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR52688.2022.00656
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhao B, 2020, INFORM SCIENCES, V512, P295, DOI 10.1016/j.ins.2019.04.020
   Zhao HW, 2021, IEEE ROBOT AUTOM LET, V6, P2533, DOI 10.1109/LRA.2021.3061369
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 47
TC 1
Z9 1
U1 13
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 19
PY 2023
DI 10.1007/s00371-023-03103-6
EA OCT 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U8YY2
UT WOS:001087612700001
DA 2024-07-18
ER

PT J
AU Monika, R
   Dhanalakshmi, S
AF Monika, R.
   Dhanalakshmi, Samiappan
TI An optimal adaptive reweighted sampling-based adaptive block compressed
   sensing for underwater image compression
SO VISUAL COMPUTER
LA English
DT Article
DE Underwater image compression; Block compressed sensing; Adaptive block
   compressed sensing; Adaptive reweighted sampling; Thresholding
   orthogonal matching pursuit; Fast Haar wavelet transform; Sparse binary
   random matrix
ID RECONSTRUCTION
AB The use of Block Compressed Sensing (BCS) as an alternative to conventional Compressed Sensing (CS) in image sampling and acquisition has gained attention due to its potential benefits. However, BCS can suffer from blocking artifacts and blurs in the reconstructed images, which can degrade the overall image quality. To address these issues and improve reconstruction performance, Adaptive Block Compressed Sensing (ABCS) techniques can be used. ABCS minimizes the blurs and artifacts that occur during the reconstruction process by adaptively selecting samples from different image blocks. To further enhance the sampling efficiency and overall performance in underwater image compression, a new approach called adaptive reweighted sampling-based ABCS (ARS-ABCS) in Fast Haar Wavelet Transform (FHWT) domain is proposed in the paper. This reweighting process allows the system to allocate more samples to the areas where reconstruction quality is low or artifacts are prevalent, improving the overall image reconstruction in a targeted manner. Performance is measured in terms of Peak Signal to Noise Ratio (PSNR), Structural SIMilarity index (SSIM), Normalized Cross-Correlation (NCC) and Normalized Absolute Error (NAE). The results demonstrate that the proposed ARS-ABCS has achieved 1.5 to 5dB increase in PSNR with respect to other non-weighted adaptive schemes. It has produced space saving of 60 to 70% with utilization of only around 30% of total samples in the image. SSIM and NCC values obtained are closer to 1 with low NAE values.
C1 [Monika, R.; Dhanalakshmi, Samiappan] SRM Inst Sci & Technol, Coll Engn & Technol, Fac Engn & Technol, Dept ECE, Chengalpattu 603203, Tamilnadu, India.
C3 SRM Institute of Science & Technology Chennai
RP Dhanalakshmi, S (corresponding author), SRM Inst Sci & Technol, Coll Engn & Technol, Fac Engn & Technol, Dept ECE, Chengalpattu 603203, Tamilnadu, India.
EM dhanalas@srmist.edu.in
RI Dhanalakshmi, S./J-2073-2018
OI Dhanalakshmi, S./0000-0002-6970-2719
CR Abubakar AB, 2020, J FRANKLIN I, V357, P7266, DOI 10.1016/j.jfranklin.2020.04.022
   Akbari A, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574688
   Bhardwaj Anuj., 2009, World Applied Sciences Journal, V7, P647
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   CANH TN, 2014, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICME.2014.6890251
   Cauteruccio F, 2020, KNOWL-BASED SYST, V187, DOI 10.1016/j.knosys.2019.06.028
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fang Wang, 2012, 2012 IEEE/ACIS 11th International Conference on Computer and Information Science (ICIS), P351, DOI 10.1109/ICIS.2012.83
   Fu HS, 2020, SIGNAL PROCESS-IMAGE, V82, DOI 10.1016/j.image.2019.115774
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao XW, 2015, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2015.47
   Guo SM, 2020, J FRANKLIN I, V357, P7159, DOI 10.1016/j.jfranklin.2020.03.023
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Indyk P., 2010, SPARSE RECOVERY USIN, V1, P6034
   Jiang QR, 2022, J FRANKLIN I, V359, P2710, DOI 10.1016/j.jfranklin.2022.02.005
   Krishnaraj N, 2020, J REAL-TIME IMAGE PR, V17, P2097, DOI 10.1007/s11554-019-00879-6
   Kulkarni A, 2017, IEEE T CIRCUITS-I, V64, P1468, DOI 10.1109/TCSI.2017.2648854
   Li R, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718781751
   Li RQ, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/6758147
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu S., 2018, COMPLEXITY, V2018
   Liu S, 2019, IEEE ACCESS, V7, P62412, DOI 10.1109/ACCESS.2019.2916934
   Liu S, 2017, FRACTALS, V25, DOI 10.1142/S0218348X17400047
   Mathias A, 2019, OPTIK, V192, DOI 10.1016/j.ijleo.2019.06.025
   Monika R, 2022, WIREL NETW, DOI 10.1007/s11276-022-02921-1
   Monika R, 2022, IEEE SENS J, V22, P776, DOI 10.1109/JSEN.2021.3130947
   Monika R, 2021, MULTIMED TOOLS APPL, V80, P4751, DOI 10.1007/s11042-020-09932-0
   Monika R, 2021, VISUAL COMPUT, V37, P1499, DOI 10.1007/s00371-020-01884-8
   Monika R., 2018, ADV ELECT COMMUNICAT, P529
   Rippel O, 2017, PR MACH LEARN RES, V70
   Sun F, 2017, INT J DIGIT MULTIMED, V2017, DOI 10.1155/2017/3902543
   Thakur KV., 2016, SIGNAL IMAGE PROCESS, V7, P29, DOI [10.5121/sipij.2016.7303, DOI 10.5121/SIPIJ.2016.7303]
   Tong FH, 2020, APPL MATH COMPUT, V371, DOI 10.1016/j.amc.2019.124965
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   xahidbuffon, 2019, UND DAT
   Xiaomeng Duan, 2018, Cloud Computing and Security. 4th International Conference, ICCCS 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11063), P110, DOI 10.1007/978-3-030-00006-6_10
   Xu J, 2016, IEICE T INF SYST, VE99D, P1702, DOI 10.1587/transinf.2015EDL8230
   Yu Y, 2010, IEEE SIGNAL PROC LET, V17, P973, DOI 10.1109/LSP.2010.2080673
   Yuan XF, 2021, IEEE T NEUR NET LEAR, V32, P3296, DOI 10.1109/TNNLS.2019.2951708
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang JG, 2017, MULTIMED TOOLS APPL, V76, P4227, DOI 10.1007/s11042-016-3496-x
   Zhang Shu-fang, 2012, Journal of Tianjin University, V45, P319
   Zhang Z, 2020, MULTIMED TOOLS APPL, V79, P14777, DOI 10.1007/s11042-018-7062-6
   Zhao HH, 2020, MULTIMED TOOLS APPL, V79, P14825, DOI 10.1007/s11042-019-7647-8
   Zhou SW, 2019, MULTIMED TOOLS APPL, V78, P537, DOI 10.1007/s11042-017-5249-x
   Zhu SY, 2014, IEEE INT SYMP CIRC S, P1, DOI 10.1109/ISCAS.2014.6865050
NR 47
TC 3
Z9 3
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4071
EP 4084
DI 10.1007/s00371-023-03069-5
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001066001600002
DA 2024-07-18
ER

PT J
AU Johnson, J
   Chitra, R
AF Johnson, Jyothi
   Chitra, R.
TI Multimodal biometric identification based on overlapped fingerprints,
   palm prints, and finger knuckles using BM-KMA and CS-RBFNN techniques in
   forensic applications
SO VISUAL COMPUTER
LA English
DT Article
DE Multimodal biometric recognition; Feature extraction; Forensic
   applications; Overlapped fingerprints; Principal component analysis
   (PCA); Kendall rank correlation (KRC); Contrast limited adaptive
   histogram equalization (CLAHE); Bilateral filter (BF)
ID SCORE LEVEL FUSION
AB In several scenarios like forensic and civilian applications, biometric has emerged as a powerful technology for person authentication. Information extracted from different biometric traits is combined by the Multimodal Biometric (MB) solutions, hence showing a high resilience against presentation attacks. Additionally, they offer enhanced biometric performance and increased population coverage that is required for executing larger-scale recognition. By employing Brownian Motion enabled K-Means Algorithm (BM-KMA) and Cosine Swish activation-based Radial Basis Function Neural Network (RBFNN) (CS-RBFNN) methodologies, an MB authentication system centered on overlapped Fingerprints (FPs), Palm Prints (PPs), and finger knuckles (FKs) is proposed here. Primarily, from the publically available datasets, the overlapped FP images and hand images are taken. Next, to separate the PPs and FKs, the Region of Interest (ROI) is estimated for the hand image. Then, pre-processing, feature extraction, and feature reduction are carried out. From the overlapped FP, the noises are removed using BF; after that, the FP's contrast is enriched using SMF-CLAHE for improving the clarity of the minutiae structure of the ridges. Following this, normalization is performed using the Min-Max operation. Minute features are extracted by separating the overlapped FP using BM-KMA, which makes the system from avoidance of system complexity by separating the overlapping. From this, interest features are selected using KRC-PCA. Next, feature fusion is conducted. Finally, CS-RBFNN is wielded to categorize genuine biometrics from imposter ones. Via performance metrics, the proposed system is further affirmed. The outcomes exhibited that the proposed technique surpasses the other prevailing methodologies.
C1 [Johnson, Jyothi] Noorul Islam Ctr Higher Educ, Dept Comp Sci & Engn, Kanyakumari, Tamil Nadu, India.
   [Chitra, R.] Karunya Inst Technol & Sci, Dept Comp Sci & Engn, Coimbatore, India.
C3 Karunya Institute of Technology & Sciences
RP Johnson, J (corresponding author), Noorul Islam Ctr Higher Educ, Dept Comp Sci & Engn, Kanyakumari, Tamil Nadu, India.
EM jyothijohnson586@gmail.com; chitrajegan5@gmail.com
RI Johnson, Jyothi/KRQ-4948-2024
OI Johnson, Jyothi/0000-0003-3835-3760
CR Anbari M, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01178-6
   Anitha ML, 2014, INT CONF DIGIT SIG, P574, DOI 10.1109/ICDSP.2014.6900730
   Arora G., 2021, KNOWL-BASED SYST, V239, P1
   Attia A, 2022, MULTIMED TOOLS APPL, V81, P10961, DOI 10.1007/s11042-022-12384-3
   Bouchaffra D, 2008, PATTERN RECOGN, V41, P852, DOI 10.1016/j.patcog.2007.06.033
   Chin YJ, 2014, INFORM FUSION, V18, P161, DOI 10.1016/j.inffus.2013.09.001
   Choudhury SH, 2021, APPL SOFT COMPUT, V106, DOI 10.1016/j.asoc.2021.107344
   Farooq H., 2018, INT J INF TECHNOL, V12, P1281
   Garg P., 2020, MATER TODAY-PROC, DOI [10.1016/j.matpr.2020.11.141, DOI 10.1016/J.MATPR.2020.11.141]
   George A, 2014, 2014 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING APPLICATIONS (ICICA 2014), P249, DOI 10.1109/ICICA.2014.60
   Gopal, 2016, APPL SOFT COMPUT, V47, P12, DOI 10.1016/j.asoc.2016.05.039
   Huang HC, 2018, APPL SOFT COMPUT, V71, P127, DOI 10.1016/j.asoc.2018.06.008
   Jagadiswary D, 2016, PROCEDIA COMPUT SCI, V85, P109, DOI 10.1016/j.procs.2016.05.187
   Kaushikp S., 2016, 5 INT C REL INF TECH, DOI [https://doi.org/10.1109/ICRITO.2016.7784958, DOI 10.1109/ICRITO.2016.7784958]
   Khodadoust J, 2021, EXPERT SYST APPL, V176, DOI 10.1016/j.eswa.2021.114687
   Kumar R, 2010, PROCEDIA COMPUT SCI, V2, P159, DOI 10.1016/j.procs.2010.11.020
   Kute R., 2021, J KING SAUD U ENG SC, DOI [10.1016/j.jksues.2021.07.011, DOI 10.1016/J.JKSUES.2021.07.011]
   Lang Zhai, 2011, 2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC 2011), P3530, DOI 10.1109/AIMSEC.2011.6010656
   Leng L, 2017, MULTIMED TOOLS APPL, V76, P333, DOI 10.1007/s11042-015-3058-7
   Leng L, 2013, NEUROCOMPUTING, V108, P1, DOI 10.1016/j.neucom.2012.08.028
   Ma Xin, 2017, Journal of China Universities of Posts and Telecommunications, V24, P34, DOI 10.1016/S1005-8885(17)60221-8
   Ma Xin, 2017, Journal of China Universities of Posts and Telecommunications, V24, P55, DOI 10.1016/S1005-8885(17)60242-5
   Michael GKO, 2010, C IND ELECT APPL, P346
   Peng JL, 2014, OPTIK, V125, P6891, DOI 10.1016/j.ijleo.2014.07.027
   Usha K, 2018, AIN SHAMS ENG J, V9, P549, DOI 10.1016/j.asej.2016.04.006
   Vijay M, 2021, J INF SECUR APPL, V58, DOI 10.1016/j.jisa.2020.102707
   Vijayalakshmi V., 2013, INT C COMM SIGN PROC, DOI [10.1109/iccsp.2013.6577154, DOI 10.1109/ICCSP.2013.6577154]
   Walia GS, 2019, EXPERT SYST APPL, V116, P364, DOI 10.1016/j.eswa.2018.08.036
   Yang WC, 2018, PATTERN RECOGN, V78, P242, DOI 10.1016/j.patcog.2018.01.026
   Zhu Le-qing, 2011, 2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2011), P1879, DOI 10.1109/FSKD.2011.6019781
NR 30
TC 1
Z9 1
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3217
EP 3231
DI 10.1007/s00371-023-03023-5
EA AUG 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001047709300003
DA 2024-07-18
ER

PT J
AU Yu, CC
   Li, SA
   Feng, WB
   Zheng, T
   Liu, S
AF Yu, Chongchong
   Li, Shunan
   Feng, Wenbin
   Zheng, Tong
   Liu, Shu
TI SACA-fusion: a low-light fusion architecture of infrared and visible
   images based on self- and cross-attention
SO VISUAL COMPUTER
LA English
DT Article
DE Visible-infrared image fusion; Low light; Self-attention module;
   Cross-attention module; Transformer module
ID MULTI-FOCUS; TRANSFORM; FRAMEWORK; NETWORK; NEST
AB Visible-infrared image fusion cannot only reveal respective features of multiband imaging but also combine complementary information. It thus highlights salient information that cannot be directly obtained from a single waveband and enhances scene detection and perception. However, low-light condition for special scenarios, i.e., underground coal mine, impacts the performance of visible-infrared image fusion as they lead to lower contrast for visible light images and loss of local details. In this respect, we propose an infrared and visible image fusion architecture in low-light conditions based on self- and cross-attention (SACA-Fusion). This architecture replaces traditional fusion approaches with a transformer-based fusion network. It better extracts long-range dependencies of images and improves space recovery of fused images. The architecture has an attention mechanism composed of two modules. The self-attention module achieves global interaction and fusion of features and reduces loss in local details; the cross-attention module in nest connect enhances features in low-light conditions and achieves low-contrast space recovery. In the experiment part, through ablation, we confirm that the wonderful fusion strategy is transformer module, rather than RFN or directly connecting. Then, based on comparison experiments on TNO and LLVIP datasets, it is shown that the better fusion performance of the proposed one under some evaluation indicators. Especially in the actual low-light condition, the improvement of the fusion effect is commendable.
C1 [Yu, Chongchong; Li, Shunan; Zheng, Tong] Beijing Technol & Business Univ, Sch Artificial Intelligence, 11 Fucheng Rd, Beijing 100048, Peoples R China.
   [Feng, Wenbin] Shenyang Res Inst, China Coal Technol & Engn Grp, Fushun 113122, Peoples R China.
   [Liu, Shu] China Acad Ind Internet, Beijing 101103, Peoples R China.
C3 Beijing Technology & Business University
RP Zheng, T (corresponding author), Beijing Technol & Business Univ, Sch Artificial Intelligence, 11 Fucheng Rd, Beijing 100048, Peoples R China.
EM chongzhy@vip.sina.com; 895531424@qq.com; 625309069@qq.com;
   20211206@btbu.edu.cn; iushu1@caict.ac.cn
OI Zheng, Tong/0000-0003-2251-6844
FU Beijing Natural Science Foundation [4202015]; Chinese University
   Industry-University-Research Innovation Fund-Blue Dot Distributed
   Intelligent Computing Project [2021LDA06002]
FX Beijing Natural Science Foundation (4202015), Chinese University
   Industry-University-Research Innovation Fund-Blue Dot Distributed
   Intelligent Computing Project (2021LDA06002).
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cao L, 2015, IEEE SIGNAL PROC LET, V22, P220, DOI 10.1109/LSP.2014.2354534
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Cvejic N, 2007, IEEE SENS J, V7, P743, DOI 10.1109/JSEN.2007.894926
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fu ZZ, 2016, INFRARED PHYS TECHN, V77, P114, DOI 10.1016/j.infrared.2016.05.012
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Z., 2021, ARXIV
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Jin X, 2017, INFRARED PHYS TECHN, V85, P478, DOI 10.1016/j.infrared.2017.07.010
   Kuncheva LI, 2014, IEEE T NEUR NET LEAR, V25, P69, DOI 10.1109/TNNLS.2013.2248094
   Li HG, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050441
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu XB, 2017, NEUROCOMPUTING, V235, P131, DOI 10.1016/j.neucom.2017.01.006
   Liu YP, 2014, SIGNAL PROCESS, V97, P9, DOI 10.1016/j.sigpro.2013.10.010
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lopez-Molina C, 2018, INFORM FUSION, V44, P136, DOI 10.1016/j.inffus.2018.03.004
   Luo CW, 2019, INFRARED PHYS TECHN, V99, P265, DOI 10.1016/j.infrared.2019.04.017
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma WH, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23020599
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Shreyamsha Kumar BK, 2013, SIGNAL IMAGE VIDEO P, V7, P1125, DOI 10.1007/s11760-012-0361-x
   Tian J, 2015, NANO ENERGY, V11, P419, DOI 10.1016/j.nanoen.2014.10.025
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yang B, 2016, INT J WAVELETS MULTI, V14, DOI 10.1142/S0219691316500247
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang QH, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.5.057006
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
NR 52
TC 2
Z9 2
U1 31
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3347
EP 3356
DI 10.1007/s00371-023-03037-z
EA AUG 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001047709300002
DA 2024-07-18
ER

PT J
AU Lin, CD
   Wang, P
   Xiong, SW
   Chen, RY
AF Lin, Chengde
   Wang, Peng
   Xiong, Shengwu
   Chen, Ruyi
TI Orthogonal integral transform for 3D shape recognition with few examples
SO VISUAL COMPUTER
LA English
DT Article
DE Orthogonal integral transform; 3D shape recognition; Protein shape
   retrieval; 3D invariant descriptor; Few examples
ID CLASSIFICATION; IMAGES; SEARCH
AB 3D shape recognition with few examples is crucial for applications involving 3D scenes, but typical methods based on surface and view suffer the failure to describe the interior and exterior features uniformly. Thus, we propose 3D orthogonal integral transform (OIT). OIT is composed of three individual integrals over a group of three orthogonal planes rotating to cover all orientations by which the volumetric shape is bisected in integrals. OIT offers the following advantages: (1) It describes a 3D shape structurally from interior to exterior uniformly, which brings about discriminative shape characteristics; and (2) the shape descriptor built on OIT is invariant with respect to translation, scaling and rotation. Furthermore, a fine-grained 3D model dataset (FGModele40) is built on ModelNet40. Experiments show that OIT can provide both discriminative and robust descriptors for 3D shape recognition with few examples. Our proposed OIT outperforms typical state-of-the-art benchmarks evaluated by the protein shape retrieval contest; additionally, it also surpasses other typical deep learning models with respect to the task of 3D shape recognition with few examples on FGModele40.
C1 [Lin, Chengde] Guilin Univ Elect Technol, Sch Artificial Intelligence, Guilin 541004, Peoples R China.
   [Wang, Peng] Wuyi Univ, Sch Math & Computat Sci, Jiangmen 529020, Peoples R China.
   [Xiong, Shengwu; Chen, Ruyi] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
C3 Guilin University of Electronic Technology; Wuyi University; Wuhan
   University of Technology
RP Lin, CD (corresponding author), Guilin Univ Elect Technol, Sch Artificial Intelligence, Guilin 541004, Peoples R China.
EM lcd@guet.edu.cn; pwong@126.com; xiongsw@whut.edu.cn; rychen@whut.edu.cn
FU NSFC [62176194, 62101393]; Major project of IoV [2020AAA001]; Sanya
   Science and Education Innovation Park of Wuhan University of Technology
   [2021KF0031]; CSTC [cstc2021jcyj-msxmX1148]; Open Project of Wuhan
   University of Technology Chongqing Research Institute [ZL2021-6]
FX This work was in part supported by NSFC (Grant No. 62176194, Grant
   No.62101393), the Major project of IoV (Grant No. 2020AAA001), Sanya
   Science and Education Innovation Park of Wuhan University of Technology
   (Grant No. 2021KF0031), CSTC (Grant No. cstc2021jcyj-msxmX1148) and the
   Open Project of Wuhan University of Technology Chongqing Research
   Institute (ZL2021-6).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Aubry M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Averbuch A, 2003, APPL COMPUT HARMON A, V15, P33, DOI 10.1016/S1063-5203(03)00030-7
   Axenopoulos A, 2016, IEEE ACM T COMPUT BI, V13, P954, DOI 10.1109/TCBB.2015.2498553
   Bahroun S, 2023, VISUAL COMPUT, V39, P239, DOI 10.1007/s00371-021-02324-x
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Benhabiles Halim, 2019, Processing and Analysis of Biomedical Information. First International SIPAIM Workshop, SaMBa 2018. Held in Conjunction with MICCAI 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11379), P82, DOI 10.1007/978-3-030-13835-6_10
   Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235
   Chu HZ, 2022, VISUAL COMPUT, V38, P3703, DOI 10.1007/s00371-021-02203-5
   Cosmo Luca, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P1, DOI 10.1007/978-3-030-58565-5_1
   Craciun D., 2017, EUR WORKSH 3D OBJ RE
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287
   Daras P, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P953, DOI 10.1109/TDPVT.2004.1335419
   Fox NK, 2014, NUCLEIC ACIDS RES, V42, pD304, DOI 10.1093/nar/gkt1240
   Ganapathi II, 2020, VISUAL COMPUT, V36, P161, DOI 10.1007/s00371-018-1593-8
   Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Han L, 2021, MULTIMED TOOLS APPL, V80, P973, DOI 10.1007/s11042-020-09764-y
   He XW, 2020, IEEE T IMAGE PROCESS, V29, P7917, DOI 10.1109/TIP.2020.3008970
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kanezaki A, 2021, IEEE T PATTERN ANAL, V43, P269, DOI 10.1109/TPAMI.2019.2922640
   Langenfeld F., 2018, EUROGRAPHICS C 3D OB
   Langenfeld F, 2020, COMPUT GRAPH-UK, V91, P189, DOI 10.1016/j.cag.2020.07.013
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Li HS, 2018, INT CONF BIG DATA, P448, DOI 10.1109/BigComp.2018.00072
   Li YY, 2016, ADV NEUR IN, V29
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin CD, 2023, VISUAL COMPUT, V39, P6005, DOI 10.1007/s00371-022-02708-7
   Lin CD, 2022, IMAGE VISION COMPUT, V125, DOI 10.1016/j.imavis.2022.104517
   Lin CD, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103566
   Liu XP, 2022, VISUAL COMPUT, V38, P669, DOI 10.1007/s00371-020-02042-w
   Liu Y, 2018, BIOINFORMATICS, V34, P773, DOI 10.1093/bioinformatics/bty585
   Masoumi M., 2015, PHYS MED BIOL, V63, P34
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mirloo M, 2018, MULTIMED TOOLS APPL, V77, P6987, DOI 10.1007/s11042-017-4617-x
   Nardelli P, 2018, IEEE T MED IMAGING, V37, P2428, DOI 10.1109/TMI.2018.2833385
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Roth HR, 2016, IEEE T MED IMAGING, V35, P1170, DOI 10.1109/TMI.2015.2482920
   Said S, 2008, IEEE T SIGNAL PROCES, V56, P1522, DOI 10.1109/TSP.2007.910477
   Setio AAA, 2016, IEEE T MED IMAGING, V35, P1160, DOI 10.1109/TMI.2016.2536809
   Shao TJ, 2020, IEEE T VIS COMPUT GR, V26, P2403, DOI 10.1109/TVCG.2018.2887262
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sit A, 2019, PATTERN RECOGN, V93, P534, DOI 10.1016/j.patcog.2019.05.019
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Suryanto CH, 2018, IEEE ACM T COMPUT BI, V15, P286, DOI 10.1109/TCBB.2016.2603987
   Tabbone S, 2006, COMPUT VIS IMAGE UND, V102, P42, DOI 10.1016/j.cviu.2005.06.005
   Wang B, 2016, IEEE T IMAGE PROCESS, V25, P5635, DOI 10.1109/TIP.2016.2609816
   Wu JJ, 2016, ADV NEUR IN, V29
   Xie YT, 2019, IEEE T MED IMAGING, V38, P991, DOI 10.1109/TMI.2018.2876510
   Zhang D, 2021, MULTIMED TOOLS APPL, V80, P615, DOI 10.1007/s11042-020-09420-5
   Zhang D, 2021, VISUAL COMPUT, V37, P749, DOI 10.1007/s00371-020-01946-x
NR 56
TC 0
Z9 0
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3271
EP 3284
DI 10.1007/s00371-023-03030-6
EA AUG 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001045114100001
DA 2024-07-18
ER

PT J
AU Lu, XM
   Quan, W
   Marek, R
   Zhao, HQ
   Chen, JX
AF Lu, Xuemin
   Quan, Wei
   Marek, Reformat
   Zhao, Haiquan
   Chen, Jim X. X.
TI SiamMAST: Siamese motion-aware spatio-temporal network for video action
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Video action recognition; Siamese network; Spatio-temporal features;
   Spatial-motion awareness; Temporal-motion awareness
ID VECTOR
AB This paper proposes a Siamese motion-aware Spatio-temporal network (SiamMAST) for video action recognition. The SiamMAST is designed based on the fusion of four features via processing video frames: spatial features, temporal features, spatial dynamic features, and temporal dynamic features of a moving target. The SiamMAST comprises AlexNets as the backbone, LSTMs, and the spatial motion-awareness and temporal motion-awareness sub-modules. RGB images are fed into the network, where AlexNets extract spatial features. Further, they are fed into LSTMs to generate temporal features. Additionally, spatial motion-awareness and temporal motion-awareness sub-modules are proposed to capture spatial and temporal dynamic features. Finally, all features are fused and fed into the classification layer. The final recognition result is produced by averaging the test label probabilities across a fixed number of RGB frames and selecting the label of the highest probability. The whole network is trained offline using an end-to-end approach with large-scale image datasets using the standard SGD algorithm with back-propagation. The proposed network is evaluated on two challenging datasets UCF101 (93.53%) and HMDB51 (69.36%). The experiments have demonstrated the effectiveness and efficiency of our proposed SiamMAST.
C1 [Lu, Xuemin] Southwest China Inst Elect Technol, Chengdu 610036, Peoples R China.
   [Lu, Xuemin; Quan, Wei; Zhao, Haiquan] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Sichuan, Peoples R China.
   [Marek, Reformat] Univ Alberta, Sch Elect & Comp Engn, Edmonton, AB T6G 1H9, Canada.
   [Chen, Jim X. X.] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
C3 Southwest Jiaotong University; University of Alberta; George Mason
   University
RP Quan, W (corresponding author), Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Sichuan, Peoples R China.
EM wquan@swjtu.edu.cn
OI Quan, Wei/0000-0001-7926-9501
FU National Natural Science Foundation of China [52277127]; Science and
   Technology Innovation Talent Project of Sichuan Province [2021JDRC0012];
   Independent Research Project of National Key Laboratory of Traction
   Power of China [2019TPL-T19]; Key Interdisciplinary Basic Research
   Project of Southwest Jiaotong University [2682021ZTPY089]; Open Research
   Project of National Rail Transit Electrification and Automation
   Engineering Technology Research Center and Chengdu Guojia Electrical
   Engineering Co., Ltd [NEEC-2019-B06]; State Scholarship Fund of China
   Scholarship Council [202007000101]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 52277127), Science and Technology Innovation Talent
   Project of Sichuan Province (Grant No. 2021JDRC0012), Independent
   Research Project of National Key Laboratory of Traction Power of China
   (Grant No. 2019TPL-T19), Key Interdisciplinary Basic Research Project of
   Southwest Jiaotong University (Grant No. 2682021ZTPY089), Open Research
   Project of National Rail Transit Electrification and Automation
   Engineering Technology Research Center and Chengdu Guojia Electrical
   Engineering Co., Ltd (Grant No. NEEC-2019-B06), and State Scholarship
   Fund of China Scholarship Council. (Grant No. 202007000101).
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Abidine MB, 2012, INT CONF MULTIMED, P272, DOI 10.1109/ICMCS.2012.6320300
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chan T., 2014, ARXIV
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Diba A, 2017, ARXIV
   Dollar P., 2005, VISUAL SURVEILLANCE, V14, P65, DOI DOI 10.1109/VSPETS.2005.1570899
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Liang D., 2021, VISUAL COMPUT, V37, P1327
   Liu H, 2018, IEEE T CIRC SYST VID, V28, P2788, DOI 10.1109/TCSVT.2017.2715499
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Raman N, 2016, NEUROCOMPUTING, V199, P163, DOI 10.1016/j.neucom.2016.03.024
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Thanikachalam V., 2015, INT J APPL ENG RES, V10, P361
   Tran D., 2017, arXiv
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H., 2016, ACTION RECOGNITION D, P3169
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Yu F., 2015, ARXIV
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
NR 49
TC 0
Z9 0
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3163
EP 3181
DI 10.1007/s00371-023-03018-2
EA JUL 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001035720700001
DA 2024-07-18
ER

PT J
AU Wang, SB
   Mei, XS
   Kang, PS
   Li, Y
   Liu, D
AF Wang, Shibin
   Mei, Xueshu
   Kang, Pengshuai
   Li, Yan
   Liu, Dong
TI DFC-dehaze: an improved cycle-consistent generative adversarial network
   for unpaired image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Dehazing; CycleGAN; Unpair data
AB Recently, cycle-consistent adversarial network (CycleGAN) has been utilized in image dehazing tasks due to train without paired images. However, the quality of generated dehazed images is low. To enhance the dehazed images, this paper presents an improved CycleGAN model, called DFC-dehaze, in which the convolutional neural network (CNN)-based generator is replaced by the Dehazeformer-t model. To reduce the haze residuals in the recovered images, the local-global discriminator is used to handle locally varying haze. When the generated images are blurred or have unrealistic colors, the discriminator gives them the low scores by the negative sample punishment mechanism. Another, the structural similarity index measurement (SSIM) loss is combined with the cyclic consistent loss to improve the visual quality of the dehazed images. The experimental results demonstrate that the proposed methods outperform other unsupervised methods and yield visual outcomes that are comparable to those produced by supervised methods.
C1 [Wang, Shibin; Mei, Xueshu; Kang, Pengshuai; Li, Yan; Liu, Dong] Henan Normal Univ, Sch Comp & Informat Engn, 46 Construct Rd, Xinxiang 453007, Peoples R China.
   [Wang, Shibin; Liu, Dong] Key Lab Artificial Intelligence & Personalized Lea, Xinxiang, Peoples R China.
C3 Henan Normal University
RP Wang, SB (corresponding author), Henan Normal Univ, Sch Comp & Informat Engn, 46 Construct Rd, Xinxiang 453007, Peoples R China.; Wang, SB (corresponding author), Key Lab Artificial Intelligence & Personalized Lea, Xinxiang, Peoples R China.
EM wangshibin@htu.edu.cn; meixueshu@stu.htu.edu.cn;
   kangpengshuai@stu.htu.edu.cn; liyan@stu.htu.edu.cn; liudong@htu.edu.cn
RI Wang, Shibin/AAH-1793-2019
FU scientific and technological project in Henan Province in 2022
   [222102210187]; National Natural Science Foundation of China [62072160]
FX This work was supported by the scientific and technological project in
   Henan Province in 2022 (Grant No. 222102210187), the National Natural
   Science Foundation of China (Grant No. 62072160).
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/icip.2019.8803046, 10.1109/ICIP.2019.8803046]
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiangxin Dong, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P188, DOI 10.1007/978-3-030-58577-8_12
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li FH, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8060924
   Li Jinlong, 2022, IEEE T MULTIMEDIA
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2018, arXiv
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Maqsood S, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/4036434
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Park J, 2020, IEEE T IMAGE PROCESS, V29, P4721, DOI 10.1109/TIP.2020.2975986
   Qili Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P722, DOI 10.1007/978-3-030-58539-6_43
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song Y., 2022, ARXIV
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 47
TC 1
Z9 1
U1 16
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2807
EP 2818
DI 10.1007/s00371-023-02987-8
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001035766800003
DA 2024-07-18
ER

PT J
AU Zhu, C
   Yi, BS
   Luo, LG
AF Zhu, Chao
   Yi, Benshun
   Luo, Laigan
TI CNBCC: cubic non-uniform B-spline closed curve for arbitrary shape text
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Scene text detection; Cubic non-uniform B-spline closed curve;
   Anchor-free method; Knot vector
AB With the development of deep learning, the performance and efficiency of text detection in natural scenes have been significantly improved. Due to the irregular geometric shape of natural scene text, it is challenging to detect text of arbitrary shape. Most of the existing methods are regression-based or segmentation-based methods. This paper presents an efficient framework to detect arbitrary shape text instances by combining regression-based and segmentation-based methods. Specifically, we use cubic non-uniform B-spline closed curve to fit the boundaries of arbitrary-shaped text instances. By adopting the anchor-free method as the regression detector to obtain the coordinates of B-spline curve control points, and using the segmentation method to obtain the knot vector value, our method not only uses the detection efficiency of regression method, but also combines the insensitivity of segmentation method to arbitrary shape text to improve the accuracy of text detection. Experiments on ICAR2015, CTW1500 and total-text benchmarks, including regular shape and arbitrary shape scene text in natural images, demonstrate the effectiveness of the proposed method.
C1 [Zhu, Chao; Yi, Benshun; Luo, Laigan] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Yi, BS (corresponding author), Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
EM zhuchao_mr@whu.edu.cn; yibs@whu.edu.cn; luolaigan@whu.edu.cn
OI yi, benshun/0000-0002-2818-9357
CR [Anonymous], 2016, DETECTING TEXT NATUR
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai YC, 2018, INT C PATT RECOG, P3604, DOI 10.1109/ICPR.2018.8546066
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu Z., arXiv
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tang QS, 2022, MULTIMED TOOLS APPL, V81, P15285, DOI 10.1007/s11042-022-12619-3
   Tian ZY, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS AND COMPUTING (IEEE ICSPCC 2020)
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue C., 2019, ARXIV
   Yang C, 2022, IEEE T IMAGE PROCESS, V31, P2864, DOI 10.1109/TIP.2022.3141844
   Yaoyao Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12242, DOI 10.1109/CVPR42600.2020.01226
   Yuliang L, 2017, ARXIV
   Zhong ZY, 2019, INT J DOC ANAL RECOG, V22, P315, DOI 10.1007/s10032-019-00335-y
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336
NR 37
TC 2
Z9 2
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3023
EP 3032
DI 10.1007/s00371-023-03005-7
EA JUL 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001034268300004
DA 2024-07-18
ER

PT J
AU Ballester-Ripoll, R
   Halter, G
   Pajarola, R
AF Ballester-Ripoll, Rafael
   Halter, Gaudenz
   Pajarola, Renato
TI High-dimensional scalar function visualization using principal
   parameterizations
SO VISUAL COMPUTER
LA English
DT Article
DE Scientific visualization; Sensitivity analysis; Dimensionality
   reduction; Tensor decompositions
ID DECOMPOSITIONS; EXPLORATION; ALGORITHMS; MODEL
AB Insightful visualization of multidimensional scalar fields, in particular parameter spaces, is key to many computational science and engineering disciplines. We propose a principal component-based approach to visualize such fields that accurately reflects their sensitivity to their input parameters. The method performs dimensionality reduction on the space formed by all possible partial functions (i.e., those defined by fixing one or more input parameters to specific values), which are projected to low-dimensional parameterized manifolds such as 3D curves, surfaces, and ensembles thereof. Our mapping provides a direct geometrical and visual interpretation in terms of Sobol's celebrated method for variance-based sensitivity analysis. We furthermore contribute a practical realization of the proposed method by means of tensor decomposition, which enables accurate yet interactive integration and multilinear principal component analysis of high-dimensional models.
C1 [Ballester-Ripoll, Rafael] IE Univ, Madrid, Spain.
   [Halter, Gaudenz; Pajarola, Renato] Univ Zurich, Zurich, Switzerland.
C3 IE University; University of Zurich
RP Ballester-Ripoll, R (corresponding author), IE Univ, Madrid, Spain.
EM rafael.ballester@ie.edu; halter@ifi.uzh.ch; pajarola@ifi.uzh.ch
OI Ballester-Ripoll, Rafael/0000-0001-5831-2056
FU Swiss National Science Foundation (SNSF) SPIRIT Grant [IZSTZ0_208541];
   Swiss National Science Foundation (SNF) [IZSTZ0_208541] Funding Source:
   Swiss National Science Foundation (SNF)
FX This project was partially supported by a Swiss National Science
   Foundation (SNSF) SPIRIT Grant (project no. IZSTZ0_208541).
CR An J, 2001, J COMPLEXITY, V17, P588, DOI 10.1006/jcom.2001.0588
   Ballester-Ripoll R., 2017, ARXIV
   Ballester-Ripoll R., 2016, SIGGRAPH ASIA 2016 S, p13:1
   Ballester-Ripoll R, 2018, SIAM-ASA J UNCERTAIN, V6, P1172, DOI 10.1137/17M1160252
   Banzhaf J., 1965, RUTGERS LAW REV, V19, P317
   Bigoni D, 2016, SIAM J SCI COMPUT, V38, pA2405, DOI 10.1137/15M1036919
   Bolado-Lavin R, 2009, RELIAB ENG SYST SAFE, V94, P1041, DOI 10.1016/j.ress.2008.11.012
   Bruckner S, 2010, IEEE T VIS COMPUT GR, V16, P1468, DOI 10.1109/TVCG.2010.190
   Castura JC, 2016, FOOD QUAL PREFER, V54, P90, DOI 10.1016/j.foodqual.2016.06.011
   Constantine PG., 2015, Active subspaces: emerging ideas for dimension reduction in parameter studies, DOI DOI 10.1137/1.9781611973860
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P1842, DOI 10.1109/TVCG.2011.244
   Diaz P, 2018, APPL MATH COMPUT, V324, P141, DOI 10.1016/j.amc.2017.11.039
   Dubourg V, 2013, PROBABILIST ENG MECH, V33, P47, DOI 10.1016/j.probengmech.2013.02.002
   Fruth J., 2013, PREPRINT
   Gallagher M, 2003, IEEE T SYST MAN CY B, V33, P28, DOI 10.1109/TSMCB.2003.808183
   Gardner TS, 1998, P NATL ACAD SCI USA, V95, P14190, DOI 10.1073/pnas.95.24.14190
   Gerber S, 2010, IEEE T VIS COMPUT GR, V16, P1271, DOI 10.1109/TVCG.2010.213
   github, TTPY TENS TRAIN TOOL
   github, TTREC HIGH LEV PYTH
   GOLDBETER A, 1991, P NATL ACAD SCI USA, V88, P9107, DOI 10.1073/pnas.88.20.9107
   Gorodetsky A.A., 2018, ARXIV
   GRASEDYCK L., 2013, GAMM-Mitteilungen36, V36, P53, DOI 10.1002/gamm.201310004
   Heinrich J., 2013, STAR Proc. Eurographics, V2013, P95, DOI [DOI 10.2312/C0NF/EG2013/STARS/095, DOI 10.2312/CONF/EG2013/STARS/095-116]
   Insuasty E, 2017, COMPUTAT GEOSCI, V21, P645, DOI 10.1007/s10596-017-9641-4
   Iooss B., 2015, UNCERTAINTY MANAGEME, P101, DOI [DOI 10.1007/978-1-4899-7547-8_5, 10.1007/978-1-4899-7547-8_5]
   Knutsson H., 1989, 87654321 LITHISYI LI
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Liu SS, 2017, IEEE T VIS COMPUT GR, V23, P1249, DOI 10.1109/TVCG.2016.2640960
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Oseledets I, 2010, LINEAR ALGEBRA APPL, V432, P70, DOI 10.1016/j.laa.2009.07.024
   Saltelli A., 2008, Global Sensitivity Analysis: The Primer, DOI DOI 10.1002/9780470725184
   Sedlmair, 2012, TR201203 U BRIT COL
   Sedlmair M, 2014, IEEE T VIS COMPUT GR, V20, P2161, DOI 10.1109/TVCG.2014.2346321
   Shao L, 2017, COMPUT GRAPH FORUM, V36, P157, DOI 10.1111/cgf.13176
   Sobol' I. Y. M., 1990, MATEM MOD, V2, P112, DOI DOI 10.18287/0134-2452-2015-39-4-459-461
   Torsney-Weir T, 2017, COMPUT GRAPH FORUM, V36, P167, DOI 10.1111/cgf.13177
   Torsney-Weir T, 2011, IEEE T VIS COMPUT GR, V17, P1892, DOI 10.1109/TVCG.2011.248
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   van Wijk J. J., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P119, DOI 10.1109/VISUAL.1993.398859
   Vasilescu MAO, 2002, LECT NOTES COMPUT SC, V2350, P447
   Vervliet N, 2014, IEEE SIGNAL PROC MAG, V31, P71, DOI 10.1109/MSP.2014.2329429
   Ward M.O., 1994, P COMP GRAPH INT, P95
   Wu Q, 2008, IEEE T VIS COMPUT GR, V14, P186, DOI 10.1109/TVCG.2007.70406
   Yu-Hsuan Chan, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P43, DOI 10.1109/VAST.2010.5652460
NR 44
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2571
EP 2588
DI 10.1007/s00371-023-02937-4
EA JUN 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001019486000001
DA 2024-07-18
ER

PT J
AU Maggiordomo, A
   Uralsky, Y
   Moreton, H
   Tarini, M
AF Maggiordomo, Andrea
   Uralsky, Yury
   Moreton, Henry
   Tarini, Marco
TI The inverse barycentric displacement problem
SO VISUAL COMPUTER
LA English
DT Article
DE Displacement mapping; Vertex projection; Inverse ray casting; Texture
   baking
AB In this short paper, we analyze the problem of finding the triangular barycentric coordinates of an interpolated ray hitting a given point. This task, which we term the inverse barycentric displacement problem, is general and useful in geometry processing and computer graphics. Concrete applications of the solution include the construction of displacement maps and texture baking. We derive the set of complete, closed-form solutions and discuss the number and existence of solutions. We close with a discussion of implementation-oriented optimizations and a few example applications.
C1 [Maggiordomo, Andrea; Tarini, Marco] Univ Milan, Milan, Italy.
   [Uralsky, Yury; Moreton, Henry] NVIDIA, Santa Clara, CA USA.
C3 University of Milan; Nvidia Corporation
RP Tarini, M (corresponding author), Univ Milan, Milan, Italy.
EM marco.tarini@unimi.it
OI Maggiordomo, Andrea/0000-0003-1759-5357; Moreton,
   Henry/0009-0008-3133-4229; Uralsky, Yury/0000-0001-7142-6998
FU Universita degli Studi di Milano
FX Open access funding provided by Universita degli Studi di Milano within
   the CRUI-CARE Agreement.
CR Adobe, 2021, BAK MESH MAPS SUBST
   [Anonymous], 2021, AUT HELP TRANSF MAPS
   Blender Foundation, 2022, REND BAK BLEND 3 3 R
   Burley B, 2008, COMPUT GRAPH FORUM, V27, P1155, DOI 10.1111/j.1467-8659.2008.01253.x
   Cignoni P, 1999, VISUAL COMPUT, V15, P519, DOI 10.1007/s003710050197
   Cohen J, 1997, VISUALIZATION '97 - PROCEEDINGS, P395, DOI 10.1109/VISUAL.1997.663908
   Guidi G, 2016, ACM J COMPUT CULT HE, V9, DOI 10.1145/2843947
   Jiang Z., 2020, ACM T GRAPHIC, V39
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   Maggiordomo A, 2020, COMPUT AIDED GEOM D, V83, DOI 10.1016/j.cagd.2020.101943
   Marmoset, 2022, BAK TOOLB
   Panozzo D, 2011, IEEE T VIS COMPUT GR, V17, P1510, DOI 10.1109/TVCG.2011.28
   Skala V, 2008, COMPUT GRAPH-UK, V32, P120, DOI 10.1016/j.cag.2007.09.007
   Tarini M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P457, DOI 10.1109/VISUAL.2003.1250407
NR 15
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2081
EP 2088
DI 10.1007/s00371-023-02903-0
EA JUN 2023
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001009127800001
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Tian, ZY
   Weng, DD
   Fang, H
   Shen, T
   Zhang, W
AF Tian, Zeyu
   Weng, Dongdong
   Fang, Hui
   Shen, Tong
   Zhang, Wei
TI Robust facial marker tracking based on a synthetic analysis of optical
   flows and the YOLO network
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Head-mounted cameras; Long-term tracking
ID IMAGE; EXPRESSION; DENSE
AB Current marker-based facial motion capture methods might lose the target markers in some cases, such as those with considerable occlusion and blur. Manually revising these statuses requires extensive labor-intensive work. Thus, a robust marker tracking method that provides long-term stability must be developed, thereby simplifying manual operations. In this paper, we present a new facial marker tracking system that focuses on the accuracy and stability of performance capture. The tracking system includes a synthetic analysis step with the robust optical flow tracking method and the proposed Marker-YOLO detector. To illustrate the strength of our system, a real dataset of the performance of voluntary actors was obtained, and ground truth labels were given by artists for subsequent experiments. The results showed that our approach outperforms state-of-the-art trackers such as SiamDW and ECO in specific tasks while running at a real-time speed of 38 fps. The root-mean-squared error and area under the curve results verified the improvements in the accuracy and stability of our approach.
C1 [Tian, Zeyu; Weng, Dongdong; Fang, Hui] Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100081, Peoples R China.
   [Shen, Tong; Zhang, Wei] JD Tech, JD AI Res, Beijing, Peoples R China.
C3 Beijing Institute of Technology
RP Weng, DD (corresponding author), Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100081, Peoples R China.
EM tianty97@163.com; crgj@bit.edu.cn; 510756678@qq.com;
   then.st@outlook.com; wzhang.cu@gmail.com
RI Zhang, Wei/IVH-1676-2023
FU National Key Research and Development Program of China [2022YFF0902303];
   Beijing Municipal Science & Technology Commission; Administrative
   Commission of Zhongguancun Science Park [Z221100007722002]; National
   Natural Science Foundation of China [62072036]; 111 Project [B18005]
FX This work was supported by the National Key Research and Development
   Program of China (No.2022YFF0902303) and the Beijing Municipal Science &
   Technology Commission and Administrative Commission of Zhongguancun
   Science Park (Z221100007722002) and the National Natural Science
   Foundation of China (No. 62072036) and the 111 Project (B18005).
CR Arriaga O., 2017, ARXIV
   Barrielle V, 2019, COMPUT GRAPH FORUM, V38, P151, DOI 10.1111/cgf.13450
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat KS, 2013, P 12 ACM SIGGRAPH EU, P7
   Bickel B., 2008, Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA '08), P57
   Bickel B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239484
   Blache L, 2016, VISUAL COMPUT, V32, P205, DOI 10.1007/s00371-015-1191-y
   BORSHUKOV G., 2006, SIGGRAPH '06: ACM SIGGRAPH Courses, P8, DOI [10.1145/1185657.1185848, DOI 10.1145/1185657.1185848]
   Bouguet, 2001, INTEL CORP, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Bregler C., 2009, SIGGRAPH 2009 TALKS, DOI [10.1145/1597990.1598019, DOI 10.1145/1597990.1598019]
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Choe B, 2001, J VISUAL COMP ANIMAT, V12, P67, DOI 10.1002/vis.246
   Chuang E, 2002, Comput Sci Tech Rep, Stanf Univ, V2, P3
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   DYNAMIXYZ, HMC GRABB
   Ekman P, 2002, FACIAL ACTION CODING
   Fang XY, 2014, VISUAL COMPUT, V30, P139, DOI 10.1007/s00371-013-0790-8
   Guenter B., 2006, ACM SIGGRAPH, P18, DOI [10.1145/1185657.1185858, DOI 10.1145/1185657.1185858]]
   Guo Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10819, DOI 10.1109/ICCV48922.2021.01066
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang HD, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964969
   IMAGE METRICS, LIV DRIV
   Jin-xiang Chai, 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P193
   JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710
   Kim Y.H., 2004, P BRIT MACH VIS C, P91, DOI [10.5244/C.18.91, DOI 10.5244/C.18.91]
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Lin IC, 2005, VISUAL COMPUT, V21, P355, DOI 10.1007/s00371-005-0291-5
   LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346
   Liu SK, 2013, VISUAL COMPUT, V29, P1135, DOI 10.1007/s00371-012-0756-2
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   Luo L, 2023, VISUAL COMPUT, V39, P5869, DOI 10.1007/s00371-022-02700-1
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Moiza G, 2002, VISUAL COMPUT, V18, P445, DOI 10.1007/s003710100157
   Moser L, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214755
   Moser L, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085086
   Nusseck M, 2008, J VISION, V8, DOI 10.1167/8.8.1
   Qu ZY, 2023, VISUAL COMPUT, V39, P625, DOI 10.1007/s00371-021-02362-5
   R3dS. WRAP4D, WRAPAD
   Ravikumar S., 2016, P GRAPHICS INTERFACE, P143, DOI [10.20380/GI2016.18, DOI 10.20380/GI2016.18]
   Saragih Jason M, 2011, Proc Int Conf Autom Face Gesture Recognit, P117, DOI 10.1109/FG.2011.5771400
   Senst T, 2016, IEEE IMAGE PROC, P4478, DOI 10.1109/ICIP.2016.7533207
   Senst T, 2012, IEEE T CIRC SYST VID, V22, P1377, DOI 10.1109/TCSVT.2012.2202070
   Tianye Li, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3130800.3130813
   Vicon Motion Systems Ltd. CaraPost, US
   Vihlman M, 2020, AAAI CONF ARTIF INTE, V34, P12112
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Williams L., 1990, Proceedings of SIGGRAPH, V24, P235
   Wu CL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925882
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 62
TC 1
Z9 1
U1 4
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2471
EP 2489
DI 10.1007/s00371-023-02931-w
EA JUN 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001003121500001
DA 2024-07-18
ER

PT J
AU Yang, WY
   Chen, XD
   Wang, H
   Mao, XY
AF Yang, Wenya
   Chen, Xiao-Diao
   Wang, Hui
   Mao, Xiaoyang
TI Edge detection using multi-scale closest neighbor operator and grid
   partition
SO VISUAL COMPUTER
LA English
DT Article
DE Edge detection; Image processing; Multi-scale; De-nosing; Ellipse
   fitting
ID BOUNDARIES; EXTRACTION
AB Edge detection is one of the most fundamental and critical operations in image analysis and computer vision. This paper proposes an adaptive edge detection method that combines multi-scale closest neighbor operator with grid partition technique (MSCNOGP). The multi-scale closest neighbor operator can be used to remove both noisy data and small area textures, while the grid partition technique can improve the precision of the edges. By utilizing the concepts of both twin edge pixel and grid divergence, the resulting edges can be further improved. Compared with prevailing traditional methods, the MSCNOGP method achieves both the best precision and almost the best visual effect, where both line segment fitting and ellipse fitting are applied for testing different edge detection methods. The performance on the F-measure score of the MSCNOGP method is much better than those of prevailing traditional methods.
C1 [Yang, Wenya; Chen, Xiao-Diao; Wang, Hui] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Dept Comp Sci & Engn, Kofu 4008511, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Yang, WY; Chen, XD (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.; Mao, XY (corresponding author), Univ Yamanashi, Dept Comp Sci & Engn, Kofu 4008511, Japan.
EM yangwenya@hdu.edu.cn; xiaodiao@hdu.edu.cn; 277179337@qq.com;
   mao@yamanashi.ac.jp
RI Mao, Xiaoyang/AAG-1294-2020; 杨, 文雅/GSE-4273-2022
OI Mao, Xiaoyang/0000-0001-5010-6952; 
FU National Science Foundation of China [61972120]
FX This research work was partially supported by the National Science
   Foundation of China (61972120).
CR Aboutabit N, 2021, VISUAL COMPUT, V37, P1545, DOI 10.1007/s00371-020-01896-4
   Akinlar C, 2015, 2015 INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA) PROCEEDINGS, P432
   Akinlar C, 2016, J VIS COMMUN IMAGE R, V36, P159, DOI 10.1016/j.jvcir.2016.01.017
   Akinlar C, 2012, INT J PATTERN RECOGN, V26, DOI 10.1142/S0218001412550026
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bastan M, 2017, IET IMAGE PROCESS, V11, P1325, DOI 10.1049/iet-ipr.2017.0336
   Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492
   Chung IF, 2018, IEEE T FUZZY SYST, V26, P734, DOI 10.1109/TFUZZ.2017.2688358
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Desolneux A, 2001, J MATH IMAGING VIS, V14, P271, DOI 10.1023/A:1011290230196
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Fang M, 2009, ISIP: 2009 INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING, PROCEEDINGS, P109
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144
   Flores-Vidal PA, 2018, ADV INTELL SYST, V642, P58, DOI 10.1007/978-3-319-66824-6_6
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huan LX, 2022, IEEE T PATTERN ANAL, V44, P6602, DOI 10.1109/TPAMI.2021.3084197
   Kittler J., 1983, Image and Vision Computing, V1, P37, DOI [DOI 10.1016/0262-8856(83)90006-9, 10.1016/0262-8856(83)90006-9]
   Kumawat A, 2022, VISUAL COMPUT, V38, P3681, DOI 10.1007/s00371-021-02196-1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P5206, DOI 10.1109/TIP.2020.2980170
   Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P864
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lu JW, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 2, P289
   Mafi M, 2018, IEEE T IMAGE PROCESS, V27, P5475, DOI 10.1109/TIP.2018.2857448
   Maire M, 2008, PROC CVPR IEEE, P611
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Meng C, 2020, IEEE T IMAGE PROCESS, V29, P4406, DOI 10.1109/TIP.2020.2967601
   Modava M, 2017, INT J REMOTE SENS, V38, P355, DOI 10.1080/01431161.2016.1266104
   Ofir N, 2020, IEEE T PATTERN ANAL, V42, P894, DOI 10.1109/TPAMI.2019.2892134
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Ren X, 2012, ADV NEURAL INFORM PR, P584, DOI DOI 10.1634/THEONCOLOGIST.8-3-252
   Renjie Song, 2017, Pattern Recognition and Image Analysis, V27, P740
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Topal C, 2012, J VIS COMMUN IMAGE R, V23, P862, DOI 10.1016/j.jvcir.2012.05.004
   TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769
   Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P1285, DOI 10.1109/TIP.2018.2874279
   Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187
   Wu W, 2022, COMPUT VIS IMAGE UND, V216, DOI 10.1016/j.cviu.2021.103341
   Wu W, 2022, VISUAL COMPUT, V38, P1665, DOI 10.1007/s00371-021-02095-5
   Wu W, 2022, VISUAL COMPUT, V38, P1677, DOI 10.1007/s00371-021-02096-4
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Yuan-Kai Huo, 2010, 2010 International Conference on Image Analysis and Signal Processing (IASP 2010), P371, DOI 10.1109/IASP.2010.5476095
   Zhang ZM, 2018, IEEE T PATTERN ANAL, V40, P1209, DOI 10.1109/TPAMI.2017.2707492
NR 64
TC 2
Z9 2
U1 3
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1947
EP 1964
DI 10.1007/s00371-023-02894-y
EA JUN 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001000747800001
DA 2024-07-18
ER

PT J
AU He, B
   Qian, SS
   Niu, YC
AF He, Bin
   Qian, Shusheng
   Niu, Yongchao
TI Visual recognition and location algorithm based on optimized YOLOv3
   detector and RGB depth camera
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Improved YOLOv3; Tomato detection; Location information
ID APPLE DETECTION; IMAGES
AB Fruit recognition and location are the premises of robot automatic picking. YOLOv3 has been used to detect different fruits in complex environment. However, for the object with definite features, the complex network structure will increase the computing time and may cause overfitting. Therefore, this paper has carried out a lightweight design for the YOLOv3. This paper proposed an improved T-Net to detect tomato images. Firstly, the T-Net reduces the residual network layers. This paper changed the number of cycles in each group of the residual unit to 1, 2, 2, 1, and 1. Second, two feature layers with different scales are selected according to the features of tomatoes. Meanwhile, the convolutional layer at the neck has been reduced by two layers. Finally, the location and approximate diameter of the ripe tomato are obtained by combining the node information of the Intel D435i camera and T-Net in the Robot Operation System. T-Net obtains mean average precision (mAP) of 99.2%, F-1-score of 98.9%, precision of 99.0%, and recall of 98.8% at a detection rate of 104.2 FPS. The proposed T-Net has outperformed the YOLOv3 with 0.4%, 0.1%, and 0.2% increase in precision, mAP, and F-1-score. The detection speed of T-Net is 1.8 times faster than YOLOv3. The mean errors of the center coordinates and diameter of the tomato are 8.5 mm and 2.5 mm, respectively. This model provides a method for efficient real-time detection and location of tomatoes.
C1 [He, Bin; Qian, Shusheng; Niu, Yongchao] Shanghai Univ, Sch Mechatron Engn & Automation, Shanghai Key Lab Intelligent Mfg & Robot, 99 Shangda Rd, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP He, B (corresponding author), Shanghai Univ, Sch Mechatron Engn & Automation, Shanghai Key Lab Intelligent Mfg & Robot, 99 Shangda Rd, Shanghai 200444, Peoples R China.
EM mehebin@gmail.com
FU National Natural Science Foundation of China [52075312]
FX AcknowledgementsThe work was supported by National Natural Science
   Foundation of China (No. 52075312)
CR Afonso M, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.571299
   An GH, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7120421
   Andriyanov N, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14010148
   Buyukarikan B, 2022, NEURAL COMPUT APPL, V34, P16973, DOI 10.1007/s00521-022-07350-x
   Chandio A., 2022, ARXIV
   Chang YH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072371
   Chen JQ, 2021, J FOOD PROCESS ENG, V44, DOI 10.1111/jfpe.13803
   Fu GH, 2015, IEEE ASME INT C ADV, P1789, DOI 10.1109/AIM.2015.7222806
   Gai RL, 2023, NEURAL COMPUT APPL, V35, P13895, DOI 10.1007/s00521-021-06029-z
   Gené-Mola J, 2020, DATA BRIEF, V30, DOI 10.1016/j.dib.2020.105591
   Hsieh KW, 2021, J FOOD MEAS CHARACT, V15, P5170, DOI 10.1007/s11694-021-01074-7
   Ju MR, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183775
   Junos MH, 2022, VISUAL COMPUT, V38, P2341, DOI 10.1007/s00371-021-02116-3
   Lawal MO, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81216-5
   Linker R, 2018, BIOSYST ENG, V167, P114, DOI 10.1016/j.biosystemseng.2018.01.003
   Liu GX, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20072145
   Liu SS, 2019, INT J INTELL COMPUT, V12, P318, DOI 10.1108/IJICC-12-2018-0167
   Moreira G, 2022, AGRONOMY-BASEL, V12, DOI 10.3390/agronomy12020356
   Muslikhin, 2020, IEEE ACCESS, V8, P121765, DOI 10.1109/ACCESS.2020.3006843
   Niu Y.C., 2020, RES FLEXIBLE PRECISI
   Pan SY, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22114187
   Perez-Borrero I, 2021, NEURAL COMPUT APPL, V33, P15059, DOI 10.1007/s00521-021-06131-2
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy AM, 2023, ECOL INFORM, V75, DOI 10.1016/j.ecoinf.2022.101919
   Roy AM, 2022, COMPUT ELECTRON AGR, V193, DOI 10.1016/j.compag.2022.106694
   Roy AM, 2022, NEURAL COMPUT APPL, V34, P3895, DOI 10.1007/s00521-021-06651-x
   Septiarini A, 2021, SCI HORTIC-AMSTERDAM, V286, DOI 10.1016/j.scienta.2021.110245
   Shafiee M.J., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1709.05943
   Singh P, 2021, WIREL NETW, V27, P1999, DOI 10.1007/s11276-021-02557-7
   Thenmozhi K, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104906
   Tian YN, 2019, COMPUT ELECTRON AGR, V157, P417, DOI 10.1016/j.compag.2019.01.012
   Tsoulias N, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152481
   Uramoto S, 2021, J SIGNAL PROCESS, V25, P151, DOI [10.2299/jsp.25.151, DOI 10.2299/JSP.25.151]
   Wang XW, 2021, FRONT PLANT SCI, V12, DOI 10.3389/fpls.2021.634103
   Wu DP, 2016, IEEE ACCESS, V4, P7251, DOI 10.1109/ACCESS.2016.2611820
   Yoshida T, 2022, ROBOMECH J, V9, DOI 10.1186/s40648-022-00230-y
   Zaghari N, 2021, J SUPERCOMPUT, V77, P13421, DOI 10.1007/s11227-021-03813-5
   Zhang FJ, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2022.107582
   Zhang YM, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14051063
   Zheng TX, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107029
   Zhu CW, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12063088
NR 42
TC 2
Z9 2
U1 10
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1965
EP 1981
DI 10.1007/s00371-023-02895-x
EA JUN 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000999829400003
DA 2024-07-18
ER

PT J
AU Ehsan, TZ
   Nahvi, M
   Mohtavipour, SM
AF Ehsan, Tahereh Zarrat
   Nahvi, Manoochehr
   Mohtavipour, Seyed Mehdi
TI An accurate violence detection framework using unsupervised
   spatial-temporal action translation network
SO VISUAL COMPUTER
LA English
DT Article
DE Generative adversarial network; Unsupervised learning; Action
   recognition; Violence detection
ID RECOGNITION
AB Automatic human behavior monitoring is essential for surveillance cameras in public and private environments. Violent action is challenging because the available violence dataset is insufficient for deep network training. Also, human behavior contains high intra-class variations and inter-class similarities that make violence detection very challenging. In this paper, we proposed an unsupervised Spatial-Temporal Action Translation (STAT) network to accurately distinguish between behaviors and overcome the insufficient violence data problem. Our framework comprises a person detector, motion feature extractor, STAT network, and output interpretation. The proposed framework performed well in different environments because it detects objects in each frame and removes irrelevant background information. As violent motion pattern changes rapidly with high velocity, temporal features play a crucial role in the recognition, and we use it as the input of the STAT network. The STAT network has been trained with normal behavior data, translating normal motion to the spatial frame. Due to the complicated actions in violent behavior, the STAT network cannot reconstruct the violent frame correctly, and therefore, actions will be categorized by comparing the actual and reconstructed frames and measuring the reconstruction error in the output interpretation part of the framework. The proposed unsupervised framework achieved comparable accuracy and outperformed previous works in terms of generality.
C1 [Ehsan, Tahereh Zarrat; Nahvi, Manoochehr] Univ Guilan, Sch Elect Engn, Rasht, Iran.
   [Mohtavipour, Seyed Mehdi] Iran Univ Sci & Technol, Sch Elect Engn, Tehran, Iran.
C3 University of Guilan; Iran University Science & Technology
RP Nahvi, M (corresponding author), Univ Guilan, Sch Elect Engn, Rasht, Iran.
EM tahere_zarrat@msc.guilan.ac.ir; nahvi@guilan.ac.ir;
   mehdi_mohtavipour@elec.iust.ac.ir
RI Mohtavipour, Seyed Mehdi/AAV-3668-2020
OI Mohtavipour, Seyed Mehdi/0000-0003-2749-8980
CR Accattoli S, 2020, APPL ARTIF INTELL, V34, P329, DOI 10.1080/08839514.2020.1723876
   Asad M, 2021, VISUAL COMPUT, V37, P1415, DOI 10.1007/s00371-020-01878-6
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Ben Mabrouk A, 2017, PATTERN RECOGN LETT, V92, P62, DOI 10.1016/j.patrec.2017.04.015
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Buckchash H, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114916
   Chang YP, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108213
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deepak K, 2021, SIGNAL IMAGE VIDEO P, V15, P215, DOI 10.1007/s11760-020-01740-1
   Dong ZH, 2016, COMM COM INF SC, V662, P517, DOI 10.1007/978-981-10-3002-4_43
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ehsan Tahereh Zarrat, 2020, 2020 11th International Conference on Information and Knowledge Technology (IKT), P88, DOI 10.1109/IKT51791.2020.9345617
   Ehsan T.Z., 2022, P IEEE INT C MACHINE, P1
   Ehsan TZ, 2018, 2018 8TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P153, DOI 10.1109/ICCKE.2018.8566460
   Ehsan TZ., 2022, MULTIMED TOOLS APPL, V82, P1
   Ertl A, 2019, MMWR SURVEILL SUMM, V68, P1, DOI 10.15585/mmwr.ss.6809a1
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Farooq MU, 2022, VISUAL COMPUT, V38, P1553, DOI 10.1007/s00371-021-02088-4
   Fernandez-Ramírez J, 2020, VISUAL COMPUT, V36, P1535, DOI 10.1007/s00371-019-01754-y
   Foo GT, 2019, INTELL DECIS TECHNOL, V13, P49, DOI 10.3233/IDT-190360
   Gao Y, 2016, IMAGE VISION COMPUT, V48-49, P37, DOI 10.1016/j.imavis.2016.01.006
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hao Y, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108232
   Hassner T., 2012, 2012 IEEE COMP SOC C, P1, DOI [DOI 10.1109/CVPRW.2012.6239348, 10.1109/CVPRW.2012.6239348]
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu X, 2020, NEUROCOMPUTING, V383, P270, DOI 10.1016/j.neucom.2019.11.087
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jegham I, 2020, FORENS SCI INT-DIGIT, V32, DOI 10.1016/j.fsidi.2019.200901
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li CY, 2018, PROCEEDINGS OF 2018 VII INTERNATIONAL CONFERENCE ON NETWORK, COMMUNICATION AND COMPUTING (ICNCC 2018), P227, DOI 10.1145/3301326.3301367
   LI D, 2021, IEEE T EM TOP COMP I, V38, P1
   Li HC, 2020, MEAS CONTROL-UK, V53, P796, DOI 10.1177/0020294020902788
   Lucas B. D., 1981, P 7 INT JOINT C ART, V81, P674, DOI DOI 10.5555/1623264.1623280
   Mohtavipour SM, 2022, VISUAL COMPUT, V38, P2057, DOI 10.1007/s00371-021-02266-4
   Pareek P, 2021, ARTIF INTELL REV, V54, P2259, DOI 10.1007/s10462-020-09904-8
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rendón-Segador FJ, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10131601
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samuel D.J., 2021, Svd-gan for real-time unsupervised video anomaly detection
   Sernani P, 2021, IEEE ACCESS, V9, P160580, DOI 10.1109/ACCESS.2021.3131315
   Gracia IS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120448
   Serrano I, 2018, IEEE T IMAGE PROCESS, V27, P4787, DOI 10.1109/TIP.2018.2845742
   Soliman Mohamed Mostafa, 2019, 2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS), P80, DOI 10.1109/ICICIS46948.2019.9014714
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sun JY, 2018, IEEE ACCESS, V6, P33353, DOI 10.1109/ACCESS.2018.2848210
   Ullah FUM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19112472
   Yu J, 2019, MULTIMED TOOLS APPL, V78, P8497, DOI 10.1007/s11042-018-6923-3
   Zhang T, 2018, PATTERN RECOGN LETT, V107, P98, DOI 10.1016/j.patrec.2017.08.021
   Zhou PP, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0203668
   Zhou W, 2019, VISUAL COMPUT, V35, P489, DOI 10.1007/s00371-018-1478-x
NR 51
TC 4
Z9 4
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1515
EP 1535
DI 10.1007/s00371-023-02865-3
EA MAY 2023
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000980819600001
DA 2024-07-18
ER

PT J
AU Yu, XM
   Zhou, G
AF Yu, Xiaoming
   Zhou, Gan
TI Arbitrary style transfer via content consistency and style consistency
SO VISUAL COMPUTER
LA English
DT Article
DE Arbitrary style transfer; Content consistency; Style consistency;
   Contrastive learning; Covariance Attention Network
AB Arbitrary style transfer is an interesting and challenging technique. In the testing phase, it needs to adaptively fuse content images and style images not seen in the training phase. Users judge the model performance by the image quality alone. Simply put, the image quality is mainly concerned with two points, content consistency and style consistency. In this paper, we propose two contrastive losses to preserve content and fuse style. It is worth noting that our style consistency focuses on the fact that the style between different patches in the generated image is consistent and cannot have the style of the original content image patches, but this does not help the style of the generated images and the reference images to be consistent. Therefore, a simple and effective feature fusion method, Covariance Attention Network (CovAttN), is designed to encourage the generated image style to gradually approach the reference image style. It aligns content features and style features from both channel-wise correlation and spatial distribution to form a two-stage style injection. Compared with state-of-the-art models and ablation experiments demonstrate the superiority of our proposed method on arbitrary style transfer.
C1 [Yu, Xiaoming; Zhou, Gan] Natl Comp Syst Engn Res Inst China, Beijing 100083, Peoples R China.
RP Yu, XM (corresponding author), Natl Comp Syst Engn Res Inst China, Beijing 100083, Peoples R China.
EM xyuforart@gmail.com; zhouganterry@163.com
CR Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen H., 2021, ADV NEURAL INF PROCE, V34, P26561
   Chen T., 2016, ARXIV
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cheng K, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555399
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jeong J., 2020, INT C LEARNING REPRE
   Jing YC, 2020, AAAI CONF ARTIF INTE, V34, P4369
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Lee H-S, 2021, ARXIV
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Li YT, 2017, ADV NEUR IN, V30
   Lin TW, 2021, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR46437.2021.00510
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SG, 2022, IEEE T MULTIMEDIA, V24, P1299, DOI 10.1109/TMM.2021.3063605
   Ma ZQ, 2023, IEEE T NEUR NET LEAR, V34, P7404, DOI 10.1109/TNNLS.2022.3143356
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Phillips F, 2011, ISS ACCOUNT EDUC, V26, P593, DOI 10.2308/iace-50038
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Ulyanov Dmitry, 2016, arXiv
   Wu ZJ, 2022, LECT NOTES COMPUT SC, V13676, P189, DOI 10.1007/978-3-031-19787-1_11
   Xu WJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6363, DOI 10.1109/ICCV48922.2021.00632
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang Hongguang, 2018, EUROPEAN C COMPUTER
   Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590
   Zhang MJ, 2019, IEEE T NEUR NET LEAR, V30, P3109, DOI 10.1109/TNNLS.2018.2890017
   Zhang MJ, 2018, IEEE T CYBERNETICS, V48, P904, DOI 10.1109/TCYB.2017.2664499
   Zhizhong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7786, DOI 10.1109/CVPR42600.2020.00781
NR 38
TC 2
Z9 2
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1369
EP 1382
DI 10.1007/s00371-023-02855-5
EA APR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000979483600002
DA 2024-07-18
ER

PT J
AU Umezawa, W
   Mukai, T
AF Umezawa, Wataru
   Mukai, Tomohiko
TI Procedural modeling of artificially cultivated shrub roses
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Procedural modeling; Plant modeling; Artificial pruning; Shrub rose
AB Decorative plants require skillful pruning by gardeners. However, conventional plant modeling techniques have focused only on the biological growth rules of wild plants and do not yet consider artificial care. In this study, an interactive system was developed for modeling the branching structure of artificially cultivated plants, particularly shrub roses. The structural parameters are designed to reflect the intentions of gardeners and differences between rose varieties. The proposed graphical controllers enable users to intuitively manipulate the tree shape, considering biological growth rules and artificial pruning. The proposed system can be used to efficiently generate the ideal shape and appearance of several types of shrub roses during the blooming season.
C1 [Umezawa, Wataru; Mukai, Tomohiko] Tokyo Metropolitan Univ, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
C3 Tokyo Metropolitan University
RP Mukai, T (corresponding author), Tokyo Metropolitan Univ, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
EM tmki@acm.org
OI Mukai, Tomohiko/0000-0002-7965-5426
CR [Anonymous], 1996, P 2 CSIRO S COMPUTAT
   Boudon F, 2003, COMPUT GRAPH FORUM, V22, P591, DOI 10.1111/1467-8659.t01-2-00707
   Guo JW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3394105
   Hart JC, 2003, VISUAL COMPUT, V19, P151, DOI 10.1007/s00371-002-0189-4
   Ijiri T, 2005, ACM T GRAPHIC, V24, P720, DOI 10.1145/1073204.1073253
   Ijiri T., 2008, P GRAPHICS INTERFACE, P227
   Ijiri T, 2006, LECT NOTES COMPUT SC, V4073, P138
   Kim YJ, 2015, COMPUT ANIMAT VIRT W, V26, P423, DOI 10.1002/cav.1661
   Li JF, 2015, COMPUT ANIMAT VIRT W, V26, P433, DOI 10.1002/cav.1647
   Owens A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925982
   Palubicki W, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531364
   Pirk S., 2016, ACM SIGGRAPH 2016 CO
   Pirk S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185546
   Prusinkiewicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P351, DOI 10.1145/192161.192254
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Runions A., 2007, NPH, V7, P6, DOI [DOI 10.2312/NPH/NPH07/063-070, 10.2312/NPH/NPH07/063-070]
   Titchmarsh A., 2011, BBC PHYS AUDIO
   Wataru U., 2022, CYBERWORLDS 2022, P31
   Wesslén D, 2005, VISUAL COMPUT, V21, P397, DOI 10.1007/s00371-005-0295-1
   Xu L, 2012, COMPUT GRAPH-UK, V36, P1036, DOI 10.1016/j.cag.2012.08.005
   Zeng LL, 2009, PROG NAT SCI-MATER, V19, P255, DOI 10.1016/j.pnsc.2008.07.004
NR 21
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 APR 11
PY 2023
DI 10.1007/s00371-023-02848-4
EA APR 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D3QG0
UT WOS:000967899600003
DA 2024-07-18
ER

PT J
AU Liu, QP
   Song, Y
   Tang, Q
   Bu, XH
   Hanajima, N
AF Liu, Qunpo
   Song, Yang
   Tang, Qi
   Bu, Xuhui
   Hanajima, Naohiko
TI Wire rope defect identification based on ISCM-LBP and GLCM features
SO VISUAL COMPUTER
LA English
DT Article
DE Wire rope; Local binary pattern; Feature fusion; Support vector machine
AB The traditional local binary pattern (LBP) is susceptible to the influence of the centre pixel and noise and cannot accurately identify wire rope surface defects. To solve this problem, an image segmentation-based central multiscale local binary pattern (ISCM-LBP) and grey level cooccurrence matrix (GLCM) feature fusion method is proposed in this paper for defect recognition. Image segmentation and multiple scales are introduced into the local binary pattern algorithm to improve the image detail description and suppress noise sensitivity. Second, the centre pixel is connected with the neighbourhood pixel to enhance the robustness of the centre pixel. To further improve the image integrity description, PCA dimensionality reduction and GLCM feature fusion are performed on the features extracted by the ISCM-LBP algorithm, and the steel wire rope surface defects are identified by a support vector machine classifier. Experimental results show that the overall recognition rate reaches 97.5%, which is at least 5% higher than that of other algorithms and can effectively identify various defects on the surface of wire rope.
C1 [Liu, Qunpo; Bu, Xuhui] Henan Polytech Univ, Henan Int Joint Lab Direct Dr & Control Intellige, Sch Elect Engn & Automat, Jiaozuo, Peoples R China.
   [Song, Yang; Tang, Qi] Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Peoples R China.
   [Hanajima, Naohiko] Muroran Inst Technol, Coll Informat & Syst, 27-1 Mizumoto Cho, Muroran, Hokkaido 0508585, Japan.
C3 Henan Polytechnic University; Henan Polytechnic University; Muroran
   Institute of Technology
RP Song, Y (corresponding author), Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo, Peoples R China.
EM pangmeibai23@163.com
OI Hanajima, Naohiko/0000-0002-4707-853X
FU National Natural Science Foundation of China [U1804147]; Innovative
   Scientists and Technicians Team of Henan Provincial High Education
   [20IRTSTHN019]; Science and Technology Project of Henan Province
   [212102210508, 212102210005]
FX This study was partially supported by the National Natural Science
   Foundation of China (No.U1804147), Innovative Scientists and Technicians
   Team of Henan Provincial High Education (20IRTSTHN019), Science and
   Technology Project of Henan Province (No.212102210508 &
   No.212102210005).
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Chakraborti T, 2018, IEEE SIGNAL PROC LET, V25, P635, DOI 10.1109/LSP.2018.2817176
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Jabid T., 2010, 2010 20 INT C PATTER
   Jiang H., 2021, Research on surface defect detection method of steel wire rope based on IWOA-SVM
   Jiexian H., 2016, Surf. Technol., V45, P187
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Li Z., 2021, Intelligent elevator operation health evaluation and safety monitoring based onmachine learning
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828
   [刘丽 Liu Li], 2018, [自动化学报, Acta Automatica Sinica], V44, P584
   [刘丽 LIU li], 2009, [中国图象图形学报, Journal of Image and Graphics], V14, P622
   Liu Y, 2020, Research and application of online detection algorithm for surface defects of cold rolled strip
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan ZB, 2021, EXPERT SYST APPL, V180, DOI 10.1016/j.eswa.2021.115123
   Pietikäinen M, 2000, PATTERN RECOGN, V33, P43, DOI 10.1016/S0031-3203(99)00032-1
   Ruochen D., 2021, Adv. Lasers Optoelectron., V58, P566
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Xiaohua J., 2014, Autom. Ind. Mine, V40, P110
   Xin Z., 2018, Coal Eng., V50, P119
   Yonglei D., 2018, China Elev., V29, P10
   Yuan F., 2010, Mach. Des. Manuf., V02, P260
   Yuan F., 2010, Mach. Des. Manuf., V07, P104
   Zhang GY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165401
   Zhao J., 2014, Detection of broken wire of Oil well wire rope based on image processing
   Zhihuai L., 2018, J. Vib. Shock, V37, P271
   Zhou P, 2020, IEEE SENS J, V20, P8297, DOI 10.1109/JSEN.2020.2970070
   Zhou P, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9132771
NR 29
TC 3
Z9 3
U1 3
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 545
EP 557
DI 10.1007/s00371-023-02800-6
EA MAR 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000943920100001
DA 2024-07-18
ER

PT J
AU Guo, ZH
   Shao, MW
   Li, SH
AF Guo, Zihao
   Shao, Mingwen
   Li, Shunhang
TI Image-to-image translation using an offset-based multi-scale codes GAN
   encoder
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Generative adversarial networks; GAN inversion; Image-to-image
   translation; Super-resolution; Conditional face synthesis
AB Despite the remarkable achievements of generative adversarial networks (GANs) in high-quality image synthesis, applying pre-trained GAN models to image-to-image translation is still challenging. Previous approaches typically map the conditional image into the latent spaces of GANs by per-image optimization or learning a GAN encoder. However, neither of these two methods can ideally perform image-to-image translation tasks. In this work, we propose a novel learning-based framework which can complete common image-to-image translation tasks with high quality in real-time based on pre-trained GANs. Specifically, to mitigate the semantic misalignment between conditional and synthesized images, we propose an offset-based image synthesis method that allows our encoder to use multiple rather than one forward propagation to predict the latent codes. During the multiple forward passes, the final latent codes are adjusted continuously according to the semantic difference between the conditional image and the current synthesized image. To further reduce the loss of details during encoding, we extract multiple latent codes at multiple scales from input instead of a single code to synthesize the image. Moreover, we propose an optional multiple feature maps fusion module that combines our encoder with different generators to implement our multiple latent codes strategies. Finally, we analyze the performance and demonstrate the effectiveness of our framework by comparing it with state-of-the-art works on super-resolution and conditional face synthesis tasks.
C1 [Guo, Zihao; Shao, Mingwen; Li, Shunhang] China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
C3 China University of Petroleum
RP Shao, MW (corresponding author), China Univ Petr, Coll Comp Sci & Technol, Qingdao, Peoples R China.
EM guozh980422@163.com; smw278@126.com; z20070036@s.upc.edu.cn
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [61673396, 61976245];
   Natural Science Foundation of Shandong Province [ZR2022MF260]
FX AcknowledgementsThe authors are very indebted to the anonymous referees
   for their critical comments and suggestions for the improvement of this
   paper. This work was supported by National Key Research and development
   Program of China (2021YFA1000102), and in part by the grants from the
   National Natural Science Foundation of China (Nos. 61673396, 61976245),
   Natural Science Foundation of Shandong Province (No: ZR2022MF260).
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Creswell A, 2019, IEEE T NEUR NET LEAR, V30, P1967, DOI 10.1109/TNNLS.2018.2875194
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Fan Y, 2020, INT J MACH LEARN CYB, V11, P2077, DOI 10.1007/s13042-020-01098-3
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang HW, 2005, VISUAL COMPUT, V21, P821, DOI 10.1007/s00371-005-0328-9
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan JY, 2023, VISUAL COMPUT, V39, P6167, DOI 10.1007/s00371-022-02719-4
   Li LY, 2021, VISUAL COMPUT, V37, P2855, DOI 10.1007/s00371-021-02236-w
   Li LY, 2022, VISUAL COMPUT, V38, P3577, DOI 10.1007/s00371-021-02188-1
   Lin M, 2014, Arxiv, DOI [arXiv:1312.4400, DOI 10.48550/ARXIV.1312.4400]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ma FC, 2018, ADV NEUR IN, V31
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mohammadi P., 2014, SUBJECTIVE OBJECTIVE
   Reisfeld E, 2023, VISUAL COMPUT, V39, P2811, DOI 10.1007/s00371-022-02494-2
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464
   Shao MW, 2021, KNOWL-BASED SYST, V225, DOI 10.1016/j.knosys.2021.107122
   Shao MW, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107311
   Simo-Serra E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925972
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Song XX, 2020, SIGNAL IMAGE VIDEO P, V14, P1217, DOI 10.1007/s11760-020-01660-0
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wright L., 2019, Ranger-a synergistic optimizer
   Xia W., 2021, arXiv
   Xiu J, 2023, VISUAL COMPUT, V39, P5883, DOI 10.1007/s00371-022-02701-0
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 45
TC 6
Z9 6
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 MAR 4
PY 2023
DI 10.1007/s00371-023-02810-4
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9O5GK
UT WOS:000943629800002
DA 2024-07-18
ER

PT J
AU Poreddy, AKR
   Kara, PA
   Tamboli, RR
   Simon, A
   Appina, B
AF Poreddy, Ajay Kumar Reddy
   Kara, Peter A.
   Tamboli, Roopak R.
   Simon, Aniko
   Appina, Balasubramanyam
TI CoDIQE3D: A completely blind, no-reference stereoscopic image quality
   estimator using joint color and depth statistics
SO VISUAL COMPUTER
LA English
DT Article
DE 3D image quality; Objective quality assessment; S3D; IQA; NIQE; Color;
   Depth
ID CHROMATIC INFORMATION; PREDICTION; DISPARITY
AB In this paper, we present an unsupervised, completely blind, no-reference (NR) stereoscopic (S3D) image quality prediction model to assess the perceptual quality of natural S3D images. We study the joint dependencies between color and depth features of S3D images and empirically model these dependencies by using a bivariate generalized Gaussian distribution (BGGD). We compute the parameters of BGGD, and we also obtain the determinant and the coherence values from the covariance matrix of the proposed BGGD model. We extract the features of BGGD model and covariance matrix from the reference S3D image, followed by multivariate Gaussian (MVG) distribution modeling on the predicted features of the reference. We estimate the joint color and depth quality of the S3D images by computing the likelihood of the image features with respect to the reference MVG model. We apply the popular 2D unsupervised NIQE model on individual stereo views to estimate the overall spatial quality of the S3D images. Finally, we pool the likelihood scores and the spatial NIQE scores to achieve the estimation for the overall perceived quality of the S3D images. The performance of the proposed model is evaluated on the MICT, LIVE Phase I and II S3D image datasets. The results indicate consistent and robust performance for all datasets. Our proposed estimator is completely blind, as it requires neither training on subjective scores nor reference S3D images.
C1 [Poreddy, Ajay Kumar Reddy] Indian Inst Informat Technol Design & Mfg, Dept Elect & Commun Engn, Chennai, India.
   [Kara, Peter A.] Budapest Univ Technol & Econ, Dept Networked Syst & Serv, Budapest, Hungary.
   [Tamboli, Roopak R.] Indian Inst Technol Hyderabad, Dept Elect Engn, Kandi, India.
   [Simon, Aniko] Sigma Technol, Budapest, Hungary.
   [Appina, Balasubramanyam] Indian Inst Technol Indore, Dept Elect Engn, Simrol, Madhya Pradesh, India.
C3 Indian Institute of Information Technology, Design & Manufacturing,
   Kancheepuram; Budapest University of Technology & Economics; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Hyderabad; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Indore
RP Appina, B (corresponding author), Indian Inst Technol Indore, Dept Elect Engn, Simrol, Madhya Pradesh, India.
EM appina@iiti.ac.in
RI APPINA, BALASUBRAMANYAM/A-2941-2019
OI Simon, Aniko/0000-0001-5114-0298; Poreddy, Ajay kumar
   Reddy/0000-0002-2920-9997; Appina, Balasubramanyam/0000-0003-4751-2866
FU Department of Science and Technology - Science and Engineering Research
   Board, Government of India [SRG/2020/000336]; Ministry of Innovation and
   Technology of Hungary from the National Research, Development and
   Innovation Fund [BME-NVA-02]
FX The research reported in this paper was supported in part by the
   Department of Science and Technology - Science and Engineering Research
   Board, Government of India under Grant SRG/2020/000336. The work was
   also supported by project no. BME-NVA-02, implemented with the support
   provided by the Ministry of Innovation and Technology of Hungary from
   the National Research, Development and Innovation Fund, financed under
   the TKP2021 funding scheme.
CR Akhter R., 2010, IS TSPIE ELECT IMAGI, P271
   [Anonymous], 2003, VQEG FINAL REPORT VI
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Appina Balasubramanyam, 2020, 2020 INT C SIGN PROC, P1
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Bensalma R, 2010, IEEE IMAGE PROC, P4037, DOI 10.1109/ICIP.2010.5649390
   Cai R., 2022, VISUAL COMPUT, P1
   Campisi Patrizio, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P2110
   Che-Chun Su, 2012, Proceedings of the 2012 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI 2012), P169, DOI 10.1109/SSIAI.2012.6202480
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   den Ouden HEM, 2005, J PHYSIOL-LONDON, V567, P665, DOI 10.1113/jphysiol.2005.089516
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Fine I, 2003, J OPT SOC AM A, V20, P1283, DOI 10.1364/JOSAA.20.001283
   Gorley Paul, 2008, Proceedings of the SPIE - The International Society for Optical Engineering, V6803, P680305, DOI 10.1117/12.763530
   Ji JY, 2023, VISUAL COMPUT, V39, P443, DOI 10.1007/s00371-021-02340-x
   Jiang QP, 2020, IEEE T INSTRUM MEAS, V69, P9784, DOI 10.1109/TIM.2020.3005111
   JORDAN JR, 1992, PATTERN RECOGN, V25, P367, DOI 10.1016/0031-3203(92)90086-X
   JORDAN JR, 1991, CVGIP-IMAG UNDERSTAN, V54, P98, DOI 10.1016/1049-9660(91)90077-3
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   Jung YJ, 2013, IEEE T CIRC SYST VID, V23, P2077, DOI 10.1109/TCSVT.2013.2270394
   Khan S, 2018, IEEE T IMAGE PROCESS, V27, P5892, DOI 10.1109/TIP.2018.2860279
   Khan S, 2016, CONF REC ASILOMAR C, P1858, DOI 10.1109/ACSSC.2016.7869706
   Liu Y, 2020, IEEE ACCESS, V8, P33666, DOI 10.1109/ACCESS.2020.2974006
   Md SK, 2015, IEEE SIGNAL PROC LET, V22, P1985, DOI 10.1109/LSP.2015.2449878
   Ming-Jun Chen, 2011, 2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis, P24, DOI 10.1109/IVMSPW.2011.5970349
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Nasr S, 2016, J NEUROSCI, V36, P1841, DOI 10.1523/JNEUROSCI.3518-15.2016
   Palm C, 2000, PROCEEDINGS OF THE FIFTH JOINT CONFERENCE ON INFORMATION SCIENCES, VOLS 1 AND 2, pA45
   Pascal F, 2013, IEEE T SIGNAL PROCES, V61, P5960, DOI 10.1109/TSP.2013.2282909
   Qiuping Jiang, 2014, Journal of Software, V9, P1841, DOI 10.4304/jsw.9.7.1841-1847
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Sazzad ZMP, 2009, INT WORK QUAL MULTIM, P180, DOI 10.1109/QOMEX.2009.5246956
   Shao F, 2015, IEEE T BROADCAST, V61, P154, DOI 10.1109/TBC.2015.2402491
   Shi YQ, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107168
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Stoll Julia, 2020, NUMB DIG 3D CIN SCRE
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Su CC, 2014, PROC SPIE, V9014, DOI 10.1117/12.2036505
   Su CC, 2013, IEEE T IMAGE PROCESS, V22, P2259, DOI 10.1109/TIP.2013.2249075
   Su CC, 2011, IEEE IMAGE PROC, P257, DOI 10.1109/ICIP.2011.6116191
   Testolina P., 2022, ARXIV
   Ts'o DY, 2001, VISION RES, V41, P1333, DOI 10.1016/S0042-6989(01)00076-1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yildiz ZC, 2020, VISUAL COMPUT, V36, P127, DOI 10.1007/s00371-018-1592-9
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhou WJ, 2017, PATTERN RECOGN, V71, P207, DOI 10.1016/j.patcog.2017.06.008
NR 54
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6743
EP 6753
DI 10.1007/s00371-022-02760-3
EA JAN 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000907765600003
DA 2024-07-18
ER

PT J
AU Tan, AL
   Liao, HP
   Zhang, BZ
   Gao, MJ
   Li, SY
   Bai, Y
   Liu, ZH
AF Tan, Ailing
   Liao, Hongping
   Zhang, Bozhi
   Gao, Meijing
   Li, Shiyu
   Bai, Yang
   Liu, Zehao
TI Infrared image enhancement algorithm based on detail enhancement guided
   image filtering
SO VISUAL COMPUTER
LA English
DT Article
DE Guided image filtering; Infrared image; Detail enhancement; Edge
   perception factor; Detail regulation factor
ID TRANSFORM
AB Because of the unique imaging mechanism of infrared (IR) sensors, IR images commonly suffer from blurred edge details, low contrast, and poor signal-to-noise ratio. A new method is proposed in this paper to enhance IR image details so that the enhanced images can effectively inhibit image noise and improve image contrast while enhancing image details. First, for the traditional guided image filter (GIF) applied to IR image enhancement is prone to halo artifacts, this paper proposes a detail enhancement guided filter (DGIF). It mainly adds the constructed edge perception and detail regulation factors to the cost function of the GIF. Then, according to the visual characteristics of human eyes, this paper applies the detail regulation factor to the detail layer enhancement, which solves the problem of amplifying image noise using fixed gain coefficient enhancement. Finally, the enhanced detail layer is directly fused with the base layer so that the enhanced image has rich detail information. We first compare the DGIF with four guided image filters and then compare the algorithm of this paper with three traditional IR image enhancement algorithms and two IR image enhancement algorithms based on the GIF on 20 IR images. The experimental results show that the DGIF has better edge-preserving and smoothing characteristics than the four guided image filters. The mean values of quantitative evaluation of information entropy, average gradient, edge intensity, figure definition, and root-mean-square contrast of the enhanced images, respectively, achieved about 0.23%, 3.4%, 4.3%, 2.1%, and 0.17% improvement over the optimal parameter. It shows that the algorithm in this paper can effectively suppress the image noise in the detail layer while enhancing the detail information, improving the image contrast, and having a better visual effect.
C1 [Tan, Ailing; Liao, Hongping; Zhang, Bozhi; Li, Shiyu; Bai, Yang; Liu, Zehao] Yanshan Univ, Sch Informat Sci & Engn, Lab Special Fiber & Fiber Sensor Hebei Prov, Qinhuangdao 066004, Hebei, Peoples R China.
   [Gao, Meijing] Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
C3 Yanshan University; Beijing Institute of Technology
RP Gao, MJ (corresponding author), Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
EM gaomeijing@126.com
FU National Nature Science Foundation of China [61971373]; Natural Science
   Foundation of Hebei Province-China [C2020203010]; Hebei Innovative
   Training Program for Doctoral Candidate of China [CXZZBS2022148]
FX National Nature Science Foundation of China, 61971373, Meijing Gao,
   Natural Science Foundation of Hebei Province-China, C2020203010, Ailing
   Tan, Hebei Innovative Training Program for Doctoral Candidate of China,
   CXZZBS2022148, Shiyu Li.
CR Branchitta F, 2009, OPT ENG, V48, DOI 10.1117/1.3216575
   Chen YH, 2020, APPL OPTICS, V59, P6407, DOI 10.1364/AO.396417
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   [付青青 Fu Qingqing], 2020, [海洋学报, Acta Oceanologica Sinica], V42, P130
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   KATIRCIO LU F., 2020, EL CEZERI FEN MUHEND, V7, P1201, DOI DOI 10.31202/ECJSE.733519
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Li J, 2020, ACTA PHOTONICA SINIC, V49, DOI 10.3788/gzxb20204904.0410003
   Li S, 2018, INFRARED PHYS TECHN, V90, P164, DOI 10.1016/j.infrared.2018.03.010
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liu N, 2014, INFRARED PHYS TECHN, V67, P138, DOI 10.1016/j.infrared.2014.07.013
   Liu T, 2015, MECH SYST SIGNAL PR, V62-63, P366, DOI 10.1016/j.ymssp.2015.03.010
   Lu ZW, 2018, IEEE SIGNAL PROC LET, V25, P1585, DOI 10.1109/LSP.2018.2867896
   Lv H., 2022, SIVIP, V16, P22
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Qi YH, 2016, INFRARED PHYS TECHN, V76, P521, DOI 10.1016/j.infrared.2016.03.021
   Reynolds JH, 2003, NEURON, V37, P853, DOI 10.1016/S0896-6273(03)00097-7
   Sahu A., 2012, COMPUTER ENG INTELLI, V3, P40
   Shao YY, 2021, INFRARED PHYS TECHN, V119, DOI 10.1016/j.infrared.2021.103968
   Shi G., 2019, RES INFRARED IMAGE E
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Vickers VE, 1996, OPT ENG, V35, P1921, DOI 10.1117/1.601006
   Voronin V, 2018, IEEE SW SYMP IMAG, P5, DOI 10.1109/SSIAI.2018.8470344
   WAN MJ, 2018, REMOTE SENS-BASEL, V10
   Wang ZJ, 2020, SPECTROSC SPECT ANAL, V40, P3463, DOI 10.3964/j.issn.1000-0593(2020)11-3463-05
   Zhang H, 2022, INFRARED PHYS TECHN, V120, DOI 10.1016/j.infrared.2021.104000
   Zhou B, 2019, J MOD OPTIC, V66, P33, DOI 10.1080/09500340.2018.1511861
NR 28
TC 4
Z9 4
U1 10
U2 51
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6491
EP 6502
DI 10.1007/s00371-022-02741-6
EA DEC 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000898696000001
DA 2024-07-18
ER

PT J
AU Hurtado, J
   Gattass, M
   Raposo, A
AF Hurtado, Jan
   Gattass, Marcelo
   Raposo, Alberto
TI 3D point cloud denoising using anisotropic neighborhoods and a novel
   sharp feature detection algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud denoising; Normal estimation; Sharp feature detection;
   Anisotropic neighborhoods
ID SEGMENTATION; SURFACES
AB 3D point cloud denoising is a fundamental task in a geometry-processing pipeline, where feature preservation is essential for various applications. The literature presents several methods to overcome the denoising problem; however, most of them focus on denoising smooth surfaces and not on handling sharp features correctly. This paper proposes a new sharp feature-preserving method for point cloud denoising that incorporates solutions for normal estimation and feature detection. The denoising method consists of four major steps. First, we compute the per-point anisotropic neighborhoods by solving local quadratic optimization problems that penalize normal variation. Second, we estimate a piecewise smooth normal field that enhances sharp feature regions using these anisotropic neighborhoods. This step includes bilateral filtering and a novel corrector procedure to obtain more reliable normals for the subsequent steps. Third, we employ a novel sharp feature detection algorithm to select the feature points precisely. Finally, we update the point positions to fit them to the computed normals while retaining the sharp features that were detected. These steps are repeated until the noise is minimized. We evaluate our method using qualitative and quantitative comparisons with state-of-the-art denoising, normal estimation, and feature detection procedures. Our experiments show that our approach is competitive and, in most test cases, outperforms all other methods.
C1 [Hurtado, Jan; Gattass, Marcelo; Raposo, Alberto] Pontifical Catholic Univ Rio de Janeiro, Dept Informat, BR-22541041 Rio De Janeiro, RJ, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro
RP Hurtado, J (corresponding author), Pontifical Catholic Univ Rio de Janeiro, Dept Informat, BR-22541041 Rio De Janeiro, RJ, Brazil.
EM hurtado@tecgraf.pue-rio.br; mgattass@tecgraf.puc-rio.br;
   abraposo@tecgraf.puc-rio.br
RI Hurtado Jauregui, Jan Jose/HCH-4254-2022; Raposo, Alberto B/G-3204-2012
OI Raposo, Alberto/0000-0001-7279-1823; Hurtado Jauregui, Jan
   Jose/0000-0003-3422-3117
FU National Council for Scientific and Technological Development (CNPq);
   Tecgraf Institute (PUC-Rio)
FX We would like to express our gratitude to the National Council for
   Scientific and Technological Development (CNPq) and the Tecgraf
   Institute (PUC-Rio) for their support.
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   [Anonymous], 2003, PROC SIAM C GEOMETRI
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   Bazazian D, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P358
   Béarzi Y, 2018, COMPUT GRAPH FORUM, V37, P13, DOI 10.1111/cgf.13338
   Bellekens B., 2015, Int. J. Adv. Intell. Syst, V8, P118
   Ben-Shabat Y, 2019, PROC CVPR IEEE, P10104, DOI 10.1109/CVPR.2019.01035
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boulch A, 2016, COMPUT GRAPH FORUM, V35, P281, DOI 10.1111/cgf.12983
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Cazals F, 2005, COMPUT AIDED GEOM D, V22, P121, DOI 10.1016/j.cagd.2004.09.004
   Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   CPLEX IBM ILOG, 2020, ILOG CPLEX OPT STUD
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deschaud JE, 2010, INT ARCH PHOTOGRAMM, V38, P109
   Digne J, 2012, 2012 IEEE COMPUTER S, P73
   Digne J, 2018, IEEE T VIS COMPUT GR, V24, P2238, DOI 10.1109/TVCG.2017.2719024
   Dinesh C, 2018, IEEE INT WORKSH MULT
   Dinesh C, 2020, IEEE T IMAGE PROCESS, V29, P4143, DOI 10.1109/TIP.2020.2969052
   Duan CJ, 2019, INT CONF ACOUST SPEE, P8553, DOI [10.1109/icassp.2019.8682812, 10.1109/ICASSP.2019.8682812]
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Foi A, 2006, PROC SPIE, V6064, DOI 10.1117/12.642839
   Guennebaud G, 2008, COMPUT GRAPH FORUM, V27, P653, DOI 10.1111/j.1467-8659.2008.01163.x
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guerrero P, 2018, COMPUT GRAPH FORUM, V37, P75, DOI 10.1111/cgf.13343
   Guillemot T, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P324, DOI 10.1109/3DIMPVT.2012.71
   Guo MQ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020412
   Han XF, 2017, SIGNAL PROCESS-IMAGE, V57, P103, DOI 10.1016/j.image.2017.05.009
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hu GF, 2006, VISUAL COMPUT, V22, P147, DOI 10.1007/s00371-006-0372-0
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Hurtado J., 2018, THESIS PONTIFICIA U
   Hurtado J, 2018, SIBGRAPI, P1, DOI 10.1109/SIBGRAPI.2018.00007
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kivi P.E., 2022, IEEE ACCESS, V10, P173
   Leal E, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113206
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Liu Z, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102857
   Lu DN, 2020, COMPUT AIDED DESIGN, V125, DOI 10.1016/j.cad.2020.102860
   Lu X, 2020, IEEE Transactions on Visualization and Computer Graphics
   Lu XQ, 2018, IEEE T VIS COMPUT GR, V24, P2315, DOI 10.1109/TVCG.2017.2725948
   Luo S.T., 2021, P IEEE CVF INT C COM, P4583
   Luo ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1330, DOI 10.1145/3394171.3413727
   Mattei E, 2017, COMPUT GRAPH FORUM, V36, P123, DOI 10.1111/cgf.13068
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Preiner R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601172
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Rosman G, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12139
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Roynard X, 2018, INT J ROBOT RES, V37, P545, DOI 10.1177/0278364918767506
   Sarkar K, 2018, INT CONF 3D VISION, P444, DOI 10.1109/3DV.2018.00058
   Schoenenberger Y, 2015, 3DTV CONF
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   The CGAL Project, 2021, CGAL USER REFERENCE, V5.3
   Thompson EM, 2020, COMPUT GRAPH-UK, V91, P199, DOI 10.1016/j.cag.2020.07.011
   Wang J, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12187
   Wang J, 2012, COMPUT AIDED DESIGN, V44, P597, DOI 10.1016/j.cad.2012.03.001
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Weber C, 2012, GRAPH MODELS, V74, P335, DOI 10.1016/j.gmod.2012.04.012
   Wei MQ, 2023, IEEE T VIS COMPUT GR, V29, P1357, DOI 10.1109/TVCG.2021.3113463
   Williams RM, 2018, COMPUT AIDED GEOM D, V67, P97, DOI 10.1016/j.cagd.2018.10.003
   Wu SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818073
   Yadav SK, 2018, COMPUT GRAPH-UK, V74, P234, DOI 10.1016/j.cag.2018.05.014
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Zeng J, 2020, IEEE T IMAGE PROCESS, V29, P3474, DOI 10.1109/TIP.2019.2961429
   Zhang DB, 2021, IEEE T VIS COMPUT GR, V27, P2015, DOI 10.1109/TVCG.2020.3027069
   Zhang J, 2019, IEEE T VIS COMPUT GR, V25, P1693, DOI 10.1109/TVCG.2018.2827998
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zheng YL, 2018, COMPUT AIDED GEOM D, V62, P16, DOI 10.1016/j.cagd.2018.03.004
   Zheng YL, 2017, VISUAL COMPUT, V33, P857, DOI 10.1007/s00371-017-1391-8
   Zhu L, 2013, COMPUT GRAPH FORUM, V32, P371, DOI 10.1111/cgf.12245
NR 79
TC 4
Z9 4
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5823
EP 5848
DI 10.1007/s00371-022-02698-6
EA OCT 2022
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000871177900001
DA 2024-07-18
ER

PT J
AU Amemiya, T
   Leow, CS
   Buayai, P
   Makino, K
   Mao, XY
   Nishizaki, H
AF Amemiya, Tatsuyoshi
   Leow, Chee Siang
   Buayai, Prawit
   Makino, Koji
   Mao, Xiaoyang
   Nishizaki, Hiromitsu
TI Appropriate grape color estimation based on metric learning for judging
   harvest timing
SO VISUAL COMPUTER
LA English
DT Article
DE Color estimation; Grape harvesting; Metric learning; Smart agriculture
ID FRUIT
AB The color of a bunch of grapes is a very important factor when determining the appropriate time for harvesting. However, judging whether the color of the bunch is appropriate for harvesting requires experience and the result can vary by individuals. In this paper, we describe a system to support grape harvesting based on color estimation using deep learning. To estimate the color of a bunch of grapes, bunch detection, grain detection, removal of pest grains, and color estimation are required, for which deep learning-based approaches are adopted. In this study, YOLOv5, an object detection model that considers both accuracy and processing speed, is adopted for bunch detection and grain detection. For the detection of diseased grains, an autoencoder-based anomaly detection model is also employed. Since color is strongly affected by brightness, a color estimation model that is less affected by this factor is required. Accordingly, we propose multitask learning that uses metric learning. The color estimation model in this study is based on AlexNet. Metric learning was applied to train this model. Brightness is an important factor affecting the perception of color. In a practical experiment using actual grapes, we empirically selected the best three image channels from RGB and CIELAB (L*a*b*) color spaces and we found that the color estimation accuracy of the proposed multi-task model, the combination with "L" channel from L*a*b color space and "GB" from RGB color space for the grape image (represented as "LGB" color space), was 72.1%, compared to 21.1% for the model which used the normal RGB image. In addition, it was found that the proposed system was able to determine the suitability of grapes for harvesting with an accuracy of 81.6%, demonstrating the effectiveness of the proposed system.
C1 [Amemiya, Tatsuyoshi; Leow, Chee Siang; Buayai, Prawit; Makino, Koji; Mao, Xiaoyang; Nishizaki, Hiromitsu] Univ Yamanashi, Integrated Grad Sch Med Engn & Agr Sci, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
   [Buayai, Prawit; Makino, Koji; Mao, Xiaoyang; Nishizaki, Hiromitsu] Univ Yamanashi, Grad Fac Interdisciplinary Res, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
C3 University of Yamanashi; University of Yamanashi
RP Amemiya, T; Nishizaki, H (corresponding author), Univ Yamanashi, Integrated Grad Sch Med Engn & Agr Sci, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.; Nishizaki, H (corresponding author), Univ Yamanashi, Grad Fac Interdisciplinary Res, 4-3-11 Takeda, Kofu, Yamanashi 4008511, Japan.
EM amatatsu@alps-lab.org; cheesiang_leow@alps-lab.org;
   buayai@yamanashi.ac.jp; kohjim@yamanashi.ac.jp; mao@yamanashi.ac.jp;
   hnishi@yamanashi.ac.jp
RI Mao, Xiaoyang/AAG-1294-2020; Buayai, Prawit/JCF-0529-2023
OI Mao, Xiaoyang/0000-0001-5010-6952; Buayai, Prawit/0000-0002-0873-9569;
   Nishizaki, Hiromitsu/0000-0002-7717-8312
FU Ministry of Agriculture, Forestry, and Fisheries of Japan [20344794];
   Consortium for Analysis and Visualization of Professional Skill for
   High-Quality Shine Muscat Grape Production
FX This research was supported by a grant from the Ministry of Agriculture,
   Forestry, and Fisheries of Japan, "Feasibility Study Project on Smart
   Agriculture Using Local 5G" (Grant number 20344794). In addition, we
   received support from the Consortium for Analysis and Visualization of
   Professional Skill for High-Quality Shine Muscat Grape Production,
   consisting of eight organizations (YSK e-com Co. Ltd., NEC Corporation,
   University of Yamanashi, Yamanashi Prefectural Government, DOCOMOCS,
   Inc., ZEN-NOH YAMANASHI, and JA Fruits Yamanashi).
CR Abdalla A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11243001
   Amemiya T, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P194, DOI 10.1109/CW52790.2021.00040
   Baevski A., 2020, ADV NEURAL INFORM PR, V33, P12449
   Brown T., 2020, Advances in Neural Information Processing Systems, V33, P1877, DOI [DOI 10.48550/ARXIV.2005.14165, DOI 10.5555/3495724.3495883]
   Buayai P, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P101, DOI 10.1109/CW52790.2021.00022
   Buayai P, 2021, IEEE ACCESS, V9, P4829, DOI 10.1109/ACCESS.2020.3048374
   Chen WT, 2022, IEEE J-STARS, V15, P1150, DOI 10.1109/JSTARS.2022.3141826
   Chow JK, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101105
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Franczyk Bogdan, 2020, Procedia Computer Science, V176, P1211, DOI 10.1016/j.procs.2020.09.117
   Gan H, 2018, COMPUT ELECTRON AGR, V152, P117, DOI 10.1016/j.compag.2018.07.011
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hunt R.W. G., 2004, The Reproduction of Colour, DOI DOI 10.1002/0470024275
   International Commission on Illumination, 2018, COL
   Iscen A, 2018, PROC CVPR IEEE, P7642, DOI 10.1109/CVPR.2018.00797
   Kobayashi K., 2012, P 2012 ANN M JAPANES, P59
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee DH, 2017, J INFORM DISPLAY, V18, P73, DOI 10.1080/15980316.2017.1291454
   Li H, 2014, COMPUT ELECTRON AGR, V106, P91, DOI 10.1016/j.compag.2014.05.015
   Lin GH, 2020, PRECIS AGRIC, V21, P1, DOI 10.1007/s11119-019-09654-w
   Ma AL, 2021, ISPRS J PHOTOGRAMM, V172, P171, DOI 10.1016/j.isprsjprs.2020.11.025
   Marani R, 2021, PRECIS AGRIC, V22, P387, DOI 10.1007/s11119-020-09736-0
   Masana M., 2018, BRIT MACH VIS C BMVC
   Nafzi M., 2019, ARXIV
   Rachmawati E, 2015, 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND INFORMATICS 2015, P43, DOI 10.1109/ICEEI.2015.7352467
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Santos T., 2019, EMBRAPA WINE GRAPE I, DOI [10.5281/zenodo.3361736, DOI 10.5281/ZENODO.3361736]
   Santos TT, 2020, COMPUT ELECTRON AGR, V170, DOI 10.1016/j.compag.2020.105247
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tsai DM, 2021, ADV ENG INFORM, V48, DOI 10.1016/j.aei.2021.101272
   Xu BL, 2020, PROC CVPR IEEE, P3613, DOI 10.1109/CVPR42600.2020.00367
   Yu Y, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.06.001
   Zhang L, 2018, IEEE ACCESS, V6, P67940, DOI 10.1109/ACCESS.2018.2879324
   Zhou GQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3167569
NR 40
TC 3
Z9 3
U1 3
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4083
EP 4094
DI 10.1007/s00371-022-02666-0
EA SEP 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000861155600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Lv, W
   Shi, HB
   Tan, S
   Song, B
   Tao, Y
AF Lv, Wen
   Shi, Hongbo
   Tan, Shuai
   Song, Bing
   Tao, Yang
TI A dynamic semantic knowledge graph for zero-shot object detection
SO VISUAL COMPUTER
LA English
DT Article
DE Zero-shot object detection; Semantic embedding; Knowledge graph;
   Semantic reasoning
ID ATTRIBUTES
AB Zero-shot object detection (ZSD) learns a mapping relationship between visual space and semantic space; therefore, ZSD can rely on semantic information to identify and localize novel classes. However, due to the variety of images in the same category, ZSD algorithms usually use a fixed semantic embedding resulting in a visual-semantic significant gap. To bridge the gap, a dynamic semantic knowledge graph (DSKG) is proposed based on the visual-semantic relation. First, in order to capture the similarity between semantic information, a semantic knowledge graph is utilized to establish connections between categories. Then, a dynamic semantic reasoning mechanism is introduced to update semantic embedding based on the self-attention mechanism. Finally, experiments show that the DSKG can achieve significant improvements on MS-COCO and PASCAL VOC datasets.
C1 [Lv, Wen; Shi, Hongbo; Tan, Shuai; Song, Bing; Tao, Yang] East China Univ Sci & Technol, Minist Educ, Key Lab Smart Mfg Energy Chem Proc, Shanghai 200237, Peoples R China.
C3 East China University of Science & Technology
RP Shi, HB (corresponding author), East China Univ Sci & Technol, Minist Educ, Key Lab Smart Mfg Energy Chem Proc, Shanghai 200237, Peoples R China.
EM lvwen39808@163.com; hbshi@ecust.edu.cn; tanshuai@ecust.edu.cn;
   songbing@ecust.edu.cn; taoyang@ecust.edu.cn
RI Liu, Songjun/IWE-4263-2023; Shi, Hongbo/E-6963-2016
FU National Natural Science Foundation of China [62073140, 62073141];
   National Natural Science Foundation of Shanghai [19ZR1473200]
FX This work is supported by the National Natural Science Foundation of
   China (Nos. 62073140, 62073141); National Natural Science Foundation of
   Shanghai (No. 19ZR1473200).
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2018, Polarity loss for zero-shot object detection
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Chaudhuri U, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104003
   Dai JF, 2016, ADV NEUR IN, V29
   Demirel B., 2018, BRIT MACHINE VISION
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gupta D, 2020, IEEE WINT CONF APPL, P1198, DOI 10.1109/WACV45572.2020.9093384
   Hayat N., 2020, SYNTHESIZING UNSEEN
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He K, 2017, IEEE INT WORKSH MULT
   Ji Z, 2020, IEEE ACCESS, V8, P9287, DOI 10.1109/ACCESS.2019.2962298
   Ji Z, 2020, NEUROCOMPUTING, V373, P90, DOI 10.1016/j.neucom.2019.09.062
   Jia Z, 2020, IEEE T IMAGE PROCESS, V29, P1958, DOI 10.1109/TIP.2019.2947780
   Jiang CH, 2018, ADV NEUR IN, V31
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Joulin A., 2016, ARXIV
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104077
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Li ZH, 2019, AAAI CONF ARTIF INTE, P8690
   Li ZH, 2019, PATTERN RECOGN, V88, P595, DOI 10.1016/j.patcog.2018.12.010
   Li Z, 2019, IEEE GLOB COMM CONF, DOI 10.1109/globecom38437.2019.9014261
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Liu B, 2018, KNOWL-BASED SYST, V144, P42, DOI 10.1016/j.knosys.2017.12.022
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo YX, 2020, NEUROCOMPUTING, V391, P74, DOI 10.1016/j.neucom.2020.01.069
   Mao QM, 2020, IEEE T CIRCUITS-II, V67, P921, DOI 10.1109/TCSII.2020.2982316
   Meng M, 2020, NEUROCOMPUTING, V399, P117, DOI 10.1016/j.neucom.2020.02.077
   Meng M, 2019, IEEE T IMAGE PROCESS, V28, P1824, DOI 10.1109/TIP.2018.2881926
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Norouzi M., 2014, P INT C LEARN REPR
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rahman S, 2020, INT J COMPUT VISION, V128, P2979, DOI 10.1007/s11263-020-01355-6
   Rahman S, 2019, IEEE I CONF COMP VIS, P6081, DOI 10.1109/ICCV.2019.00618
   Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34
   Redmon J., 2016, P IEEE C COMP VIS PA, P779, DOI DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K., 2020, 2020 IEEE 32 INT C T
   Wang K., 2020, IEEE 32 INT C TOOLS
   Wang K, 2020, PROC INT C TOOLS ART, P230, DOI 10.1109/ICTAI50040.2020.00045
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Yan CX, 2020, IEEE T IMAGE PROCESS, V29, P8163, DOI 10.1109/TIP.2020.3011807
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhao S, 2020, arXiv
   Zheng Y., 2020, 15 ASIAN C COMPUTER, P107
   Zhu PK, 2020, IEEE T CIRC SYST VID, V30, P998, DOI 10.1109/TCSVT.2019.2899569
NR 60
TC 0
Z9 0
U1 6
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4513
EP 4527
DI 10.1007/s00371-022-02604-0
EA SEP 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000853284800001
DA 2024-07-18
ER

PT J
AU Si, TZ
   He, FZ
   Li, PL
AF Si, Tongzhen
   He, Fazhi
   Li, Penglei
TI Hybrid feature constraint with clustering for unsupervised person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Unsupervised learning; Feature constraint;
   Clustering
AB Unsupervised person re-identification (Re-ID) has better scalability and usability in real-world deployments due to the lack of annotations, which is more challenging than supervised methods. State-of-the-art approaches mainly employ clustering algorithms to generate pseudo-labels for transferring the process into a supervised operation. However, the clustering algorithm depends on discriminative pedestrian features. Only using the clustering algorithm produces low-quality labels and hinders the performance of the Re-ID model. In the paper, we propose the hybrid feature constraint network (HFCN) to adequately restrict the pedestrian feature distribution for unsupervised person Re-ID. Specifically, we first define a feature constraint loss to restrict the feature distribution so that different pedestrians can be clearly distinguished at the first step. And then, we design a multi-task operation with the iterative update for clustering algorithm to further implement the feature constraint. This can adequately utilize predicted label information and identify complex samples. Finally, we integrate the feature constraint loss and multi-task operation to optimize the Re-ID model, which could promote the clustering to generate high-quality labels and learn valuable information. Extensive experiments prove that the proposed HFCN is effective and outperforms the state-of-the-art.
C1 [Si, Tongzhen; He, Fazhi] Wuhan Univ, Sch Comp Sci, Bayi Rd, Wuhan 430072, Peoples R China.
   [Li, Penglei] China Univ Geosci, Inst Geophys & Geomat, Lumo Rd, Wuhan 430074, Peoples R China.
C3 Wuhan University; China University of Geosciences
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Bayi Rd, Wuhan 430072, Peoples R China.
EM fzhe@whu.edu.cn
RI Si, Tongzhen/JDD-5696-2023; He, Fazhi/Q-3691-2018
OI Si, Tongzhen/0000-0002-1141-9718
FU National Natural Science Foundation of China [62072348]; National Key
   R&D Program of China [2019YFC1509604]; Science and Technology Major
   Project of Hubei Province (NextGeneration AI Technologies) [2019AEA170]
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 62072348, National Key R&D Program of China under
   Grant No. 2019YFC1509604, the Science and Technology Major Project of
   Hubei Province (NextGeneration AI Technologies) under Grant No.
   2019AEA170.
CR Ainam JP, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106644
   Arora S, 2022, VISUAL COMPUT, V38, P2461, DOI 10.1007/s00371-021-02123-4
   Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902
   Chen H, 2021, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV48630.2021.00005
   Chen ZC, 2021, VISUAL COMPUT, V37, P685, DOI 10.1007/s00371-020-01880-y
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding Y, 2022, VISUAL COMPUT, V38, P1871, DOI 10.1007/s00371-021-02246-8
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Ge Y., 2020, ADV NEURAL INFORM PR
   Ge Yixiao, 2020, ARXIV200101526
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364
   Ji Z, 2020, EUR C COMP VIS, P20, DOI DOI 10.1007/978-3-030-58604-1_2
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Li HR, 2021, MEMET COMPUT, V13, P1, DOI 10.1007/s12293-021-00328-7
   Li HF, 2022, IEEE T CIRC SYST VID, V32, P2814, DOI 10.1109/TCSVT.2021.3099943
   Li Q, 2022, PATTERN RECOGN, V125, DOI 10.1016/j.patcog.2022.108521
   Liang WQ, 2021, IEEE T IMAGE PROCESS, V30, P6392, DOI 10.1109/TIP.2021.3092578
   Liang YQ, 2022, INTEGR COMPUT-AID E, V29, P23, DOI 10.3233/ICA-210661
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu JM, 2019, IEEE COMPUT SOC CONF, P2070, DOI 10.1109/CVPRW.2019.00259
   Liu S, 2021, AAAI CONF ARTIF INTE, V35, P2172
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Pan YT, 2020, WORLD WIDE WEB, V23, P2259, DOI 10.1007/s11280-020-00793-z
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Raj S, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108287
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Si TZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108462
   Si TZ, 2019, AD HOC NETW, V95, DOI 10.1016/j.adhoc.2019.101984
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Sun J, 2021, IEEE T IMAGE PROCESS, V30, P2935, DOI 10.1109/TIP.2021.3056889
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tulsulkar G, 2021, VISUAL COMPUT, V37, P3019, DOI 10.1007/s00371-021-02242-y
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang WT, 2015, INT CONF MACH LEARN, P445, DOI 10.1109/ICMLC.2015.7340962
   Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005
   Wei D, 2023, VISUAL COMPUT, V39, P501, DOI 10.1007/s00371-021-02344-7
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Xie K., 2021, INT J MACH LEARN CYB, V12, P1, DOI [10.1007/s13042-020-01103-9, DOI 10.1007/S13042-020-01103-9]
   Xin Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P735, DOI 10.1007/978-3-030-58571-6_43
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang H, 2021, IEEE T IMAGE PROCESS, V30, P5287, DOI 10.1109/TIP.2021.3082298
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang Z, 2022, IEEE T CIRC SYST VID, V32, P1160, DOI 10.1109/TCSVT.2021.3074745
   Zhang Z, 2018, IEEE ACCESS, V6, P36887, DOI 10.1109/ACCESS.2018.2852712
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhuang WM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P433, DOI 10.1145/3474085.3475182
NR 69
TC 3
Z9 3
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5121
EP 5133
DI 10.1007/s00371-022-02649-1
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844462000001
DA 2024-07-18
ER

PT J
AU Chai, TT
   Prasad, S
   Yan, JN
   Zhang, ZX
AF Chai, Tingting
   Prasad, Shitala
   Yan, Jianen
   Zhang, Zhaoxin
TI Contactless palmprint biometrics using DeepNet with dedicated assistant
   layers
SO VISUAL COMPUTER
LA English
DT Article
DE Palmprint recognition; CNN; Dedicated assistant layers; Blurred and
   noisy data
AB Palmprint biometrics has a broad application prospect owing to non-intrusiveness, ease of image acquisition and stable textural pattern. Hand-crafted approaches are vulnerable to non-ideal palmprint images caused by uneven illumination, motion blur and noise contamination. Many researchers have designed excellent texture descriptors or/and advanced image pre-processing algorithms. Nevertheless, they are highly targeted at some specific data, less adaptable to the emerging data. In this paper, a semi-pretrained contactless palmprint recognition deep network is developed to achieve high accuracy and robustness. Semi-CPRN is composed of underlying network structure from ResNet-152 and the proposed assistant layers dedicated to high-performance palmprint recognition. The well-designed assistant layers enhance convolutional neural network to steadily extract the real palmprint features even from the degraded images without being deceived by the degradation factors. Besides, to better carry out the research on palmprint recognition in the open environment, we established a new contactless database HIT-NIST-V1 under natural scene. The comparative experiments on CASIA, IITD, PolyU3D/2D, Tongji and HIT-NIST-V1 illustrate that Semi-CPRN is comparable and superior to previously published state-of-the-art approaches. Simultaneously, CNN-based palmprint biometrics methods show significant robustness to motion blur, Gaussian noise, and salt and pepper noise.
C1 [Chai, Tingting; Yan, Jianen; Zhang, Zhaoxin] Harbin Inst Technol, Fac Comp, Harbin, Heilongjiang, Peoples R China.
   [Prasad, Shitala] ASTAR, Inst Infocomm Res, Singapore, Singapore.
C3 Harbin Institute of Technology; Agency for Science Technology & Research
   (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Yan, JN; Zhang, ZX (corresponding author), Harbin Inst Technol, Fac Comp, Harbin, Heilongjiang, Peoples R China.
EM ttchai@hit.edu.cn; shitala@ieee.org; yanjianen@hit.edu.cn;
   zhangzhaoxin@hit.edu.cn
RI wang, yatong/KDN-3824-2024; wang, yan/GSE-6489-2022
FU Natural Science Foundation of Shandong Province [ZR2020KF009]; Young
   Teacher Development Fund of Harbin Institute of Technology
   [IDGA10002081]
FX The authors' gratitude goes first and foremost to Assoc. Prof. Adams
   Wai-Kin Kong for his instructive advices and useful suggestions some
   time ago. The authors express the gratitude to Chinese Academy of
   Sciences, IIT Delhi, The Hong Kong Polytechnic University and Tongji
   University for sharing contactless palmprint databases. The authors also
   thank Jianhui Li for his contribution on DL model verification
   experiments. This work is supported by Natural Science Foundation of
   Shandong Province (Grant No. ZR2020KF009) and Young Teacher Development
   Fund of Harbin Institute of Technology (Grant No. IDGA10002081).
CR Balduzzi D, 2017, PR MACH LEARN RES, V70
   Cai D, 2008, IEEE T KNOWL DATA EN, V20, P1, DOI 10.1109/TKDE.2007.190669
   Chai TT, 2019, FUTURE GENER COMP SY, V99, P41, DOI 10.1016/j.future.2019.04.013
   Chai TT, 2018, INT J BIOMETRICS, V10, P232, DOI 10.1504/IJBM.2018.093635
   Fei LK, 2019, IEEE T IMAGE PROCESS, V28, P3808, DOI 10.1109/TIP.2019.2903307
   Genovese A, 2019, IEEE T INF FOREN SEC, V14, P3160, DOI 10.1109/TIFS.2019.2911165
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hong D., 2014, PLOS ONE, V9, P1
   Hong DF, 2016, NEUROCOMPUTING, V174, P999, DOI 10.1016/j.neucom.2015.10.031
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaafar H, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/360217
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jain AK, 2016, PATTERN RECOGN LETT, V79, P80, DOI 10.1016/j.patrec.2015.12.013
   Jalali Amin, 2015, Proceedings of the 3rd International Conference on Human-Agent Interaction, P209
   Jia W, 2014, IEEE T SYST MAN CY-S, V44, P385, DOI 10.1109/TSMC.2013.2258010
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurimo E, 2009, LECT NOTES COMPUT SC, V5575, P81, DOI 10.1007/978-3-642-02230-2_9
   Lagendijk RL, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P323, DOI 10.1016/B978-0-12-374457-9.00014-7
   Li SY, 2021, IEEE T INF FOREN SEC, V16, P3186, DOI 10.1109/TIFS.2021.3074315
   Luo WJ, 2016, ADV NEUR IN, V29
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Matkowski WM, 2020, IEEE T INF FOREN SEC, V15, P1601, DOI 10.1109/TIFS.2019.2945183
   Palma D, 2015, IEEE CONF COMM NETW, P659, DOI 10.1109/CNS.2015.7346883
   Prasad S, 2022, COMPUT J, V65, P355, DOI 10.1093/comjnl/bxaa045
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Springenberg J. T., 2015, ARXIV PREPRINT ARXIV
   Svoboda J, 2016, INT C PATT RECOG, P4232, DOI 10.1109/ICPR.2016.7900298
   Wang G, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL INTELLIGENCE (CSCI), P492, DOI 10.1109/CSCI.2015.15
   Xu NY, 2021, VISUAL COMPUT, V37, P695, DOI 10.1007/s00371-020-01962-x
   Zhang L, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10040078
   Zhang L, 2017, PATTERN RECOGN, V69, P199, DOI 10.1016/j.patcog.2017.04.016
   Zhao SP, 2021, IEEE T IMAGE PROCESS, V30, P1001, DOI 10.1109/TIP.2020.3039895
   Zhong DX, 2020, IEEE T CIRC SYST VID, V30, P1559, DOI 10.1109/TCSVT.2019.2904283
NR 35
TC 4
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4029
EP 4047
DI 10.1007/s00371-022-02571-6
EA JUL 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825052100001
DA 2024-07-18
ER

PT J
AU Markaki, S
   Panagiotakis, C
AF Markaki, Smaragda
   Panagiotakis, Costas
TI Jigsaw puzzle solving techniques and applications: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Jigsaw puzzles; Pictorial puzzles; Apictorial puzzles; Shape matching;
   Fresco fragments; Image reconstruction
ID AUTOMATIC SOLUTION; 2D; IMAGE; ALGORITHM
AB A jigsaw puzzle is a recreational activity that involves assembling a certain number of pieces into a combined and well-fitting unit without creating gaps between adjacent pieces. Two-dimensional puzzles are divided into two main categories, the "apictorial" in which the only information available is the shape of the pieces and the "pictorial" which may take into account not only the shape of the pieces, but also their content. Jigsaw puzzles are considered as one of the most popular category of puzzles. The majority of them are accompanied by a guiding image and there is only one "counterpart" for each side of each piece (pictorial jigsaw puzzles), although some more difficult variants have blank pieces, the so-called apictorial jigsaw puzzles. In this paper, we will examine the open problem of solving pictorial and apictorial jigsaw puzzles, and their various applications, such as the reconstruction of two-dimensional fragmented objects, the restoration of fragmented wall-paintings and the repair of shredded documents. We will also present an evaluation of the state-of-the- art jigsaw puzzle reassembly techniques in pictorial and apictorial puzzles.
C1 [Markaki, Smaragda; Panagiotakis, Costas] Hellen Mediterranean Univ, Dept Management Sci & Technol, Agios Nikolaos 72100, Greece.
C3 Hellenic Mediterranean University
RP Markaki, S (corresponding author), Hellen Mediterranean Univ, Dept Management Sci & Technol, Agios Nikolaos 72100, Greece.
EM smarkaki@hmu.gr; cpanag@hmu.gr
RI Panagiotakis, Costas/I-5115-2019
OI Panagiotakis, Costas/0000-0003-3680-7087; Markaki,
   Smaragda/0000-0002-9821-7499
FU European Union; Greek national funds through the Operational Program
   Competitiveness, Entrepreneurship and Innovation, under the call
   RESEARCH - CREATE - INNOVATE B cycle [T2EDK-03135]
FX This research has been co-financed by the European Union and Greek
   national funds through the Operational Program Competitiveness,
   Entrepreneurship and Innovation, under the call RESEARCH - CREATE -
   INNOVATE B cycle (Project Code: T2EDK-03135).
CR Alajlan Naif, 2009, American Journal of Applied Sciences, V6, P1941, DOI 10.3844/ajassp.2009.1941.1947
   Ali FAB, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATIONS, AND CONTROL TECHNOLOGY (I4CT), P426, DOI 10.1109/I4CT.2014.6914219
   Andaló FA, 2017, IEEE T PATTERN ANAL, V39, P385, DOI 10.1109/TPAMI.2016.2547394
   [Anonymous], 1994, Polyominoes, puzzles, patterns, problems, and packagings
   [Anonymous], 2017, Neural combinatorial optimization for solving jigsaw puzzles: A step towards unsupervised pre-training
   Bunke H., 1993, Computer Analysis of Images and Patterns. 5th International Conference, CAIP '93 Proceedings, P299
   Cantoni Virginio, 2020, CompSysTech '20: Proceedings of the 21st International Conference on Computer Systems and Technologies '20, P208, DOI 10.1145/3407982.3408025
   CHO TS, 2010, PROC CVPR IEEE, P183, DOI DOI 10.1109/CVPR.2010.5540212
   Chung MG, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P877, DOI 10.1109/ICOSP.1998.770751
   De Bock J, 2004, IEEE IMAGE PROC, P2127
   Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4
   Derech N, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108065
   Elson J, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P366
   Fornasier M, 2005, PATTERN RECOGN, V38, P2074, DOI 10.1016/j.patcog.2005.03.014
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Funkhouser T., 2011, Journal on Computing and Cultural Heritage (JOCCH), V4, P1
   Gallagher AC, 2012, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2012.6247699
   Garcin M., 2013, CLIMATE CHANGE SEA L
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Haichang Gao, 2010, Proceedings 2010 IEEE 13th International Conference on Computational Science and Engineering (CSE 2010), P351, DOI 10.1109/CSE.2010.53
   Harel P., 2020, PREPRINT
   Hirota K., 1986, IMAGE RECOGNITION JI, P87
   Hoff DJ, 2014, J MATH IMAGING VIS, V49, P234, DOI 10.1007/s10851-013-0454-3
   Hoff DJ, 2013, J MATH IMAGING VIS, V45, P176, DOI 10.1007/s10851-012-0358-7
   Huroyan V, 2020, SIAM J IMAGING SCI, V13, P1717, DOI 10.1137/19M1290760
   Kita N, 2021, VISUAL COMPUT, V37, P777, DOI 10.1007/s00371-020-01968-5
   Kleber Florian, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1061, DOI 10.1109/ICDAR.2009.154
   Kong WX, 2001, PROC CVPR IEEE, P583
   KOSIBA DA, 1994, INT C PATT RECOG, P616, DOI 10.1109/ICPR.1994.576377
   Kumar Ashwani, 2021, 2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS), P723, DOI 10.1109/ICACCS51430.2021.9441737
   Kwon H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20051495
   Lalitha KS, 2017, LECT NOTES COMPUT SC, V10127, P135, DOI 10.1007/978-3-319-52503-7_11
   Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li R, 2022, IEEE T IMAGE PROCESS, V31, P513, DOI 10.1109/TIP.2021.3120052
   Liu HR, 2011, IEEE T MULTIMEDIA, V13, P1154, DOI 10.1109/TMM.2011.2160845
   Makridis M, 2010, IEEE T SYST MAN CY B, V40, P789, DOI 10.1109/TSMCB.2009.2029868
   Mascret Ariane., 2006, Progress in Spatial Data Handling, P383, DOI [DOI 10.1007/3-540-35589-8_25, 10.1007/3-540-35589-8_25]
   McBride JC, 2003, C COMPUTER VISION PA, V1, P3, DOI 10.1109/CVPRW.2003.10008
   Mondal D, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P249, DOI 10.1109/CRV.2013.54
   Montusiewicz J, 2020, ADV SCI TECHNOL-RES, V14, P49, DOI 10.12913/22998624/122570
   Nagura K., 1986, Systems and Computers in Japan, V17, P30, DOI 10.1002/scj.4690170204
   Naiman AE, 2019, INFORM-J COMPUT INFO, V43, P243, DOI 10.31449/inf.v43i2.1823
   Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Ostertag C, 2020, PATTERN RECOGN LETT, V131, P336, DOI 10.1016/j.patrec.2020.01.012
   Paikin G, 2015, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR.2015.7299116
   Panagopoulos T., 2002, AUTOMATED RECONSTRUC
   Papaodysseus C, 2008, IEEE T SYST MAN CY A, V38, P958, DOI 10.1109/TSMCA.2008.923078
   Paumard MM, 2020, IEEE T IMAGE PROCESS, V29, P3569, DOI 10.1109/TIP.2019.2963378
   Paumard MM, 2018, IEEE IMAGE PROC, P1018, DOI 10.1109/ICIP.2018.8451094
   Paumard MM, 2018, LECT NOTES COMPUT SC, V11210, P155, DOI 10.1007/978-3-030-01231-1_10
   Payal N, 2016, 2016 IEEE UTTAR PRADESH SECTION INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ELECTRONICS ENGINEERING (UPCON), P38, DOI 10.1109/UPCON.2016.7894621
   Pomeranz D, 2011, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2011.5995331
   RADACK GM, 1982, COMPUT VISION GRAPH, V19, P1, DOI 10.1016/0146-664X(82)90111-3
   Richter F, 2013, IEEE T MULTIMEDIA, V15, P582, DOI 10.1109/TMM.2012.2235415
   Sauer G., 2008, P 4 S USABLE PRIVACY, V6, p, P1
   Schwartz J.T., 1985, IDENTIFICATION PARTI
   Shen B., 2018, PREPRINT
   Shih H.-C., 2018, 2018 IEEE VISUAL COM, P1
   Shin H., 2010, VAST 10, P71, DOI DOI 10.2312/VAST/VAST10/071-078
   Sholomon D, 2016, LECT NOTES COMPUT SC, V9887, P170, DOI 10.1007/978-3-319-44781-0_21
   Sholomon D, 2016, GENET PROGRAM EVOL M, V17, P291, DOI 10.1007/s10710-015-9258-0
   Sizikova E, 2018, ACM J COMPUT CULT HE, V11, DOI 10.1145/3084547
   Son K, 2019, IEEE T PATTERN ANAL, V41, P2222, DOI 10.1109/TPAMI.2018.2857776
   Toler-Franklin C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866207
   Toyama F, 2002, INT C PATT RECOG, P389, DOI 10.1109/ICPR.2002.1047477
   Tsamoura E, 2010, IEEE T IMAGE PROCESS, V19, P680, DOI 10.1109/TIP.2009.2035840
   WEBSTER RW, 1991, IEEE T SYST MAN CYB, V21, P1271, DOI 10.1109/21.120080
   Wegener A., 1912, GEOL RUNDSCH, V3, P276, DOI [10.1007/BF02202896, DOI 10.1007/BF02202896]
   Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201
   Wolfson H., 1988, Annals of Operations Research, V12, P51, DOI 10.1007/BF02186360
   Yao FH, 2003, PATTERN RECOGN LETT, V24, P1819, DOI 10.1016/S0167-8655(03)00006-0
   Yu R., 2015, PREPRINT
   Zhang K, 2014, GRAPH MODELS, V76, P484, DOI 10.1016/j.gmod.2014.03.001
   Zhang M, 2017, VISUAL COMPUT, V33, P1601, DOI 10.1007/s00371-016-1303-3
   Zhu LJ, 2006, LECT NOTES CONTR INF, V345, P645
NR 77
TC 4
Z9 4
U1 6
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4405
EP 4421
DI 10.1007/s00371-022-02598-9
EA JUL 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000825006500002
DA 2024-07-18
ER

PT J
AU Weller, R
   Brennecke, B
   Zachmann, G
AF Weller, Rene
   Brennecke, Benjamin
   Zachmann, Gabriel
TI Redirected walking in virtual reality with auditory step feedback
SO VISUAL COMPUTER
LA English
DT Article
DE Redirected Walking; Virtual Reality; Auditory Step
AB We present a novel approach of redirected walking (RDW) based on step feedback sounds to redirect users in virtual reality. The main idea is to achieve path manipulation by changing step noises to deviate the users, who still believe that they are walking a straight line. Our approach can be combined with traditional visual approaches for RDW based on eye-blinking. Moreover, we have conducted a user study in a large area (10x20m) using a within-subject design. We achieved a translational redirection of 1.7m in average with pure audio feedback. Moreover, our results show that visual methods can amplify the deviation of our new auditory approach by 80cm in average at the distance of 20 m.
C1 [Weller, Rene; Brennecke, Benjamin; Zachmann, Gabriel] Univ Bremen, Comp Graph & Virtual Real Grp, Bremen, Germany.
C3 University of Bremen
RP Weller, R (corresponding author), Univ Bremen, Comp Graph & Virtual Real Grp, Bremen, Germany.
EM weller@informatik.uni-bremen.de
RI Zachmann, Gabriel/AAI-9685-2020
OI Zachmann, Gabriel/0000-0001-8155-1127
CR Nguyen A, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281515
   [Anonymous], 2009, JVRB-Journal of Virtual Reality and Broadcasting, DOI 10.20385/1860-2037/6.2009.2
   Baalman M.A. J., 2008, WAVE FIELD SYNTHESIS
   Bimberg P, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P464, DOI [10.1109/VRW50115.2020.00098, 10.1109/VRW50115.2020.0-178]
   Borrego A., 2019, 2019 International Conference on Virtual Rehabilitation (ICVR), P1, DOI [DOI 10.1109/ICVR46560.2019, 10.1109/ICVR46560.2019.8994546, DOI 10.1109/ICVR46560.2019.8994546]
   Feigl T, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3141205
   Gao PZ, 2020, INT SYM MIX AUGMENT, P639, DOI 10.1109/ISMAR50242.2020.00092
   Kassner M, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P1151, DOI 10.1145/2638728.2641695
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kim J, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10090400
   Kruse L, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P305, DOI 10.1109/VR.2018.8446216
   Langbehn E, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201335
   Lo WT, 2001, APPL ERGON, V32, P1, DOI 10.1016/S0003-6870(00)00059-4
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Nogalski M., 2016, P SOUND MUS COMP C, V16, P17
   Razzaque S., 2001, Proc. Eurogr, P289, DOI [10.2312/egs.20011036, DOI 10.2312/EGS.20011036]
   Regenbrecht H, 2002, PRESENCE-TELEOP VIRT, V11, P425, DOI 10.1162/105474602760204318
   Rewkowski N, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P395, DOI [10.1109/vr.2019.8798286, 10.1109/VR.2019.8798286]
   Sakono H, 2021, IEEE T VIS COMPUT GR, V27, P4278, DOI 10.1109/TVCG.2021.3106501
   Schwind V, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300590
   Serafin S, 2013, P IEEE VIRT REAL ANN, P161, DOI 10.1109/VR.2013.6549412
   Steinicke F., 2008, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P149, DOI 10.1145/1450579.1450611
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Turchet Luca, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P269, DOI 10.1109/MMSP.2010.5662031
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
NR 25
TC 1
Z9 1
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3475
EP 3486
DI 10.1007/s00371-022-02565-4
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705400005
OA hybrid
DA 2024-07-18
ER

PT J
AU Kunert, C
   Schwandt, T
   Nadar, CR
   Broll, W
AF Kunert, Christian
   Schwandt, Tobias
   Nadar, Christon R.
   Broll, Wolfgang
TI Neural network adaption for depth sensor replication
SO VISUAL COMPUTER
LA English
DT Article
DE Depth estimation; Reconstruction; Computer vision; Neural networks
AB In recent years, various depth sensors that are small enough to be used with mobile hardware have been introduced. They provide important information for use cases like 3D reconstruction or in the context of augmented reality where tracking and camera data alone would be insufficient. However, depth sensors may not always be available due to hardware limitations or when simulating augmented reality applications for prototyping purposes. In these cases, different approaches like stereo matching or depth estimation using neural networks may provide a viable alternative. In this paper, we therefore explore the imitation of depth sensors using deep neural networks. For this, we use a state-of-the-art network for depth estimation and adapt it in order to mimic a Structure Sensor as well as an iPad LiDAR sensor. We evaluate the network which was pre-trained on NYU V2 directly as well as several variations where transfer learning is applied in order to adapt the network to different depth sensors while using various data preprocessing and augmentation techniques. We show that a transfer learning approach together with appropriate data processing can enable an accurate modeling of the respective depth sensors.
C1 [Kunert, Christian; Broll, Wolfgang] Ilmenau Univ Technol, Virtual Worlds & Digital Games Grp, Ehrenbergstr 29, Ilmenau, Germany.
   [Schwandt, Tobias] Ilmenau Univ Technol, Res Grp Virtual Worlds & Digital Games, Ehrenbergstr 29, Ilmenau, Germany.
   [Nadar, Christon R.] Fraunhofer IDMT, Ehrenbergstr 31, Ilmenau, Germany.
C3 Technische Universitat Ilmenau; Technische Universitat Ilmenau
RP Kunert, C (corresponding author), Ilmenau Univ Technol, Virtual Worlds & Digital Games Grp, Ehrenbergstr 29, Ilmenau, Germany.
EM christian.kunert@tu-ilmenau.de; tobias.schwandt@tu-ilmenau.de;
   christon-ragavan.nadar@idrnt.fraunhofer.de; wolfgang.broll@tu-ilmenau.de
OI Kunert, Christian/0000-0003-4187-8365
FU Free State of Thuringia, Germany [FKZ: 2018-FGI-0019]; Carl Zeiss
   Foundation
FX This work has partially been funded by the CYTEMEX project funded by the
   Free State of Thuringia, Germany (FKZ: 2018-FGI-0019) as well as the
   CO-HUMANICS project funded by the Carl Zeiss Foundation in their
   breakthroughs program.
CR Alhashim I., 2018, ARXIV E PRINTS ARXIV
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58
   Deng J., 2009, IEEE C COMP VIS PATT
   Eigen D, 2014, ADV NEUR IN, V27
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gui ZG, 2011, OPTIK, V122, P697, DOI 10.1016/j.ijleo.2010.05.010
   He Y, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010092
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Janoch A., 2012, The Berkeley 3D Object Dataset
   Kalantari M, 2016, GEO-SPAT INF SCI, V19, P202, DOI 10.1080/10095020.2016.1235817
   Khan F, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082272
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lindner M, 2010, COMPUT VIS IMAGE UND, V114, P1318, DOI 10.1016/j.cviu.2009.11.002
   Marin G, 2014, IEEE IMAGE PROC, P1565, DOI 10.1109/ICIP.2014.7025313
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Nadar CR, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P9, DOI 10.1109/CW52790.2021.00010
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Ramamonjisoa M, 2019, IEEE INT CONF COMP V, P2109, DOI 10.1109/ICCVW.2019.00266
   Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002
   Saxena A., 2006, NIPS, P1161, DOI DOI 10.1109/TPAMI.2015.2505283A
   Silberman N, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Ummenhofer B., 2017, P IEEE C COMP VIS PA, P5038
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
NR 33
TC 1
Z9 1
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4071
EP 4081
DI 10.1007/s00371-022-02531-0
EA JUN 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000814960800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, CM
   Luan, WN
   Fu, RH
   Pang, HB
   Li, YH
AF Liu, Cheng-ming
   Luan, Wan-na
   Fu, Rong-hua
   Pang, Hai-bo
   Li, Ying-hao
TI Attention-embedding mesh saliency
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh saliency; Mesh simplification; Attention-embedding; Weakly
   supervision
AB Recently, the learning method is gradually penetrating into the field of 3D saliency, but the ground truth annotation is too insufficient to directly train a 3D saliency network. Here, we propose a novel attention-embedding strategy for 3D saliency estimation by directly applying the attention embedding scheme to 3D mesh. With this method, the network is trained in a weakly supervised manner, requiring no saliency annotations but generalizing well on different categories of objects, such as animals, furniture, cars and people. Experimental results show that our approach is comparable with existing state-of-the-art methods. We also apply saliency results to mesh simplification. Evaluations on simplified models show that the visually significant parts can be retained during saliency-aware simplification.
C1 [Liu, Cheng-ming; Luan, Wan-na; Fu, Rong-hua; Pang, Hai-bo; Li, Ying-hao] Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450002, Peoples R China.
C3 Zhengzhou University
RP Pang, HB (corresponding author), Zhengzhou Univ, Sch Cyber Sci & Engn, Zhengzhou 450002, Peoples R China.
EM phb@zzu.edu.cn
OI Liu, Cheng-ming/0000-0002-8650-4271
FU National Key Research and Development Program of China [2020YFB1712401,
   2018YFC0824402]
FX We thank all the anonymous reviewers for their valuable comments. This
   work was supported by the National Key Research and Development Program
   of China (2020YFB1712401, 2018YFC0824402).
CR [Anonymous], 2014, WORKSHOP INT C LEARN
   [Anonymous], 2014, ABS14053531 CORR
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen L., 2017, P IEEE C COMPUTER VI
   Chen S., 2018, P EUROPEAN C COMPUTE
   Chen X, 2017, P IEEE C COMPUTER VI
   Chen Xiaowu., 2012, ACM Transactions on Graphics, V31
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Ding XY, 2019, IEEE T IMAGE PROCESS, V28, P5379, DOI 10.1109/TIP.2019.2918735
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Engelmann F., 2020, INT C ROBOTICS AUTOM, V1
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Garland M., 1997, P 24 ANN C COMP GRAP, V1997, P209, DOI DOI 10.1145/258734.258849
   HAMANN B, 1994, COMPUT AIDED GEOM D, V11, P197, DOI 10.1016/0167-8396(94)90032-9
   Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959
   Hoppe, 1993, C COMPUTER GRAPHICS
   Hou TB, 2013, IEEE T VIS COMPUT GR, V19, P3, DOI 10.1109/TVCG.2012.111
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu SF, 2020, NEUROCOMPUTING, V400, P11, DOI 10.1016/j.neucom.2020.02.106
   Huang J., 2016, 2016 23 INT C PATTER
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jiang JJ, 2014, 2014 IEEE INTERNATIONAL CONFERENCE (ITHINGS) - 2014 IEEE INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) - 2014 IEEE INTERNATIONAL CONFERENCE ON CYBER-PHYSICAL-SOCIAL COMPUTING (CPS), P1, DOI 10.1109/iThings.2014.10
   Kingma D. P., 2014, arXiv
   Koch C, 1999, NAT NEUROSCI, V2, P9, DOI 10.1038/4511
   Komarichev A., 2019, P IEEE C COMPUTER VI
   Lahav A., 2020, ARXIV PREPRINT ARXIV
   Lan S., 2019, 2019 IEEECVF C COMPU
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G., 2012, 2012 IEEE C COMPUTER
   Limper Max., 2016, Proceedings of the Thirty-Seventh Annual Conference of the European Association for Computer Graphics: Short Papers, P13, DOI DOI 10.2312/EGSH.20161003
   Liu F., 2019, P 2019 ACM SIGSAC C
   Low K.L., 1997, S INTERACTIVE 3D GRA
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nousias S., 2020, 2020 IEEE INT C MULT, P1
   Papon J., 2013, P IEEE C COMPUTER VI
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, P IEEE C COMPUTER VI
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Sibson R., 1981, Interpreting Multivariate Data, P21
   Song R, 2021, IEEE T VIS COMPUT GR, V27, P151, DOI 10.1109/TVCG.2019.2928794
   Song R, 2018, VISUAL COMPUT, V34, P323, DOI 10.1007/s00371-016-1334-9
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Tatarchenko M., 2018, P IEEE C COMPUTER VI
   Thomas H., 2019, P IEEE INT C COMPUTE
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei N, 2018, IEEE ACCESS, V6, P54536, DOI 10.1109/ACCESS.2018.2872168
   WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   Wu W., 2019, P IEEE C COMPUTER VI
   Xi W., 2018, SIGGRAPH ASIA 2018 T
   Xu K., 2015, INT C MACHINE LEARNI
   Zhao H., 2019, P IEEE C COMPUTER VI
   Zheng T., 2019, P IEEE INT C COMPUTE
NR 60
TC 2
Z9 3
U1 2
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1783
EP 1795
DI 10.1007/s00371-022-02444-y
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000793634200002
DA 2024-07-18
ER

PT J
AU Zhang, GH
   Huang, GH
   Chen, HY
   Pun, CM
   Yu, ZW
   Ling, WK
AF Zhang, Ganghan
   Huang, Guoheng
   Chen, Haiyuan
   Pun, Chi-Man
   Yu, Zhiwen
   Ling, Wing-Kuen
TI Video action recognition with Key-detail Motion Capturing based on
   motion spectrum analysis and multiscale feature fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Key frame extraction; Multiscale feature fusion;
   Spatiotemporal feature pyramid
AB At present, existing research works on action recognition are still not ideal, when most of the video content is redundant such as video clips without any object motion, and human actions in the video are complex. The reasons are as follows: (1) Most of them lack attention to key-motion information of the video, thus irrelevant information will be input into the model. (2) And there is a lack of interaction between video spatial and temporal information, which may cause the loss of detailed motion information in the video. In this paper, we propose a Key-detail Motion Capturing Network (K-MCN) to solve these problems, which contains two modules. The first one is the Video Key-motion Spectrum Analyzer (VKSA) module. In this module, the video optical flow can be subjected to frequency spectrum analysis, filtering and clustering to extract the key-motion frames. The second one is the Multiscale Motion Spatiotemporal Interaction module, which allows multi-scale modeling and fusion of spatial and temporal features extracted from key-motion frames, enabling the network to realize the interaction and supplement of multiscale spatiotemporal information. Finally, we conducted extensive experiments on the UCF101, HMDB51 and Something-SomethingV1 datasets, and the results showed that our method achieves better performance compared with other state-of-the-art methods.
C1 [Zhang, Ganghan; Huang, Guoheng; Chen, Haiyuan] Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
   [Pun, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Taipa 999078, Macao, Peoples R China.
   [Yu, Zhiwen] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Ling, Wing-Kuen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; University of Macau; South China
   University of Technology; Guangdong University of Technology
RP Huang, GH (corresponding author), Guangdong Univ Technol, Sch Comp, Guangzhou 510006, Peoples R China.
EM kevinwong@gdut.edu.cn
RI Pun, Chi Man/GRJ-3703-2022
OI Huang, Guoheng/0000-0002-3640-3229
FU Key-Area Research and Development Program of Guangdong Province
   [2018B010109007, 2019B010153002]; Guangzhou R&D Programme in Key Areas
   of Science and Technology Projects [202007040006]; Guangdong Provincial
   Key Laboratory of Cyber-Physical System [2020B1212060069]; Program of
   Marine Economy Development (Six Marine Industries) Special Foundation of
   Department of Natural Resources of Guangdong Province [GDNRC [2020]056]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province under Grant 2018B010109007 and
   2019B010153002, and the Guangzhou R&D Programme in Key Areas of Science
   and Technology Projects under Grant 202007040006, and the Guangdong
   Provincial Key Laboratory of Cyber-Physical System under Grant
   2020B1212060069, and the Program of Marine Economy Development (Six
   Marine Industries) Special Foundation of Department of Natural Resources
   of Guangdong Province under Grant GDNRC [2020]056.
CR [Anonymous], 2012, CoRR
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang SK, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351415
   Chen CF, 2021, PROC CVPR IEEE, P6161, DOI 10.1109/CVPR46437.2021.00610
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Davis JW, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P39, DOI 10.1109/EVENT.2001.938864
   Diba A., 2017, Temporal 3D ConvNets: New architecture and transfer learning for video classification
   Du T., 2017, CONVN ARCH SEARCH SP
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Guan GL, 2013, IEEE T CIRC SYST VID, V23, P729, DOI 10.1109/TCSVT.2012.2214871
   Guo WZ, 2015, INFORM SCIENCES, V320, P418, DOI 10.1016/j.ins.2015.04.034
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604
   Krishnan Dilip, ARXIV PREPRINT ARXIV
   Kuehne H., 2013, HMDB LARGE VIDEO DAT
   Kulhare S, 2016, INT C PATT RECOG, P835, DOI 10.1109/ICPR.2016.7899739
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Mademlis I, 2018, INFORM SCIENCES, V432, P319, DOI 10.1016/j.ins.2017.12.020
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pareek P, 2021, ARTIF INTELL REV, V54, P2259, DOI 10.1007/s10462-020-09904-8
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Shao H, 2020, AAAI CONF ARTIF INTE, V34, P11966
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Simonyan K, 2014, ADV NEUR IN, V27
   Smith S.W, 1997, SCI ENG GUIDE DIGITA, V14
   Tang C, 2018, INFORM SCIENCES, V467, P219, DOI 10.1016/j.ins.2018.08.003
   Thien HT, 2020, INFORM SCIENCES, V513, P112, DOI 10.1016/j.ins.2019.10.047
   Thien HT, 2018, INFORM SCIENCES, V444, P20, DOI 10.1016/j.ins.2018.02.042
   Tian Y, 2019, IEEE INT CON MULTI, P272, DOI 10.1109/ICME.2019.00055
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang Y, 2017, IEEE INT CONF COMP V, P2129, DOI 10.1109/ICCVW.2017.249
   Xu Q, 2014, INFORM SCIENCES, V278, P736, DOI 10.1016/j.ins.2014.03.088
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yan X., 2018, ARXIV180607272
   Yang ZY, 2019, IEEE T CIRC SYST VID, V29, P2405, DOI 10.1109/TCSVT.2018.2864148
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zheng ZX, 2019, NEUROCOMPUTING, V358, P446, DOI 10.1016/j.neucom.2019.05.058
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou L, 2017, P 9 INT C MACH LEARN, P272
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 64
TC 2
Z9 2
U1 5
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 539
EP 556
DI 10.1007/s00371-021-02355-4
EA JAN 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000741634600006
DA 2024-07-18
ER

PT J
AU Zhang, SQ
   Su, SQ
   Li, L
   Lu, JF
   Zhou, QL
   Chang, CC
AF Zhang, Shanqing
   Su, Shengqi
   Li, Li
   Lu, Jianfeng
   Zhou, Qili
   Chang, Chin-Chen
TI CSST-Net: an arbitrary image style transfer network of coverless
   steganography
SO VISUAL COMPUTER
LA English
DT Article
DE Adaptive encoding mechanism; Arbitrary image style transfer; Coverless
   steganography; Dilated convolution
AB A traditional image steganography embeds secret information into a cover image to generate a secret-embedded image. The modification traces imposed on the cover image can be easily detected by steganalysis tools. Coverless steganography has been introduced to solve this problem. In this study, coverless steganography is combined with image style transfer, an arbitrary image style transfer network CSST-Net is put forward, and a secret information is encoded into the parameters (an adaptive steganography matrix) of CSST-Net, which is used to restrict the style transfer. Arbitrary image style transfer is performed instructed by the adaptive steganography matrix, and the image style transfer result driven by secret information is directly synthesized. Our experiments show that CSST-Net can not only synthesize any image style transfer result with good visual effect, but also achieve good performance in capacity, anti-steganalysis and security.
C1 [Zhang, Shanqing; Su, Shengqi; Li, Li; Lu, Jianfeng; Zhou, Qili] Hangzhou Dianzi Univ, Dept Comp Sci, Hangzhou 310018, Peoples R China.
   [Chang, Chin-Chen] Feng Chia Univ, Dept Informat Engn & Comp Sci, Taichung, Taiwan.
C3 Hangzhou Dianzi University; Feng Chia University
RP Li, L (corresponding author), Hangzhou Dianzi Univ, Dept Comp Sci, Hangzhou 310018, Peoples R China.
EM sqzhang@hdu.edu.cn; sushengqi1994@163.com; lili2008@hdu.edu.cn;
   jflu@hdu.edu.cn; zql@hdu.edu.cn; alan3c@gmail.com
RI Chang, Ching-Chun/JAN-6210-2023
FU National Natural Science Foundation of China [61802101]; Public Welfare
   Technology and Industry Project of Zhejiang Provincial Science
   Technology Department [LGG18F020013, LGG19F020016]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 61802101 and in part by the Public
   Welfare Technology and Industry Project of Zhejiang Provincial Science
   Technology Department under Grant Nos. LGG18F020013 and LGG19F020016.
CR [Anonymous], 2018, INT C SEC INT COMP B
   [Anonymous], 2018, INT C SEC INT COMP B
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   BLASHFIELD RK, 1991, J CLASSIF, V8, P277
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gulati A.N., 2010, INT C WORKSH EM TREN
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   KUANAR S, VISUAL COMPUT
   Li XX, 2007, INFORM SCIENCES, V177, P3099, DOI 10.1016/j.ins.2007.02.008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Ming-ming, 2018, Journal of Applied Sciences - Electronics and Information Engineering, V36, P371, DOI 10.3969/j.issn.0255-8297.2018.02.015
   Liu Q, 2020, KNOWL-BASED SYST, V192, DOI 10.1016/j.knosys.2019.105375
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Otori H, 2009, IEEE COMPUT GRAPH, V29, P74, DOI 10.1109/MCG.2009.127
   Qin JH, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8091394
   Shen CX, 2007, SCI CHINA SER F, V50, P273, DOI 10.1007/s11432-007-0037-2
   Wu HC, 2005, IEE P-VIS IMAGE SIGN, V152, P611, DOI 10.1049/ip-vis:20059022
   Wu KC, 2015, IEEE T IMAGE PROCESS, V24, P130, DOI 10.1109/TIP.2014.2371246
   Yu F., 2015, ARXIV
   Zhang S., 2020, ADV INTELL SYST COMP, V895
   Zhang SQ, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091152
   Zhou Z., 2015, COVERLESS IMAGE STEG, DOI 10.1007/978-3-319-27051-7_11
   Zhou Zhi-li, 2016, Journal of Applied Sciences - Electronics and Information Engineering, V34, P527, DOI 10.3969/j.issn.0255-8297.2016.05.005
NR 24
TC 11
Z9 11
U1 7
U2 43
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2125
EP 2137
DI 10.1007/s00371-021-02272-6
EA NOV 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000714316000001
DA 2024-07-18
ER

PT J
AU RoselinKiruba, R
   Sharmila, TS
AF RoselinKiruba, R.
   Sharmila, T. Sree
TI A novel data hiding by image interpolation using edge quad-tree block
   complexity
SO VISUAL COMPUTER
LA English
DT Article
DE Block complexity; Data hiding; Edge; Quad-tree
ID STEGANOGRAPHIC METHOD
AB Data hiding method embed secret data inside a cover medium. Image quality and capacity plays a significant role in the performance of data hiding. The motivation of this paper is to improve the interpolation technique and bit plane data hiding method using edge quad-tree block complexity. Edges in the cover image are identified using edge detector and are further partitioned using quad-tree. The edges are smooth regions divided into smaller blocks in order to obtain good embedding capacity, whereas the rough regions involve those regions other than the edges and are kept as larger blocks to avoid distortion. Each quad-tree block is implemented with up-sampling interpolation based on edges and every pixel is divided into two-bit plane namely high and low bit plane. In each bit plane, two different data are embedded based on their hiding capacity. The hiding capacity of the high bit plane is calculated by two prediction levels namely Pixel Value Differencing and block complexity. In low bit plane, Least Significant Bit method is used for hiding the data. Experimental results demonstrate that the proposed method significantly improved the embedding performance, capacity and also resist to attacks when compared to other state-of-the-art methods.
C1 [RoselinKiruba, R.; Sharmila, T. Sree] Sri Sivasubramaniya Nadar Coll Engn, Dept Informat Technol, Chennai, Tamil Nadu, India.
C3 SSN College of Engineering
RP RoselinKiruba, R (corresponding author), Sri Sivasubramaniya Nadar Coll Engn, Dept Informat Technol, Chennai, Tamil Nadu, India.
EM roselinkirubar@ssn.edu.in
RI R, RoselinKiruba/HOH-2068-2023
OI T, Sree Sharmila/0009-0009-1736-2669
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chang YT, 2013, J SUPERCOMPUT, V66, P1093, DOI 10.1007/s11227-013-1016-6
   Chen YQ, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102584
   Di FQ, 2019, MULTIMED TOOLS APPL, V78, P7125, DOI 10.1007/s11042-018-6469-4
   Mukherjee N, 2020, MULTIMED TOOLS APPL, V79, P13449, DOI 10.1007/s11042-019-08178-9
   Guo TD, 2021, VISUAL COMPUT, V37, P2069, DOI 10.1007/s00371-020-01964-9
   Hassan FS, 2021, ARAB J SCI ENG, V46, P8441, DOI 10.1007/s13369-021-05529-3
   Hassan FS, 2020, MULTIMED TOOLS APPL, V79, P30087, DOI 10.1007/s11042-020-09513-1
   Huang CT, 2018, J SUPERCOMPUT, V74, P4295, DOI 10.1007/s11227-016-1874-9
   Jung KH, 2009, COMPUT STAND INTER, V31, P465, DOI 10.1016/j.csi.2008.06.001
   Kim C, 2018, J REAL-TIME IMAGE PR, V14, P101, DOI 10.1007/s11554-016-0641-8
   Ko-Chin Chang, 2008, Journal of Multimedia, V3, P37
   Kumar RJR, 2021, VISUAL COMPUT, V37, P2315, DOI 10.1007/s00371-020-01988-1
   Lee CF, 2012, EXPERT SYST APPL, V39, P6712, DOI 10.1016/j.eswa.2011.12.019
   Liao X, 2011, J VIS COMMUN IMAGE R, V22, P1, DOI 10.1016/j.jvcir.2010.08.007
   Liu JC, 2008, FUND INFORM, V83, P319
   Malik A, 2020, MULTIMED TOOLS APPL, V79, P18005, DOI 10.1007/s11042-020-08691-2
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Mohtavipour SM, 2022, VISUAL COMPUT, V38, P2057, DOI 10.1007/s00371-021-02266-4
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2014, SIGNAL PROCESS-IMAGE, V29, P760, DOI 10.1016/j.image.2014.05.003
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Parah SA, 2018, MULTIMED TOOLS APPL, V77, P185, DOI 10.1007/s11042-016-4253-x
   Peng F, 2014, DIGIT SIGNAL PROCESS, V25, P255, DOI 10.1016/j.dsp.2013.11.002
   Qiu YH, 2021, VISUAL COMPUT, V37, P2253, DOI 10.1007/s00371-020-01984-5
   Qu HZ, 2021, VISUAL COMPUT, V37, P2331, DOI 10.1007/s00371-020-01989-0
   Raja T.S.R, 2013, J ELECT IMAG, V22
   Kiruba RR, 2021, MULTIDIM SYST SIGN P, V32, P405, DOI 10.1007/s11045-019-00697-w
   Roselinkiruba R., 2021, International Journal of Information Technology, V13, P1797, DOI 10.1007/s41870-021-00774-z
   RoselinKiruba R, 2018, 2018 2ND INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATION, AND SIGNAL PROCESSING (ICCCSP): SPECIAL FOCUS ON TECHNOLOGY AND INNOVATION FOR SMART ENVIRONMENT, P156
   RoselinKiruba R., 2017, 2017 IEEE International Conference on Circuits and Systems (ICCS). Proceedings, P112, DOI 10.1109/ICCS1.2017.8325974
   Shanableh T, 2020, SN COMPUTER SCI, V309
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Sharmila TS, 2014, SIGNAL IMAGE VIDEO P, V8, P1399, DOI 10.1007/s11760-012-0369-2
   Sharmila TS, 2014, SIGNAL IMAGE VIDEO P, V8, P149, DOI 10.1007/s11760-013-0505-7
   Sree Sharmila T., 2013, Journal of Computer Science, V9, P176, DOI 10.3844/jcssp.2013.176.182
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tsai P, 2009, SIGNAL PROCESS, V89, P1129, DOI 10.1016/j.sigpro.2008.12.017
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Wang JX, 2019, SIGNAL PROCESS, V159, P193, DOI 10.1016/j.sigpro.2019.02.013
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Weber A.G, 1997, USC SIPI IMAGE DATAB, V5
   Wu DC, 2003, PATTERN RECOGN LETT, V24, P1613, DOI 10.1016/S0167-8655(02)00402-6
   Wu HC, 2005, IEE P-VIS IMAGE SIGN, V152, P611, DOI 10.1049/ip-vis:20059022
   Yang CH, 2011, J SYST SOFTWARE, V84, P669, DOI 10.1016/j.jss.2010.11.889
   Yang Cll., 2008, J INF MANAG, V15, P29
NR 48
TC 7
Z9 8
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 59
EP 72
DI 10.1007/s00371-021-02312-1
EA OCT 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000705827500001
DA 2024-07-18
ER

PT J
AU Mohtavipour, SM
   Saeidi, M
   Arabsorkhi, A
AF Mohtavipour, Seyed Mehdi
   Saeidi, Mahmoud
   Arabsorkhi, Abouzar
TI A multi-stream CNN for deep violence detection in video sequences using
   handcrafted features
SO VISUAL COMPUTER
LA English
DT Article
DE Computer vision; Deep learning; Convolutional neural network; Human
   behavior analysis
ID ANOMALY DETECTION; FALL DETECTION; OPTICAL-FLOW; RECOGNITION
AB Intelligent video surveillance systems have been used recently for automatic monitoring of human interactions. Although they play a significant role in reducing security concerns, there are many challenges for distinguishing between normal and abnormal behaviors such as crowded environments and camera viewpoint. In this paper, we propose a novel deep violence detection framework based on the specific features derived from handcrafted methods. These features are related to appearance, speed of movement, and representative image and fed to a convolutional neural network (CNN) as spatial, temporal, and spatiotemporal streams. The spatial stream trained the network with each frame in the video to learn environment patterns. The temporal stream contained three consecutive frames to learn motion patterns of violent behavior with a modified differential magnitude of optical flow. Moreover, in spatio-temporal stream, we introduced a discriminative feature with a novel differential motion energy image to represent violent actions more interpretable. This approach covers different aspects of violent behavior by fusing the results of these streams. The proposed CNN network is trained with violence-labeled and normal-labeled frames of 3 Hockey, Movie, and ViF datasets which comprised both crowded and uncrowded situations. The experimental results showed that the proposed deep violence detection approach outperformed state-of-the-art works in terms of accuracy and processing time.
C1 [Mohtavipour, Seyed Mehdi] Iran Univ Sci & Technol, Sch Elect Engn, Tehran, Iran.
   [Saeidi, Mahmoud; Arabsorkhi, Abouzar] Iran Telecommun Res Ctr, Tehran, Iran.
C3 Iran University Science & Technology
RP Mohtavipour, SM (corresponding author), Iran Univ Sci & Technol, Sch Elect Engn, Tehran, Iran.
EM mehdi_mohtavipour@elec.iust.ac.ir; msaeidi40@itrc.ac.ir;
   abouzar_arab@itrc.ac.ir
RI Mohtavipour, Seyed Mehdi/AAV-3668-2020
OI Mohtavipour, Seyed Mehdi/0000-0003-2749-8980
CR [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2712608
   Asad M, 2021, VISUAL COMPUT, V37, P1415, DOI 10.1007/s00371-020-01878-6
   Aslan M, 2015, APPL SOFT COMPUT, V37, P1023, DOI 10.1016/j.asoc.2014.12.035
   Ben Mabrouk A, 2017, PATTERN RECOGN LETT, V92, P62, DOI 10.1016/j.patrec.2017.04.015
   Berlin SJ, 2022, VISUAL COMPUT, V38, P223, DOI 10.1007/s00371-020-02012-2
   Berlin SJ, 2020, MULTIMED TOOLS APPL, V79, P17349, DOI 10.1007/s11042-020-08704-0
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Black M.J, 2018, GCPR
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Carneiro SA, 2019, SIBGRAPI, P8, DOI 10.1109/SIBGRAPI.2019.00010
   Dapogny A, 2018, INT J COMPUT VISION, V126, P255, DOI 10.1007/s11263-017-1010-1
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   de Souza F. D. M., 2010, Proceedings of the 23rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2010), P224, DOI 10.1109/SIBGRAPI.2010.38
   Deniz O, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P478
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ehsan Tahereh Zarrat, 2020, 2020 11th International Conference on Information and Knowledge Technology (IKT), P88, DOI 10.1109/IKT51791.2020.9345617
   Ehsan TZ, 2018, 2018 8TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P153, DOI 10.1109/ICCKE.2018.8566460
   Francois Chollet., 2018, DEEP LEARNING PYTHON
   Gnanavel VK, 2015, ADV INTELL SYST, V328, P441, DOI 10.1007/978-3-319-12012-6_48
   Goodfellow I., 2016, ADV NEURAL INFORM PR, P64
   Halder R., 2020, SN Comput. Sci, V1, P1, DOI DOI 10.1007/S42979-020-00207-X
   Hao T, 2017, J VIS COMMUN IMAGE R, V48, P453, DOI 10.1016/j.jvcir.2017.01.019
   Hassner T., 2012, 2012 IEEE COMP SOC C, P1, DOI [DOI 10.1109/CVPRW.2012.6239348, 10.1109/CVPRW.2012.6239348]
   HORN BKP, 1981, P SOC PHOTO-OPT INST, V281, P319
   Colque RVHM, 2017, IEEE T CIRC SYST VID, V27, P673, DOI 10.1109/TCSVT.2016.2637778
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Jalal A, 2019, J ELECTR ENG TECHNOL, V14, P455
   Khan SU, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224963
   Kingma D. P., 2014, arXiv
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li HC, 2020, MEAS CONTROL-UK, V53, P796, DOI 10.1177/0020294020902788
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Maldonado-Mendez C, 2017, P I CONF MEC EL AUT, P43, DOI 10.1109/ICMEAE.2017.9
   Meinhardt-Llopis E, 2013, IMAGE PROCESS ON LIN, V3, P151, DOI 10.5201/ipol.2013.20
   Meng ZH, 2017, LECT NOTES COMPUT SC, V10528, P437, DOI 10.1007/978-3-319-68345-4_39
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Mu CD, 2016, MULTIMEDIA SYST, V22, P275, DOI 10.1007/s00530-015-0456-7
   Nazir S, 2018, PATTERN RECOGN LETT, V103, P39, DOI 10.1016/j.patrec.2017.12.024
   Poynton C, 2012, DIGITAL VIDEO AND HD: ALGORITHMS AND INTERFACES, 2ND EDITION, P1
   Ryoo M.S., 2017, 31 AAAI C ART INT 31 AAAI C ART INT
   Saravanakumar S., 2010, 2010 International Conference on Signal and Image Processing (ICSIP 2010), P79, DOI 10.1109/ICSIP.2010.5697446
   Gracia IS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120448
   Serrano I, 2018, IEEE T IMAGE PROCESS, V27, P4787, DOI 10.1109/TIP.2018.2845742
   Shen MY, 2018, NEUROCOMPUTING, V289, P55, DOI 10.1016/j.neucom.2018.02.012
   Stratou Giota, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P611, DOI 10.1109/FG.2011.5771467
   Su WJ., 2014, Neural Inf Process Syst, V27, P2510
   Sudhakaran S., 2017, 2017 14 IEEE INT C A, P1, DOI DOI 10.1109/AVSS.2017.8078468
   Tripathi RK, 2018, ARTIF INTELL REV, V50, P283, DOI 10.1007/s10462-017-9545-7
   Ullah FUM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19112472
   Nguyen VD, 2014, C IND ELECT APPL, P994, DOI 10.1109/ICIEA.2014.6931308
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang J, 2015, CROWD ANOMALY DETECT
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Xia Q, 2018, LECT NOTES COMPUT SC, V10996, P157, DOI 10.1007/978-3-319-97909-0_17
   Yan Chen, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P95, DOI 10.1109/IBICA.2011.28
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Zhang T, 2017, MULTIMED TOOLS APPL, V76, P1419, DOI 10.1007/s11042-015-3133-0
   Zhang YG, 2014, PHYSIOL REP, V2, DOI 10.14814/phy2.12147
   Zhou P., 2017, J PHYS C SER, V844
   Zhou PP, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0203668
   Zhu SH, 2016, MULTIMED TOOLS APPL, V75, P9445, DOI 10.1007/s11042-015-3122-3
   Zin TT, 2015, 2015 IEEE 4TH GLOBAL CONFERENCE ON CONSUMER ELECTRONICS (GCCE), P519, DOI 10.1109/GCCE.2015.7398694
NR 65
TC 16
Z9 16
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2057
EP 2072
DI 10.1007/s00371-021-02266-4
EA JUL 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000678565400001
DA 2024-07-18
ER

PT J
AU Mofarreh-Bonab, M
   Seyedarabi, H
   Tazehkand, BM
   Kasaei, S
AF Mofarreh-Bonab, Mohammad
   Seyedarabi, Hadi
   Mozaffari Tazehkand, Behzad
   Kasaei, Shohreh
TI 3D hand pose estimation using RGBD images and hybrid deep learning
   networks
SO VISUAL COMPUTER
LA English
DT Article
DE 3D Hand pose estimation; Depth data; Deep learning; Residual networks;
   RGBD images
ID GESTURE RECOGNITION; REGRESSION; MOTION
AB Hand pose estimation is one of the most attractive research areas for image processing. Among the human body parts, hands are particularly important for human-machine interactions. The advent of commercial depth cameras along with the rapid growth of deep learning has made great progress in all image processing fields, especially in hand pose estimation. In this study, using depth data, we introduce two hybrid deep neural networks to estimate 3D hand poses with fewer computations and higher accuracy compared with their counterparts. Due to the fact that the dimensions of data are reduced while passing through successive layers of networks, which causes data to be lost, we use the concept of residual network to compensate this phenomenon. By incorporating data from several views, the estimated poses are more robust in the occlusions. Evaluation results show the superiority of the proposed networks in terms of accuracy and implementation complexity.
C1 [Mofarreh-Bonab, Mohammad; Seyedarabi, Hadi; Mozaffari Tazehkand, Behzad] Univ Tabriz, Fac Elect & Comp Engn, Tabriz, Iran.
   [Kasaei, Shohreh] Sharif Univ Technol, Comp Engn, Tehran, Iran.
C3 University of Tabriz; Sharif University of Technology
RP Seyedarabi, H (corresponding author), Univ Tabriz, Fac Elect & Comp Engn, Tabriz, Iran.
EM seyedarabi@tabrizu.ac.ir
RI mozaffari tazehkand, behzad/CAA-1082-2022; Seyedarabi,
   Hadi/ABD-4603-2021
OI mozaffari tazehkand, behzad/0000-0002-0734-5816; Seyedarabi,
   Hadi/0000-0001-6652-2467
CR Breuer P, 2007, LECT NOTES COMPUT SC, V4418, P247, DOI 10.1007/978-3-540-71457-6_23
   Cai Y., 2020, IEEE T PATTERN ANAL
   Cai YJ, 2018, LECT NOTES COMPUT SC, V11210, P678, DOI 10.1007/978-3-030-01231-1_41
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1
   Chen XH, 2020, NEUROCOMPUTING, V395, P138, DOI 10.1016/j.neucom.2018.06.097
   Chen YL, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106335
   Dan Xu, 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P952, DOI 10.1109/ROBIO.2011.6181410
   de La Gorce M, 2011, IEEE T PATTERN ANAL, V33, P1793, DOI 10.1109/TPAMI.2011.33
   Doosti B., 2019, CoRR
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Fan Q, 2018, VISUAL COMPUT, V34, P1145, DOI 10.1007/s00371-018-1546-2
   Gilbert A, 2019, INT J COMPUT VISION, V127, P381, DOI 10.1007/s11263-018-1118-y
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Jung HY, 2016, LECT NOTES COMPUT SC, V9909, P747, DOI 10.1007/978-3-319-46454-1_45
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   Khamis S, 2015, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2015.7298869
   Kingma D. P., 2014, arXiv
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013
   Nalepa J, 2014, ADV INTEL SOFT COMPU, V242, P79, DOI 10.1007/978-3-319-02309-0_8
   Oberweger M., 2015, Hands deep in deep learning for hand pose estimation, P21, DOI DOI 10.1177/0093650215617505
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Rad Mahdi, 2018, ACCV, P69
   Rasim A, 2013, TEM J, V2, P150
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Spurr A, 2018, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2018.00017
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Supancic JS, 2015, IEEE I CONF COMP VIS, P1868, DOI 10.1109/ICCV.2015.217
   Tan DJ, 2016, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR.2016.605
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Vinh TQ, 2015, PROCEEDINGS OF 2015 2ND NATIONAL FOUNDATION FOR SCIENCE AND TECHNOLOGY DEVELOPMENT CONFERENCE ON INFORMATION AND COMPUTER SCIENCE NICS 2015, P34, DOI 10.1109/NICS.2015.7302218
   von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wu Y, 2005, IEEE T PATTERN ANAL, V27, P1910, DOI 10.1109/TPAMI.2005.233
   Ye Q, 2018, LECT NOTES COMPUT SC, V11214, P817, DOI 10.1007/978-3-030-01249-6_49
   Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21
   Yu HP, 2020, MULTIMED TOOLS APPL, V79, P5743, DOI 10.1007/s11042-019-08493-1
   Zhang CY, 2015, COMPUT VIS IMAGE UND, V139, P29, DOI 10.1016/j.cviu.2015.05.010
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 47
TC 3
Z9 4
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2023
EP 2032
DI 10.1007/s00371-021-02263-7
EA JUL 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000677248300003
DA 2024-07-18
ER

PT J
AU Chen, WT
   Sang, T
   Ma, YT
   Chen, Q
   Xiao, YW
   Pan, ZG
   Yang, XB
AF Chen, Wentao
   Sang, Tian
   Ma, Yitian
   Chen, Qian
   Xiao, Yuwei
   Pan, Zhigeng
   Yang, Xubo
TI Real-time simulation of violent boiling in concentrated sulfuric acid
   dilution
SO VISUAL COMPUTER
LA English
DT Article
DE Violent boiling simulation; Real-time simulation; Fluid simulation;
   Position-based fluids; Virtual experiment
ID SPH
AB Concentrated sulfuric acid dilution is an essential yet hazardous experiment in chemistry education. Incorrect operations can lead to intensive boiling and splattering or even endanger the experiment operator. This paper presents a novel approach to simulate the violent boiling phenomenon in real time and an efficient method to deal with particle penetration caused by fast motion. We introduce the anti-viscosity method to inject momentum into high-temperature particles and capture the chaotic dynamics in the boiling process with small computational overhead. To address the particle penetration issue, we propose a new constraint type called flask constraint which uses radius lookup tables to represent axisymmetric shapes efficiently and projects the particles back into their container when penetration occurs. These methods are integrated into a virtual reality application to simulate the concentrated sulfuric acid dilution experiment and demonstrate the efficiency and effectiveness of our method. Our work improves the capability and stability of particle-based fluid solvers and provides appealing solutions for integrating fluid simulation into interactive applications.
C1 [Chen, Wentao; Sang, Tian; Ma, Yitian; Chen, Qian; Xiao, Yuwei; Yang, Xubo] Shanghai Jiao Tong Univ, Digital ART Lab, Shanghai, Peoples R China.
   [Pan, Zhigeng] Nanjing Univ Sci & Technol, Sch Artificial Intelligence, Nanjing, Peoples R China.
C3 Shanghai Jiao Tong University; Nanjing University of Science &
   Technology
RP Yang, XB (corresponding author), Shanghai Jiao Tong Univ, Digital ART Lab, Shanghai, Peoples R China.
EM vitalight@sjtu.edu.cn; sangtian0820@sjtu.edu.cn; mkochab@sjtu.edu.cn;
   kcsnow@sjtu.edu.cn; xiaoyuwei@sjtu.edu.cn; zgpan@hznu.edu.cn;
   yangxubo@sjtu.edu.cn
RI Xiao, Yuwei/HTS-3369-2023
FU National Key Research and Development Program of China [2018YFB1004902];
   National Natural Science Foundation of China [61772329, 61373085]
FX This work was supported by the National Key Research and Development
   Program of China (2018YFB1004902) and the National Natural Science
   Foundation of China (61772329, 61373085).
CR Akinci N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508395
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Bender J, 2017, IEEE T VIS COMPUT GR, V23, P1193, DOI 10.1109/TVCG.2016.2578335
   Bender Jan, 2017, ser. EG '17, DOI [DOI 10.2312/EGT.20171034, 10.2312/egt.20171034]
   Cummins SJ, 1999, J COMPUT PHYS, V152, P584, DOI 10.1006/jcph.1999.6246
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   Extend Reality Ltd, VRTK VIRT REAL TOOLK
   Goswami P., 2010, P 2010 ACM SIGGRAPHE, P55
   Green S., 2010, NVIDIA Whitepaper, V6, P121
   Ihmsen M, 2014, IEEE T VIS COMPUT GR, V20, P426, DOI 10.1109/TVCG.2013.105
   Kim T, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P27
   Koschier Dan., 2019, Eurographics 2019-Tutorials, DOI DOI 10.2312/EGT.20191035
   Leenson IA, 2004, J CHEM EDUC, V81, P991, DOI 10.1021/ed081p991
   Li Z, 2018, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2018), P142, DOI 10.1145/3198910.3198916
   LUCY LB, 1977, ASTRON J, V82, P1013, DOI 10.1086/112164
   Luo TR, 2020, IEEE T VIS COMPUT GR, V26, P3524, DOI 10.1109/TVCG.2020.3023602
   Macklin M., 2019, P 18 ANN ACM SIGGRAP, P1
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Maier P, 2013, 2013 2ND EXPERIMENT@ INTERNATIONAL CONFERENCE (EXP.AT'13), P164, DOI 10.1109/ExpAt.2013.6703055
   Mihalef Viorel., 2006, S COMPUTER ANIMATION, P317
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   NVIDIA, MAN NVIDIA FLEX 1 2
   Pan JJ, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P548, DOI [10.1109/VR46266.2020.1580817835575, 10.1109/VR46266.2020.00-28]
   Prakash M, 2015, COMPUT GRAPH-UK, V53, P118, DOI 10.1016/j.cag.2015.08.010
   Quarles J, 2019, IEEE T VIS COMPUT GR
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Solenthaler B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531346
   Sussman M, 2003, J COMPUT PHYS, V187, P110, DOI 10.1016/S0021-9991(03)00087-1
   Thomsen J, 1882, THERMOCHEMISCHE UNTE, V2
   Van der Laan W.J., 2009, P 2009 S INT 3D GRAP, P91, DOI [DOI 10.1145/1507149.1507164, 10.1145/1507149.1507164]
   Xiao X, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P557, DOI [10.1109/vr46266.2020.1581028031480, 10.1109/VR46266.2020.00-27, 10.1109/VR46266.2020.1581028031480]
   Yang Y.H, 2016, SIGGRAPH ASIA 2016 T, DOI 10.1145/3005358.3005385
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Zhang S, 2015, PROCEEDINGS - I3D 2015, P61, DOI 10.1145/2699276.2699287
NR 39
TC 1
Z9 1
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2631
EP 2642
DI 10.1007/s00371-021-02208-0
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000672455800002
DA 2024-07-18
ER

PT J
AU Kosarevych, R
   Lutsyk, O
   Rusyn, B
AF Kosarevych, R.
   Lutsyk, O.
   Rusyn, B.
TI Detection of pixels corrupted by impulse noise using random point
   patterns
SO VISUAL COMPUTER
LA English
DT Article
DE Noise detection; Random-valued impulse noise; Random point pattern
ID MEDIAN FILTER; REMOVAL; ALGORITHM; STATISTICS; SPACE
AB This paper presents a novel method for the detection of binary- and random-valued impulsive noise in contaminated images. The noise detector has been developed to classify the certain intensity image pixels as corrupted or uncorrupted based on their relative position. To perform such classification, we regard noise as random points and propose to use the properties of random point patterns that are formed based on a noisy image. Pixels of each image intensity are checked by what type of point pattern they form based on the Clark-Evans test. In the case of random or regular type, pixels of this intensity are classified as noise. For intensity pixels designated as a cluster, the search of isolated points is performed. In case of a homogeneity test fail, they are also classified as noise. The proposed technique can be applied to color images. Extensive simulation experiments indicate that the proposed detection approach outperforms many well-known means.
C1 [Kosarevych, R.; Lutsyk, O.; Rusyn, B.] Natl Acad Sci Ukraine, Phys Mech Inst, Dept Remote Sensing Informat Technol, Lvov, Ukraine.
C3 National Academy of Sciences Ukraine; Karpenko Physico-Mechanical
   Institute, NAS of Ukraine
RP Kosarevych, R (corresponding author), Natl Acad Sci Ukraine, Phys Mech Inst, Dept Remote Sensing Informat Technol, Lvov, Ukraine.
EM kosar2311@gmail.com; olutsyk@ipm.lviv.ua; rusyn@ipm.lviv.ua
RI Rusyn, Bohdan/AED-2795-2022
OI , Rostyslav/0000-0001-9108-0365; Rusyn, Bohdan/0000-0001-8654-2270
CR Abreu E., 2001, NONLINEAR IMAGE PROC, P111, DOI [10.1016/B978-012500451-0/50004-7, DOI 10.1016/B978-012500451-0/50004-7]
   Alajlan N, 2004, SIGNAL PROCESS-IMAGE, V19, P993, DOI 10.1016/j.image.2004.08.003
   ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807
   Awad AS, 2011, IEEE SIGNAL PROC LET, V18, P407, DOI 10.1109/LSP.2011.2154330
   Baddeley A, 2007, LECT NOTES MATH, V1892, P1
   BOVIK AC, 1983, IEEE T ACOUST SPEECH, V31, P1342, DOI 10.1109/TASSP.1983.1164247
   Chan RH, 2004, IEEE SIGNAL PROC LET, V11, P921, DOI 10.1109/LSP.2004.838190
   Chanu PR, 2019, MULTIMED TOOLS APPL, V78, P15375, DOI 10.1007/s11042-018-6925-1
   Chen T, 1999, IEEE T IMAGE PROCESS, V8, P1834, DOI 10.1109/83.806630
   Chen T, 2001, IEEE SIGNAL PROC LET, V8, P1, DOI 10.1109/97.889633
   Chen T, 2001, IEEE T CIRCUITS-II, V48, P784, DOI 10.1109/82.959870
   Chung KL, 2014, SIGNAL IMAGE VIDEO P, V8, P1691, DOI 10.1007/s11760-012-0403-4
   CLARK PJ, 1954, ECOLOGY, V35, P445, DOI 10.2307/1931034
   Crnojevic V, 2004, IEEE SIGNAL PROC LET, V11, P589, DOI 10.1109/LSP.2004.830117
   Deka B, 2017, MULTIMED TOOLS APPL, V76, P6355, DOI 10.1007/s11042-016-3290-9
   Dhiraj Kumar, 2009, International Journal of Recent Trends in Engineering, V1, P87
   Dong YQ, 2007, IEEE T IMAGE PROCESS, V16, P1112, DOI 10.1109/TIP.2006.891348
   Dong YQ, 2007, IEEE SIGNAL PROC LET, V14, P193, DOI 10.1109/LSP.2006.884014
   Garnett R, 2005, IEEE T IMAGE PROCESS, V14, P1747, DOI 10.1109/TIP.2005.857261
   Ghanekar U, 2010, IEEE SIGNAL PROC LET, V17, P1, DOI 10.1109/LSP.2009.2032479
   Hung CC, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.2.023023
   Illian J., 2008, Statistical analysis and modelling of spatial point patterns, DOI 10.1002/9780470725160
   Kamble VM, 2019, VISUAL COMPUT, V35, P5, DOI 10.1007/s00371-017-1437-y
   Khmag A, 2018, VISUAL COMPUT, V34, P575, DOI 10.1007/s00371-017-1362-0
   Khryashchev VV., 2005, IEEE INT C IM PROC, V1, P1
   Kosarevych RY, 2020, 15TH INTERNATIONAL CONFERENCE ON ADVANCED TRENDS IN RADIOELECTRONICS, TELECOMMUNICATIONS AND COMPUTER ENGINEERING (TCSET - 2020), P491, DOI 10.1109/TCSET49122.2020.235481
   Kosarevych RY, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.034521
   Luo WB, 2005, IEICE T FUND ELECTR, VE88A, P2579, DOI 10.1093/ietfec/e88-a.10.2579
   Moller J, 2007, SCAND J STAT, V34, P643, DOI 10.1111/j.1467-9469.2007.00569.x
   NODES TA, 1984, IEEE T COMMUN, V32, P532, DOI 10.1109/TCOM.1984.1096099
   Peng, 2020, IEEE T CYBERN
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Pok G, 2003, IEEE T IMAGE PROCESS, V12, P85, DOI 10.1109/TIP.2002.804278
   Sangave PH, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P715, DOI 10.1109/ISS1.2017.8389266
   Smith T.E., 2016, Lect. Note
   Smolka B, 2008, LECT NOTES COMPUT SC, V5197, P699, DOI 10.1007/978-3-540-85920-8_85
   Srinivasan KS, 2007, IEEE SIGNAL PROC LET, V14, P189, DOI 10.1109/LSP.2006.884018
   Wagas M, 2014, INT BHURBAN C APPL S, P130, DOI 10.1109/IBCAST.2014.6778135
   Wang Z, 1999, IEEE T CIRCUITS-II, V46, P78, DOI 10.1109/82.749102
   Windyga PS, 2001, IEEE T IMAGE PROCESS, V10, P173, DOI 10.1109/83.892455
   Xiong B, 2012, IEEE T IMAGE PROCESS, V21, P1663, DOI 10.1109/TIP.2011.2172804
   Zhao Y., 2007, 2007 2 INT C COMM NE, P651
NR 42
TC 1
Z9 1
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3719
EP 3730
DI 10.1007/s00371-021-02207-1
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000664824400001
DA 2024-07-18
ER

PT J
AU Dai, WT
   Erdt, M
   Sourin, A
AF Dai, Wenting
   Erdt, Marius
   Sourin, Alexei
TI Self-supervised pairing image clustering for automated quality control
SO VISUAL COMPUTER
LA English
DT Article
DE Image clustering; Self-supervised training; Manufacturing applications
ID NETWORK
AB In manufacturing, artificial intelligence is attracting widespread attention to maximize industrial productivity. Image clustering, as a fundamental research direction in unsupervised learning, has been applied in various fields. Since no label information is demanded in clustering, a preliminary analysis of the unlabeled data can be done while saving lots of manpower. In this paper, we propose a novel Self-supervised Pairing Image Clustering network. It predicts clustering results in an end-to-end pair classification network, which is trained excluding any label information. Specifically, we devised a self-supervised pairing module that is able to accurately and efficiently create both types of pairs as training data after exploiting the pair distribution. Cooperating with the pair classification loss function, we added two regularization terms to ensure the clustering results to be unambiguous and close to the real data distribution. In the experiments, our method exceeds most of the state-of-art benchmarks in the publicly available manufacturing datasets-NEU and DAGM. Besides, the method also demonstrates an excellent generalization capability on general public datasets including MNIST, Omniglot, CIFAR-10, and CIFAR-100.
C1 [Dai, Wenting; Sourin, Alexei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Erdt, Marius] Nanyang Technol Univ, Fraunhofer Res Ctr, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Dai, WT (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM daiw0004@e.ntu.edu.sg; marius.erdt@fraunhofer.sg; assourin@ntu.edu.sg
RI Wenting, Dai/KGM-7881-2024
OI Wenting, Dai/0000-0001-7033-2332
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   BRIDLE JS, 1992, ADV NEUR IN, V4, P1096
   Caron Mathilde, 2018, P EUR C COMP VIS ECC, P132, DOI [DOI 10.1007/978-3-030-01264-9_9, 10.48550/arXiv.1807.05520, DOI 10.48550/ARXIV.1807.05520]
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dai WT, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P25, DOI 10.1109/CW49994.2020.00012
   Dilokthanakul N., 2016, Deep unsupervised clustering with gaussian mixture variational autoencoders
   Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   Gdalyahu Y, 2001, IEEE T PATTERN ANAL, V23, P1053, DOI 10.1109/34.954598
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gowda K. C., 1978, Pattern Recognition, V10, P105, DOI 10.1016/0031-3203(78)90018-3
   Hsu CC, 2018, IEEE T MULTIMEDIA, V20, P421, DOI 10.1109/TMM.2017.2745702
   Hu Weihua, 2017, INT C MACH LEARN, P1558
   Ihar V, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P101, DOI 10.1109/CW.2019.00025
   Januzaj E, 2004, LECT NOTES ARTIF INT, V3202, P231
   Jiang Z., P INT JOINT C ART IN, P1965
   Kingma D. P., 2014, arXiv
   Kriegel HP, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P689, DOI 10.1109/ICDM.2005.75
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li FF, 2018, PATTERN RECOGN, V83, P161, DOI 10.1016/j.patcog.2018.05.019
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1805, DOI 10.1109/TKDE.2013.97
   Mujeeb A, 2019, ADV ENG INFORM, V42, DOI 10.1016/j.aei.2019.100933
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Song KC, 2013, APPL SURF SCI, V285, P858, DOI 10.1016/j.apsusc.2013.09.002
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Weimer D, 2016, CIRP ANN-MANUF TECHN, V65, P417, DOI 10.1016/j.cirp.2016.04.072
   Xie JY, 2016, PR MACH LEARN RES, V48
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
NR 31
TC 5
Z9 5
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1181
EP 1194
DI 10.1007/s00371-021-02137-y
EA MAY 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000653632500002
DA 2024-07-18
ER

PT J
AU Roy, K
   Sahay, RR
AF Roy, Kankana
   Sahay, Rajiv Ranjan
TI A robust multi-scale deep learning approach for unconstrained hand
   detection aided by skin segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Adversarial learning; Dilated convolution; Faster-RCNN; Hand detection;
   Skin detection
ID REAL-TIME; RECOGNITION; FACE
AB Robust detection of hands in images at different scales, especially, small-sized hands, has remained a challenge in computer vision. In this work, we design a multi-scale deep learning algorithm to detect hands in unconstrained scenarios as well as frames from driving videos. Our carefully crafted deep learning models have achieved improvement in detection accuracies on several widely used benchmark datasets. We have shown that a set of shallow parallel Faster-RCNNs can lead to higher accuracies than one deep Faster-RCNN since deeper layers cause loss of fine features due to larger strides. We achieve 77.1%, 86.53%, 91.43%, and 74.43% average precision over Oxford hand, VIVA, CVRR and ICD datasets, respectively. Furthermore, the proposed approach can detect hands as small as 15x15 pixels, which was not possible for previous works. Our analysis shows that different context modules (human and skin) can benefit the detection result by reducing false positives. For this purpose, several approaches for segmentation using dilated convolution and adversarial learning are proposed which can isolate skin regions faster and more accurately. The skin detection accuracies obtained using the proposed algorithm over IBTD, Pratheepan, Uchile and HRG datasets are 94.52%, 96.49%, 90.74%, and 98.86%, respectively.
C1 [Roy, Kankana] Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur, W Bengal, India.
   [Sahay, Rajiv Ranjan] Indian Inst Technol, Dept Elect Engn, Kharagpur, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kharagpur
RP Roy, K (corresponding author), Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur, W Bengal, India.
EM kankana.kankana.roy@gmail.com
OI Roy, Kankana/0000-0003-1296-7748
CR Al-Mohair HK, 2015, APPL SOFT COMPUT, V33, P337, DOI 10.1016/j.asoc.2015.04.046
   [Anonymous], ICCV
   [Anonymous], 2016, NIPS Workshop on Adversarial Training
   CAI Z, 2016, EUR C COMP VIS, P354, DOI DOI 10.1007/978-3-319-46493-0_22
   Chakraborty BK, 2015, PROCEEDINGS OF THE 2015 IEEE RECENT ADVANCES IN INTELLIGENT COMPUTATIONAL SYSTEMS (RAICS), P246, DOI 10.1109/RAICS.2015.7488422
   Chen CY, 2017, LECT NOTES COMPUT SC, V10115, P214, DOI 10.1007/978-3-319-54193-8_14
   Chen TY, 2016, INT C PATT RECOG, P615, DOI 10.1109/ICPR.2016.7899702
   Dandan Shan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9866, DOI 10.1109/CVPR42600.2020.00989
   Das N, 2015, IEEE INT C INTELL TR, P2953, DOI 10.1109/ITSC.2015.473
   Deng XM, 2018, IEEE T IMAGE PROCESS, V27, P1888, DOI 10.1109/TIP.2017.2779600
   Do N.H., 2014, P AS C COMP VIS, P19
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gunes H, 2007, J NETW COMPUT APPL, V30, P1334, DOI 10.1016/j.jnca.2006.09.007
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hwang I, 2017, INT CONF ACOUST SPEE, P1273, DOI 10.1109/ICASSP.2017.7952361
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   KAWULOK M, 2013, P IEEE INT C AUT FAC, P1
   Kawulok M, 2014, PATTERN RECOGN LETT, V41, P3, DOI 10.1016/j.patrec.2013.08.028
   Kim Yoon, 2017, arXiv preprint, arXiv170200887, P2
   Kim Y, 2017, IEEE IMAGE PROC, P3919, DOI 10.1109/ICIP.2017.8297017
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lab, 2008, SCUT HCII LAB
   Le THN, 2017, IEEE COMPUT SOC CONF, P1203, DOI 10.1109/CVPRW.2017.159
   Le THN, 2016, INT C PATT RECOG, P573, DOI 10.1109/ICPR.2016.7899695
   Lei Y, 2017, IEEE T MULTIMEDIA, V19, P740, DOI 10.1109/TMM.2016.2638204
   Li C, 2013, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2013.458
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma ZY, 2014, VISUAL COMPUT, V30, P1133, DOI 10.1007/s00371-013-0894-1
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Mohanty A, 2016, SIGNAL PROCESS-IMAGE, V47, P529, DOI 10.1016/j.image.2016.05.019
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   NARASIMHASWAMY S, 2019, ARXIV PREPRINT ARXIV
   NGANLE TH, 2016, IEEE COMPUT SOC CONF, P46
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P912, DOI 10.1109/CVPRW.2013.134
   Ohn-Bar E, 2013, IEEE INT VEH SYM, P1034, DOI 10.1109/IVS.2013.6629602
   Oikonomidis I., 2011, P BRIT MACHINE VISIO, p101.1
   Parikh D, 2012, IEEE T PATTERN ANAL, V34, P1978, DOI 10.1109/TPAMI.2011.276
   Pisharady PK, 2013, INT J COMPUT VISION, V101, P403, DOI 10.1007/s11263-012-0560-5
   Qin HW, 2016, PROC CVPR IEEE, P3456, DOI 10.1109/CVPR.2016.376
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2017, IEEE INT CONF COMP V, P640, DOI 10.1109/ICCVW.2017.81
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Ruiz-del-Solar J, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P463, DOI 10.1109/AFGR.2004.1301576
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu Xiangbo, 2019, IEEE Trans. Pattern Analysis a nd Machine Intelligence
   Sun X., 2017, ARXIV PREPRINT ARXIV
   Tan WR, 2012, IEEE T IND INFORM, V8, P138, DOI 10.1109/TII.2011.2172451
   Tang J., 2019, ARXIV PREPRINT ARXIV
   Vezhnevets V., 2003, GRAPHICON03, P85
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang TY, 2017, PROC INT C TOOLS ART, P1272, DOI 10.1109/ICTAI.2017.00192
   Xue Y, 2018, NEUROINFORMATICS, V16, P383, DOI 10.1007/s12021-018-9377-x
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang YZ, 2015, PROC CVPR IEEE, P400, DOI 10.1109/CVPR.2015.7298637
   Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386
   Yu F., 2015, ARXIV
   Yu ZW, 2006, IEEE IMAGE PROC, P2997, DOI 10.1109/ICIP.2006.312967
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang W, 2020, VISUAL COMPUT, V36, P2433, DOI 10.1007/s00371-020-01955-w
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3
   Zhu Q., 2004, P 12 ANN ACM INT C M, P56
   Zuo HQ, 2017, IEEE SIGNAL PROC LET, V24, P289, DOI 10.1109/LSP.2017.2654803
NR 75
TC 9
Z9 9
U1 3
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2801
EP 2825
DI 10.1007/s00371-021-02157-8
EA MAY 2021
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000651680800002
DA 2024-07-18
ER

PT J
AU Wu, W
   Wu, XT
   Wan, Y
AF Wu, Wen
   Wu, Xiantao
   Wan, Yi
TI Single-image shadow removal using detail extraction and illumination
   estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Shadow removal; Generative adversarial networks;
   Illumination estimation; Multi-scale decomposition
AB Deep learning-based shadow removal methods are frequently hard to obtain a detail-rich and boundary-smoothing shadow removal result. In this work, we propose an illumination-sensitive filter and a multi-task generative adversarial networks architecture to tackle these problems. Firstly, we detect the shadow for the input shadow image and use the illumination-sensitive filter to extract the texture information for generating a coarse image with fewer texture details. Secondly, we conduct illumination estimation for this coarse shadow image to remove the shadow indirectly. Next, we restore the shadow boundary realistically inspired by the idea of image in painting. Finally, we recover the texture details for obtaining the final shadow removal result. Besides, we filter two large benchmark datasets, i.e., SRD and ISTD, to create a Low Error Synthesized Dataset (LESD). The extensive experiments demonstrate that our method can achieve superior performance to state of the arts.
C1 [Wu, Wen] Xinjiang Inst Technol, Sch Informat Engn, Aksu 843100, Peoples R China.
   [Wu, Xiantao] Xinjiang Univ, Sch Informat Sci & Engn, Urumqi 830046, Peoples R China.
   [Wan, Yi] Wenzhou Univ, Sch Elect & Elect Engn, Wenzhou 325035, Peoples R China.
C3 Xinjiang Institute of Technology; Xinjiang University; Wenzhou
   University
RP Wu, W (corresponding author), Xinjiang Inst Technol, Sch Informat Engn, Aksu 843100, Peoples R China.
EM 1119764335@qq.com
RI Wu, Wen/HKW-7234-2023
OI Wu, Wen/0000-0003-0919-3948
FU Natural Science Foundation of Xinjiang Autonomous Region in China
   [2020D01A48]; National Social Science Foundation Western Project
   [20XGL029]
FX This work was supported by the Natural Science Foundation of Xinjiang
   Autonomous Region in China (NO. 2020D01A48) and the National Social
   Science Foundation Western Project (NO. 20XGL029).
CR [Anonymous], 2019, ARXIV PREPRINT ARXIV
   Arbel E, 2011, IEEE T PATTERN ANAL, V33, P1202, DOI 10.1109/TPAMI.2010.157
   Bharath Raj N., 2018, ARXIV PREPRINT ARXIV
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gong H., 2014, P BRIT MACH VIS C BM, P1
   Guo RQ, 2011, PROC CVPR IEEE
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hang Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10353, DOI 10.1109/CVPR42600.2020.01037
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Liu, 2020, P IEEE CVF C COMP VI, P8136, DOI DOI 10.1109/CVPR42600.2020.00816
   Liu F, 2008, LECT NOTES COMPUT SC, V5305, P437
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Mohammadi SM, 2018, IEEE ENG MED BIO, P3501, DOI 10.1109/EMBC.2018.8513009
   Mohan A, 2007, IEEE COMPUT GRAPH, V27, P23, DOI 10.1109/MCG.2007.30
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Sidorov O, 2019, IEEE COMPUT SOC CONF, P1748, DOI 10.1109/CVPRW.2019.00225
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tian JD, 2016, PATTERN RECOGN, V51, P85, DOI 10.1016/j.patcog.2015.09.006
   Vicente TFY, 2015, IEEE I CONF COMP VIS, P3388, DOI 10.1109/ICCV.2015.387
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang TY, 2020, PROC CVPR IEEE, P1877, DOI 10.1109/CVPR42600.2020.00195
   Yagyu S, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P458, DOI 10.1109/GlobalSIP.2015.7418237
   Yang X, 2020, VISUAL COMPUT, V36, P1783, DOI 10.1007/s00371-019-01772-w
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12829
   Zhang SY, 2019, COMPUT VIS MEDIA, V5, P105, DOI 10.1007/s41095-019-0136-1
   Zhang X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392390
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
NR 38
TC 8
Z9 8
U1 2
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1677
EP 1687
DI 10.1007/s00371-021-02096-4
EA MAR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000627699100001
DA 2024-07-18
ER

PT J
AU Hijam, D
   Saharia, S
AF Hijam, Deena
   Saharia, Sarat
TI On developing complete character set Meitei Mayek handwritten character
   database
SO VISUAL COMPUTER
LA English
DT Article
DE Classification models; Handwritten character recognition; Meitei Mayek;
   Optical character recognition; Handwritten character database; Manipuri;
   Convolutional neural network
ID IMAGE DATABASE; RECOGNITION
AB This paper introduces a large-scale Meitei Mayek handwritten character database. It consists of the complete character set of the script. There are a total of 85,124 character images of 55 character classes with 72,330 and 12,794 images in training and test sets, respectively. The present work focuses on collecting the natural handwriting of individuals by carrying out sample collection in two phases: (a) unconstrained handwriting in the form of answer sheets and classroom notes and (b) tabular forms. A total of nearly 500 individuals have contributed in the development of the database. Recognition of the character images in the database is carried out using different feature descriptors with four popular classifiers, namely KNN, Linear Support Vector Classifier, Random Forest and Support Vector Machine. The paper also proposes a convolutional neural network (CNN) model by enhancing a base CNN architecture by optimally tuning the hyperparameters. Experimental results show that the CNN model can be benchmarked against the concerned database with a test accuracy of 95.56%.
C1 [Hijam, Deena; Saharia, Sarat] Tezpur Univ, Dept CSE, Tezpur 784028, Assam, India.
C3 Tezpur University
RP Hijam, D (corresponding author), Tezpur Univ, Dept CSE, Tezpur 784028, Assam, India.
EM deenahij@gmail.com; sarat@tezu.ernet.in
RI hijam, deena/AGY-9622-2022; SAHARIA, SARAT/HLW-9422-2023
OI SAHARIA, SARAT/0000-0001-8026-3174; Hijam, Deena/0000-0002-0552-9857
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Achom A, 2015, 2015 International Symposium on Advanced Computing and Communication (ISACC), P90, DOI 10.1109/ISACC.2015.7377322
   Al-Ma'adeed S, 2002, EIGHTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION: PROCEEDINGS, P485, DOI 10.1109/IWFHR.2002.1030957
   Al-Ohali Y, 2003, PATTERN RECOGN, V36, P111, DOI 10.1016/S0031-3203(02)00064-X
   Alaei A, 2011, PROC INT CONF DOC, P141, DOI 10.1109/ICDAR.2011.37
   [Anonymous], 2016, NIST special database 19 handprinted forms and characters
   [Anonymous], 2013, INT J COMPUTER APPL
   [Anonymous], 2012, ACM Transactions on Asian Language Information Processing, DOI [DOI 10.1145/2090176.2090177, DOI 10.1145/2090176]
   [Anonymous], 2003, REVIVAL CLOSED ACCOU
   [Anonymous], 2012, P INT C ADV EL EL CO
   [Anonymous], 2002, PRIK CIFED
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   Ashiquzzaman Akm., 2017, IEEE INT C IMAGING V, P1, DOI [DOI 10.1109/ICIVPR.2017.7890866, 10.1109/ICIVPR.2017.7890866]
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Chollet F, 2015, KERAS
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   GRAPS A, 1995, IEEE COMPUT SCI ENG, V2, P50, DOI 10.1109/99.388960
   Harit, 2018, WORKSH DOC AN REC, P143
   Hijam D., 2018, 2018 4 INT C COMP CO, P1
   Hijam D, 2020, INT C INT COMP SMART, P61
   Hijam D, 2018, LECT NOTES COMPUT SC, V11278, P207, DOI 10.1007/978-3-030-04021-5_19
   Honggang Zhang, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P286, DOI 10.1109/ICDAR.2009.15
   Inunganbi S., 2019, INT C COMPUT VIS IMA, P307
   INUNGANBI S, 2018, INT J NAT LANG COMPU, V7, P99
   Inunganbi S, 2021, VISUAL COMPUT, V37, P291, DOI 10.1007/s00371-020-01799-4
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kalita S.K., 2018, ADV ELECT COMMUNICAT, P431
   Kim DH, 1996, IEICE T INF SYST, VE79D, P943
   Kim IJ, 2015, INT J DOC ANAL RECOG, V18, P1, DOI 10.1007/s10032-014-0229-4
   Kshetrimayum N, 2010, THESIS U READING REA, P1
   Kumar CJ, 2015, 2015 COMMUNICATION, CONTROL AND INTELLIGENT SYSTEMS (CCIS), P186, DOI 10.1109/CCIntelS.2015.7437905
   Laishram R, 2014, IEEE I C COMP INT CO, P997
   Lawgali A., 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P255
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee G.R., 2019, Journal of Open Source Software, V4, P1237, DOI [10.21105/joss.01237, DOI 10.21105/JOSS.01237]
   Li ZY, 2018, INT J DOC ANAL RECOG, V21, P233, DOI 10.1007/s10032-018-0311-4
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu H, 2018, ADV ENERGY MATER, V8, DOI 10.1002/aenm.201701616
   Manjusha K, 2019, ENG SCI TECHNOL, V22, P637, DOI 10.1016/j.jestch.2018.10.011
   Maring KA, 2014, IJCSIT, V1.2
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Masters D, 2018, ARXIV, DOI 10.48550/arXiv.1804.07612
   Matisoff JamesA., 1996, Languages and dialects of Tibeto-Burman
   Mozaffari S., 2006, 10 INT WORKSH FRONT
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nongmeikapam K., 2017, P 14 INT C NAT LANG, P328
   Nongmeikapam K, 2019, ACM T ASIAN LOW-RESO, V18, DOI 10.1145/3309497
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Prabhu VU., 2019, ARXIV PREPRINT ARXIV
   Reed R., 1999, NEURAL SMITHING SUPE
   Roy S, 2017, PATTERN RECOGN LETT, V90, P15, DOI 10.1016/j.patrec.2017.03.004
   Sagheer MW, 2009, LECT NOTES COMPUT SC, V5716, P538, DOI 10.1007/978-3-642-04146-4_58
   Sarkhel R, 2017, PATTERN RECOGN, V71, P78, DOI 10.1016/j.patcog.2017.05.022
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su TH, 2007, INT J DOC ANAL RECOG, V10, P27, DOI 10.1007/s10032-006-0037-6
   Tangkeshwar T., 2005, SOFT COMPUT, P365
   Thokchom T, 2010, J COMPUT, V5, P1570, DOI 10.4304/jcp.5.10.1570-1574
   Vig, 2017, THESIS THAPAR I ENG
   Williams T, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P233, DOI [10.1109/ICMLA.2016.0046, 10.1109/ICMLA.2016.36]
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
NR 62
TC 5
Z9 5
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 525
EP 539
DI 10.1007/s00371-020-02032-y
EA FEB 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000618961200002
DA 2024-07-18
ER

PT J
AU Hazgui, M
   Ghazouani, H
   Barhoumi, W
AF Hazgui, Mohamed
   Ghazouani, Haythem
   Barhoumi, Walid
TI Genetic programming-based fusion of HOG and LBP features for fully
   automated texture classification
SO VISUAL COMPUTER
LA English
DT Article
DE Genetic programming; Texture classification; Patch detection; Feature
   extraction; Feature fusion
ID FEATURE-EXTRACTION; PATTERNS
AB Classifying texture images relies heavily on the quality of the extracted features. However, producing a reliable set of features is a difficult task that often requires human intervention to select a set of prominent primitives. The process becomes more difficult when it comes to fuse low-level descriptors because of data redundancy and high dimensionality. To overcome these challenges, several approaches use machine learning to automate primitive detection and feature extraction while combining low-level descriptors. Nevertheless, most of these approaches performed the two processes separately while ignoring the correlation between them. In this paper, we propose a genetic programming (GP)-based method that combines the two well-known features of histograms of oriented gradients and local binary patterns. Indeed, a three-layer tree-based binary program is learned using genetic programming for each pair of classes. The three layers incorporate patch detection, feature fusion and classification in the GP optimization process. The feature fusion function is designed to handle different variations, notably illumination and rotation, while reducing dimensionality. The proposed method has been compared, using six challenging collections of images, with multiple domain-expert GP and non-GP methods for binary and multi-class classifications. Results show that the proposed method significantly outperforms or achieves similar performance to relevant methods from the state-of-the-art, even with a limited number of training instances.
C1 [Hazgui, Mohamed; Ghazouani, Haythem; Barhoumi, Walid] Univ Tunis Manar, Inst Super Informat, Res Team Intelligent Syst Imaging & Artificial Vi, Lab Rech Informat Modelisat & Traitement Informat, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.
   [Ghazouani, Haythem; Barhoumi, Walid] Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Carthage 2035, Tunisia.
C3 Universite de Tunis-El-Manar; Universite de Carthage
RP Barhoumi, W (corresponding author), Univ Tunis Manar, Inst Super Informat, Res Team Intelligent Syst Imaging & Artificial Vi, Lab Rech Informat Modelisat & Traitement Informat, 2 Rue Abou Rayhane Bayrouni, Ariana 2080, Tunisia.; Barhoumi, W (corresponding author), Univ Carthage, Ecole Natl Ingn Carthage, 45 Rue Entrepreneurs, Carthage 2035, Tunisia.
EM mohamed.hazgui@fst.utm.tn; haythem.ghazouani@enicar.u-carthage.tn;
   walid.barhoumi@enicarthage.rnu.tn
RI Barhoumi, Walid/C-6576-2014; Ghazouani, Haythem/N-1013-2014
OI Barhoumi, Walid/0000-0003-2123-4992; Ghazouani,
   Haythem/0000-0002-6521-5024; Hazgui, Mohamed/0000-0002-7080-7163
CR Al-Sahaf H, 2012, IEEE C EVOL COMPUTAT
   Al-Sahaf H, 2012, EXPERT SYST APPL, V39, P12291, DOI 10.1016/j.eswa.2012.02.123
   ALSAHAF H, 2013, 2013 28 INT C IM VIS
   Amirolad A, 2016, VISUAL COMPUT, V32, P1633, DOI 10.1007/s00371-016-1220-5
   Atkins D, 2011, IEEE C EVOL COMPUTAT, P238
   Beygelzimer A., PREPRINT
   Bi Y, 2018, IEEE C EVOL COMPUTAT, P392, DOI 10.1109/CEC.2018.8477911
   BIRA C, 2014, P 18 INT C COMP ADV, V2, P432
   Carcagnì P, 2015, SPRINGERPLUS, V4, DOI 10.1186/s40064-015-1427-3
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   DANIELSSON PE, 1980, COMPUT VISION GRAPH, V14, P227, DOI 10.1016/0146-664X(80)90054-4
   Dhara AK, 2016, J DIGIT IMAGING, V29, P466, DOI 10.1007/s10278-015-9857-6
   Dogra J, 2020, VISUAL COMPUT, V36, P875, DOI 10.1007/s00371-019-01698-3
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Douze Matthijs, 2008, TREC VID RETR EV TRE
   Ebner M., 1999, Evolutionary Image Analysis, Signal Processing and Telecommunications. First European Workshops, EvoIASP'99 and EuroEcTel'99. Proceedings, P74
   El Merabet Y, 2019, ENG APPL ARTIF INTEL, V78, P158, DOI 10.1016/j.engappai.2018.11.011
   Espejo PG, 2010, IEEE T SYST MAN CY C, V40, P121, DOI 10.1109/TSMCC.2009.2033566
   Fortin FA, 2012, J MACH LEARN RES, V13, P2171
   Ghazouani H., 2020, P INT C ENG APPL NEU, P593, DOI DOI 10.1007/978-3-030-48791-1_47
   Ghazouani H, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113667
   Ghorbani M., 2015, 2015 10 INT C DIG IN, DOI 10.1109/icdim.2015.7381860
   Goldberg D.E., 1991, FDN GENETIC ALGORITH, V1, P69
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017
   Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Hwang WJ, 1998, ELECTRON LETT, V34, P2062, DOI 10.1049/el:19981427
   Islam B, 2018, 2018 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING (ICECE), P42, DOI 10.1109/ICECE.2018.8636780
   Islam B, 2018, INT CONF ELECTR ENG, P641, DOI 10.1109/CEEICT.2018.8628140
   Jlassi A, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P545, DOI 10.5220/0007400205450552
   Kaiser A, 2019, COMPUT GRAPH FORUM, V38, P167, DOI 10.1111/cgf.13451
   KOZA JR, 1994, STAT COMPUT, V4, P87, DOI 10.1007/BF00175355
   Latif A, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/9658350
   Lensen A, 2016, LECT NOTES COMPUT SC, V9594, P51, DOI 10.1007/978-3-319-30668-1_4
   LUMIA R, 1983, PATTERN RECOGN, V16, P39, DOI 10.1016/0031-3203(83)90006-7
   Mallikarjuna P., KTH TIPS2 DATABASE
   Montana DJ, 1995, EVOL COMPUT, V3, P199, DOI 10.1162/evco.1995.3.2.199
   Nanni L, 2012, EXPERT SYST APPL, V39, P3634, DOI 10.1016/j.eswa.2011.09.054
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rish I., 2001, IJCAI 2001 WORKSH EM, P41
   SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458
   Satorra A, 2001, PSYCHOMETRIKA, V66, P507, DOI 10.1007/BF02296192
   Seetharaman, 2010, 2010 13 INT C INF FU, DOI 10.1109/icif.2010.5711891
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Tabatabaei SM, 2020, VISUAL COMPUT, V36, P967, DOI 10.1007/s00371-019-01704-8
   Trier OD, 1996, PATTERN RECOGN, V29, P641, DOI 10.1016/0031-3203(95)00118-2
   Trujillo L., 2006, P 8 ANN C GEN EV COM
   VARGAS F, 2007, 9 INT C DOC AN REC I, V2
   Nguyen VL, 2017, IEEE INT CONF COMP V, P1238, DOI 10.1109/ICCVW.2017.149
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Yilmaz M. B., 2011, 2011 INT JOINT C BIO
   Zhang JE, 2011, PROC CVPR IEEE, P1393, DOI 10.1109/CVPR.2011.5995678
   Zhang XW, 2020, VISUAL COMPUT, V36, P1141, DOI 10.1007/s00371-019-01723-5
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 60
TC 24
Z9 25
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 457
EP 476
DI 10.1007/s00371-020-02028-8
EA JAN 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605853500004
DA 2024-07-18
ER

PT J
AU Vinolin, V
   Sucharitha, M
AF Vinolin, V.
   Sucharitha, M.
TI Dual adaptive deep convolutional neural network for video forgery
   detection in 3D lighting environment
SO VISUAL COMPUTER
LA English
DT Article
DE Video forgery detection; Deep learning network; Rider optimization
   algorithm; Taylor series; Spliced images
ID DETECTION ALGORITHM
AB Video forgery detection is one of the challenges in this digital era, where the focus is on discovering authenticity. Though there are so many methods available to detect forgeries in the video, there is no method that utilizes illumination-based forgery detection. Hence, this research focuses on establishing the 3D model of the video frame to generate light coefficients in order to detect the forgeries in the video. On the other hand, this paper proposes dual adaptive-Taylor-rider optimization algorithm-based deep convolutional neural network (DA-Taylor-ROA-based DCNN) for video forgery detection, where DCNN is trained using the dual adaptive-Taylor-rider optimization algorithm (DA-TROA) that inherits the adaptive concept and Taylor series within the standard rider optimization algorithm (ROA). For the detection process, the distance-based features from the light coefficients and face objects detected using the Viola-Jones algorithm from the video frames are used. The significance of the method is analyzed using the real images for varying noise conditions based on the performance metrics, such as accuracy, true positive rate, and true negative rate. The percentage improvement of accuracy for proposed DA-Taylor-ROA-based DCNN with respect to Taylor-ROA-Based deep CNN is 4.3626% in the absence of noise, and 1.5985% of accuracy improvement in the presence of speckle noise, respectively.
C1 [Vinolin, V.] Noorul Islam Ctr Higher Educ, Dept Elect & Commun Engn, Kanyakumari, Tamil Nadu, India.
   [Sucharitha, M.] Malla Reddy Coll Engn & Technol, Dept Elect & Commun Engn, Medchal, Telangana, India.
C3 Malla Reddy College of Engineering & Technology
RP Vinolin, V (corresponding author), Noorul Islam Ctr Higher Educ, Dept Elect & Commun Engn, Kanyakumari, Tamil Nadu, India.
EM v.vinolin@gmail.com
RI M, Sucharitha/K-6504-2019; V, Vinolin/AAX-7287-2021
OI M, Sucharitha/0000-0002-2744-4117; V, Vinolin/0000-0002-9708-1884
CR Al-Sanjary OI, 2018, 2018 IEEE SYMPOSIUM ON COMPUTER APPLICATIONS & INDUSTRIAL ELECTRONICS (ISCAIE 2018), P388, DOI 10.1109/ISCAIE.2018.8405504
   Alkawaz MH, 2018, NEURAL COMPUT APPL, V30, P183, DOI 10.1007/s00521-016-2663-3
   [Anonymous], 2015, 2015 7 WORKSHOP HYPE
   Bidokhti A, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ARTIFICIAL INTELLIGENCE AND SIGNAL PROCESSING (AISP), P13, DOI 10.1109/AISP.2015.7123529
   Binu D, 2019, IEEE T INSTRUM MEAS, V68, P2, DOI 10.1109/TIM.2018.2836058
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Delp E, 2009, IEEE SIGNAL PROC MAG, V26, P14, DOI 10.1109/MSP.2008.931089
   Fan W, 2012, EUR SIGNAL PR CONF, P1777
   Ghorbani M.A., 2011, 19 IR C EL ENG, P1
   Guo CC, 2018, PROC SPIE, V10806, DOI 10.1117/12.2502817
   Johnson MK, 2007, LECT NOTES COMPUT SC, V4567, P311, DOI 10.1007/978-3-540-77370-2_21
   Johnson MK, 2008, LECT NOTES COMPUT SC, V5041, P19
   Joshi Vaishali, 2020, International Journal of Information Technology, V12, P273, DOI 10.1007/s41870-018-0268-z
   Kee E, 2010, IEEE INT WORKS INFOR
   Li L., 2012, INT WORKSH DIG WAT, P242
   Mangai SA., 2014, International Journal of Computer Application, V89, P41, DOI DOI 10.5120/15470-4112
   Meena K.B., 2019, DATA ENG APPL, P163, DOI [DOI 10.1007/978-981-13-6351-1_14, 10.1007/978-981-13-6351-1_14]
   Peng B, 2017, IEEE T INF FOREN SEC, V12, P479, DOI 10.1109/TIFS.2016.2623589
   Pomari T, 2018, IEEE IMAGE PROC, P3788, DOI 10.1109/ICIP.2018.8451227
   Rácz A, 2018, J CHEMINFORMATICS, V10, DOI 10.1186/s13321-018-0302-y
   Saddique M, 2019, ADV ELECTR COMPUT EN, V19, P97, DOI 10.4316/AECE.2019.03012
   Singh G, 2019, MULTIMED TOOLS APPL, V78, P11527, DOI 10.1007/s11042-018-6585-1
   Sitara K, 2019, DIGIT INVEST, V30, P1, DOI 10.1016/j.diin.2019.05.001
   Su LC, 2018, MULTIDIM SYST SIGN P, V29, P1173, DOI 10.1007/s11045-017-0496-6
   Su LC, 2015, MULTIMED TOOLS APPL, V74, P6641, DOI 10.1007/s11042-014-1915-4
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Tu FB, 2017, IEEE T VLSI SYST, V25, P2220, DOI 10.1109/TVLSI.2017.2688340
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Zhaohe Zhang, 2020, Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery. Advances in Intelligent Systems and Computing (AISC 1075), P415, DOI 10.1007/978-3-030-32591-6_44
NR 29
TC 16
Z9 16
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2369
EP 2390
DI 10.1007/s00371-020-01992-5
EA NOV 2020
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000592624500002
DA 2024-07-18
ER

PT J
AU Cevik, T
   Sahin, F
AF Cevik, Taner
   Sahin, Fatih
TI A high-discriminative facial recognition method based on shape and
   grey-level appearances using landmark-points
SO VISUAL COMPUTER
LA English
DT Article
DE Facial recognition; Landmark; Shape appearance; Grey-level appearance;
   Partial occlusion; Noise; Illumination; Pose; Expression
ID LOCAL BINARY PATTERNS; FACE RECOGNITION; EXPRESSION RECOGNITION; TEXTURE
   CLASSIFICATION; DESCRIPTOR; MODEL; REPRESENTATION; EIGENFACES;
   ALGORITHM; HISTOGRAM
AB This paper proposes a high-discriminative facial recognition method that fuses shape and grey-level features of faces. Facial landmark-points are characteristic to individuals, hence can be exploited for recognition. Spatial relationships of these landmark-points, namely their Euclidean distances between each other, are included in the feature set. Besides, the mean grey-level values calculated at the vicinity of these landmark-points are also considered as a discriminative factor and incorporated into the feature set. The results of the comprehensive simulations show the remarkable and competitive performance of the proposed method regarding recognition accuracy, as well as robustness against partial occlusion, noise, expression changes and variances in illumination.
C1 [Cevik, Taner] Istanbul Aydin Univ, Dept Software Engn, Istanbul, Turkey.
   [Sahin, Fatih] Istanbul Aydin Univ, Dept Comp Engn, Istanbul, Turkey.
C3 Istanbul Aydin University; Istanbul Aydin University
RP Cevik, T (corresponding author), Istanbul Aydin Univ, Dept Software Engn, Istanbul, Turkey.
EM tanercevik@aydin.edu.tr; fsahin1976@yahoo.com
RI ÇEVİK, TANER/AAD-9934-2022; ÇEVİK, TANER/AAD-9997-2022
OI ÇEVİK, TANER/0000-0001-9653-5832; ÇEVİK, TANER/0000-0001-9653-5832
CR Abudarham N, 2019, COGNITION, V182, P73, DOI 10.1016/j.cognition.2018.09.002
   Adur J, 2014, CANCER INFORM, V13, P67, DOI 10.4137/CIN.S12419
   Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2005, UNCONSTRAINED FACE R
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Barbu T, 2013, ABSTR APPL ANAL, DOI 10.1155/2013/856876
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Çevik N, 2020, PATTERN ANAL APPL, V23, P371, DOI 10.1007/s10044-019-00803-5
   Cevik N, 2019, MULTIMED TOOLS APPL, V78, P15909, DOI 10.1007/s11042-018-6967-4
   Cevik N, 2019, IET IMAGE PROCESS, V13, P1097, DOI 10.1049/iet-ipr.2018.6423
   Chakraborty S, 2017, MULTIMED TOOLS APPL, V76, P1201, DOI 10.1007/s11042-015-3111-6
   Champion I, 2014, IEEE GEOSCI REMOTE S, V11, P5, DOI 10.1109/LGRS.2013.2244060
   Chen X, 2012, FUTURE GENER COMP SY, V28, P212, DOI 10.1016/j.future.2010.11.002
   Choi SI, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/52939
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Cootes T.F., 1992, BRIT MACHINCE VISION, P266, DOI DOI 10.1007/978-1-4471-3201-1_28
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Dahmane M., 2011, IEEE INT C AUT FAC G, P884, DOI DOI 10.1109/FG.2011.5771368
   Dailey MN, 2010, EMOTION, V10, P874, DOI 10.1037/a0020019
   Dan ZP, 2014, OPTIK, V125, P6320, DOI 10.1016/j.ijleo.2014.08.003
   Doshi NP, 2012, INT C PATT RECOG, P2760
   Duan YQ, 2018, IEEE T PATTERN ANAL, V40, P1139, DOI 10.1109/TPAMI.2017.2710183
   Dubey SR, 2017, ARXIV170909518CSCV
   FAN DP, 2019, P IEEE INT C COMP VI
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Guan ZY, 2010, NEUROCOMPUTING, V73, P2744, DOI 10.1016/j.neucom.2010.04.010
   Hafiane A, 2007, LECT NOTES COMPUT SC, V4633, P387
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hosseini H, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-15
   Huang H, 2012, FUTURE GENER COMP SY, V28, P244, DOI 10.1016/j.future.2010.11.005
   Islam M.S., 2014, TRENDS APPL SCI RES, V9, P113, DOI [10.3923/tasr.2014.113.120, DOI 10.3923/tasr.2014.113.120]
   Islam M.S., 2014, J. AI Data Min, V2, P33
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126
   Jafri R, 2009, J INF PROCESS SYST, V5, P41, DOI 10.3745/JIPS.2009.5.2.041
   Jain A, 2000, COMMUN ACM, V43, P90, DOI 10.1145/328236.328110
   JayaMohan C, 2015, INT J COMPUT SCI NET, V15, P59
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Lee JE, 2008, HYDROL PROCESS, V22, P1, DOI 10.1002/hyp.6637
   Lei Z, 2011, IEEE T IMAGE PROCESS, V20, P247, DOI 10.1109/TIP.2010.2060207
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Liu SS, 2014, CHIN CONTR CONF, P4664, DOI 10.1109/ChiCC.2014.6895725
   Liu XG, 2018, FUTURE GENER COMP SY, V80, P653, DOI 10.1016/j.future.2016.07.007
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Luisier F, 2011, IEEE T IMAGE PROCESS, V20, P696, DOI 10.1109/TIP.2010.2073477
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Melendez J, 2008, PATTERN ANAL APPL, V11, P365, DOI 10.1007/s10044-007-0097-3
   Min R., 2011, IEEE INT C AUT FAC G, P442, DOI DOI 10.1109/FG.2011.5771439
   Mohammad T., 2011, 2011 14th International Conference on Computer and Information Technology (ICCIT), P572, DOI 10.1109/ICCITechn.2011.6164854
   Munasinghe M. I. N. P., 2018, 2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS). Proceedings, P423, DOI 10.1109/ICIS.2018.8466510
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Nanni L, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083554
   Ng PE, 2006, IEEE T IMAGE PROCESS, V15, P1506, DOI 10.1109/TIP.2005.871129
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ou X, 2014, INT J PHARMACEUT, V460, P28, DOI 10.1016/j.ijpharm.2013.10.024
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Penev PS, 1996, NETWORK-COMP NEURAL, V7, P477, DOI 10.1088/0954-898X/7/3/002
   Pietikäinen M, 2011, COMPUT IMAGING VIS, V40, P193, DOI 10.1007/978-0-85729-748-8_13
   Qian XM, 2011, PATTERN RECOGN, V44, P2502, DOI 10.1016/j.patcog.2011.03.029
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Wang ZH, 2014, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING - 2013, VOL 4
   Wang ZH, 2011, CHINA PET PROCESS PE, V13, P15
   Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z
   Yang M, 2012, IEEE T INF FOREN SEC, V7, P1738, DOI 10.1109/TIFS.2012.2217332
   Yin QB, 2008, CHINESE J ELECTRON, V17, P646
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang HW, 2018, IEEE T INF FOREN SEC, V13, P2409, DOI 10.1109/TIFS.2018.2800901
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhong FJ, 2013, NEUROCOMPUTING, V119, P375, DOI 10.1016/j.neucom.2013.03.020
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 88
TC 0
Z9 0
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1139
EP 1150
DI 10.1007/s00371-020-01858-w
EA AUG 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000556624200001
DA 2024-07-18
ER

PT J
AU Aboutabit, N
AF Aboutabit, N.
TI A new construction of an image edge detection mask based on
   Caputo-Fabrizio fractional derivative
SO VISUAL COMPUTER
LA English
DT Article
DE Image edge detection; Fractional-order derivative; Caputo-Fabrizio;
   Non-singular kernel
ID DIFFERENTIATION
AB In recent years, the research on image processing based on fractional calculus has attracted much attention. In this work, we proposed a new way to construct an image edge detection mask based on the fractional-order derivative using the Caputo-Fabrizio formulation. The proposed mask was experimented on a large dataset of natural images in both noiseless and noisy situations. In comparison with both classical and fractional edge detectors, the achieved results demonstrated the advantageous performances of the proposed edge detector.
C1 [Aboutabit, N.] Sultan Moulay Slimane Univ, ENSA Khouribga, IPIM Lab, POB 523, Beni Mellal 23000, Morocco.
C3 Sultan Moulay Slimane University of Beni Mellal
RP Aboutabit, N (corresponding author), Sultan Moulay Slimane Univ, ENSA Khouribga, IPIM Lab, POB 523, Beni Mellal 23000, Morocco.
EM n.aboutabit@usms.ma
RI aboutabit, noureddine/AAF-9534-2020; aboutabit, noureddine/JHU-3706-2023
OI aboutabit, noureddine/0000-0002-6984-4776; aboutabit,
   noureddine/0000-0002-6984-4776
CR Amoako-Yirenkyi P, 2016, ADV DIFFER EQU-NY, P1, DOI 10.1186/s13662-016-0946-8
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bento T., 2017, J. Appl. Nonlinear Dyn., V6, P181
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CAPUTO M, 1967, GEOPHYS J ROY ASTR S, V13, P529, DOI 10.1111/j.1365-246X.1967.tb02303.x
   Caputo M., 2015, PROGR FRACT DIFFER A, V1, P73, DOI [DOI 10.12785/PFDA/010201, 10.12785/pfda/010201]
   Chen X, 2012, INT C IM VIS COMP
   DERICHE R, 1990, IEEE T PATTERN ANAL, V12, P78, DOI 10.1109/34.41386
   Gao CB, 2014, INT ARAB J INF TECHN, V11, P223
   He N, 2015, SIGNAL PROCESS, V112, P180, DOI 10.1016/j.sigpro.2014.08.025
   JALAB HA, 2013, DISCRETE DYN NAT SOC
   Lavín-Delgado JE, 2020, CIRC SYST SIGNAL PR, V39, P1419, DOI 10.1007/s00034-019-01200-3
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Mathieu B, 2003, SIGNAL PROCESS, V83, P2421, DOI 10.1016/S0165-1684(03)00194-4
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Pu YF, 2010, IEEE T IMAGE PROCESS, V19, P491, DOI 10.1109/TIP.2009.2035980
   Roberts L., 1963, MACHINE PERCEPTION 3
   Sobel I., 1990, An Isotropic 3x3 Image Gradient Operator, P376, DOI [10.13140/RG.2.1.1912.4965, DOI 10.13140/RG.2.1.1912.4965]
   Yang Q, 2018, ISA T, V82, P163, DOI 10.1016/j.isatra.2017.03.001
   Yang Q, 2016, FRACT CALC APPL ANAL, V19, P1222, DOI 10.1515/fca-2016-0063
   Yang X, 2018, VISUAL COMPUT, V35, P1
   Zhang Y., 2010, J COMPUT INF SYST, V6, P3191
NR 22
TC 9
Z9 9
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1545
EP 1557
DI 10.1007/s00371-020-01896-4
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000553760200004
DA 2024-07-18
ER

PT J
AU Lécuyer, F
   Gouranton, V
   Lamercerie, A
   Reuzeau, A
   Caillaud, B
   Arnaldi, B
AF Lecuyer, Flavien
   Gouranton, Valerie
   Lamercerie, Aurelien
   Reuzeau, Adrien
   Caillaud, Benoit
   Arnaldi, Bruno
TI Unveiling the implicit knowledge, one scenario at a time
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Scenario; Generalization; Authoring
ID VIRTUAL-REALITY
AB When defining virtual reality applications with complex procedures, such as medical operations or mechanical assembly or maintenance procedures, the complexity and the variability of the procedures make the definition of the scenario difficult and time-consuming. Indeed, the variability complicates the definition of the scenario by the experts, and its combinatorics demand a comprehension effort for the developer, which is often out of reach. Additionally, the experts have a hard time explaining the procedures with a sufficient level of details, as they usually forget to mention some actions that are, in fact, important for the application. To ease the creation of scenario, we propose a complete methodology, based on (1) an iterative process composed of: (2) the recording of actions in virtual reality to create sequences of actions and (3) the use of mathematical tools that can generate a complete scenario from a few of those sequences, with (4) graphical visualization of the scenarios and complexity indicators. This process helps the expert to determine the sequences that must be recorded to obtain a scenario with the required variability.
C1 [Lecuyer, Flavien; Gouranton, Valerie; Reuzeau, Adrien; Arnaldi, Bruno] Univ Rennes, IRISA, CNRS, INSA Rennes,Inria, Rennes, France.
   [Lamercerie, Aurelien; Caillaud, Benoit] Univ Rennes, IRISA, CNRS, INRIA, Rennes, France.
C3 Institut National des Sciences Appliquees de Rennes; Centre National de
   la Recherche Scientifique (CNRS); Universite de Rennes; Inria;
   Universite de Rennes; Inria; Centre National de la Recherche
   Scientifique (CNRS)
RP Lécuyer, F (corresponding author), Univ Rennes, IRISA, CNRS, INSA Rennes,Inria, Rennes, France.
EM flavien.lecuyer@irisa.fr
OI Lamercerie, Aurelien/0000-0002-3034-4280; Caillaud,
   Benoit/0000-0002-3234-5033; Bruno, ARNALDI/0000-0002-2868-8826; Lecuyer,
   Flavien/0000-0002-4659-3820
FU "Investing for the Future" program [ANR-16-FRQC-0004 INTROSPECT,
   ANR-10-LABX-07-01]
FX This work is part of the ANR-16-FRQC-0004 INTROSPECT Project and the
   SUNSET project funded by the ANR-10-LABX-07-01 "Investing for the
   Future" program. We would also like to thank people from the Hybrid team
   who provided relevant feedback on this work.
CR Angros R.  Jr., 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P1372
   Badouel E, 1997, THEOR COMPUT SCI, V186, P107, DOI 10.1016/S0304-3975(96)00219-8
   Badouel E, 2015, EATCS SERIES
   Bailenson JN, 2008, J LEARN SCI, V17, P102, DOI 10.1080/10508400701793141
   Brom C, 2007, LECT NOTES COMPUT SC, V4871, P38
   Caillaud B., 2013, APPL REGION THEORY A, P43
   Chan JCP, 2011, IEEE T LEARN TECHNOL, V4, P187, DOI 10.1109/TLT.2010.27
   Chevaillier P., 2012, 2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems, P1, DOI 10.1109/SEARIS.2012.6231174
   Claude G., 2014, ICAT EGVE INT C ARTI, P1
   Cremer J., 1995, ACM Transactions on Modeling and Computer Simulation, V5, P242, DOI 10.1145/217853.217857
   Dijkman R. M., 2007, FORMAL SEMANTICS ANA, P1
   Gerbaud S, 2007, VIRTUAL ENV TRAINING, P116
   Green TRG, 1996, J VISUAL LANG COMPUT, V7, P131, DOI 10.1006/jvlc.1996.0009
   Klopfer E, 2008, ETR&D-EDUC TECH RES, V56, P203, DOI 10.1007/s11423-007-9037-6
   Klopfer E, 2005, CSCL 2005: COMPUTER SUPPORTED COLLABORATIVE LEARNING 2005: THE NEXT 10 YEARS, PROCEEDINGS, P316
   Lamarche F., 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P1309
   Lecuyer F., 2019, ICAT EGVE 2019
   Lécuyer F, 2019, LECT NOTES COMPUT SC, V11542, P329, DOI 10.1007/978-3-030-22514-8_27
   Lugrin JL, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P225
   Mateas M, 2002, IEEE INTELL SYST, V17, P39, DOI 10.1109/MIS.2002.1024751
   Paiva A., 2001, P 6 INT C INTELLIGEN, P129
   Schmitt V., 1996, STACS 96. 13th Annual Symposium on Theoretical Aspects of Computer Science. Proceedings, P517
   Soos M, 2009, LECT NOTES COMPUT SC, V5584, P244, DOI 10.1007/978-3-642-02777-2_24
   van der Aalst WMP, 2011, PROCESS MINING: DISCOVERY, CONFORMANCE AND ENHANCEMENT OF BUSINESS PROCESSES, P1, DOI 10.1007/978-3-642-19345-3
NR 24
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 1951
EP 1963
DI 10.1007/s00371-020-01904-7
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000547790100002
DA 2024-07-18
ER

PT J
AU Tabatabaei, SM
   Chalechale, A
AF Tabatabaei, Sayed Mohamad
   Chalechale, Abdolah
TI Noise-tolerant texture feature extraction through directional
   thresholded local binary pattern
SO VISUAL COMPUTER
LA English
DT Article
DE Local binary pattern; Texture classification; Noise tolerance; Feature
   extraction
ID INTEREST REGIONS; REPRESENTATION; CLASSIFICATION; FACE
AB Local binary pattern (LBP) is a multi-applicable texture descriptor applied in machine vision. Despite its outstanding abilities in revealing textural properties of image, it is sensitive to noise, due to its thresholding mechanism. To make LBP robust against noise, a directional thresholded LBP (DTLBP) is developed in this article which applies the directional neighboring pixels average values for thresholding. Applying this type of thresholding in addition to reducing noise, due to using the information of neighboring pixels with bigger radii, increases efficiency in extracting features. The DTLBP is able to be combined with other descriptors like completed LBP (CLBP) and local ternary pattern (LTP) which improves their functionality against noise. To evaluate the functionality of DTLBP, four known datasets including Outex (TC10), CUReT, UIUC and UMD are tested. Numerous and extensive experiments on these datasets with different kinds of noises indicate this newly developed descriptor's efficiency, with or without incremental white Gaussian and Gaussian blur noises. The proposed descriptor is compared with its available state of the art counterparts. The results show that the combination of DTLBP with CLBP descriptors provide the best classification accuracy in the experiments, which confirms the efficiency and robustness of the proposed descriptor when extracting features from noisy and raw images.
C1 [Tabatabaei, Sayed Mohamad; Chalechale, Abdolah] RAZI Univ, Comp Engn Dept, Kermanshah, Iran.
C3 Razi University
RP Chalechale, A (corresponding author), RAZI Univ, Comp Engn Dept, Kermanshah, Iran.
EM chalechale@razi.ac.ir
RI Chalechale, Abdolah/AIC-3770-2022
OI Chalechale, Abdolah/0000-0002-7217-905X
CR Ahmed Faisal, 2013, Chinese Journal of Engineering, DOI 10.1155/2013/831747
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], LOCAL BINARY PATTERN
   [Anonymous], 2010, Computer Vision Winter Workshop
   Bashar MK, 2014, 2014 IEEE CONFERENCE ON BIOMEDICAL ENGINEERING AND SCIENCES (IECBES), P1, DOI 10.1109/IECBES.2014.7047459
   Brodatz P., 1966, TEXTURES PHOTOGRAPHI
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Doshi NP, 2012, INT C PATT RECOG, P2760
   Fernández A, 2013, J MATH IMAGING VIS, V45, P76, DOI 10.1007/s10851-012-0349-8
   Guo ZH, 2016, IEEE T IMAGE PROCESS, V25, P687, DOI 10.1109/TIP.2015.2507408
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Hafiane A, 2007, LECT NOTES COMPUT SC, V4633, P387
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Heikkilä M, 2006, LECT NOTES COMPUT SC, V4338, P58
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hongliang Jin, 2004, Proceedings. Third International Conference on Image and Graphics, P306
   Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456
   Kandaswamy U, 2011, IEEE T IMAGE PROCESS, V20, P2260, DOI 10.1109/TIP.2010.2101612
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Mallikarjuna PM Fritz., 2006, The kth-tips and kth-tips2 databases
   Nanni L, 2010, EXPERT SYST APPL, V37, P7888, DOI 10.1016/j.eswa.2010.04.048
   Nanni L, 2010, ARTIF INTELL MED, V49, P117, DOI 10.1016/j.artmed.2010.02.006
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojala T., 1999, 11 SCAND C IM AN KAN, P103
   Pietikainen M, 2011, COMPUT IMAGING VIS, V40, P1
   Pietikinen M., 2015, Advances in Independent Component Analysis and Learning Machines, P175
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Rivera AR, 2015, PATTERN RECOGN LETT, V51, P94, DOI 10.1016/j.patrec.2014.08.012
   Ryu J, 2015, IEEE T IMAGE PROCESS, V24, P2254, DOI 10.1109/TIP.2015.2419081
   Shang J, 2017, VISUAL COMPUT, V33, P221, DOI 10.1007/s00371-015-1179-7
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song KC, 2015, J VIS COMMUN IMAGE R, V33, P323, DOI 10.1016/j.jvcir.2015.09.016
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Turan C, 2018, J VIS COMMUN IMAGE R, V55, P331, DOI 10.1016/j.jvcir.2018.05.024
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Xu Y, 2010, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2010.5540217
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhao Y, 2017, ADVANCES IN ENERGY AND ENVIRONMENT RESEARCH, P345
NR 47
TC 10
Z9 10
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 967
EP 987
DI 10.1007/s00371-019-01704-8
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100008
DA 2024-07-18
ER

PT J
AU Liang, DD
   Liang, HG
   Yu, ZB
   Zhang, YP
AF Liang, Dandan
   Liang, Huagang
   Yu, Zhenbo
   Zhang, Yipu
TI Deep convolutional BiLSTM fusion network for facial expression
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Deep network; BiLSTM; Spatial-temporal
   features
AB Deep learning algorithms have shown significant performance improvements for facial expression recognition (FER). Most deep learning-based methods, however, focus more attention on spatial appearance features for classification, discarding much useful temporal information. In this work, we present a novel framework that jointly learns spatial features and temporal dynamics for FER. Given the image sequence of an expression, spatial features are extracted from each frame using a deep network, while the temporal dynamics are modeled by a convolutional network, which takes a pair of consecutive frames as input. Finally, the framework accumulates clues from fused features by a BiLSTM network. In addition, the framework is end-to-end learnable, and thus temporal information can be adapted to complement spatial features. Experimental results on three benchmark databases, CK+, Oulu-CASIA and MMI, show that the proposed framework outperforms state-of-the-art methods.
C1 [Liang, Dandan; Liang, Huagang; Zhang, Yipu] Changan Univ, Sch Elect & Control Engn, Middle South Second Ring Rd, Xian 710064, Shanxi, Peoples R China.
   [Yu, Zhenbo] Nanjing Univ Informat Sci & Technol, Sch Informat & Control, B DAT, 219 Nanjin Rd, Nanjing 210044, Jiangsu, Peoples R China.
C3 Chang'an University; Nanjing University of Information Science &
   Technology
RP Liang, DD (corresponding author), Changan Univ, Sch Elect & Control Engn, Middle South Second Ring Rd, Xian 710064, Shanxi, Peoples R China.
EM 1261523826@qq.com
CR Afshar S, 2016, IEEE COMPUT SOC CONF, P1517, DOI 10.1109/CVPRW.2016.189
   Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   [Anonymous], INT J SOFT COMPUTING
   [Anonymous], 2017, Hierarchical representations for efficient architecture search
   [Anonymous], INT C PATT REC
   [Anonymous], CORR
   Bargal SA, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433, DOI 10.1145/2993148.2997627
   Chi J, 2014, VISUAL COMPUT, V30, P649, DOI 10.1007/s00371-014-0960-3
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guo YM, 2012, LECT NOTES COMPUT SC, V7573, P631, DOI 10.1007/978-3-642-33709-3_45
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kacem A, 2017, IEEE I CONF COMP VIS, P3199, DOI 10.1109/ICCV.2017.345
   Kahou SE, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2818346.2830596
   Khorrami P, 2016, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2016.7532431
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   LeCun Y., 1990, ADV NEURAL INFORM PR, P396
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Sanin A, 2013, IEEE WORK APP COMP, P103, DOI 10.1109/WACV.2013.6475006
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Sikka K, 2016, PROC CVPR IEEE, P5580, DOI 10.1109/CVPR.2016.602
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinha A, 2015, 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION ICCUBEA 2015, P1, DOI 10.1109/ICCUBEA.2015.11
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Valstar M.F., 2015, P 2015 IEEE INT C WO, P1
   Valstar MF, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, pJ65
   Ye B, 2015, MACHINERY, MATERIALS SCIENCE AND ENERGY ENGINEERING, P625
   Yi Dong, 2014, ARXIV14117923
   Yu Z, 2017, VISUAL COMPUT, V34, P1
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
NR 44
TC 47
Z9 50
U1 3
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 499
EP 508
DI 10.1007/s00371-019-01636-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500005
DA 2024-07-18
ER

PT J
AU Agrawal, A
   Mittal, N
AF Agrawal, Abhinav
   Mittal, Namita
TI Using CNN for facial expression recognition: a study of the effects of
   kernel size and number of filters on accuracy
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; CNN; FER-2013
AB Facial expression recognition is a challenging problem in image classification. Recently, the use of deep learning is gaining importance in image classification. This has led to increased efforts in solving the problem of facial expression recognition using convolutional neural networks (CNNs). A significant challenge in deep learning is to design a network architecture that is simple and effective. A simple architecture is fast to train and easy to implement. An effective architecture achieves good accuracy on the test data. CNN architectures are black boxes to us. VGGNet, AlexNet and Inception are well-known CNN architectures. These architectures have strongly influenced CNN model designs for new datasets. Almost all CNN models known to achieve high accuracy on facial expression recognition problem are influenced by these architectures. This work tries to overcome this limitation by using FER-2013 dataset as starting point to design new CNN models. In this work, the effect of CNN parameters namely kernel size and number of filters on the classification accuracy is investigated using FER-2013 dataset. Our major contribution is a thorough evaluation of different kernel sizes and number of filters to propose two novel CNN architectures which achieve a human-like accuracy of 65% (Goodfellow et al. in: Neural information processing, Springer, Berlin, pp 117-124, 2013) on FER-2013 dataset. These architectures can serve as a basis for standardization of the base model for the much inquired FER-2013 dataset.
C1 [Agrawal, Abhinav; Mittal, Namita] MNIT, Dept CSE, Jaipur 302017, Rajasthan, India.
C3 National Institute of Technology (NIT System); Malaviya National
   Institute of Technology Jaipur
RP Agrawal, A (corresponding author), MNIT, Dept CSE, Jaipur 302017, Rajasthan, India.
EM abhinav0653@gmail.com
RI Mittal, Namita/AAL-3336-2020
OI Mittal, Namita/0000-0001-6886-9974; Agrawal, Abhinav/0000-0001-7053-0144
CR [Anonymous], FACIAL EXPRESSION RE
   [Anonymous], ARXIV13060239 CORR
   [Anonymous], 2016, FACIAL EXPRESSION RE
   [Anonymous], 2017, REAL TIME CONVOLUTIO
   [Anonymous], 9 INT C KNOWL SYST E
   [Anonymous], 2015, Recent Advances in Convolutional Neural Networks
   Gogic I, 2020, VISUAL COMPUT, V36, P97, DOI 10.1007/s00371-018-1585-8
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Shan, 2018, arXiv:1804.08348
   Liu K, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P163, DOI 10.1109/CW.2016.34
   Ruder S., 2016, ARXIV
   Shin M., 2016, 2016 25 IEEE INT S R
   Simonyan K., 2014, CORR
   Springenberg J.T., 2014, STRIVING SIMPLICITY
NR 15
TC 117
Z9 126
U1 4
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 405
EP 412
DI 10.1007/s00371-019-01630-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300013
DA 2024-07-18
ER

PT J
AU Wei, LX
   Cui, W
   Hu, ZY
   Sun, H
   Hou, SJ
AF Wei, Lixin
   Cui, Wei
   Hu, Ziyu
   Sun, Hao
   Hou, Shijie
TI A single-shot multi-level feature reused neural network for object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Deep convolutional neural network; Feature reused
AB Recent years have witnessed the significant progress in object detection using deep convolutional neutral networks. However, there are few object detectors achieving high precision with low computational cost. In this paper, a novel and lightweight framework named multi-level feature reused detector (MFRDet) is proposed, which can reach a better accuracy than two-stage methods. It also can maintain comparable high efficiency of one-stage methods without employing very deep convolution neural networks as most modern detectors do. The proposed framework is suitable for reusing information included in deep and shallow feature maps, by which property the detection precision can be higher. For the Pascal VOC2007 test set trained with VOC 2007 and VOC 2012 training sets, the proposed MFRDet with the input size of 300 x 300 can achieve 80.7% mAP at the speed of 62.5 FPS. As for a high-resolution input version, MFRDet can obtain 82.0% mAP with the speed of 37.0 FPS using single Nvidia Tesla P100 GPU. The proposed framework shows the state-of-the-art mAP with high FPS, which is better than most of other modern object detectors.
C1 [Wei, Lixin; Cui, Wei; Hu, Ziyu; Sun, Hao; Hou, Shijie] Yanshan Univ, Inst Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
C3 Yanshan University
RP Cui, W (corresponding author), Yanshan Univ, Inst Elect Engn, Qinhuangdao 066004, Hebei, Peoples R China.
EM wlx2000@ysu.edu.cn; wey@stumail.ysu.edu.cn
RI Hu, Ziyu/AAH-2396-2020; Wei, Lixin/AFJ-0797-2022
FU Youth Foundation of Hebei Province (CN) [E2018203162]
FX This research is supported by the Youth Foundation of Hebei Province
   (CN) No. E2018203162.
CR Abdellatef E, 2020, VISUAL COMPUT, V36, P1097, DOI 10.1007/s00371-019-01715-5
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   [Anonymous], COMPUT VIS PATTERN R
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Dai JF, 2016, ADV NEUR IN, V29
   Demirel B., 2018, IEEE C COMP VIS PATT
   Everingham M., 2007, The PASCAL Visual Object Classes Chal- lenge (VOC) Results, DOI 10.1007/s11263-009-0275-4
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Heng F., 2016, IEEE C COMP VIS PATT
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hyungtae L., 2017, IEEE INT C COMP VIS
   Jiahui C., 2019, VISUAL COMPUT, P1
   Jisoo J., 2017, IEEE C COMP VIS PATT
   Joseph Redmon, 2018, COMPUTER VISION PATT
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li T, 2014, VISUAL COMPUT, V30, P59, DOI 10.1007/s00371-013-0780-x
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romeraparedes B., INT C MACH ICML
   Schneider L, 2017, LECT NOTES COMPUT SC, V10269, P98, DOI 10.1007/978-3-319-59126-1_9
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Shengjing T., 2019, VISUAL COMPUT, P1
   Shrivastava A, 2016, LECT NOTES COMPUT SC, V9905, P330, DOI 10.1007/978-3-319-46448-0_20
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2019, IEEE T IMAGE PROCESS, V28, P436, DOI 10.1109/TIP.2018.2869721
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yazhou Y., 2016, AUTOMATIC IMAGE DATA, P1
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   ZhaoYue Z., 2018, RECOGNIZE ACTIONS DI, P6566
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Ziming Z., 2015, IEEE C COMP VIS PATT
NR 58
TC 25
Z9 26
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 133
EP 142
DI 10.1007/s00371-019-01787-3
EA JAN 2020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QE3UO
UT WOS:000574080900001
DA 2024-07-18
ER

PT J
AU Luo, LK
   Wang, XF
   Hu, SQ
   Hu, X
   Zhang, HL
   Liu, YH
   Zhang, JM
AF Luo, Lingkun
   Wang, Xiaofang
   Hu, Shiqiang
   Hu, Xing
   Zhang, Huanlong
   Liu, Yaohua
   Zhang, James
TI A unified framework for interactive image segmentation via Fisher rules
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive segmentation; Weakly supervised; Sparse reconstruction;
   Latent subspace; Superpixel classification
ID LEVEL SET IMAGE; RECONSTRUCTION
AB Interactive image segmentation has been an active research topic in image processing and computer graphics. One of its appealing advantage is the optimization of human feedback and interactions to generate user-desired results. Segmentation results of most previous methods usually depend on the quality and quantity of the user input. In this paper, we propose an algorithm to solve one important challenge arising from inputs with bad or limited quality. Our work is notably different from previous methods. First, the weakly interactive image segmentation is formulated and deduced in theory, then we propose to reconstruct enough samples via sparse reconstruction to enhance the robustness to weakly interactive labels. More importantly, we leverage interactive labels to extract a latent subspace which jointly optimizes multiclass classification and binary classification based on fisher rules. Numerous experiments are conducted on MSRC (Ning et al. in Interact Imaging Vis Pattern Recognit 43(2):445-456, 2010) and KIM (Kim et al. in: 2010 IEEE computer society conference on computer vision and pattern recognition, 2010) database. The results demonstrate effectiveness and efficiency of our method.
C1 [Luo, Lingkun; Hu, Shiqiang; Hu, Xing; Zhang, Huanlong] Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Luo, Lingkun; Wang, Xiaofang] Ecole Cent Lyon, LIRIS, UMR 5205, CNRS, 36 Ave Guy de Collongue, F-69130 Ecully, France.
   [Liu, Yaohua] Univ Sci & Technol China, Dept Automat, Hefei 230026, Peoples R China.
   [Zhang, James] Deakin Univ, Inst Intelligent Syst Res & Innovat, Melbourne, Vic, Australia.
C3 Shanghai Jiao Tong University; Ecole Centrale de Lyon; Centre National
   de la Recherche Scientifique (CNRS); Institut National des Sciences
   Appliquees de Lyon - INSA Lyon; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS; Deakin University
RP Hu, SQ (corresponding author), Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM lolinkun@gmail.com; xiaofang.wang@ec-lyon.fr; sqhu@sjtu.edu.cn;
   vcifer@mail.ustc.edu.cn; mailjameszhang@gmail.com
RI Zhang, James J/B-2343-2017; zhang, huanlong/AAE-3160-2020; Lu,
   Rui/KCJ-8212-2024
CR Bai JJ, 2014, PROC CVPR IEEE, P392, DOI 10.1109/CVPR.2014.57
   Bai XM, 2007, J IND ECOL, V11, P1, DOI 10.1162/jie.2007.1296
   Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deeba F, 2016, IEEE IJCNN, P4650, DOI 10.1109/IJCNN.2016.7727810
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Donoser M, 2007, PROC CVPR IEEE, P2000
   Duchenne O., 2008, IEEE COMPUTER VISION, P1, DOI DOI 10.1109/CVPR.2008.4587419
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Friedland G., 2005, 7 IEEE INT S MULT IS
   Ham B, 2013, IEEE T IMAGE PROCESS, V22, P2574, DOI 10.1109/TIP.2013.2253479
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Jian M, 2016, IEEE T IMAGE PROCESS, V25, P1301, DOI 10.1109/TIP.2016.2518480
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Li H., 2015, COMPUTATIONAL VISUAL, V1, P183
   Li KQ, 2015, IEEE T MULTIMEDIA, V17, P994, DOI 10.1109/TMM.2015.2433795
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Luo LK, 2017, J VIS COMMUN IMAGE R, V43, P138, DOI 10.1016/j.jvcir.2016.12.012
   Michailidis GT, 2017, VISUAL COMPUT, V33, P1347, DOI 10.1007/s00371-016-1230-3
   Mille J, 2015, INT J COMPUT VISION, V112, P1, DOI 10.1007/s11263-014-0751-3
   Mumford D., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P22
   Ning JF, 2010, PATTERN RECOGN, V43, P445, DOI 10.1016/j.patcog.2009.03.004
   Pan RJ, 2016, VISUAL COMPUT, V32, P601, DOI 10.1007/s00371-015-1076-0
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Sourati J, 2014, IEEE T IMAGE PROCESS, V23, P3057, DOI 10.1109/TIP.2014.2325783
   Subr K, 2013, COMPUT GRAPH FORUM, V32, P41, DOI 10.1111/cgf.12024
   Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wang LF, 2014, PATTERN RECOGN, V47, P1917, DOI 10.1016/j.patcog.2013.11.014
   Wang T, 2016, IEEE T MULTIMEDIA, V18, P2358, DOI 10.1109/TMM.2016.2600441
   Wang T, 2016, PATTERN RECOGN, V55, P28, DOI 10.1016/j.patcog.2016.01.018
   Wang TH, 2015, NEUROCOMPUTING, V168, P55, DOI 10.1016/j.neucom.2015.06.014
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wang XF, 2015, IEEE T IMAGE PROCESS, V24, P1399, DOI 10.1109/TIP.2015.2397313
   Welling M., 2005, Technical Report
   Wu J, 2014, IEEE C COMP VIS PATT
   Wu LY, 2012, SYNLETT, P1529, DOI 10.1055/s-0031-1291042
   Xiao CX, 2013, VISUAL COMPUT, V29, P27, DOI 10.1007/s00371-012-0672-5
   Yang WX, 2010, IEEE T IMAGE PROCESS, V19, P2470, DOI 10.1109/TIP.2010.2048611
   Yao JC, 2017, VISUAL COMPUT, V33, P179, DOI 10.1007/s00371-015-1171-2
   Yin SB, 2014, PATTERN RECOGN, V47, P2894, DOI 10.1016/j.patcog.2014.03.009
   Zemene E, 2016, LECT NOTES COMPUT SC, V9912, P278, DOI 10.1007/978-3-319-46484-8_17
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012
   Zhu SC, 1996, IEEE T PATTERN ANAL, V18, P884, DOI 10.1109/34.537343
NR 48
TC 8
Z9 8
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2019
VL 35
IS 12
BP 1869
EP 1882
DI 10.1007/s00371-018-1580-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KI6XQ
UT WOS:000511494300014
DA 2024-07-18
ER

PT J
AU Qi, BB
   Pang, MY
AF Qi, Binbin
   Pang, Mingyong
TI An enhanced sweep and prune algorithm for multi-body continuous
   collision detection
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 17th International Conference on Cyberworlds (CW)
CY OCT 03-05, 2018
CL Fraunhofer Singapore, Singapore, SINGAPORE
SP Nanyang Technol Univ Singapore, ACM SIGGRAPH, Eurograph Assoc, Int Federat Informat Proc, ACM
HO Fraunhofer Singapore
DE Collision detection; Event-driven; Multi-body collision; Event blocking;
   Priority queue; Virtual reality
AB Multi-body collision detection is a key and important technology in societies of computer graphics, system simulation, virtual reality, etc. and has been widely used in various graphics applications. To deal with the collision detection problem in large-scale multi-body scenes, we in this paper proposed a robust and efficient algorithm based on the kinetic "sweep and prune" technique and the event-driven mechanism. Our algorithm first culls redundant detection calculations in finding object pairs that do not collide in the scene with very large number of moving objects and then automatically generates events to predict the object collisions, which probably take place in near future. All the events are pushed into an optimized priority queue to drive the proposed algorithm. By introducing a hybrid bounding box hierarchy in the event processing process, the algorithm can detect the positions where the object pairs collide. Based on our finding that event blocking is an important factor affecting the robustness of the algorithm, we further propose several techniques to alarm blockings to be occurred or relieve the system from blocking state. Experimental results show that our algorithm has good stability and robustness, and it can improve the operating efficiency of multi-body continuous collision detections in an efficient way.
C1 [Qi, Binbin; Pang, Mingyong] Nanjing Normal Univ, Inst EduInfo Sci & Engn, Nanjing 210097, Jiangsu, Peoples R China.
   [Qi, Binbin] Nanjing Univ, Sch Informat Management, Nanjing 210023, Jiangsu, Peoples R China.
C3 Nanjing Normal University; Nanjing University
RP Pang, MY (corresponding author), Nanjing Normal Univ, Inst EduInfo Sci & Engn, Nanjing 210097, Jiangsu, Peoples R China.
EM panion@netease.com
FU National Natural Science Foundation of China [41631175]; Key Project of
   the Ministry of Education for the 13th Five-year Plan of National
   Education Science of China [DCA170302]; Social Science Foundation of
   Jiangsu Province of China [15TQB005]; Priority Academic Program
   Development of Jiangsu Higher Education Institutions of China
FX The work in this paper was supported by the National Natural Science
   Foundation of China [Grand No. 41631175], the Key Project of the
   Ministry of Education for the 13th Five-year Plan of National Education
   Science of China [Grand No. DCA170302], the Social Science Foundation of
   Jiangsu Province of China [Grand No. 15TQB005] and the Priority Academic
   Program Development of Jiangsu Higher Education Institutions of China.
CR AGARWAL PK, 1993, J ALGORITHM, V15, P229, DOI 10.1006/jagm.1993.1040
   BARAFF D, 1995, IEEE COMPUT GRAPH, V15, P63, DOI 10.1109/38.376615
   Basch J, 2004, COMP GEOM-THEOR APPL, V27, P211, DOI 10.1016/j.comgeo.2003.11.001
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   Capannini G, 2018, IEEE T VIS COMPUT GR, V24, P2064, DOI 10.1109/TVCG.2017.2709313
   Chang JW, 2010, COMPUT AIDED DESIGN, V42, P50, DOI 10.1016/j.cad.2009.04.010
   Cohen J. D., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P189, DOI 10.1145/199404.199437
   Coming DS, 2008, IEEE T VIS COMPUT GR, V14, P1, DOI 10.1109/TVCG.2007.70405
   Coming DS, 2006, COMPUT GRAPH-UK, V30, P439, DOI 10.1016/j.cag.2006.02.014
   Corrales JA, 2011, ROBOT CIM-INT MANUF, V27, P177, DOI 10.1016/j.rcim.2010.07.005
   DOBKIN DP, 1985, J ALGORITHM, V6, P381, DOI 10.1016/0196-6774(85)90007-0
   Echegaray G, 2012, VIRTUAL REAL-LONDON, V16, P205, DOI 10.1007/s10055-011-0199-5
   Fredman ML, 1986, ALGORITHMICA, V1, P111, DOI 10.1007/BF01840439
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Han D, 2018, ROBOT CIM-INT MANUF, V49, P98, DOI 10.1016/j.rcim.2017.05.013
   Huh SB, 2006, VISUAL COMPUT, V22, P434, DOI 10.1007/s00371-006-0019-1
   Jia XH, 2011, COMPUT AIDED GEOM D, V28, P164, DOI 10.1016/j.cagd.2011.01.004
   Lee Y, 2016, COMPUT AIDED DESIGN, V78, P14, DOI 10.1016/j.cad.2016.05.012
   Liu F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866180
   Montanari M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083724
   Moser H, 2014, DISTRIB COMPUT, V27, P203, DOI 10.1007/s00446-013-0204-1
   Oh S, 2008, ADVANCES IN COGNITIVE NEURODYNAMICS, PROCEEDINGS, P817, DOI 10.1007/978-1-4020-8387-7_142
   Paul DV, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON DATA MINING AND ADVANCED COMPUTING (SAPIENCE), P113, DOI 10.1109/SAPIENCE.2016.7684154
   Pettie S, 2005, ANN IEEE SYMP FOUND, P174, DOI 10.1109/SFCS.2005.75
   Rasool S, 2016, VISUAL COMPUT, V32, P1
   Tang M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661237
   TARJAN RE, 1985, SIAM J ALGEBRA DISCR, V6, P306, DOI 10.1137/0606031
   Tracy DJ, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P191, DOI 10.1109/VR.2009.4811022
   Tang D, 2014, COMPUT AIDED DESIGN, V51, P1, DOI 10.1016/j.cad.2014.02.001
   Tropp O, 2008, INT J SHAPE MODEL, V12, P159
   Tu CQ, 2009, PROCEEDINGS OF THE FIRST INTERNATIONAL WORKSHOP ON EDUCATION TECHNOLOGY AND COMPUTER SCIENCE, VOL I, P331, DOI 10.1109/ETCS.2009.82
   Vemuri BC, 1998, COMPUT GRAPH FORUM, V17, P121, DOI 10.1111/1467-8659.00233
   Wang XL, 2018, COMPUT GRAPH FORUM, V37, P227, DOI 10.1111/cgf.13356
   Wei Ying-Mei, 2001, Journal of Software, V12, P1056
   Weller R, 2017, COMPUT GRAPH FORUM, V36, P131, DOI 10.1111/cgf.13113
   Weller Rene., 2013, New Geometric Data Structures for Collision Detection and Haptics
   Wong SK, 2015, VISUAL COMPUT, V31, P377, DOI 10.1007/s00371-014-0933-6
   Wong TH, 2014, VISUAL COMPUT, V30, P729, DOI 10.1007/s00371-014-0954-1
   Wong TH, 2012, VISUAL COMPUT, V28, P829, DOI 10.1007/s00371-012-0706-z
   Zhang XY, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239466
   Zhang XY, 2006, VISUAL COMPUT, V22, P749, DOI 10.1007/s00371-006-0060-0
NR 41
TC 3
Z9 5
U1 0
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1503
EP 1515
DI 10.1007/s00371-019-01718-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JD5IZ
UT WOS:000490018000002
DA 2024-07-18
ER

PT J
AU Sharma, S
   Mehra, R
AF Sharma, Shallu
   Mehra, Rajesh
TI Effect of layer-wise fine-tuning in magnification-dependent
   classification of breast cancer histopathological image
SO VISUAL COMPUTER
LA English
DT Article
DE Transfer learning; Layer-wise fine-tuning; Convolutional neural network;
   Magnification factor; Histopathological images; Breast cancer
ID FEATURES
AB A large and balanced training data are the foremost requirement in proper convergence of a deep convolutional neural network (CNN). Medical data always suffer from the problem of unbalancing and inadequacy that makes it difficult to train CNN from scratch. It is known that the transfer learning approach provides great potential to deal with inadequate dataset besides the benefit of faster training. The efficient transfer of knowledge from natural images to histopathological images has yet to be achieved. In view of the foregoing, an attempt has been made toward the classification of BreakHis dataset using pre-trained 'AlexNet' model with a suitable fine-tuning approach. The effective depth of fine-tuning is also determined at different levels of magnification (40x, 100x, 200x and 400x). The experimental trials conform that the moderate level of fine-tuning is an optimum choice for the classification of magnification-dependent histology images in contrast to the shallow and deep tuning of the pre-trained network which in turn depends on the size and relative distribution of a dataset. Additionally, the layer-wise fine-tuning approach provides a neck-to-neck performance with the latest state-of-the-art developments.
C1 [Sharma, Shallu] Natl Inst Tech Teachers Training & Res, Chandigarh, India.
   [Mehra, Rajesh] Natl Inst Tech Teachers Training & Res, Elect & Commun Engn Dept, Chandigarh, India.
C3 National Institute of Technical Teachers Training & Research,
   Chandigarh; National Institute of Technical Teachers Training &
   Research, Chandigarh
RP Sharma, S (corresponding author), Natl Inst Tech Teachers Training & Res, Chandigarh, India.
EM shallu.ece@nitttrchd.ac.in
RI Sharma, Shallu/AAM-8885-2020; Mehra, Rajesh/AAX-4908-2020
OI Sharma, Shallu/0000-0002-9445-4251; 
CR Alhindi TJ, 2018, IEEE IJCNN
   Araújo T, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177544
   Arevalo J, 2015, IEEE ENG MED BIO, P797, DOI 10.1109/EMBC.2015.7318482
   Azizpour Hossein., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P36
   BAR Y, MED IMAGING 2015 COM
   Bardou D, 2018, IEEE ACCESS, V6, P24680, DOI 10.1109/ACCESS.2018.2831280
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   CARNEIRO G, INT C MED IM COMP CO, P652
   Chan A, 2016, ROY SOC OPEN SCI, V3, DOI 10.1098/rsos.160558
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Gao MC, 2018, COMP M BIO BIO E-IV, V6, P1, DOI 10.1080/21681163.2015.1124249
   Han ZY, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04075-z
   Margeta J, 2017, COMP M BIO BIO E-IV, V5, P339, DOI 10.1080/21681163.2015.1061448
   Motlagh N.H., 2018, Breast Cancer Histopathological Image Classification: A Deep Learning Approach
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sathish D, 2019, VISUAL COMPUT, V35, P57, DOI 10.1007/s00371-017-1447-9
   Schlegl T, 2014, LECT NOTES COMPUT SC, V8848, P82, DOI 10.1007/978-3-319-13972-2_8
   Sharma S, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2018, VOL 5B
   Shin HC, 2015, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2015.7298712
   Soekhoe D, 2016, LECT NOTES COMPUT SC, V9897, P50, DOI 10.1007/978-3-319-46349-0_5
   Spanhol FA, 2017, IEEE SYS MAN CYBERN, P1868, DOI 10.1109/SMC.2017.8122889
   Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264
   Spanhol FA, 2016, IEEE IJCNN, P2560, DOI 10.1109/IJCNN.2016.7727519
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tang ZC, 2017, OPTIK, V130, P11, DOI 10.1016/j.ijleo.2016.10.117
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   Wan SH, 2015, I S BIOMED IMAGING, P195, DOI 10.1109/ISBI.2015.7163848
   WANDERLEY MDS, INT C ART NEUR NETW, P582
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yang Z, 2018, OPTIK, V171, P287, DOI 10.1016/j.ijleo.2018.06.024
   Yosinski J., 2014, Advances in neural information processing systems 27, P3320
   Zhang J, 2019, VISUAL COMPUT, V35, P1181, DOI 10.1007/s00371-019-01667-w
NR 33
TC 31
Z9 31
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1755
EP 1769
DI 10.1007/s00371-019-01768-6
EA OCT 2019
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000493500000001
DA 2024-07-18
ER

PT J
AU da Silva, RRO
   Paiva, JGS
   Telles, GP
   Zampieri, CEA
   Rolli, FP
   Minghim, R
AF da Silva, Renato R. O.
   Paiva, Jose Gustavo S.
   Telles, Guilherme P.
   Zampieri, Carlos E. A.
   Rolli, Fabio P.
   Minghim, Rosane
TI The Visual SuperTree: similarity-based multi-scale visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Similarity trees; Multi-dimensional data; Multi-scale visualization;
   Image and text visualization
ID PROJECTION TECHNIQUE; TREES
AB Similarity-based exploration of multi-dimensional data sets is a difficult task, in which most techniques do not perform well with large data sets, particularly in handling clutter that invariably happens as data sets grow larger. In this paper, we introduce the Visual SuperTree (VST), a method to build a multi-scale similarity tree that can deal with large data sets at interactive rates, maintaining most of the accuracy and the data organization capabilities of other available methods. The VST is built on top of a clustered multi-level configuration of the data that allows the user to quickly explore data sets by similarity. The method is shown to be useful for both unlabeled and labeled data, and it is capable of revealing external and internal cluster structures. We demonstrate its application on artificial and real data sets, showing additional advantages of the approach when exploring data that can be summarized meaningfully.
C1 [da Silva, Renato R. O.; Zampieri, Carlos E. A.] Univ Sao Paulo, Sao Carlos, SP, Brazil.
   [Rolli, Fabio P.] Univ Sao Paulo, Comp Sci, Sao Carlos, SP, Brazil.
   [Minghim, Rosane] Univ Sao Paulo, ICMC Inst Math & Comp Sci, Sao Carlos, SP, Brazil.
   [Paiva, Jose Gustavo S.] Univ Fed Uberlandia, Uberlandia, MG, Brazil.
   [Telles, Guilherme P.] Univ Estadual Campinas, Campinas, SP, Brazil.
   [Zampieri, Carlos E. A.] Fed Univ Grande Dourados, Dourados, MS, Brazil.
   [da Silva, Renato R. O.] Univ Groningen, Groningen, Netherlands.
C3 Universidade de Sao Paulo; Universidade de Sao Paulo; Universidade de
   Sao Paulo; Universidade Federal de Uberlandia; Universidade Estadual de
   Campinas; Universidade Federal da Grande Dourados; University of
   Groningen
RP da Silva, RRO (corresponding author), Univ Sao Paulo, Sao Carlos, SP, Brazil.; da Silva, RRO (corresponding author), Univ Groningen, Groningen, Netherlands.
EM rros@usp.br
RI Telles, Guilherme P./E-6620-2011; Minghim, Rosane/E-5703-2011; Minghim,
   Rosane/JBR-8763-2023
OI Minghim, Rosane/0000-0002-4799-8774; Rodrigues Oliveira da Silva,
   Renato/0000-0002-5900-4263; Paiva, Jose Gustavo de
   Souza/0000-0003-3228-6974; Telles, Guilherme P./0000-0003-2608-4807
FU Sao Paulo Research Foundation (FAPESP) [2011/18838-5]; National Council
   for Scientific and Technological Development (CNPq) [307411/2016-8,
   310299/2018-7]
FX This work was funded by the Sao Paulo Research Foundation (FAPESP),
   Grant 2011/18838-5, and the National Council for Scientific and
   Technological Development (CNPq), Grants 307411/2016-8 and
   310299/2018-7.
CR [Anonymous], 2005, Proceedings of the 2005 ACM symposium on Software visualization, DOI DOI 10.1145/1056018.1056041
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   Bachmaier C, 2005, LECT NOTES COMPUT SC, V3827, P1110, DOI 10.1007/11602613_110
   Bederson B. B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P71, DOI 10.1145/502348.502359
   Bederson BB, 2002, ACM T GRAPHIC, V21, P833, DOI 10.1145/571647.571649
   Bifet A, 2010, J MACH LEARN RES, V11, P1601
   Chalmers M, 1996, IEEE VISUAL, P127, DOI 10.1109/VISUAL.1996.567787
   Cockburn A, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456652
   Cuadros AM, 2007, IEEE CONF VIS ANAL, P99, DOI 10.1109/VAST.2007.4389002
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eler DM, 2009, VISUAL COMPUT, V25, P923, DOI 10.1007/s00371-009-0368-7
   Gascuel O, 2006, MOL BIOL EVOL, V23, P1997, DOI 10.1093/molbev/msl072
   Gomi A, 2008, IEEE INT CONF INF VI, P82, DOI 10.1109/IV.2008.8
   Ingram S, 2009, IEEE T VIS COMPUT GR, V15, P249, DOI 10.1109/TVCG.2008.85
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Neves TTAT, 2018, INFORM VISUAL, V17, P269, DOI 10.1177/1473871617700683
   Nguyen Q. V., 2003, Information Visualization, V2, P3, DOI 10.1057/palgrave.ivs.9500031
   Nocaj A, 2012, COMPUT GRAPH FORUM, V31, P855, DOI 10.1111/j.1467-8659.2012.03078.x
   Paiva JGS, 2011, IEEE T VIS COMPUT GR, V17, P2459, DOI 10.1109/TVCG.2011.212
   PAL NR, 1995, IEEE T FUZZY SYST, V3, P370, DOI 10.1109/91.413225
   Pass G., 1996, P 4 ACM INT C MULT, V96, P65, DOI DOI 10.1145/244130.244148
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Paulovich FV, 2012, COMPUT GRAPH FORUM, V31, P1145, DOI 10.1111/j.1467-8659.2012.03107.x
   Pavlopoulos GA, 2010, BIODATA MIN, V3, DOI 10.1186/1756-0381-3-1
   SAITOU N, 1987, MOL BIOL EVOL, V4, P406, DOI 10.1093/oxfordjournals.molbev.a040454
   Schulz HJ, 2011, IEEE COMPUT GRAPH, V31, P11, DOI 10.1109/MCG.2011.103
   Schulz HJ, 2009, IEEE PAC VIS SYMP, P81, DOI 10.1109/PACIFICVIS.2009.4906841
   Stehling R. O., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P102, DOI 10.1145/584792.584812
   Tan L, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2011.89
   Telles GP, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2162-x
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Ying A. T. T, 2015, P WORK C MIN SOFTW R
NR 35
TC 2
Z9 2
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1067
EP 1080
DI 10.1007/s00371-019-01696-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200024
OA Bronze
DA 2024-07-18
ER

PT J
AU Yang, WW
   Marshak, N
   Sykora, D
   Ramalingam, S
   Kavan, L
AF Yang, Wenwu
   Marshak, Nathan
   Sykora, Daniel
   Ramalingam, Srikumar
   Kavan, Ladislav
TI Building anatomically realistic jaw kinematics model from data
SO VISUAL COMPUTER
LA English
DT Article
DE Jaw kinematics; Teeth motion; TMJ
ID TEMPOROMANDIBULAR-JOINT; POSE ESTIMATION; DYNAMICS
AB Recent work on anatomical face modeling focuses mainly on facial muscles and their activation. This paper considers a different aspect of anatomical face modeling: kinematic modeling of the jaw, i.e., the temporomandibular joint (TMJ). Previous work often relies on simple models of jaw kinematics, even though the actual physiological behavior of the TMJ is quite complex, allowing not only for mouth opening, but also for some amount of sideways (lateral) and front-to-back (protrusion) motions. Fortuitously, the TMJ is the only joint whose kinematics can be accurately measured with optical methods, because the bones of the lower and upper jaw are rigidly connected to the lower and upper teeth. We construct a person-specific jaw kinematic model by asking an actor to exercise the entire range of motion of the jaw while keeping the lips open so that the teeth are at least partially visible. This performance is recorded with three calibrated cameras. We obtain highly accurate 3D models of the teeth with a standard dental scanner and use these models to reconstruct the rigid body trajectories of the teeth from the videos (markerless tracking). The relative rigid transformations samples between the lower and upper teeth are mapped to the Lie algebra of rigid body motions in order to linearize the rotational motion. Our main contribution is to fit these samples with a three-dimensional nonlinear model parameterizing the entire range of motion of the TMJ. We show that standard principal component analysis (PCA) fails to capture the nonlinear trajectories of the moving mandible. However, we found these nonlinearities can be captured with a special modification of autoencoder neural networks known as nonlinear PCA. By mapping back to the Lie group of rigid transformations, we obtain a parametrization of the jaw kinematics which provides an intuitive interface allowing the animators to explore realistic jaw motions in a user-friendly way.
C1 [Yang, Wenwu] Zhejiang Gongshang Univ, Sch Comp & Informat Engn, Hangzhou 310012, Zhejiang, Peoples R China.
   [Marshak, Nathan] Univ Utah, Salt Lake City, UT USA.
   [Ramalingam, Srikumar] Univ Utah, Sch Comp, Salt Lake City, UT USA.
   [Kavan, Ladislav] Univ Utah, Comp Sci, Salt Lake City, UT USA.
   [Sykora, Daniel] Czech Tech Univ, Fac Elect Engn, Dept Comp Graph & Interact, Prague, Czech Republic.
C3 Zhejiang Gongshang University; Utah System of Higher Education;
   University of Utah; Utah System of Higher Education; University of Utah;
   Utah System of Higher Education; University of Utah; Czech Technical
   University Prague
RP Yang, WW (corresponding author), Zhejiang Gongshang Univ, Sch Comp & Informat Engn, Hangzhou 310012, Zhejiang, Peoples R China.
EM wwyang@mail.zjgsu.edu.cn; tauneutrino3@gmail.com; sykorad@fel.cvut.cz;
   srikumar.ramalingam@gmail.com; ladislav.kavan@gmail.com
RI Sýkora, Daniel/C-3075-2014
FU National Science Foundation [IIS-1617172, IIS-1622360, IIS-1764071]; NSF
   of China [U1609215, 61472363]; Fulbright Commission in the Czech
   Republic; Technology Agency of the Czech Republic [TE01020415]; Grant
   Agency of the Czech Technical University in Prague
   [SGS17/215/OHK3/3T/18]; Research Center for Informatics
   [CZ.02.1.01/0.0/0.0/16_19/0000765]
FX This study was funded by the National Science Foundation (IIS-1617172,
   IIS-1622360 and IIS-1764071), NSF of China (U1609215 and 61472363), the
   Fulbright Commission in the Czech Republic, the Technology Agency of the
   Czech Republic under research program TE01020415 (V3C-Visual Computing
   Competence Center), the Grant Agency of the Czech Technical University
   in Prague (No. SGS17/215/OHK3/3T/18), and Research Center for
   Informatics (No. CZ.02.1.01/0.0/0.0/16_19/0000765).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Aichert A, 2012, LECT NOTES COMPUT SC, V7511, P601, DOI 10.1007/978-3-642-33418-4_74
   Alexa M, 2002, ACM T GRAPHIC, V21, P380, DOI 10.1145/566570.566592
   [Anonymous], 2008, Principal manifolds for data visualization and dimension reduction
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Cong Matthew, 2015, P 14 ACM SIGGRAPH EU, P175, DOI DOI 10.1145/2786784.278678632M
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   de Zee M, 2007, J BIOMECH, V40, P1192, DOI 10.1016/j.jbiomech.2006.06.024
   Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hannam AG, 2008, J BIOMECH, V41, P1069, DOI 10.1016/j.jbiomech.2007.12.001
   Hannam AG, 2010, J PROSTHET DENT, V104, P191, DOI 10.1016/S0022-3913(10)60120-5
   HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
   Henderson SE, 2014, J BIOMECH, V47, P1360, DOI 10.1016/j.jbiomech.2014.01.051
   Herda L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P95, DOI 10.1109/AFGR.2002.1004138
   Ichim AE, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073664
   Kanade T., 1981, P 7 INT JOINT C ARTI, V1, P674, DOI DOI 10.5555/1623264.1623280
   KIEFER J, 1953, P AM MATH SOC, V4, P502, DOI 10.2307/2032161
   Klein Georg., 2006, BMVC, P1119
   Koolstra JH, 2002, CRIT REV ORAL BIOL M, V13, P366, DOI 10.1177/154411130201300406
   Kozlov Y, 2017, COMPUT GRAPH FORUM, V36, P75, DOI 10.1111/cgf.13108
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   Lan L., 2017, P ACM SIGGRAPH DIG P
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Murray RM, 1994, MATH INTRO ROBOTIC M
   Ramalingam S, 2010, IEEE INT C INT ROBOT, P3816, DOI 10.1109/IROS.2010.5649105
   Rosenhahn B, 2005, INT J COMPUT VISION, V62, P267, DOI 10.1007/s11263-005-4883-3
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholz M., 2002, 10th European Symposium on Artificial Neural Networks. ESANN'2002. Proceedings, P439
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Sorkine-Hornung O., 2009, TECHNICAL NOTES ETHZ
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Villamil MB, 2012, COMPUT METH PROG BIO, V105, P217, DOI 10.1016/j.cmpb.2011.09.010
   Wang JC, 2017, INT J MED ROBOT COMP, V13, DOI 10.1002/rcs.1754
   Wang JC, 2014, IEEE T BIO-MED ENG, V61, P1295, DOI 10.1109/TBME.2014.2301191
   WU C, 2001, ACM T GRAPHIC, V35
   Yoon HJ, 2007, J ORAL MAXIL SURG, V65, P1569, DOI 10.1016/j.joms.2006.10.009
   Zhang Shuo., 2011, The Open Medical Imaging Journal, V5, P1, DOI [DOI 10.2174/1874347101105010001, 10.2174/1874347101105010001]
   Zoss G, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201382
NR 42
TC 8
Z9 11
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1105
EP 1118
DI 10.1007/s00371-019-01677-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200027
DA 2024-07-18
ER

PT J
AU Wang, ML
   Guo, SH
   Liao, MH
   He, DJ
   Chang, J
   Zhang, JJ
AF Wang, Meili
   Guo, Shihui
   Liao, Minghong
   He, Dongjian
   Chang, Jian
   Zhang, Jianjun
TI Action snapshot with single pose and viewpoint
SO VISUAL COMPUTER
LA English
DT Article
DE Action snapshot; Information entropy; Pose; Viewpoint selection
ID EXTRACTION
AB Many art forms present visual content as a single image captured from a particular viewpoint. How to select a meaningful representative moment from an action performance is difficult, even for an experienced artist. Often, a well-picked image can tell a story properly. This is important for a range of narrative scenarios, such as journalists reporting breaking news, scholars presenting their research, or artists crafting artworks. We address the underlying structures and mechanisms of a pictorial narrative with a new concept, called the action snapshot, which automates the process of generating a meaningful snapshot (a single still image) from an input of scene sequences. The input of dynamic scenes could include several interactive characters who are fully animated. We propose a novel method based on information theory to quantitatively evaluate the information contained in a pose. Taking the selected top postures as input, a convolutional neural network is constructed and trained with the method of deep reinforcement learning to select a single viewpoint, which maximally conveys the information of the sequence. User studies are conducted to experimentally compare the computer-selected poses and viewpoints with those selected by human participants. The results show that the proposed method can assist the selection of the most informative snapshot effectively from animation-intensive scenarios.
C1 [Wang, Meili; He, Dongjian] Northwest A&F Univ, Coll Informat Engn, Xianyang, Peoples R China.
   [Wang, Meili; He, Dongjian] Minist Agr, Key Lab Agr Internet Things, Xianyang, Peoples R China.
   [Guo, Shihui; Liao, Minghong] Xiamen Univ, Sch Software, Xiamen, Peoples R China.
   [Chang, Jian; Zhang, Jianjun] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
C3 Northwest A&F University - China; Ministry of Agriculture & Rural
   Affairs; Xiamen University; Bournemouth University
RP Guo, SH (corresponding author), Xiamen Univ, Sch Software, Xiamen, Peoples R China.
EM guoshihui@xmu.edu.cn
FU National Natural Science Foundation, China [61402374, 61661146002,
   61702433]; Fundamental Research Funds for the Central Universities
   [QN2012033];  [2014M562457];  [2016M600506]
FX We are grateful to the reviewers and editors for their valuable comments
   and constructive suggestions. This work is supported by National Natural
   Science Foundation (61402374, 61661146002, 61702433), China,
   Postdoctoral Science Foundation (2014M562457, 2016M600506) and the
   Fundamental Research Funds for the Central Universities (QN2012033). We
   also thank the researchers who maintains the CMU motion capture database
   and the Stanford 3D Scanning Repository.
CR Akgun B, 2012, INT J SOC ROBOT, V4, P343, DOI 10.1007/s12369-012-0160-0
   [Anonymous], P 2006 IEEE COMP SOC
   [Anonymous], 2006, FIELDS MITACS IND PR
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   Assa J, 2010, COMPUT GRAPH FORUM, V29, P595, DOI 10.1111/j.1467-8659.2009.01629.x
   Assa J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409068
   Caspi Y, 2006, VISUAL COMPUT, V22, P642, DOI 10.1007/s00371-006-0046-y
   Coleman P., 2008, Proceedings of sca 2008, P137
   Correa CD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778825
   Halit C, 2011, COMPUT ANIMAT VIRT W, V22, P3, DOI 10.1002/cav.380
   Huang KS, 2005, VISUAL COMPUT, V21, P532, DOI 10.1007/s00371-005-0316-0
   Jin C, 2012, COMPUT ANIMAT VIRT W, V23, P559, DOI 10.1002/cav.1471
   Kwon JY, 2008, VISUAL COMPUT, V24, P475, DOI 10.1007/s00371-008-0228-x
   LEE HJ, VIRTUAL, V23, P417, DOI DOI 10.1002/cav.1448
   Lessing Gotthold, 1976, COSMOPOLITAN ART J, V3, P56
   Lino C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766965
   Lino Christophe, 2012, P ACM SIGGRAPH EUR S, P65, DOI [10.1145/1409060.1409068, DOI 10.1145/1409060.1409068]
   Liu XM, 2013, VISUAL COMPUT, V29, P85, DOI 10.1007/s00371-012-0676-1
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Rudoy D, 2012, INT J COMPUT VISION, V97, P243, DOI 10.1007/s11263-011-0484-5
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Speidel K., 2013, DIEGESIS
   Turkay C, 2009, VISUAL COMPUT, V25, P451, DOI 10.1007/s00371-009-0337-1
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Wang M, 2017, COMP GRAPH INT C, P31
   Wang ML, 2013, IETE TECH REV, V30, P454, DOI 10.4103/0256-4602.125659
   Wang WC, 2016, PROC CVPR IEEE, P4114, DOI 10.1109/CVPR.2016.446
   Williams R., 2009, The animator's survival kit
   Wolf W, 2003, WORD IMAGE, V19, P180
   Xia GY, 2017, IEEE T IND ELECTRON, V64, P1589, DOI 10.1109/TIE.2016.2610946
   Zhang YW, 2015, IEEE T VIS COMPUT GR, V21, P328, DOI 10.1109/TVCG.2014.2377773
NR 31
TC 2
Z9 2
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 507
EP 520
DI 10.1007/s00371-018-1479-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, R
   Liu, MM
   Zhang, YZ
   Li, DH
   Zheng, YT
AF Liu, Ran
   Liu, Mingming
   Zhang, Yanzhen
   Li, Dehao
   Zheng, Yangting
TI Real-time ultra-high definition multiview glasses-free 3D display system
SO VISUAL COMPUTER
LA English
DT Article
DE Glasses-free 3D display system; Depth-image-based rendering; Multiview
   rendering; Asymmetric shift-sensor camera setup; FPGA
ID DEPTH; ARCHITECTURE; IMAGES; MAP
AB The design and implementation of a real-time ultra-high definition (UHD) multiview glasses-free 3D display system always suffer from the high transmission bandwidth, high memory cost, and high computational complexity. This paper presents a glasses-free 3D display system based on depth-image-based rendering (DIBR) technique by solving these problems. It can convert a V+D video (RGBD) stream in real time to a multiview representation suitable for a multiview autostereoscopic display. As V+D video format is used for the system, the transmission bandwidth is reduced. For the memory cost, we introduce an asymmetric shift-sensor camera setup to avoid external memory usage and reduce the storage requirement of multiviews. For the computational complexity, as our camera setup ensures that all the virtual views can be generated with the same DIBR algorithm, the computational complexity is reduced. In addition, we simplify the view fusion method so as to rearrange the subpixels from multiviews with lower complexity to form a single glasses-free 3D image. Moreover, we propose a hardware architecture for the system and implement it using field programmable gate array. Simulation results show that our system can support the UHD V+D videos for an 8-view glasses-free 3D display. Performance evaluation results show that the proposed system can provide reasonably good stereoscopic image quality if appropriate parameters of the system are applied.
C1 [Liu, Ran; Zhang, Yanzhen] Chongqing Univ, Coll Commun Engn, Chongqing 400044, Peoples R China.
   [Liu, Ran; Liu, Mingming; Li, Dehao; Zheng, Yangting] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
C3 Chongqing University; Chongqing University
RP Liu, MM (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM ran.liu_cqu@qq.com; 1457619420@qq.com
RI Zhang, Yanzhen/L-6446-2018; Liu, Ran/C-7952-2014
FU National Natural Science Foundation of China [61201347]; Chongqing
   Foundation and Advanced Research Project [cstc2016jcyjA0103];
   Entrepreneurship and Innovation Program for Chongqing Overseas Returned
   Scholars [cx2017094]
FX This work was jointly supported by the National Natural Science
   Foundation of China (No. 61201347), the Chongqing Foundation and
   Advanced Research Project (cstc2016jcyjA0103), and the Entrepreneurship
   and Innovation Program for Chongqing Overseas Returned Scholars (No.
   cx2017094).
CR [Anonymous], 2011, 2011 IEEE VIS COMM I
   [Anonymous], J INF COMPUT SCI
   [Anonymous], 2013, CHANGSH 5 P CHIN SUS
   [Anonymous], 2004, INT SOC OPTICS PHOTO, DOI DOI 10.1117/12.524762
   Arnold J., 2007, Digital Television: Technology and Standards
   Chen HJ, 2010, IEEE INT SYMP CIRC S, P1165, DOI 10.1109/ISCAS.2010.5537312
   Chen J, 2008, J CHANGCHUN U SCI TE, V31, P8
   Chen WY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2069, DOI 10.1109/ICME.2006.262622
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Donatsch D, 2014, VISUAL COMPUT, V30, P897, DOI 10.1007/s00371-014-0979-5
   Fezza SA, 2015, SIGNAL IMAGE VIDEO P, V9, P177, DOI 10.1007/s11760-015-0761-9
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Horng YR, 2011, IEEE T CIRC SYST VID, V21, P1329, DOI 10.1109/TCSVT.2011.2148410
   Hsia CH, 2015, IEEE SENS J, V15, P994, DOI 10.1109/JSEN.2014.2359225
   Jincheol Park, 2015, IEEE Transactions on Image Processing, V24, P1101, DOI 10.1109/TIP.2014.2383327
   Jung Y. J., 2009, P SPIE INT SOC OPTIC
   Lin TC, 2010, IEEE T CONSUM ELECTR, V56, P720, DOI 10.1109/TCE.2010.5505993
   Liu R., 2016, International Journal of Hybrid Information Technology, V9, P145, DOI [10.14257/ijhit.2016.9.5.12, DOI 10.14257/IJHIT.2016.9.5.12]
   Liu R, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.063021
   Liu R, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.3.033031
   Liu R, 2012, KSII T INTERNET INF, V6, P2663, DOI 10.3837/tiis.2012.10.013
   Liu Ran, 2013, Journal of Tongji University (Natural Science), V41, P142, DOI 10.3969/j.issn.0253-374x.2013.01.024
   Liu R, 2012, KSII T INTERNET INF, V6, P1171, DOI 10.3837/tiis.2012.04.013
   Marroquim R, 2012, VISUAL COMPUT, V28, P983, DOI 10.1007/s00371-012-0743-7
   Peng-Fei Jin, 2012, 2012 International Conference on Systems and Informatics (ICSAI 2012), P1981, DOI 10.1109/ICSAI.2012.6223438
   Ren PJ, 2017, IEEE T CIRCUITS-II, V64, P705, DOI 10.1109/TCSII.2016.2595591
   Schaffner M, 2016, IEEE T CIRC SYST VID, V26, P2093, DOI 10.1109/TCSVT.2015.2501640
   Sfikas K, 2013, VISUAL COMPUT, V29, P1351, DOI 10.1007/s00371-013-0876-3
   Shao-Jun Yao, 2013, 2013 Seventh International Conference on Image and Graphics (ICIG), P774, DOI 10.1109/ICIG.2013.157
   Shim H, 2012, VISUAL COMPUT, V28, P1139, DOI 10.1007/s00371-011-0664-x
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Tang Y, 2011, J COMPUT INF SYST, V7, P2051
   Tsung P. K., IEEE INT SOL STAT CI
   Yang X., 2016, SID Symposium Digest of Technical Papers, V47, P1237
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 35
TC 0
Z9 0
U1 2
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 303
EP 321
DI 10.1007/s00371-018-1508-8
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600002
DA 2024-07-18
ER

PT J
AU Saini, R
   Roy, PP
   Dogra, DP
AF Saini, Rajkumar
   Roy, Partha Pratim
   Dogra, Debi Prosad
TI A novel point-line duality feature for trajectory classification
SO VISUAL COMPUTER
LA English
DT Article
DE Trajectory classification; Hidden Markov model (HMM); Point-line duality
   (PLD); Fusion
ID CONTEXT
AB Trajectory classification is important for understanding object movements within the surveillance area. Raw trajectories are represented by object location in form of (x,y) coordinates. The length of trajectories varies in terms of number of points; thus, it is difficult to classify them into correct classes. The raw features extracted from trajectory do not yield satisfactory results in classification. Thus, robust features are needed that can efficiently represent trajectory sequences and help to improve the classification performance. In this paper, we present a new feature vector that is based on the concept of point-line duality (PLD) transformation, i.e., transformation of a trajectory point from its primal plane into a straight line in dual plane. Classification has been done using hidden Markov model (HMM) framework. We also propose a fusion approach combining classification results obtained from raw feature and PLD feature to improve the performance. Experiments have been carried out on raw trajectories with reduced lengths as well as adding Gaussian noise. Proposed approach has been tested on three publicly available datasets, namely T15, MIT, and CROSS. It has been found that the PLD feature outperforms existing features as well as raw feature when used in HHM-based classification. We have obtained encouraging results by feature combination at the decision level with 97, 96.75 and 99.80% accuracy, respectively, on T15, MIT, and CROSS datasets.
C1 [Saini, Rajkumar; Roy, Partha Pratim] IIT Roorkee, Dept Comp Sci & Engn, Roorkee, Uttar Pradesh, India.
   [Dogra, Debi Prosad] IIT Bhubaneswar, Sch Elect Sci, Bhubaneswar, Odisha, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Bhubaneswar
RP Saini, R (corresponding author), IIT Roorkee, Dept Comp Sci & Engn, Roorkee, Uttar Pradesh, India.
EM rajkumarsaini.rs@gmail.com; proy.fcs@iitr.ac.in; dpdogra@iitbbs.ac.in
RI Roy, Partha Pratim/AAV-9061-2020; Roy, Partha Pratim/AAW-2994-2020; Roy,
   Partha Pratim/GPF-4253-2022
OI Roy, Partha Pratim/0000-0002-5735-5254; 
CR [Anonymous], P 2008 IEEE COMP SOC, DOI DOI 10.1109/CVPR.2008.4587718
   [Anonymous], 1998, COMPUTATIONAL GEOMET
   Appiah K, 2014, IEEE INT FUZZY SYST, P2430, DOI 10.1109/FUZZ-IEEE.2014.6891833
   Brun L, 2014, IEEE T CIRC SYST VID, V24, P1669, DOI 10.1109/TCSVT.2014.2302521
   Cai YF, 2015, IET INTELL TRANSP SY, V9, P810, DOI 10.1049/iet-its.2014.0238
   Dahmane M, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P136, DOI 10.1109/CRV.2005.65
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Dogra D. P., 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P104
   Dominguez R., 2011, Proceedings of the 2011 11th International Conference on Intelligent Systems Design and Applications (ISDA), P790, DOI 10.1109/ISDA.2011.6121753
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Fuse  T., 2017, IEEE T INTELL TRANSP, P1
   Hu WM, 2013, IEEE T PATTERN ANAL, V35, P1051, DOI 10.1109/TPAMI.2012.188
   Jan T, 2004, IEEE IJCNN, P1309
   Kim E, 2010, IEEE PERVAS COMPUT, V9, P48, DOI 10.1109/MPRV.2010.7
   Kim K, 2011, IEEE I CONF COMP VIS, P1164, DOI 10.1109/ICCV.2011.6126365
   Kumar P, 2017, NEUROCOMPUTING, V259, P21, DOI 10.1016/j.neucom.2016.08.132
   Kwon Y, 2017, EXPERT SYST APPL, V78, P386, DOI 10.1016/j.eswa.2017.02.026
   Lee AJT, 2009, INFORM SCIENCES, V179, P2218, DOI 10.1016/j.ins.2009.02.016
   Lee JG, 2008, PROC VLDB ENDOW, V1, P1081, DOI 10.14778/1453856.1453972
   Mehta P, 2015, 2015 INT C INNOVATIO, P1, DOI DOI 10.1109/ICIIECS.2015.7193070
   Melo J, 2006, IEEE T INTELL TRANSP, V7, P188, DOI 10.1109/TITS.2006.874706
   Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64
   Mozerov M, 2008, OPT ENG, V47, DOI 10.1117/1.2909665
   Nascimento JC, 2010, IEEE T IMAGE PROCESS, V19, P1338, DOI 10.1109/TIP.2009.2039664
   Pan X., 2017, SONAR NAVIGATION IET
   Pei W, 2017, IEEE T NEUR NET LEAR, VPP, P1
   Pereira EM, 2015, LECT NOTES COMPUT SC, V9117, P13, DOI 10.1007/978-3-319-19390-8_2
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Saini R., 2017, AS C PATT REC
   Saini R, 2018, EXPERT SYST APPL, V93, P169, DOI 10.1016/j.eswa.2017.10.021
   Siang K. L. Y., 2012, Proceedings of the 2012 8th International Conference on Computing Technology and Information Management (NCM and ICNIT), P449
   Sim G., 2017, LECT NOTES ELECT ENG, V448
   Suzuki S, 2007, IEEE SYS MAN CYBERN, P2980
   Tang KS, 2016, IEEE T INTELL TRANSP, V17, P206, DOI 10.1109/TITS.2015.2462738
   Xu D, 2013, IEEE IMAGE PROC, P3597, DOI 10.1109/ICIP.2013.6738742
   Xu HT, 2015, IEEE I CONF COMP VIS, P4328, DOI 10.1109/ICCV.2015.492
   Zhong JH, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P801
NR 38
TC 5
Z9 5
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2019
VL 35
IS 3
BP 415
EP 427
DI 10.1007/s00371-018-1473-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HP0MR
UT WOS:000461360600009
DA 2024-07-18
ER

PT J
AU Liu, QG
   Li, SQ
   Xiong, JJ
   Qin, BJ
AF Liu, Qiegen
   Li, Sanqian
   Xiong, Jiaojiao
   Qin, Binjie
TI WpmDecolor: weighted projection maximum solver for contrast-preserving
   decolorization
SO VISUAL COMPUTER
LA English
DT Article
DE Color-to-gray conversion; Weighted projection; Linear parametric model;
   Projected gradient descent; Nonnegative constraint; Discrete searching
ID COLOR; CONVERSION; IMAGES
AB This paper presents a novel semi-reference inspired color-to-gray conversion model for faithfully preserving the contrast details of the color image, essentially differs from most of the no-reference and reference approaches. In the proposed model, on the basic assumption that a good gray conversion should make the conveyed gradient values (i.e., contrast) to be maximal, we present a projection maximum function to model the decolorization procedure. We further incorporate weights of the original gradients into the maximum function. The Gaussian weighted factor consisting of the gradients of each channel of the input color image is employed to better reflect the degree of preserving feature discriminability and color ordering in color-to-gray conversion. The projected gradient descent and discrete searching techniques are developed to solve the proposed model with and without nonnegative constraint, respectively. Extensive experimental evaluations on two existing datasets, containing abundant colors and patterns, show that the proposed method outperforms the state-of-the-art methods quantitatively and qualitatively.
C1 [Liu, Qiegen; Li, Sanqian; Xiong, Jiaojiao] Nanchang Univ, Dept Elect Informat Engn, Nanchang 330031, Jiangxi, Peoples R China.
   [Qin, Binjie] Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.
C3 Nanchang University; Shanghai Jiao Tong University
RP Qin, BJ (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.
EM bjqin@sjtu.edu.cn
RI sanqian, li/KIK-7023-2024; Li, Yuanyuan/J-3539-2014; Qin,
   Binjie/L-7882-2019
OI Li, Yuanyuan/0000-0001-6151-9306; Qin, Binjie/0000-0001-7445-1582
FU National Natural Science Foundation of China [61661031, 61362001,
   61365013, 61503176]; international scientific and technological
   cooperation projects of Jiangxi Province [20141BDH80001]; Young
   scientists training plan of Jiangxi province [20142BCB23001,
   20162BCB23019]
FX The authors sincerely thank the anonymous reviewers for their valuable
   comments and constructive suggestions that are very helpful in the
   improvement of this paper. The authors also thank Lu et al. for sharing
   their experiment materials and source codes. This work was supported in
   part by the National Natural Science Foundation of China under 61661031,
   61362001, 61365013, 61503176, the international scientific and
   technological cooperation projects of Jiangxi Province
   (No.20141BDH80001), and Young scientists training plan of Jiangxi
   province (Nos.20142BCB23001, 20162BCB23019).
CR Ancuti CO, 2011, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2011.5995414
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2010, P AS C COMP VIS
   [Anonymous], ACM SIGGRAPH ASIA TE
   [Anonymous], 2010, P AS C COMP VIS
   [Anonymous], P IEEE INT C COMP PH
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Cadík M, 2008, COMPUT GRAPH FORUM, V27, P1745
   CALAMAI PH, 1987, MATH PROGRAM, V39, P93, DOI 10.1007/BF02592073
   Du H, 2015, IEEE T IMAGE PROCESS, V24, P434, DOI 10.1109/TIP.2014.2380172
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Ji ZP, 2016, VISUAL COMPUT, V32, P1621, DOI 10.1007/s00371-015-1145-4
   Jin ZM, 2014, SIAM J IMAGING SCI, V7, P944, DOI 10.1137/130935197
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Kline DM, 2005, NEURAL COMPUT APPL, V14, P310, DOI 10.1007/s00521-005-0467-y
   Kuhn GR, 2008, VISUAL COMPUT, V24, P505, DOI 10.1007/s00371-008-0231-2
   Lau C, 2011, IEEE I CONF COMP VIS, P1172, DOI 10.1109/ICCV.2011.6126366
   Liu Q, 2014, MATH PROBL ENG, V2014, P18
   Liu QG, 2015, IEEE T IMAGE PROCESS, V24, P2889, DOI 10.1109/TIP.2015.2423615
   Liu QG, 2013, SIAM J IMAGING SCI, V6, P1689, DOI 10.1137/110857349
   Lu C., 2012, ACM SIGGRAPH ASIA TE
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P4673, DOI 10.1109/TIP.2015.2460015
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Rasche K, 2005, COMPUT GRAPH FORUM, V24, P423, DOI 10.1111/j.1467-8659.2005.00867.x
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Song ML, 2013, NEUROCOMPUTING, V119, P222, DOI 10.1016/j.neucom.2013.03.037
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Song YB, 2014, IEEE WINT CONF APPL, P159, DOI 10.1109/WACV.2014.6836106
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Yoo MJ, 2015, COMPUT GRAPH FORUM, V34, P373, DOI 10.1111/cgf.12567
   Yuemin Z., 2011, INT J COMPUT APPL, V23, P16, DOI [10.5120/2900-3798, DOI 10.5120/2900-3798]
   Zhu W, 2014, VISUAL COMPUT, V30, P299, DOI 10.1007/s00371-013-0854-9
NR 36
TC 13
Z9 15
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 205
EP 221
DI 10.1007/s00371-017-1464-8
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600005
DA 2024-07-18
ER

PT J
AU Dehshibi, MM
   Shanbehzadeh, J
AF Dehshibi, Mohammad Mahdi
   Shanbehzadeh, Jamshid
TI Cubic norm and kernel-based bi-directional PCA: toward age-aware facial
   kinship verification
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Age estimation; Age clustering; Convolutional neural network; Cube norm;
   Feature extraction; Kernelized bi-directional PCA; Kinship verification
ID 2-DIMENSIONAL PCA; RECOGNITION; INFORMATION
AB A recent challenge in computer vision is exploring the cardinality of a relationship among multiple visual entities to answer questions like whether the subjects in a photograph have a kin relationship. This paper tackles kinship recognition from the aging viewpoint in which the system could find the parent of a child where the input image of the parent belongs to the age range that is lower than the child is. Technical contributions of this research are twofold. (1) An efficient discriminative feature space is constructed by proposing kernelized bi-directional PCA to form a topological cubic feature space. Cubic feature space in conjunction with the introduced cubic norm is used to solve the kinship problem. (2) To fill the gap of aging effect in finding a kin relation, a semi-supervised learning paradigm is proposed. To do this, first, the pooling layer of a convolutional neural network is modified to do a soft pooling. Then, the last pooling layer, as a rich feature vector, is fed into density-based spatial clustering of applications with noise algorithm. This pre-classification phase would be useful when there is no aggregation on how many classes should be used in the age group estimation task. Finally, by adding kernel computation to sparse representation classifier, the age classification is done. Evaluation of the proposed method on five publicly available facial kinship datasets shows the superiority of the proposed method over both the state-of-the-art kinship verification methods and what is known as human decision-making.
C1 [Dehshibi, Mohammad Mahdi] Islamic Azad Univ, Sci & Res Branch, Dept Comp Engn, Tehran, Iran.
   [Shanbehzadeh, Jamshid] Kharazmi Univ, Dept Elect & Comp Engn, Fac Engn, Tehran, Iran.
C3 Islamic Azad University; Kharazmi University
RP Shanbehzadeh, J (corresponding author), Kharazmi Univ, Dept Elect & Comp Engn, Fac Engn, Tehran, Iran.
EM dehshibi@iranprc.org; jamshid@khu.ac.ir
RI Dehshibi, Mohammad Mahdi/S-9946-2017
OI Dehshibi, Mohammad Mahdi/0000-0001-8112-5419
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2008, P 2008 23 INT S COMP
   [Anonymous], 2009, 2009 IEEE 3 INT C BI, DOI DOI 10.1109/BTAS.2009.5339053
   [Anonymous], 2007, P IEEE 11 INT C COMP
   [Anonymous], 2006, P 14 ANN ACM INT C M
   Bastanfard A, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P50
   Bocklet T, 2008, INT CONF ACOUST SPEE, P1605, DOI 10.1109/ICASSP.2008.4517932
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chao WL, 2013, PATTERN RECOGN, V46, P628, DOI 10.1016/j.patcog.2012.09.011
   Chellappa R., 2009, Journal of Visual Languages and Computing, V15, P3349
   Cootes T, 2002, FACE GESTURE RECOGNI
   Davis J. V., 2007, ICML, P209
   Dehghan A, 2014, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR.2014.227
   Dehshibi MM, 2012, 2012 12TH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT SYSTEMS (HIS), P219, DOI 10.1109/HIS.2012.6421337
   Dehshibi MM, 2010, SIGNAL PROCESS, V90, P2431, DOI 10.1016/j.sigpro.2010.02.015
   Du HS, 2015, J VIS COMMUN IMAGE R, V32, P55, DOI 10.1016/j.jvcir.2015.07.011
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2012, IEEE T INSTRUM MEAS, V61, P2322, DOI 10.1109/TIM.2012.2187468
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Han H, 2013, IEEE NUCL SCI CONF R
   Han J, 2012, MOR KAUF D, P1
   Hinton G.E., 2012, ABS12070580 CORR
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Hu XR, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P1001, DOI 10.1109/ICInfA.2016.7831965
   Kohli N., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P245, DOI 10.1109/BTAS.2012.6374584
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Kyaw S. P., 2013, 2013 9 INT C INF COM, P1
   Li Y, 2016, VISUAL COMPUT, V32, P1525, DOI 10.1007/s00371-015-1137-4
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Meyers E, 2008, INT J COMPUT VISION, V76, P93, DOI 10.1007/s11263-007-0058-8
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   PLOMIN R, 1987, BEHAV BRAIN SCI, V10, P1, DOI 10.1017/S0140525X00055941
   Qin XQ, 2015, IEEE T MULTIMEDIA, V17, P1855, DOI 10.1109/TMM.2015.2461462
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Shu XB, 2016, PATTERN RECOGN, V59, P156, DOI 10.1016/j.patcog.2015.12.015
   Steinley D, 2004, PSYCHOL METHODS, V9, P386, DOI 10.1037/1082-989X.9.3.386
   Suo J., 2008, Automatic Face and Gesture Recognition, P1
   Vieira TF, 2014, VISUAL COMPUT, V30, P1333, DOI 10.1007/s00371-013-0884-3
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xu AB, 2006, INT C PATT RECOG, P481
   Yan HB, 2014, IEEE T INF FOREN SEC, V9, P1169, DOI 10.1109/TIFS.2014.2327757
   Yan SC, 2008, INT CONF ACOUST SPEE, P737
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhou XZ, 2016, INFORM FUSION, V32, P40, DOI 10.1016/j.inffus.2015.08.006
   Zhou Xiuzhuang, 2012, P 20 ACM INT C MULT, P725, DOI [DOI 10.1145/2393347, DOI 10.1145/2393347.2396297]
   Zuo W., 2005, IEEE INT C IM PROC, pII
NR 58
TC 19
Z9 19
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 23
EP 40
DI 10.1007/s00371-017-1442-1
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900004
DA 2024-07-18
ER

PT J
AU Liu, BW
   Li, P
   Sheng, B
   Nie, YW
   Wu, EH
AF Liu, Bowen
   Li, Ping
   Sheng, Bin
   Nie, Yongwei
   Wu, Enhua
TI Structure-preserving image completion with multi-level dynamic patches
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Image completion; Patch-based approach; Dynamic patches; Structure
   preservation; Parallel search; Multi-scale solution
ID TEXTURE SYNTHESIS
AB In this paper, we present a novel structure-preserving image completion approach equipped with dynamic patches. We formulate the image completion problem into an energy minimization framework that accounts for coherence within the hole and global coherence simultaneously. The completion of the hole is achieved through iterative optimizations combined with a multi-scale solution. In order to avoid abnormal structure and disordered texture, we utilize a dynamic patch system to achieve efficient structure restoration. Our dynamic patch system functions in both horizontal and vertical directions of the image pyramid. In the horizontal direction, we conduct a parallel search for multi-size patches in each pyramid level and design a competitive mechanism to select the most suitable patch. In the vertical direction, we use large patches in higher pyramid level to maximize the structure restoration and use small patches in lower pyramid level to reduce computational workload. We test our approach on massive images with complex structure and texture. The results are visually pleasing and preserve nice structure. Apart from effective structure preservation, our approach outperforms previous state-of-the-art methods in time consumption.
C1 [Liu, Bowen] Educ Univ Hong Kong, Dept Math & Informat Technol, Hong Kong, Peoples R China.
   [Li, Ping] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Macao, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Nie, Yongwei] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
   [Wu, Enhua] Univ Macau, Macau, Macao, Peoples R China.
   [Wu, Enhua] SKLCS IOSCAS, Macau, Macao, Peoples R China.
C3 Education University of Hong Kong (EdUHK); Macau University of Science &
   Technology; Shanghai Jiao Tong University; South China University of
   Technology; University of Macau
RP Li, P (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau, Macao, Peoples R China.
EM bowenliulbw@gmail.com; pli@must.edu.mo; shengbin@sjtu.edu.cn;
   nieyongwei@scut.edu.cn; ehwu@umac.no
RI Li, Ping/AAO-2019-2020
OI Li, Ping/0000-0002-1503-0240
FU National Natural Science Foundation of China [61632003]; Research Grants
   Council of Hong Kong [28200215]
FX The authors would like to thank all reviewers for their helpful
   suggestions and constructive comments. The work was supported by the
   National Natural Science Foundation of China (No. 61632003), and a Grant
   from the Research Grants Council of Hong Kong (No. 28200215).
CR [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], 2002, P 27 ANN C COMPUTER
   [Anonymous], FLOWS NETWORKS
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bugeau A, 2010, IEEE T IMAGE PROCESS, V19, P2634, DOI 10.1109/TIP.2010.2049240
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   CAI N, 2015, VISUAL COMPUT, P1
   Chan TF, 2010, SIAM J APPL MATH, V63, P491
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Criminisi A., 2003, PROC CVPR IEEE, V2, pII, DOI DOI 10.1109/CVPR.2003.1211538
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gracias N, 2009, IMAGE VISION COMPUT, V27, P597, DOI 10.1016/j.imavis.2008.04.014
   Hao CY, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-015-5359-x
   Hua M, 2015, VISUAL COMPUT, V31, P1113, DOI 10.1007/s00371-015-1126-7
   Ignacio UA, 2007, VISUAL COMPUT, V23, P733, DOI 10.1007/s00371-007-0139-2
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Liu BW, 2017, CGI'17: PROCEEDINGS OF THE COMPUTER GRAPHICS INTERNATIONAL CONFERENCE, DOI 10.1145/3095140.3095143
   NILL NB, 1992, OPT ENG, V31, P813, DOI 10.1117/12.56114
   Qin XM, 2015, IEEE T CYBERNETICS, V45, P1549, DOI 10.1109/TCYB.2014.2355140
   Qin XM, 2015, IEEE T MULTIMEDIA, V17, P295, DOI 10.1109/TMM.2015.2395078
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Shen JB, 2007, COMPUT GRAPH-UK, V31, P119, DOI 10.1016/j.cag.2006.10.004
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Whitaker RT, 2000, IEEE T IMAGE PROCESS, V9, P1849, DOI 10.1109/83.877208
NR 32
TC 4
Z9 5
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 85
EP 98
DI 10.1007/s00371-017-1454-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900008
DA 2024-07-18
ER

PT J
AU Teng, CH
   Chuo, KY
   Hsieh, CY
AF Teng, Chin-Hung
   Chuo, Kai-Yuan
   Hsieh, Chen-Yuan
TI Reconstructing three-dimensional models of objects using a Kinect sensor
SO VISUAL COMPUTER
LA English
DT Article
DE 3D reconstruction; Kinect sensor; RGB-D sensor; Lazy Snapping;
   KinectFusion
AB Advanced sensor technology has allowed us to acquire three-dimensional (3D) information from a scene using a low-cost RGB-D sensor such as Kinect. Although this sensor can recover the 3D structure of a scene, it cannot distinguish a target object from the background. In view of this, we incorporate an interactive 3D segmentation algorithm with a well-known Kinect scene reconstruction system, the KinectFusion, to effectively extract an object from the scene, and hence obtain a 3D point cloud of the object. With this system, a user can freely move the Kinect sensor to reconstruct the scene and then select the foreground/background seeds from the reconstructed point cloud. The system can take over the following tasks to complete the 3D reconstruction of the selected object. The advantage of this system is that users need not select the foreground/background seeds very carefully, which greatly reduces the operational complexity. Moreover, previous segmentation results are inherited to the next phase as new foreground/background seeds, which minimizes the required user intervention. With a simple seed selection, the point cloud of the selected object can be gradually recovered when a user moves the sensor to different viewpoints. Several experiments were conducted, and the results confirmed the effectiveness of the proposed system. The 3D structures of objects with complex shapes are well reconstructed by our system.
C1 [Teng, Chin-Hung] Yuan Ze Univ, Innovat Ctr Big Data & Digital Convergence, 135 Yuan Tung Rd, Chungli, Taiwan.
   [Teng, Chin-Hung; Chuo, Kai-Yuan; Hsieh, Chen-Yuan] Yuan Ze Univ, Dept Informat Commun, 135 Yuan Tung Rd, Chungli, Taiwan.
C3 Yuan Ze University; Yuan Ze University
RP Teng, CH (corresponding author), Yuan Ze Univ, Innovat Ctr Big Data & Digital Convergence, 135 Yuan Tung Rd, Chungli, Taiwan.; Teng, CH (corresponding author), Yuan Ze Univ, Dept Informat Commun, 135 Yuan Tung Rd, Chungli, Taiwan.
EM chteng@saturn.yzu.edu.tw; s1026428@mail.yzu.edu.tw;
   s976424@mail.yzu.edu.tw
FU Ministry of Science and Technology, Taiwan [NSC 102-2221-E-155-075, MOST
   105-2218-E-155-010]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan, under Grant Nos. NSC 102-2221-E-155-075 and MOST
   105-2218-E-155-010.
CR Albitar C, 2007, IEEE I CONF COMP VIS, P1207
   [Anonymous], WORKSH COL DEPTH CAM
   [Anonymous], 2010, P INT S EXP ROB ISER
   Anwer Atif, 2016, 2016 6th International Conference on Intelligent and Advanced Systems (ICIAS), DOI 10.1109/ICIAS.2016.7824132
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bradley D., 2008, CVPR, P1
   Chen CM, 2013, INT CONF INFO SCI, P986, DOI 10.1109/ICIST.2013.6747702
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Clark J. J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P29, DOI 10.1109/CVPR.1992.223231
   COCHRAN SD, 1992, IEEE T PATTERN ANAL, V14, P981, DOI 10.1109/34.159902
   Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19
   Du H, 2011, UBICOMP'11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P75
   Endres F, 2012, IEEE INT CONF ROBOT, P1691, DOI 10.1109/ICRA.2012.6225199
   Engelhard N., 2011, RGB WORKSH 3D PERC R
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Geng J, 2011, ADV OPT PHOTONICS, V3, P128, DOI 10.1364/AOP.3.000128
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Higo T, 2009, IEEE I CONF COMP VIS, P1234, DOI 10.1109/ICCV.2009.5459331
   Horaud R, 2016, MACH VISION APPL, V27, P1005, DOI 10.1007/s00138-016-0784-4
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meilland M., 2012, WORKSH NAV POS MAPP
   Meilland M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.45
   Morana M, 2014, ADV INTERNET THINGS, P179
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Pan RJ, 2016, VISUAL COMPUT, V32, P601, DOI 10.1007/s00371-015-1076-0
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Ribo M, 2005, INT WORKSH ROB SENS, P2
   Roth H, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.112
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sahillioglu Y, 2010, COMPUT VIS IMAGE UND, V114, P334, DOI 10.1016/j.cviu.2009.12.003
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Song Tiangang, 2013, International Journal of Computer Theory and Engineering, V5, P567, DOI 10.7763/IJCTE.2013.V5.751
   Song XB, 2014, VISUAL COMPUT, V30, P855, DOI 10.1007/s00371-014-0965-y
   Tomasi C., 1998, P IEEE INT C COMP VI
   Valentin J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2751556
   van den Hengel A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239537, 10.1145/1276377.1276485]
   Whelan T., 2012, TECH REP
   Wolberg G, 2018, VISUAL COMPUT, V34, P605, DOI 10.1007/s00371-017-1365-x
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   ZHENG JY, 1994, IEEE T PATTERN ANAL, V16, P163, DOI 10.1109/34.273734
NR 43
TC 6
Z9 8
U1 2
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1507
EP 1523
DI 10.1007/s00371-017-1425-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700004
DA 2024-07-18
ER

PT J
AU Zhao, JF
   Mao, X
   Zhang, J
AF Zhao, Jianfeng
   Mao, Xia
   Zhang, Jian
TI Learning deep facial expression features from image and optical flow
   sequences using 3D CNN
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Static features; Dynamic features; 3D
   convolutional neural networks; Accumulative optical flow
ID RECOGNITION
AB Facial expression is highly correlated with the facial motion. According to whether the temporal information of facial motion is used or not, the facial expression features can be classified as static and dynamic features. The former, which mainly includes the geometric features and appearance features, can be extracted by convolution or other learning filters; the latter, which are aimed to model the dynamic properties of facial motion, can be calculated through optical flow or other methods, respectively. When 3D convolutional neural networks (CNNs) are introduced, the extraction of two different types of features mentioned above becomes easy. In this paper, one 3D CNN architecture is presented to learn the static and dynamic features from facial image sequences and extract high-level dynamic features from optical flow sequences. Two types of dense optical flow, which contain the tracking information of facial muscle movement, are calculated according to different image pair construction methods. One is the common optical flow, and the other is an enhanced optical flow which is called accumulative optical flow. Four components of each type of optical flow are used in experiments. Three databases, two acted databases and one nearly realistic database, are selected to conduct the experiments. The experiments on the two acted databases achieve state-of-the-art accuracy, and indicate that the vertical component of optical flow has an advantage over other components in recognizing facial expression. The experimental results on the three selected databases show that more discriminative features can be learned from image sequences than from optical flow or accumulative optical flow sequences, and the accumulative optical flow contains more motion information than optical flow if the frame distance of the image pairs used to calculate them is not too large.
C1 [Zhao, Jianfeng; Mao, Xia] Beihang Univ, Sch Elect & Informat Engn, Mailbox 206,37 XueYuan Rd, Beijing 100191, Peoples R China.
   [Zhao, Jianfeng] Inner Mongolia Univ Sci & Technol, Sch Elect & Informat Engn, Baotou 014010, Peoples R China.
   [Zhang, Jian] Univ Technol Sydney, Sch Comp & Commun, Sydney, NSW, Australia.
C3 Beihang University; Inner Mongolia University of Science & Technology;
   University of Technology Sydney
RP Mao, X (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Mailbox 206,37 XueYuan Rd, Beijing 100191, Peoples R China.
EM moukyou@buaa.edu.cn
OI Zhang, Jian/0000-0002-7240-3541
CR Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   [Anonymous], 2017 12 IEEE INT C A
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2014, ACM ICMI
   [Anonymous], AM J PHYS ANTHR
   Barros P, 2016, ADAPT BEHAV, V24, P373, DOI 10.1177/1059712316664017
   Barros P, 2015, IEEE-RAS INT C HUMAN, P582, DOI 10.1109/HUMANOIDS.2015.7363421
   Behnke S, 2003, IEEE IJCNN, P2758
   Berger J, 2014, PHILOS PSYCHOL, V27, P829, DOI 10.1080/09515089.2013.771241
   Bergstra J., 2011, Adv. Neural Inf. Process. Syst., V24
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Charles Darwin, 1872, EXPRESSION EMOTIONS
   Chi J, 2014, VISUAL COMPUT, V30, P649, DOI 10.1007/s00371-014-0960-3
   Chollet F, 2015, KERAS
   Cohn J., 2005, What the face reveals(2nd edition):Basic and applied studies of spontaneous expression using the Facial Action Coding System, P388
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   Dhall A, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2993148.2997638
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Fan XJ, 2017, PATTERN RECOGN, V64, P399, DOI 10.1016/j.patcog.2016.12.002
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Gharavian D, 2017, MULTIMED TOOLS APPL, V76, P2331, DOI 10.1007/s11042-015-3180-6
   GIBSON JJ, 1950, AM J PSYCHOL, V63, P367, DOI 10.2307/1418003
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goto T, 2002, SIGNAL PROCESS-IMAGE, V17, P243, DOI 10.1016/S0923-5965(01)00021-2
   Happy SL, 2015, 2015 EIGHTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION (ICAPR), P67
   Haq S., 2009, INT C AUDITORY VISUA, P53
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang YZ, 2010, IEEE T MULTIMEDIA, V12, P536, DOI 10.1109/TMM.2010.2052792
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Kanade T., 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]
   Kaya H, 2017, IMAGE VISION COMPUT, V65, P66, DOI 10.1016/j.imavis.2017.01.012
   Khorrami P., 2016, 2016 IEEE INT C IM P
   Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0
   KROGH A, 1992, ADV NEUR IN, V4, P950
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee SH, 2016, PATTERN RECOGN, V54, P52, DOI 10.1016/j.patcog.2015.12.016
   Levi G, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P503, DOI 10.1145/2823327.2823333
   Loughrey J., 2005, USING EARLY STOPPING
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mayya V, 2016, PROCEDIA COMPUT SCI, V93, P453, DOI 10.1016/j.procs.2016.07.233
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Pantic M., 2007, FACE RECOGNITION, P77
   Russell JamesA., 1997, The Psychology of Facial Expression
   Saha C, 2015, EMOTION RECOGNITION: A PATTERN ANALYSIS APPROACH, P69
   Snoek J., 2012, Advances in Neural Information Processing Systems, V25, DOI DOI 10.48550/ARXIV.1206.2944
   Srivastava N., 1929, J MACH LEARN RES, V15
   Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629
   Tian Y.-L., HDB FACE RECOGNITION, P247
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   WILSON P.I., 2006, J COMPUT SMALL COLL, V21, P127
   Zhang YM, 2005, IEEE T PATTERN ANAL, V27, P699, DOI 10.1109/TPAMI.2005.93
NR 54
TC 48
Z9 53
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1461
EP 1475
DI 10.1007/s00371-018-1477-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400014
DA 2024-07-18
ER

PT J
AU Liu, YL
   Chen, XH
   Gu, TL
   Zhang, YC
   Xing, GY
AF Liu, Yanli
   Chen, Xianghui
   Gu, Tianlun
   Zhang, Yanci
   Xing, Guanyu
TI Real-time camera pose estimation via line tracking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Augmented reality; Camera calibration; Vanishing points; Geometric
   constraints
ID VANISHING POINTS
AB Real-time camera calibration has been intensively studied in augmented reality. However, for texture-less and texture-repeated scenes as well as poorly illuminated scenes, obtaining a stable calibration is still an open problem. In the paper, we propose a method of calibrating a live video by tracking orthogonal vanishing points. Since vanishing points cannot be obtained directly on the image, the tracking is achieved by tracking parallel lines. This is a changeling problem due to the fact that vanishing points are sensitive to image noise, camera movement, and illumination variation. We tackle the challenges by three optimization procedures and flexible process of degenerated cases. During three optimizations, several explicitly geometric constraints are incorporated, ensuring the calibration result robust to poor illumination and camera movement. A variety of challenging examples demonstrate that the proposed algorithm outperforms state-of-the-art methods for texture-less and texture-repeated scenes.
C1 [Liu, Yanli; Chen, Xianghui; Gu, Tianlun; Zhang, Yanci] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
   [Liu, Yanli; Chen, Xianghui; Gu, Tianlun; Zhang, Yanci] Sichuan Univ, Natl Key Lab Fundamental Sci Synthet Vis, Chengdu, Sichuan, Peoples R China.
   [Xing, Guanyu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
C3 Sichuan University; Sichuan University; University of Electronic Science
   & Technology of China
RP Xing, GY (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
EM xingguanyu@uestc.edu.cn
FU National Natural Science Foundation (NSFC) of China [61572333, 61472261,
   61402081]
FX This study was funded by National Natural Science Foundation (NSFC) of
   China (Nos. 61572333, 61472261, 61402081).
CR Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802
   Boulanger K., 2006, TPCG, P71
   Cantoni V, 2001, 11TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P90, DOI 10.1109/ICIAP.2001.956990
   Elloumi W., 2013, INT C COMP VIS THEOR
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   He Q., 2007, IEEE INT C APPL SPEC, P203
   Hornacek M., 2010, COMPUTER VISION PATT, P953
   Junejo I., 2006, IEEE INT C VID SIGN, P77
   Kroeger T, 2015, PROC CVPR IEEE, P2449, DOI 10.1109/CVPR.2015.7298859
   Kroeger T, 2015, 2015 IEEE WINTER APPLICATIONS AND COMPUTER VISION WORKSHOPS (WACVW), P63, DOI 10.1109/WACVW.2015.6
   Liu HM, 2016, INT SYM MIX AUGMENT, P1, DOI 10.1109/ISMAR.2016.24
   Liu R, 2009, 2009 INTERNATIONAL CONFERENCE ON MEASURING TECHNOLOGY AND MECHATRONICS AUTOMATION, VOL I, P352, DOI 10.1109/ICMTMA.2009.338
   Liu YL, 2012, IEEE T VIS COMPUT GR, V18, P573, DOI 10.1109/TVCG.2012.53
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XH, 2015, IEEE IMAGE PROC, P507, DOI 10.1109/ICIP.2015.7350850
   LUTTON E, 1994, IEEE T PATTERN ANAL, V16, P430, DOI 10.1109/34.277598
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Orghidan R, 2012, FED CONF COMPUT SCI, P123
   QUAN L, 1989, PATTERN RECOGN LETT, V9, P279, DOI 10.1016/0167-8655(89)90006-8
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sánchez J, 2013, IMAGE PROCESS ON LIN, V3, P252, DOI 10.5201/ipol.2013.21
   Simon G, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P120, DOI 10.1109/ISAR.2000.880935
   Straub J, 2014, PROC CVPR IEEE, P3770, DOI 10.1109/CVPR.2014.488
   Tardif Jean-Philippe, 2009, 2009 IEEE 12th International Conference on Computer Vision (ICCV), P1250, DOI 10.1109/ICCV.2009.5459328
   Tsai F., 2013, 34 ASIAN C REMOTE SE, V2, P1182
   Xu C, 2017, IEEE T PATTERN ANAL, V39, P1209, DOI 10.1109/TPAMI.2016.2582162
   Xuehui Chen, 2010, Proceedings of the Third International Joint Conference on Computational Sciences and Optimization (CSO 2010), P440, DOI 10.1109/CSO.2010.163
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 28
TC 12
Z9 13
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 899
EP 909
DI 10.1007/s00371-018-1523-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400014
DA 2024-07-18
ER

PT J
AU Ju, MY
   Zhang, DY
   Wang, XM
AF Ju, Mingye
   Zhang, Dengyin
   Wang, Xuemei
TI Single image dehazing via an improved atmospheric scattering model
SO VISUAL COMPUTER
LA English
DT Article
DE Atmospheric scattering model; Single image dehazing; Scene segmentation;
   Guided total variation model
ID VISION
AB Under foggy or hazy weather conditions, the visibility and color fidelity of outdoor images are prone to degradation. Hazy images can be the cause of serious errors in many computer vision systems. Consequently, image haze removal has practical significance for real-world applications. In this study, we first analyze the inherent weaknesses of the atmospheric scattering model and propose an improvement to address those weaknesses. Then, we present a fast image haze removal algorithm based on the improved model. In our proposed method, the input image is partitioned into several scenes based on the haze thickness. Next, averaging and erosion operations calculate the rough scene luminance map in a scene-wise manner. We obtain the rough scene transmission map by maximizing the contrast in each scene and then develop a way to gently remove the haze using an adaptive method for adjusting scene transmission based on scene features. In addition, we propose a guided total variation model for edge optimization, so as to prevent from the block effect as well as to eliminate the negative effect from the wrong scene segmentation results. The experimental results demonstrate that our method is effective in solving a series of common problems, including uneven illuminance, overenhanced and oversaturated images, and so forth. Moreover, our method outperforms most current dehazing algorithms in terms of visual effects, universality, and processing speed.
C1 [Ju, Mingye; Zhang, Dengyin; Wang, Xuemei] Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Zhang, DY (corresponding author), Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing, Jiangsu, Peoples R China.
EM 2014070245@njupt.edu.cn
RI Wang, Xuemei/GXF-3702-2022; Ju, Mingye/AAZ-2612-2020
OI Zhang, Dengyin/0000-0001-6080-6151; Ju, Mingye/0000-0003-4378-3781
FU National Natural Science Foundations of P. R. China [61571241]; Jiangsu
   Province Graduate Research and Innovation Project [CXZZ130476]; Science
   Research Fund of NUPT [NY215169]
FX The authors wish to thank Dr. Pengfei Wu and Dr. Zhenfei Gu for their
   help with proofreading. We would also like to thank the reviewers for
   their valuable comments. This work is supported by the National Natural
   Science Foundations of P. R. China (Grant No. 61571241), the Jiangsu
   Province Graduate Research and Innovation Project (Grant No.
   CXZZ130476), and the Science Research Fund of NUPT (Grant No. NY215169).
CR [Anonymous], OPTIK INT J LIGHT EL
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Khmag A, 2016, IEEJ T ELECTR ELECTR, V11, P339, DOI 10.1002/tee.22223
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li B, 2014, IET COMPUT VIS, V8, P131, DOI 10.1049/iet-cvi.2013.0011
   Li JF, 2015, NEUROCOMPUTING, V156, P1, DOI 10.1016/j.neucom.2015.01.026
   Liu CX, 2016, VISUAL COMPUT, V32, P911, DOI 10.1007/s00371-016-1259-3
   Liu Q, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-014-5143-3
   Liu X, 2013, IEEE IMAGE PROC, P909, DOI 10.1109/ICIP.2013.6738188
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   [南栋 Nan Dong], 2015, [电子学报, Acta Electronica Sinica], V43, P500
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Shao L, 2014, IEEE T CYBERNETICS, V44, P1001, DOI 10.1109/TCYB.2013.2278548
   Shi ZW, 2014, OPTIK, V125, P3868, DOI 10.1016/j.ijleo.2014.01.170
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Tan R. T., 2008, IEEE C COMPUTERVISIO, P1
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 33
TC 44
Z9 47
U1 0
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1613
EP 1625
DI 10.1007/s00371-016-1305-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600010
OA hybrid
DA 2024-07-18
ER

PT J
AU Díaz-García, J
   Brunet, P
   Navazo, I
   Perez, F
   Vázquez, PP
AF Diaz-Garcia, Jesus
   Brunet, Pere
   Navazo, Isabel
   Perez, Frederic
   Vazquez, Pere-Pau
TI Adaptive transfer functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Transfer function; Multiresolution volume model; Direct volume
   rendering; Level of detail
ID VOLUME VISUALIZATION; DATA REDUCTION; FRAMEWORK; OCTREE
AB Medical datasets are continuously increasing in size. Although larger models may be available for certain research purposes, in the common clinical practice the models are usually of up to voxels. These resolutions exceed the capabilities of conventional GPUs, the ones usually found in the medical doctors' desktop PCs. Commercial solutions typically reduce the data by downsampling the dataset iteratively until it fits the available target specifications. The data loss reduces the visualization quality and this is not commonly compensated with other actions that might alleviate its effects. In this paper, we propose adaptive transfer functions, an algorithm that improves the transfer function in downsampled multiresolution models so that the quality of renderings is highly improved. The technique is simple and lightweight, and it is suitable, not only to visualize huge models that would not fit in a GPU, but also to render not-so-large models in mobile GPUs, which are less capable than their desktop counterparts. Moreover, it can also be used to accelerate rendering frame rates using lower levels of the multiresolution hierarchy while still maintaining high-quality results in a focus and context approach. We also show an evaluation of these results based on perceptual metrics.
C1 [Diaz-Garcia, Jesus] Univ Politecn Cataluna, Barcelona, Spain.
   [Brunet, Pere] Univ Politecn Cataluna, Comp Graph, Barcelona, Spain.
   [Navazo, Isabel] Univ Politecn Cataluna, Visualizat Virtual Real & Interact Grp ViRVIG, Barcelona, Spain.
   [Vazquez, Pere-Pau] Univ Politecn Cataluna, Comp Sci, Barcelona, Spain.
   [Diaz-Garcia, Jesus; Perez, Frederic] Alma IT Syst, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya; Universitat Politecnica de
   Catalunya; Universitat Politecnica de Catalunya; Universitat Politecnica
   de Catalunya
RP Díaz-García, J (corresponding author), Univ Politecn Cataluna, Barcelona, Spain.; Díaz-García, J (corresponding author), Alma IT Syst, Barcelona, Spain.
EM jesusdz@cs.upc.edu
RI Vázquez, Pere-Pau/HTP-9691-2023
OI Vázquez, Pere-Pau/0000-0003-4638-4065
CR [Anonymous], P EUROVIS 2014
   [Anonymous], IEEE INT S VOL POINT
   Bergner S, 2006, IEEE T VIS COMPUT GR, V12, P1353, DOI 10.1109/TVCG.2006.113
   Boada I, 2001, VISUAL COMPUT, V17, P185, DOI 10.1007/PL00013406
   Crassin C., 2009, P 2009 S INT 3D GRAP, P15, DOI [10.1145/1507149.1507152, DOI 10.1145/1507149.1507152]
   Fisher M, 2013, INT J COMPUT ASS RAD, V8, P313, DOI 10.1007/s11548-012-0783-5
   Fogal Thomas, 2013, Proc IEEE Symp Large Scale Data Anal Vis, V2013, P43, DOI 10.1109/LDAV.2013.6675157
   Gobbetti E, 2008, VISUAL COMPUT, V24, P797, DOI 10.1007/s00371-008-0261-9
   Gobbetti E, 2012, COMPUT GRAPH FORUM, V31, P1315, DOI 10.1111/j.1467-8659.2012.03124.x
   Guthe S, 2004, COMPUT GRAPH-UK, V28, P51, DOI 10.1016/j.cag.2003.10.018
   Hadwiger M., 2006, REAL TIME VOLUME GRA
   Jankun-Kelly TJ, 2001, SPRING EUROGRAP, P51
   Kitware Inc., 2014, VES VTK OPENGL ES RE
   Knoll A, 2011, IEEE PAC VIS SYMP, P3, DOI 10.1109/PACIFICVIS.2011.5742355
   Knoll AM, 2009, VISUAL COMPUT, V25, P209, DOI 10.1007/s00371-008-0215-2
   Kraus Martin., 2008, Proceedings of Vision, Modeling and Visualization, P323
   LaMar E., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P355, DOI 10.1109/VISUAL.1999.809908
   Ljung P, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P25
   Martin S., 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P19, DOI 10.1109/LDAV.2012.6378971
   Mobeen M. M., 2012, 2012 International Conference on Information Society (i-Society), P93
   Moser M., 2008, Vision, Modeling, and Visualization VMV '08 Conference Proceedings, P217
   Rodríguez MB, 2014, COMPUT GRAPH FORUM, V33, P77, DOI 10.1111/cgf.12280
   Ruiz M, 2011, IEEE T VIS COMPUT GR, V17, P1932, DOI 10.1109/TVCG.2011.173
   Sereda P., 2006, Proceedings of Eurographics/IEEE VGTC Symp on Visualization, P243
   Sicat R, 2014, IEEE T VIS COMPUT GR, V20, P2417, DOI 10.1109/TVCG.2014.2346324
   Sousa R, 2009, LECT NOTES COMPUT SC, V5726, P870, DOI 10.1007/978-3-642-03655-2_95
   Thelen S, 2011, LECT NOTES COMPUT SC, V6431, P142, DOI 10.1007/978-3-642-19641-6_10
   Wang CL, 2005, VOLUME GRAPHICS 2005, P11
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P171, DOI 10.1109/TVCG.2010.34
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weiler M., 2000, Volume Visualization and Graphics Symposium 2000, VVis, P7
   Xu X, 2014, COMPUT GRAPH FORUM, V33, P111, DOI 10.1111/cgf.12367
   Younesy Hamid., 2006, Proc. EuroVis 2006, P251
NR 33
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 835
EP 845
DI 10.1007/s00371-016-1253-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Giachetti, A
   Isaia, L
   Garro, V
AF Giachetti, Andrea
   Isaia, Luca
   Garro, Valeria
TI Multiscale descriptors and metric learning for human body shape
   retrieval
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Shape retrieval; Body scan; Metric learning; Re-identification
AB The aim of this paper was to show the usefulness of applying feature projection or metric learning techniques to multiscale descriptor spaces for the effective retrieval of human bodies of labeled subjects. Using learned subspace projections it is possible to strongly improve the retrieval performance obtained with state-of-the-art global descriptors, and, in some cases, to perform an effective feature fusion. Results obtained on different human scan datasets show that Linear Discriminant Analysis, applied to Histograms of Area Projection Transform and Shape DNA features after a preliminary dimensionality reduction, creates compact descriptors that are quite effective in improving the subject retrieval scores both when class (subject) examples are available in the training set and when only examples of classes not included in the test set are used for training. Other mappings tested are less effective even if still able to improve the results. Retrieval scores obtained in the same experimental settings used in recent related papers show that the approach based on our mapped features largely outperforms the other methods proposed for the task, even those specifically designed for human body characterization.
C1 [Giachetti, Andrea; Isaia, Luca] Univ Verona, Dipartimento Informat, Verona, Italy.
   [Garro, Valeria] CNR, Ist Sci & Tecnol Informaz A Faedo, Pisa, Italy.
C3 University of Verona; Consiglio Nazionale delle Ricerche (CNR); Istituto
   di Scienza e Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP Giachetti, A (corresponding author), Univ Verona, Dipartimento Informat, Verona, Italy.
EM andrea.giachetti@univr.it
RI Giachetti, Andrea/AAD-8247-2020
OI Giachetti, Andrea/0000-0002-7523-6806
CR [Anonymous], 2008, P 2008 INT C CONTENT, DOI DOI 10.1145/1386352.1386373
   [Anonymous], 2014, Proceedings of the Eurographics Workshop on 3D Object Retrieval
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   [Anonymous], 2010, EUROGRAPHICS WORKSHO
   Barra V, 2013, PATTERN RECOGN, V46, P2985, DOI 10.1016/j.patcog.2013.03.019
   Ben-Chen M., 2008, Proceedings of the 1st Eurographics Conference on 3D Object Retrieval, P1
   BOGO F., 2014, P CVPR IEEE
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118
   Duin R., 2005, CLASSIFICATION PARAM
   Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x
   Globerson A, 2006, ADV NEURAL INFORM PR, P451
   Goldberger J., 2004, Advances in Neural Information Processing Systems
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Leifman G, 2005, VISUAL COMPUT, V21, P865, DOI 10.1007/s00371-005-0341-z
   Lian Z., 2015, PROC 8 EUROGRAPHICS, P107
   Lian ZH, 2010, IEEE IMAGE PROC, P3181, DOI 10.1109/ICIP.2010.5654226
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Marini S., 2010, P 3DOR, P31, DOI DOI 10.2312/3DOR/3DOR10/031-038
   OHBUCHI R., 2010, Proceedings of the ACM workshop on 3D object retrieval, P63
   Pickup D., 2014, PROC 7 EUROGRAPHICS, P101
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Van Der Maaten Laurens, 2009, Journal of Machine Learning Research, V10, P13
   Wang JP, 2015, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2015.186
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
NR 29
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 693
EP 703
DI 10.1007/s00371-016-1234-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600003
DA 2024-07-18
ER

PT J
AU Qian, GP
   Tang, M
   Tong, RF
   Zhang, XH
   Pan, RF
AF Qian, Guiping
   Tang, Min
   Tong, Ruofeng
   Zhang, Xiaohong
   Pan, Ruifang
TI Interactive mesh cloning driven by boundary loop
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Mesh deformation; Mesh cloning; Pyramid spherical coordinates; B-spline
   fitting
AB In order to copy arbitrary irregular mesh between two models continuously, this paper presents an interactive mesh cloning approach based on pyramid spherical coordinates driven by boundary loop. The approach extends an existing algorithm for computing offset membrane on mesh. A parametric paint brush is constructed to define canvas both on the source mesh and the target mesh. They are mapped onto a 2D parametric domain using discrete geodesic polar maps to register correspondingly. During cloning, the boundary loop of the region of interest (ROI) on the target mesh is fitted in real time by B-spline curve to register the boundary loop of the source ROI. Via the reconstructed boundary loop, the ROI is deformed to register the target mesh by pyramid spherical coordinates to ensure that the clone result is seamless and natural. Our approach can clone arbitrary irregular meshes between two 3D models, even if the mesh is non-manifold. The cloning process is operated in real time by GPU acceleration. Experimental results demonstrate the effectiveness of our interactive mesh cloning.
C1 [Qian, Guiping; Zhang, Xiaohong; Pan, Ruifang] Zhejiang Univ Media & Commun, Coll New Media, Hangzhou 310018, Zhejiang, Peoples R China.
   [Tang, Min; Tong, Ruofeng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Communication University of Zhejiang; Zhejiang University
RP Qian, GP (corresponding author), Zhejiang Univ Media & Commun, Coll New Media, Hangzhou 310018, Zhejiang, Peoples R China.
EM qianguiping@163.com
RI Tang, Min/KOC-3090-2024; Zhang, Xiaohong/A-3060-2015
FU National Basic Research Program of China [2011CB302205]; Zhejiang
   Provincial Natural Science Foundation of China [LY12A01028, LY14F020050,
   LY13F020036]; Zhejiang Leading Team of Science and Technology Innovation
   of China [2011R50019-06]
FX We thank all anonymous reviewers for their valuable comments. This work
   was supported by National Basic Research Program of China (No.
   2011CB302205), Zhejiang Provincial Natural Science Foundation of China
   (No. LY12A01028, LY14F020050 and LY13F020036) and Zhejiang Leading Team
   of Science and Technology Innovation of China (No. 2011R50019-06).
CR Alhashim I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601102
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Bernstein GL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462027
   Cai D, 2010, ACM SIGGRAPH 2010 TA
   Chen Y, 2011, COMPUT AIDED DESIGN, V43, P1674, DOI 10.1016/j.cad.2011.07.011
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Floater MS, 2005, COMPUT AIDED GEOM D, V22, P623, DOI 10.1016/j.cagd.2005.06.004
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Fu HB, 2007, COMPUT GRAPH FORUM, V26, P34, DOI 10.1111/j.1467-8659.2007.00940.x
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Huang X., 2007, PROC SPM 07, P35
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Langer T., 2006, S GEOMETRY PROCESSIN, P81
   Li XY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461917
   Li XY, 2013, IEEE T VIS COMPUT GR, V19, P344, DOI 10.1109/TVCG.2012.109
   Lipman Y., 2007, S GEOM PROC, P117
   Lipman Y, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360677
   Melvær EL, 2012, COMPUT GRAPH FORUM, V31, P2423, DOI 10.1111/j.1467-8659.2012.03187.x
   Rustamov RM, 2010, COMPUT GRAPH FORUM, V29, P1507, DOI 10.1111/j.1467-8659.2010.01759.x
   Schmidt R, 2013, COMPUT GRAPH FORUM, V32, P255, DOI 10.1111/cgf.12045
   Takayama K, 2011, COMPUT GRAPH FORUM, V30, P613, DOI 10.1111/j.1467-8659.2011.01883.x
   Thiery JM, 2014, VISUAL COMPUT, V30, P981, DOI 10.1007/s00371-013-0889-y
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zatzarinni R, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1618452.1618482, 10.1145/1618452.1616482]
NR 24
TC 1
Z9 2
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 513
EP 521
DI 10.1007/s00371-015-1085-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200009
DA 2024-07-18
ER

PT J
AU Buschmann, S
   Trapp, M
   Döllner, J
AF Buschmann, Stefan
   Trapp, Matthias
   Doellner, Juergen
TI Animated visualization of spatial-temporal trajectory data for
   air-traffic analysis
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds (CW)
CY OCT 06-08, 2014
CL Santander, SPAIN
SP IEEE Comp Soc, Univ Cantabria, Comp Graph & Geometr Modeling Grp, Toho Univ, Fac Sci, Dept Informat Sci, European Assoc Comp Graph, Int Federat Informat Proc, Workgroup 5 10 Comp Graph & Virtual Worlds, Univ Cantabria, Dept Appl Math & Computatl Sci, Vice Rector Res & Knowledge Transfer, Municipal Santander, Reg Govt Cantabria, Spanish Minist Econ & Competitiveness, Cantabria Campus Int, Int Federat Informat Proc, Tech Comm 5 Informat Technol Applicat
DE Spatio-temporal visualization; Trajectory visualization; 3D
   visualization; Visual analytics; Real-time rendering
ID SPACE-TIME CUBE
AB With increasing numbers of flights worldwide and a continuing rise in airport traffic, air-traffic management is faced with a number of challenges. These include monitoring, reporting, planning, and problem analysis of past and current air traffic, e.g., to identify hotspots, minimize delays, or to optimize sector assignments to air-traffic controllers. To cope with these challenges, cyber worlds can be used for interactive visual analysis and analytical reasoning based on aircraft trajectory data. However, with growing data size and complexity, visualization requires high computational efficiency to process that data within real-time constraints. This paper presents a technique for real-time animated visualization of massive trajectory data. It enables (1) interactive spatio-temporal filtering, (2) generic mapping of trajectory attributes to geometric representations and appearance, and (3) real-time rendering within 3D virtual environments such as virtual 3D airport or 3D city models. Different visualization metaphors can be efficiently built upon this technique such as temporal focus+context, density maps, or overview+detail methods. As a general-purpose visualization technique, it can be applied to general 3D and 3+1D trajectory data, e.g., traffic movement data, geo-referenced networks, or spatio-temporal data, and it supports related visual analytics and data mining tasks within cyber worlds.
C1 [Buschmann, Stefan; Trapp, Matthias; Doellner, Juergen] Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
C3 University of Potsdam
RP Buschmann, S (corresponding author), Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
EM stefan.buschmann@hpi.de
RI Trapp, Matthias/J-4456-2014
OI Trapp, Matthias/0000-0003-3861-5759
CR Andrienko G., 2014, CARTOGRAPHY POLE POL, P157
   Andrienko G, 2010, INT J GEOGR INF SCI, V24, P1577, DOI 10.1080/13658816.2010.508043
   Andrienko G, 2009, LECT NOTES GEOINF CA, P3, DOI 10.1007/978-3-642-00304-2_1
   Andrienko N., 2003, 21 INT CART C, P1981
   Andrienko N., 2005, EXPLORATORY ANAL SPA
   [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 2018, Real-Time Rendering
   [Anonymous], THEORY PRACTICE COMP
   [Anonymous], 1994, ACM Transactions on Computer-Human Interaction, DOI [10.1145/180171.180173, DOI 10.1145/180171.180173]
   [Anonymous], 2010, SIGKDD Explor. Newsl, DOI DOI 10.1145/1809400.1809405
   Bavoil L., 2008, NVIDIA, V6
   Carvalho A, 2008, INFORM VISUAL, V7, P265, DOI 10.1057/palgrave.ivs.9500188
   Chang R, 2014, ACM T INTERACT INTEL, V4, DOI 10.1145/2594648
   Elmqvist N, 2008, IEEE T VIS COMPUT GR, V14, P1095, DOI 10.1109/TVCG.2008.59
   Hagerstrand T, 1970, PAPERS REGIONAL SCI, V24, P7, DOI [10.1007/BF01936872, DOI 10.1111/J.1435-5597.1970.TB01464.X]
   Hurter C, 2012, COMPUT GRAPH FORUM, V31, P865, DOI 10.1111/j.1467-8659.2012.03079.x
   Hurter C., 2014, COMPUTERS ENV URBAN
   Hurter C., 2013, IEEE TVCG
   Hurter C., 2014, TRANSP RES C
   Hurter C, 2009, IEEE T VIS COMPUT GR, V15, P1017, DOI 10.1109/TVCG.2009.145
   KESSENICH J., 2014, OPENGL SHADING LANGU
   Klein T., 2014, VISAPP 2014
   Kraak M, 2003, P 21 INT CART C, P10
   Kraak MJ, 2005, DEVELOPMENTS IN SPATIAL DATA HANDLING, P189, DOI 10.1007/3-540-26772-7_15
   Kristensson PO, 2009, IEEE T VIS COMPUT GR, V15, P696, DOI 10.1109/TVCG.2008.194
   Kveladze I, 2013, CARTOGR J, V50, P201, DOI 10.1179/1743277413Y.0000000061
   Li X, 2008, CARTOGR J, V45, P193, DOI 10.1179/000870408X311387
   Lottes T., 2009, FXAA
   Luft T, 2006, ACM T GRAPHIC, V25, P1206, DOI 10.1145/1141911.1142016
   MacEachren AM, 1995, How Maps Work: Representation, Visualization, and Design
   Nienhaus M, 2005, IEEE COMPUT GRAPH, V25, P40, DOI 10.1109/MCG.2005.53
   SAITO T, 1990, ACM SIGGRAPH, V24, P197, DOI DOI 10.1145/97880.97901
   Scheepens R, 2012, IEEE COMPUT GRAPH, V32, P56, DOI 10.1109/MCG.2011.88
   Sidharth T., 2010, 14 INT C INF VIS, P336
   Tominski C., 2012, WORKSH VIS MOD VIS V, P199
   Tominski C., 9 INT C INF VIS IV 0, P175
   Tominski C, 2012, IEEE T VIS COMPUT GR, V18, P2565, DOI 10.1109/TVCG.2012.265
   Trapp M., 2013, GRAPP IVAPP, P165
   Ware C., 2000, INFORM VISUALIZATION, V2
   Willems N, 2009, COMPUT GRAPH FORUM, V28, P959, DOI 10.1111/j.1467-8659.2009.01440.x
NR 40
TC 21
Z9 22
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2016
VL 32
IS 3
BP 371
EP 381
DI 10.1007/s00371-015-1185-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DF9FK
UT WOS:000371666200010
DA 2024-07-18
ER

PT J
AU Ganestam, P
   Doggett, M
AF Ganestam, Per
   Doggett, Michael
TI Real-time multiply recursive reflections and refractions using hybrid
   rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Hybrid rendering; Real-time; Ray tracing; Rasterization; Reflections;
   Refractions
ID HARDWARE; GPU
AB We present a new method for real-time rendering of multiple recursions of reflections and refractions. The method uses the strengths of real-time ray tracing for objects close to the camera, by storing them in a per-frame constructed bounding volume hierarchy (BVH). For objects further from the camera, rasterization is used to create G-buffers which store an image-based representation of the scene outside the near objects. Rays that exit the BVH continue tracing in the G-buffers' perspective space using ray marching, and can even be reflected back into the BVH. Our hybrid renderer is to our knowledge the first method to merge real-time ray tracing techniques with image-based rendering to achieve smooth transitions from accurately ray-traced foreground objects to image-based representations in the background. We are able to achieve more complex reflections and refractions than existing screen space techniques, and offer reflections by off-screen objects. Our results demonstrate that our algorithm is capable of rendering multiple bounce reflections and refractions, for scenes with millions of triangles, at 720p resolution and above 30 FPS.
C1 [Ganestam, Per; Doggett, Michael] Lund Univ, Ole Romers Vag 3, S-22363 Lund, Sweden.
C3 Lund University
RP Ganestam, P (corresponding author), Lund Univ, Ole Romers Vag 3, S-22363 Lund, Sweden.
EM per.ganestam@cs.lth.se; michael.doggett@cs.lth.se
OI Doggett, Michael/0000-0002-4848-3481
FU ELLIIT; Intel Visual Computing Institute
FX We thank ELLIIT and Intel Visual Computing Institute for funding. Thanks
   to TurboSquid artist cjx3711 for the chess piece models.
CR Andersson J., 2012, PROGRAMMABLE SHADING
   [Anonymous], 2014, UNREAL ENGINE 4 UNRE
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Hakura ZS, 2001, SPRING EUROGRAP, P289
   Hirche J, 2004, PROC GRAPH INTERF, P153
   Hu W, 2014, VISUAL COMPUT, V30, P697, DOI 10.1007/s00371-014-0968-8
   Karras T., 2012, P 4 ACM SIGGRAPH EUR, P33, DOI [10.2312/EGGH/HPG12/033-037, DOI 10.2312/EGGH/HPG12/033-037]
   Knecht M, 2013, IEEE T VIS COMPUT GR, V19, P576, DOI 10.1109/TVCG.2013.39
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   Mara M., 2013, LIGHTING DEEP G BUFF
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   McTaggart G., 2004, DIRECT3D TUTORIAL
   Pharr M., 2010, PHYS BASED RENDERING
   Reinbothe C, 2009, EUROGRAPHICS 2009 AR
   Ritschel T, 2012, COMPUT GRAPH FORUM, V31, P160, DOI 10.1111/j.1467-8659.2012.02093.x
   Rosen P, 2011, IEEE COMPUT GRAPH, V31, P68, DOI 10.1109/MCG.2011.32
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Sebastien L., 2012, ACM SIGGRAPH 2012 Talks, page, P36
   Sousa T., 2011, ADV REAL TIME RENDER
   Sun X, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360634
   Szirmay-Kalos L, 2005, COMPUT GRAPH FORUM, V24, P695, DOI 10.1111/j.1467-8659.2005.0m894.x
   Tatarchuk N., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P63
   Thiedemann Sinje., 2011, Symposium on Interactive 3D Graphics and Games, I3D '11, P103
   WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882
   Wyman C, 2005, ACM T GRAPHIC, V24, P1050, DOI 10.1145/1073204.1073310
   Zhou K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409079
   [No title captured]
NR 27
TC 11
Z9 12
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2015
VL 31
IS 10
BP 1395
EP 1403
DI 10.1007/s00371-014-1021-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CQ7CG
UT WOS:000360759800008
DA 2024-07-18
ER

PT J
AU Che, XD
   Niu, Y
   Shui, B
   Fu, JB
   Fei, GZ
   Goswami, P
   Zhang, YN
AF Che, Xiaodong
   Niu, Yu
   Shui, Bin
   Fu, Jianbo
   Fei, Guangzheng
   Goswami, Prashant
   Zhang, Yanci
TI A novel simulation framework based on information asymmetry to evaluate
   evacuation plan
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Crowd simulation; Information asymmetry; Influence space; Emergency
   evacuation
ID CROWD; BEHAVIORS
AB In this paper, we present a novel framework to simulate the crowd behavior under emergency situations in a confined space with multiple exits. In our work, we take the information asymmetry into consideration, which is used to model the different behaviors presented by pedestrians because of their different knowledge about the environment. We categorize the factors influencing the preferred velocity into two groups, the intrinsic and extrinsic factors, which are unified into a single space called influence space. At the same time, a finite state machine is employed to control the individual behavior. Different strategies are used to compute the preferred velocity in different states, so that our framework can produce the phenomena of decision change. Our experimental results prove that our framework can be employed to analyze the factors influencing the escape time, such as the number and location of exits, the density distribution of the crowd and so on. Thus it can be used to design and evaluate the evacuation plans.
C1 [Che, Xiaodong; Niu, Yu; Shui, Bin; Fu, Jianbo; Zhang, Yanci] Sichuan Univ, Coll Comp Sci, Chengdu 610064, Peoples R China.
   [Fei, Guangzheng] Commun Univ China, Sch Animat & Digital Arts, Beijing, Peoples R China.
   [Goswami, Prashant] Blekinge Inst Technol, Dept Creat Technol, Karlskrona, Sweden.
C3 Sichuan University; Communication University of China; Blekinge
   Institute Technology
RP Zhang, YN (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610064, Peoples R China.
EM yczhang@scu.edu.cn
FU National Natural Science Foundation of China [61472261]; National Key
   Technology R&D Program of China [2012BAH62F03]
FX The work presented in this paper is supported by National Natural
   Science Foundation of China (Grant No. 61472261) and National Key
   Technology R&D Program of China (Grant No. 2012BAH62F03).
CR Abdelghany A, 2014, EUR J OPER RES, V237, P1105, DOI 10.1016/j.ejor.2014.02.054
   [Anonymous], 2005, P 1 INT WORKSH CROWD
   Augustijn-Beckers EW, 2010, PROCEDIA ENGINEER, V3, P23, DOI 10.1016/j.proeng.2010.07.005
   Ballerini M, 2008, P NATL ACAD SCI USA, V105, P1232, DOI 10.1073/pnas.0711437105
   Bode NWF, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2013.0904
   Braun A, 2003, COMP ANIM CONF PROC, P143, DOI 10.1109/CASA.2003.1199317
   Camillen F, 2009, IEEE TIC-STH 09: 2009 IEEE TORONTO INTERNATIONAL CONFERENCE: SCIENCE AND TECHNOLOGY FOR HUMANITY, P375, DOI 10.1109/TIC-STH.2009.5444471
   Chen X, 2012, COMPUT ENVIRON URBAN, V36, P207, DOI 10.1016/j.compenvurbsys.2011.11.002
   Guy S.J., 2010, P 9 INT C AUTONOMOUS, V2, P575
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   Hector Jr L., 2013, P E ASIA SOC TRANSPO, V9
   Johansson A., 2007, Adv. Complex Syst, V10, P271, DOI [10.1142/S0219525907001355, DOI 10.1142/S0219525907001355]
   Kamkarian P., 2012, ADV ARTIF INTELL, V2012, P4
   Kim S, 2015, VISUAL COMPUT, V31, P541, DOI 10.1007/s00371-014-0946-1
   Kim SH, 2012, KOREAN J ORTHOD, V42, P55, DOI 10.4041/kjod.2012.42.2.55
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Ninomiya K, 2015, COMPUT ANIMAT VIRT W, V26, P119, DOI 10.1002/cav.1622
   Papageorgiou A, 2015, VISUAL COMPUT, V31, P235, DOI 10.1007/s00371-014-1039-x
   Patrix J, 2012, INT J SWARM INTELL R, V3, P50, DOI 10.4018/jsir.2012070104
   Pelechano N, 2008, AUTOMAT CONSTR, V17, P377, DOI 10.1016/j.autcon.2007.06.005
   Pelechano N, 2006, IEEE COMPUT GRAPH, V26, P80, DOI 10.1109/MCG.2006.133
   Pluchino A., 2013, ARXIV13027153
   Sagun A, 2011, SIMUL MODEL PRACT TH, V19, P1007, DOI 10.1016/j.simpat.2010.12.001
   Sarmady S, 2011, SIMUL MODEL PRACT TH, V19, P969, DOI 10.1016/j.simpat.2010.12.004
   Shendarkar A, 2006, PROCEEDINGS OF THE 2006 WINTER SIMULATION CONFERENCE, VOLS 1-5, P545, DOI 10.1109/WSC.2006.323128
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   Vasudevan K, 2011, COMPUT IND ENG, V61, P1135, DOI 10.1016/j.cie.2011.07.003
   Wagner N, 2014, EXPERT SYST APPL, V41, P2807, DOI 10.1016/j.eswa.2013.10.013
   Zhang L, 2014, J SYST SCI COMPLEX, V27, P430, DOI 10.1007/s11424-014-3029-5
NR 30
TC 12
Z9 13
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 853
EP 861
DI 10.1007/s00371-015-1119-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500011
DA 2024-07-18
ER

PT J
AU Li, C
   Wang, CB
   Qin, H
AF Li, Chen
   Wang, ChangBo
   Qin, Hong
TI Novel adaptive SPH with geometric subdivision for brittle fracture
   animation of anisotropic materials
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Brittle fracture; Anisotropic material; Adaptive tetrahedral
   subdivision; Animation control
ID SMOOTHED PARTICLE HYDRODYNAMICS; SIMULATION
AB In this paper, we articulate a novel particle-centric method to simulate the dynamics of brittle fracture for anisotropic materials. The key motivation of this paper is to develop a new hybrid, particle-based simulation that inherits advantages from both powerful finite element methods and popular mesh-free methods, while overcoming certain disadvantages of both types of methods. Our method stems from two novel aspects: (1) a physical model built upon an improved mechanical framework and the adaptive smoothed particle hydrodynamics (SPH), an improved variant of traditional SPH, which can handle complicated anisotropic elastic behaviors with little extra cost; and (2) a hybrid, adaptive particle system that serves for more accurate fracture modeling with richer details. At the physical level, in order to facilitate better control during the formation of fracture and improve its time performance, we develop a physical framework based on contact mechanics and adopt the stress and energy analysis on the anisotropic SPH numerical integration to pinpoint fracture generation and propagation. At the geometric level, in order to reduce time consumption and enhance accuracy in rigid dynamics and fracture generation, we employ hybrid, fully adaptive particles in the vicinity of fracture regions via geometric subdivision. Our novel approach can facilitate the user to control the generation of cracks with low computational cost and retain high-fidelity crack details during animation. Our comprehensive experiments demonstrate the controllability, effectiveness, and accuracy of our method when simulating various brittle fracture patterns for anisotropic materials.
C1 [Li, Chen; Wang, ChangBo] E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook
RP Wang, CB (corresponding author), E China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
EM cbwangcg@gmail.com
FU Natural Science Foundation of China [61190120, 61190121, 61190125,
   61272199]; National Science Foundation of USA [IIS-0949467, IIS-1047715,
   IIS-1049448]; National High-tech R&D Program of China (863 Program)
   [SS2015AA010504]; Shanghai Municipal Education Commission [12ZZ042];
   Specialized Research Fund for Doctoral Program of Higher Education
   [20130076110008]; State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University [BUAA-VR-15KF-14]; Shanghai Collaborative
   Center of Trustworthy Software for Internet of Things [ZF1213]
FX This paper is supported in part by Natural Science Foundation of China
   (No. 61190120, 61190121, 61190125, and 61272199), National Science
   Foundation of USA (IIS-0949467, IIS-1047715, and IIS-1049448), National
   High-tech R&D Program of China (863 Program, No. SS2015AA010504),
   Innovation Program of Shanghai Municipal Education Commission (No.
   12ZZ042), the Specialized Research Fund for Doctoral Program of Higher
   Education (20130076110008), the open funding project of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   (Grant No. BUAA-VR-15KF-14), and Shanghai Collaborative Center of
   Trustworthy Software for Internet of Things (No. ZF1213). The authors
   wish to thank Dr. Feibin Chen for his technical support on mechanics
   theory. The authors also wish to thank all the anonymous reviewers for
   their insightful comments that have helped improve this paper's quality.
CR [Anonymous], 2012, ACM T GRAPHICS TOG
   Bao ZS, 2007, IEEE T VIS COMPUT GR, V13, P370, DOI 10.1109/TVCG.2007.39
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   BICKNELL GV, 1983, ASTROPHYS J, V273, P749, DOI 10.1086/161410
   Chen FB, 2013, COMPUT ANIMAT VIRT W, V24, P215, DOI 10.1002/cav.1514
   Desbenoit B, 2005, VISUAL COMPUT, V21, P717, DOI 10.1007/s00371-005-0317-z
   Estivill-Castro V., 2002, Em: SIGKDD Explor. Newsl, V4, P65, DOI [10.1145/568574.568575, DOI 10.1145/568574.568575]
   Glondu L, 2013, IEEE T VIS COMPUT GR, V19, P201, DOI 10.1109/TVCG.2012.121
   Gross D, 2011, MECH ENG SER, P1, DOI 10.1007/978-3-642-19240-1
   Guo XH, 2009, SCI CHINA SER F, V52, P401, DOI 10.1007/s11432-009-0069-x
   Hegemann J., 2013, P 12 ACM SIGGRAPH EU, P193
   Ihmsen Markus, 2014, P 35 ANN C EUR ASS C, DOI [10.2312/egst.20141034, DOI 10.2312/EGST.20141034]
   Johnson K.L., 1987, CONTACT MECH
   Koschier Dan, 2014, P ACM SIGGRAPHEUROGR, P57
   Levine J.A., 2014, P ACM SIGGRAPHEUROGR, P47
   Markus Becker, 2009, NPH, V9, P27, DOI [10.2312/EG/DL/conf/EG2009/nph/027-034, DOI 10.2312/EG/DL/CONF/EG2009/NPH/027-034]
   Martin S, 2008, COMPUT GRAPH FORUM, V27, P1521, DOI 10.1111/j.1467-8659.2008.01293.x
   Molino Neil., 2005, ACM SIGGRAPH 2005 Courses, SIGGRAPH'05, DOI [10.1145/1198555.1198574, DOI 10.1145/1198555.1198574]
   Müller M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461934
   Ning Liu, 2011, 2011 12th International Conference on Computer-Aided Design and Computer Graphics, P349, DOI 10.1109/CAD/Graphics.2011.33
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Owen JM, 1998, ASTROPHYS J SUPPL S, V116, P155, DOI 10.1086/313100
   Pauly M, 2005, ACM T GRAPHIC, V24, P957, DOI 10.1145/1073204.1073296
   Pfaff T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601132
   Shapiro PR, 1996, ASTROPHYS J SUPPL S, V103, P269, DOI 10.1086/192279
   Smith J, 2001, COMPUT GRAPH FORUM, V20, P81, DOI 10.1111/1467-8659.t01-1-00202
   Steinemann D., 2006, PROC ACM SIGGRAPHEUR, P63
   Teran J., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P68
   Terzopoulos D., 1988, Computer Graphics, V22, P269, DOI 10.1145/378456.378522
   Yu JH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421641
   Zheng CX, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778806
NR 31
TC 5
Z9 5
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 937
EP 946
DI 10.1007/s00371-015-1117-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500019
DA 2024-07-18
ER

PT J
AU Berger, B
   Vais, A
   Wolter, FE
AF Berger, Benjamin
   Vais, Alexander
   Wolter, Franz-Erich
TI Subimage sensitive eigenvalue spectra for image comparison Can one hear
   what's painted on a drum?
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT New Advances in Shape Analysis and Geometric Modeling Workshop (NASAGEM)
   at CGI Conference
CY JUN 11-14, 2013
CL Hannover, GERMANY
DE Laplace; Eigenvalue; Fingerprint; Image retrieval; Image comparison;
   Partial matching; Perturbation theory
ID SHAPE
AB This publication is a contribution to basic research in image comparison using eigenvalue spectra as features. The differential-geometric approach of eigenvalue spectrum-based descriptors is naturally applicable to shape data, but so far little work has been done to transfer it to the setting of image data painted on a rectangle or general curved surface. We present a new semi-global feature descriptor that also contains information about geometry of shapes visible in the image. This may not only improve the performance of the resulting distance measures, but may even enable us to approach the partial matching problem using eigenvalue spectra, which were previously only considered as global feature descriptors. We introduce some concepts that are useful in designing and understanding the behaviour of similar fingerprinting algorithms for images (and surfaces) and discuss some preliminary results.
C1 [Berger, Benjamin; Vais, Alexander; Wolter, Franz-Erich] Leibniz Univ Hannover, Hannover, Germany.
C3 Leibniz University Hannover
RP Berger, B (corresponding author), Leibniz Univ Hannover, Hannover, Germany.
EM bberger@welfenlab.de; vais@welfenlab.de; few@welfenlab.de
RI Berger, Benjamin/HJA-7645-2022; Wolter, Franz-Erich/AAV-3008-2020;
   Wolter, Franz-Erich/JAC-5956-2023
OI Berger, Benjamin/0000-0003-2654-2898; Wolter,
   Franz-Erich/0000-0002-2293-5494; Wolter, Franz-Erich/0000-0002-2293-5494
FU National German Academic Foundation
FX This research was partially supported by the National German Academic
   Foundation.
CR [Anonymous], 1995, PERTURBATION THEORY
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Berger M., 2003, PANORAMIC VIEW RIEMA
   Biasotti S, 2010, P ACM WORKSH 3D OBJ, P33
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein AM, 2009, INT J COMPUT VISION, V84, P163, DOI 10.1007/s11263-008-0147-3
   Courant R., 1962, Methods of mathematical physics; II: Partial differential equations, VII
   Cremers D, 2007, INT J COMPUT VISION, V72, P195, DOI 10.1007/s11263-006-8711-1
   Dirac PAM, 1939, P CAMB PHILOS SOC, V35, P416, DOI 10.1017/S0305004100021162
   GORDON C, 1992, B AM MATH SOC, V27, P134, DOI 10.1090/S0273-0979-1992-00289-6
   Grigoryan A., 2009, AMSIP STUDIES ADV MA, V47
   Hernandez V, 2005, ACM T MATH SOFTWARE, V31, P351, DOI 10.1145/1089014.1089019
   Hildebrandt K, 2012, COMPUT AIDED GEOM D, V29, P204, DOI 10.1016/j.cagd.2012.01.001
   Jinkerson R.A., 1993, Journal of Ship Production, V9, P88
   KAC M, 1966, AM MATH MON, V73, P1, DOI 10.2307/2313748
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ko K., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P196
   Ko K.H., 2003, Journal of Computing and Information Science in Engineering, V3, P325
   McKean H.P., 1967, Journal of Differential Geometry, V1, P43, DOI [10.4310/jdg/1214427880, DOI 10.4310/JDG/1214427880]
   Mémoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nixon Mark S, 2012, FEATURE EXTRACTION I, DOI DOI 10.1016/B978-0-12-396549-3.00007-0
   Peinecke N., 2007, P NASAGEM HANN 26 OC
   Peinecke N, 2007, COMPUT AIDED DESIGN, V39, P460, DOI 10.1016/j.cad.2007.01.014
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Reuter M., 2006, THESIS LEIBNITZ U HA
   Reuter M., 2005, P 2005 ACM S SOLID P, P101, DOI DOI 10.1145/1060244.1060256
   Saloff-Coste L, 2010, ADV STU P M, V57, P405
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
   Wolter FE, 2011, LECT NOTES APPL COMP, V57, P211
   Yang WK, 2011, PATTERN RECOGN, V44, P1649, DOI 10.1016/j.patcog.2011.01.019
   Zuliani M, 2008, IMAGE VISION COMPUT, V26, P347, DOI 10.1016/j.imavis.2006.12.001
NR 33
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2015
VL 31
IS 2
BP 205
EP 221
DI 10.1007/s00371-014-1038-y
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AZ6EG
UT WOS:000348310800009
DA 2024-07-18
ER

PT J
AU Toledo, L
   De Gyves, O
   Rudomín, I
AF Toledo, Leonel
   De Gyves, Oriam
   Rudomin, Isaac
TI Hierarchical level of detail for varied animated crowds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Crowd simulation; GPGPU; Animation; Hierarchical structures
ID REAL-TIME; IMPOSTORS
AB Crowd simulation is typically an expensive task. We introduce a level of detail system, useful for varied animated crowds, capable of handling several thousands of different animated characters at interactive frame rates. This is accomplished using two complementary structures to reduce memory consumption and optimize the rendering stage. The first structure is a skeleton, with associated octrees per limb, that is used for computing level of detail of geometry and animation. The second structure is a tiling of the scene that is used to select the character's level of detail for geometry, animation and behavior. A quadtree is built on top of this tiling and used for further rendering optimization, allowing us to combine geometry from different characters in parts of the scene that are far away from the camera. The system outperforms similar methods in memory requirements and/or complexity and is capable of rendering crowds composed of a quarter million characters.
C1 [Toledo, Leonel; De Gyves, Oriam] Tecnol Monterrey, Mexico City, DF, Mexico.
   [Rudomin, Isaac] Barcelona Supercomp Ctr, Barcelona, Spain.
C3 Tecnologico de Monterrey; Universitat Politecnica de Catalunya;
   Barcelona Supercomputer Center (BSC-CNS)
RP Toledo, L (corresponding author), Tecnol Monterrey, Campus Estado Mexico, Mexico City, DF, Mexico.
EM ltoledo@itesm.mx; odegyves@gmail.com; isaac.rudomin@bsc.es
OI Toledo, Leonel/0000-0002-1196-9189; Rudomin, Isaac/0000-0002-1672-1756
CR Aubel A, 2000, IEEE T CIRC SYST VID, V10, P207, DOI 10.1109/76.825720
   Beacco A, 2012, COMPUT ANIMAT VIRT W, V23, P33, DOI 10.1002/cav.1422
   Beacco A., 2010, CEIG SPAN C COMP GRA
   CLARK JH, 1976, COMMUN ACM, V19, P547, DOI 10.1145/360349.360354
   Dobbyn S., 2005, ACM SIGGRAPH 2005 S, P95, DOI DOI 10.1145/1053427.1053443
   Hardy A., 2010, 3 VIEW IMPOSTORS, DOI [10.1145/1811158.1811180, DOI 10.1145/1811158.1811180]
   Hernandez Benjamin., 2011, GPU PRO, V2, P369
   Kavan L, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P149
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Maïm J, 2009, IEEE COMPUT GRAPH, V29, P44, DOI 10.1109/MCG.2009.76
   Rudomin I., 2006, P INT C COMP AN SOC
   Ruiz S., 2013, P MOT GAM DUBL IR MI, P77, DOI [10.1145/2522628.2522901, DOI 10.1145/2522628.2522901]
   Tecchia F, 2002, COMPUT GRAPH FORUM, V21, P753, DOI 10.1111/1467-8659.00633
   Wand M., 2004, THESIS U TUBINGEN
   WAND M, 2002, COMPUTER GRAPHICS FO, V21
   Zach C., 2002, Proceedings of the ACM symposium on Virtual reality software and technology, P1, DOI DOI 10.1145/585740.585742
NR 16
TC 11
Z9 12
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 949
EP 961
DI 10.1007/s00371-014-0975-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700036
DA 2024-07-18
ER

PT J
AU Shellshear, E
AF Shellshear, Evan
TI 1D sweep-and-prune self-collision detection for deforming cables
SO VISUAL COMPUTER
LA English
DT Article
DE Bounding volume hierarchy; Sweep-and-prune; Self-collision detection;
   Cable simulation
ID SIMULATION
AB Detecting self-collision for cables and similar objects is an important part of numerous models in computational biology (protein chains), robotics (electric cables), hair modeling, computer graphics, etc. In this paper the 1D sweep-and-prune algorithm for detecting self-collisions of a deforming cable comprising linear segments is investigated. The sweep-and-prune algorithm is compared with other state-of-the-art self-collision detection algorithms for deforming cables and is shown to be up to an order of magnitude faster than existing algorithms for cables with a high proportion of segments moving. We also present a multi-threaded version of the algorithm and investigate its performance. In addition, we present worst-case bounds for 1D sweep-and-prune algorithms whereby the colliding objects do not exceed a certain object density, and apply these results to deforming cables.
C1 Fraunhofer Chalmers Res Ctr, Gothenburg, Sweden.
C3 Chalmers University of Technology; Fraunhofer-Chalmers Research Centre
   for Industrial Mathematics
RP Shellshear, E (corresponding author), Fraunhofer Chalmers Res Ctr, Gothenburg, Sweden.
EM evan.shellshear@fcc.chalmers.se
FU Swedish Governmental Agency for Innovation Systems; VINNOVA through the
   FFI Sustainable Production Technology Program; Wingquist Laboratory VINN
   Excellence Center, Sustainable Production Initiative; Production Area of
   Advance at Chalmers University of Technology
FX The author is greatly indebted to the suggestions and comments provided
   by a number of anonymous referees. This work was supported by the
   Swedish Governmental Agency for Innovation Systems, VINNOVA through the
   FFI Sustainable Production Technology Program, the Wingquist Laboratory
   VINN Excellence Center and is part of the Sustainable Production
   Initiative and the Production Area of Advance at Chalmers University of
   Technology. The author is also greatly indebted to Tomas Hermansson for
   his many valuable discussions, the help with programming, for providing
   the code for the sphere BVH and also for helping teach the author how to
   use the IPS software.
CR Agarwal P, 2004, COMP GEOM-THEOR APPL, V28, P137, DOI 10.1016/j.comgeo.2004.03.008
   Baraff David, 1992, Dynamic Simulation of Non-penetrating Rigid Bodies
   Barbic J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778818
   Bean K., 2004, P JAP C DISCR COMP G, P87
   Brown J, 2004, VISUAL COMPUT, V20, P165, DOI 10.1007/s00371-003-0226-y
   Cani M.-P., 2006, EUROGRAPHICS TUTORIA
   Craig JJ, 2009, INTRO ROBOTICS MECH
   Curtis S, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P61
   Eppstein D., 2009, Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, P23
   Ericson C., 2004, REAL TIME COLLISION
   Govindaraju NK, 2005, ACM T GRAPHIC, V24, P991, DOI 10.1145/1073204.1073301
   Halperin D, 1997, ALGORITHMS FOR ROBOTIC MOTION AND MANIPULATION, P155
   Halperin D, 1998, COMP GEOM-THEOR APPL, V11, P83, DOI 10.1016/S0925-7721(98)00023-6
   Ketchel J.S., 2005, P 2005 IDETC CIE, P1
   Kubiak B, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P85
   Lang H., 2009, MULT DYN 2009 ECCOMA, V3, P285
   Lin M., 2003, HDB DISCRETE COMPUTA
   Liu F, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866180
   Lotan I., 2003, P 3 WORKSH ALG BIOIN
   Lotan I., EFFICIENT MONTE CARL
   Lotan Itay., 2002, SCG'02: Proceedings o f the eighteenth annual symposium on Computational geometry, P43
   Madera FA, 2010, THIRD INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER-HUMAN INTERACTIONS: ACHI 2010, P107, DOI 10.1109/ACHI.2010.11
   Otaduy MA, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P181
   Raghupathi L, 2003, LECT NOTES COMPUT SC, V2673, P15
   Schmidl H., 2004, Journal of Graphics Tools, V9, P1, DOI 10.1080/10867651.2004.10504891
   Selle A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360663
   Sobottka G, 2005, CISST '05: PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY: COMPUTER GRAPHICS, P244
   Spillmann J, 2008, COMPUT GRAPH FORUM, V27, P497, DOI 10.1111/j.1467-8659.2008.01147.x
   Tang M, 2009, IEEE T VIS COMPUT GR, V15, P544, DOI 10.1109/TVCG.2009.12
   Teschner M, 2005, COMPUT GRAPH FORUM, V24, P61, DOI 10.1111/j.1467-8659.2005.00829.x
NR 30
TC 6
Z9 8
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 553
EP 564
DI 10.1007/s00371-013-0880-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100008
DA 2024-07-18
ER

PT J
AU Wang, ZF
   Miao, ZJ
   Wu, QMJ
   Wan, YL
   Tang, Z
AF Wang, Zhifei
   Miao, Zhenjiang
   Wu, Q. M. Jonathan
   Wan, Yanli
   Tang, Zhen
TI Low-resolution face recognition: a review
SO VISUAL COMPUTER
LA English
DT Review
DE Review; Face recognition; Low-resolution; Super-resolution; Feature
   extraction; Feature classification
ID HALLUCINATING FACES; IMAGE SUPERRESOLUTION; ILLUMINATION;
   RECONSTRUCTION; RESTORATION; ALGORITHMS; DATABASE; SYSTEM; LIMITS; SPACE
AB Low-resolution face recognition (LR FR) aims to recognize faces from small size or poor quality images with varying pose, illumination, expression, etc. It has received much attention with increasing demands for long distance surveillance applications, and extensive efforts have been made on LR FR research in recent years. However, many issues in LR FR are still unsolved, such as super-resolution (SR) for face recognition, resolution-robust features, unified feature spaces, and face detection at a distance, although many methods have been developed for that. This paper provides a comprehensive survey on these methods and discusses many related issues. First, it gives an overview on LR FR, including concept description, system architecture, and method categorization. Second, many representative methods are broadly reviewed and discussed. They are classified into two different categories, super-resolution for LR FR and resolution-robust feature representation for LR FR. Their strategies and advantages/disadvantages are elaborated. Some relevant issues such as databases and evaluations for LR FR are also presented. By generalizing their performances and limitations, promising trends and crucial issues for future research are finally discussed.
C1 [Wang, Zhifei; Miao, Zhenjiang; Tang, Zhen] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
   [Wang, Zhifei; Miao, Zhenjiang; Tang, Zhen] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
   [Wan, Yanli] Beijing Jiaotong Univ, Inst Syst Engn & Control, Beijing, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; University of
   Windsor; Beijing Jiaotong University
RP Wang, ZF (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
EM yichen0201@163.com
RI Wu, Q.M.Jonathan/O-3234-2017
OI Wu, Q.M. Jonathan/0000-0002-5208-7975
FU National Key Technology R&D Program of China [2012BAH01F03]; National
   Natural Science Foundation of China [60973061]; National Basic Research
   (973) Program of China [2011CB302203]; Ph.D. Programs Foundation of
   Ministry of Education of China [20100009110004]; Beijing Natural Science
   Foundation [4123104]; China Postdoctoral Science Foundation
   [2013M530020]
FX This work is supported by the National Key Technology R&D Program of
   China (2012BAH01F03), National Natural Science Foundation of China
   (60973061), National Basic Research (973) Program of China
   (2011CB302203), Ph.D. Programs Foundation of Ministry of Education of
   China (20100009110004), Beijing Natural Science Foundation (4123104),
   and China Postdoctoral Science Foundation (2013M530020). The authors
   would like to thank Professor Shengyong Chen from Zhejiang University of
   Technology and the anonymous reviewers for their comments and
   suggestions.
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   Ahonen T, 2008, ICPR2009, P1, DOI DOI 10.1109/ICPR.2008.4761847
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], P IEEE 8 INT C AUT F
   [Anonymous], HDB FACE RECOGNITION
   [Anonymous], BIOM S MAR US SEP
   [Anonymous], 2011, 2011 2 INT C WIRELES
   [Anonymous], 2004, JDLTR04FR001 ICTISVI
   [Anonymous], ORL FAC DAT
   [Anonymous], 2008, International Trade Theory, DOI DOI 10.1007/978-3-540-78265-0
   [Anonymous], 2010, CHIN C PATT REC CCPR
   [Anonymous], 2001, Cmu Ri Tr 01-18
   [Anonymous], P IEEE 6 AS C COMP V
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], THESIS CARNEGIE MELL
   [Anonymous], IET J ELECT LETT
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], YAL FAC DAT
   [Anonymous], 2006, 2006 9 ICCARC
   [Anonymous], P IAPR IEEE 5 INT C
   [Anonymous], 2008, 2008 IEEE C COMP VIS
   [Anonymous], 2008, IEEE 2 INT C BIOMETR
   [Anonymous], 2010, P CCPR
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], P IAPR IEEE 18 INT C
   Ao M, 2009, ADV PATTERN RECOGNIT, P155, DOI 10.1007/978-1-84882-385-3_6
   Arandjelovic O, 2005, PROC CVPR IEEE, P581
   Arandjelovic O., 2007, Proceedings of ICCP, P1, DOI DOI 10.1109/SIU.2007.4298708
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bilgazyev E, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.52
   Biswas S, 2012, IEEE T PATTERN ANAL, V34, P2019, DOI 10.1109/TPAMI.2011.278
   Biswas S, 2011, PROC CVPR IEEE, P601, DOI 10.1109/CVPR.2011.5995443
   Blackburn DuaneM., 2001, FACE RECOGNITION VEN
   Capel D, 2001, PROC CVPR IEEE, P627
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chang JM, 2007, LECT NOTES COMPUT SC, V4844, P733
   Choi H.-C., 2010, PROC IEEE 4 INT C BI, P1
   Choi JY, 2011, IEEE T IMAGE PROCESS, V20, P1425, DOI 10.1109/TIP.2010.2093906
   Choi JY, 2010, IEEE IMAGE PROC, P4541, DOI 10.1109/ICIP.2010.5653653
   Choi JY, 2011, PATTERN RECOGN, V44, P412, DOI 10.1016/j.patcog.2010.08.020
   Choi JY, 2009, IEEE T SYST MAN CY B, V39, P1217, DOI 10.1109/TSMCB.2009.2014245
   Dedeoglu G, 2004, PROC CVPR IEEE, P151
   Ebrahimpour R., 2010, 2010 International Conference of Soft Computing and Pattern Recognition (SoCPaR 2010), P265, DOI 10.1109/SOCPAR.2010.5686495
   Elad M, 1997, IEEE T IMAGE PROCESS, V6, P1646, DOI 10.1109/83.650118
   Ersotelos N, 2008, VISUAL COMPUT, V24, P13, DOI 10.1007/s00371-007-0175-y
   Fookes C, 2012, J VIS COMMUN IMAGE R, V23, P75, DOI 10.1016/j.jvcir.2011.06.004
   France SL, 2011, IEEE T SYST MAN CY C, V41, P644, DOI 10.1109/TSMCC.2010.2078502
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Hadid A, 2004, PROC CVPR IEEE, P797
   Hadid A, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P813, DOI 10.1109/AFGR.2004.1301634
   Han H, 2010, IEEE IMAGE PROC, P2825, DOI 10.1109/ICIP.2010.5651505
   Hardie RC, 1997, IEEE T IMAGE PROCESS, V6, P1621, DOI 10.1109/83.650116
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hennings-Yeomans PH, 2009, IEEE IMAGE PROC, P33, DOI 10.1109/ICIP.2009.5413920
   Hu Y, 2011, IEEE T IMAGE PROCESS, V20, P433, DOI 10.1109/TIP.2010.2063437
   Huang G.B., 2008, PROC WORKSHOP FACES
   Huang H, 2011, IEEE T NEURAL NETWOR, V22, P121, DOI 10.1109/TNN.2010.2089470
   Hwang BW, 2003, LECT NOTES COMPUT SC, V2688, P557
   Jain A. K., 2011, HDB FACE RECOGNITION
   Jia K, 2005, IEEE I CONF COMP VIS, P1683
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Kumar BVKV, 2006, P IEEE, V94, P1963, DOI 10.1109/JPROC.2006.884094
   KURITA T, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P213, DOI 10.1109/ICPR.1992.201757
   Lee KC, 2003, PROC CVPR IEEE, P313
   Lee SW, 2006, PATTERN RECOGN, V39, P1809, DOI 10.1016/j.patcog.2006.04.033
   Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860
   Lemieux A, 2002, INT C PATT RECOG, P421, DOI 10.1109/ICPR.2002.1044743
   Li Baihuan., 2008, International Communication Association Conference, P1
   Li B, 2010, IEEE SIGNAL PROC LET, V17, P20, DOI 10.1109/LSP.2009.2031705
   Li B, 2009, LECT NOTES ARTIF INT, V5828, P220
   Li SZ, 2009, ADV PATTERN RECOGNIT, P3, DOI 10.1007/978-1-84882-385-3_1
   Lin DH, 2006, LECT NOTES COMPUT SC, V3954, P13
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu W, 2005, PROC CVPR IEEE, P478
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Marchesotti L, 2003, IEEE IMAGE PROC, P681
   Martinez A., 1998, AR FACE DATABASE
   Medioni G, 2009, IEEE T SYST MAN CY A, V39, P12, DOI 10.1109/TSMCA.2008.2007979
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X
   Nasrollahi K, 2011, IEEE T CIRC SYST VID, V21, P1353, DOI 10.1109/TCSVT.2011.2162267
   Ni J, 2010, IEEE IMAGE PROC, P1581, DOI 10.1109/ICIP.2010.5652608
   Park JS, 2008, IEEE T IMAGE PROCESS, V17, P1806, DOI 10.1109/TIP.2008.2001394
   Peng Y, 2010, NEURAL PROCESS LETT, V31, P1, DOI 10.1007/s11063-009-9123-3
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Pinto Nicolas, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2591, DOI 10.1109/CVPRW.2009.5206605
   Pnevmatikakis A., 2007, Face Recognition, P467
   Prince S. J. D., 2006, Institution of Engineering and Technology Conference on Crime and Security, P570
   Rara Ham M., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1221, DOI 10.1109/ICPR.2010.304
   Razzak MI, 2011, INT J INNOV COMPUT I, V7, P4679
   Ren CX, 2012, IEEE T IMAGE PROCESS, V21, P3770, DOI 10.1109/TIP.2012.2192285
   Ruiz-Del-Solar J, 2005, IEEE T SYST MAN CY C, V35, P315, DOI 10.1109/TSMCC.2005.848201
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   Seokwon Yeom, 2008, 2008 Second International Conference on Future Generation Communication and Networking Symposia (FGCNS), P230, DOI 10.1109/FGCNS.2008.59
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Shekhar S., 2011, P INT JOINT C BIOMET P 2011 INT JOINT C B, P1
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Tan XY, 2006, PATTERN RECOGN, V39, P1725, DOI 10.1016/j.patcog.2006.03.013
   Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2
   Tome P., 2010, CVPR Workshops, P67
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wang ZF, 2008, INT C PATT RECOG, P3735
   Wheeler F. W., 2007, PROC IEEE 1 INT C BI, P1
   Wheeler Frederick W., 2010, 2010 4 IEEE INT C BI, P1
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   WURM LH, 1993, J EXP PSYCHOL HUMAN, V19, P899, DOI 10.1037/0096-1523.19.4.899
   Yang A. Y., 2007, Tech. Rep. UCB/EECS-2007-99
   Yang J, 2010, PATTERN RECOGN, V43, P1454, DOI 10.1016/j.patcog.2009.11.014
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Li, 2004, Proceedings. Third International Conference on Image and Graphics, P298
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Yao Y, 2008, COMPUT VIS IMAGE UND, V111, P111, DOI 10.1016/j.cviu.2007.09.004
   Yongkang Wong, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1200, DOI 10.1109/ICPR.2010.299
   Yu J, 2006, INT C PATT RECOG, P342
   Zhang JP, 2010, IEEE T SYST MAN CY B, V40, P986, DOI 10.1109/TSMCB.2010.2042166
   Zhang X, 2009, PATTERN RECOGN, V42, P2876, DOI 10.1016/j.patcog.2009.04.017
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhao Wenyi., 2005, Face Processing: Advanced Modeling and Methods: Advanced Modeling and Methods
   Zhen Lei, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P161, DOI 10.1109/FG.2011.5771391
   Zheng J, 2010, LECT NOTES COMPUT SC, V6111, P454, DOI 10.1007/978-3-642-13772-3_46
   Zhon XL, 2007, IEEE T SYST MAN CY B, V37, P1119, DOI 10.1109/TSMCB.2006.889612
   Zhuang LS, 2009, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS (ICIG 2009), P200, DOI 10.1109/ICIG.2009.154
   Zhuang YT, 2007, PATTERN RECOGN, V40, P3178, DOI 10.1016/j.patcog.2007.03.011
   Zou W. W. W., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1152, DOI 10.1109/ICPR.2010.288
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
   Zou X., 2007, PROC IEEE 1 INT C BI, P1
NR 143
TC 83
Z9 93
U1 0
U2 115
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 359
EP 386
DI 10.1007/s00371-013-0861-x
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500002
DA 2024-07-18
ER

PT J
AU Vishwakarma, S
   Agrawal, A
AF Vishwakarma, Sarvesh
   Agrawal, Anupam
TI A survey on activity recognition and behavior understanding in video
   surveillance
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Automated surveillance; Video tracking; Human activity
ID COMPUTER VISION SYSTEM; OBJECT CLASSIFICATION; MOTION DETECTION; VISUAL
   TRACKING; VIEW-INVARIANCE; REGION; MODELS; SPACE; IMAGE; REPRESENTATION
AB This paper provides a comprehensive survey for activity recognition in video surveillance. It starts with a description of simple and complex human activity, and various applications. The applications of activity recognition are manifold, ranging from visual surveillance through content based retrieval to human computer interaction. The organization of this paper covers all aspects of the general framework of human activity recognition. Then it summarizes and categorizes recent-published research progresses under a general framework. Finally, this paper also provides an overview of benchmark databases for activity recognition, the market analysis of video surveillance, and future directions to work on for this application.
C1 [Vishwakarma, Sarvesh; Agrawal, Anupam] Indian Inst Informat Technol, Allahabad, Uttar Pradesh, India.
C3 Indian Institute of Information Technology Allahabad
RP Vishwakarma, S (corresponding author), BHEL Corp, N-297 3rd Phase, Haridwar, India.
EM rs51@iiita.ac.in; anupam@iiita.ac.in
OI Vishwakarma, Dr. Sarvesh/0000-0001-6923-2764; Agrawal,
   Anupam/0000-0003-3392-5045
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
   Allili MS, 2007, FOURTH CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P503, DOI 10.1109/CRV.2007.7
   [Anonymous], THESIS CALTECH
   [Anonymous], 2010, Proceedings of the seventh international symposium on visualization for cyber security
   [Anonymous], P EUR C COMP VIS
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], ANN IEEE IND C INDIC
   [Anonymous], 2007, 2007 IEEE C COMP VIS
   [Anonymous], P BRIT MACH VIS C BM
   [Anonymous], 2011, 2011 IEEE WORKSHOP A
   [Anonymous], C TECHN HOM SEC WALT
   [Anonymous], 2000, SYSTEM VIDEO SURVEIL
   [Anonymous], UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)
   [Anonymous], P SPIE
   [Anonymous], P 4 ANN ACM BANG C C
   [Anonymous], INT C P SER 05
   [Anonymous], P ICPR CONT SEM DESC
   [Anonymous], P 1 ACM INT C MULT I
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], P IEEE INT C SIGN PR
   [Anonymous], SEMISUPERVISED HUMAN
   [Anonymous], 2006, P IEEE COMP SOC C CO
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE 17 INT C IM PRO
   [Anonymous], P DIG IM COMP TECH A
   [Anonymous], 10 M IM REC UND MIRU
   [Anonymous], 2007, IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2009, P BRIT MACH VIS C BM
   [Anonymous], IEEE WORKSH MOT VID
   [Anonymous], P 17 WORLD C INT FED
   [Anonymous], IEEE IM PROC P
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2007, P IEEE 15 SIGN PROC
   [Anonymous], 2008, CVPR
   [Anonymous], 9 IAPR INT C MACH VI
   [Anonymous], 2008, CVPR
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], DARPA81
   [Anonymous], 11 IEEE INT WORKSH P
   Blunsom P., 2004, Hidden Markov models
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bobick AF, 1997, IEEE T PATTERN ANAL, V19, P1325, DOI 10.1109/34.643892
   Bose B, 2004, PROC CVPR IEEE, P181
   Brown L.M., 2004, Proceedings of the ACM 2nd International Workshop on Video Surveillance Sensor Networks, P114
   Cai L, 2011, IEEE T CIRC SYST VID, V21, P1784, DOI 10.1109/TCSVT.2011.2133550
   CAMPBELL LW, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P624, DOI 10.1109/ICCV.1995.466880
   Camplani M, 2011, OPT ENG, V50, DOI 10.1117/1.3662422
   Cavallaro A, 2005, IEEE T CIRC SYST VID, V15, P575, DOI 10.1109/TCSVT.2005.844447
   Chai Y, 2010, IEEE T CONSUM ELECTR, V56, P510, DOI 10.1109/TCE.2010.5505963
   Chang SF, 2002, IEEE MULTIMEDIA, V9, P6, DOI 10.1109/93.998041
   Chen Q, 2010, IEEE T CIRC SYST VID, V20, P605, DOI 10.1109/TCSVT.2010.2041819
   Cheng FH, 2006, PATTERN RECOGN, V39, P1126, DOI 10.1016/j.patcog.2005.12.010
   Cheung KM, 2005, INT J COMPUT VISION, V63, P225, DOI 10.1007/s11263-005-6879-4
   Chiverton J, 2012, IEEE T IMAGE PROCESS, V21, P1231, DOI 10.1109/TIP.2011.2167343
   Cohen W. W., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P115
   Coifman B, 1998, TRANSPORT RES C-EMER, V6, P271, DOI 10.1016/S0968-090X(98)00019-9
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Cupillard F, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P177, DOI 10.1109/ACV.2002.1182178
   Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681
   Dai P, 2008, IEEE T SYST MAN CY B, V38, P275, DOI 10.1109/TSMCB.2007.909939
   Damen D, 2009, PROC CVPR IEEE, P927, DOI 10.1109/CVPRW.2009.5206636
   Darrell T., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P335, DOI 10.1109/CVPR.1993.341109
   Denman S, 2007, PATTERN RECOGN LETT, V28, P1232, DOI 10.1016/j.patrec.2007.02.008
   Denman S, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P175, DOI 10.1109/DICTA.2009.35
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Fazli Saeid, 2009, 2009 6th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), P1130, DOI 10.1109/ECTICON.2009.5137243
   Fergus R, 2003, PROC CVPR IEEE, P264
   Filipovych R, 2007, LECT NOTES COMPUT SC, V4842, P21
   Forsyth DA, 2005, FOUND TRENDS COMPUT, V1, P77, DOI 10.1561/0600000005
   Gallagher M, 2003, IEEE T SYST MAN CY B, V33, P28, DOI 10.1109/TSMCB.2003.808183
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   Ghanem N., 2004, Computer Vision and Pattern Recognition Workshop, P112, DOI DOI 10.1109/CVPR.2004.430
   Gilbert A, 2009, IEEE I CONF COMP VIS, P925, DOI 10.1109/ICCV.2009.5459335
   Gong SG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P742, DOI 10.1109/ICCV.2003.1238423
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Gupta A., 2007, IEEE C COMPUTER VISI, P1
   Gupta A, 2009, PROC CVPR IEEE, P2012, DOI 10.1109/CVPRW.2009.5206492
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Heisele B, 2003, COMPUT VIS IMAGE UND, V91, P6, DOI 10.1016/S1077-3142(03)00073-0
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Hu WM, 2004, IEEE T SYST MAN CY B, V34, P1618, DOI 10.1109/TSMCB.2004.826829
   Huei-Hung Liao, 2008, 2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance, P132, DOI 10.1109/AVSS.2008.9
   Ince S, 2008, IEEE T IMAGE PROCESS, V17, P1443, DOI 10.1109/TIP.2008.925381
   Intille SS, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P518
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   Jan T, 2004, IEEE IJCNN, P1309
   Jang DS, 2000, PATTERN RECOGN, V33, P1135, DOI 10.1016/S0031-3203(99)00100-4
   Javed O, 2002, LECT NOTES COMPUT SC, V2353, P343
   Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022
   Jing Huang, 2008, 5th International Conference on Visual Information Engineering, VIE 2008, P437, DOI 10.1049/cp:20080353
   Joo S.W., 2006, Computer Vision and Pattern Recognition Workshop, P107, DOI [DOI 10.1109/CVPRW.2006.32, 10.1109/CVPRW.2006.32]
   Kameda Y., 1996, INT C VIRTUAL SYSTEM, P135
   Kang WX, 2007, 6TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, PROCEEDINGS, P824, DOI 10.1109/ICIS.2007.157
   Ke Y., 2007, IEEE C COMPUTER VISI, P1
   Khan S. M., 2005, 13th Annual ACM International Conference on Multimedia, P403, DOI 10.1145/1101149.1101237
   Kim JB, 2003, PATTERN RECOGN LETT, V24, P113, DOI 10.1016/S0167-8655(02)00194-0
   Kim TK, 2009, ELECTRON LETT, V45, P542, DOI 10.1049/el.2009.0663
   Ko T, 2008, IEEE APP IMG PAT, P84
   Kuno Y., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P865, DOI 10.1109/ICPR.1996.547291
   Ladikos Alexander, 2007, VISAPP
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Laptev I., 2007, PROC 11 IEEE INT C C, P1
   Lei Chen, 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2447, DOI 10.1109/ROBIO.2011.6181672
   Leordeanu M, 2005, PROC CVPR IEEE, P1142
   Lin HH, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P893, DOI 10.1109/ICIP.2002.1039116
   Lipton A.J., 1999, LOCAL APPL OPTIC FLO
   Lipton AJ, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P8, DOI 10.1109/ACV.1998.732851
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Jingen Liu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1996, DOI [10.1109/ICINIS.2009.13, 10.1109/CVPRW.2009.5206744]
   Lublinerman R, 2006, INT C PATT RECOG, P347
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Luo RJ, 2007, LECT NOTES COMPUT SC, V4810, P118
   Ma XX, 2005, IEEE I CONF COMP VIS, P1185
   McHugh JM, 2009, IEEE SIGNAL PROC LET, V16, P390, DOI 10.1109/LSP.2009.2016447
   MEYER FG, 1994, CVGIP-IMAG UNDERSTAN, V60, P119, DOI 10.1006/ciun.1994.1042
   Migdal J., 2005, Application of Computer Vision, V1, P58
   Minnen D, 2003, PROC CVPR IEEE, P626
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   Moore D, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P770
   Moore D. J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P80, DOI 10.1109/ICCV.1999.791201
   Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109
   Nevatia R., 2004, Computer Vision and Pattern Recognition Workshop, P119
   Nevatia R., 2003, P C COMP VIS PATT RE, V4, P39
   Nguyen NT, 2005, PROC CVPR IEEE, P955
   Niethammer M, 2006, IEEE T AUTOMAT CONTR, V51, P562, DOI 10.1109/TAC.2006.872837
   Nowozin S, 2007, IEEE I CONF COMP VIS, P1727
   Ogale A. S., 2005, Dynamical Vision. ICCV 2005 and ECCV 2006 Workshops WDV 2005 and WDV 2006. Revised Papers (Lecture Notes in Computer Science Vol. 4358), P115
   Oh S., 2011, P CVPR, P3153, DOI DOI 10.1109/CVPR.2011.5995586
   Oikonomopoulos A, 2007, LECT NOTES COMPUT SC, V4451, P133
   Oikonomopoulos A, 2006, IEEE T SYST MAN CY B, V36, P710, DOI 10.1109/TSMCB.2005.861864
   Oliver N, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P3, DOI 10.1109/ICMI.2002.1166960
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Ong EJ, 2002, IMAGE VISION COMPUT, V20, P397, DOI 10.1016/S0262-8856(02)00011-2
   Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Parameswaran V, 2010, PROC CVPR IEEE, P1982, DOI 10.1109/CVPR.2010.5539873
   Park S, 2004, MULTIMEDIA SYST, V10, P164, DOI 10.1007/s00530-004-0148-1
   Paruchuri J. K., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1745, DOI 10.1109/ICCVW.2011.6130460
   Pentland A, 1998, INT C PATT RECOG, P949, DOI 10.1109/ICPR.1998.711845
   Peursum P, 2005, IEEE I CONF COMP VIS, P82
   Pinhanez CS, 1998, PROC CVPR IEEE, P898, DOI 10.1109/CVPR.1998.698711
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Porikli F, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/197875
   Quinlan J.R.:., 1999, C4.5: Programs for Machine Learning
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Rao C, 2001, PROC CVPR IEEE, P316
   Reddy Vikas, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P172, DOI 10.1109/AVSS.2010.84
   Ren Y, 2008, IMAGE VISION COMPUT, V26, P1530, DOI 10.1016/j.imavis.2008.04.023
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Ryoo M., 2006, 2006 IEEE COMP SOC C, V2, P1709, DOI DOI 10.1109/CVPR.2006.242
   Ryoo MS, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P95
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Ryoo MS, 2009, INT J COMPUT VISION, V82, P1, DOI 10.1007/s11263-008-0181-1
   Ryoo M.S., 2007, IEEE C COMPUTER VISI, P1
   Sakaino H, 2012, IEEE T IMAGE PROCESS, V21, P441, DOI 10.1109/TIP.2011.2165220
   Salembier P, 1999, IEEE T CIRC SYST VID, V9, P1147, DOI 10.1109/76.809153
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Schmaltz C, 2012, MACH VISION APPL, V23, P557, DOI 10.1007/s00138-010-0317-5
   Schmaltz C, 2009, LECT NOTES COMPUT SC, V5748, P21, DOI 10.1007/978-3-642-03798-6_3
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   SCHUNCK BG, 1986, COMPUT VISION GRAPH, V35, P20, DOI 10.1016/0734-189X(86)90124-6
   Sclaroff S, 2003, COMPUT VIS IMAGE UND, V89, P197, DOI 10.1016/S1077-3142(03)00003-1
   Shechtman E, 2005, PROC CVPR IEEE, P405
   Sheikh Y, 2005, IEEE I CONF COMP VIS, P144
   Sheikh Y, 2009, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2009.5459334
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shi YF, 2004, PROC CVPR IEEE, P862
   Siskind JM, 2001, J ARTIF INTELL RES, V15, P31, DOI 10.1613/jair.790
   Starner T., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P265, DOI 10.1109/ISCV.1995.477012
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P333, DOI 10.1109/CVPR.1999.784654
   Stenger B, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P294, DOI 10.1109/ICCV.2001.937532
   Tavakkoli A, 2006, LECT NOTES COMPUT SC, V4291, P40
   Tsai DM, 2009, IEEE T IMAGE PROCESS, V18, P158, DOI 10.1109/TIP.2008.2007558
   Tsuchiya M, 2006, INT C PATT RECOG, P978
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   Varcheie PDZ, 2010, SENSORS-BASEL, V10, P1041, DOI 10.3390/s100201041
   Vaswani N, 2005, IEEE T IMAGE PROCESS, V14, P1603, DOI 10.1109/TIP.2005.852197
   Vaswani N, 2003, PROC CVPR IEEE, P633
   Veeraraghavan A., 2006, CVPR 06, P959, DOI [DOI 10.1109/CVPR.2006.304, 10.1109/CVPR.2006.304]
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Vosters L. P. J., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P384, DOI 10.1109/AVSS.2010.72
   Vu V.T., 2003, Proc. of Int'l Joint Conf. o Artificial Intelligence, P9
   Walton D, 2010, PSYCHOL EMOT MOTIV A, P1
   Wang J.X., 2006, OTCBVS, P137
   Weinland D., 2006, COMPUTER VISION PATT, V2, P1639
   Weinland D, 2007, IEEE I CONF COMP VIS, P170
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wen ZQ, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 2, PROCEEDINGS, P170
   Wong K. O., 2007, ICCV, P1, DOI DOI 10.1109/VTSA.2007.378923
   Wong S.-F., 2007, IEEE COMPUTER SOC C, P1
   Wunsch P, 1997, IEEE INT CONF ROBOT, P2868, DOI 10.1109/ROBOT.1997.606722
   Xiang T, 2008, IEEE T PATTERN ANAL, V30, P893, DOI 10.1109/TPAMI.2007.70731
   Xu M, 2011, IEEE ENG MED BIO, P1794, DOI 10.1109/IEMBS.2011.6090511
   Yacoob Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P120, DOI 10.1109/ICCV.1998.710709
   Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161
   Yamazaki M, 2006, LECT NOTES COMPUT SC, V3852, P467
   Yan Chen, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P95, DOI 10.1109/IBICA.2011.28
   Yang FL, 2012, VISUAL COMPUT, V28, P175, DOI 10.1007/s00371-011-0616-5
   Yilmaz A, 2005, PROC CVPR IEEE, P984
   Yilmaz A, 2004, IEEE T PATTERN ANAL, V26, P1531, DOI 10.1109/TPAMI.2004.96
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yohannes Yisehac., 1999, CLASSIFICATION REGRE
   Yokoyama M., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P271
   Yu E, 2006, INT C PATT RECOG, P375
   Yunguang Qi, 2011, Proceedings of 2011 International Conference on Computer Science and Network Technology, P2452
   Zelnik-Manor L, 2001, PROC CVPR IEEE, P123
   Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
   Zhang JX, 2009, 2009 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL I, P333, DOI 10.1109/CCCM.2009.5268114
   Zhang L., 2007, Corporate social responsibility, applicants' ethical predispositions and organizational attraction: A person-organization fit perspective, P1
   Zhao X, 2008, INT C PATT RECOG, P2006
   Zhong H, 2004, PROC CVPR IEEE, P819
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
   Zhu YD, 2010, COMPUT VIS IMAGE UND, V114, P1362, DOI 10.1016/j.cviu.2009.11.005
NR 228
TC 270
Z9 301
U1 2
U2 78
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 983
EP 1009
DI 10.1007/s00371-012-0752-6
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400002
DA 2024-07-18
ER

PT J
AU Huang, RG
   Keyser, J
AF Huang, Ruoguan
   Keyser, John
TI Automated sampling and control of gaseous simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Eulerian simulation; Simulation control; Poisson disk sampling;
   Bilateral sampling
ID ANIMATION
AB In this work, we describe a method that automates the sampling and control of gaseous fluid simulations. Several recent approaches have provided techniques for artists to generate high-resolution simulations based on a low-resolution simulation. However, often in applications the overall flow in the low-resolution simulation that an animator observes and intends to preserve is composed of even lower frequencies than the low resolution itself. In such cases, attempting to match the low-resolution simulation precisely is unnecessarily restrictive. We propose a new sampling technique to efficiently capture the overall flow of a fluid simulation, at the scale of user's choice, in such a way that the sampled information is sufficient to represent what is virtually perceived and no more. Thus, by applying control based on the sampled data, we ensure that in the resulting high-resolution simulation, the overall flow is matched to the low-resolution simulation and the fine details on the high resolution are preserved. The samples we obtain have both spatial and temporal continuity that allows smooth keyframe matching and direct manipulation of visible elements such as smoke density through temporal blending of samples. We demonstrate that a user can easily configure a simulation with our system to achieve desired results.
C1 [Huang, Ruoguan] Texas A&M Univ, College Stn, TX 77843 USA.
   [Keyser, John] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX USA.
C3 Texas A&M University System; Texas A&M University College Station; Texas
   A&M University System; Texas A&M University College Station
RP Huang, RG (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.
EM hrg@cs.tamu.edu; keyser@cs.tamu.edu
OI Keyser, John/0000-0002-4829-9975
FU NSF [IIS-0917286]; King Abdullah University of Science and Technology
   (KAUST) [KUS-CI-016-04]; Direct For Computer & Info Scie & Enginr; Div
   Of Information & Intelligent Systems [0917286] Funding Source: National
   Science Foundation
FX This publication is based in part on work supported by NSF Grant
   IIS-0917286 and by Award Number KUS-CI-016-04, made by King Abdullah
   University of Science and Technology (KAUST).
CR Angelidis Alexis., 2006, S COMPUTER ANIMATION, P25
   [Anonymous], ACM T GRAPH
   [Anonymous], 2004, COMPUTER ANIMATION 2, DOI DOI 10.1145/1028523.1028549
   Fattal R, 2004, ACM T GRAPHIC, V23, P441, DOI 10.1145/1015706.1015743
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P178, DOI 10.1109/CGI.1997.601299
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   He S, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/j.1467-8659.2012.03228.x
   Hong JM, 2004, COMPUT ANIMAT VIRT W, V15, P147, DOI 10.1002/cav.17
   Huang Ruoguan., 2011, Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'11, P177, DOI DOI 10.1145/2019406.2019430
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   Kim Y., 2006, P 2006 ACM SIGGRAPHE, P33
   Lagae A, 2008, COMPUT GRAPH FORUM, V27, P114, DOI 10.1111/j.1467-8659.2007.01100.x
   McCool M., 1992, Proceedings. Graphics Interface '92, P94
   McNamara A, 2004, ACM T GRAPHIC, V23, P449, DOI 10.1145/1015706.1015744
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Nielsen MB, 2010, COMPUT GRAPH FORUM, V29, P705, DOI 10.1111/j.1467-8659.2009.01640.x
   Nielsen MichaelB., 2009, SCA '09: Proc. of the 2009 ACM SIGGRAPH/Eurographics Symp. on Comput. Anim, P217
   Pfaff T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866196
   Pighin Frederic., 2004, SCA'04: Proceedings ofthe 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P223, DOI 10.1145/1028523.1028552
   Schechter H., 2008, Symposium on Computer animation, P1
   Schpok J., 2005, P 2005 ACM SIGGRAPHE, P97, DOI [10.1145/1073368.1073381, DOI 10.1145/1073368.1073381]
   Shi L, 2005, ACM T GRAPHIC, V24, P140, DOI 10.1145/1037957.1037965
   Shi Lin., 2005, Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '05, P229, DOI DOI 10.1145/1073368.1073401
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Steinhoff J, 2003, J SCI COMPUT, V19, P457, DOI 10.1023/A:1025376630288
   Thurey N., 2006, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'06, P7, DOI [10.5555/1218064.1218066, DOI 10.5555/1218064.1218066]
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Treuille A, 2003, ACM T GRAPHIC, V22, P716, DOI 10.1145/882262.882337
   Yuan Z., 2011, P 2011 SIGGRAPH AS C, P136
NR 32
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 751
EP 760
DI 10.1007/s00371-013-0798-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400028
DA 2024-07-18
ER

PT J
AU Chen, CH
   Tsai, MH
   Lin, IC
   Lu, PH
AF Chen, Cheng-Hao
   Tsai, Ming-Han
   Lin, I-Chen
   Lu, Pin-Hua
TI Skeleton-driven surface deformation through lattices for real-time
   character animation
SO VISUAL COMPUTER
LA English
DT Article
DE Lattice-based shape; Character skinning; Secondary deformation;
   Skeleton-driven animation
AB In this paper, an efficient deformation framework is presented for skeleton-driven polygonal characters. Standard solutions, such as linear blend skinning, focus on primary deformations and require intensive user adjustment. We propose constructing a lattice of cubic cells embracing the input surface mesh. Based on the lattice, our system automatically propagates smooth skinning weights from bones to drive the surface primary deformation, and it rectifies the over-compressed regions by volume preservation. The secondary deformation is, in the meanwhile, generated by the lattice shape matching with dynamic particles. The proposed framework can generate both low- and high-frequency surface motions such as muscle deformation and vibrations with few user interventions. Our results demonstrate that the proposed lattice-based method is liable to GPU computation, and it is adequate to real-time character animation.
C1 [Chen, Cheng-Hao; Tsai, Ming-Han; Lin, I-Chen; Lu, Pin-Hua] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Lin, IC (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, 1001 Ta Hsueh Rd, Hsinchu 30010, Taiwan.
EM ichenlin@cs.nctu.edu.tw
OI Lin, I-Chen/0000-0001-9924-4723
CR Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Botsch M, 2007, COMPUT GRAPH FORUM, V26, P339, DOI 10.1111/j.1467-8659.2007.01056.x
   Capell S, 2002, ACM T GRAPHIC, V21, P586, DOI 10.1145/566570.566622
   Cordier F, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P257, DOI 10.1109/PCCGA.2004.1348356
   Faloutsos P, 1997, IEEE T VIS COMPUT GR, V3, P201, DOI 10.1109/2945.620488
   Forstmann S, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P141
   Hyun DE, 2005, VISUAL COMPUT, V21, P542, DOI 10.1007/s00371-005-0343-x
   James D.L., 2004, P ACM SIGGRAPH 2004
   Joshi P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239522
   Ju T, 2005, ACM T GRAPHIC, V24, P561, DOI 10.1145/1073204.1073229
   Ju T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409075
   Kavan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409627
   Kim BU, 2010, VISUAL COMPUT, V26, P487, DOI 10.1007/s00371-010-0474-6
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lee J, 2009, COMPUT ANIMAT VIRT W, V20, P321, DOI 10.1002/cav.308
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li JT, 2011, VISUAL COMPUT, V27, P585, DOI 10.1007/s00371-011-0585-8
   Lin IC, 2011, IEEE T VIS COMPUT GR, V17, P527, DOI 10.1109/TVCG.2010.87
   Magnenat-Thalmann, 1988, Proceedings of Graphics Interface '88, P26
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   Müller M, 2004, PROC GRAPH INTERF, P239
   Muller M., 2005, P ACM SIGGRAPH S COM, P49
   O'Brien JF, 2000, IEEE COMPUT GRAPH, V20, P86, DOI 10.1109/38.851756
   Peng JY, 2007, COMPUT ANIMAT VIRT W, V18, P549, DOI 10.1002/cav.208
   Rivers AR, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239533
   Shi XH, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360628
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Takamatsu K., 2009, P ACM SIGGRAPH ASIA
   von Funck W., 2007, P 5 EUR S GEOM PROC, P99
   Wang RY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239524, 10.1145/1276377.1276468]
   Wang YS, 2008, IEEE T VIS COMPUT GR, V14, P926, DOI 10.1109/TVCG.2008.38
   Zhou K, 2005, ACM T GRAPHIC, V24, P496, DOI 10.1145/1073204.1073219
NR 32
TC 5
Z9 7
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2013
VL 29
IS 4
SI SI
BP 241
EP 251
DI 10.1007/s00371-012-0759-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 115AM
UT WOS:000316784200002
DA 2024-07-18
ER

PT J
AU Bergervoet, EJ
   van der Sluis, F
   van Dijk, EMAG
   Nijholt, A
AF Bergervoet, E. J.
   van der Sluis, F.
   van Dijk, E. M. A. G.
   Nijholt, A.
TI Bombs, fish, and coral reefs The role of in-game explanations and
   explorative game behavior on comprehension
SO VISUAL COMPUTER
LA English
DT Article
DE Educational games; Endogenous games; Exogenous games; Game design; Game
   experience
AB Often, the way subject matter is included in educational games does not fully utilize or sometimes even inhibits the full learning potential of games. This paper argues that in order to optimally use the potential of games for learning, games should be endogenous. An endogenous educational game is a game where the educational content is integrated in the game play mechanics themselves, rather than bolted-on using explicit messages. This research examines the relation between explicit messages, explorative game behavior, and comprehension by developing two versions of an endogenous educational game about overfishing, one with and one without an explicit purpose. The game was tested with 13 children aged 8 to 11. The results indicate that factual knowledge and comprehension is increased with explicit messages, and in particular deep comprehension is fostered by explorative game behavior. This confirms the plea for endogenous games to teach about bombs, fish, coral reefs, and more.
C1 [Bergervoet, E. J.; van der Sluis, F.; van Dijk, E. M. A. G.; Nijholt, A.] Univ Twente, NL-7500 AE Enschede, Netherlands.
C3 University of Twente
RP Nijholt, A (corresponding author), Univ Twente, POB 217, NL-7500 AE Enschede, Netherlands.
EM hmi_secr@ewi.utwente.nl
OI Nijholt, Anton/0000-0002-5669-9290
FU European Union [FP7-ICT-2007-3]
FX This work was part of the PuppyIR project, which is supported by a grant
   of the 7th Framework ICT Programme (FP7-ICT-2007-3) of the European
   Union.
CR Bellotti F., 2009, ACM COMPUTERS ENTERT, V7, P23, DOI DOI 10.1145/1541895.1541903
   Bloom B., 1956, HDB 1 COGNITIVE DOMA, P1956
   Conati C., 2004, P IUI 04 INT C INTEL, P6, DOI DOI 10.1145/964442.964446
   Egenfeldt-Nielson S., 2007, UNDERSTANDING VIDEO
   Gee JP, 2003, WHAT VIDEO GAMES HAVE TO TEACH US ABOUT LEARNING AND LITERACY, P1
   Halverson R., 2005, INNOVATE, V1, P6
   Hostetter O., 2003, Video games: The necessity of incorporating video games as part of constructivist leaning
   Isbister K, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2041
   Kolb D.A., 2000, EXPERIENTIAL LEARNIN
   Leutner D., 1993, LEARN INSTR, V3, P113, DOI DOI 10.1016/0959-4752(93)90011-N
   Pauly D., 1983, FOOD AGR ORG
   Zagal Jose., 2010, LUDOLITERACY DEFININ
NR 12
TC 2
Z9 2
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2013
VL 29
IS 2
SI SI
BP 99
EP 110
DI 10.1007/s00371-012-0720-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 072FM
UT WOS:000313654700002
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Nasri, A
   Sinno, K
   Zheng, J
AF Nasri, Ahmad
   Sinno, Khaled
   Zheng, Jianmin
TI Local T-spline surface skinning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE T-splines; Skinning; Nonuniform B-spline surfaces; Computer graphics and
   geometric modeling
ID PLANAR CROSS-SECTIONS; SHAPE RECONSTRUCTION; INTERPOLATION
AB Skinning or lofting remains a challenging problem in computer graphics and free-form surface design. Although it was addressed by many researchers, no sufficiently general solution has been proposed yet. In the interpolating approach, the incompatibility of the input NURBS curves are solved by knot insertion. This process leads to an explosion in the number of control points defining the skinned surface. Other methods avoid this problem by generating skinned surfaces that approximate rather than interpolate the input curves. In this paper, we provide a solution to this problem using T-splines. Compared with existing approaches, a T-spline skinned surface interpolates a set of incompatible curves with a control mesh of fewer vertices. Typically, the linear system involved could be solved globally. However, our approach provides a local solution for each skinned curve. As such, local modification could be used to meet additional constraints such as given normal and/or predefined curvature across the skinned curves.
C1 [Nasri, Ahmad; Sinno, Khaled] Amer Univ Beirut, Dept Comp Sci, ASHA Comp Graph & Animat Lab, Beirut, Lebanon.
   [Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
C3 American University of Beirut; Nanyang Technological University
RP Nasri, A (corresponding author), Amer Univ Beirut, Dept Comp Sci, ASHA Comp Graph & Animat Lab, Beirut, Lebanon.
EM anasri@aub.edu.lb; kws05@aub.edu.lb; asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Nasri, Ahmad/0000-0002-2047-6693
CR Bajaj CL, 1996, GRAPH MODEL IM PROC, V58, P524, DOI 10.1006/gmip.1996.0044
   Ball A. A., 1974, Computer Aided Design, V6, P243, DOI 10.1016/0010-4485(74)90009-8
   Ball A. A., 1975, Computer Aided Design, V7, P237, DOI 10.1016/0010-4485(75)90068-8
   BOISSONNAT JD, 1988, COMPUT VISION GRAPH, V44, P1, DOI 10.1016/S0734-189X(88)80028-8
   Farin G., 2000, The Essentials of CAGD
   FAUX I.D., 1979, COMPUTATIONAL GEOMET
   Nasri A, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P102, DOI 10.1109/PCCGA.2003.1238252
   Park H, 2003, COMPUT AIDED DESIGN, V35, P1261, DOI 10.1016/S0010-4485(03)00040-X
   Piegl LA, 2002, VISUAL COMPUT, V18, P273, DOI 10.1007/s003710100156
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Sederberg TW, 2004, ACM T GRAPHIC, V23, P276, DOI 10.1145/1015706.1015715
   Wang WK, 2008, COMPUT AIDED DESIGN, V40, P999, DOI 10.1016/j.cad.2008.08.001
   WOODWARD CD, 1988, COMPUT AIDED DESIGN, V20, P441, DOI 10.1016/0010-4485(88)90002-4
NR 13
TC 9
Z9 11
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 787
EP 797
DI 10.1007/s00371-012-0692-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500026
DA 2024-07-18
ER

PT J
AU Bouënard, A
   Gibet, S
   Wanderley, MM
AF Bouenard, A.
   Gibet, S.
   Wanderley, M. M.
TI Hybrid inverse motion control for virtual characters interacting with
   sound synthesis Application to percussion motion
SO VISUAL COMPUTER
LA English
DT Article
DE Physics-based computer animation; Motion control; Motion-sound
   interaction; Sound synthesis
ID GESTURE CONTROL
AB The ever growing use of virtual environments requires more and more engaging elements for enhancing user experiences. Specifically regarding sounding virtual environments, one promising option to achieve such realism and interactivity requirements is the use of virtual characters interacting with sounding objects. In this paper, we focus as a case study on virtual characters playing virtual music instruments. We address more specially the real-time motion control and interaction of virtual characters with their sounding environment for proposing engaging and compelling virtual music performances. Combining physics-based simulation with motion data is a recent approach to finely represent and modulate this motion-sound interaction, while keeping the realism and expressivity of the original captured motion. We propose a physically-enabled environment in which a virtual percussionist interacts with a physics-based sound synthesis algorithm. We introduce and extensively evaluate the Hybrid Inverse Motion Control (HIMC), a motion-driven hybrid control scheme dedicated to the synthesis of upper-body percussion movements. We also propose a physics-based sound synthesis model with which the virtual character can interact. Finally, we present an architecture offering an effective way to manage heterogenous data (motion and sound parameters) and feedback (visual and sound) that influence the resulting virtual percussion performances.
C1 [Bouenard, A.; Wanderley, M. M.] McGill Univ, CIRMMT IDMIL Lab, Montreal, PQ H3A 1E3, Canada.
   [Gibet, S.] Univ Bretagne Sud, VALORIA SEASIDE Lab, F-56000 Vannes, France.
C3 McGill University
RP Bouënard, A (corresponding author), McGill Univ, CIRMMT IDMIL Lab, 555 Sherbrooke W, Montreal, PQ H3A 1E3, Canada.
EM abouenard@gmail.com
FU Natural Sciences and Engineering Research Council of Canada; Agence
   Nationale de la Recherche; Pole de Com-petitivite Images & Reseaux
   (France)
FX This work was partially supported by the Natural Sciences and
   Engineering Research Council of Canada (Discovery and Special Research
   Opportunity grants), the Agence Nationale de la Recherche, and the Pole
   de Com-petitivite Images & Reseaux (France).
CR Allen B, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P239
   Aubry M, 2010, LECT NOTES ARTIF INT, V5934, P231, DOI 10.1007/978-3-642-12553-9_20
   Bonneel N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360623
   Bouenard A, 2008, P INT C EN INT, P22
   Bouenard A., 2009, P INT COMP MUS C, P255
   Bouenard A, 2009, P INT C COMP AN SOC, P17
   Bouënard A, 2011, COMPUT MUSIC J, V35, P57, DOI 10.1162/COMJ_a_00069
   Bouënard A, 2010, ACTA ACUST UNITED AC, V96, P668, DOI 10.3813/AAA.918321
   Bouenard Alexandre., 2008, P 2008 C NEW INTERFA, P38
   Cardle M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P349
   Corbett R, 2007, PRESENCE-VIRTUAL AUG, V16, P643, DOI 10.1162/pres.16.6.643
   Coros S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618516
   da Silva M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531388
   Dahl S, 2004, ACTA ACUST UNITED AC, V90, P762
   DEMPSTER WT, 1967, AM J ANAT, V120, P33, DOI 10.1002/aja.1001200104
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   GAVER WW, 1993, ECOL PSYCHOL, V5, P285, DOI 10.1207/s15326969eco0504_2
   Gibet S., 2003, P INT C COMP AN SOC, P465
   Hodgins J. K., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P71, DOI 10.1145/218380.218414
   JOHNSON M. P., 2003, THESIS MIT US
   KLEIN CA, 1983, IEEE T SYST MAN CYB, V13, P245, DOI 10.1109/TSMC.1983.6313123
   Ko H, 1996, IEEE COMPUT GRAPH, V16, P50, DOI 10.1109/38.486680
   Loy G., 2007, MATH FDN MUSIC, V2
   Macchietto A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531386
   Mordatch I., 2010, T GRAPH, V29, P71
   OBrien J.F., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation, P175
   Picard C., 2009, P INT C DIG AUD EFF, P221
   Raghuvanshi N, 2006, P 2006 S INT 3D GRAP, P101, DOI DOI 10.1145/1111411.1111429
   Raibert M. H., 1986, LEGGED ROBOTS BALANC, DOI DOI 10.1109/MEX.1986.4307016
   Reitsma PSA, 2003, ACM T GRAPHIC, V22, P537, DOI 10.1145/882262.882304
   Ren L, 2005, ACM T GRAPHIC, V24, P1090, DOI 10.1145/1073204.1073316
   Shapiro A, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P455, DOI 10.1109/PCCGA.2003.1238294
   Takala T., 1992, Computer Graphics, V26, P211, DOI 10.1145/142920.134063
   van den Doel K, 2001, COMP GRAPH, P537, DOI 10.1145/383259.383322
   van Welbergen H., 2009, P EUR WORKSH AN SIM, P45
   Wagner A., 2006, THESIS KTH ROYAL I T
   WAMPLER CW, 1986, IEEE T SYST MAN CYB, V16, P93, DOI 10.1109/TSMC.1986.289285
   Yang P.-F., 2004, Proceedings of the 2004 ACM SIGGRAPH/EG Symposium on Computer Animation, SCA '04, P39
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
   Zordan VB, 1999, SPRING COMP SCI, P13
NR 40
TC 4
Z9 5
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 357
EP 370
DI 10.1007/s00371-011-0620-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300003
DA 2024-07-18
ER

PT J
AU Gagnon, J
   Paquette, E
AF Gagnon, Jonathan
   Paquette, Eric
TI Procedural and interactive icicle modeling
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Natural phenomena; Procedural modeling; Icicle; Ice modeling; Glaze ice
ID GROWTH
AB Icicle formation is a complex phenomenon which makes it difficult to model for computer graphics applications. The methods commonly used in computer graphics to model icicles provide only minimal control over the results and require several minutes or even hours of computation. This paper proposes a procedural approach allowing interactive modeling, which is broken down into four stages. The first computes the water motion on the surface; the second determines where the water drips; the third computes the trajectories of the icicles growth, and the fourth creates the surface. In addition, the approach allows the creation of glaze ice. The results are not only realistic but also rapidly computed. This approach provides a significant increase in control over results and computation speed.
C1 [Paquette, Eric] Ecole Technol Super, Dept Genie Logiciel & TI, Multimedia Lab, Montreal, PQ, Canada.
   [Gagnon, Jonathan] Mokko Studio, R&D, Montreal, PQ, Canada.
   [Gagnon, Jonathan] Mokko Studio, Res & Dev Grp, Montreal, PQ, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada
RP Paquette, E (corresponding author), Ecole Technol Super, Dept Genie Logiciel & TI, Multimedia Lab, Montreal, PQ, Canada.
EM jgagnon@mokkostudio.com; eric.paquette@etsmtl.ca
OI Paquette, Eric/0000-0001-9236-647X
CR Dorsey Julie., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, P411, DOI [10.1145/237170. 237280, DOI 10.1145/237170.237280]
   Fournier P, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P133
   Kharitonsky D., 1993, Visual Computer, V10, P88, DOI 10.1007/BF01901945
   KIM T, 2004, P 2004 ACM SIGGRAPH, P305, DOI DOI 10.1145/1028523.1028564
   Kim T., 2006, Proc 2006 ACM SIGGRAPH/Eurographics Symp Comp Anim, P167
   MAENO N, 1994, J GLACIOL, V40, P319, DOI 10.3189/S0022143000007401
   MAKKONEN L, 1988, J GLACIOL, V34
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   SZILDER K, 1994, ANN GLACIOL, V19, P141, DOI 10.3189/1994AoG19-1-141-145
   Tong RF, 2002, VISUAL COMPUT, V18, P469, DOI 10.1007/s003710100164
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Yu YJ, 1999, COMPUT GRAPH-UK, V23, P213, DOI 10.1016/S0097-8493(99)00031-X
NR 12
TC 11
Z9 14
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 451
EP 461
DI 10.1007/s00371-011-0584-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600005
DA 2024-07-18
ER

PT J
AU Liang, XZ
   Xue, YH
   Li, QA
AF Liang, Xue-Zhang
   Xue, Yao-Hong
   Li, Qiang
TI Some applications of Loop-subdivision wavelet tight frames to the
   processing of 3D graphics
SO VISUAL COMPUTER
LA English
DT Article
DE Multiresolution analysis; Subdivision wavelet; Subdivision wavelet tight
   frames; Denoising of 3D graphics; Compression of 3D graphics;
   Progressive transmission
AB Multiresolution analysis based on subdivision wavelets is an important method of 3D graphics processing. Many applications of this method have been studied and developed, including denoising, compression, progressive transmission, multiresolution editing and so on. Recently Charina and Stockler firstly gave the explicit construction of wavelet tight frame transform for subdivision surfaces with irregular vertices, which made its practical applications to 3D graphics became a subject worthy of investigation. Based on the works of Charina and Stockler, we present in detail the wavelet tight frame decomposition and reconstruction formulas for Loop-subdivision scheme. We further implement the algorithm and apply it to the denoising, compression and progressive transmission of 3D graphics. By comparing it with the biorthogonal Loop-subdivision wavelets of Bertram, the numerical results illustrate the good performance of the algorithm. Since multiresolution analysis based on subdivision wavelets or subdivision wavelet tight frames requires the input mesh to be semi-regular, we also propose a simple remeshing algorithm for constructing meshes which not only have subdivision connectivity but also approximate the input mesh.
C1 [Xue, Yao-Hong] Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
   [Liang, Xue-Zhang; Li, Qiang] Jilin Univ, Inst Math, Changchun 130012, Peoples R China.
C3 Changchun University of Science & Technology; Jilin University
RP Xue, YH (corresponding author), Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
EM xueyaohong@email.jlu.edu.cn
RI Liang, Xue-Zhang/B-8122-2008
FU National Natural Science Foundation of China [60673021, 60773098]; Jilin
   University [20091009]
FX We would like to express our heartfelt thanks to the reviewers for their
   helpful suggestions. This work is supported by the National Natural
   Science Foundation of China (Nos. 60673021 and 60773098), the 985
   Fundamental Research Fund of Jilin University, and Graduate Innovation
   Fund of Jilin University (No. 20091009).
CR Bertram M, 2004, COMPUTING, V72, P29, DOI 10.1007/s00607-003-0044-0
   Bertram M, 2000, IEEE VISUAL, P389, DOI 10.1109/VISUAL.2000.885720
   Charina M, 2008, J COMPUT APPL MATH, V221, P293, DOI 10.1016/j.cam.2007.10.033
   Charina M, 2008, APPL COMPUT HARMON A, V25, P98, DOI 10.1016/j.acha.2007.09.007
   Chui CK, 2002, APPL COMPUT HARMON A, V13, P224, DOI 10.1016/S1063-5203(02)00510-9
   Chui CK, 2005, APPL COMPUT HARMON A, V18, P25, DOI 10.1016/j.acha.2004.09.004
   Chui CK, 2004, APPL COMPUT HARMON A, V17, P141, DOI 10.1016/j.acha.2004.02.004
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Daubechies I, 2003, APPL COMPUT HARMON A, V14, P1, DOI 10.1016/S1063-5203(02)00511-0
   Dong S, 2006, ACM T GRAPHIC, V25, P1057, DOI 10.1145/1141911.1141993
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   [高福顺 GAO Fushun], 2008, [吉林大学学报. 理学版, Journal of Jilin University. Science Edition], V46, P413
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Krishnamurthy V., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P313, DOI 10.1145/237170.237270
   Lee A. W. F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P95, DOI 10.1145/280814.280828
   Loop C, 1987, THESIS U UTAH
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Melax S., 1998, Game Developer, V11, P44
   Warren J., 2002, SUBDIVISION METHODS
NR 21
TC 5
Z9 5
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2011
VL 27
IS 1
BP 35
EP 43
DI 10.1007/s00371-010-0511-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 700YI
UT WOS:000285781100003
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Eagleson, R
   Peters, TM
AF Zhang, Qi
   Eagleson, Roy
   Peters, Terry M.
TI Rapid scalar value classification and volume clipping for interactive 3D
   medical image visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Volume visualization; Scalar value classification; Volume clipping;
   Endoscopic view; Transfer function; Image power spectrum
ID DESIGN
AB In many clinical scenarios, medical data visualization and interaction are important to physicians for exploring inner anatomical structures and extracting meaningful diagnostic information. Real-time high-quality volume rendering, artifact-free clipping, and rapid scalar value classification are important techniques employed in this process. Unfortunately, in practice, it is still difficult to achieve an optimal balance. In this paper, we present some strategies to address this issue, which are based on the calculation of segment-based post color attenuation and dynamic ray-plane intersection (RPI) respectively. When implemented within our visualization system, the new classification algorithm can deliver real-time performance while avoiding the "color over-accumulation" artifacts suffered by the commonly used acceleration algorithms that employ pre-integrated classification. Our new strategy can achieve an optimized balance between image quality and classification speed. Next, the RPI algorithm is used with opacity adjustment technique to effectively remove the "striping" artifacts on the clipping plane caused by the nonuniform integration length. Furthermore, we present techniques for multiple transfer function (TF) based anatomical feature enhancement and "keyhole" based endoscopic inner structure view. Finally, the algorithms are evaluated subjectively by radiologists and quantitatively compared using image power spectrum analysis.
C1 [Zhang, Qi; Eagleson, Roy; Peters, Terry M.] Univ Western Ontario, Robarts Res Inst, Imaging Res Labs, London, ON, Canada.
   [Eagleson, Roy] Univ Western Ontario, Fac Engn, London, ON, Canada.
   [Peters, Terry M.] Univ Western Ontario, Dept Med Imaging Med Biophys & Biomed Engn, London, ON, Canada.
C3 Western University (University of Western Ontario); Western University
   (University of Western Ontario); Western University (University of
   Western Ontario)
RP Zhang, Q (corresponding author), Univ Western Ontario, Robarts Res Inst, Imaging Res Labs, London, ON, Canada.
EM qzhang@imaging.robarts.ca; eagleson@imaging.robarts.ca;
   tpeters@imaging.robarts.ca
RI Peters, Terry M/K-6853-2013; Peters, Terry Malcolm/AAD-7797-2022;
   Eagleson, Roy/B-7702-2015
OI Peters, Terry Malcolm/0000-0003-1440-7488; Eagleson,
   Roy/0000-0001-9264-8135
FU Canadian Institutes for Health Research [MOP 74626]; National Science
   and Engineering Research Council of Canada [R314GA01]; Ontario Research
   and Development Challenge Fund; Canadian Foundation for Innovation;
   Ontario Innovation Trust; Ontario Ministry of Education; University of
   Western Ontario
FX The authors would like to thank Dr. I.A. Cunningham for valuable
   discussions and assistance in the quantitative image measurement, Dr. G.
   Guiraudon for the clinical based performance evaluation and analysis,
   Drs. J. White and A. Islam for clinical data acquisition, Drs. U. Aladl,
   C. Wedlake and J. Moore for technical support and assistance, and Ms. J.
   Williams for editorial assistance. This project was supported by the
   Canadian Institutes for Health Research (Grant MOP 74626), the National
   Science and Engineering Research Council of Canada (Grant R314GA01), the
   Ontario Research and Development Challenge Fund, the Canadian Foundation
   for Innovation and Ontario Innovation Trust, as well as the scholarships
   from the Ontario Ministry of Education and the University of Western
   Ontario.
CR Bergner S, 2006, IEEE T VIS COMPUT GR, V12, P1353, DOI 10.1109/TVCG.2006.113
   Bethune C., 2005, Journal of Graphics Tools, V10, P55
   Beyer J, 2007, IEEE T VIS COMPUT GR, V13, P1696, DOI 10.1109/TVCG.2007.70560
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Buhler K., 2005, DIGITAL REVOLUTION R, Vsecond
   Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   El Hajjar JF, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P9
   Engel K, 2006, Real-Time Volume Graphics
   Engel Klaus, 2001, P ACM SIGGRAPH EUROG, P9, DOI [DOI 10.1145/383507.383515, 10.1145/383507.383515]
   Hauser H, 2001, IEEE T VIS COMPUT GR, V7, P242, DOI 10.1109/2945.942692
   Kniss J., 2003, P 14 IEEE VISUALIZAT, P65
   Knoll A, 2009, IEEE T VIS COMPUT GR, V15, P1571, DOI 10.1109/TVCG.2009.204
   Kraus M., 2008, P IEEE EG S VOL POIN, P97
   KRAUS M, 2003, THESIS U STUTTGART G
   Krüger J, 2006, IEEE T VIS COMPUT GR, V12, P941, DOI 10.1109/TVCG.2006.124
   Kye H, 2008, GRAPH MODELS, V70, P125, DOI 10.1016/j.gmod.2008.05.001
   Lum Eric., 2004, P JOINT EUROGRAPHICS, P25
   Lundström C, 2007, IEEE T VIS COMPUT GR, V13, P1648, DOI 10.1109/TVCG.2007.70518
   Lundström C, 2006, IEEE T VIS COMPUT GR, V12, P1570, DOI 10.1109/TVCG.2006.100
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Moreland K, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P9
   Pfister H, 2001, IEEE COMPUT GRAPH, V21, P16, DOI 10.1109/38.920623
   Pinto FD, 2008, COMPUT GRAPH-UK, V32, P540, DOI 10.1016/j.cag.2008.08.006
   Pommert A., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P598
   Preim B., 2007, M KAUFMANN SERIES CO
   Reottger S., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P231
   Rezk-Salama C., 2001, THESIS U ERLANGEN NU
   Rezk-Salama C, 2009, ADV INFORM KNOWL PRO, P99, DOI 10.1007/978-1-84800-269-2_5
   Roettger S, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P23, DOI 10.1109/SWG.2002.1226506
   Röttger S, 2000, IEEE VISUAL, P109, DOI 10.1109/VISUAL.2000.885683
   Salama CR, 2006, IEEE T VIS COMPUT GR, V12, P1021, DOI 10.1109/TVCG.2006.148
   Scharsach H., 2006, IEEE-VGTC Symposium on Visualization, P315
   Schulze J.P., 2003, VG 03, P109
   Siewerdsen JH, 1998, MED PHYS, V25, P614, DOI 10.1118/1.598243
   Stegmaier S, 2005, VOLUME GRAPHICS 2005, P187
   Viola I, 2005, IEEE T VIS COMPUT GR, V11, P408, DOI 10.1109/TVCG.2005.62
   Weiskopf D, 2003, IEEE T VIS COMPUT GR, V9, P298, DOI 10.1109/TVCG.2003.1207438
   Westermann R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P169, DOI 10.1145/280814.280860
   Williams PL, 1998, IEEE T VIS COMPUT GR, V4, P37, DOI 10.1109/2945.675650
   Wittenbrink CM, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P135, DOI 10.1109/SVV.1998.729595
   Wu YC, 2007, IEEE T VIS COMPUT GR, V13, P1027, DOI 10.1109/TVCG.2007.1051
   Xie K, 2007, COMPUT BIOL MED, V37, P1155, DOI 10.1016/j.compbiomed.2006.10.002
   Zhang Q, 2007, LECT NOTES COMPUT SC, V4792, P86
   Zhang Q, 2006, PROC SPIE, V6141, DOI 10.1117/12.653537
NR 44
TC 10
Z9 11
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2011
VL 27
IS 1
BP 3
EP 19
DI 10.1007/s00371-010-0509-z
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 700YI
UT WOS:000285781100001
DA 2024-07-18
ER

PT J
AU Kim, JW
   Chung, WK
   Cho, HG
AF Kim, Jong-Woo
   Chung, Woo-Keun
   Cho, Hwan-Gue
TI A new image-based CAPTCHA using the orientation of the polygonally
   cropped sub-images
SO VISUAL COMPUTER
LA English
DT Article
DE Image orientation; CAPTCHA; Machine learning; Perceptual recognition
AB With an increasing number of automated software bots and automated scripts that exploit public web services, the user is commonly required to solve a Turing test problem, namely a Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA), before they are allowed to use web services. As a solution of CAPTCHAs, the Image Orientation CAPTCHA is based on the hardness of image orientation. So, there is a close correlation between image orientation detection and the performance of image orientation CAPTCHA. In this paper, we introduce a reliable and effective CAPTCHA based on the orientation of cropped sub-images. Also, we try to investigate the key spatial features of sub-image orientation detection such as crop size, major color components, and the number of orientations. So, the goal of this paper is discovering the relationship between these spatial features and the detecting sub-image orientation by human manual work and machine learning-based softwares, respectively. Our experimental results enable our CAPTCHA system to filter out any sub-images difficult for human. Therefore, our experiment showed that exploiting the key spatial features of cropped sub-images is very useful to design a new image-based CAPTCHA system.
C1 [Chung, Woo-Keun; Cho, Hwan-Gue] Pusan Natl Univ, Dept Comp Sci, Pusan 609735, South Korea.
   [Kim, Jong-Woo] Pusan Natl Univ, Ctr U Port IT Res & Educ, Pusan 609735, South Korea.
C3 Pusan National University; Pusan National University
RP Cho, HG (corresponding author), Pusan Natl Univ, Dept Comp Sci, Pusan 609735, South Korea.
EM hgcho@pusan.ac.kr
FU MKE/MCST/KEIT [KI001820]
FX This work was supported by the IT R&D program of MKE/MCST/KEIT
   (KI001820, Development of Computational Photography Technologies for
   Image and Video Contents).
CR [Anonymous], P INT C SIGN IM ENG
   Chellapilla K., 2005, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P711
   CHUNG WK, 2010, P WSCG
   ELSON J, 2007, P 14 ACM C COMP COMM, P366
   Gossweiler R., 2009, P 18 INT C WORLD WID, P841, DOI DOI 10.1145/1526709.1526822
   JAVIER C, 2009, COMPUTER SECURITY, P1
   JEFF Y, 2008, P S US PRIV SEC, P44
   LUO J, 2004, P C CVPR
   Luo JB, 2003, SPATIAL VISION, V16, P429, DOI 10.1163/156856803322552757
   Prasad BG, 2004, COMPUT VIS IMAGE UND, V94, P193, DOI 10.1016/j.cviu.2003.10.016
   SIMARD P, 2004, ADV NEURAL INFORM PR
   SIWEI L, 2005, P 13 ACM MULT, P491
   Vailaya A, 2002, IEEE T IMAGE PROCESS, V11, P746, DOI [10.1109/TIP.2002.801590, 10.1109/TIP2002.801590]
   WANG Y, 2001, IEEE WORKSH CONT BAS
   Wang YM, 2004, COMPUT VIS IMAGE UND, V93, P328, DOI 10.1016/j.cviu.2003.10.006
NR 15
TC 10
Z9 10
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1135
EP 1143
DI 10.1007/s00371-010-0469-3
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800072
DA 2024-07-18
ER

PT J
AU Petit, J
   Brémond, R
AF Petit, Josselin
   Bremond, Roland
TI A high dynamic range rendering pipeline for interactive applications
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; High dynamic range; Perceptual realism; Interactivity; Pixel
   shader
AB Realistic images can be computed at interactive frame rates for Computer Graphics applications. Meanwhile, High Dynamic Range (HDR) rendering has a growing success in video games and virtual reality applications, as it improves the image quality and the player's immersion feeling. In this paper, we propose a new method, based on a physical lighting model, to compute in real time a HDR illumination in virtual environments. Our method allows to re-use existing virtual environments as input, and computes HDR images in photometric units. Then, from these HDR images, displayable 8-bit images are rendered with a tone mapping operator and displayed on a standard display device. The HDR computation and the tone mapping are implemented in OpenSceneGraph with pixel shaders. The lighting model, together with a perceptual tone mapping, improves the perceptual realism of the rendered images at low cost. The method is illustrated with a practical application where the dynamic range of the virtual environment is a key rendering issue: night-time driving simulation.
C1 [Petit, Josselin; Bremond, Roland] Univ Paris Est, LEPSiS, F-75015 Paris, France.
   [Petit, Josselin] Univ Lyon 1, LIRIS, UMR5205, F-69622 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Universite Claude Bernard Lyon 1
RP Brémond, R (corresponding author), Univ Paris Est, LEPSiS, F-75015 Paris, France.
EM josselin.petit@lcpc.fr; roland.bremond@lcpc.fr
RI Bremond, Roland/AAT-1408-2021
OI Bremond, Roland/0000-0003-3150-7624
FU Laboratoire Central des Ponts et Chaussees
FX This study was supported by a grant from the Laboratoire Central des
   Ponts et Chaussees. We wish to thank E. Dumont, V. Ledoux, V. Biri and
   B. Peroche for fruitful discussions.
CR Alferdinck JWAM, 2006, OPHTHAL PHYSL OPT, V26, P264, DOI 10.1111/j.1475-1313.2006.00324.x
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2006, ACM Trans. Appl. Percept.
   [Anonymous], P C ACM SIGGRAPH 200
   ARTUSI A, 2003, P 14 EUR WORKSH REND, P38
   Ashikhmin M., 2006, ACM T APPL PERCEPT, V3, P399
   Colbert M, 2006, IEEE COMPUT GRAPH, V26, P30, DOI 10.1109/MCG.2006.13
   Debevec P, 2002, IEEE COMPUT GRAPH, V22, P26, DOI 10.1109/38.988744
   Durand F, 2000, SPRING COMP SCI, P219
   Ferwerda JA, 2003, PROC SPIE, V5007, P290, DOI 10.1117/12.473899
   Giannopulu I, 2008, ADV TRANSP STUD, V14, P49
   Grave J, 2008, ACM T APPL PERCEPT, V5, DOI 10.1145/1279920.1361704
   HUBERT R, 1993, B LIAISON LAB PONTS, V176, P19
   Hunt R.W.G., 1995, The Reproduction of Color
   Irawan P., 2005, Rendering Techniques, P231
   Kakimoto M, 2005, COMPUT GRAPH FORUM, V24, P185, DOI 10.1111/j.1467-8659.2005.00842.x
   KAWASE M, 2004, GAM DEV C
   KRAWCZYK G, 2005, P SPRING C COMP GRAP, P195
   Kuang J., 2007, ACM T APPL PERCEPT, V3, P286
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Luksh C., 2007, REALTIME HDR RENDERI
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   MITCHELL J, 2006, P SIGGRAPH, P129
   Nakamae E., 1990, Computer Graphics, V24, P395, DOI 10.1145/97880.97922
   *NVIDIA, 2009, OPTIX WEB PAG
   Pattanaik S.N., 1998, P SIGGRAPH 98, P287
   Pattanaik SN, 2000, COMP GRAPH, P47, DOI 10.1145/344779.344810
   PETIT J, 2009, P VIRT REAL SOFTW TE, P251
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Ritschel T, 2009, COMPUT GRAPH FORUM, V28, P183, DOI 10.1111/j.1467-8659.2009.01357.x
   Scheel A, 2000, COMPUT GRAPH FORUM, V19, pC301, DOI 10.1111/1467-8659.00422
   Schoettle B, 2003, MARKET WEIGHTED DESC
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Spencer G., 1995, P ACM SIGGRAPH 95, P325
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Wandell B. A, 1995, Foundations of vision
   Yoshida A, 2005, PROC SPIE, V5666, P192, DOI 10.1117/12.587782
NR 40
TC 11
Z9 11
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 533
EP 542
DI 10.1007/s00371-010-0430-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800014
DA 2024-07-18
ER

PT J
AU Li, X
   Deng, JS
   Chen, FL
AF Li, Xin
   Deng, Jiansong
   Chen, Falai
TI Polynomial splines over general T-meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Splines; PHT-splines; T-meshes; Basis function; Simplification
AB The present authors have introduced polynomial splines over T-meshes (PHT-splines) and provided theories and applications for PHT-splines over hierarchical T-meshes. This paper generalizes PHT-splines to arbitrary topology over general T-meshes with any structures (GPT-splines). GPT-spline surfaces can be constructed through a unified scheme to interpolate the local geometric information at the basis vertices of the T-mesh. We also discuss general edge insertion and removal algorithms for GPT-splines. As applications, we present algorithms to construct a GPT-spline surface from a quadrilateral mesh and to simplify a tensor-product B-spline surface into a GPT-spline surface with superfluous edges removal.
C1 [Li, Xin; Deng, Jiansong; Chen, Falai] Univ Sci & Technol China, Dept Math, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, X (corresponding author), Univ Sci & Technol China, Dept Math, Hefei 230026, Anhui, Peoples R China.
EM lixustc@ustc.edu.cn; dengjs@ustc.edu.cn; chenfl@ustc.edu.cn
RI Li, Xin/B-1324-2012; Li, Xin/AAS-5967-2021; Chen, Falai/C-3846-2013;
   Deng, Jiansong/F-1869-2010
OI Li, Xin/0000-0003-0477-7098; Li, Xin/0000-0002-0144-9489; Deng,
   Jiansong/0000-0003-4093-373X
FU National Key Basic Research Project of China [2004CB318000]; NSF of
   China [60533060, 10671192, 10701069, 60903148]; CAS; Program for New
   Century Excellent Talents in University; Specialized Research Fund for
   the Doctoral Program of Higher Education [20060358055]; 111 Project
   [b07033]
FX The authors are supported by the National Key Basic Research Project of
   China (No. 2004CB318000), the NSF of China (Nos. 60533060, 10671192,
   10701069, 60903148), One Hundred Talent Project supported by CAS,
   Program for New Century Excellent Talents in University, Chinese Academy
   of Science (Startup Scientific Research Foundation), the Specialized
   Research Fund for the Doctoral Program of Higher Education (No.
   20060358055), and the 111 Project (No. b07033).
CR [Anonymous], ACM T GRAPH
   Cardon D.L., 2007, THESIS BRIGHAM YOUNG
   Deng JS, 2008, GRAPH MODELS, V70, P76, DOI 10.1016/j.gmod.2008.03.001
   Deng JS, 2006, J COMPUT APPL MATH, V194, P267, DOI 10.1016/j.cam.2005.07.009
   Dierckx P, 1997, COMPUT AIDED GEOM D, V15, P61, DOI 10.1016/S0167-8396(97)81785-2
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   FORSEY D, 1998, GRAPH INTERF, V2, P57
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   GONZALEZOCHOA C, 1999, P 1999 S INT 3D GRAP, V2, P7
   Kraft R, 1997, SURFACE FITTING MULT, V2, P209
   Li X, 2007, VISUAL COMPUT, V23, P1027, DOI 10.1007/s00371-007-0170-3
   Sederberg TW, 2004, ACM T GRAPHIC, V23, P276, DOI 10.1145/1015706.1015715
NR 12
TC 50
Z9 54
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2010
VL 26
IS 4
SI SI
BP 277
EP 286
DI 10.1007/s00371-009-0410-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 584JA
UT WOS:000276746600005
DA 2024-07-18
ER

PT J
AU Ye, JT
   Webber, RE
   Wang, YS
AF Ye, Juntao
   Webber, Robert E.
   Wang, Yangsheng
TI A reduced unconstrained system for the cloth dynamics solver
SO VISUAL COMPUTER
LA English
DT Article
DE Direct-iterative method; Cloth simulation; Physically based modeling;
   Matrix reordering; Direct method; Preconditioner
ID SPARSE; SIMULATION; COLLISIONS; GPU
AB Modern direct solvers have been more and more widely used by computer graphics community for solving sparse linear systems, such as those that arise in cloth simulation. However, external constraints usually prevent a direct method from being used for cloth simulation due to the singularity of the constrained system. This paper makes two major contributions towards the re-introduction of direct methods for cloth dynamics solvers. The first one is an approach which eliminates all the constrained variables from the system so that we obtain a reduced, nonsingular and unconstrained system. As alternatives to the well-known MPCG algorithm, not only the original, unmodified PCG method, but also any direct method can be used to solve the reduced system at a lower cost. Our second contribution is a novel direct-iterative scheme applied for the reduced system, which is basically the conjugate gradient method using a special preconditioner. Specifically, we use the stiff part of the coefficient matrix, which we call the matrix core, as the preconditioner for the PCG. The inverse of this preconditioner is computed by any eligible direct solver. The direct-iterative method has proved to be more efficient than both direct and iterative methods. Our experiments show a factor of two speedup over direct methods when stiff springs are used, even greater improvements over the MPCG iterative method.
C1 [Ye, Juntao; Wang, Yangsheng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Webber, Robert E.] Univ Western Ontario, Dept Comp Sci, London, ON N6A 5B7, Canada.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Western
   University (University of Western Ontario)
RP Ye, JT (corresponding author), Chinese Acad Sci, Inst Automat, 95 Zhongguancun E Rd, Beijing 100190, Peoples R China.
EM yejuntao@gmail.com; webber@csd.uwo.ca; yangsheng.wang@ia.ac.cn
RI Wang, Yangsheng/AAD-8041-2021
OI Wang, Yangsheng/0000-0002-8350-2115
FU National High Technology Research and Development Program of China
   [2007AA01Z341]
FX Research supported in part by the National High Technology Research and
   Development Program of China ( 863 Program) #2007AA01Z341. The authors
   would like to sincerely thank the anonymous reviewers for their
   suggestions that made this paper better.
CR Amestoy PR, 2004, ACM T MATH SOFTWARE, V30, P381, DOI 10.1145/1024074.1024081
   Ascher UM, 2003, VISUAL COMPUT, V19, P526, DOI 10.1007/s00371-003-0220-4
   ASCHER UM, 1995, SIAM J NUMER ANAL, V32, P797, DOI 10.1137/0732037
   Baraff D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P137, DOI 10.1145/237170.237226
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   BERGOU M, 2006, P 4 EUR S GEOM PROC, P227
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Boxerman Eddy., 2004, SCA 04 P 2004 ACM SI, P153
   Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623
   BRIDSON R, 2003, P 2003 ACM SIGGRAPH, P28
   CHEN Y, 2006, TR2006005 U FLOR
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Davis TA, 2005, SIAM J MATRIX ANAL A, V26, P621, DOI 10.1137/S089547980343641X
   Davis TA, 2001, SIAM J MATRIX ANAL A, V22, P997, DOI 10.1137/S0895479899357346
   Davis TA, 1999, SIAM J MATRIX ANAL A, V20, P606, DOI 10.1137/S0895479897321076
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   EBERHARDT B, 2000, EUR WORKSH COMP AN S, V11, P137
   English E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360665
   Goldenthal R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239500
   Golub G.H., 1989, MATRIX COMPUTATIONS
   GOULD NI, 2007, ACM T MATH SOFTW, V33
   Hauth M, 2003, VISUAL COMPUT, V19, P581, DOI 10.1007/s00371-003-0206-2
   HAUTH M, 2004, THESIS U TUBINGEN
   Heggernes P., 2001, P 14 NORWEGIAN COMPU, P98, DOI [DOI 10.2172/15002765, 10.2172/15002765]
   Hong M, 2005, IEEE INT CONF ROBOT, P4520
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Krüger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   PROVOT X, 1995, GRAPH INTER, P147
   Schenk O, 2006, ELECTRON T NUMER ANA, V23, P158
   SHEWCHUK J, 1994, CMUCSTR94125
   TSIKNIS D, 2006, P SIGGRAPH EUR S COM
   Volino P, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P265, DOI 10.1109/CGI.2001.934683
   Volino P, 2006, ACM T GRAPHIC, V25, P1154, DOI 10.1145/1141911.1142007
   YE J, 2008, COMPUT GRAPH FORUM, V27
   Ye JT, 2005, LECT NOTES COMPUT SC, V3515, P331
NR 36
TC 5
Z9 8
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 959
EP 971
DI 10.1007/s00371-008-0307-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 500TP
UT WOS:000270328200007
DA 2024-07-18
ER

PT J
AU Dardenne, J
   Valette, S
   Siauve, N
   Burais, N
   Prost, R
AF Dardenne, J.
   Valette, S.
   Siauve, N.
   Burais, N.
   Prost, R.
TI Variational tetrahedral mesh generation from discrete volume data
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Discrete data; Tetrahedral meshing; Centroidal Voronoi Diagrams; Sizing
   field
ID OPTIMIZATION
AB In this paper, we propose a novel tetrahedral mesh generation algorithm, which takes volumic data (voxels) as an input. Our algorithm performs a clustering of the original voxels within a variational framework. A vertex replaces each cluster and the set of created vertices is triangulated in order to obtain a tetrahedral mesh, taking into account both the accuracy of the representation and the elements quality. The resulting meshes exhibit good elements quality with respect to minimal dihedral angle and tetrahedra form factor. Experimental results show that the generated meshes are well suited for Finite Element Simulations.
C1 [Dardenne, J.; Valette, S.; Prost, R.] Univ Lyon, CREATIS LRMN, F-69621 Villeurbanne, France.
   [Dardenne, J.; Valette, S.; Prost, R.] CNRS, UMR5220, F-69621 Villeurbanne, France.
   [Dardenne, J.; Valette, S.; Prost, R.] INSERM, U630, F-69621 Villeurbanne, France.
   [Dardenne, J.; Valette, S.; Prost, R.] INSA Lyon, F-69621 Villeurbanne, France.
   [Dardenne, J.; Valette, S.; Siauve, N.; Burais, N.; Prost, R.] Univ Lyon 1, F-69621 Villeurbanne, France.
   [Dardenne, J.; Siauve, N.; Burais, N.] Univ Lyon, AMPERE, F-69621 Villeurbanne, France.
   [Dardenne, J.; Siauve, N.; Burais, N.] CNRS, UMR5005, F-69621 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Institut
   National de la Sante et de la Recherche Medicale (Inserm); Institut
   National des Sciences Appliquees de Lyon - INSA Lyon; Centre National de
   la Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS); Institut National de la Sante et de la
   Recherche Medicale (Inserm); Institut National des Sciences Appliquees
   de Lyon - INSA Lyon; Universite Claude Bernard Lyon 1; Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Universite Claude Bernard
   Lyon 1; Centre National de la Recherche Scientifique (CNRS); CNRS -
   Institute for Engineering & Systems Sciences (INSIS); Universite Claude
   Bernard Lyon 1; Institut National des Sciences Appliquees de Lyon - INSA
   Lyon
RP Dardenne, J (corresponding author), Univ Lyon, CREATIS LRMN, F-69621 Villeurbanne, France.
EM julien.dardenne@creatis.insa-lyon.fr;
   sebastien.valette@creatis.insa-lyon.fr; nicolas.siauve@univ-lyon1.fr;
   noel.burais@univ-lyon1.fr; remy.prost@creatis.insa-lyon.fr
RI Prost, Rémy/I-2675-2014; Valette, Sebastien/H-4195-2014
OI Valette, Sebastien/0000-0001-7549-4808
CR Alliez P, 2005, ACM T GRAPHIC, V24, P617, DOI 10.1145/1073204.1073238
   [Anonymous], P 16 INT MESH ROUNDT
   Baker T.J., 1989, 7 INT C FIN EL METH, P1018
   BRIDSON R, 2005, ENG COMPUT, V21, P2
   Chen KH, 1999, ELEC SOC S, V99, P13
   Choi WY, 2003, INT J NUMER METH ENG, V58, P1857, DOI 10.1002/nme.840
   Dardenne J., 2008, CGI 2008 C P, P299
   Du Q, 2003, INT J NUMER METH ENG, V56, P1355, DOI 10.1002/nme.616
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   KOVALEVSKY VA, 1989, COMPUT VISION GRAPH, V46, P141, DOI 10.1016/0734-189X(89)90165-5
   MARCUM DL, 1995, AIAA J, V33, P1619, DOI 10.2514/3.12701
   Mitchell SA, 2000, SIAM J COMPUT, V29, P1334, DOI 10.1137/S0097539796314124
   Shewchuk J.R., 2002, Proceedings of International Meshing Roundtable, P115
   Shewchuk JR, 1997, DISCRETE COMPUT GEOM, V18, P305, DOI 10.1007/PL00009321
   Siauve N, 2004, IEEE T MAGN, V40, P1264, DOI 10.1109/TMAG.2004.825432
   Thiele J P, 2002, Biomed Tech (Berl), V47 Suppl 1 Pt 2, P743, DOI 10.1515/bmte.2002.47.s1b.743
   Valette S, 2008, IEEE T VIS COMPUT GR, V14, P369, DOI 10.1109/TVCG.2007.70430
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 22
TC 16
Z9 22
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 401
EP 410
DI 10.1007/s00371-009-0323-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300004
DA 2024-07-18
ER

PT J
AU El Hajjar, JF
   Jolivet, V
   Ghazanfarpour, D
   Pueyo, X
AF El Hajjar, Jean-Francois
   Jolivet, Vincent
   Ghazanfarpour, Djamchid
   Pueyo, Xavier
TI A model for real-time on-surface flows
SO VISUAL COMPUTER
LA English
DT Article
DE On-surface flow; Droplet; Real-time; Graphics hardware
ID WATER DROPLETS; SIMULATION; ANIMATION
AB Simulating fluid flows for visualization purposes is known to be one of the most challenging fields of the computer graphics domain. While rendering vast liquid areas has been widely addressed this last decade, few papers have tackled the problematic of on-surface flows, even though real-time applications such as drive simulators or video games could greatly benefit from such methods. We present a novel empirical method for the animation of liquid droplets lying on a flat surface, the core of our technique being a simulation operating on a 2D grid which is implementable on GPU. The wetted surface can freely be oriented in space and is not limited to translucent materials, the liquid flow being governed by external forces, the viscosity parameter and the presence of obstacles. Furthermore, we show how to simply incorporate in our simulation scheme two enriching visual effects, namely absorption and ink transport. Rendering can be achieved from an arbitrary view point using a GPU image based raycasting approach and takes into account the refraction and reflection of light. Even though our method doesn't benefit from the literature of fluid mechanics, we show that convincing animations in terms of realism can be achieved in real-time.
C1 [El Hajjar, Jean-Francois; Jolivet, Vincent; Ghazanfarpour, Djamchid] Univ Limoges, XLIM, F-87065 Limoges, France.
   [Pueyo, Xavier] Univ Girona, IIiA, Girona, Spain.
C3 Universite de Limoges; Universitat de Girona; Consejo Superior de
   Investigaciones Cientificas (CSIC); CSIC - Instituto de Investigacion en
   Inteligencia Artificial (IIIA)
RP El Hajjar, JF (corresponding author), Univ Limoges, XLIM, F-87065 Limoges, France.
EM jean-francois.el-hajjar@xlim.fr
RI Pueyo, Xavier/L-3302-2017
OI Pueyo, Xavier/0000-0002-3622-583X
FU French Conseil Regional du Limousin of the Spanish M.E.C
   [TIN2004-07672-C03-01]
FX This work has been developed under the French Conseil Regional du
   Limousin and under the Project TIN2004-07672-C03-01 of the Spanish
   M.E.C.
CR BABOUD L, 2006, EUR WORKSH NAT PHEN, P25
   BRACKBILL JU, 1992, J COMPUT PHYS, V100, P335, DOI 10.1016/0021-9991(92)90240-Y
   Chen JX, 1997, IEEE COMPUT GRAPH, V17, P52, DOI 10.1109/38.586018
   CHEN JX, 1995, GRAPH MODEL IM PROC, V57, P107, DOI 10.1006/gmip.1995.1012
   DEGENNES PG, 1985, REV MOD PHYS, V57, P827, DOI 10.1103/RevModPhys.57.827
   DORSEY J, 1996, SIGGRAPH 96, P411
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Fedkiw RP, 1999, J COMPUT PHYS, V152, P457, DOI 10.1006/jcph.1999.6136
   Foster N, 1996, GRAPH MODEL IM PROC, V58, P471, DOI 10.1006/gmip.1996.0039
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Fournier P, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P133
   Gueyffier D, 1999, J COMPUT PHYS, V152, P423, DOI 10.1006/jcph.1998.6168
   HINSINGER D, 2002, S COMP AN, P161
   Hong JM, 2005, ACM T GRAPHIC, V24, P915, DOI 10.1145/1073204.1073283
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   Iglesias A, 2001, CAD/GRAPHICS '2001: PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON COMPUTER AIDED DESIGN AND COMPUTER GRAPHICS, VOLS 1 AND 2, P350
   Kaneda K, 1999, J VISUAL COMP ANIMAT, V10, P15, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<15::AID-VIS192>3.0.CO;2-P
   KANEDA K, 1993, P COMP AN 93, P177
   KANEDA K, 1996, P PAC GRAPH 96, P50
   Kass M., 1990, Computer Graphics, V24, P49, DOI 10.1145/97880.97884
   Liu YQ, 2005, VISUAL COMPUT, V21, P727, DOI 10.1007/s00371-005-0314-2
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Muller M., 2003, SCA, P154
   MURTA A, 1999, WSCG 99 P, P194
   Policarpo F, 2005, ACM T GRAPHIC, V24, P935, DOI 10.1145/1073204.1073292
   SATO T, 2002, ENTERTAINMENT COMPUT, P125
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Thürey N, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P191
   Thürey N, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P39, DOI 10.1109/PG.2007.33
   Tong RF, 2002, VISUAL COMPUT, V18, P469, DOI 10.1007/s003710100164
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Yang YG, 2004, COMPUT SCI ENG, V6, P69, DOI 10.1109/MCSE.2004.20
   Yu YJ, 1999, COMPUT GRAPH-UK, V23, P213, DOI 10.1016/S0097-8493(99)00031-X
   Yu YJ, 1998, WSCG '98, VOL 3, P432
   Yuksel C, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239550
   Zheng W., 2006, Proc. of the ACM Siggraph/Eurographics Symposium on Computer Animation, P325
NR 37
TC 3
Z9 4
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 87
EP 100
DI 10.1007/s00371-007-0207-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700001
DA 2024-07-18
ER

PT J
AU Stork, A
   Thole, CA
   Klimenko, S
   Nikitin, I
   Nikitina, L
   Astakhov, Y
AF Stork, Andre
   Thole, Clemens-August
   Klimenko, Stanislav
   Nikitin, Igor
   Nikitina, Lialia
   Astakhov, Yuri
TI Towards interactive simulation in automotive design
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th INTUITION International Conference and Workshop
CY OCT 04-05, 2007
CL Athens, GREECE
DE Simulation; Optimization; Progressive meshes; Virtual production
   engineering
ID OPTIMIZATION
AB One of the important tasks in Mechanical Engineering is to increase the safety of the vehicle and decrease its production costs. This task is typically solved by means of Multiobjective Optimization, which formulates the problem as a mapping from the space of design variables to the space of target criteria and tries to find an optimal region in these multidimensional spaces. Due to high computational costs of numerical simulations, the sampling of this mapping is usually very sparse and scattered. Combining design of experiments methods, metamodeling, new interpolation schemes and innovative graphics methods, we enable the user to interact with simulation parameters, optimization criteria, and come to a new interpolated crash result within seconds. We denote this approach as Simulated Reality, a new concept for the interplay between simulation, optimization and interactive visualization. In this paper we show the application of Simulated Reality for solution of real life car design optimization problems.
C1 [Thole, Clemens-August; Nikitina, Lialia] SCAI, Fraunhofer Inst Algorithms & Sci Comp, St Augustin, Germany.
   [Stork, Andre] IGD, Fraunhofer Inst Comp Graph, Darmstadt, Germany.
   [Klimenko, Stanislav; Astakhov, Yuri] ICPT, Inst Comp Phys & Technol, Moscow, Russia.
   [Nikitin, Igor; Nikitina, Lialia] Univ Cologne, Inst Math, D-5000 Cologne, Germany.
C3 Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss
   Birlinghoven; Fraunhofer Gesellschaft; University of Cologne
RP Nikitina, L (corresponding author), SCAI, Fraunhofer Inst Algorithms & Sci Comp, St Augustin, Germany.
EM Lialia.Nikitina@scai.fraunhofer.de
CR [Anonymous], P SIGGRAPH 96 NEW OR
   BRUSENTSEV P, 2002, P 2 INT WORKSH VIRT, P6
   Buhmann M.D., 2003, C MO AP C M, V12, DOI 10.1017/CBO9780511543241
   DONOHO DL, 2000, AUG 8 2000 AM MATH S
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   *SCAI FRAUNH I, 2005, BMBF01IRB01 SCAI FRA
   Sóbester A, 2005, J GLOBAL OPTIM, V33, P31, DOI 10.1007/s10898-004-6733-1
   STORK A, 2006, P CADFEM 06 24 CADFE
   THOLE CA, 2007, P C VIRT PROD DEV AU
   Tukey J.W., 1997, EXPLORATORY DATA ANA
   Wundrak S, 2006, WSCG 2006: SHORT PAPERS PROCEEDINGS, P61
NR 11
TC 9
Z9 9
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2008
VL 24
IS 11
BP 947
EP 953
DI 10.1007/s00371-008-0274-4
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 359BQ
UT WOS:000259961600004
DA 2024-07-18
ER

PT J
AU Rousselle, F
   Clarberg, P
   Leblanc, L
   Ostromoukhov, V
   Poulin, P
AF Rousselle, Fabrice
   Clarberg, Petrik
   Leblanc, Luc
   Ostromoukhov, Victor
   Poulin, Pierre
TI Efficient product sampling using hierarchical thresholding
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE importance sampling; rejection sampling; multiple functions; ray
   tracing; visibility
AB We present an efficient method for importance sampling the product of multiple functions. Our algorithm computes a quick approximation of the product on the fly, based on hierarchical representations of the local maxima and averages of the individual terms. Samples are generated by exploiting the hierarchical properties of many low-discrepancy sequences, and thresholded against the estimated product. We evaluate direct illumination by sampling the triple product of environment map lighting, surface reflectance, and a visibility function estimated per pixel. Our results show considerable noise reduction compared to existing state-of-the-art methods using only the product of lighting and BRDF.
C1 [Rousselle, Fabrice; Poulin, Pierre] Univ Montreal, Comp Sci & Operat Res Dept, Montreal, PQ H3C 3J7, Canada.
   [Rousselle, Fabrice] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Clarberg, Petrik] Lund Univ, Dept Comp Sci, S-22100 Lund, Sweden.
C3 Universite de Montreal; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne; Lund University
RP Rousselle, F (corresponding author), Univ Montreal, Comp Sci & Operat Res Dept, POB 6128,Stn Ctr Ville, Montreal, PQ H3C 3J7, Canada.
EM fabrice.rousselle@epfl.ch
OI Rousselle, Fabrice/0009-0003-2978-2130
CR [Anonymous], 2006, Advanced Global Illumination
   Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Burke David., 2005, PROC EGSR 2005, P147
   Clarberg P, 2005, ACM T GRAPHIC, V24, P1166, DOI 10.1145/1073204.1073328
   Cline D., 2006, EUROGRAPHICS WORKSHO, P103
   Debevec P., 1998, SIGGRAPH98, P189, DOI DOI 10.1145/280814.280864
   Friedel I, 2002, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2000, P257
   Ghosh A, 2006, VISUAL COMPUT, V22, P693, DOI 10.1007/s00371-006-0055-x
   Górski KM, 2005, ASTROPHYS J, V622, P759, DOI 10.1086/427976
   Jensen H., 2003, ACM SIGGRAPH 2003 CO
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Keller A, 1998, THESIS U KAISERSLAUT
   KOLLIG T, 2003, RENDERING TECHNIQUES, P45
   Lawrence J, 2004, ACM T GRAPHIC, V23, P496, DOI 10.1145/1015706.1015751
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Niederreiter H, 1992, RANDOM NUMBER GENERA, V63
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Ostromoukhov V, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239529, 10.1145/1276377.1276475]
   OWEN AB, 1995, LECT NOTES STAT, V107, P299
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Sameer Agarwal, 2003, ACM Transactions on Graphics, V22, P605, DOI 10.1145/882262.882314
   TALBOT J, 2005, RENDERING TECHNIQUES, P139
   Veach E., 1995, P 22 ANN C COMP GRAP, P419, DOI [DOI 10.1145/218380.218498, 10.1145/218380.218498]
   Wang R, 2006, VISUAL COMPUT, V22, P612, DOI 10.1007/s00371-006-0052-0
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
NR 25
TC 12
Z9 13
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 465
EP 474
DI 10.1007/s00371-008-0227-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800002
DA 2024-07-18
ER

PT J
AU Wan, CK
   Yuan, BZ
   Miao, ZJ
AF Wan, Chengkai
   Yuan, Baozong
   Miao, Zhenjiang
TI Markerless human body motion capture using Markov random field and
   dynamic graph cuts
SO VISUAL COMPUTER
LA English
DT Article
DE motion capture; dynamic graph cuts; Markov random field
AB Current vision-based human body motion capture methods always use passive markers that are attached to key locations on the human body. However, such systems may confront subjects with cumbersome markers, making it difficult to convert the marker data into kinematic motion. In this paper, we propose a new algorithm for markerless computer vision-based human body motion capture. We compute volume data (voxels) representation from the images using the method of SFS (shape from silhouettes), and consider the volume data as a MRF (Markov random field). Then we match a predefined human body model with pose parameters to the volume data, and the calculation of this matching is transformed into energy function minimization. We convert the problem of energy function construction into a 3D graph construction, and get the minimal energy by the max-flow theory. Finally, we recover the human pose by Powell algorithm.
C1 [Wan, Chengkai; Yuan, Baozong; Miao, Zhenjiang] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Wan, CK (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM wanchengkai@gmail.com
CR ANKUR A, 2005, IEEE COMP SOC C COMP, V3, P72
   Boykov Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P26
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   CAILLETTE F, 2004, BMVC, V2, P596
   Cheung GKM, 2003, PROC CVPR IEEE, P375
   CHEUNG GKM, 2003, P IEEE COMP SOC C CO, V1
   CHU CW, 2003, IEEE COMP SOC C COMP, V2
   DEUTSCHER J, 1999, ICCV, V2, P1144
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   KEHL R, 2005, IEEE COMP SOC C COMP, V2, P129
   KINDERMAN R, 1980, M RANDOM FILEDS THEI
   Kohli P, 2005, IEEE I CONF COMP VIS, P922
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kumar MP, 2005, PROC CVPR IEEE, P18
   Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   NISKANEN M, 2005, P 16 BRIT MACH VIS C, V1, P439
   Press W. H., 1988, Numerical Recipes
   Saboune J, 2005, PROC INT C TOOLS ART, P621
   SUN Y, 2006, 18 IEEE INT C PATT R, V4, P49
   Sundaresan A, 2006, COMPUTER VISION FOR INTERACTIVE AND INTELLIGENT ENVIRONMENTS, P15
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
NR 22
TC 10
Z9 10
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2008
VL 24
IS 5
BP 373
EP 380
DI 10.1007/s00371-007-0195-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 279NZ
UT WOS:000254363200006
DA 2024-07-18
ER

PT J
AU Livny, Y
   Press, M
   El-Sana, J
AF Livny, Yotam
   Press, Michael
   El-Sana, Jihad
TI Interactive GPU-based adaptive cartoon-style rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE cartoon rendering; view-dependent rendering; hardware acceleration
AB In this paper we present a novel real-time cartoon-style rendering approach, which targets very large meshes. Cartoon drawing usually uses a limited number of colors for shading and emphasizes special effects, such as sharp curvature and silhouettes. It also paints the remaining large regions with uniform solid colors. Our approach quantizes light intensity to generate different shadow colors and utilizes multiresolution mesh hierarchy to maintain appropriate levels of detail across various regions of the mesh. To comply with visual requirements, our algorithm exploits graphics hardware programmability to draw smooth silhouette and color boundaries within the vertex and fragment processors. We have adopted a simplification scheme that executes simplification operators without incurring extra simplification operations as a precondition. The real-time refinement of the mesh, which is performed by the graphics processing unit (GPU), dramatically improves image quality and reduces CPU load.
C1 [Livny, Yotam; Press, Michael; El-Sana, Jihad] Ben Gurion Univ Negev, IL-84105 Beer Sheva, Israel.
C3 Ben Gurion University
RP Livny, Y (corresponding author), Ben Gurion Univ Negev, IL-84105 Beer Sheva, Israel.
EM livnyy@cs.bgu.ac.il; pressmi@cs.bgu.ac.il; el-sana@cs.bgu.ac.il
CR Barla P., 2006, Proceedings of International Symposium on Non-Photorealistic Animation and Rendering, P127, DOI [10.1145/1124728.1124749, DOI 10.1145/1124728.1124749]
   BUCHANAN JW, 2000, P 1 INT S NONPH AN R, P39
   Claes J., 2001, P IM VIS COMP NZ IVC, P13
   De Floriani L, 1998, VISUALIZATION '98, PROCEEDINGS, P43, DOI 10.1109/VISUAL.1998.745283
   El-Sana J., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P131, DOI 10.1109/VISUAL.1999.809877
   El-Sana J, 2000, COMPUT GRAPH FORUM, V19, pC139, DOI 10.1111/1467-8659.00406
   Gooch B., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P31, DOI 10.1145/300523.300526
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe Hugues., 1997, Proceedings of the 24rd Annual Conference on Computer Graphics and Interactive Techniques, V31, P189
   JOHNSON D, 2001, ACM S INTERACTIVE 3D, P129
   KIM J, 2001, P GRAPH INT 01 OTT O
   Lake A., 2000, Proceedings of the 1st International Symposium on Non-Photorealistic Animation and Rendering New York, NY, USA,, P13
   LUEBKE D, 1997, COMPUTER GRAPHICS, V31, P199
   MARKOSIAN L, 1997, P SIGGRAPH 97, P415
   Nienhaus M, 2003, WSCG'2003, VOL 11, NO 2, CONFERENCE PROCEEDINGS, P346
   Pajarola R, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P22, DOI 10.1109/PCCGA.2001.962854
   PRAUN E, 2001, P SIGGRAPH 2001, P579
   RAKAR R, 2001, P EUR WORKSH GRAPH H, P41
   RASKAR R, 1999, ACM S INTERACTIVE 3D, P135
   Rossignac J. R., 1992, P 7 EUR WORKSH COMP, P188, DOI [10. 2312/EGGH/EGGH92/188-203, DOI 10.2312/EGGH/EGGH92/188-203]
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Sander PV, 2000, COMP GRAPH, P327, DOI 10.1145/344779.344935
   Shamir A, 2000, IEEE VISUAL, P423, DOI 10.1109/VISUAL.2000.885724
   Webb Matthew., 2002, Proceedings of the 2nd international symposium on Non- photorealistic animation and rendering, NPAR '02, P53, DOI DOI 10.1145/508530.508540
   Xia JC, 1997, IEEE T VIS COMPUT GR, V3, P171, DOI 10.1109/2945.597799
NR 26
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 239
EP 247
DI 10.1007/s00371-007-0201-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100003
DA 2024-07-18
ER

PT J
AU Bergamasco, M
   Salsedo, F
   Fontana, M
   Tarri, F
   Avizzano, CA
   Frisoli, A
   Ruffaldi, E
   Marcheschi, S
AF Bergamasco, M.
   Salsedo, F.
   Fontana, M.
   Tarri, F.
   Avizzano, C. A.
   Frisoli, A.
   Ruffaldi, E.
   Marcheschi, S.
TI High performance haptic device for force rendering in textile
   exploration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2006 HAPTEX Workshop
CY NOV 21-23, 2006
CL Natl Commun Assoc, Helsinki, FINLAND
HO Natl Commun Assoc
DE haptic interfaces; textile modeling; visual/tactile integration
ID MODELS
AB The HAPTEX system aims to develop a new multi-sensory environment for the immersive exploration of textiles.
   HAPTEX is based on a multi-layer/multi-thread architecture that optimizes the computational speed and integrates three different types of sensory feedback: vision, tactile and haptic.
   Such kind of environment is suitable for demanding VR applications such as the online marketing of novel textiles or garments, however it requires the design of a high performance multi-point haptic interface. The present work focuses on the haptic device design and describes how demanding requirements can be met by integrating on a high performance device a force sensor to achieve closed loop control.
   The methodology for the dimensioning of a motion based explicit force control on the basis of the dynamic parameters will be discussed and the specific implementation for the HAPTEX system presented.
C1 Scuola Super Sant Anna, PERCRO, Pontedera, Italy.
C3 Scuola Superiore Sant'Anna
RP Bergamasco, M (corresponding author), Scuola Super Sant Anna, PERCRO, Pontedera, Italy.
EM m.bergamasco@sssup.it
RI Fontana, Marco/L-8467-2015; Ruffaldi, Emanuele/A-2352-2009
OI Fontana, Marco/0000-0002-5691-8115; Ruffaldi,
   Emanuele/0000-0001-6084-6938; AVIZZANO, Carlo
   Alberto/0000-0001-5802-541X; BERGAMASCO, Massimo/0000-0002-7418-2332
CR Adams RJ, 2002, IEEE T CONTR SYST T, V10, P3, DOI 10.1109/87.974333
   [Anonymous], P IEEE RSJ INT C INT
   Astley OR, 1998, IEEE INT CONF ROBOT, P989, DOI 10.1109/ROBOT.1998.677216
   AVIZZANO CA, 2003, P 2003 IEEE INT WORK
   BRIDSON R, 2003, P 2003 ACM SIGGRAPH, P28
   De S, 1999, STUD HEALTH TECHNOL, V62, P94
   Eppinger S. D., 1987, Proceedings of the 1987 IEEE International Conference on Robotics and Automation (Cat. No.87CH2413-3), P904
   Fagergren A, 2000, IEEE T BIO-MED ENG, V47, P1366, DOI 10.1109/10.871410
   Frank AO, 2001, P IEEE VIRT REAL ANN, P257, DOI 10.1109/VR.2001.913794
   GOVINDARAJ M, 2003, P EUR C
   HAJIAN AZ, 1994, P ASME INT MECH ENG, V55, P319
   Hauth M, 2001, COMPUT GRAPH FORUM, V20, pC319, DOI 10.1111/1467-8659.00524
   HUI CL, 2004, TEXTILE RES J    MAY
   Kawabata S., 1980, STANDARDIZATION ANAL, V2nd
   Salsedo F., 2005, P HAPT 2005 WORKSH H, P1
   Ueberle M, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P58, DOI 10.1109/HAPTIC.2004.1287178
   Wen K, 2004, 3RD IEEE INTERNATIONAL WORKSHOP ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND THEIR APPLICATIONS - HAVE 2004, P147, DOI 10.1109/HAVE.2004.1391897
   YOSHIKAWA T, 2003, P 2003 IEEE RSJ INT, V4, P3094
NR 18
TC 8
Z9 10
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2007
VL 23
IS 4
BP 247
EP 256
DI 10.1007/s00371-007-0103-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 172ZW
UT WOS:000246844300003
DA 2024-07-18
ER

PT J
AU Gutiérrez, M
   García-Rojas, A
   Thalmann, D
   Vexo, F
   Moccozet, L
   Magnenat-Thalmann, N
   Mortara, M
   Spagnuolo, M
AF Gutierrez A., Mario
   Garcia-Rojas, Alejandra
   Thalmann, Daniel
   Vexo, Frederic
   Moccozet, Laurent
   Magnenat-Thalmann, Nadia
   Mortara, Michela
   Spagnuolo, Michela
TI An ontology of virtual humans -: Incorporating semantics into human
   shapes
SO VISUAL COMPUTER
LA English
DT Article
DE human shape reconstruction and synthesis; semantic annotations;
   ontologies
ID PARAMETERIZATION
AB Most of the efforts concerning graphical representations of humans (Virtual Humans) have been focused on synthesizing geometry for static or animated shapes. The next step is to consider a human body not only as a 3D shape, but as an active semantic entity with features, functionalities, interaction skills, etc. We are currently working on an ontology-based approach to make Virtual Humans more active and understandable both for humans and machines. The ontology for Virtual Humans we are defining will provide the "semantic layer" required to reconstruct, stock, retrieve, reuse and share content and knowledge related to Virtual Humans.
C1 Ecole Polytech Fed Lausanne, VRlab, Lausanne, Switzerland.
   Univ Geneva, MIRALab, CH-1211 Geneva 4, Switzerland.
   CNR, IMATI, I-00185 Rome, Italy.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; University of Geneva; Consiglio Nazionale delle
   Ricerche (CNR); Istituto di Matematica Applicata e Tecnologie
   Informatiche "Enrico Magenes" (IMATI-CNR)
EM m.gutierrez@itesm.mx; alejandra.garciarojas@epfl.ch;
   daniel.thalmann@epfl.ch; frederic.vexo@epfl.ch;
   moccozet@miralab.unige.ch; thalmann@miralab.unige.ch;
   michela.mortara@ge.imati.cnr.it; spagnuolo@ge.imati.cnr.it
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008; Spagnuolo,
   Michela/ABA-1927-2021; Thalmann, Nadia/AAK-5195-2021; MOCCOZET,
   Laurent/H-6472-2019; Mortara, Michela/B-4262-2015; Spagnuolo,
   Michela/F-5068-2013
OI Thalmann, Daniel/0000-0002-0451-7491; Spagnuolo,
   Michela/0000-0002-5682-6990; Thalmann, Nadia/0000-0002-1459-5960;
   MOCCOZET, Laurent/0000-0003-0333-1932; Spagnuolo,
   Michela/0000-0002-5682-6990
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B., 2004, P C DIG HUM MOD DES
   [Anonymous], P CASA
   BOOKSTEIN FL, 1997, MORPHMETRIC TOOLS LA
   Cordier F, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P257, DOI 10.1109/PCCGA.2004.1348356
   Cordier F, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1159612
   Daanen HAM, 1998, DISPLAYS, V19, P111, DOI 10.1016/S0141-9382(98)00034-1
   GRUBER TR, 1991, PRINCIPLES OF KNOWLEDGE REPRESENTATION AND REASONING, P601
   Gruninger M., 1995, Methodology for the design and evaluation of ontologies
   GUARINO F, 1998, P FOIS 98 TRENT IT J, P3
   Gutiérrez M, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P277, DOI 10.1109/MMMC.2005.65
   Gutierrez M, 2005, INT J COMPUT APPL T, V23, P229, DOI 10.1504/IJCAT.2005.006484
   *H ANIM WORK GROUP, 19774200X ISO IEC FC
   KAHLER K, 2002, SCA 02, P55
   Kallmann M., 1999, VRST'99. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, P124, DOI 10.1145/323663.323683
   MOCCOZET L, 2004, P WORKSH MOD MOT CAP, P73
   Mortara M, 2004, ALGORITHMICA, V38, P227, DOI 10.1007/s00453-003-1051-4
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Shah JJ, 1995, Parametric and feature-based CAD/CAM: concepts, techniques, and applications
   THALMANN D, 2000, AVI 00, P14
   Uschold M, 1996, KNOWL ENG REV, V11, P93, DOI 10.1017/S0269888900007797
   Wang CCL, 2005, COMPUT AIDED DESIGN, V37, P83, DOI 10.1016/j.cad.2004.05.001
   Wang Z, 2003, CHINESE J ASTRON AST, V3, P241, DOI 10.1088/1009-9271/3/3/241
NR 23
TC 35
Z9 39
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2007
VL 23
IS 3
BP 207
EP 218
DI 10.1007/s00371-006-0093-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 134PM
UT WOS:000244095300004
OA Green Published
DA 2024-07-18
ER

PT J
AU Claustres, L
   Paulin, M
   Boucher, Y
AF Claustres, L
   Paulin, M
   Boucher, Y
TI A wavelet-based framework for acquired radiometric quantity
   representation and accurate physical rendering
SO VISUAL COMPUTER
LA English
DT Article
DE global illumination; wavelets; spectral rendering
AB In this paper, we present a framework based on a generic representation, which is able to handle most of the radiometric quantities required by global illumination software. A sparse representation in the wavelet space is built using the separation between the directional and the wavelength dependencies of such radiometric quantities. Particularly, we show how to use this representation for spectral power distribution, spectral reflectance and phase function measurements modeling. Then, we explain how the representation is useful for performing spectral rendering. On the one hand, it speeds up spectral path tracing by importance sampling to generate reflected directions and by avoiding expensive computations usually done on-the-fly. On the other hand, it allows efficient spectral photon mapping, both in terms of memory and speed. We also show how complex light emission from real luminaires can be efficiently sampled to emit photons with our numerical model.
C1 Univ Toulouse 3, IRIT, F-31062 Toulouse 4, France.
   Off Natl Etud & Rech Aerosp, CERT, F-31055 Toulouse 4, France.
C3 Universite de Toulouse; Universite Toulouse III - Paul Sabatier;
   National Office for Aerospace Studies & Research (ONERA)
RP Univ Toulouse 3, IRIT, 118 Route Narbonne, F-31062 Toulouse 4, France.
EM claustre@irit.fr; paulin@irit.fr; boucher@onecert.fr
CR [Anonymous], COMPUT GRAPH
   [Anonymous], 2000, The C++ Programming Language
   BEYLKIN G, 1991, COMMUN PUR APPL MATH, V44, P141, DOI 10.1002/cpa.3160440202
   Blasi P., 1993, Computer Graphics Forum, V12, pC201, DOI 10.1111/1467-8659.1230201
   Bohren C.F., 2008, Absorption and Scattering of Light by Small Particles
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Claustres L, 2003, COMPUT GRAPH FORUM, V22, P701, DOI 10.1111/j.1467-8659..00718.x
   Cohen Michael F., 1993, Radiosity and realistic image synthesis
   Deville PM, 1995, SPRING COMP SCI, P147
   DEVLIN K, 2002, CSTR02005 U BRIST DE
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Geist R, 1996, COLOR RES APPL, V21, P121, DOI 10.1002/(SICI)1520-6378(199604)21:2<121::AID-COL6>3.0.CO;2-W
   Glassner A. S., 1995, Principles of Digital Image Synthesis
   Hall R., 1989, ILLUMINATION COLOR C
   Iehl JC, 2000, COMPUT GRAPH FORUM, V19, pC291, DOI 10.1111/1467-8659.00421
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kaewpijit S, 2003, IEEE T GEOSCI REMOTE, V41, P863, DOI 10.1109/TGRS.2003.810712
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kautz J, 1999, SPRING EUROGRAP, P247
   KLASSEN RV, 1987, ACM T GRAPHIC, V6, P215, DOI 10.1145/35068.35071
   KOENDERINK J, 1996, EUR C COMP VIS, P28
   Lalonde P, 1997, IEEE T VIS COMPUT GR, V3, P329, DOI 10.1109/2945.646236
   Latta L, 2002, ACM T GRAPHIC, V21, P509, DOI 10.1145/566570.566610
   Lewis RR, 2000, COMPUT GRAPH FORUM, V19, P135, DOI 10.1111/1467-8659.00450
   LIU X, 2004, P EUR S REND, P337
   Liu XG, 2004, IEEE T VIS COMPUT GR, V10, P278, DOI 10.1109/TVCG.2004.1272727
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Mie G, 1908, ANN PHYS-BERLIN, V25, P377, DOI 10.1002/andp.19083300302
   Nicodemus F. E., 1977, MONOGRAPH NBS, V160
   Nishita Tomoyuki., 1987, COMPUTER GRAPHICS P, V21, P303, DOI [10.1145/37401.37437, DOI 10.1145/37401.37437]
   PAUL JC, 1995, J VISUAL COMP ANIMAT, V6, P231, DOI 10.1002/vis.4340060406
   Rao Raghuveer M., 1998, Wavelet Transform: Introduction to theory and application
   RASO MG, 1991, GRAPH INTER, P40
   REVESZ G, 1988, LAMBDA CALCULUS COMB
   Richards J., 1993, Remote sensing digital image analysis: an introduction, V2nd
   Rougeron G., 1997, Rendering Techniques '97. Proceedings of the Eurographics Workshop. Eurographics, P127
   Scott D. W., 1992, CURSE DIMENSIONALITY
   Serrot G, 1998, P SOC PHOTO-OPT INS, V3494, P34, DOI 10.1117/12.332431
   SILLION FX, 1991, COMP GRAPH, V25, P187, DOI 10.1145/127719.122739
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Spanier J., 1969, Monte Carlo Principles and Neutron Transport Problems
   SUN Y, 1998, 199818 SFU CMPT TR
   Sun YL, 2001, VISUAL COMPUT, V17, P429, DOI 10.1007/s003710100116
   Suykens F, 2003, COMPUT GRAPH FORUM, V22, P463, DOI 10.1111/1467-8659.00694
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Ten Daubechies I., 1992, lecture on wavelets
   Veach E., 1998, ROBUST MONTE CARLO M
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Wyszecki G., 1982, Color science: Concepts and methods, quantitative data and formulae
   Xu HYZ, 2004, PROC SPIE, V5299, P22, DOI 10.1117/12.527295
   Zeghers E, 1997, VISUAL COMPUT, V13, P424, DOI 10.1007/s003710050115
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 76
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2006
VL 22
IS 4
BP 221
EP 237
DI 10.1007/s00371-006-0001-y
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 032DG
UT WOS:000236753500001
DA 2024-07-18
ER

PT J
AU Rossignac, J
AF Rossignac, J
TI Shape complexity
SO VISUAL COMPUTER
LA English
DT Article
DE compression; simplification; morphology; geometry
ID BOUNDARY EVALUATION; BOOLEAN OPERATIONS; CSG
AB The complexity of 3D shapes that are represented in digital form and processed in CAD/CAM/CAE, entertainment, biomedical, and other applications has increased considerably. Much research was focused on coping with or on reducing shape complexity. However, what exactly is shape complexity? We discuss several complexity measures and the corresponding complexity reduction techniques. Algebraic complexity measures the degree of polynomials needed to represent the shape exactly in its implicit or parametric form. Topological complexity measures the number of handles and components or the existence of non-manifold singularities, non-regularized components, holes or self-intersections. Morphological complexity measures smoothness and feature size. Combinatorial complexity measures the vertex count in polygonal meshes. Representational complexity measures the footprint and ease-of-use of a data structure, or the storage size of a compressed model. The latter three vary as a function of accuracy.
C1 Georgia Inst Technol, Coll Comp, GVU Ctr, Atlanta, GA 30332 USA.
   Georgia Inst Technol, IRIS Cluster, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Georgia Inst Technol, Coll Comp, GVU Ctr, Atlanta, GA 30332 USA.
CR Alexandroff P.., 1961, Elementary concepts of Topology. A translation of (Alexandroff
   ANDUJAR C, 2004, J COMPUT AIDED DES A, V1, P503
   [Anonymous], 1988, An introduction to solid modeling
   Attene M, 2003, ACM T GRAPHIC, V22, P982, DOI 10.1145/944020.944022
   Banerjee R, 1996, COMPUT GRAPH FORUM, V15, P205, DOI 10.1111/1467-8659.1540205
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   BROWN CM, 1978, ACM CSC ER, P770
   BRUN JM, 1985, P CAM IS 3 GEOM MOD
   CAMERON S, 1988, P THEOR PRACT GEOM M, P369
   CIGNONI P, 1998, P EUROGRAPHICS, V17, P167
   COORS V, 2004, VISUAL COMPUT, V20, P1
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Farin G., 2002, HDB COMPUTER AIDED G
   Garland M., 1999, QSLIM 2 0 COMPUTER S
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Goodman J., 2004, Handbook of Discrete and Computational Geometry
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KING D, 1999, 11 CAN C COMP GEOM, P146
   KORMOS JG, 1985, CAM IS 3 GEOM MOD SE
   KRIEZIS GA, 1992, COMPUT AIDED DESIGN, V24, P41, DOI 10.1016/0010-4485(92)90090-W
   LAZARD S, 2004, P 20 ANN ACM S COMP, P419
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Liepa P., 2003, Symposium on Geometry Processing, P200
   LLAMAS I, 2000, P ACM SIGGRAPH, V22, P663
   LOOP C, 1994, COMPUTER GRAPHICS, V28, P303
   LUEBKE D, 2002, LEVELS DETAIL 3D GRA
   MANTYLA M, 1986, ACM T GRAPHIC, V5, P1, DOI 10.1145/7529.7530
   MILLER JR, 1995, GRAPH MODEL IM PROC, V57, P55, DOI 10.1006/gmip.1995.1006
   MURALI TM, 1997, S INT 3D GRAPH, P155
   OKINO N, 1973, DRAWING MANUFACTURIN, P141
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   REQUICHA AAG, 1985, P IEEE, V73, P30, DOI 10.1109/PROC.1985.13108
   Requicha AAG, 1980, ACM COMPUT SURV, V12, P437
   RONFARD R, 1996, P EUR 96 COMP GRAPH, V15, P67
   Rossignac J, 2004, COMPUT AIDED DESIGN, V36, P1461, DOI 10.1016/j.cad.2003.10.008
   ROSSIGNAC J, 2004, HDB DISCRETE COMPUTA
   Rossignac J., 1999, P 5 ACM S SOL MOD AP, P31
   Rossignac J.R., 1989, GEOMETRIC MODELING P, P145
   ROSSIGNAC J.R., 1993, Geometric Modeling in Computer Graphics
   ROSSIGNAC JR, 1991, COMPUT AIDED DESIGN, V23, P21, DOI 10.1016/0010-4485(91)90078-B
   ROSSIGNAC JR, 1989, ACM T GRAPHIC, V8, P51, DOI 10.1145/49155.51123
   van Emmerik M., 1993, Visual Computer, V9, P239, DOI 10.1007/BF01908447
NR 45
TC 25
Z9 29
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2005
VL 21
IS 12
BP 985
EP 996
DI 10.1007/s00371-005-0362-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 996AB
UT WOS:000234145700005
DA 2024-07-18
ER

PT J
AU Hyun, DE
   Yoon, SH
   Chang, JW
   Seong, JK
   Kim, MS
   Jüttler, B
AF Hyun, DE
   Yoon, SH
   Chang, JW
   Seong, JK
   Kim, MS
   Jüttler, B
TI Sweep-based human deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE human modeling and deformation; sweep surface; vertex binding; shape
   blending; anatomical features; volume preservation
AB We present a sweep-based approach to human body modeling and deformation. A rigid 3D human model, given as a polygonal mesh, is approximated with control sweep surfaces. The vertices on the mesh are bound to nearby sweep surfaces and then follow the deformation of the sweep surfaces as the model bends and twists its arms, legs, spine and neck. Anatomical features including bone-protrusion, muscle-bulge, and skin-folding are supported by a GPU-based collision detection procedure. The volumes of arms, legs, and torso are kept constant by a simple control using a volume integral formula for sweep surfaces. We demonstrate the effectiveness of this sweep-based human deformation in several test animation clips.
C1 Samsung Elect, Suwon, South Korea.
   Seoul Natl Univ, Sch Engn & Comp Sci, Seoul, South Korea.
   Johannes Kepler Univ Linz, Inst Appl Geometry, A-4040 Linz, Austria.
C3 Samsung Electronics; Samsung; Seoul National University (SNU); Johannes
   Kepler University Linz
RP Samsung Elect, Suwon, South Korea.
EM danny.hyun@samsung.com
OI Juttler, Bert/0000-0002-5518-7795
CR *AL WAV TECHN, 2003, MAYA 5 0 US MAN
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   [Anonymous], 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'02
   [Anonymous], P ACM SIGGRAPH EUROG
   [Anonymous], 2002, P 2002 ACM SIGGRAPHE
   Aubel A, 2001, COMP ANIM CONF PROC, P167, DOI 10.1109/CA.2001.982390
   Botsch M, 2003, COMPUT GRAPH FORUM, V22, P483, DOI 10.1111/1467-8659.00696
   Chadwick J. E., 1989, Computer Graphics, V23, P243, DOI 10.1145/74334.74358
   Foley J.D., 1990, Computer graphics: Principles and practice
   Hilton A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P246, DOI 10.1109/TDPVT.2002.1024069
   Hyun DE, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P204
   KARLA P, 1998, IEEE COMPUT GRAPH, P42
   *KAYD INC, 2001, FILMBOX REF GUID
   Mohr A, 2003, ACM T GRAPHIC, V22, P562, DOI 10.1145/882262.882308
   SCHEEPERS F, 1997, P 24 ANN C COMP GRAP, P163, DOI DOI 10.1145/258734.258827
   Seo H, 2004, GRAPH MODELS, V66, P1, DOI 10.1016/j.gmod.2003.07.004
   Sheynin S. A., 2003, Pattern Recognition and Image Analysis, V13, P174
   Singh K, 2000, PROC GRAPH INTERF, P35
   Starck J, 2003, MACH VISION APPL, V14, P248, DOI 10.1007/s00138-002-0080-3
   WILHELMS J, 1997, P SIGGRAPH 97, P173
NR 21
TC 44
Z9 50
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 542
EP 550
DI 10.1007/s00371-005-0343-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400006
DA 2024-07-18
ER

PT J
AU Lum, EB
   Ma, KL
AF Lum, EB
   Ma, KL
TI Expressive line selection by example
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE non-photorealistic rendering; silhouettes; example-based rendering;
   machine learning
AB An important problem in computer generated line drawing is determining which set of lines produces a representation that is in agreement with a user's communication goals. We describe a method that enables a user to intuitively specify which types of lines should appear in rendered images. Our method employs conventional silhouette-edge and other feature-line extraction algorithms to derive a set of candidate lines, and integrates machine learning into a user-directed line removal process using a sketching metaphor. The method features a simple and intuitive user interface that provides interactive control over the resulting line selection criteria and can be easily adapted to work in conjunction with existing line detection and rendering algorithms. Much of the method's power comes from its ability to learn the relationships between numerous geometric attributes that define a line style. Once learned, a user's style and intent can be passed from object to object as well as from view to view.
C1 Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Univ Calif Davis, Dept Comp Sci, 1 Shields Av, Davis, CA 95616 USA.
EM lume@cs.ucdavis.edu
CR [Anonymous], 2005, NEURAL NETWORKS PATT
   BUCHANAN J, 2000, P 1 INT S NONPH AN R
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chen H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P433, DOI 10.1109/ICCV.2001.937657
   Chen Hong., 2004, P 3 INT S NONPHOTORE, P95, DOI DOI 10.1145/987
   CURTIS C, 1998, SIGGRAPH 98 C ABSTR, P317, DOI DOI 10.1145/281388.281913
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   GOOCH B, 1999, 1999 ACM S INT 3D GR, P31
   GRABLI S, 2004, RENDERING TECHNIQUES
   Hertz J., 1991, LECT NOTES SANTA FE
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   HERTZMANN A, 1999, SIGGRAPH 99 COURSE N
   Isenberg T, 2002, COMPUT GRAPH FORUM, V21, P249, DOI 10.1111/1467-8659.00584
   Isenberg T, 2003, IEEE COMPUT GRAPH, V23, P28, DOI 10.1109/MCG.2003.1210862
   Jodoin Pierre-Marc., 2002, PROC NPAR, P29
   Kalnins RD, 2002, ACM T GRAPHIC, V21, P755, DOI 10.1145/566570.566648
   Markosian L., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P415, DOI 10.1145/258734.258894
   Northrup J. D., 2000, Proceedings of the 1st International Symposium on Non-photorealistic Animation and Rendering, P31, DOI DOI 10.1145/340916.340920
   Pasztor E, 1999, TR9911 MERL
   SAITO T, 2000, P SIGGRAPH 1990 C, P197
   SOUSA M, 2003, COMPUTER GRAPHICS, V22
   Werbos P. J., 1974, REGRESSION NEW TOOLS
NR 23
TC 6
Z9 8
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 811
EP 820
DI 10.1007/s00371-005-0342-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400034
DA 2024-07-18
ER

PT J
AU Hauth, M
   Etzmuss, O
   Strasser, W
AF Hauth, M
   Etzmuss, O
   Strasser, W
TI Analysis of numerical methods for the simulation of deformable models
SO VISUAL COMPUTER
LA English
DT Article
DE deformable objects; cloth modeling; numerical integration methods;
   differential equations
AB Simulating deformable objects based on physical laws has become the most popular technique for modeling textiles, skin, or volumetric soft objects like human tissue. The physical model leads to an ordinary differential equation. Recently, several approaches to fast algorithms have been proposed.
   In this work, more profound numerical background about numerical stiffness is provided. Stiff equations impose stability restrictions on a numerical integrator. Some one-step and multistep methods with adequate stability properties are presented. For an efficient implementation, the inexact Newton method is discussed. Applications to 2D and 3D elasticity problems show that the discussed methods are faster and give higher-quality solutions than the commonly used linearized Euler method.
C1 Univ Tubingen, WSI, GRIS, Tubingen, Germany.
C3 Eberhard Karls University of Tubingen
RP Univ Tubingen, WSI, GRIS, Tubingen, Germany.
EM hauth@gris.uni-tuebingen.de; etzmuss@gris.uni-tuebingen.de;
   strasser@gris.uni-tuebingen.de
CR Atanackovic T.M., 2000, Theory of Elasticity for Scientists and Engineers
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bathe K. J., 1982, FINITE ELEMENT METHO
   Breen David E., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques, P365
   Debunne G, 2001, COMP GRAPH, P31, DOI 10.1145/383259.383262
   Debunne G, 1999, SPRING COMP SCI, P133
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   Eberhardt B, 2000, SPRING COMP SCI, P137
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   ETZMUSS O, 2001, IN PRESS T VIS COMPU
   Freund RW, 1994, NUMER LINEAR ALGEBR, V1, P403, DOI 10.1002/nla.1680010406
   Hairer E, 1999, J COMPUT APPL MATH, V111, P93, DOI 10.1016/S0377-0427(99)00134-X
   HARIER E, 1996, SOLVING ORDINARY DIF, V2
   HAUTH M, 2001, P EUR MANCH UK SEPT, P137
   HOUSE DH, 2000, CLOTH MODELING ANIMA
   James DL, 1999, COMP GRAPH, P65, DOI 10.1145/311535.311542
   KING YM, 2000, P WSCG 2000 PILS CZE, P322
   Lin M.C., 1998, PROC IMA C MATH SURF, P37
   MEZGER J, 2002, WSI20025 U TUB
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   PICINBONO G, 2001, P IEEE INT C ROB AUT
   PLATT JC, 1988, P SIGGRAPH 88, P279
   Press W. H., 1988, Numerical Recipes
   PROVOT X, 1995, GRAPH INTER, P147
   RHEINBOLDT WC, 1998, CBMS-NSF MA, V70, P1
   Saad Y., 1996, Iterative Methods for Sparse Linear Systems
   Terzopoulos D., 1988, Visual Computer, V4, P306, DOI 10.1007/BF01908877
   Volino P, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P265, DOI 10.1109/CGI.2001.934683
   Volino P, 2000, COMP ANIM CONF PROC, P154, DOI 10.1109/CA.2000.889073
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   VOLINO P, 1994, COMPUT GRAPH FORUM, V13, pC155, DOI 10.1111/1467-8659.1330155
NR 31
TC 35
Z9 43
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 581
EP 600
DI 10.1007/s00371-003-0206-2
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600013
DA 2024-07-18
ER

PT J
AU Zhao, F
   Zhang, ZW
   Li, HN
   Wen, ZQ
   Qu, FY
AF Zhao, Fan
   Zhang, Zhiwei
   Li, Haining
   Wen, Zhiquan
   Qu, Fangying
TI A pseudo-color image-based cylindrical object surface text detection
   method
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Text segmentation; 3D point clouds; Pseudo-color image; U-Net network;
   Text on the surface of cylindrical-shape objects
ID R-CNN
AB With the development of deep learning theory and computer vision technology, text detection has been widely used in automatic navigation, product recognition and language translation. However, the results of the existing text detection methods on the surface of cylindrical-shape objects are not ideal; the main reason for their poor performance is that these methods only use the color, texture and deep learning feature and do not make full use of the spatial differences of text instances. In order to solve the problem of poor text detection on cylindrical-shape object such as bottles and jars, a novel method for text detection on the curved surface of cylindrical-shape objects based on 3D point cloud is proposed in this paper. Firstly, the captured multi-view images of cylinder-shaped objects such as bottles and jars are used to reconstruct the 3D point cloud. Secondly, the 2D grid of the point cloud is established by using the graph rendering technology, which is followed by the 2D grid pixels are filled with XYZ coordinates, RGB and SWT features to generate a pseudo-color image. Finally, the U-Net network is used to segment the generated pseudo-color image, and the detection result of the text instance is obtained after the post-processing. For verifying the effectiveness of this proposed method, extensive experiments are carried out on the 3D text point cloud dataset established by us. The Precision (P), Recall (R) and F-score average of our method are 87.1%, 79.4% and 83.1%, respectively. Compared with the classical algorithms such as PSENet and CRAFT, the accuracy is increased by 11.2% and 7.0%, respectively, and the average of F-score is increased by 2.7% and 2.2%, respectively. And compared to color images, the F-score was improved by 4.4% using 3D information, which further demonstrates the effectiveness of 3D position detection for text instances on bottles and jars. The ablation experiment also demonstrated the applicability of the generated pseudo-color images based on 3D information on different models. Thus, the experimental results show that the proposed method can accurately detect the text on the surface of cylindrical objects such as bottles and jars and can be well applied to the text recognition of pharmacies, supermarkets, and daily bottle and jar products.
C1 [Zhao, Fan; Zhang, Zhiwei; Li, Haining; Wen, Zhiquan] Xian Univ Technol, Dept Informat Sci, Xian 710048, Peoples R China.
   [Qu, Fangying] Georgia Inst Technol, Dept Comp Sci, Atlanta, GA 30322 USA.
C3 Xi'an University of Technology; University System of Georgia; Georgia
   Institute of Technology
RP Zhao, F (corresponding author), Xian Univ Technol, Dept Informat Sci, Xian 710048, Peoples R China.
EM vcu@xaut.edu.cn
OI Zhao, Fan/0000-0001-6672-7948
FU Key Research and Development Projects of Shaanxi Province
FX No Statement Available
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Busta M, 2019, LECT NOTES COMPUT SC, V11367, P127, DOI 10.1007/978-3-030-21074-8_11
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Cong Y, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, P1703, DOI 10.1109/ICMA.2016.7558820
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng JJ, 2021, IEEE T CIRC SYST VID, V31, P4722, DOI 10.1109/TCSVT.2021.3100848
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Han X, 2024, VISUAL COMPUT, V40, P2641, DOI 10.1007/s00371-023-02963-2
   Harizi R, 2022, MULTIMED TOOLS APPL, V81, P24691, DOI 10.1007/s11042-022-11998-x
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Liu H, 2022, VISUAL COMPUT, V38, P3231, DOI 10.1007/s00371-022-02570-7
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu ZD, 2019, MULTIMED TOOLS APPL, V78, P18205, DOI 10.1007/s11042-019-7177-4
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Chu PM, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS (MFI), P656, DOI 10.1109/MFI.2017.8170397
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Raisi Z, 2022, INT C PATT RECOG, P3238, DOI 10.1109/ICPR56361.2022.9956488
   Raisi Z, 2021, IEEE COMPUT SOC CONF, P3156, DOI 10.1109/CVPRW53098.2021.00353
   Roy AM, 2023, Arxiv, DOI arXiv:2303.04275
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shinde Anuja, 2021, Proceedings of the 2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS), P961, DOI 10.1109/ICAIS50930.2021.9395776
   Singh A, 2023, DRONES-BASEL, V7, DOI 10.3390/drones7020081
   Wang BX, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14092110
   Wang P., 2022, Lecture Notes in Computer Science, V13688, DOI [10.1007/978-3-031-19815, DOI 10.1007/978-3-031-19815]
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wu H, 2017, VISUAL COMPUT, V33, P113, DOI 10.1007/s00371-015-1156-1
   Wu X, 2021, PROCEEDINGS OF THE 2021 IEEE INTERNATIONAL CONFERENCE ON PROGRESS IN INFORMATICS AND COMPUTING (PIC), P102, DOI 10.1109/PIC53636.2021.9687065
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zeng C, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION ENGINEERING (ICRAE), P389, DOI 10.1109/ICRAE.2017.8291416
   Zhang Y, 2021, MULTIMED TOOLS APPL, V80, P29005, DOI 10.1007/s11042-021-11101-w
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu C, 2024, VISUAL COMPUT, V40, P3023, DOI 10.1007/s00371-023-03005-7
NR 44
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 29
PY 2023
DI 10.1007/s00371-023-03190-5
EA DEC 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM5K2
UT WOS:001132471200003
DA 2024-07-18
ER

PT J
AU Huang, GY
   Wen, Y
   Qian, B
   Bi, L
   Chen, TL
   Sheng, B
AF Huang, Gengyou
   Wen, Yang
   Qian, Bo
   Bi, Lei
   Chen, Tingli
   Sheng, Bin
TI Attention-based multi-scale feature fusion network for myopia grading
   using optical coherence tomography images
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Optical coherence tomography (OCT); Myopia grading; Deep Learning;
   Vision Transformer; Attention fusion
ID OCT
AB Myopia is a serious threat to eye health and can even cause blindness. It is important to grade myopia and carry out targeted intervention. Nowadays, various studies using deep learning models based on optical coherence tomography (OCT) images to screen for high myopia. However, since regions of interest (ROIs) of pre-myopia and low myopia on OCT images are relatively small, it is rather difficult to use OCT images to conduct detailed myopia grading. There are few studies using OCT images for more detailed myopia grading. To address these problems, we propose a novel attention-based multi-scale feature fusion network named AMFF for myopia grading using OCT images. The proposed AMFF mainly consists of five modules: a pre-trained vision transformer (ViT) module, a multi-scale convolutional module, an attention feature fusion module, an Avg-TopK pooling module and a fully connected (FC) classifier. Firstly, unsupervised pre-training of ViT on the training set can better extract feature maps. Secondly, multi-scale convolutional layers further extract multi-scale feature maps to obtain more receptive fields and extract scale-invariant features. Thirdly, feature maps of different scales are fused through channel attention and spatial attention to further obtain more meaningful features. Lastly, the most prominent features are obtained by the weighted average of the highest activation values of each channel, and then they are used to classify myopia through a fully connected layer. Extensive experiments show that our proposed model has the superior performance compared with other state-of-the-art myopia grading models.
C1 [Huang, Gengyou; Qian, Bo; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comupter Sci & Engn, Shanghai, Peoples R China.
   [Wen, Yang] Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen, Peoples R China.
   [Bi, Lei] Shanghai Jiao Tong Univ, Inst Translat Med, Shanghai, Peoples R China.
   [Chen, Tingli] Huadong Sanat, Wuxi, Jiangsu, Peoples R China.
C3 Shanghai Jiao Tong University; Shenzhen University; Shanghai Jiao Tong
   University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comupter Sci & Engn, Shanghai, Peoples R China.
EM huanggengyou@sjtu.edu.cn; wen_yang@szu.edu.cn; shengbin@sjtu.edu.cn
RI Chen, Ting-Li/AAF-5752-2020
FU National Science Foundation of China
FX No Statement Available
CR Baba T, 2002, ACTA OPHTHALMOL SCAN, V80, P82, DOI 10.1034/j.1600-0420.2002.800116.x
   Baird PN, 2020, NAT REV DIS PRIMERS, V6, DOI 10.1038/s41572-020-00231-4
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Bullimore MA, 2019, OPTOMETRY VISION SCI, V96, P463, DOI 10.1097/OPX.0000000000001367
   Byer NE, 2002, EYE, V16, P359, DOI 10.1038/sj.eye.6700191
   Cai LL, 2022, IEEE IMAGE PROC, P696, DOI 10.1109/ICIP46576.2022.9897966
   Chandrasekaran R, 2023, VISUAL COMPUT, V39, P2741, DOI 10.1007/s00371-022-02489-z
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   De Fauw J, 2018, NAT MED, V24, P1342, DOI 10.1038/s41591-018-0107-6
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Faghihi Hooshang, 2010, J Ophthalmic Vis Res, V5, P110
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng Y., 2019, 2019 IEEE INT TRANSP, P1
   Flaxman SR, 2017, LANCET GLOB HEALTH, V5, pE1221, DOI 10.1016/S2214-109X(17)30393-5
   Flitcroft DI, 2019, INVEST OPHTH VIS SCI, V60, pM20, DOI 10.1167/iovs.18-25957
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Han XT, 2022, EYE, V36, P921, DOI 10.1038/s41433-021-01805-6
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XG, 2021, ACTA OPHTHALMOL, V99, pE489, DOI 10.1111/aos.14617
   He XG, 2019, CLIN EXP OPHTHALMOL, V47, P171, DOI 10.1111/ceo.13391
   He XX, 2020, NEUROCOMPUTING, V405, P37, DOI 10.1016/j.neucom.2020.04.044
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Imran A, 2021, VISUAL COMPUT, V37, P2407, DOI 10.1007/s00371-020-01994-3
   Jaffe GJ, 2004, AM J OPHTHALMOL, V137, P156, DOI 10.1016/S0002-9394(03)00792-X
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Lee CS, 2017, OPHTHALMOL RETINA, V1, P322, DOI 10.1016/j.oret.2016.12.009
   Leveziel N, 2013, AM J OPHTHALMOL, V155, P913, DOI 10.1016/j.ajo.2012.11.021
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, DOI 10.48550/ARXIV.1805.10180]
   Li SH, 2019, LECT NOTES COMPUT SC, V11767, P531, DOI 10.1007/978-3-030-32251-9_58
   Liefers B, 2021, AM J OPHTHALMOL, V226, P1, DOI 10.1016/j.ajo.2020.12.034
   Lin HT, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002674
   Lin SQ, 2023, VISUAL COMPUT, V39, P3259, DOI 10.1007/s00371-023-02945-4
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu W, 2018, TRANSL VIS SCI TECHN, V7, DOI 10.1167/tvst.7.6.41
   Matsoukas C., 2021, Is it time to replace cnns with transformers for medical images?
   Michl M, 2022, BRIT J OPHTHALMOL, V106, P113, DOI 10.1136/bjophthalmol-2020-317416
   Ng DSC, 2016, EYE, V30, P901, DOI 10.1038/eye.2016.47
   Özdemir C, 2023, EXPERT SYST APPL, V223, DOI 10.1016/j.eswa.2023.119892
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099
   Röhlig M, 2018, VISUAL COMPUT, V34, P1209, DOI 10.1007/s00371-018-1486-x
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruiz-Medrano J, 2019, PROG RETIN EYE RES, V69, P80, DOI 10.1016/j.preteyeres.2018.10.005
   Sayanagi K, 2005, AM J OPHTHALMOL, V139, P658, DOI 10.1016/j.ajo.2004.11.025
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Tan TE, 2019, INVEST OPHTH VIS SCI, V60
   Varadarajan AV, 2018, INVEST OPHTH VIS SCI, V59, P2861, DOI 10.1167/iovs.18-23887
   Viedma IA, 2022, NEUROCOMPUTING, V507, P247, DOI 10.1016/j.neucom.2022.08.021
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wilson M, 2021, JAMA OPHTHALMOL, V139, P964, DOI 10.1001/jamaophthalmol.2021.2273
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yang X, 2020, INT J ENV RES PUB HE, V17, DOI 10.3390/ijerph17020463
   Yang YH, 2020, ANN TRANSL MED, V8, DOI 10.21037/atm.2019.12.39
   Yoo TK, 2022, EYE, V36, P1959, DOI 10.1038/s41433-021-01795-5
   Zhang Z, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0065736
   Zou KH, 2007, CIRCULATION, V115, P654, DOI 10.1161/CIRCULATIONAHA.105.594929
NR 63
TC 0
Z9 0
U1 6
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 22
PY 2023
DI 10.1007/s00371-023-03189-y
EA DEC 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DC1N7
UT WOS:001129743300004
DA 2024-07-18
ER

PT J
AU Lan, CD
   Qiu, X
   Miao, CQ
   Zheng, MT
AF Lan, ChengDong
   Qiu, Xu
   Miao, Chenqi
   Zheng, MengTing
TI A self-attention model for viewport prediction based on distance
   constraint
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Self-attention; Panoramic video; Viewport prediction; Distance
   constraints
AB Panoramic video multimedia technology has made significant advancements in recent years, providing users with an immersive experience by displaying the entire 360 degrees spherical scene centered around their virtual location. However, due to its larger data volume compared to traditional video formats, transmitting high-quality videos requires more bandwidth. It is important to note that users do not see the whole 360 degrees content simultaneously, but only a portion that is within their viewport. To save bandwidth, viewport-based adaptive streaming has become a significant technology that transmits only the viewports of interest to the user in high quality. Therefore, the accuracy of viewport prediction plays a crucial role. However, the performance of viewport prediction is affected by the size of the prediction window, which decreases significantly as the window size increases. In order to address this issue, we propose an effective self-attention viewport prediction model based on distance constraint in this paper. Firstly, by analyzing the existing viewport trajectory dataset, we find the randomness and continuity of the viewport trajectory. Secondly, to solve the randomness problem, we design a viewport prediction model based on a self-attention mechanism to provide more trajectory information for long inputs. Thirdly, in order to ensure the continuity of the predicted viewport trajectory, the loss function is modified with the distance constraint to reduce the change in the continuity of prediction results. Finally, the experimental results based on the real viewport trajectory datasets show that the algorithm we propose has higher prediction accuracy and stability compared with the advanced models.
C1 [Lan, ChengDong; Qiu, Xu] Fuzhou Univ, Sch Adv Mfg, Quanzhou 362200, Fujian, Peoples R China.
   [Lan, ChengDong; Miao, Chenqi] Fujian Prov Key Lab Media Informat Intelligent Pro, Fuzhou 350108, Peoples R China.
   [Lan, ChengDong; Miao, Chenqi; Zheng, MengTing] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
C3 Fuzhou University; Fuzhou University
RP Lan, CD (corresponding author), Fuzhou Univ, Sch Adv Mfg, Quanzhou 362200, Fujian, Peoples R China.; Lan, CD (corresponding author), Fujian Prov Key Lab Media Informat Intelligent Pro, Fuzhou 350108, Peoples R China.; Lan, CD (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
EM lancd@fzu.edu.cn; 763426143@qq.com; 429366683@qq.com; 1798304422@qq.com
CR Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Atev S, 2010, IEEE T INTELL TRANSP, V11, P647, DOI 10.1109/TITS.2010.2048101
   Ban Y., 2018, 2018 IEEE INT C MULT, P1
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Chakareski J., 2018, 2018 IEEE INT C COMM, P1
   Chao FY, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733647
   Chen JY, 2021, IEEE T MULTIMEDIA, V23, P3853, DOI 10.1109/TMM.2020.3033127
   Choromanski K.M., 2020, INT C LEARN REPR
   Duanmu F, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P13, DOI 10.1145/3097895.3097898
   Feng XL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P800, DOI [10.1109/VR46266.2020.00005, 10.1109/VR46266.2020.1584727730619]
   Hou Xueshi, 2018, P 2018 MORNING WORKS, P20
   Jamali M., 2020, IEEE INT SYMP CIRC S, DOI DOI 10.1109/iscas45731.2020.9180528
   Jiang XL, 2018, C LOCAL COMPUT NETW, P393, DOI 10.1109/LCN.2018.8638092
   Kammachi-Sreedhar K, 2016, IEEE INT SYM MULTIM, P583, DOI [10.1109/ISM.2016.0126, 10.1109/ISM.2016.143]
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Kloiber S, 2020, VISUAL COMPUT, V36, P1937, DOI 10.1007/s00371-020-01942-1
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Mavlankar A, 2010, SIGNALS COMMUN TECHN, P431, DOI 10.1007/978-3-642-12802-8_19
   Nasrabadi AT, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P34, DOI 10.1145/3386290.3396934
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Nguyen DV, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3373359
   Park J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P447, DOI 10.1145/3343031.3351021
   Petrangeli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P157, DOI 10.1109/AIVR.2018.00033
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rossi S, 2019, INT CONF ACOUST SPEE, P4020, DOI 10.1109/ICASSP.2019.8683854
   Sherstyuk A, 2010, IEEE COMPUT GRAPH, V30, P93, DOI 10.1109/MCG.2010.34
   Van Damme S., 2022, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V2, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xiao GW, 2019, IEEE T COGN COMMUN, V5, P1167, DOI 10.1109/TCCN.2019.2938947
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Xu ZM, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351404
   Yang Q., 2019, 2019 IEEE INT S CIRC, P1
   Yang Q., 2019, IEEE INT SYMP CIRC S, P1, DOI [10.1109/ISCAS.2019.8702654, DOI 10.1109/iscas.2019.8702654]
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Yu Jiang, 2019, P 11 ACM WORKSH IMM, P37, DOI DOI 10.1145/3304113.3326118
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zhang MM, 2017, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2017.377
   Zhang RP, 2023, VISUAL COMPUT, V39, P1163, DOI 10.1007/s00371-021-02395-w
   ZhiQian J., 2020, IEEE Trans. Broadcast, V67, P409
   Zou JN, 2020, IEEE J-STSP, V14, P161, DOI 10.1109/JSTSP.2019.2956716
NR 43
TC 0
Z9 0
U1 4
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 28
PY 2023
DI 10.1007/s00371-023-03149-6
EA NOV 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Y9DM7
UT WOS:001108193100002
DA 2024-07-18
ER

PT J
AU Li, YL
   Feng, Y
   Zhou, ML
   Xiong, XC
   Wang, YH
   Qiang, BH
AF Li, Ya-ling
   Feng, Yong
   Zhou, Ming-liang
   Xiong, Xian-cai
   Wang, Yong-heng
   Qiang, Bao-hua
TI DMA-YOLO: multi-scale object detection method with attention mechanism
   for aerial images
SO VISUAL COMPUTER
LA English
DT Article
DE Aerial images; Object detection; YOLOv5; Attention mechanism
AB Unmanned aerial vehicles are increasingly popular due to their ease of operation, low noise, and portability. However, existing object detection methods perform poorly in detecting small targets in densely arranged, sparsely distributed aerial images. To tackle this issue, we enhanced the general object detection method YOLOv5 and introduced a multi-scale detection method called Detach-Merge Attention YOLO (DMA-YOLO). Specifically, we proposed a Detach-Merge Convolution (DMC) module and embedded it into the backbone network to maximize feature retention. Furthermore, we embedded the Bottleneck Attention Module (BAM) into the detection head to suppress interference from complex background information without significantly increasing computational complexity. To represent and process multi-scale features more effectively, we have integrated an extra detection head and enhanced the neck network into the Bi-directional Feature Pyramid Network (BiFPN) structure. Finally, we adopted the SCYLLA-IoU (SIoU) as a loss function to expedite the convergence rate of our model and enhance the precision of detection results. A series of experiments on the VisDrone2019 and UAVDT datasets have illustrated the effectiveness of DMA-YOLO. Code is available at https://github.com/Yaling-Li/DMA-YOLO.
C1 [Li, Ya-ling; Feng, Yong; Zhou, Ming-liang] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Xiong, Xian-cai] Chongqing Inst Planning & Nat Resources Invest & M, Chongqing 401121, Peoples R China.
   [Wang, Yong-heng] 8 Zhejiang Lab, Hangzhou 311121, Peoples R China.
   [Qiang, Bao-hua] Guilin Univ Elect Technol, Guangxi Key Lab Trusted Software, Guilin 541004, Peoples R China.
C3 Chongqing University; Guilin University of Electronic Technology
RP Feng, Y (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM fengyong@cqu.edu.cn
RI ZHOU, MING/JVP-2920-2024
FU This research was supported by National Nature Science Foundation of
   China (No. 62262006), State Key Laboratory of Geo-Information
   Engineering and Key Laboratory of Surveying and Mapping Science and
   Geospatial Information Technology of MNR, CASM (No.2023-0 [62262006];
   National Nature Science Foundation of China; State Key Laboratory of
   Geo-Information Engineering and Key Laboratory of Surveying and Mapping
   Science and Geospatial Information Technology of MNR
   [cstc2021jscx-gksbX0058]; Technology Innovation and Application
   Development Key Project of Chongqing [kx202006]; Guangxi Key Laboratory
   of Trusted Software [2021KE0AB01]; Zhejiang Lab
FX This research was supported by National Nature Science Foundation of
   China (No. 62262006), State Key Laboratory of Geo-Information
   Engineering and Key Laboratory of Surveying and Mapping Science and
   Geospatial Information Technology of MNR, CASM (No.2023-04-03),
   Technology Innovation and Application Development Key Project of
   Chongqing (No. cstc2021jscx-gksbX0058), Guangxi Key Laboratory of
   Trusted Software (No. kx202006), and Zhejiang Lab (No. 2021KE0AB01).
CR Abdelraouf A, 2022, IEEE T INTELL TRANSP, V23, P18546, DOI 10.1109/TITS.2022.3150715
   Benjumea A, 2021, ARXIV
   Cai EY, 2022, IEEE COMPUT SOC CONF, P1675, DOI 10.1109/CVPRW56347.2022.00174
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Cao Y, 2019, IEEE ICC
   Chalavadi V, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108548
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng ST, 2021, IEEE T IMAGE PROCESS, V30, P1556, DOI 10.1109/TIP.2020.3045636
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gevorgyan Z, 2022, arXiv
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang YC, 2022, AAAI CONF ARTIF INTE, P1026
   Jeong JY, 2022, IEEE COMPUT SOC CONF, P2352, DOI 10.1109/CVPRW56347.2022.00262
   Kirillov A., 2023, ARXIV
   Li CL, 2020, IEEE COMPUT SOC CONF, P737, DOI 10.1109/CVPRW50498.2020.00103
   Li H, 2022, LECT NOTES COMPUT SC, V13670, P532, DOI 10.1007/978-3-031-20080-9_31
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu C, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174355
   Liu S, 2019, ARXIV
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZG, 2021, PHYTOCHEM ANALYSIS, V32, P794, DOI 10.1002/pca.3025
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rolet P., 2012, P ECML C, P1255
   Ru LX, 2022, PROC CVPR IEEE, P16825, DOI 10.1109/CVPR52688.2022.01634
   Sunkara R, 2023, LECT NOTES ARTIF INT, V13715, P443, DOI 10.1007/978-3-031-26409-2_27
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang JJ, 2022, APPL INTELL, V52, P12844, DOI 10.1007/s10489-021-03147-y
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi Y, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14163919
   Xu J., 2021, ARXIV
   Yang F, 2019, IEEE I CONF COMP VIS, P8310, DOI 10.1109/ICCV.2019.00840
   Yi Wang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P651, DOI 10.1007/978-3-030-66823-5_39
   Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Zhang YM, 2022, IEEE IMAGE PROC, P1316, DOI 10.1109/ICIP46576.2022.9897517
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 55
TC 2
Z9 2
U1 52
U2 89
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4505
EP 4518
DI 10.1007/s00371-023-03095-3
EA SEP 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001075960500002
DA 2024-07-18
ER

PT J
AU Liu, YP
   Yang, DZ
   Zhang, F
   Xie, QS
   Zhang, CM
AF Liu, Yepeng
   Yang, Dezhi
   Zhang, Fan
   Xie, Qingsong
   Zhang, Caiming
TI Deep recurrent residual channel attention network for single image
   super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Single image super-resolution; Channel feature fusion module; Recursive
   block; Residual channel attention unit
AB The models based on convolutional neural network have achieved excellent results in image super-resolution by acquiring prior knowledge from a large number of images, but such models still have problems such as the features between layers in the depth network cannot be effectively fused, the number of parameters is too large, and cross-channel feature learning is impossible. Based on this, a deep recursive residual channel attention network (DRRCAN) model was proposed in this paper. To solve the problem that the information between different layers in the deep network cannot be fused effectively, this paper constructs a channel feature fusion module, which can effectively fuse the feature information of different layers. To solve the problem that the parameters increase sharply due to the increase of network depth, recursive blocks are adopted in this paper, which greatly reduces the number of parameters in the deep network. The channel attention is integrated to enable the model to learn features across channels. In addition, to avoid gradient explosion or disappearance, residual modules, long skip connections are introduced to improve the stability and generalization ability of the model. Extensive benchmark evaluations validate the superiority of the proposed DRRCAN model compared with existing algorithms.
C1 [Liu, Yepeng; Yang, Dezhi; Zhang, Fan; Xie, Qingsong] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
   [Liu, Yepeng; Zhang, Fan; Xie, Qingsong; Zhang, Caiming] Shandong Future Intelligent Financial Engn Lab, Yantai 264005, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Zhang, Caiming] Digital Media Technol Key Lab Shandong Prov, Jinan 250014, Peoples R China.
C3 Shandong Technology & Business University; Shandong University
RP Liu, YP (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.; Liu, YP (corresponding author), Shandong Future Intelligent Financial Engn Lab, Yantai 264005, Peoples R China.
EM liuyepengdream@gmail.com
RI Zhang, Caiming/AHD-6558-2022; Liu, Yepeng/AAT-7017-2021
OI Zhang, Caiming/0000-0002-6365-6221; Liu, Yepeng/0000-0001-6340-7818
FU National Natural Science Foundation of China [62002200, 62202268,
   62272281]; Shandong Provincial Natural Science Foundation [ZR2020QF012,
   ZR2021QF134, ZR2021MF068, ZR2021MF015, ZR2021MF107]
FX This research was supported by the National Natural Science Foundation
   of China (62002200, 62202268, 62272281) and Shandong Provincial Natural
   Science Foundation (ZR2020QF012, ZR2021QF134, ZR2021MF068, ZR2021MF015,
   ZR2021MF107).
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Chudasama V, 2022, VISUAL COMPUT, V38, P3643, DOI 10.1007/s00371-021-02193-4
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du QJ, 2018, SENSYS'18: PROCEEDINGS OF THE 16TH CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P410, DOI 10.1145/3274783.3275208
   Gao Y, 2023, VISUAL COMPUT, V39, P1137, DOI 10.1007/s00371-021-02393-y
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang Y, 2015, ADV NEUR IN, V28
   Jing YH, 2022, J HYDROL, V613, DOI 10.1016/j.jhydrol.2022.128388
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kirkland EJ., 2010, ADV COMPUTING ELECT, P261, DOI [DOI 10.1007/978-1-4419-6533-212, 10.1007/978-1-4419-6533-2_12, DOI 10.1007/978-1-4419-6533-2_12]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang J, 2023, arXiv
   Liu BZ, 2021, IEEE ACCESS, V9, P139138, DOI 10.1109/ACCESS.2021.3100069
   Liu D., 2023, IEEE T IMAGE PROCESS
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu T., 2022, VISUAL COMPUT, P1
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mnih V, 2014, ADV NEUR IN, V27
   Raiko T., 2012, Artificial intelligence and statistics, P924
   Srivastava RK, 2015, ADV NEUR IN, V28
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang W., 2022, ARXIV
   Wang ZY, 2015, IEEE COMPUT SOC CONF
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Yang X, 2022, VISUAL COMPUT, V38, P4307, DOI 10.1007/s00371-021-02297-x
   Yu J., 2006, 2006 8 INT C SIGN PR
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang WL, 2022, IEEE T PATTERN ANAL, V44, P7149, DOI 10.1109/TPAMI.2021.3096327
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou D., 2022, VISUAL COMPUT, P1
NR 42
TC 4
Z9 4
U1 5
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3441
EP 3456
DI 10.1007/s00371-023-03044-0
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001060100700002
DA 2024-07-18
ER

PT J
AU Yu, Y
   Li, D
   Li, BY
   Li, NL
AF Yu, Yue
   Li, Ding
   Li, Benyuan
   Li, Nengli
TI Multi-style image generation based on semantic image
SO VISUAL COMPUTER
LA English
DT Article
DE Image generation; Generative adversarial network; Style transfer
AB Image generation has always been one of the important research directions in the field of computer vision. It has rich applications in virtual reality, image design, and video synthesis. Our experiments proved that the proposed multi-style image generative network can efficiently generate high-quality images with different artistic styles based on the semantic images. Compared with the current state-of-the-art methods, the result generation speed of our proposed method is the fastest. In this paper, we focus on implementing arbitrary style transfer based on semantic images with high resolution (512 x 1024). We propose a new multi-channel generative adversarial network which uses fewer parameters to generate multi-style images. The network framework consists of a content feature extraction network, a style feature extraction network, and a content-stylistic feature fusion network. Our qualitative experiments show that the proposed multi-style image generation network can efficiently generate semantic-based, high-quality images with multiple artistic styles and with greater clarity and richer details. We adopt a user preference study, and the results show that the results generated by our method are more popular. Our speed study shows that our proposed method has the fastest result generation speed compared to the current state-of-the-art methods. We publicly release the source code of our project, which can be accessed at.
C1 [Yu, Yue; Li, Ding; Li, Benyuan; Li, Nengli] Beijing Inst Technol, 5,South St,Zhongguancun, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Yu, Y (corresponding author), Beijing Inst Technol, 5,South St,Zhongguancun, Beijing 100081, Peoples R China.
EM yuyueanny@hotmail.com
FU National Natural Science Foundation of China [61807002]
FX This work is supported by National Natural Science Foundation of China
   (61807002).
CR Benyuan Li, 2020, ICVIP 2020: Proceedings of 2020 4th International Conference on Video and Image Processing, P218, DOI 10.1145/3447450.3447484
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Dey N, 2006, MICROSC RES TECHNIQ, V69, P260, DOI 10.1002/jemt.20294
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Li P, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P241, DOI 10.1109/ICIVC.2018.8492852
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YJ, 2017, ADV NEUR IN, V30
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Miyato T, 2018, INT C LEARN REPR
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Reimann M, 2019, VISUAL COMPUT, V35, P1531, DOI 10.1007/s00371-019-01654-1
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Virtusio J.J., 2020, IEEE T MULTIMEDIA, VPP, P1
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang L, 2020, VISUAL COMPUT, V36, P317, DOI 10.1007/s00371-018-1609-4
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yu X., 2023, VISUAL COMPUT, V39, P1
   Zhao HH, 2020, VISUAL COMPUT, V36, P1307, DOI 10.1007/s00371-019-01726-2
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 29
TC 3
Z9 3
U1 10
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3411
EP 3426
AR s00371-023-03042-2
DI 10.1007/s00371-023-03042-2
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001045114100003
DA 2024-07-18
ER

PT J
AU Liu, WN
   Liu, TL
   Han, T
   Wan, L
AF Liu, Wennan
   Liu, Tianling
   Han, Tong
   Wan, Liang
TI Multi-modal deep-fusion network for meningioma presurgical grading with
   integrative imaging and clinical data
SO VISUAL COMPUTER
LA English
DT Article
DE Meningioma; Multi-model fusion; Presurgical grading; Image and clinical
   data
ID BRAIN-TUMOR; MRI
AB Predicting meningioma grade before surgery is crucial to making decisions on therapy planning and prognosis prediction. Prior works mainly investigate traditional classification techniques with hand-crafted features or rely on image data only, thus having limited accuracy. In this study, we propose a novel multi-modal classification method, i.e., multi-modal deep-fusion network (MMDF), integrating high-dimensional 3D MRI imaging information and low-dimensional tabular clinical data to classify low-grade and high-grade meningiomas. Specifically, the MMDF adopts two modality-specific branches to extract image and clinical features, respectively, and leverages an image-clinical integration module in the shared branch to fuse cross-model features, while specifically considering the impact of low-dimensional clinical data. Besides, we propose a multi-modal image feature aggregation module to integrate three image modalities in the image-specific branch, which can compensate for the feature distribution gaps among the contrast-enhanced T1, contrast-enhanced T2-FLAIR and ADC modalities. Comprehensive experiments show that our approach significantly outperforms the SOTA methods using imaging data only and those combining image and tabular data both, with an AUC of 0.958, sensitivity of 0.877, specificity of 0.926, and accuracy of 0.921. Our approach holds high potential to aid radiologists in doing the presurgical evaluation for clinical decision making.
C1 [Liu, Wennan; Wan, Liang] Tianjin Univ, Acad Med Engn & Translat Med, Tianjin, Peoples R China.
   [Liu, Tianling] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Han, Tong] Tianjin Univ, Tianjin Huanhu Hosp, Tianjin, Peoples R China.
C3 Tianjin University; Tianjin University; Tianjin University
RP Wan, L (corresponding author), Tianjin Univ, Acad Med Engn & Translat Med, Tianjin, Peoples R China.
EM lwnnan@tju.edu.cn; liu_dling@tju.edu.cn; mrbold@163.com; lwan@tju.edu.cn
RI Li, Lei/JPE-6543-2023; Wang, Guanhua/JXM-6373-2024; zhang,
   ling/JXW-6931-2024; Zhang, Wenbin/JXX-8070-2024; liu,
   xingyu/JXW-9444-2024
FU Tianjin Natural Science Foundation [20JCYBJC00960]
FX AcknowledgementsThis work was supported by the grant from Tianjin
   Natural Science Foundation (Grant No. 20JCYBJC00960).
CR Abiwinanda N, 2019, IFMBE PROC, V68, P183, DOI 10.1007/978-981-10-9035-6_33
   Banerjee S., 2019, ARXIV
   Cornelius JF, 2013, ACTA NEUROCHIR, V155, P407, DOI 10.1007/s00701-012-1611-y
   Dai Z, 2021, ADV NEUR IN, V34
   Deepak S, 2019, COMPUT BIOL MED, V111, DOI 10.1016/j.compbiomed.2019.103345
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   El-Dahshan ESA, 2014, EXPERT SYST APPL, V41, P5526, DOI 10.1016/j.eswa.2014.01.021
   Guo Z, 2018, I S BIOMED IMAGING, P903, DOI 10.1109/ISBI.2018.8363717
   Hawkins-Daarud A, 2013, FRONT ONCOL, V3, DOI 10.3389/fonc.2013.00066
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jie H, 2017, IEEE Trans Pattern Anal Mach Intell PP, VPP, P1
   Kunyi Z., 2008, J MED EQUIP
   Park YW, 2019, EUR RADIOL, V29, P4068, DOI 10.1007/s00330-018-5830-3
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Pölsterl S, 2021, LECT NOTES COMPUT SC, V12905, P688, DOI 10.1007/978-3-030-87240-3_66
   Qu YH, 2020, THORAC CANCER, V11, P651, DOI 10.1111/1759-7714.13309
   Saraf S, 2011, ONCOLOGIST, V16, P1604, DOI 10.1634/theoncologist.2011-0193
   Shaowei Z., 2017, LINGNAN MODERN CLIN, V17, P742
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Surov A, 2015, TRANSL ONCOL, V8, P517, DOI 10.1016/j.tranon.2015.11.012
   Talo M, 2019, COGN SYST RES, V54, P176, DOI 10.1016/j.cogsys.2018.12.007
   Tseng KL, 2017, PROC CVPR IEEE, P3739, DOI 10.1109/CVPR.2017.398
   Yan PF, 2017, TRANSL ONCOL, V10, P570, DOI 10.1016/j.tranon.2017.04.006
   Yang SY, 2008, J NEUROL NEUROSUR PS, V79, P574, DOI 10.1136/jnnp.2007.121582
   Yin B, 2012, EUR J RADIOL, V81, P4050, DOI 10.1016/j.ejrad.2012.06.002
   Zeng, 2014, FRONTIER FUTURE DEV, P2669
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
   Zhang H, 2021, NEUROINFORMATICS, V19, P393, DOI 10.1007/s12021-020-09492-6
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu H, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/7289273
   Zhu YB, 2019, EUR J RADIOL, V116, P128, DOI 10.1016/j.ejrad.2019.04.022
NR 32
TC 0
Z9 0
U1 5
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3561
EP 3571
DI 10.1007/s00371-023-02978-9
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001026397800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Zhang, JP
   Chen, JS
   Guo, RX
   Wu, J
AF Zhang, Xiaoyu
   Zhang, Jinping
   Chen, Jiusheng
   Guo, Runxia
   Wu, Jun
TI A dual-structure attention-based multi-level feature fusion network for
   automatic surface defect detection
SO VISUAL COMPUTER
LA English
DT Article
DE Attention pooling; Channel attention; Multi-level feature fusion;
   Surface defect detection
AB The detection of surface defects is crucial to industrial manufacturing. In recent years, numerous detection methods based on computer vision have been successfully applied in the industry. However, industrial defect detection is still full of challenges. In one aspect, most of the industrial defects are extremely small. In another aspect, even though the intra-class defects have numerous similar elements, their outward appearances differ significantly. In this paper, we propose a dual-structure attention-based multi-level feature fusion network (DaMFFN) to address these two issues. In the first attention-based multi-level feature extraction structure, we introduce novel attention pooling to preserve more detailed information about the defective features of the tiny defect by giving certain regions varying weights. In the second attention-based multi-level feature fusion structure, we propose channel attention to capture the defect feature with the greatest potential for discrimination rather than all possible defect features, which is employed to prevent the incorrect detection of intra-class defects. The experiments demonstrate that the detection performance of the DaMFFN is better than other methods in five surface defect datasets.
C1 [Zhang, Xiaoyu; Zhang, Jinping; Chen, Jiusheng; Guo, Runxia] Civil Aviat Univ China, Coll Elect Informat & Automat, 2898 Jinbei Rd, Tianjin 300300, Peoples R China.
   [Wu, Jun] Civil Aviat Univ China, Coll Aeronaut Engn, 2898 Jinbei Rd, Tianjin 300300, Peoples R China.
C3 Civil Aviation University of China; Civil Aviation University of China
RP Zhang, XY (corresponding author), Civil Aviat Univ China, Coll Elect Informat & Automat, 2898 Jinbei Rd, Tianjin 300300, Peoples R China.
EM xy_zhang@cauc.edu.cn
RI Zhang, Xiaoyu/GQY-4651-2022
OI ZHANG, XIAOYU/0000-0002-5933-1977; Zhang, Jinping/0000-0002-0870-5131
FU National Natural Science Foundation of China [62173331, 52005500];
   Natural Science Foundation of Tianjin Municipal Science and Technology
   Commission [2020KJ013]; Civil Aviation University of China Research
   Innovation Project for Postgraduate Students [2022YJS018]; Basic
   Science-research Funds of National University [3122023044, 3122023PY06]
FX AcknowledgementsThis research was supported by funding from the National
   Natural Science Foundation of China (62173331, 52005500), Natural
   Science Foundation of Tianjin Municipal Science and Technology
   Commission (2020KJ013), Civil Aviation University of China Research
   Innovation Project for Postgraduate Students (2022YJS018), and The Basic
   Science-research Funds of National University (3122023044, 3122023PY06).
CR Baheti B, 2020, IEEE COMPUT SOC CONF, P1473, DOI 10.1109/CVPRW50498.2020.00187
   Bhatt PM, 2021, J COMPUT INF SCI ENG, V21, DOI 10.1115/1.4049535
   Bin Roslan Muhammad Izzat, 2022, 2022 IEEE 12th Symposium on Computer Applications & Industrial Electronics (ISCAIE)., P111, DOI 10.1109/ISCAIE54458.2022.9794475
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cao JG, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3033726
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Dong HW, 2020, IEEE T IND INFORM, V16, P7448, DOI 10.1109/TII.2019.2958826
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hang JF, 2022, IEEE OPEN J IND ELEC, V3, P473, DOI 10.1109/OJIES.2022.3193572
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Hu BZ, 2021, IEEE T IMAGE PROCESS, V30, P472, DOI 10.1109/TIP.2020.3036770
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang WB, 2021, IEEE T IND INFORM, V17, P5485, DOI 10.1109/TII.2020.3033170
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Konovalenko I, 2022, MACHINES, V10, DOI 10.3390/machines10050327
   Konovalenko I, 2021, METALS-BASEL, V11, DOI 10.3390/met11111851
   Li Yinghao, 2022, 2022 4th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP), P227, DOI 10.1109/ICMSP55950.2022.9859056
   Li Z., 2018, arXiv
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma Ningning, 2018, P EUR C COMP VIS ECC, DOI [10.1007/978-3-030-01264-9_8, DOI 10.1007/978-3-030-01264-9_8]
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Singh SA, 2023, EXPERT SYST APPL, V218, DOI 10.1016/j.eswa.2023.119623
   Su BY, 2021, IEEE T IND INFORM, V17, P4084, DOI 10.1109/TII.2020.3008021
   Tao JM, 2022, IEEE SENS J, V22, P8693, DOI 10.1109/JSEN.2022.3159743
   tianchi.aliyun, 2020, TIANCHI SMART DIAGNO
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Üzen H, 2022, EXPERT SYST APPL, V209, DOI 10.1016/j.eswa.2022.118269
   Uzen H, 2023, NEURAL COMPUT APPL, V35, P3263, DOI 10.1007/s00521-022-07885-z
   Üzen H, 2023, VISUAL COMPUT, V39, P1745, DOI 10.1007/s00371-022-02442-0
   Wang WY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3127648
   Wei P., PKU MARKET PCB OPEN
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wieler M., Weakly Supervised Learning for Industrial Optical Inspection
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhu Z., 2022, P 2022 IEEE 10 JOINT, P2580, DOI [10.1109/ITAIC54216.2022.9836478, DOI 10.1109/ITAIC54216.2022.9836478]
NR 42
TC 2
Z9 2
U1 16
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2713
EP 2732
DI 10.1007/s00371-023-02980-1
EA JUL 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001017717100002
DA 2024-07-18
ER

PT J
AU Huang, H
   Xiao, D
   Liang, J
   Li, M
AF Huang, Hui
   Xiao, Di
   Liang, Jia
   Li, Min
TI Prior-based privacy-assured compressed sensing scheme in cloud
SO VISUAL COMPUTER
LA English
DT Article
DE Compressed sensing; Prior information; Measurement matrix; Privacy
   assurance; Sparse recovery
ID SPARSE REPRESENTATION; PRIOR INFORMATION; OPTIMIZED PROJECTIONS;
   IMAGE-RECONSTRUCTION; MATRIX; PERMUTATION; RECOVERY; FRAMES
AB Compressed sensing (CS) is a popular signal processing technique. However, some of its performances still need to be improved for possible secure visual applications, including the optimization of the measurement matrix, privacy assurance, and the sparse recovery performance. To this end, we present prior-based measurement matrix design and sparse recovery algorithm for privacy-assured CS scheme in the cloud. More specifically, the measurement matrix is modeled by minimizing a Frobenius difference between the identity matrix and the Gram of the weighted sensing matrix. The gradient descent method is employed to derive the prior probability-weighted measurement matrix. Further, privacy-assured CS can be achieved by using within-row permutation and chaotic matrices. Finally, we also employ the prior information to enhance the accuracy of the sparse recovery algorithm by using prior probability-weighted orthogonal matching pursuit. Theoretical analyses and simulation results demonstrate that the proposed scheme can optimize measurement matrix, achieve privacy-assured CS, and improve sparse recovery performance.
C1 [Huang, Hui; Xiao, Di; Liang, Jia; Li, Min] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
C3 Chongqing University
RP Xiao, D (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM cqyyhuang@163.com; xiaodi_cqu@hotmail.com; 1054701003@qq.com;
   li_min_1995@163.com
RI Wang, YUJIE/JXY-8442-2024
FU National Key R &D Program of China [2020YFB1805400]; National Natural
   Science Foundation of China [62072063]; Graduate Student Research and
   Innovation Foundation of Chongqing, China [CYB 21062]
FX AcknowledgementsThe work was supported by the National Key R &D Program
   of China (Grant No. 2020YFB1805400), the National Natural Science
   Foundation of China (Grant No. 62072063), and the Project Supported by
   Graduate Student Research and Innovation Foundation of Chongqing, China
   (Grant No. CYB 21062).
CR Abolghasemi V, 2012, SIGNAL PROCESS, V92, P999, DOI 10.1016/j.sigpro.2011.10.012
   Bai H, 2015, IEEE T SIGNAL PROCES, V63, P1581, DOI 10.1109/TSP.2015.2399864
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chen HY, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108827
   Chen W, 2012, IEEE SIGNAL PROC LET, V19, P8, DOI 10.1109/LSP.2011.2173675
   Cleju N, 2014, APPL COMPUT HARMON A, V36, P495, DOI 10.1016/j.acha.2013.08.005
   Cui MS, 2016, PATTERN RECOGN LETT, V84, P120, DOI 10.1016/j.patrec.2016.08.017
   Ding X, 2017, IEEE T SIGNAL PROCES, V65, P3632, DOI 10.1109/TSP.2017.2699639
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Duarte MF, 2011, IEEE T SIGNAL PROCES, V59, P4053, DOI 10.1109/TSP.2011.2161982
   Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Hu GQ, 2017, J VIS COMMUN IMAGE R, V44, P116, DOI 10.1016/j.jvcir.2017.01.022
   Huang H, 2018, SIGNAL PROCESS, V150, P183, DOI 10.1016/j.sigpro.2018.04.014
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Joseph G, 2021, IEEE T SIGNAL PROCES, V69, P905, DOI 10.1109/TSP.2021.3051743
   Kuldeep G, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9348093
   Kuldeep G, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9322181
   KULKARNI K, 2016, PROC CVPR IEEE, P449, DOI DOI 10.1109/CVPR.2016.55
   Li B, 2017, SIGNAL PROCESS, V135, P36, DOI 10.1016/j.sigpro.2016.11.024
   Li G, 2013, IEEE T SIGNAL PROCES, V61, P2887, DOI 10.1109/TSP.2013.2253776
   Marques EC, 2019, IEEE ACCESS, V7, P1300, DOI 10.1109/ACCESS.2018.2886471
   Mota JFC, 2017, IEEE T INFORM THEORY, V63, P4472, DOI 10.1109/TIT.2017.2695614
   Scarlett J, 2013, IEEE T SIGNAL PROCES, V61, P427, DOI 10.1109/TSP.2012.2225051
   Strohmer T, 2003, APPL COMPUT HARMON A, V14, P257, DOI 10.1016/S1063-5203(03)00023-X
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang C, 2013, IEEE T EMERG TOP COM, V1, P166, DOI 10.1109/TETC.2013.2273797
   Zelnik-Manor L, 2011, IEEE T SIGNAL PROCES, V59, P4300, DOI 10.1109/TSP.2011.2159211
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang B, 2019, J VIS COMMUN IMAGE R, V60, P69, DOI 10.1016/j.jvcir.2019.02.023
   Zhang JH, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107885
   Zhang Y., 2018, SECURE COMPRESSIVE S
   Zhang YS, 2021, IEEE T IND INFORM, V17, P3401, DOI 10.1109/TII.2020.3008914
   Zhang YS, 2020, IEEE T IND INFORM, V16, P6641, DOI 10.1109/TII.2020.2966511
   Zhang YS, 2019, INFORM SCIENCES, V496, P150, DOI 10.1016/j.ins.2019.05.024
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zheng YC, 2023, INTEL MED, V3, P115, DOI 10.1016/j.imed.2022.05.004
NR 37
TC 3
Z9 3
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2103
EP 2117
DI 10.1007/s00371-023-02906-x
EA JUN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001000345700001
DA 2024-07-18
ER

PT J
AU Di, YD
   Liao, Y
   Zhu, KJ
   Zhou, H
   Zhang, YJ
   Duan, Q
   Liu, JH
   Lu, MY
AF Di, Yide
   Liao, Yun
   Zhu, Kaijun
   Zhou, Hao
   Zhang, Yijia
   Duan, Qing
   Liu, Junhui
   Lu, Mingyu
TI MIVI: multi-stage feature matching for infrared and visible image
SO VISUAL COMPUTER
LA English
DT Article
DE Feature matching; Infrared image; Visible image; Transformer
ID REGISTRATION; DESCRIPTOR
AB The matching of infrared and visible images has a wide range of applications across various fields. However, the large difference between these two types of images poses a significant challenge to achieving accurate feature matching. In this paper, we introduce a novel feature matching method for infrared and visible images, named MIVI. Our proposed multi-stage matching architecture enables the model to capture both fine local feature details and remote dependencies, while our novel composite loss function optimizes the model at each stage and significantly improves the matching accuracy. Qualitative and quantitative experiments demonstrate that MIVI outperforms other excellent algorithms in terms of accuracy. The code will be released at: https:// github.com/LiaoYun0x0/ MIVI.
C1 [Di, Yide; Zhang, Yijia; Lu, Mingyu] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian, Liaoning, Peoples R China.
   [Liao, Yun; Duan, Qing; Liu, Junhui] Yunnan Univ, Natl Pilot Sch Software, Kunming, Yunnan, Peoples R China.
   [Di, Yide; Liao, Yun; Zhu, Kaijun; Zhou, Hao] Yunnan Lanyi Network Technol Co, Kunming, Yunnan, Peoples R China.
C3 Dalian Maritime University; Yunnan University
RP Lu, MY (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian, Liaoning, Peoples R China.
EM ghostdyd@126.com; LiaoYun@ynu.edu.cn; xunfeng.zkj@gmail.com;
   1083480050@qq.com; zhangyijia@dlmu.edu.cn; qduan@ynu.edu.cn;
   hanks@ynu.edu.cn; dlmuitrec@163.com
OI Lu, Mingyu/0000-0002-8663-9870
FU Social and Science Foundation of Liaoning Province [L20BTQ008]; National
   Natural Science Foundation of China [61976124]; Scientific Research Fund
   of Yunnan Provincial Education Department [2021J0007]
FX AcknowledgementsThis work is supported by a grant from the Social and
   Science Foundation of Liaoning Province (No. L20BTQ008), in part by the
   National Natural Science Foundation of China under Grant 61976124 and in
   part by the Scientific Research Fund of Yunnan Provincial Education
   Department under Grant 2021J0007.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   Arar M., 2020, IEEECVF C COMPUT VIS, P13407, DOI 10.1109/cvpr42600.2020.01342
   Balntas V., 2016, Procedings of the British Machine Vision Conference 2016, (York, UK), p119.1
   Bhattacharjee D, 2021, IEEE T PATTERN ANAL, V43, P595, DOI 10.1109/TPAMI.2019.2930192
   Bökman G, 2022, IEEE COMPUT SOC CONF, P5106, DOI 10.1109/CVPRW56347.2022.00559
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chen H., 2021, arXiv
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Cui S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3099506
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Di YD, 2021, MULTIMED TOOLS APPL, V80, P35629, DOI 10.1007/s11042-021-10830-2
   Dosovitskiy Alexey, 2021, ICLR
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Fang YC, 2020, IEEE INT VEH SYM, P1885, DOI [10.1109/iv47402.2020.9304612, 10.1109/IV47402.2020.9304612]
   Ghannadi MA, 2019, IEEE GEOSCI REMOTE S, V16, P568, DOI 10.1109/LGRS.2018.2876661
   Giang KT, 2022, ARXIV, DOI DOI 10.48550/ARXIV.2207.00328
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Hrkac T, 2007, LECT NOTES COMPUT SC, V4522, P383
   Li JY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3165940
   Liao Y, 2022, IEEE J-STARS, V15, P448, DOI 10.1109/JSTARS.2021.3134676
   Lindenberger P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5967, DOI 10.1109/ICCV48922.2021.00593
   Liu XM, 2021, MULTIMED TOOLS APPL, V80, P16491, DOI 10.1007/s11042-020-10213-z
   Liu XM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194244
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Ma JY, 2015, PATTERN RECOGN, V48, P772, DOI 10.1016/j.patcog.2014.09.005
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Min CB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107377
   Min CB, 2020, IEEE ACCESS, V8, P42562, DOI 10.1109/ACCESS.2020.2976767
   Mishchuk A., 2017, P ADV NEURAL INFORM, P4826
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Rocco I, 2018, ADV NEUR IN, V31
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schmitt M., 2021, ARXIV
   Schmitt M., 2018, ARXIV
   Son J, 2015, EXPERT SYST APPL, V42, P8830, DOI 10.1016/j.eswa.2015.07.035
   Sun JM, 2021, PROC CVPR IEEE, P8918, DOI 10.1109/CVPR46437.2021.00881
   Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752
   Tang S., 2022, 10 INT C LEARN REPR
   Tyszkiewicz M., 2020, ADV NEURAL INFORM PR
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L, 2018, IEEE IMAGE PROC, P1248, DOI 10.1109/ICIP.2018.8451370
   Wang Q, 2023, LECT NOTES COMPUT SC, V13843, P256, DOI 10.1007/978-3-031-26313-2_16
   Wu FH, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.5.053017
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Yoon S, 2021, IEEE ROBOT AUTOM LET, V6, P8726, DOI 10.1109/LRA.2021.3111760
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zhou QJ, 2021, PROC CVPR IEEE, P4667, DOI 10.1109/CVPR46437.2021.00464
NR 51
TC 3
Z9 3
U1 12
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1839
EP 1851
DI 10.1007/s00371-023-02889-9
EA MAY 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000993019500001
DA 2024-07-18
ER

PT J
AU Qiu, JX
   Zhu, YF
   Jiang, PT
   Cheng, MM
   Ren, B
AF Qiu, Jiaxiong
   Zhu, Yifan
   Jiang, Peng-Tao
   Cheng, Ming-Ming
   Ren, Bo
TI RDNeRF: relative depth guided NeRF for dense free view synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE Dense free view synthesis; Neural radiance fields; Relative depth;
   Internal relevance
AB In this paper, we focus on dense view synthesis with free movements in indoor scenes for better user interactions than sparse views. Neural radiance field (NeRF) handles sparsely and spherically captured scenes well, while it struggles in scenes with dense free views. We extend NeRF to handle these views of indoor scenes. We present a learning-based approach named relative depth guided NeRF (RDNeRF), which jointly renders RGB images and recovers scene geometry in dense free views. To recover the geometry of each view without the ground-truth depth, we propose to directly learn the relative depth by implicit functions and transform it as a geometric volume bound for geometry-aware sampling and integration of NeRF. With correct scene geometry, we further model the implicit internal relevance of inputs to enhance the representation ability of NeRF in dense free views. We conduct extensive experiments in indoor scenes for dense free view synthesis. RDNeRF outperforms current state-of-the-art methods and achieves 24.95 PSNR score and 0.77 SSIM score. Besides, it recovers more accurate geometry than basic models.
C1 [Qiu, Jiaxiong; Zhu, Yifan; Cheng, Ming-Ming; Ren, Bo] Nankai Univ, Coll Comp Sci, VCIP, Tianjin, Peoples R China.
   [Jiang, Peng-Tao] Zhejiang Univ, Hangzhou, Peoples R China.
C3 Nankai University; Zhejiang University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, VCIP, Tianjin, Peoples R China.
EM qiujiaxiong727@gmail.com; zhuyifan@mail.nankai.edu.cn;
   pt.jiang@mail.nankai.edu.cn; cmm@nankai.edu.cn; rb@nankai.edu.cn
RI zhu, yifan/JMR-2845-2023; ren, bo/IST-0814-2023; Cheng,
   Ming-Ming/A-2527-2009; Jiang, Peng-Tao/HHN-3328-2022; Qiu,
   Jiaxiong/KDM-8471-2024
OI Cheng, Ming-Ming/0000-0001-5550-8758; Jiang,
   Peng-Tao/0000-0002-1786-4943; Qiu, Jiaxiong/0000-0002-6065-7296
FU National Key Research and Development Program of China Grant
   [2018AAA0100400]; NSFC [61922046, 62132012]
FX This work is supported by the National Key Research and Development
   Program of China Grant (No. 2018AAA0100400), NSFC (No.61922046) and NSFC
   (No. 62132012).
CR Aliev KA., 2019, NEURAL POINT BASED G, V2, P4
   Andraghetti L, 2019, INT CONF 3D VISION, P424, DOI 10.1109/3DV.2019.00054
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   Battiato S, 2004, PROC SPIE, V5302, P95, DOI 10.1117/12.526634
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Chen D., 2019, OPT EXPRESS, V27, p24,624
   Chen W., 2016, ARXIV
   Chen Z, 2022, ARXIV
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai P, 2020, P IEEECVF C COMPUTER, P7830
   Deng Kangle, 2021, arXiv
   DeVries Terrance, 2021, arXiv
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Glocker B, 2013, INT SYM MIX AUGMENT, P173, DOI 10.1109/ISMAR.2013.6671777
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lindell D.B., 2020, ARXIV
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666
   Martin-Brualla R., 2020, ARXIV
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mildenhall Ben, 2020, EUR C COMP VIS ECCV
   Neff T, 2021, COMPUT GRAPH FORUM, V40, P45, DOI 10.1111/cgf.14340
   Nguyen HT, 2009, IEEE T IMAGE PROCESS, V18, P703, DOI 10.1109/TIP.2009.2012884
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Pumarola A., 2020, ARXIV
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Reizenstein Jeremy, 2021, P IEEE CVF INT C COM
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Riegler G, 2021, PROC CVPR IEEE, P12211, DOI 10.1109/CVPR46437.2021.01204
   Roessle B., 2021, ARXIV
   Srinivasan PP, 2019, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2019.00026
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang CY, 2019, INT CONF 3D VISION, P348, DOI 10.1109/3DV.2019.00046
   Wang P., 2022, ARXIV
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu XC, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530153
   Yin W, 2021, PROC CVPR IEEE, P204, DOI 10.1109/CVPR46437.2021.00027
   YiWei Shaohui Liu, 2021, P IEEE CVF INT C COM, P5610
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zhang C, 2004, SIGNAL PROCESS-IMAGE, V19, P1, DOI 10.1016/j.image.2003.07.001
   Zhang K., 2020, ARXIV
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou T., 2018, ARXIV
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 52
TC 4
Z9 4
U1 5
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1485
EP 1497
DI 10.1007/s00371-023-02863-5
EA MAY 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000982760900002
DA 2024-07-18
ER

PT J
AU Garcia-Nonoal, Z
   Mata-Mendoza, D
   Cedillo-Hernandez, M
   Nakano-Miyatake, M
AF Garcia-Nonoal, Zaira
   Mata-Mendoza, David
   Cedillo-Hernandez, Manuel
   Nakano-Miyatake, Mariko
TI Secure management of retinal imaging based on deep learning,
   zero-watermarking and reversible data hiding
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Zero-watermarking; DRIVE digital retinal images for
   vessel extraction; Reversible data hiding
ID COLOR IMAGES
AB Advances in communication and information technologies have allowed for improvements in the distribution and management of several types of imaging in digital medical environments. The scientific literature has reported data hiding methods that can contribute to improving medical image management and mitigate information security risks. This paper proposes a secure management scheme for retinal imaging based on deep learning, reversible data hiding and zero-watermarking. To create a proper link between a patient and their retinal image, a unique feature is obtained through retina vessel segmentation and optic disk detection using U-Net and RetinaNet deep learning architectures, respectively. The unique feature, in conjunction with a halftoned version of the patient's image, are employed to generate a zero-watermarking code using a zero-watermarking technique based on message digest, spread spectrum, and seam-carving methods. Finally, using a color channel of the retinal image, the zero-watermarking code is concealed in a reversible manner using a data hiding technique based on code division multiplexing. The proposed method ensures patient authentication and verification of integrity, and avoids detachment between the patient and their retinal image. Experimental results show the contribution of the proposed scheme to and its efficiency in retinal image management.
C1 [Garcia-Nonoal, Zaira; Mata-Mendoza, David; Cedillo-Hernandez, Manuel; Nakano-Miyatake, Mariko] Inst Politecn Nacl SEPI ESIME Culhuacan, Ave St Ana 1000 Culhuacan CTM V, Mexico City 04440, Mexico.
RP Cedillo-Hernandez, M (corresponding author), Inst Politecn Nacl SEPI ESIME Culhuacan, Ave St Ana 1000 Culhuacan CTM V, Mexico City 04440, Mexico.
EM mcedilloh@ipn.mx
RI Nakano, Mariko/N-4075-2019; Nakano, Mariko/O-2954-2017;
   Cedillo-Hernandez, Manuel/R-2154-2018
OI Garcia-Nonoal, Zaira/0000-0003-2832-3496; Nakano,
   Mariko/0000-0003-1346-7825; Cedillo-Hernandez,
   Manuel/0000-0002-9149-9841
CR Al-qdah M., 2018, SIGNAL IMAGE PROCESS, DOI 10.5121/sipij.2018.9101
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Barni M., 2005, DIGITAL WATERMARKING, DOI [10.1007/11551492, DOI 10.1007/11551492]
   Barni M., 2004, WATERMARKING SYSTEMS, P23, DOI [10.1201/9780203913512, DOI 10.1201/9780203913512]
   Cedillo-Hernandez M, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101695
   Cedillo-Hernandez M, 2017, RADIOENGINEERING, V26, P536, DOI 10.13164/re.2017.0536
   Cox I, 2002, DIGITAL WATERMARKING, P11
   Dutta MK, 2015, 2015 38TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P781, DOI 10.1109/TSP.2015.7296372
   Gong ZT, 2022, VISUAL COMPUT, V38, P707, DOI 10.1007/s00371-020-02045-7
   Hashemzadeh M, 2019, SIGNAL PROCESS, V155, P233, DOI 10.1016/j.sigpro.2018.09.037
   Hassan B, 2019, IEEE ACCESS, V7, P69758, DOI 10.1109/ACCESS.2019.2919381
   Hill R.B., 1978, Patent, Patent No. 4109237
   Hu K, 2021, VISUAL COMPUT, V37, P2841, DOI 10.1007/s00371-021-02168-5
   Liu DR, 2022, VEHICLE SYST DYN, V60, P433, DOI 10.1080/00423114.2020.1817508
   Ma B, 2016, IEEE T INF FOREN SEC, V11, P1914, DOI 10.1109/TIFS.2016.2566261
   Magdy M, 2022, MULTIMED TOOLS APPL, V81, P25101, DOI 10.1007/s11042-022-11956-7
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Mese M, 2002, IEEE T CIRCUITS-I, V49, P790, DOI 10.1109/TCSI.2002.1010034
   Mousavi SM, 2014, J DIGIT IMAGING, V27, P714, DOI 10.1007/s10278-014-9700-5
   Mukherjee Subhayan, 2018, Smart Multimedia. First International Conference, ICSM 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11010), P193, DOI 10.1007/978-3-030-04375-9_17
   National Electrical Manufacturers Association (NEMA), 2023, DICOM SEC, P1
   Nelson SE, 2022, BMC HEALTH SERV RES, V22, DOI 10.1186/s12913-022-08197-7
   Pelzl J., 2010, Understanding Cryptography: A Textbook for Students and Practitioners, DOI DOI 10.1007/978-3-642-04101-3
   Prakash SJ, 2022, VISUAL COMPUT, V38, P4129, DOI 10.1007/s00371-021-02285-1
   Qasim AF, 2018, COMPUT SCI REV, V27, P45, DOI 10.1016/j.cosrev.2017.11.003
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   RoselinKiruba R, 2023, VISUAL COMPUT, V39, P59, DOI 10.1007/s00371-021-02312-1
   Selvi TM, 2022, VISUAL COMPUT, V38, P385, DOI 10.1007/s00371-020-02021-1
   Shi CY, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-13919-2
   Singh A., 2018, P INT C COMP INT COM, P1, DOI [10.1109/CIACT.2018.8480151, DOI 10.1109/CIACT.2018.8480151]
   Singh A, 2022, MULTIMED TOOLS APPL, V81, P2429, DOI 10.1007/s11042-021-11600-w
   Singh A, 2020, J KING SAUD UNIV-COM, V32, P895, DOI 10.1016/j.jksuci.2017.12.008
   Singh A, 2017, INT J MED INFORM, V108, P110, DOI 10.1016/j.ijmedinf.2017.10.010
   Singh A, 2016, COMPUT METH PROG BIO, V135, P61, DOI 10.1016/j.cmpb.2016.07.011
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Su GD, 2023, VISUAL COMPUT, V39, P4623, DOI 10.1007/s00371-022-02613-z
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HT, 2018, SIGNAL PROCESS-IMAGE, V62, P64, DOI 10.1016/j.image.2017.12.006
   Yuan ZH, 2021, VISUAL COMPUT, V37, P1867, DOI 10.1007/s00371-020-01945-y
NR 40
TC 3
Z9 3
U1 5
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 245
EP 260
DI 10.1007/s00371-023-02778-1
EA JAN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000917808600001
DA 2024-07-18
ER

EF