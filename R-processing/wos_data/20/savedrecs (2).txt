FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Liang, S
   Wang, JW
   Zhuang, ZK
AF Liang, Shuang
   Wang, Jiewen
   Zhuang, Zikun
TI Patch excitation network for boxless action recognition in still images
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Attention mechanism; Patch-based vision task; Weak
   supervision
AB Action recognition in still images is considered to be a challenging task, mainly due to the lack of temporal information. To address this issue, researchers usually need to locate the human in the image first and then combined it with some information that is closely related to the action, such as human pose and surrounding objects. Such methods are very effective, but they rely heavily on annotations, especially the human bounding box. To get rid of this limitation, we propose a novel patch excitation network in this paper, which requires only images as input in both training and testing phases. The images will be evenly divided into patches, and "excitation" will be performed in two levels. First, the action excitation module will process the whole image so that the action-related regions get a higher response. After that, the activated feature will be assigned to patches of different sizes. Finally, the patches of different sizes will be fed into the patch interaction module for mutual enhancement. Throughout the process, no step deliberately discovers a specific action-related information like the human body or pose, but rather looks more broadly for all clues related to the action in the image. This idea is different from some previous boxless methods. Experiments show that the proposed solution is able to obtain state-of-the-art results in the boxless methods on widely used datasets. In particular, on the Stanford 40 dataset, the proposed solution's performance is comparable to those of the methods using additional annotations.
C1 [Liang, Shuang; Wang, Jiewen; Zhuang, Zikun] Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
C3 Tongji University
RP Liang, S (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
EM shuangliang@tongji.edu.cn; wjwlaservne@tongji.edu.cn;
   2131488@tongji.edu.cn
RI Wang, Jin/KAM-5595-2024; WANG, YILUN/KFB-0627-2024
FU National Natural Science Foundation of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076183, 61936014 and 61976159, in part
   by the Natural Science Foundation of Shanghai under Grant 20ZR1473500,
   in part by the Shanghai Science and Technology Innovation Action Project
   of under Grant 20511100700 and 22511105300, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, and in part by the Fundamental Research Funds for the
   Central Universities. The authors would also like to thank the anonymous
   reviewers for their careful work and valuable suggestions.
NR 0
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4099
EP 4113
DI 10.1007/s00371-023-03071-x
EA SEP 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001071597500003
DA 2024-07-18
ER

PT J
AU Long, M
   Yu, X
   Cong, S
   Zoujian, W
   Jiangbin, D
   Jiayao, Z
AF Long, Ma
   Yu, Xu
   Cong, Shu
   Zoujian, Wei
   Jiangbin, Du
   Jiayao, Zhao
TI Face image deblurring with feature correction and fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Face deblurring; Feature extraction; Feature fusion; Self-attention;
   Generative adversarial networks (GANs)
AB The key to image deblurring is to extract and screen valid information from blurred images and use this information to restore sharp images. Given this, we carefully design a face deblurring method. We adopt a simple feature extraction technique that extracts multilevel face features using a pretrained feature extraction subnetwork (FEN), e.g., VGGNet. To ensure adequate and accurate face information, we use all but the lowest-level features, which may contain much blurring information, for restoration. The selected features are preprocessed with self-attention modules (SAMs), correcting the features by synthesizing the context, and then injected into a feature fusion network (FFN) to restore clear images. The FFN stem is intentionally designed as a mirror of the FEN. Therefore, the corrected features are directly added to the corresponding (same-sized) features in the FFN naturally without manual compression/expansion. In the FFN, the features are layerwise fused and corrected again in the last layers with the SAMs; the deblurring results are finally output. Thus, the resulting images are firmly controlled by the extracted face features. This is advantageous for restoring true faces. To pursue results with both high sharpness and fidelity, the networks are trained with discriminator and fidelity constraints. Experimental results obtained on abundant challenging datasets show that our method achieves competitive results relative to state-of-the-art method outputs. Specifically, the proposed method does not need to align faces or perform iterations; thus, it is simple and practical.
C1 [Long, Ma; Yu, Xu; Cong, Shu; Zoujian, Wei; Jiangbin, Du; Jiayao, Zhao] Xian Technol Univ, Sch Comp Sci & Engn, Xian 710021, Shaanxi, Peoples R China.
   [Long, Ma; Yu, Xu; Cong, Shu; Zoujian, Wei; Jiangbin, Du; Jiayao, Zhao] State & Local Joint Lab Adv Network & Monitoring, Xian 710021, Shaanxi, Peoples R China.
C3 Xi'an Technological University
RP Long, M (corresponding author), Xian Technol Univ, Sch Comp Sci & Engn, Xian 710021, Shaanxi, Peoples R China.; Long, M (corresponding author), State & Local Joint Lab Adv Network & Monitoring, Xian 710021, Shaanxi, Peoples R China.
EM malong@xatu.edu.cn
OI Ma, Long/0000-0002-8357-424X
CR Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chrysos GG, 2019, INT J COMPUT VISION, V127, P801, DOI 10.1007/s11263-018-1138-7
   Chrysos GG, 2017, IEEE COMPUT SOC CONF, P2015, DOI 10.1109/CVPRW.2017.252
   Dosovitskiy Alexey, 2021, ICLR
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Jin MG, 2018, IEEE COMPUT SOC CONF, P858, DOI 10.1109/CVPRW.2018.00118
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lin SN, 2020, AAAI CONF ARTIF INTE, V34, P11523
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ludwiczuk B., 2016, Tech. Rep. CMU-CS-16-118
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ni J, 2011, IEEE T IMAGE PROCESS, V20, P3086, DOI 10.1109/TIP.2011.2145386
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Shen ZR, 2021, IEEE WINT CONF APPL, P3530, DOI 10.1109/WACV48630.2021.00357
   Shen ZY, 2020, INT J COMPUT VISION, V128, P1829, DOI 10.1007/s11263-019-01288-9
   Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2019, INT J COMPUT VISION, V127, P785, DOI 10.1007/s11263-019-01148-6
   Sun L., 2013, IEEE INT C COMPUTATI, P1, DOI [10.1109/ICCPhot.2013.6528301, DOI 10.1109/ICCPHOT.2013.6528301]
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Xia ZH, 2019, ADV NEUR IN, V32
   Xiang S., 2017, arXiv
   Xiaoming Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P399, DOI 10.1007/978-3-030-58545-7_23
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36
   Yasarla R, 2020, IEEE T IMAGE PROCESS, V29, P6251, DOI 10.1109/TIP.2020.2990354
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang R, 2019, PR MACH LEARN RES, V97
NR 46
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3693
EP 3707
DI 10.1007/s00371-023-03059-7
EA AUG 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001060582400001
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YP
   Yang, ZP
   Ma, B
   Wu, JH
   Jin, FS
AF Zhang, Yuping
   Yang, Zepeng
   Ma, Bo
   Wu, Jiahao
   Jin, Fusheng
TI Structural-appearance information fusion for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Siamese networks; Multi-information fusion
ID NETWORKS
AB In this work, we propose a visual tracking algorithm based on structural-appearance information fusion that aims to distinguish the target from distractors, including both semantical and visual distractors. It measures the similarity of targets using both appearance information and structural information, with the former extracted from siamese networks and the latter learned from appearance information using a target-cross attention mechanism. The structural and appearance information can be dynamically fused by using a gating recurrent unit, which can control the fusion ratio between them.Additionally, we introduce a similarity matching loss function to explicitly guide feature extraction. Our proposed method can extract discriminative features that facilitate the identification of the target, thus improving tracking performance. Extensive experimental results show that our proposed similarity feature extraction method can improve the tracking performance.
C1 [Zhang, Yuping; Yang, Zepeng; Ma, Bo; Wu, Jiahao; Jin, Fusheng] Beijing Inst Technol, Sch Comp Sci & Technol, 5 South-St Zhongguancun, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Ma, B (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, 5 South-St Zhongguancun, Beijing 100081, Peoples R China.
EM bma000@bit.edu.cn
FU National Natural Science Foundation of China [62072042]
FX AcknowledgementsThis work was supported in part by the National Natural
   Science Foundation of China under Grant 62072042.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao Y, 2022, IEEE T CIRC SYST VID, V32, P674, DOI 10.1109/TCSVT.2021.3063001
   Cao Z., 2022, P IEEE C COMP VIS PA
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20
   Chen F, 2022, COMPUT VIS IMAGE UND, V222, DOI 10.1016/j.cviu.2022.103508
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468
   Chung Junyoung, 2014, ARXIV14123555
   Cui Z, 2016, PROC CVPR IEEE, P1449, DOI 10.1109/CVPR.2016.161
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Fu C., 2021, IEEE T MULTIMEDIA
   Fu CH, 2021, IEEE INT CONF ROBOT, P510, DOI 10.1109/ICRA48506.2021.9560756
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Guizhu Shen, 2018, Procedia Computer Science, V131, P895, DOI 10.1016/j.procs.2018.04.298
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Gupta DK, 2021, PROC CVPR IEEE, P12357, DOI 10.1109/CVPR46437.2021.01218
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jian Wang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2612, DOI 10.1109/ICCV.2017.283
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y., 2015, ARXIV
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li YJ, 2019, PR MACH LEARN RES, V97
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma D, 2021, PROC CVPR IEEE, P10943, DOI 10.1109/CVPR46437.2021.01080
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Milan A., 2017, P ASS ADV ART INT
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ni JZ, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1189, DOI 10.1145/3132847.3133022
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ravanelli M, 2018, IEEE T EM TOP COMP I, V2, P92, DOI 10.1109/TETCI.2017.2762739
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song ZK, 2022, PROC CVPR IEEE, P8781, DOI 10.1109/CVPR52688.2022.00859
   TAN H, 2022, APPL INTELL, P1
   Tan QX, 2020, AAAI CONF ARTIF INTE, V34, P930
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang N, 2020, AAAI CONF ARTIF INTE, V34, P12184
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang X, 2022, 2022 6TH INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION SCIENCES (ICRAS 2022), P1, DOI 10.1109/ICRAS55217.2022.9842062
   Xiao D., 2022, APPL INTELL, P1
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   YUAN Y, 2022, P IEEE 95 VEH TECHN, P1, DOI DOI 10.1109/IUS54386.2022.9958305
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhou L., 2022, CHEM ENG J, P1
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
   Zhu XF, 2022, IEEE T MULTIMEDIA, V24, P301, DOI 10.1109/TMM.2021.3050073
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Ziang Cao, 2021, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P3086, DOI 10.1109/IROS51168.2021.9636309
NR 93
TC 1
Z9 1
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3103
EP 3117
DI 10.1007/s00371-023-03013-7
EA AUG 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001052016900002
DA 2024-07-18
ER

PT J
AU Lee, K
   Kim, T
AF Lee, Kyuhong
   Kim, Taeyong
TI Group emotion recognition based on psychological principles using a
   fuzzy system
SO VISUAL COMPUTER
LA English
DT Article
DE Group emotion recognition; Facial expression recognition; Fuzzy system;
   Unconstrained conditions
AB In recent times, group emotion recognition has attracted considerable research attention, and diverse applications such as situation-aware models and security surveillance models that require this capability have emerged. Group emotion recognition is a complex task that depends on human visual stimuli and correlation inference. We propose a novel group emotion recognition model using a fuzzy system based on psychological principles. Based on psychological factors defined by a fuzzy system, we classify group emotions into universal classes used to classify seven basic single face emotions: happiness, sadness, surprise, disgust, anger, fear, and neutral, and achieve approximately 14% higher accuracy than an average method in qualitative and quantitative experiments. Furthermore, the contribution of each factor to group emotion recognition was determined using the psychological principles commonly used in cinematography, at a low cost.
C1 [Lee, Kyuhong; Kim, Taeyong] Chung Ang Univ, Grad Sch Adv Imaging Sci Multimedia & Film, Seoul 06974, South Korea.
C3 Chung Ang University
RP Kim, T (corresponding author), Chung Ang Univ, Grad Sch Adv Imaging Sci Multimedia & Film, Seoul 06974, South Korea.
EM kimty@cau.ac.kr
RI Kim, Tae-Yong/Q-8585-2019
OI Kim, Tae-Yong/0000-0001-5598-9627
CR Carroll Matthew., 2012, Forbes
   Cutting JE, 2016, ATTEN PERCEPT PSYCHO, V78, P891, DOI 10.3758/s13414-015-1003-5
   Cutting JE, 2015, ART PERCEPT, V3, P191, DOI 10.1163/22134913-00002031
   Dhall Abhinav, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P784, DOI 10.1145/3382507.3417973
   Ekman P., 1978, Facial action coding system
   Esau N, 2007, IEEE INT CONF FUZZY, P697
   Franzoni V, 2020, MULTIMED TOOLS APPL, V79, P36063, DOI 10.1007/s11042-020-09428-x
   Frith C, 2009, PHILOS T R SOC B, V364, P3453, DOI 10.1098/rstb.2009.0142
   Guo X, 2020, IEEE WINT CONF APPL, P2910, DOI [10.1109/WACV45572.2020.9093547, 10.1109/wacv45572.2020.9093547]
   Hayamizu T, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING (ICCSCE 2012), P177, DOI 10.1109/ICCSCE.2012.6487137
   Ilbeygi M, 2012, ENG APPL ARTIF INTEL, V25, P130, DOI 10.1016/j.engappai.2011.07.004
   Khan AS, 2021, IEEE WINT CONF APPL, P1149, DOI 10.1109/WACV48630.2021.00119
   Kosti R, 2017, IEEE COMPUT SOC CONF, P2309, DOI 10.1109/CVPRW.2017.285
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li YF, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030582
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828
   Liliana DY, 2019, COGN PROCESS, V20, P391, DOI 10.1007/s10339-019-00923-0
   Liu S, 2023, INFORM SCIENCES, V619, P679, DOI 10.1016/j.ins.2022.11.076
   Lugaresi C., 2019, ARXIV
   Nicolai A, 2015, IEEE SYS MAN CYBERN, P2216, DOI 10.1109/SMC.2015.387
   Park S., 2013, IEEE INT EL DEV M IE, DOI DOI 10.1109/IEDM.2013.6724692
   Park SB, 2015, MULTIMED TOOLS APPL, V74, P6431, DOI 10.1007/s11042-014-2088-x
   pythonhosted, 2012, SCIKIT FUZZ
   Quiroz M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103749
   Shamsi S. N., 2018, P IEEE WINT APPL COM, P77
   Shi YG, 2000, IEEE INT POWER ELEC, P1160
   Surace Luca, 2017, P 19 ACM INT C MULT, P593
   Tanaka T., 2019, SNAPSHOT CANON ASIA
   Veltmeijer E. A., 2021, IEEE Trans. Affect. Comput
   Wang Y, 2022, INFORM SCIENCES, V610, P707, DOI 10.1016/j.ins.2022.08.003
   Wen ZY, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8020199
   Younis O., 2019, INT J ADV COMPUT SC, V10
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang JH, 2020, INFORM FUSION, V59, P103, DOI 10.1016/j.inffus.2020.01.011
   Zhang XG, 2021, NEUROCOMPUTING, V445, P194, DOI 10.1016/j.neucom.2021.02.047
NR 35
TC 1
Z9 1
U1 8
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3503
EP 3514
DI 10.1007/s00371-023-03048-w
EA AUG 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001049165400001
DA 2024-07-18
ER

PT J
AU Sang, QB
   Zhang, HG
   Liu, LX
   Wu, XJ
   Bovik, AC
AF Sang, Qingbing
   Zhang, Hongguo
   Liu, Lixiong
   Wu, Xiaojun
   Bovik, Alan C.
TI On the generation of adversarial examples for image quality assessment
SO VISUAL COMPUTER
LA English
DT Article
DE Image quality assessment; Adversarial example; Deep learning
AB We study the generation of adversarial examples to test, assess, and improve deep learning-based image quality assessment (IQA) algorithms. This is important since social media platforms and other providers rely on IQA models to monitor the content they ingest, and to control the quality of pictures that are shared. Unfortunately, IQA models based on deep learning are vulnerable to adversarial attacks. Combining the characteristics of IQA, we analyze several methods of generating adversarial examples in the classification field, and generate adversarial image quality assessment examples by obtaining model gradient information, image pixel information and reconstruction loss function. And we create an adversarial examples image generation tool that generates aggressive adversarial examples having good attack success rates. We hope that it can be used to help IQA researchers assess and improve the robustness of IQA.
C1 [Sang, Qingbing; Zhang, Hongguo; Wu, Xiaojun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.
   [Sang, Qingbing; Zhang, Hongguo; Wu, Xiaojun] Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi, Peoples R China.
   [Liu, Lixiong] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Bovik, Alan C.] Univ Texas Austin, Lab Image & Video Engn LIVE, Austin, TX 78712 USA.
C3 Jiangnan University; Beijing Institute of Technology; University of
   Texas System; University of Texas Austin
RP Sang, QB (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.; Sang, QB (corresponding author), Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi, Peoples R China.
EM qingbings@jiangnan.edu.cn; bovik@ece.utexas.edu
CR Co KT, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P275, DOI 10.1145/3319535.3345660
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Goodfellow I. J., 2014, ARXIV
   Han JH, 2014, IEEE GEOSCI REMOTE S, V11, P2168, DOI 10.1109/LGRS.2014.2323236
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang Z., 2020, P INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1911.07140
   Kurakin Alexey, 2017, INT C LEARN REPR
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Miyato T., 2016, ARXIV
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Snoek J., 2012, Advances in Neural Information Processing Systems, V25, DOI DOI 10.48550/ARXIV.1206.2944
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun S., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2103.07666
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wang XS, 2021, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR46437.2021.00196
   Wang Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2106.14076
   Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Yang S., 2022, ARXIV
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhu H., 2020, P IEEE CVF C COMP VI, P14143
NR 31
TC 1
Z9 1
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3183
EP 3198
DI 10.1007/s00371-023-03019-1
EA AUG 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001040862800002
DA 2024-07-18
ER

PT J
AU Baluja, S
AF Baluja, Shumeet
TI The infinite doodler: expanding textures within tightly constrained
   manifolds
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Image generation; Free hand drawing;
   Non-photorealistic rendering; Doodles
ID IMAGE; MODEL
AB Hand-drawn doodles present a difficult set of textures to model and synthesize. Unlike the typical natural images that are most often used in texture synthesis studies, the doodles examined here are characterized by the use of sharp, irregular, and imperfectly scribbled patterns, frequent imprecise strokes, haphazardly connected edges, and randomly or spatially shifting themes. The almost binary nature of the doodles examined makes it difficult to hide common mistakes such as discontinuities. Further, there is no color or shading to mask flaws and repetition; any process that relies on, even stochastic, region copying is readily discernible. To tackle the problem of synthesizing these textures, we model the underlying generation process of the doodle taking into account potential unseen, but related, expansion contexts. We demonstrate how to generate infinitely long textures, such that the texture can be extended far beyond a single image's source material. This is accomplished by creating a novel learning mechanism that is taught to condition the generation process on its own generated context-what was generated in previous steps-not just upon the original.
C1 [Baluja, Shumeet] Google Inc, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Baluja, S (corresponding author), Google Inc, Mountain View, CA 94043 USA.
EM shumeet@google.com
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Atalay V., 1992, International Journal of Pattern Recognition and Artificial Intelligence, V6, P131, DOI 10.1142/S0218001492000072
   Automatic1111, 2022, AUT 1111 STABL DIFF
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Bhagyashree V., 2020, 2020 IEEE 4th Conference on Information Communication Technology (CICT), P1
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Chang H., 2022, P COMP VIS PAT REC C, P11315
   Dhariwal P, 2021, ADV NEUR IN, V34
   Durall R., 2020, ARXIV
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0
   FUKUSHIMA K, 1986, BIOL CYBERN, V55, P5, DOI 10.1007/BF00363973
   Gal R, 2022, ARXIV
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys L. A., 2015, arXiv
   Gonthier N., 2020, GITHUB CODE HIGH RES
   Gonthier N, 2022, J MATH IMAGING VIS, V64, P478, DOI 10.1007/s10851-022-01078-y
   Goodfellow I. J., 2014, ARXIV
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hertz A., 2020, ARXIV
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li L, 2022, IEEE ACCESS, V10, P82668, DOI 10.1109/ACCESS.2022.3196021
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liu G., 2016, ARXIV
   Niklasson E., 2021, SELF ORG TEXTURES
   OpenAI, 2022, DALL INTR OUTP
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Raad L, 2018, ANN MATH SCI APPL, V3, P89
   Ramesh A., 2022, arXiv
   Rodriguez-Pardo C., 2022, ARXIV
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruiz N, 2022, ARXIV
   Saharia C., ARXIV
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song Y, 2020, ARXIV
   Song Y, 2019, ADV NEUR IN, V32
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang Y, 2018, ADV NEUR IN, V31
   Wei L. Y., 2009, EUROGRAPHICS 2009 ST, P93, DOI [DOI 10.2312/EGST.20091063, 10.2312/egst.20091063]
   Wei L.Y., 2018, STATE ART EXAMPLE BA
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wu X., 2022, ARXIV
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Yin L., 2022, ARXIV
   Yu N, 2019, PROC CVPR IEEE, P12156, DOI 10.1109/CVPR.2019.01244
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201285
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 59
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3271
EP 3283
DI 10.1007/s00371-023-03025-3
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001037161700001
DA 2024-07-18
ER

PT J
AU Bi, WJ
   Chen, M
   Wu, DL
   Lu, SL
AF Bi, Weijie
   Chen, Ming
   Wu, Dongliu
   Lu, Shenglian
TI EBStereo: edge-based loss function for real-time stereo matching
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo matching; Bilateral grid learning; Loss function; Deep learning;
   Feature fusion
AB Deep learning-based stereo matching has made significant progress, but it still faces challenges: The disparity prediction error maps of current models show that errors are concentrated primarily on object boundaries. We find that executing the smooth L1 loss function on the entire region during stereo matching model training cannot effectively address the imbalance between edge regions and flat regions, resulting in poor disparity estimates for edge regions. In this paper, a new weighted smooth L1 loss function, which considers the loss function calculation on edge regions and can yield improved accuracy, is proposed. An improved bilateral grid upsampling module is also added to the training model, and a strategy is adopted to balance the computational consumption introduced by the new loss function-weighted item, allowing for real-time inference. Extensive experiments conducted on two datasets, i.e., Scene Flow and KITTI, verify the simplicity and effectiveness of this approach. Under the condition of 33 frames per second (FPS), the endpoint error of the proposed model can be improved to 0.63. In addition, the proposed edge-based loss function can be easily embedded into many existing stereo matching networks, such as GwcNet, AANet, and PSMNet. After embedding the proposed edge-based loss function, the reduction rates of the endpoint errors of the existing models can be improved to 3.5%, 11.6%, and 27.2% for GwcNet, AANet, and PSMNet, respectively.
C1 [Bi, Weijie; Chen, Ming; Wu, Dongliu; Lu, Shenglian] Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin 541004, Peoples R China.
   [Chen, Ming; Lu, Shenglian] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Chen, M (corresponding author), Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin 541004, Peoples R China.; Chen, M (corresponding author), Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
EM wjbi2020@163.com; hustcm@hotmail.com; wdl1216@qq.com; lsl@gxnu.edu.cn
FU Natural Science Foundationof Jilin Province [62062015, 61662006]
FX The funding was provided by the Natural Science Foundationof Jilin
   Province (Grant No. 62062015, No. 61662006)
CR Bao W, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2803-x
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chen JW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982423
   Cheng Xuelian, 2020, Advances in Neural Information Processing Systems, V33
   Deng Y, 2007, IEEE T PATTERN ANAL, V29, P1068, DOI 10.1109/TPAMI.2007.1043
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Findeisen M, 2014, 2014 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P364, DOI 10.1109/CRV.2014.56
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Humenberger M., 2010, 2010 IEEE COMP SOC C, P77, DOI [10.1109/CVPRW.2010.5543769, DOI 10.1109/CVPRW.2010.5543769]
   Kendall Alex, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P66, DOI 10.1109/ICCV.2017.17
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Kingma D. P., 2014, arXiv
   Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668
   Li X, 2022, VISUAL COMPUT, V38, P3881, DOI 10.1007/s00371-021-02228-w
   Li YJ, 2019, VISUAL COMPUT, V35, P257, DOI 10.1007/s00371-018-1491-0
   Lin GC, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106107
   Liu BY, 2022, AAAI CONF ARTIF INTE, P1647
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, ISPRS ANN PHOTO REM, VII-3, P427, DOI 10.5194/isprsannals-II-3-W5-427-2015
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Seif G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1468, DOI 10.1109/ICASSP.2018.8461664
   Shamsafar Faranak, 2022, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, P2417
   Shankar K, 2022, IEEE ROBOT AUTOM LET, V7, P2305, DOI 10.1109/LRA.2022.3143895
   Sivaraman S, 2013, IEEE INT VEH SYM, P310, DOI 10.1109/IVS.2013.6629487
   Song X, 2019, LECT NOTES COMPUT SC, V11365, P20, DOI 10.1007/978-3-030-20873-8_2
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tang YC, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118573
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang Q, 2020, IEEE INT CONF ROBOT, P101, DOI [10.1109/icra40945.2020.9197031, 10.1109/ICRA40945.2020.9197031]
   Xu B., 2018, P IEEE CVF C COMP VI, P12497
   Xu GW, 2022, PROC CVPR IEEE, P12971, DOI 10.1109/CVPR52688.2022.01264
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Xu QY, 2021, IEEE SIGNAL PROC LET, V28, P613, DOI 10.1109/LSP.2021.3066125
   Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39
   Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhang SY, 2021, PROC CVPR IEEE, P5429, DOI 10.1109/CVPR46437.2021.00539
   Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zheng Zhuoran, 2021, P IEEE CVF INT C COM, P4449
NR 54
TC 1
Z9 1
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2975
EP 2986
DI 10.1007/s00371-023-03002-w
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001028383100002
DA 2024-07-18
ER

PT J
AU Han, X
   Gao, JY
   Yuan, Y
   Wang, Q
AF Han, Xu
   Gao, Junyu
   Yuan, Yuan
   Wang, Qi
TI Text kernel calculation for arbitrary shape text detection
SO VISUAL COMPUTER
LA English
DT Article
DE Instance segmentation; Text detection; Text kernel calculation;
   Arbitrary-shaped text
AB With the speedy progress of deep learning, text detection has received progressively increasing attention and considerable progress. The current mainstream approaches are usually based on instance segmentation to obtain the label of whether the pixel is text, as this can cope with arbitrary-shaped text. However, pixel-based prediction usually leads to overlapping neighboring texts, resulting in misdetection. To mitigate the above problems, we propose an approach to calculate text kernels and determine the attribution of boundary pixels. This way, all texts are labeled uniformly, facilitating model learning and effectively separating adherent texts. In addition, to cope with the complex and variable background of the text, we propose a practical feature enhancement module to handle it. The proposed module can explore different levels of features to represent text information of diverse sizes. Compared with current advanced algorithms, our method is competitive, which achieves the F1-measure of 87.3, 88.0, 82.8, 85.7, and 90.0% on the ICDAR2015, MSRA-TD500, CTW1500, Total-Text, and ICDAR2013 datasets, respectively.
C1 [Han, Xu] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shannxi, Peoples R China.
   [Han, Xu; Gao, Junyu; Yuan, Yuan; Wang, Qi] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shannxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shannxi, Peoples R China.
EM hxu04100@gmail.com; gjy3035@gmail.com; y.yuan1.ieee@gmail.com;
   crabwq@gmail.com
RI LIU, HUI/JPX-8014-2023; Li, Xinyue/JVN-4601-2024; Chen,
   Liang/JXX-7887-2024; zhang, xinyi/JWA-0980-2024; chang,
   yu/KFB-2822-2024; WANG, Bin/JGM-2639-2023; Wang, Minghao/JMD-0670-2023;
   WANG, HUI/JFA-9683-2023; wang, Xiaoming/KBB-8854-2024; LI,
   HAO/KBD-0866-2024; 韩, 旭/GLV-0834-2022; wang, yixuan/JGM-3893-2023; Liu,
   yujing/JQI-7225-2023; zhang, yueqi/JXM-4287-2024; chen,
   bin/KBQ-8114-2024; wang, hao/JKH-5890-2023; Wang, Hao/ABB-8923-2020;
   Chen, Xin/JDN-2017-2023; Wang, Jin/KAM-5595-2024; Chen,
   Xiao/KBD-1464-2024; luo, yuan/JLS-6416-2023; wang, hang/JND-8481-2023;
   Yuan, Yuan/GXW-1549-2022; LI, LI/KCJ-5600-2024; yang, yue/KCK-7870-2024;
   Yu, Yue/JWP-9103-2024; yang, rui/JHI-3328-2023; Liu,
   Yilin/JWP-9153-2024; Li, jiaqi/JOZ-6395-2023; liu, huan/JKI-3764-2023;
   wang, wenxin/JOZ-3291-2023; li, wenjing/JMP-7498-2023; zhang,
   zheng/KBQ-7815-2024
OI 韩, 旭/0000-0003-0028-3218; chen, bin/0000-0002-3398-1314; Wang,
   Hao/0000-0001-9109-6017; Yuan, Yuan/0000-0003-1860-3275; Liu,
   Yilin/0000-0002-7581-3933; 
FU National Natural Science Foundation of China [U21B2041, 61825603];
   National Key R amp;D Program of China [2020YFB2103902]
FX AcknowledgementsThis work was supported by the National Natural Science
   Foundation of China under Grant Nos. U21B2041, 61825603, National Key R
   &D Program of China 2020YFB2103902.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Cai YQ, 2021, IEEE T CIRC SYST VID, V31, P2725, DOI 10.1109/TCSVT.2020.3029167
   Cao M., 2021, IEEE T CIRC SYST VID
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Gao JY, 2020, IEEE T CIRC SYST VID, V30, P3486, DOI 10.1109/TCSVT.2019.2919139
   Gao JY, 2019, IEEE ACCESS, V7, P96424, DOI 10.1109/ACCESS.2019.2929819
   He MH, 2021, PROC CVPR IEEE, P8809, DOI 10.1109/CVPR46437.2021.00870
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kera SB., 2022, VISUAL COMPUT, V2022, P1
   Li Y, 2021, VISUAL COMPUT, ppp1
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu H, 2022, VISUAL COMPUT, V38, P3231, DOI 10.1007/s00371-022-02570-7
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu Z-Y., 2022, VIS COMP, V2022, P1
   Liu Z, 2018, IEEE ICC
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Su YC, 2023, IEEE T MULTIMEDIA, V25, P5030, DOI 10.1109/TMM.2022.3186431
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Wang H., 2022, VISUAL COMPUT, V2022, P1
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P947
   Wu H, 2017, VISUAL COMPUT, V33, P113, DOI 10.1007/s00371-015-1156-1
   Xiao HG, 2023, VISUAL COMPUT, V39, P2291, DOI 10.1007/s00371-022-02414-4
   Xu XQ, 2021, PROC CVPR IEEE, P12040, DOI 10.1109/CVPR46437.2021.01187
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yang C., 2023, IEEE T MULTIMED, V2023, P1
   Yang Chengyi, 2022, IEEE Transactions on Big Data, V2022, P1
   Yang C, 2022, IEEE T IMAGE PROCESS, V31, P2864, DOI 10.1109/TIP.2022.3141844
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yuliang L, 2017, ARXIV
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1285, DOI 10.1109/ICCV48922.2021.00134
   Zhang SX, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3152596
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 64
TC 1
Z9 1
U1 12
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2641
EP 2654
DI 10.1007/s00371-023-02963-2
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001019508700001
DA 2024-07-18
ER

PT J
AU Zhao, H
   Zheng, JR
   Shang, XK
   Zhong, W
   Liu, JY
AF Zhao, Hao
   Zheng, Jingrun
   Shang, Xiaoke
   Zhong, Wei
   Liu, Jinyuan
TI Coarse-to-fine multi-scale attention-guided network for multi-exposure
   image fusion
SO VISUAL COMPUTER
LA English
DT Article
DE Image fusion; Multi-exposure; Attention mechanism
ID ARCHITECTURE; ENSEMBLE; MODEL
AB In recent years, deep learning networks have achieved prominent success in the field of multi-exposure image fusion. However, it is still challenging to prevent color distortion and blurry edges which leads to bad visual effects. In this paper, we present a multi-scale attention-guided network for multi-exposure image fusion in a coarse-to-fine manner. The network generates multi-scale enhanced attention weight maps of images in different sizes which possess vital details and can emphasize essential regions of interest from both sides. The multi-scale structure can extract features on different scales, and the bilayer structure can extract features from different image sizes. Moreover, we designed a coarse-to-fine attention module to finally generate the weight maps; the module combines channel attention with spatial attention. Fused results will be generated under the guidance of the weight maps. Qualitative and quantitative experiments are performed on a publicly available dataset which shows our method outperforms the state-of-the-art methods in visual effect and objective analysis. Also, ablation experiments prove each part of our method has a great advantage in generating images with significant details, prominent targets, and faithful color.
C1 [Zhao, Hao; Zheng, Jingrun; Zhong, Wei] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Shang, Xiaoke] Zhejiang Univ, Sch Biomed Engn & Instrument Sci, Hangzhou, Peoples R China.
   [Liu, Jinyuan] Dalian Univ Technol, Dept Engn Mech, Dalian, Peoples R China.
C3 Dalian University of Technology; Zhejiang University; Dalian University
   of Technology
RP Shang, XK (corresponding author), Zhejiang Univ, Sch Biomed Engn & Instrument Sci, Hangzhou, Peoples R China.
EM 794229979@qq.com; sxk-1212@zju.edu.cn; jrzheng261018@gmail.com
RI Liu, Jinyuan/GNP-2535-2022
OI Liu, Jinyuan/0000-0003-2085-2676
FU National Natural Science Foundation of China [61906029]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61906029.
CR Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Chorowski J, 2015, ADV NEUR IN, V28
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Deng X, 2021, IEEE T IMAGE PROCESS, V30, P3098, DOI 10.1109/TIP.2021.3058764
   Galassi A, 2021, IEEE T NEUR NET LEAR, V32, P4291, DOI 10.1109/TNNLS.2020.3019893
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   HARSANYI JC, 1994, IEEE T GEOSCI REMOTE, V32, P779, DOI 10.1109/36.298007
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hou XL, 2021, IEEE PHOTONICS J, V13, DOI 10.1109/JPHOT.2021.3058740
   Jiang ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3783, DOI 10.1145/3503161.3547966
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ST, 2011, INFORM FUSION, V12, P74, DOI 10.1016/j.inffus.2010.03.002
   Liu J., 2022, P IEEECVF C COMPUTER, P5802
   Liu JW, 2022, HEALTH COMMUN, V37, P1401, DOI 10.1080/10410236.2021.1895417
   Liu JY, 2023, INFORM FUSION, V95, P237, DOI 10.1016/j.inffus.2023.02.027
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu JY, 2021, IEEE SIGNAL PROC LET, V28, P1818, DOI 10.1109/LSP.2021.3109818
   Liu RS, 2022, IEEE T IMAGE PROCESS, V31, P4922, DOI 10.1109/TIP.2022.3190209
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma KD, 2020, IEEE T IMAGE PROCESS, V29, P2808, DOI 10.1109/TIP.2019.2952716
   Ma KD, 2015, IEEE IMAGE PROC, P1717, DOI 10.1109/ICIP.2015.7351094
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mo Y, 2021, INFORM FUSION, V75, P41, DOI 10.1016/j.inffus.2021.04.005
   Pajares G, 2004, PATTERN RECOGN, V37, P1855, DOI 10.1016/j.patcog.2004.03.010
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu LH, 2022, AAAI CONF ARTIF INTE, P2126
   Qu LH, 2023, INFORM FUSION, V92, P389, DOI 10.1016/j.inffus.2022.12.002
   RamPrabhakar K., 2017, DEEPFUSE DEEP UNSUPE
   Shan Q, 2010, IEEE T VIS COMPUT GR, V16, P663, DOI 10.1109/TVCG.2009.92
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Wang F., 2016, ARXIV
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang JH, 2014, NEUROCOMPUTING, V135, P145, DOI 10.1016/j.neucom.2013.12.042
   Wu KL, 2023, IEEE T MULTIMEDIA, V25, P8103, DOI 10.1109/TMM.2022.3233299
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 52
TC 2
Z9 2
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1697
EP 1710
DI 10.1007/s00371-023-02880-4
EA JUN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000999829400002
DA 2024-07-18
ER

PT J
AU Yao, YY
   Li, T
   Wu, WM
   Zhang, GF
   Zheng, LP
AF Yao, Yuyou
   Li, Tao
   Wu, Wenming
   Zhang, Gaofeng
   Zheng, Liping
TI PowerHierarchy: visualization approach of hierarchical data via power
   diagram
SO VISUAL COMPUTER
LA English
DT Article
DE Power diagram; Treemap; Hierarchical data; Visualization
ID TREEMAPS
AB Voronoi treemaps are widely used for hierarchical data visualization. Existing methods calculate the visualization layouts of hierarchical data by combining the proportion optimization of weights and Lloyd's method of sites. However, this may not only produce results with large area errors but also require more time consumption. Besides, the relative visualization position of the same data element between adjacent frames in dynamic hierarchical data may be changed abruptly, resulting in unclear visual results. To this end, we propose an efficient and topological structure preserved visualization approach, called PowerHierarchy, for visualizing hierarchical data. Firstly, an improved version of the power diagram computing algorithm is introduced to generate the visualization layouts of each data element in the hierarchy. Unlike random initialization, we construct a centroidal Voronoi tessellation as input and then use a Breadth-First traversing strategy to adapt the depth information to produce visual layouts of static hierarchical data. Based on this, an updating scheme is presented for visualizing dynamic hierarchical data, where previous results are iteratively fed as inputs to initialize current layouts. Besides, the external boundary sites and their subsites are projected onto the visual boundary and then moved into the visual region with the relative position preserved. Experimental results on several datasets demonstrate the efficiency, accuracy, and topology preservation advantage of our proposed visualization approach.
C1 [Yao, Yuyou; Li, Tao; Wu, Wenming; Zheng, Liping] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Peoples R China.
   [Zhang, Gaofeng] Hefei Univ Technol, Sch Software, Hefei 230601, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Zheng, LP (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Peoples R China.
EM zhenglp@hfut.edu.cn
RI Zhang, Gaofeng/JXL-4324-2024; Li, Tao/IWU-9607-2023
OI Zhang, Gaofeng/0000-0003-0536-7226; 
FU National Natural Science Foundation of China [61972128]
FX AcknowledgementsThis study was supported in part by a grant from the
   National Natural Science Foundation of China (61972128).
CR Ahmed A.G., 2018, P C COMP GRAPH VIS C, P115, DOI [10.2312/cgvc.20181214, DOI 10.2312/CGVC.20181214]
   Aurenhammer F, 1998, ALGORITHMICA, V20, P61, DOI 10.1007/PL00009187
   AURENHAMMER F, 1987, SIAM J COMPUT, V16, P78, DOI 10.1137/0216006
   Balzer M, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P49, DOI 10.1109/INFVIS.2005.1532128
   Balzer M, 2009, 2009 6TH INTERNATIONAL SYMPOSIUM ON VORONOI DIAGRAMS (ISVD 2009), P79, DOI 10.1109/ISVD.2009.28
   Bernhardt J, 2009, 2009 6TH INTERNATIONAL SYMPOSIUM ON VORONOI DIAGRAMS (ISVD 2009), P233, DOI 10.1109/ISVD.2009.33
   Chen Y, 2017, VISUAL COMPUT, V33, P1073, DOI 10.1007/s00371-017-1373-x
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Görtler J, 2018, IEEE T VIS COMPUT GR, V24, P719, DOI 10.1109/TVCG.2017.2743959
   Gotz D., 2011, PHYS REV A, V30, P150, DOI 10.1109/TADVP.2007.896008
   Hahn Sebastian, 2014, 5th International Conference on Information Visualization Theory and Applications (IVAPP 2014). Proceedings, P50
   JOHNSON B, 1991, VISUALIZATION 91, P284
   Knauthe V., 2020, EUR IEEE VGTC C VIS, P97, DOI [10.2312/evs20201055, DOI 10.2312/EVS20201055]
   Li B, 2020, J COMB OPTIM, V39, P156, DOI 10.1007/s10878-019-00461-7
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Nocaj A, 2012, COMPUT GRAPH FORUM, V31, P855, DOI 10.1111/j.1467-8659.2012.03078.x
   Scheibel W, 2020, IVAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 3: IVAPP, P273, DOI 10.5220/0009153902730280
   Scherr W, 2020, INT FORUM DES LANG
   Sondag M, 2018, IEEE T VIS COMPUT GR, V24, P729, DOI 10.1109/TVCG.2017.2745140
   Sud Avneesh, 2010, Proceedings of the Seventh International Symposium on Voronoi Diagrams in Science and Engineering (ISVD 2010), P85, DOI 10.1109/ISVD.2010.16
   Vernier E, 2020, COMPUT GRAPH FORUM, V39, P393, DOI 10.1111/cgf.13989
   Vernier EF, 2018, 2018 SIXTH IEEE WORKING CONFERENCE ON SOFTWARE VISUALIZATION (VISSOFT), P96, DOI 10.1109/VISSOFT.2018.00018
   Wang YC, 2022, J VISUAL-JAPAN, V25, P875, DOI 10.1007/s12650-022-00830-1
   Wang YC, 2020, LECT NOTES COMPUT SC, V12221, P394, DOI 10.1007/978-3-030-61864-3_33
   Xin SQ, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982428
   Yang B., 2019, CELL, V30, P31, DOI DOI 10.3966/199115992019103005003
   ZHENG L, 2021, J HIGH ENERGY ASTROP, V97, P108, DOI DOI 10.1016/J.CAG.2021.04.007
   Zheng LP, 2019, COMPUT GRAPH-UK, V80, P29, DOI 10.1016/j.cag.2019.03.011
   Zhwan S. R. M. Z., 2021, International Journal of Science and Business, V5, P64, DOI 10.5281/zenodo.4462042
NR 30
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1499
EP 1514
DI 10.1007/s00371-023-02864-4
EA MAY 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000983529900001
DA 2024-07-18
ER

PT J
AU Khan, SS
   Sengupta, D
   Ghosh, A
   Chaudhuri, A
AF Khan, Soumya Suvra
   Sengupta, Diganta
   Ghosh, Anupam
   Chaudhuri, Atal
TI MTCNN plus plus : A CNN-based face detection algorithm inspired by MTCNN
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time system; Face detection; Deep neural network; Computer vision;
   MTCNN; Haar cascade; Dlib
AB Increasing security concerns in crowd centric topologies have raised major interests in reliable face recognition systems globally. In this context, certain deep learning frameworks have been proposed till date, for example, Haar Cascade, MTCNN, Dlib to name a few. In this communication, we propose a deep neural network for reliable face recognition in high face density images. The proposed framework is inspired by multi-task cascaded convolutional neural Networks (MTCNN) and, hence the name MTCNN++. In this framework, we have modified the layer density with increasing the neuron count. All the three internal layers of MTCNN, viz. P-Net, R-Net, and O-Net layers and observe that the modified Net-Layer MTCNN (MTCNN++) perform equally well to the MTCNN library or better. Moreover, 20% dropout has been used for tuning the framework for better recognition of the faces, both in terms of face clarity and face count. MTCNN++ exhibits better results as the preprocessing is done dynamically in contrast to the previous versions. The training of the model was done on a dataset comprising of 113,586 human faces in a bucket of 9661 images. The comprehensive dataset comprised of photographs from varied events, thereby presenting multiple human expressions. The accuracy of the model varies from 87.7% (average of 12 faces per image) to 99.7% (average of 2 images per images). The proposed framework fares better with large face count per image. MTCNN++ has further been compared to other literary proposals, and the results are appreciable.
C1 [Khan, Soumya Suvra] Meghnad Saha Inst Technol, Dept Comp Sci & Engn, Kolkata, W Bengal, India.
   [Sengupta, Diganta] Meghnad Saha Inst Technol, Dept Comp Sci & Engn & Comp Sci & Business Syst, Kolkata, W Bengal, India.
   [Ghosh, Anupam] Netaji Subhash Engn Coll, Dept Comp Sci & Engn, Kolkata, W Bengal, India.
   [Chaudhuri, Atal] Jadavpur Univ, Dept Comp Sci & Engn, Kolkata, W Bengal, India.
C3 Netaji Subhash Engineering College Kolkata; Jadavpur University
RP Sengupta, D (corresponding author), Meghnad Saha Inst Technol, Dept Comp Sci & Engn & Comp Sci & Business Syst, Kolkata, W Bengal, India.
EM soumyasuvra@msit.edu.in; sg.diganta@ieee.org;
   anupam.ghosh@rediffmail.com; atalc23@gmail.com
RI Sengupta, Diganta/T-7988-2017
CR Ali N., 2021, 2021 22 INT AR C INF
   Andrie AsmaraR., 2021, 2021 IEEE 26 INT WOR, P1, DOI DOI 10.1109/IEIT53149.2021.9587388
   Arunraja A., 2022, 2022 6th International Conference on Computing Methodologies and Communication (ICCMC), P1501, DOI 10.1109/ICCMC53470.2022.9754017
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Boughanem H., 2022, Visual Comput., V20, P22
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chou KY, 2019, CACS INT AUTOMAT CON, DOI 10.1109/cacs47674.2019.9024357
   Ejaz MS., 2019, 2019 INT C SUSTAINAB
   Elmahmudi A, 2021, VISUAL COMPUT, V37, P2023, DOI 10.1007/s00371-020-01960-z
   Enadula S. M., 2021, 2021 4 INT C ELECTR
   Ghofrani A., 2019, 2019 5 C KNOWL BAS E
   Gong LH, 2022, PHYSICA A, V593, DOI 10.1016/j.physa.2022.126907
   Gunawan A., 2019, ICACSIS 2019
   Guo Q., 2020, 13 INT C IM SIGN PRO
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   Guravaiah K., 2021, 2022 INT C INN TREND
   Gyawali D., 2020, 11 IEEE INT C COMP C
   HE J., 2021, 2021 2 INT C COMP DA
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Huang G. B., 2008, WORKSH FAC REAL LIF, P10
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Jain V., 2010, Fddb: A benchmark for face detection in unconstrained settings
   Jang Y, 2019, COMPUT VIS IMAGE UND, V182, P17, DOI 10.1016/j.cviu.2019.01.006
   Ji VS., 2020, 2020 IEEE CVF C COMP
   Jose E, 2019, INT CONF ADVAN COMPU, P608, DOI [10.1109/icaccs.2019.8728466, 10.1109/ICACCS.2019.8728466]
   Kim H., 2019, INT CONF BIG DATA
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Krizhevsky A., 2012, Advances in Neural Information Processing Systems, V25, P1106
   Liu Y, 2021, VISUAL COMPUT, V37, P1613, DOI 10.1007/s00371-020-01925-2
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lucena J. O. D., 2019, 21 S VIRT AUGM REAL
   Ma M, 2018, CHIN AUTOM CONGR, P4200, DOI 10.1109/CAC.2018.8623535
   Moschoglou S., 2017, 2017 IEEE C COMP VIS
   Nagarajan B., 2019, 2019 14 IEEE INT C A
   Rusli Muhammad Haziq, 2021, 2021 IEEE 17th International Colloquium on Signal Processing & Its Applications (CSPA), P171, DOI 10.1109/CSPA52141.2021.9377283
   Sanchez-Moreno AS, 2021, J IMAGING, V7, DOI 10.3390/jimaging7090161
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sengupta S, 2016, IEEE WINT CONF APPL
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Sikder J., 2021, 2021 INT C INT TECHN
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, Arxiv, DOI arXiv:1512.00567
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Ugail H, 2018, VISUAL COMPUT, V34, P1243, DOI 10.1007/s00371-018-1494-x
   Viola P., 2001, 8 IEEE INT C COMPUTE
   Wu CY, 2022, PHYSICA A, V605, DOI 10.1016/j.physa.2022.128017
   Wu W., 2017, 2017 INT C COMP SYST
   Xiang J, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P424, DOI 10.1109/ICISCE.2017.95
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yu BS, 2019, IEEE T IMAGE PROCESS, V28, P2490, DOI 10.1109/TIP.2018.2886790
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang X., 2019, 2019 28 WIR OPT COMM
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhou NR, 2023, SIGNAL PROCESS-IMAGE, V110, DOI 10.1016/j.image.2022.116891
   Zhou NR, 2021, INT J THEOR PHYS, V60, P1209, DOI 10.1007/s10773-021-04747-7
   Zhou N, 2021, IEEE ACCESS, V9, P5573, DOI 10.1109/ACCESS.2020.3046715
NR 59
TC 1
Z9 1
U1 5
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 899
EP 917
DI 10.1007/s00371-023-02822-0
EA APR 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967899600002
DA 2024-07-18
ER

PT J
AU Arya, AS
   Saha, A
   Mukhopadhyay, S
AF Arya, Amit Soni
   Saha, Akash
   Mukhopadhyay, Susanta
TI ADMM optimizer for integrating wavelet-patch and group-based sparse
   representation for image inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Sparse representation; Image inpainting; Wavelet based dictionary; ADMM
   optimizer; Non-local self similarity
ID K-SVD; RESTORATION
AB Recovery or filling in of missing pixels in damaged images is a challenging problem known as image inpainting. Many currently used techniques still suffer from artifacts and other visual defects. In the proposed inpainting approach, the authors have combined wavelet patch-based and group-based sparse representation learning so as to exploit the benefits of (a) multiresolution decomposition using wavelets, (b) sparsity and (c) coherence. The proposed method creates multiple dictionaries employing adaptive K-SVD (K-singular value decomposition) on wavelet decomposed components. The method also creates another dictionary employing PCA (principal component analysis) on group-based image patches. Finally, to accomplish the operation of inpainting, dictionaries of both types are integrated using the ADMM (alternating direction method of multipliers). The proposed method has been tested on images with varying degrees of degradation in terms of the percentage of missing pixels or blocks. We have rated the performance and compared proposed method with other state-of-the-art inpainting methods based on measures like the peak signal-to-noise ratio, the structural similarity index measure, and the figure of merit. The high values of the performance measures establish the efficacy of the proposed method.
C1 [Arya, Amit Soni; Saha, Akash; Mukhopadhyay, Susanta] Indian Inst Technol ISM, Depatment Comp Sci & Engn, Dhanbad 826004, Jharkhand, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (Indian School of Mines) Dhanbad
RP Arya, AS (corresponding author), Indian Inst Technol ISM, Depatment Comp Sci & Engn, Dhanbad 826004, Jharkhand, India.
EM amitsoniuoh@gmail.com; akashsaha534@gmail.com; msushanta2001@gmail.com
RI Soni Arya, Amit/AIE-5590-2022
OI Soni Arya, Amit/0000-0001-9178-7256
CR Adam T, 2021, MULTIMED TOOLS APPL, V80, P18503, DOI 10.1007/s11042-021-10583-y
   Afonso MV, 2011, IEEE T IMAGE PROCESS, V20, P681, DOI 10.1109/TIP.2010.2076294
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Bale AS., 2022, SPAT INF RES, V22, P281
   Bhawre R.R, 2014, 2014 IEEE INT C COMP, P1
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Candès E, 2006, MULTISCALE MODEL SIM, V5, P861, DOI 10.1137/05064182X
   Chang MH, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00531-5
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dar Y., 2021, HDB MATH MODELS ALGO, P1
   Demirel H, 2011, IEEE T GEOSCI REMOTE, V49, P1997, DOI 10.1109/TGRS.2010.2100401
   Fan LW, 2019, VIS COMPUT IND BIOME, V2, DOI 10.1186/s42492-019-0016-7
   Guo Q, 2018, IEEE T VIS COMPUT GR, V24, P2023, DOI 10.1109/TVCG.2017.2702738
   He JW, 2021, SIGNAL IMAGE VIDEO P, V15, P967, DOI 10.1007/s11760-020-01821-1
   He LT, 2014, IEEE T IMAGE PROCESS, V23, P5470, DOI 10.1109/TIP.2014.2362051
   Jin KH, 2015, IEEE T IMAGE PROCESS, V24, P3498, DOI 10.1109/TIP.2015.2446943
   KELLER JM, 1985, IEEE T SYST MAN CYB, V15, P580, DOI 10.1109/TSMC.1985.6313426
   Khmag A, 2023, MULTIMED TOOLS APPL, V82, P7757, DOI 10.1007/s11042-022-13569-6
   Khmag A, 2018, VISUAL COMPUT, V34, P1661, DOI 10.1007/s00371-017-1439-9
   Li XH, 2016, IEEE J-STARS, V9, P3629, DOI 10.1109/JSTARS.2016.2533547
   Li YY, 2020, SIGNAL PROCESS, V176, DOI 10.1016/j.sigpro.2020.107655
   Liu HF, 2017, IEEE T CIRC SYST VID, V27, P1909, DOI 10.1109/TCSVT.2016.2556498
   Magnier B, 2019, INT CONF ACOUST SPEE, P2407, DOI [10.1109/icassp.2019.8683357, 10.1109/ICASSP.2019.8683357]
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Mallat S., 1998, WAVELET TOUR SIGNAL
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mortazavi M, 2023, VISUAL COMPUT, V39, P3011, DOI 10.1007/s00371-022-02509-y
   Nozaripour A, 2022, MULTIMED TOOLS APPL, V81, P40849, DOI 10.1007/s11042-022-12395-0
   Qi N, 2018, IEEE T PATTERN ANAL, V40, P163, DOI 10.1109/TPAMI.2017.2663423
   Ram I, 2013, IEEE T IMAGE PROCESS, V22, P2764, DOI 10.1109/TIP.2013.2257813
   Ravishankar S, 2015, IEEE T SIGNAL PROCES, V63, P2389, DOI 10.1109/TSP.2015.2405503
   Rolet A, 2022, VISUAL COMPUT, V38, P477, DOI 10.1007/s00371-020-02029-7
   Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445
   Serra JG, 2017, IEEE T IMAGE PROCESS, V26, P3344, DOI 10.1109/TIP.2017.2681436
   Sniba F., 2022, IMAGE PROCESS ON LIN, V4, P1
   Song Q, 2020, IEEE T IMAGE PROCESS, V29, P7399, DOI 10.1109/TIP.2020.3002452
   Tirer T, 2020, IEEE T IMAGE PROCESS, V29, P6164, DOI 10.1109/TIP.2020.2988779
   Tirer T, 2019, IEEE T IMAGE PROCESS, V28, P1220, DOI 10.1109/TIP.2018.2875569
   Wang HH, 2019, MULTIMED TOOLS APPL, V78, P16945, DOI 10.1007/s11042-018-6888-2
   Wei X, 2020, IEEE T PATTERN ANAL, V42, P3119, DOI 10.1109/TPAMI.2019.2921031
   Wen BH, 2015, INT J COMPUT VISION, V114, P137, DOI 10.1007/s11263-014-0761-1
   Wu H, 2022, COMPUT VIS MEDIA, V8, P597, DOI 10.1007/s41095-021-0259-z
   Ye F., 2021, WIREL COMMUN MOB COM, P1
   Zha ZY, 2021, IEEE T IMAGE PROCESS, V30, P5223, DOI 10.1109/TIP.2021.3078329
   Zha ZY, 2022, IEEE T NEUR NET LEAR, V33, P4451, DOI 10.1109/TNNLS.2021.3057439
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P7735, DOI 10.1109/TIP.2020.3005515
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P5094, DOI 10.1109/TIP.2020.2972109
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P3254, DOI 10.1109/TIP.2019.2958309
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang J, 2014, IEEE T CIRC SYST VID, V24, P915, DOI 10.1109/TCSVT.2014.2302380
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang KF, 2022, VISUAL COMPUT, V38, P2853, DOI 10.1007/s00371-021-02160-z
   Zhang Z., 2020, IEEE T INSTRUM MEAS, V70, P1, DOI DOI 10.1109/TIM.2020.3028401
   Zhou MY, 2012, IEEE T IMAGE PROCESS, V21, P130, DOI 10.1109/TIP.2011.2160072
NR 56
TC 3
Z9 3
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 345
EP 372
DI 10.1007/s00371-023-02786-1
EA FEB 2023
PG 28
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000930314500001
DA 2024-07-18
ER

PT J
AU Sharma, V
   Tripathi, AK
   Mittal, H
   Parmar, A
   Soni, A
   Amarwal, R
AF Sharma, Vivek
   Tripathi, Ashish Kumar
   Mittal, Himanshu
   Parmar, Abhishek
   Soni, Ashutosh
   Amarwal, Rahul
TI WeedGan: a novel generative adversarial network for cotton weed
   identification
SO VISUAL COMPUTER
LA English
DT Article
DE Cotton weed identification; Deep learning; Image augmentation; Weed
   generative adversarial networks; Federated learning
ID CLASSIFICATION
AB Recently, precision weed management has emerged as a promising solution for reducing the use of herbicides which is hazardous to crops and human health. Thus, accurate identification of weed in the early stage is the urge of current agricultural practices. Despite recent progress, developing an efficient weed identification system for real field scenarios is a serious challenge. To overcome this, a number of deep learning-based methods have been introduced in the literature. However, these methods require large volume of annotated data images which is rarely available. To address this gap, in this paper, a novel WeedGan method has been introduced for generating realistic synthetic images. The proposed WeedGan adopted concept of federated learning to reduce the computational load by introducing two discriminators. Additionally, a new loss function is defined for the efficient training of the generator. Extensive experiments have been performed to validate the performance of the WeedGan. First, the quality of images generated by the WeedGan was validated on cotton weed dataset in terms of FID score, and discriminator accuracy. The results are compared against four other state-of-the-art GAN models namely, DC-GAN, W-GAN, Info-Gan, and VIT-GAN. Further, classifiers performance of the generated dataset was evaluated using seven state-of-the-art transfer learning-based methods on the original, basic augmented, and WeedGan augmented datasets. The experimental results demonstrate that the proposed WeedGan has outperformed all the considered methods by achieving FID score of 282.76. Moreover, the classification performance of WeedGan augmented dataset was recorded highest with 97.82% on testing and 99.87% training accuracy with DenseNet121.
C1 [Sharma, Vivek; Tripathi, Ashish Kumar; Parmar, Abhishek; Soni, Ashutosh; Amarwal, Rahul] Malaviya Natl Inst Technol MNIT, Jaipur, India.
   [Mittal, Himanshu] Jaypee Inst Informat Technol, Noida, India.
C3 National Institute of Technology (NIT System); Malaviya National
   Institute of Technology Jaipur; Jaypee Institute of Information
   Technology (JIIT)
RP Tripathi, AK (corresponding author), Malaviya Natl Inst Technol MNIT, Jaipur, India.
EM ashish.cse@mnit.ac.in
CR [Anonymous], 2016, ARXIV160207360
   Barbedo JGA, 2019, BIOSYST ENG, V180, P96, DOI 10.1016/j.biosystemseng.2019.02.002
   Barbedo JGA, 2018, BIOSYST ENG, V172, P84, DOI 10.1016/j.biosystemseng.2018.05.013
   Pedroso TC, 2022, LECT NOTES COMPUT SC, V13480, P168, DOI 10.1007/978-3-031-14463-9_11
   Cap QH, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106271
   Chen D., ARXIV
   Chen W, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3074119
   Chen WT, 2022, IEEE J-STARS, V15, P1591, DOI 10.1109/JSTARS.2022.3144339
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Chenyou Fan, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12307), P3, DOI 10.1007/978-3-030-60636-7_1
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Chong MJ, 2020, PROC CVPR IEEE, P6069, DOI 10.1109/CVPR42600.2020.00611
   Coleman G, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-021-03858-9
   Cotton I., PUBLICATIONS
   Cui SG, 2022, INT J IMAG SYST TECH, V32, P857, DOI 10.1002/ima.22677
   Dhakshayani J., 2022, P INT C PARADIGMS CO, P493
   Dosovitskiy A., arXiv
   Espejo-Garcia B, 2021, BIOSYST ENG, V204, P79, DOI 10.1016/j.biosystemseng.2021.01.014
   Espejo-Garcia B, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105593
   Fawakherji M, 2021, ROBOT AUTON SYST, V146, DOI 10.1016/j.robot.2021.103861
   Fei ZH, 2021, IEEE INT CONF COMP V, P1269, DOI 10.1109/ICCVW54120.2021.00147
   Feng L., ARXIV
   Gai JY, 2021, COMPUT ELECTRON AGR, V188, DOI 10.1016/j.compag.2021.106301
   Gai JY, 2020, J FIELD ROBOT, V37, P35, DOI 10.1002/rob.21897
   Ghosh M, 2022, VISUAL COMPUT, V38, P1645, DOI 10.1007/s00371-021-02094-6
   Gomaa AA, 2021, INT J ADV COMPUT SC, V12, P514
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gour M., ARXIV
   Hardy C, 2019, INT PARALL DISTRIB P, P866, DOI 10.1109/IPDPS.2019.00095
   Hasan ASMM, 2021, COMPUT ELECTRON AGR, V184, DOI 10.1016/j.compag.2021.106067
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hua S, 2022, NEURAL COMPUT APPL, V34, P9471, DOI 10.1007/s00521-021-06388-7
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin HB, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107055
   Jiqing C., 2022, VISUAL COMPUT, V38, P1
   Kamath R, 2022, COGENT ENG, V9, DOI 10.1080/23311916.2021.2018791
   Kerdegari H., ARXIV
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Khan S, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251008
   Konecny J, ARXIV
   Koonce B., 2021, Convolutional Neural Networks With Swift for Tensorflow: Image Recognition and Dataset Categorization, P109, DOI [DOI 10.1007/978-1-4842-6168-2_10, DOI 10.1007/978-1-4842-6168-210]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee K., ARXIV
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Liang CB, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13061083
   Liang M., 2022, VISUAL COMPUT, V38, P1
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Manalil S, 2017, CROP PROT, V95, P53, DOI 10.1016/j.cropro.2016.08.008
   Manu C.M., 2022, VISUAL COMPUT, V38, P1
   Mylonas N, 2022, SMART AGR TECHNOL, V2, DOI 10.1016/j.atech.2021.100028
   Nagaraju M, 2022, EXPERT SYST, V39, DOI 10.1111/exsy.12885
   Oerke EC, 2006, J AGR SCI-CAMBRIDGE, V144, P31, DOI 10.1017/S0021859605005708
   Olaniyi E., ARXIV
   Oliveira DAB, 2021, LIVEST SCI, V253, DOI 10.1016/j.livsci.2021.104700
   Olsen A, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38343-3
   OShea K., ARXIV
   Pan SQ, 2022, J INTEGR AGR, V21, P1094, DOI 10.1016/S2095-3119(21)63707-3
   Pandian JA, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11081266
   Phaphuangwittayakul A., 2022, VISUAL COMPUT, V38, P1
   Picon A, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106719
   Prakash AJ, 2023, VISUAL COMPUT, V39, P1765, DOI 10.1007/s00371-022-02443-z
   Rao J, 2022, VIRTUAL PHYS PROTOTY, V17, P1047, DOI 10.1080/17452759.2022.2086142
   Sa I, 2018, IEEE ROBOT AUTOM LET, V3, P588, DOI 10.1109/LRA.2017.2774979
   Salimans T, 2016, ADV NEUR IN, V29
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sharma V, 2022, COMPUT ELECTRON AGR, V201, DOI 10.1016/j.compag.2022.107217
   Sharma V, 2022, ARRAY-NY, V14, DOI 10.1016/j.array.2022.100164
   Simonyan K., 2014, 14091556 ARXIV
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Su JY, 2022, COMPUT ELECTRON AGR, V192, DOI 10.1016/j.compag.2021.106621
   Suh HK, 2018, BIOSYST ENG, V174, P50, DOI 10.1016/j.biosystemseng.2018.06.017
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Nguyen TD, 2017, ADV NEUR IN, V30
   Wang F., 2022, VISUAL COMPUT, V38, P1
   Wang L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020507
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang YW, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11193174
   Wu QF, 2020, IEEE ACCESS, V8, P98716, DOI 10.1109/ACCESS.2020.2997001
   Yang XJ, 2021, BIOSYST ENG, V208, P176, DOI 10.1016/j.biosystemseng.2021.05.016
   Yu Y., Frechet Inception Distance (FID) for Evaluating GANs
   Zhang LJ, 2022, APPL SOFT COMPUT, V123, DOI 10.1016/j.asoc.2022.108969
   Zhang S., 2022, VISUAL COMPUT, V38, P1
   Zhang Y., 2022, VISUAL COMPUT, P1
   Zhang Y, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.875693
   Zhao YF, 2022, IEEE ACM T COMPUT BI, V19, P1817, DOI 10.1109/TCBB.2021.3056683
   Zhuang JY, 2022, PEST MANAG SCI, V78, P521, DOI 10.1002/ps.6656
NR 90
TC 7
Z9 7
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6503
EP 6519
DI 10.1007/s00371-022-02742-5
EA DEC 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000896503900001
DA 2024-07-18
ER

PT J
AU Sun, BB
   Jiang, P
   Kong, DL
   Shen, T
AF Sun, Beibei
   Jiang, Ping
   Kong, Dali
   Shen, Ting
TI IV-Net: single-view 3D volume reconstruction by fusing features of image
   and recovered volume
SO VISUAL COMPUTER
LA English
DT Article
DE Single-view 3D reconstruction; Multi-scale convolution; Deep learning;
   Residual convolutional neural network
AB Single-view 3D reconstruction aims to recover the 3D shape from one image of an object and has attracted increasingly attention in recent years. Mostly, previous works are devoted to learning a mapping from 2 to 3D, and lack of spatial information of objects will cause inaccurate reconstruction on the details of objects. To address this issue, for single-view 3D reconstruction, we propose a novel voxel-based network by fusing features of image and recovered volume, named IV-Net. By a pre-trained baseline, it achieves image feature and a coarse volume from each image input, where the recovered volume contains spatial semantic information. Specially, the multi-scale convolutional block is designed to improve 2D encoder by extracting multi-scale image information. To recover more accurate shape and details of the object, an IV refiner is further used to reconstruct the final volume. We conduct experimental evaluations on both synthetic ShapeNet dataset and real-world Pix3D dataset, and results of comparative experiments indicate that our IV-Net outperforms state-of-the-art approaches about accuracy and parameters.
C1 [Sun, Beibei; Jiang, Ping; Kong, Dali; Shen, Ting] Hefei Univ Technol, Sch Math, Hefei 230000, Peoples R China.
C3 Hefei University of Technology
RP Jiang, P (corresponding author), Hefei Univ Technol, Sch Math, Hefei 230000, Peoples R China.
EM hfutus@hfut.edu.cn
OI Jiang, Ping/0000-0001-8448-2876
FU National Natural Science Foundation of China [11471093]
FX This work was supported by National Natural Science Foundation of China
   (Grant Number [11471093]).
CR Avetisyan A, 2019, PROC CVPR IEEE, P2609, DOI 10.1109/CVPR.2019.00272
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Chang A.X., 2015, IEEE C COMPUTER VISI, P1912
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kingma D. P., 2014, arXiv
   Li YY, 2023, VISUAL COMPUT, V39, P2223, DOI 10.1007/s00371-021-02328-7
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mandikal P, 2018, ARXIV
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Montefusco LB, 2011, IEEE T MED IMAGING, V30, P1064, DOI 10.1109/TMI.2010.2068306
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Popa AI, 2017, PROC CVPR IEEE, P4714, DOI 10.1109/CVPR.2017.501
   Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207
   Shi Z., 2021, ARXIV
   Sra M, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P191, DOI 10.1145/2993369.2993372
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wiles O., 2017, BRIT MACH VIS C
   Wu JJ, 2016, ADV NEUR IN, V29
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6
   Yagubbayli F., 2021, ARXIV
   Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w
NR 33
TC 2
Z9 2
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6237
EP 6247
DI 10.1007/s00371-022-02725-6
EA NOV 2022
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000886847000001
DA 2024-07-18
ER

PT J
AU Hosny, KM
   Zaki, MA
   Lashin, NA
   Hamza, HM
AF Hosny, Khalid M.
   Zaki, Mohamed A.
   Lashin, Nabil A.
   Hamza, Hanaa M.
TI Fast colored video encryption using block scrambling and multi-key
   generation
SO VISUAL COMPUTER
LA English
DT Article
DE Video encryption; IoT; Chaotic logistic map; Cryptography
ID IMAGE ENCRYPTION; SECURITY; CML
AB Multimedia information usage is increasing with new technologies such as the Internet of things (IoT), cloud computing, and big data processing. Video is one of the most widely used types of multimedia. Videos are played and transmitted over different networks in many IoT applications. Consequently, securing videos during transmission over various networks is necessary to prevent unauthorized access to the video's content. The existing securing schemes have limitations in terms of high resource consumption and high processing time, which are not liable to IoT devices with limited resources in terms of processor size, memory, time, and power consumption. This paper proposed a new encryption scheme for securing the colored videos. The video frames are extracted, and then, the frame components (red, green, and blue) are separated and padded by zero. Then, every frame component (channel) is split into blocks of different sizes. Then, the scrambled blocks of a component are obtained by applying a zigzag scan, rotating the blocks, and randomly changing the blocks' arrangements. Finally, a secret key produced from a chaotic logistic map is used to encrypt the scrambled frame component. Security analysis and time complexity are used to evaluate the efficiency of the proposed scheme in encrypting the colored videos. The results reveal that the proposed scheme has high-level security and encryption efficiency. Finally, a comparison between the proposed scheme and existing schemes is performed. The results confirmed that the proposed scheme has additional encryption efficiency.
C1 [Hosny, Khalid M.; Zaki, Mohamed A.; Lashin, Nabil A.; Hamza, Hanaa M.] Zagazig Univ, Fac Comp & Informat, Dept Informat Technol, Zagazig 44519, Egypt.
C3 Egyptian Knowledge Bank (EKB); Zagazig University
RP Hosny, KM (corresponding author), Zagazig Univ, Fac Comp & Informat, Dept Informat Technol, Zagazig 44519, Egypt.
EM k_hosny@yahoo.com
RI Hosny, Khalid M./B-1404-2008
OI Hosny, Khalid M./0000-0001-8065-8977; Hamza, Hanaa/0000-0003-1008-2612;
   Ameen, Mohamed/0000-0002-7102-4961
FU Science, Technology & Innovation Funding Authority (STDF); Egyptian
   Knowledge Bank (EKB)
FX Open access funding provided by The Science, Technology & Innovation
   Funding Authority (STDF) in cooperationwith The Egyptian Knowledge Bank
   (EKB). No fund is available for this study.
CR Alarifi A, 2020, IEEE ACCESS, V8, P128548, DOI 10.1109/ACCESS.2020.3008644
   [Anonymous], YUV SEQUENCES
   Asikuzzaman M, 2018, IEEE T CIRC SYST VID, V28, P2131, DOI 10.1109/TCSVT.2017.2712162
   Auyporn W., 2014, INT J SIGNAL PROCESS, DOI [10.12720/IJSPS.3.1.8-13, DOI 10.12720/IJSPS.3.1.8-13]
   Balaji R., 2011, P ASIA INT C MODELLI, DOI [10.1109/EIT.2011.5978601, DOI 10.1109/EIT.2011.5978601]
   Cai H, 2022, J REAL-TIME IMAGE PR, V19, P775, DOI 10.1007/s11554-022-01220-4
   Chai XL, 2017, SIGNAL PROCESS-IMAGE, V52, P6, DOI 10.1016/j.image.2016.12.007
   Cheng SL, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12030332
   Dixit M, 2015, 2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)
   Dolati N, 2021, MULTIMED TOOLS APPL, V80, P2319, DOI 10.1007/s11042-020-09654-3
   Duan CF, 2022, OPT LASER ENG, V150, DOI 10.1016/j.optlaseng.2021.106881
   El-Shafai W, 2022, J INF SECUR APPL, V64, DOI 10.1016/j.jisa.2021.103039
   Elkamchouchi H, 2020, IET IMAGE PROCESS, V14, P397, DOI 10.1049/iet-ipr.2018.5250
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Faragallah OS, 2022, INTELL AUTOM SOFT CO, V31, P177, DOI 10.32604/iasc.2022.019348
   Faragallah OS, 2022, J AMB INTEL HUM COMP, V13, P1215, DOI 10.1007/s12652-020-02832-z
   Gong LH, 2022, PHYSICA A, V591, DOI 10.1016/j.physa.2021.126793
   Gupta G, 2018, MICROSYST TECHNOL, V24, P2539, DOI 10.1007/s00542-017-3689-x
   Hafsa A, 2022, MULTIMED TOOLS APPL, V81, P2275, DOI 10.1007/s11042-021-11668-4
   Hosny KM, 2023, VISUAL COMPUT, V39, P1027, DOI 10.1007/s00371-021-02382-1
   Hosny KM, 2022, J AMB INTEL HUM COMP, V13, P973, DOI 10.1007/s12652-021-03675-y
   Hosny KM, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091066
   Hosny KM, 2008, J REAL-TIME IMAGE PR, V3, P97, DOI 10.1007/s11554-007-0058-5
   Huang X, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ELECTRO INFORMATION TECHNOLOGY (EIT), P369, DOI 10.1109/EIT51626.2021.9491868
   JarJar A, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-1305-7
   Kamal ST, 2021, IEEE ACCESS, V9, P37855, DOI 10.1109/ACCESS.2021.3063237
   Kaur G, 2022, VISUAL COMPUT, V38, P1027, DOI 10.1007/s00371-021-02066-w
   Kotel S., 2014, RECENT ADV TELECOMMU, P27
   Kumar RR, 2019, INT CONF ADVAN COMPU, P439, DOI [10.1109/icaccs.2019.8728443, 10.1109/ICACCS.2019.8728443]
   Li XD, 2020, MULTIMED TOOLS APPL, V79, P23995, DOI 10.1007/s11042-020-09200-1
   Lian S., 2008, Multimedia Content Encryption: Techniques And Applications
   Liu FW, 2010, COMPUT SECUR, V29, P3, DOI 10.1016/j.cose.2009.06.004
   Liu YX, 2019, NEUROCOMPUTING, V335, P238, DOI 10.1016/j.neucom.2018.09.091
   Mohamed FK, 2014, OPT LASER TECHNOL, V64, P145, DOI 10.1016/j.optlastec.2014.05.012
   Munagala V, 2021, VISUAL COMPUT, V37, P2173, DOI 10.1007/s00371-020-01978-3
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Mustafa RJ., 2014, SYST APPL TECHN C LI, V2014, P2014, DOI [10.1109/LISAT.2014.6845191, DOI 10.1109/LISAT.2014.6845191]
   Niyat AY, 2017, OPT LASER ENG, V90, P225, DOI [10.1016/j.optlaseng.2016.10:019, 10.1016/j.optlaseng.2016.10.019]
   Sethi Jagannath, 2022, Proceedings of the Seventh International Conference on Mathematics and Computing: ICMC 2021. Advances in Intelligent Systems and Computing (1412), P201, DOI 10.1007/978-981-16-6890-6_15
   Song XH, 2020, PHYSICA A, V537, DOI 10.1016/j.physa.2019.122660
   Valli D, 2017, EUR PHYS J PLUS, V132, DOI 10.1140/epjp/i2017-11819-7
   von Solms R, 2013, COMPUT SECUR, V38, P97, DOI 10.1016/j.cose.2013.04.004
   Wang XY, 2016, BIOSYSTEMS, V144, P18, DOI 10.1016/j.biosystems.2016.03.011
   Wang XY, 2015, OPT COMMUN, V342, P51, DOI 10.1016/j.optcom.2014.12.043
   Wu XJ, 2018, SIGNAL PROCESS, V148, P272, DOI 10.1016/j.sigpro.2018.02.028
   Yang ZL, 2022, OPT LASER ENG, V152, DOI 10.1016/j.optlaseng.2022.106969
   Yasser I, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22111253
   Ye GD, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.4071
NR 48
TC 6
Z9 6
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6041
EP 6072
DI 10.1007/s00371-022-02711-y
EA NOV 2022
PG 32
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000885222800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Xiu, J
   Qu, XJ
   Yu, HW
AF Xiu, Jie
   Qu, Xiujie
   Yu, Haowei
TI Double discriminative face super-resolution network with facial landmark
   heatmaps
SO VISUAL COMPUTER
LA English
DT Article
DE Face super-resolution; Generative adversarial network; Facial landmark
   Heatmaps; Deep learning
ID IMAGE SUPERRESOLUTION
AB At present, most of face super-resolution (SR) networks cannot balance the visual quality and the pixel accuracy. The networks with high objective index values often reconstruct too smooth images, while the networks which can restore texture information often introduce too much high-frequency noise and artifacts. Besides, some face super-resolution networks do not consider the mutual promotion between the extracting face prior knowledge part and the super-resolution reconstruction part. To solve these problems, we propose the double discriminative face super-resolution network (DDFSRNet). We propose a collaborative generator and two discriminators. Specifically, the collaborative generator, including the face super-resolution module (FSRM) and the face alignment module (FAM), can strengthen the reconstruction of facial key components, under the restriction of the perceptual similarity loss, the facial heatmap loss and double generative adversarial loss. We design the feature fusion unit (FFU) in FSRM, which integrates the facial heatmap features and SR features. FFU can use the facial landmarks to correct the face edge shape. Moreover, the double discriminators, including the facial SR discriminator (FSRD) and the facial landmark heatmap discriminator (FLHD), are used to judge whether face SR images and face heatmaps are from real data or generated data, respectively. Experiments show that the perceptual effect of our method is superior to other advanced methods on 4x reconstruction and fit the face high-resolution (HR) images as much as possible.
C1 [Xiu, Jie; Qu, Xiujie; Yu, Haowei] Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Xiu, J (corresponding author), Beijing Inst Technol, Sch Integrated Circuits & Elect, Beijing 100081, Peoples R China.
EM jiexiu79@163.com; quxiujie@bit.edu.cn; 18810120876@163.com
CR Baltrusaitis T, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P354, DOI 10.1109/ICCVW.2013.54
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   [曹明明 Cao Mingming], 2015, [电子与信息学报, Journal of Electronics & Information Technology], V37, P777
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jo Y, 2020, IEEE COMPUT SOC CONF, P1705, DOI 10.1109/CVPRW50498.2020.00220
   Jolicoeur-Martineau A., 2018, PREPRINT
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Liu F., 2017, TELECOMMUN ENG, V57, P957
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Ma TS, 2021, VISUAL COMPUT, V37, P925, DOI 10.1007/s00371-020-01843-3
   Shang TZ, 2020, IEEE COMPUT SOC CONF, P1778, DOI 10.1109/CVPRW50498.2020.00228
   [沈明玉 Shen Mingyu], 2019, [中国图象图形学报, Journal of Image and Graphics], V24, P1258
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Torabi H, 2016, EUR PHYS J-SPEC TOP, V225, P107, DOI 10.1140/epjst/e2016-02619-6
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44
   Xu W., 2020, IND CONTROL COMPUT, V33, P36
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yin Y, 2020, AAAI CONF ARTIF INTE, V34, P12693
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YC, 2020, IEEE COMPUT SOC CONF, P2120, DOI 10.1109/CVPRW50498.2020.00260
NR 39
TC 2
Z9 1
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5883
EP 5895
DI 10.1007/s00371-022-02701-0
EA NOV 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000877416900002
DA 2024-07-18
ER

PT J
AU Wu, P
   Gu, LP
   Yan, XF
   Xie, HR
   Wang, FL
   Cheng, G
   Wei, MQ
AF Wu, Peng
   Gu, Lipeng
   Yan, Xuefeng
   Xie, Haoran
   Wang, Fu Lee
   Cheng, Gary
   Wei, Mingqiang
TI PV-RCNN plus plus : semantical point-voxel feature interaction for 3D
   object detection
SO VISUAL COMPUTER
LA English
DT Article
DE PV-RCNN plus; 3D object detection; Point-voxel feature interaction;
   Semantic segmentation; Voxel query
ID GRAPH NEURAL-NETWORK
AB Large imbalance often exists between the foreground points (i.e., objects) and the background points in outdoor LiDAR point clouds. It hinders cutting-edge detectors from focusing on informative areas to produce accurate 3D object detection results. This paper proposes a novel object detection network by semantical point-voxel feature interaction, dubbed PV-RCNN++. Unlike most of existing methods, PV-RCNN++ explores the semantic information to enhance the quality of object detection. First, a semantic segmentation module is proposed to retain more discriminative foreground keypoints. Such a module will guide our PV-RCNN++ to integrate more object-related point-wise and voxel-wise features in the pivotal areas. Then, to make points and voxels interact efficiently, we utilize voxel query based on Manhattan distance to quickly sample voxel-wise features around keypoints. Such the voxel query will reduce the time complexity from O(N) to O(K), compared to the ball query. Further, to avoid being stuck in learning only local features, an attention-based residual PointNet module is designed to expand the receptive field to adaptively aggregate the neighboring voxel-wise features into keypoints. Extensive experiments on the KITTI dataset show that PV-RCNN++ achieves 81.60%, 40.18%, 68.21% 3D mAP on Car, Pedestrian, and Cyclist, achieving comparable or even better performance to the state-of-the-arts.
C1 [Wu, Peng; Gu, Lipeng; Yan, Xuefeng] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Xie, Haoran] Lingnan Univ, Dept Comp & Decis Sci, Lingnan, Hong Kong 999077, Peoples R China.
   [Wang, Fu Lee] Hong Kong Metropolitan Univ, Sch Sci & Technol, Ho Man Tin, Hong Kong 999077, Peoples R China.
   [Cheng, Gary] Educ Univ Hong Kong, Dept Math & Informat Technol, Ting Kok, Hong Kong 999077, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Shenzhen 518063, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Lingnan University;
   Hong Kong Metropolitan University; Education University of Hong Kong
   (EdUHK); Nanjing University of Aeronautics & Astronautics
RP Yan, XF (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM wupengupon1@gmail.com; glp1224@163.com; yxf@nuaa.edu.cn;
   hrxie@ln.edu.hk; pwang@hkmu.edu.hk; chengks@eduhk.hk;
   mingqiang.wei@gmail.com
RI liu, shanshan/JPA-0852-2023; Wang, Fu Lee/AAD-9782-2021; Wang,
   Weiyi/JZC-7841-2024; Wang, Shan/JPX-1098-2023; Wang,
   Yunwen/KBR-2884-2024; yang, zhou/KBB-6972-2024; Xie,
   Haoran/AFS-3515-2022; Yan, Xuefeng/JGL-6667-2023
OI Wang, Fu Lee/0000-0002-3976-0053; Wang, Yunwen/0009-0008-3083-7799; Xie,
   Haoran/0000-0003-0965-3617; 
FU 14th Five-Year Planning Equipment Pre-Research Program [JZX7Y20220301001
   801]; National Natural Science Foundation of China [62172218]; Free
   Exploration of Basic Research Project, Local Science and Technology
   Development Fund Guided by the Central Government of China
   [2021Szvup060]; General Program of Natural Science Foundation of
   Guangdong Province [2022A1515010170]
FX This work was supported by the 14th Five-Year Planning Equipment
   Pre-Research Program (No. JZX7Y20220301001 801), by the National Natural
   Science Foundation of China (No. 62172218), by the Free Exploration of
   Basic Research Project, Local Science and Technology Development Fund
   Guided by the Central Government of China (No. 2021Szvup060), and by the
   General Program of Natural Science Foundation of Guangdong Province (No.
   2022A1515010170).
CR Chen C., 2022, AAAI
   Chen X., 2017, PROC CVPR IEEE, V1, P3, DOI DOI 10.1109/CVPR.2017.691
   Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685
   Chen Y., 2020, CVPR, P10337
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Geiger A., 2012, CVPR
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He QD, 2022, AAAI CONF ARTIF INTE, P870
   Hinton G., 2015, arXiv preprint arXiv:1503.02531
   Ji CF, 2023, VISUAL COMPUT, V39, P4543, DOI 10.1007/s00371-022-02607-x
   Jiang TY, 2021, IEEE INT CONF ROBOT, P13408, DOI 10.1109/ICRA48506.2021.9561597
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li P, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8508702
   Luo C., 2021, PROC IEEECVF INT C C, P10488
   Ma X., 2022, arXiv
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   O. D. Team, 2020, OpenPCDet: an open-source toolbox for 3D object detection from point clouds
   Pang S, 2022, IEEE WINT CONF APPL, P3747, DOI 10.1109/WACV51458.2022.00380
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Reading C, 2021, PROC CVPR IEEE, P8551, DOI 10.1109/CVPR46437.2021.00845
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi S., 2019, ARXIV190703670, V2
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Wang YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3465455
   Wang ZT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462219
   Wei ZQ, 2022, IEEE J-STARS, V15, P2721, DOI 10.1109/JSTARS.2022.3158903
   Xia RL, 2022, J KING SAUD UNIV-COM, V34, P6008, DOI 10.1016/j.jksuci.2022.02.004
   Xie Qian, 2021, INT C COMP VIS, P3712
   Yan C, 2018, IEEE T CIRCUITS-I, V65, P1075, DOI 10.1109/TCSI.2017.2768330
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Ye MS, 2020, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR42600.2020.00170
   Yoo JH, 2020, SELECTED PAPERS FROM THE NINETEENTH BIENNIAL IEEE CONFERENCE ON ELECTROMAGNETIC FIELD COMPUTATION (IEEE CEFC 2020), DOI [10.1109/CEFC46938.2020.9451336, 10.1007/978-3-030-58583-9_43]
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zarzar J., 2019, ARXIV
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang JM, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108485
   Zhang JM, 2022, COMPUT ELECTR ENG, V98, DOI 10.1016/j.compeleceng.2022.107730
   Zhang Yabin, 2022, CVPR
   Zhang YN, 2021, AAAI CONF ARTIF INTE, V35, P3430, DOI 10.1609/aaai.v35i4.16456
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zheng CY, 2022, COMPUT GRAPH FORUM, V41, P377, DOI 10.1111/cgf.14441
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zheng W, 2021, AAAI CONF ARTIF INTE, V35, P3555
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 63
TC 5
Z9 5
U1 4
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2425
EP 2440
DI 10.1007/s00371-022-02672-2
EA SEP 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J1YY4
UT WOS:000863126900002
DA 2024-07-18
ER

PT J
AU Liu, G
   Han, J
AF Liu, Gen
   Han, Jin
TI Cross-domain object detection using minimized instance shift image-image
   translation
SO VISUAL COMPUTER
LA English
DT Article
DE Cross-domain; Image-image translation; CycleGAN; Object detection
AB With the rapid development of deep learning, the field of object detection has made a breakthrough. These techniques are data-driven, the performance of model largely depends on the dataset used for training. But the acquisition of a large-scale dataset is expensive and time-consuming. One alternative is to train the model on the expanded cross-domain dataset by image-image translation, while the existing image translation methods often fail to generalize mainly due to the instance shift which adverse to annotation reuse. In view of this problem, an image translation method based on improved CycleGAN is proposed in this paper. By the introducing of total variation loss and structural consistency loss to the full objective, the instance shifts between the source dataset and expanded cross-domain dataset is minimized, and the quality of the expanded cross-domain dataset has been greatly improved. The proposed approach is applied to the cross-domain detection in automatic driving scene, and the results demonstrate the robustness and superiority of the proposed method.
C1 [Liu, Gen; Han, Jin] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266000, Peoples R China.
C3 Shandong University of Science & Technology
RP Han, J (corresponding author), Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266000, Peoples R China.
EM lg97@sdust.edu.cnG; shnk123@163.com
FU Natural Science Foundation of Shandong Province [ZR2020KE023,
   ZR2021MD057]; China Scholarship Council
FX This work is supported by Natural Science Foundation of Shandong
   Province (ZR2020KE023) and (ZR2021MD057). Jin Han is thankful for the
   financial support from the China Scholarship Council.
CR Arruda V. F., 2019, IEEE IJCNN, P1
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Bochkovskiy A, ARXIV
   Chen T., 2016, ARXIV
   Choi J, 2019, IEEE I CONF COMP VIS, P6829, DOI 10.1109/ICCV.2019.00693
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   [高忠文 Gao Zhongwen], 2020, [汽车技术, Automobile Technology], P14
   Gatys L. A., 2015, arXiv
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Liao J., 2017, ARXIV
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Niu SL, 2020, IEEE T AUTOM SCI ENG, V17, P1611, DOI 10.1109/TASE.2020.2967415
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Semmo A., 2015, P WORKSHOP COMPUTATI, P149
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun CY., 2015, TRAFFIC SIGNS DETECT
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian Min, 2021, COMPUTER ENG APPL
   Tseng H-Y, 2020, ARXIV
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Usman M, 2020, J ADV MECH DES SYST, V14, DOI 10.1299/jamdsm.2020jamdsm0048
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu F., 2018, ARXIV
   Yu H., 2022, DAIR V2X LARGE SCALE
   Yu H., DAIR V2X LARGE SCALE
   Yuan Y, 2019, IEEE T IMAGE PROCESS, V28, P3423, DOI 10.1109/TIP.2019.2896952
   Yunhui Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P124, DOI 10.1007/978-3-030-58583-9_8
   Zhao H., 2015, arXiv
   [赵坤 Zhao Kun], 2020, [工程科学学报, Chinese Journal of Engineering], V42, P1074
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 43
TC 0
Z9 0
U1 3
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5013
EP 5026
DI 10.1007/s00371-022-02643-7
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000841133300001
DA 2024-07-18
ER

PT J
AU Bazazian, D
   Magland, B
   Grimm, C
   Chambers, E
   Leonard, K
AF Bazazian, Dena
   Magland, Bonnie
   Grimm, Cindy
   Chambers, Erin
   Leonard, Kathryn
TI Perceptually grounded quantification of 2D shape complexity
SO VISUAL COMPUTER
LA English
DT Article
DE Shape complexity; Complexity measures; 2D shapes
ID INFORMATION
AB The importance of measuring the complexity of shapes can be seen by the wide range of its application such as computer vision, robotics, cognitive studies, eye tracking, and psychology. However, it is very challenging to define an accurate and precise metric to measure the complexity of the shapes. In this paper, we explore different notions of shape complexity, drawing from established work in mathematics, computer science, and computer vision. We integrate results from user studies with quantitative analyses to identify three measures that capture important axes of shape complexity, out of a list of almost 300 measures previously considered in the literature. We then explore the connection between specific measures and the types of complexity that each one can elucidate. Finally, we contribute a dataset of both abstract and meaningful shapes with designated complexity levels both to support our findings and to share with other researchers.
C1 [Bazazian, Dena] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Magland, Bonnie] BYU, Provo, UT USA.
   [Grimm, Cindy] Oregon State Univ, Corvallis, OR 97331 USA.
   [Chambers, Erin] St Louis Univ, Dept Comp Sci, St Louis, MO USA.
   [Leonard, Kathryn] Occidental Coll, Los Angeles, CA 90041 USA.
C3 University of Bristol; Oregon State University; Washington University
   (WUSTL); Saint Louis University; Occidental College
RP Leonard, K (corresponding author), Occidental Coll, Los Angeles, CA 90041 USA.
EM dena.bazazian@bristol.ac.uk; bonnie.magland@gmail.com;
   grimmc@oregonstate.edu; erin.chambers@gmail.com; kleonard.ci@gmail.com
OI Grimm, Cindy/0000-0002-1711-7112; Leonard, Kathryn/0000-0002-0450-4682
FU NSF-AWM Advance grant; NSF [DMS-1953052, CCF-1907612, CCF-2106672,
   DBI-1759807]
FX Dena Bazazian, Bonnie Magland, and Kathryn Leonard acknowledge
   theMITSummerGeometry Institute. ErinChambers, Cindy Grimm, and Kathryn
   Leonard acknowledge the Women in Shape Modeling network and the NSF-AWM
   Advance grant. Kathryn Leonard acknowledges NSF grant DMS-1953052. Erin
   Chambers acknowledges NSF grants CCF-1907612, CCF-2106672, and
   DBI-1759807.
CR Arai K, 2012, J VISUAL-JAPAN, V15, P155, DOI 10.1007/s12650-011-0118-6
   Arslan M.F., 2021, COMPUT GRAPH-UK
   ATTNEAVE F, 1957, J EXP PSYCHOL, V53, P221, DOI 10.1037/h0043921
   Backes AR, 2010, LECT NOTES COMPUT SC, V6419, P14
   Balreira DG, 2020, PATTERN RECOGN LETT, V138, P447, DOI 10.1016/j.patrec.2020.08.011
   Bensefia A, 2019, 2019 7 INT WORKSHOP, P1
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Bober M, 2001, IEEE T CIRC SYST VID, V11, P716, DOI 10.1109/76.927426
   Bohg J, 2010, ROBOT AUTON SYST, V58, P362, DOI 10.1016/j.robot.2009.10.003
   Chambers E., 2018, RES SHAPE MODELING
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9
   Chau K.W., 1999, CONSTR MANAG ECON, V17, P473, DOI [10.1080/014461999371394, DOI 10.1080/014461999371394]
   CHAZELLE B, 1984, ACM T GRAPHIC, V3, P135, DOI 10.1145/357337.357340
   Chen Y., 2005, P MULTIMEDIA SIGNAL
   Feldman J, 2005, PSYCHOL REV, V112, P243, DOI 10.1037/0033-295X.112.1.243
   Joshi Durgesh., 2010, COMPUT AIDED DESIGN, V7, P685, DOI DOI 10.3722/CADAPS.2010.685-700
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226
   Kim SH, 2016, NEUROIMAGE, V135, P163, DOI 10.1016/j.neuroimage.2016.04.053
   Larsson LJ, 2015, ASSN WOMEN MATH, V1, P129, DOI 10.1007/978-3-319-16348-2_9
   Leonard K, 2007, INT J COMPUT VISION, V74, P183, DOI 10.1007/s11263-006-0010-3
   Leonard K, 2016, INT C PATT RECOG, P3216, DOI 10.1109/ICPR.2016.7900130
   Liu L, 2011, COMPUT AIDED DESIGN, V43, P1496, DOI 10.1016/j.cad.2011.09.002
   Luo Zijing, 2019, Lecture Notes in Computer Science, V11584, P44, DOI [10.1007/978-3-030-23541-3_4, DOI 10.1007/978-3-030-23541-3_4]
   Matsumoto T, 2019, COMPUT GRAPH-UK, V78, P108, DOI 10.1016/j.cag.2018.10.009
   Nitzken M., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2657, DOI 10.1109/ICIP.2011.6116213
   Page DL, 2003, IEEE IMAGE PROC, P229
   Panagiotakis C, 2016, PATTERN RECOGN, V53, P259, DOI 10.1016/j.patcog.2015.11.004
   Papadimitriou F., 2020, SPATIAL COMPLEXITY, P39
   Polasek T, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480519
   Radlinski Filip., 2005, KDD 05, P239
   Rajasekaran Suren Deepak, 2019, ARXIV PREPRINT ARXIV
   Rigau J, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P355, DOI 10.1109/SMI.2005.42
   Rui Hu, 2021, Advances in Neuroergonomics and Cognitive Engineering. Proceedings of the AHFE 2021 Virtual Conferences on Neuroergonomics and Cognitive Engineering, Industrial Cognitive Ergonomics and Engineering Psychology, and Cognitive Computing and Internet of Things. Lecture Notes in Networks and Systems (LNNS 259), P127, DOI 10.1007/978-3-030-80285-1_16
   Saraee E, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102949
   Volarevic N, 2005, Annals of DAAAM for 2005 & Proceedings of the 16th International DAAAM Symposium, P375
NR 35
TC 1
Z9 1
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3351
EP 3363
DI 10.1007/s00371-022-02634-8
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000837917100002
DA 2024-07-18
ER

PT J
AU Frenkel, D
   Six, N
   De Beenhouwer, J
   Sijbers, J
AF Frenkel, Daniel
   Six, Nathanael
   De Beenhouwer, Jan
   Sijbers, Jan
TI Tabu-DART: a dynamic update strategy for efficient discrete algebraic
   reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE X-ray tomography; Reconstruction algorithms; Discrete tomography;
   Limited data tomography
ID TOMOGRAPHY; SEGMENTATION; ALGORITHM; IMAGES
AB In X-ray computed tomography, discrete tomography (DT) algorithms have been successful at reconstructing objects composed of only a few distinct materials. Many DT-based methods rely on a divide-and-conquer procedure to reconstruct the volume in parts, which improves their run-time and reconstruction quality. However, this procedure is based on static rules, which introduces redundant computation and diminishes the efficiency. In this work, we introduce an update strategy framework that allows for dynamic rules and increases control for divide-and-conquer methods for DT. We illustrate this framework by introducing Tabu-DART, which combines our proposed framework with the Discrete Algebraic Reconstruction Technique (DART). Through simulated and real data reconstruction experiments, we show that our approach yields similar or improved reconstruction quality compared to DART, with substantially lower computational complexity.
C1 [Frenkel, Daniel; Six, Nathanael; De Beenhouwer, Jan; Sijbers, Jan] Univ Antwerp, Dept Phys, Univ Pl 1, B-2610 Antwerp, Belgium.
C3 University of Antwerp
RP Six, N; Sijbers, J (corresponding author), Univ Antwerp, Dept Phys, Univ Pl 1, B-2610 Antwerp, Belgium.
EM nathanael.six@uantwerpen.be; jan.sijbers@uantwerpen.be
RI Six, Nathanael/HTQ-2537-2023; Sijbers, Jan/C-4214-2011
OI Sijbers, Jan/0000-0003-4225-2487; Six, Nathanael/0000-0002-6207-481X; De
   Beenhouwer, Jan/0000-0001-5253-1274
FU FWO SBO project MetroFlex [S004217N]; FWO project [G090020N]; FWO
   [11D8319N]
FX This work is supported by the FWO SBO project MetroFlex (S004217N) and
   FWO project G090020N. Nathanael Six has a Ph.D. fellowship of the FWO
   (11D8319N). Parts of this research have been presented at iCT 2020 [33]
   and Fully3D 2021 [34].
CR AnanthaLakshmi, 2021, INT T J ENG MANAG AP, V12, P1
   Balázs P, 2007, IMAGE VISION COMPUT, V25, P1609, DOI 10.1016/j.imavis.2006.06.015
   Batenburg KJ, 2007, APPL NUMER HARMON AN, P175, DOI 10.1007/978-0-8176-4543-4_9
   Batenburg KJ, 2009, ULTRAMICROSCOPY, V109, P730, DOI 10.1016/j.ultramic.2009.01.009
   Batenburg KJ, 2011, IEEE T IMAGE PROCESS, V20, P2542, DOI 10.1109/TIP.2011.2131661
   Bleichrodt F, 2014, COMPUT VIS IMAGE UND, V129, P63, DOI 10.1016/j.cviu.2014.06.002
   Capricelli TD, 2007, APPL NUMER HARMON AN, P207, DOI 10.1007/978-0-8176-4543-4_10
   Dabravolski A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0106090
   Frenkel D., 2021, P 16 VIRTUAL INT M F, P173
   Frenkel D., 2020, 10 C IND COMP TOM IC
   Glover F., 2013, Handbook of Combinatorial Optimization, V2nd, P3261
   Goris B, 2013, ULTRAMICROSCOPY, V127, P40, DOI 10.1016/j.ultramic.2012.07.003
   Gritzmann P, 2000, SIAM J OPTIMIZ, V11, P522, DOI 10.1137/S105262349935726X
   Guo Y, 2019, IEEE T IMAGE PROCESS, V28, P4206, DOI 10.1109/TIP.2019.2907461
   HANSEN P. C., 2021, Computed Tomography: Al- gorithms, Insight, and Just Enough Theory, DOI DOI 10.1137/1.9781611976670
   HERMAN G. T., 2012, Discrete Tomography: Foundations, Algorithms, and Applications
   Herman GT, 2007, APPL NUMER HARMON AN, P1, DOI 10.1007/978-0-8176-4543-4
   Hunger R., 2005, Floating Point Operations in Matrix-vector Calculus
   Maestre-Deusto FJ, 2011, IEEE T IMAGE PROCESS, V20, P2146, DOI 10.1109/TIP.2011.2114894
   Kadu A, 2017, LECT NOTES COMPUT SC, V10502, P122, DOI 10.1007/978-3-319-66272-5_11
   Köhler T, 2004, IEEE NUCL SCI CONF R, P3961
   Lékó G, 2019, LECT NOTES COMPUT SC, V11679, P74, DOI 10.1007/978-3-030-29891-3_7
   Liu JH, 2018, J SYNCHROTRON RADIAT, V25, P1847, DOI 10.1107/S1600577518013681
   Lukic T, 2022, VISUAL COMPUT, V38, P695, DOI 10.1007/s00371-020-02044-8
   Miklos P., 2011, 2011 Proceedings of IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI 2011), P341, DOI 10.1109/CINTI.2011.6108530
   Miklos Poth, 2011, Proceedings of the 2011 IEEE 9th International Symposium on Intelligent Systems and Informatics (SISY 2011), P387, DOI 10.1109/SISY.2011.6034359
   Perelli A, 2020, SIAM J IMAGING SCI, V13, P1860, DOI 10.1137/19M1310013
   Roelandts T, 2012, ULTRAMICROSCOPY, V114, P96, DOI 10.1016/j.ultramic.2011.12.003
   Roelandts T, 2014, COMPUT VIS IMAGE UND, V126, P28, DOI 10.1016/j.cviu.2014.05.007
   Sanders T, 2016, IEEE T COMPUT IMAG, V2, P71, DOI 10.1109/TCI.2016.2521340
   Segers H, 2013, FUND INFORM, V125, P223, DOI 10.3233/FI-2013-861
   Six N, 2019, OPT EXPRESS, V27, P33671, DOI 10.1364/OE.27.033670
   Slaney M., 2001, Principles of Computerized Tomographic Imaging, DOI DOI 10.1137/1.9780898719277
   Stolk A, 2010, SIAM J DISCRETE MATH, V24, P1056, DOI 10.1137/090766693
   Tuysuzoglu A, 2015, IEEE T IMAGE PROCESS, V24, P1614, DOI 10.1109/TIP.2015.2409568
   van Aarle W, 2015, ULTRAMICROSCOPY, V157, P35, DOI 10.1016/j.ultramic.2015.05.002
   Van de Casteele E, 2018, J BONE MINER METAB, V36, P40, DOI 10.1007/s00774-017-0815-x
   Van Gompel G, 2011, MED PHYS, V38, pS36, DOI 10.1118/1.3577758
   van Lith BS, 2021, SIAM J SCI COMPUT, V43, pS173, DOI 10.1137/20M1349011
   Varga LG, 2023, VISUAL COMPUT, V39, P1557, DOI 10.1007/s00371-022-02428-y
   Varga LG, 2014, COMPUT VIS IMAGE UND, V129, P52, DOI 10.1016/j.cviu.2014.05.006
   Wang C, 2021, SIAM J IMAGING SCI, V14, P749, DOI 10.1137/20M1341490
   Wei ZH, 2018, IEEE ACCESS, V6, P7780, DOI 10.1109/ACCESS.2018.2800719
   Yang FQ, 2018, MEAS SCI TECHNOL, V29, DOI 10.1088/1361-6501/aa9a07
   Zeegers M, 2018, LECT NOTES COMPUT SC, V11255, P164, DOI 10.1007/978-3-030-05288-1_13
   Zhao Yunsong, 2018, IEEE Trans Image Process, DOI 10.1109/TIP.2018.2845098
   Zhuge XD, 2016, IEEE T IMAGE PROCESS, V25, P455, DOI 10.1109/TIP.2015.2504869
   Zisler M, 2016, IEEE T COMPUT IMAG, V2, P335, DOI 10.1109/TCI.2016.2563321
NR 48
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4671
EP 4683
DI 10.1007/s00371-022-02616-w
EA AUG 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000837917100001
OA hybrid
DA 2024-07-18
ER

PT J
AU Hu, XR
   Zhang, JY
   Huang, J
   Liang, JX
   Yu, F
   Peng, T
AF Hu, Xinrong
   Zhang, Junyu
   Huang, Jin
   Liang, JinXing
   Yu, Feng
   Peng, Tao
TI Virtual try-on based on attention U-Net
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual try-on; Cascaded attention mechanism; Cross-domain fusion; Thin
   plate spline; Reduce occlusion
AB Image-based virtual try-on, aiming to fit new in-shop clothes into a person image, has gained extensive attention in the fields of computer vision and image process community. Most of the current virtual try-on methods are based on thin plate spline transformation and composition mask. Such methods are limited by the spatial feature retention performance of the network, hard to warp clothes aligning with new body when the body shape and posture change largely, and its difficult for them to handle self-occlusion. We employ a two-stage approach, warping clothes in the clothes warping module (CWM), and generating try-on results in cross-domain fusion module (CFM). To address the problem of hard to warping clothes aligning with new body, we add a combined loss to the Clothes Warping Module, including a perceptual loss for the clothes parsing region and a L1 loss for the whole image. To solve the self-occlusion problem, firstly, we adopt an attention-based U-Net network as the backbone of the cross-domain fusion module. Then, we improved the framework of CFM to generate composition mask, adjusted clothes and rendered person, and composite the final try-on result through mask operations. Experiments on the Zalando dataset demonstrate that this work can warp clothes naturally with details preserved and produce photo-realistic try-on results without self-occlusion.
C1 [Hu, Xinrong; Zhang, Junyu; Huang, Jin; Liang, JinXing; Yu, Feng; Peng, Tao] Wuhan Text Univ, Yangguang Campus, Wuhan, Peoples R China.
C3 Wuhan Textile University
RP Huang, J (corresponding author), Wuhan Text Univ, Yangguang Campus, Wuhan, Peoples R China.
EM derick0320@foxmail.com
RI Cheng, Lin/KFQ-3111-2024; Yu, Feng/U-9998-2019; Hu,
   Xinrong/HGA-1351-2022
OI Yu, Feng/0000-0003-1913-2882; 
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Fang H.S., 2018, ARXIV PREPRINT ARXIV
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lewis KM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459884
   Li KD, 2021, PROC CVPR IEEE, P15541, DOI 10.1109/CVPR46437.2021.01529
   Minar M. R., 2020, P AS C COMP VIS
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Ren B, 2021, ARXIV PREPRINT ARXIV
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy D., 2020, ARXIV PREPRINT ARXIV
   Sattar H, 2019, IEEE WINT CONF APPL, P968, DOI 10.1109/WACV.2019.00108
   Sekine M., 2014, Int. Conf. on 3D Body Scanning Technologies, P406
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song D, 2020, MULTIMED TOOLS APPL, V79, P33757, DOI 10.1007/s11042-019-08363-w
   Trebing K, 2021, PATTERN RECOGN LETT, V145, P178, DOI 10.1016/j.patrec.2021.01.036
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
NR 36
TC 2
Z9 2
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3365
EP 3376
DI 10.1007/s00371-022-02563-6
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000825908300001
DA 2024-07-18
ER

PT J
AU Lai, Y
   Xu, HY
   Lin, C
   Luo, T
   Wang, LH
AF Lai, Yong
   Xu, Haiyong
   Lin, Chi
   Luo, Ting
   Wang, Lihong
TI A two-stage and two-branch generative adversarial network-based
   underwater image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Two-stage; Two-branch; Underwater image enhancement; Generative
   adversarial network
ID QUALITY
AB Due to the absorption and scattering of light under the water, various distortions are formed in the underwater images, which seriously limit the underwater visual task. To address this problem, a two-stage and two-branch underwater image enhancement generative adversarial network (TTE-GAN) is proposed. Specifically, considering the characteristics of the underwater images in different frequency domains in the first stage of the generator, guided filtering is used to decompose the raw underwater images, and two branches of low-frequency and high-frequency images are obtained. Since the low-frequency information represents the comprehensive measurement of the intensity of the image, the multi-scale module is used to enhance the comprehensive measurement of the image intensity. Considering that high frequency is a measure of the edge information of an image, a convolutional neural network is constructed for high-frequency information. Furthermore, due to the complexity of the underwater images, a single-step network is challenging to achieve good results. A refinement network is designed to improve the quality of the underwater image, thereby obtaining high-quality underwater images in the second stage of the generator. Finally, comprehensive experiments on three benchmarks demonstrate the validity of the proposed TTE-GAN method both subjectively and objectively.
C1 [Lai, Yong; Xu, Haiyong; Wang, Lihong] Ningbo Univ, Sch Math & Stat, Ningbo 315211, Zhejiang, Peoples R China.
   [Lin, Chi; Luo, Ting] Ningbo Univ, Coll Sci & Technol, Ningbo 315212, Peoples R China.
C3 Ningbo University; Ningbo University
RP Lin, C (corresponding author), Ningbo Univ, Coll Sci & Technol, Ningbo 315212, Peoples R China.
EM linchi_nbu@163.com
RI wang, lihong/HNS-8957-2023
FU Natural Science Foundation of China [62171243, 61971247, 61501270];
   Zhejiang Provincial Natural Science Foundation of China [LY22F020020];
   Education of Zhejiang Province [Y201839115]; Natural Science Foundation
   of Ningbo [2021J134]; K. C. Wong Magna Fund at Ningbo University
FX This work was supported by the Natural Science Foundation of China under
   Grant No. 62171243, 61971247, and 61501270, Zhejiang Provincial Natural
   Science Foundation of China under Grant No. LY22F020020, Education of
   Zhejiang Province under Grant No. Y201839115, Natural Science Foundation
   of Ningbo under Grant No.2021J134. It was also sponsored by the K. C.
   Wong Magna Fund at Ningbo University.
CR Ancuti C., 2012, PROC CVPR IEEE, P81, DOI DOI 10.1109/CVPR.2012.6247661
   Armanini C, 2022, IEEE T ROBOT, V38, P731, DOI 10.1109/TRO.2021.3094051
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen XY, 2019, IEEE T IND ELECTRON, V66, P9350, DOI 10.1109/TIE.2019.2893840
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Ju MY, 2017, VISUAL COMPUT, V33, P1613, DOI 10.1007/s00371-016-1305-1
   King DB, 2015, ACS SYM SER, V1214, P1
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li HY, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116248
   Li HY, 2019, IEEE DATA COMPR CONF, P589, DOI 10.1109/DCC.2019.00101
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liu K, 2021, OPT EXPRESS, V29, P10321, DOI 10.1364/OE.413164
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   LIU YC, 1995, IEEE T CONSUM ELECTR, V41, P460, DOI 10.1109/30.468045
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Pan X, 2021, IEEE T WIREL COMMUN, V20, P3958, DOI 10.1109/TWC.2021.3054745
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Peng Lintao, 2021, ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.2111.11843, DOI 10.48550/ARXIV.2111.11843]
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Perez J, 2017, LECT NOTES COMPUT SC, V10338, P183, DOI 10.1007/978-3-319-59773-7_19
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Prabhakar CJ., 2012, INT J MACHINE INTELL, V4, P217
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, CORR
   Singh G, 2015, IEEE UNDERWATER TECH
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang S, 2017, NEUROCOMPUTING, V245, P1, DOI 10.1016/j.neucom.2017.03.029
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou Y., 2020, DOMAIN ADAPTIVE ADVE
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 49
TC 5
Z9 5
U1 4
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4133
EP 4147
DI 10.1007/s00371-022-02580-5
EA JUL 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000824323400001
DA 2024-07-18
ER

PT J
AU Luchetti, A
   Zanetti, M
   Kalkofen, D
   De Cecco, M
AF Luchetti, A.
   Zanetti, M.
   Kalkofen, D.
   De Cecco, M.
TI Stabilization of spherical videos based on feature uncertainty
SO VISUAL COMPUTER
LA English
DT Article
DE Video stabilization; 360 degrees video; Particle swarm optimization;
   Chauvenet's criterion; Uncertainty estimation; Shaking
ID MOTION
AB Nowadays the trend is to acquire and share information in an immersive and natural way with new technologies such as Virtual Reality (VR) and 360 degrees video. However, the use of 360 degrees video, even more the use of VR head-mounted display, can generate general discomfort ("cybersickness") and one factor is the video shaking. In this work, we developed a method to make the viewing of 360 degrees video smoother and more comfortable to watch. First, the rotations are obtained with an innovative technique using a Particle Swarm Optimization algorithm considering the uncertainty estimation among features. In addition, a modified Chauvenet criterion is used to find and suppress outliers features from the algorithm. Afterward, a time-weighted color filter is applied to each frame in order to handle also videos with small translational jitter, rolling shutter wobble, parallax, and lens deformation. Thanks to our complete offline stabilization process, we achieved good-quality results in terms of video stabilization. Achieving better robustness compared to other works. The method was validated using virtual and real 360 degrees video data of a mine environment acquired by a drone. Finally, a user study based on a subjective and standard Simulator Sickness Questionnaire was submitted to quantify simulator sickness before and after the stabilization process. The questionnaire underlined alleviation of cybersickness using stabilized videos with our approach
C1 [Luchetti, A.; Zanetti, M.; De Cecco, M.] Univ Trento, Dept Ind Engn, Sommar 9, I-38123 Trento, Italy.
   [Kalkofen, D.] Graz Univ Technol, Inst Comp Graph & Vis, Rechbauerstr 12, A-8010 Graz, Austria.
C3 University of Trento; Graz University of Technology
RP Luchetti, A (corresponding author), Univ Trento, Dept Ind Engn, Sommar 9, I-38123 Trento, Italy.
EM alessandro.luchetti@unimi.it; matteo.zanetti@unitn.it;
   kalkofen@icg.tugraz.at; mariolino.dececco@unitn.it
RI Luchetti, Alessandro/JLL-8935-2023
OI Luchetti, Alessandro/0000-0003-0960-0996
FU EIT Raw Materials
FX This paper was developed inside the European project MiReBooks Mixed
   Reality Handbooks for Mining Education, a project funded by EIT Raw
   Materials.
CR Argyriou L., 2017, 3 ANN INT C IMM LEAR
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bell J., 1995, The Investigation and Application of Virtual Reality as an Educational Tool
   Blaedel W.J., 1951, J. Chem. Ecol, V28, P643
   Bonato F, 2009, AVIAT SPACE ENVIR MD, V80, P941, DOI 10.3357/ASEM.2394.2009
   Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   Buehler C, 2001, PROC CVPR IEEE, P609
   Butaslac I, 2020, LECT NOTES COMPUT SC, V12243, P165, DOI 10.1007/978-3-030-58468-9_13
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Google Inc, REND OMN DIR STER CO
   Guervos E., 2019, ELECT IMAGING, P217
   Hebbel-Seeger A., 2017, ATHENS J SPORTS, V4, P243, DOI [10.30958/ajspo.4.4.1, DOI 10.30958/AJSPO.4.4.1]
   Huang T.S., 2002, ADV IMAGE PROCESSING, P331, DOI DOI 10.1142/9789812776952_0013
   KABSCH W, 1976, ACTA CRYSTALLOGR A, V32, P922, DOI 10.1107/S0567739476001873
   Kamali M., 2011, IAPR MVA, V1, P2
   Kamranian Z, 2021, APPL INTELL, V51, P3581, DOI 10.1007/s10489-020-01982-z
   Kasahara S., 2015, Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video. TVX'15, P33, DOI DOI 10.1145/2745197.2745202
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kopf Johannes, 2016, ACM Transactions on Graphics, V35, DOI 10.1145/2980179.2982405
   Lai W.-S., SEMANTIC DRIVEN GENE
   Li SJ, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P1368, DOI [10.1109/ITNEC.2019.8729084, 10.1109/itnec.2019.8729084]
   Litleskare S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02436
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Luchetti Alessandro, 2019, 2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI). Proceedings, P51, DOI 10.1109/RTSI.2019.8895555
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Nhat-Tan Nguyen, 2010, International Journal of Intelligent Systems Technologies and Applications, V9, P228, DOI 10.1504/IJISTA.2010.036578
   Perozzi G, 2018, J FRANKLIN I, V355, P4809, DOI 10.1016/j.jfranklin.2018.04.042
   Raptis GE, 2018, INT J HUM-COMPUT ST, V114, P69, DOI 10.1016/j.ijhcs.2018.02.003
   Shen LC, 2018, IEEE IMAGE PROC, P3184, DOI 10.1109/ICIP.2018.8451037
   Stanney KM, 1997, PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 41ST ANNUAL MEETING, 1997, VOLS 1 AND 2, P1138, DOI 10.1177/107118139704100292
   Tang CZ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3211889
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Trelea IC, 2003, INFORM PROCESS LETT, V85, P317, DOI 10.1016/S0020-0190(02)00447-7
   Walter H., 2019, APAL COUPLING STUDY
   Zhang XB, 2011, IEEE T ROBOT, V27, P1167, DOI 10.1109/TRO.2011.2162765
NR 36
TC 0
Z9 0
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4103
EP 4116
DI 10.1007/s00371-022-02578-z
EA JUL 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000825006500001
OA hybrid
DA 2024-07-18
ER

PT J
AU Jiang, XD
   Zhu, LF
   Liu, J
   Song, AG
AF Jiang, Xudong
   Zhu, Lifeng
   Liu, Jia
   Song, Aiguo
TI A SLAM-based 6DoF controller with smooth auto-calibration for virtual
   reality
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual; augmented reality; 3D interaction; 6DoF input; SLAM
ID ROBUST
AB The ability to track handheld devices in three-dimensional (3D) space is basic and critical in virtual reality (VR) and other systems. Although outside-in tracking solutions for six-degrees-of-freedom (6DoF) input are stable, such passive tracking systems are often expensive and require additional space and time to set up the base stations. We focus on an inside-out 6DoF input prototype based on Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM), which is easy to deploy. For a long-term use of the inside-out tracker, auto-calibration is required to deal with the pose jump caused by relocalization or loop closing in VI-SLAM. We study the auto-calibration for the interaction in virtual reality to check if the pose jump interrupts the user experience. We further introduce a pose smoothing method to make the interaction more natural and coherent. Our user experiments have validated that the proposed inside-out input device with smooth auto-calibration had the comparable performance with existing commercial controllers for 6DoF VR interaction.
C1 [Jiang, Xudong; Zhu, Lifeng; Song, Aiguo] Southeast Univ, Dept Instrument Sci & Engn, Jiangsu Key Lab Remote Measurement & Control, State Key Lab Bioelect, Sipailou 2, Nanjing 210096, Jiangsu, Peoples R China.
   [Zhu, Lifeng; Liu, Jia; Song, Aiguo] Nanjing Univ Informat Sci & Technol, CICAEET, Nanjing, Jiangsu, Peoples R China.
C3 Southeast University - China; Nanjing University of Information Science
   & Technology
RP Zhu, LF (corresponding author), Southeast Univ, Dept Instrument Sci & Engn, Jiangsu Key Lab Remote Measurement & Control, State Key Lab Bioelect, Sipailou 2, Nanjing 210096, Jiangsu, Peoples R China.; Zhu, LF (corresponding author), Nanjing Univ Informat Sci & Technol, CICAEET, Nanjing, Jiangsu, Peoples R China.
EM 220193276@seu.edu.cn; lfzhulf@gmail.com
RI Wang, Ling/AGR-4917-2022; zhu, lifeng/IST-2069-2023
OI Wang, Ling/0000-0003-0272-2974; 
FU NSFC [62133009, 92148205]; Natural Science Foundation of Jiangsu
   Province [BK20211159]; Nanjing National Commission on Health and Family
   Planning [ZKX19042]
FX The authors would like to thank anonymous reviewers for their valuable
   comments. This work has been supported by the NSFC under Grants
   No.62133009 and 92148205, the Natural Science Foundation of Jiangsu
   Province under Grants No. BK20211159, and the Nanjing National
   Commission on Health and Family Planning No.ZKX19042.
CR [Anonymous], MOT CAPT HARDW
   Babic T, 2016, SUI'18: PROCEEDINGS OF THE 2018 SYMPOSIUM ON SPATIAL USER INTERACTION, P2, DOI 10.1145/3267782.3267785
   Berg L.P., 2015, P 3 ACM S SPATIAL US, P6978
   Besancon L, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P4727, DOI 10.1145/3025453.3025863
   BRADLEY JV, 1958, J AM STAT ASSOC, V53, P525, DOI 10.2307/2281872
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Campos C., 2020, ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM
   Casiez Gery, 2012, P SIGCHI C HUM FACT
   Casterson S, 2016, HTC VIVE GUIDE BEGIN, V1
   Chalmers A, 2009, VISUAL COMPUT, V25, P1101, DOI 10.1007/s00371-009-0389-2
   Chan LW, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3001, DOI 10.1145/2702123.2702464
   Eubanks JC, 2020, INT SYM MIX AUGMENT, P54, DOI 10.1109/ISMAR50242.2020.00025
   Facebook, 2021, OC RIFTS
   Hattori K., 2020, ACM SIGGRAPH 2020 PO, P12
   He M, 2020, VISUAL COMPUT, V36, P1053, DOI 10.1007/s00371-019-01714-6
   HTC, VIV FOC SER
   Hu P, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356490
   Imran J, 2020, VISUAL COMPUT, V36, P1233, DOI 10.1007/s00371-019-01725-3
   Ishii H, 2008, COMMUN ACM, V51, P32, DOI 10.1145/1349026.1349034
   Kang N, 2019, VISUAL COMPUT, V35, P849, DOI 10.1007/s00371-019-01678-7
   Katzakis N, 2009, EIGHTH IEEE INTERNATIONAL CONFERENCE ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING, PROCEEDINGS, P345, DOI 10.1109/DASC.2009.76
   Kilteni K, 2012, PRESENCE-TELEOP VIRT, V21, P373, DOI 10.1162/PRES_a_00124
   Kim D, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P167
   La Viola J. J.  Jr., 2000, SIGCHI Bulletin, V32, P47, DOI 10.1145/333329.333344
   Liu FL, 2017, CHIN CONT DECIS CONF, P2127, DOI 10.1109/CCDC.2017.7978867
   Liu MY, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P63, DOI 10.1145/2807442.2807489
   Lu W, 2016, IEEE SIGNAL PROC LET, V23, P1188, DOI 10.1109/LSP.2016.2590470
   MACKENZIE IS, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P488
   Madgwick Sebastian O H, 2011, IEEE Int Conf Rehabil Robot, V2011, P5975346, DOI 10.1109/ICORR.2011.5975346
   Medeiros D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P327, DOI 10.1145/2993369.2996348
   Miao RH, 2022, VISUAL COMPUT, V38, P2207, DOI 10.1007/s00371-021-02278-0
   Mohr P, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300815
   Niehorster DC, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517708205
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rahim MA, 2020, VISUAL COMPUT, V36, P1559, DOI 10.1007/s00371-019-01758-8
   Seo M, 2021, VISUAL COMPUT, V37, P2783, DOI 10.1007/s00371-021-02179-2
   Serrano A, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417773
   Shaer Orit, 2010, Foundations and Trends in Human-Computer Interaction, V3, P1, DOI 10.1561/1100000026
   Shiratori T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964926
   Singha J, 2018, NEURAL COMPUT APPL, V29, P1129, DOI 10.1007/s00521-016-2525-z
   Smith S. P., 2004, Virtual Reality, V8, P55, DOI 10.1007/s10055-004-0137-x
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Tschiedel M, 2022, VISUAL COMPUT, V38, P2635, DOI 10.1007/s00371-021-02138-x
   Vanukuru R., 2020, USE MOBILE PHONE APP
   Vicon, MOT CAPT HARDW
   Wormell D., 2007, EGVE SHORT PAPERS PO
   Zenner A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P47, DOI [10.1109/vr.2019.8798143, 10.1109/VR.2019.8798143]
NR 47
TC 7
Z9 7
U1 8
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 3873
EP 3886
DI 10.1007/s00371-022-02530-1
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000810352600005
DA 2024-07-18
ER

PT J
AU Cai, G
   Zhu, Y
   Wu, Y
   Jiang, XB
   Ye, JY
   Yang, DW
AF Cai, Gan
   Zhu, Yu
   Wu, Yue
   Jiang, Xiaoben
   Ye, Jiongyao
   Yang, Dawei
TI A multimodal transformer to fuse images and metadata for skin disease
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Skin disease; Deep learning; Transformer; Multimodal fusion; Attention
AB Skin disease cases are rising in prevalence, and the diagnosis of skin diseases is always a challenging task in the clinic. Utilizing deep learning to diagnose skin diseases could help to meet these challenges. In this study, a novel neural network is proposed for the classification of skin diseases. Since the datasets for the research consist of skin disease images and clinical metadata, we propose a novel multimodal Transformer, which consists of two encoders for both images and metadata and one decoder to fuse the multimodal information. In the proposed network, a suitable Vision Transformer (ViT) model is utilized as the backbone to extract image deep features. As for metadata, they are regarded as labels and a new Soft Label Encoder (SLE) is designed to embed them. Furthermore, in the decoder part, a novel Mutual Attention (MA) block is proposed to better fuse image features and metadata features. To evaluate the model's effectiveness, extensive experiments have been conducted on the private skin disease dataset and the benchmark dataset ISIC 2018. Compared with state-of-the-art methods, the proposed model shows better performance and represents an advancement in skin disease diagnosis.
C1 [Cai, Gan; Zhu, Yu; Wu, Yue; Jiang, Xiaoben; Ye, Jiongyao] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Yang, Dawei] Fudan Univ, Zhongshan Hosp, Dept Pulm & Crit Care Med, Shanghai 200032, Peoples R China.
   [Yang, Dawei] Shanghai Engn Res Ctr Internet Things Resp Med, Shanghai 200032, Peoples R China.
C3 East China University of Science & Technology; Fudan University
RP Zhu, Y (corresponding author), East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.; Yang, DW (corresponding author), Fudan Univ, Zhongshan Hosp, Dept Pulm & Crit Care Med, Shanghai 200032, Peoples R China.; Yang, DW (corresponding author), Shanghai Engn Res Ctr Internet Things Resp Med, Shanghai 200032, Peoples R China.
EM zhuyu@ecust.edu.cn; yang_dw@hotmail.com
RI fang, li/JNS-8415-2023
OI Zhu, Yu/0000-0003-1535-6520; Jiang, Xiaoben/0000-0002-9454-0037
FU Science and Technology Commission of Shanghai Municipality [20DZ2254400,
   21DZ2200600]; National Scientific Foundation of China [82170110];
   Zhongshan Hospital Clinical Research Foundation [2019ZSGG15]; Shanghai
   Pujiang Program [20PJ1402400]
FX This research is supported in part by Science and Technology Commission
   of Shanghai Municipality (20DZ2254400, 21DZ2200600), National Scientific
   Foundation of China (82170110), Zhongshan Hospital Clinical Research
   Foundation(2019ZSGG15), and Shanghai Pujiang Program (20PJ1402400).
CR Chen C-F., ARXIV PREPRINT ARXIV
   Codella Noel, 2019, arXiv
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gao X, 2023, VISUAL COMPUT, V39, P1583, DOI 10.1007/s00371-022-02430-4
   Gessert N, 2020, METHODSX, V7, DOI 10.1016/j.mex.2020.100864
   González-Díaz I, 2019, IEEE J BIOMED HEALTH, V23, P547, DOI 10.1109/JBHI.2018.2806962
   Gu YY, 2020, IEEE J BIOMED HEALTH, V24, P1379, DOI 10.1109/JBHI.2019.2942429
   Hao Y., 2017, P 55 ANN M ASS COMPU, V1
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hoehn J, 2021, J MED INTERNET RES, V23, DOI 10.2196/20708
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HW, 2021, J DERMATOL, V48, P310, DOI 10.1111/1346-8138.15683
   Javed R, 2019, I C DEV ESYST ENG, P164, DOI 10.1109/DeSE.2019.00039
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kawahara J, 2019, IEEE J BIOMED HEALTH, V23, P538, DOI 10.1109/JBHI.2018.2824327
   Khan MA, 2019, 2019 INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCES (ICCIS), P63, DOI 10.1109/iccisci.2019.8716400
   Kim JH, 2016, ADV NEUR IN, V29
   Kim JH, 2018, ADV NEUR IN, V31
   Liu QD, 2020, IEEE T MED IMAGING, V39, P3429, DOI 10.1109/TMI.2020.2995518
   Liu Z., ARXIV PREPRINT ARXIV
   Mohamed E.H., 2019, 2019 9 INT C INT COM
   Ningrum DNA, 2021, J MULTIDISCIP HEALTH, V14, P877, DOI 10.2147/JMDH.S306284
   Pacheco AGC, 2021, IEEE J BIOMED HEALTH, V25, P3554, DOI 10.1109/JBHI.2021.3062002
   Phung SL, 2005, IEEE T PATTERN ANAL, V27, P148, DOI 10.1109/TPAMI.2005.17
   Salah KB., 2021, VISUAL COMPUT, V38, P1
   Serte S, 2019, COMPUT BIOL MED, V113, DOI 10.1016/j.compbiomed.2019.103423
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song L, 2020, IEEE J BIOMED HEALTH, V24, P2912, DOI 10.1109/JBHI.2020.2973614
   Tan M., 2019, INT C MACHINE LEARNI
   Tang P, 2020, IEEE J BIOMED HEALTH, V24, P2870, DOI 10.1109/JBHI.2020.2977013
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W., ARXIV PREPRINT ARXIV
   Xiao H., 2022, VISUAL COMPUT, P114
   Xiong C., 2016, INT C MACHINE LEARNI
   Yang J., ARXIV PREPRINT ARXIV, DOI DOI 10.1002/cssc.202001216
   Yu Z., 2017, P IEEE INT C COMPUTE
   Zhang JP, 2019, MED IMAGE ANAL, V54, P10, DOI 10.1016/j.media.2019.02.010
   Zhang Z., ARXIV PREPRINT ARXIV
NR 39
TC 34
Z9 34
U1 18
U2 75
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2781
EP 2793
DI 10.1007/s00371-022-02492-4
EA MAY 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000791065400002
PM 35540957
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Lim, J
   Park, JY
   Park, HM
AF Lim, Jaechan
   Park, Jin-Young
   Park, Hyung-Min
TI Minimax Monte Carlo object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Bhattacharyya distance; Minimax; Object tracking; Particle filtering;
   Risk function
ID PERFORMANCE; FILTER; VIDEO
AB We propose a new approach for visual object tracking based on a combined method of minimax estimator and sequential Monte Carlo filtering. The proposed approach adopts a minimax strategy in the standard particle filtering framework for the problem. Particle filtering is based on probabilistic methodology, while a minimax estimator belongs to deterministic approaches. Experiments show outperforming results of the proposed approach compared to the standard particle filtering in terms of tracking accuracy. We also investigate the computational complexity of the proposed algorithm in terms of elapsed processing time. In this paper, we focus on the particle filtering framework only for the performance comparison between the two methods.
C1 [Lim, Jaechan] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
   [Lim, Jaechan; Park, Jin-Young; Park, Hyung-Min] Sogang Univ, Dept Elect Engn, Seoul 04107, South Korea.
C3 University of Michigan System; University of Michigan; Sogang University
RP Park, HM (corresponding author), Sogang Univ, Dept Elect Engn, Seoul 04107, South Korea.
EM jaechan@umich.edu; wlsdud7907@sogang.ac.kr; hpark@sogang.ac.kr
OI Park, Hyung-Min/0000-0002-7105-5493
FU National Research Foundation (NRF) - Korea Government (MSIT)
   [NRF-2019R1I1A1A01058976]; Korea Agency for Infrastructure Technology
   Advancement (KAIA) - Ministry of the Interior and Safety
   [22PQWO-C153369-04]
FX This work was supported by Basic Science Research Program through the
   National Research Foundation (NRF) funded by the Korea Government (MSIT)
   (No. NRF-2019R1I1A1A01058976) and the Korea Agency for Infrastructure
   Technology Advancement (KAIA) Grant funded by the Ministry of the
   Interior and Safety (Grant 22PQWO-C153369-04).
CR Agarwal A., 2017, Technical report
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   [Anonymous], 2012, Computation Time Comparison between Matlab and C++ Using Launch Windows
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Barina, 2016, ARXIV PREPRINT ARXIV
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Bhat PG, 2020, IEEE SENS J, V20, P2405, DOI 10.1109/JSEN.2019.2954331
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bu F, 2016, MULTIPLE OBJECT TRAC
   Chahyati D, 2017, PROCEDIA COMPUT SCI, V124, P167, DOI 10.1016/j.procs.2017.12.143
   Choi J., 2017, Visual tracking by reinforced decision making'
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Engineer P, 2020, J REAL-TIME IMAGE PR, V17, P1117, DOI 10.1007/s11554-018-0841-5
   FOTOUHI M., 2011, 2011 7 IRANIAN C MAC, P1
   Gunjal PR, 2018, 2018 INTERNATIONAL CONFERENCE ON ADVANCES IN COMMUNICATION AND COMPUTING TECHNOLOGY (ICACCT), P544, DOI 10.1109/ICACCT.2018.8529402
   Ha ND, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/8839725
   Hariyono J, 2014, SCI WORLD J, DOI 10.1155/2014/196415
   Harmon M.E., 1997, REINFORCEMENT LEARNI
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Kay S. M., 1993, FUNDAMENTALS STAT SI
   Kim DS, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010023
   Kwame A.B., 2019, 25 INT C AUT COMP IC, P1
   Li X, 2004, IEEE SYS MAN CYBERN, P3105, DOI 10.1109/ICSMC.2004.1400816
   Li Y, 2008, IEEE T PATTERN ANAL, V30, P1728, DOI 10.1109/TPAMI.2008.73
   Lim J., NONLINEAR DYNAM
   Lim J, 2020, INT J ROBUST NONLIN, V30, P636, DOI 10.1002/rnc.4785
   Luo W.H., 2018, PR MACH LEARN RES
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mihaylova L., 2007, Advances and Challenges in Multisensor Data and Information Processing, V8, P260
   NAJAFZADEH N., 2015, 2015 23 IRANIAN C EL, P781
   Nordsjo AE, 2004, RADAR CONF, P123, DOI 10.1109/NRC.2004.1316407
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Pal SK, 2021, APPL INTELL, V51, P6400, DOI 10.1007/s10489-021-02293-7
   Panda Jyotiranjan, 2020, Advances in Electrical Control and Signal Systems. Select Proceedings of AECSS 2019. Lecture Notes in Electrical Engineering (LNEE 665), P945, DOI 10.1007/978-981-15-5262-5_73
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   PoormehdiGhaemmaghami, 2017, TRACKING HUMANS VIDE
   Sun W, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/3805320
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian M, 2007, LECT NOTES COMPUT SC, V4843, P355
   Wagenaar DA, 2010, NEUROINFORMATICS, V8, P33, DOI 10.1007/s12021-010-9062-1
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang Y, 2019, IEEE ACCESS, V7, P133694, DOI 10.1109/ACCESS.2019.2941365
   Yang MW, 2022, VISUAL COMPUT, V38, P625, DOI 10.1007/s00371-020-02038-6
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   ZHAO A., 2007, 9 BIENNIAL C AUSTR P, P45, DOI [10.1109/dicta.2007.4426774, DOI 10.1109/DICTA.2007.4426774]
   Zhao HJ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107796
   Zhao ZX, 2009, 2008 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-4, P2150
   Zolfaghari M, 2022, VISUAL COMPUT, V38, P849, DOI 10.1007/s00371-020-02055-5
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 60
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1853
EP 1868
DI 10.1007/s00371-022-02449-7
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000778080200001
DA 2024-07-18
ER

PT J
AU Cammarasana, S
   Patanè, G
AF Cammarasana, Simone
   Patane, Giuseppe
TI Spatio-temporal analysis and comparison of 3D videos
SO VISUAL COMPUTER
LA English
DT Article
DE Shape analysis; 3D videos; 3D Video understanding; 3D Video
   classification and spatio-temporal reasoning; 3D Video action
   recognition
ID ACTION RECOGNITION; HUMAN MOTION; RETRIEVAL; CAPTURE; MODEL
AB Depth sensors of low-cost acquisition devices (e.g. Microsoft Kinect, Asus Xtion) are coming into widespread use; however, 3D acquired data are generally large, heterogeneous, and complex to analyse and interpret. In this context, our overall goal is the analysis of the action of a subject in a 3D video, e.g. the action of a human or the movement of its subparts. To this end, the action classification is achieved through the analysis of the temporal variation of geometric (e.g. centroid path, volume variation, activated voxels) and kinematic (e.g. speed) properties in consecutive frames. Then, these descriptors and the corresponding histograms are used to search a frame in a 3D video and to compare 3D videos. Our approach is applied to 3D videos represented as triangle meshes or point sets, and eventually to an underlying skeleton or to markers (if available). Our tests on the MIT, Berkley, i3DPost, NTU, and DUTH data sets confirm the usefulness of the proposed approach for the analysis and comparison of 3D videos, as well as for action classification.
C1 [Cammarasana, Simone; Patane, Giuseppe] CNR, IMATI, Via Marini 6, Genoa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Matematica
   Applicata e Tecnologie Informatiche "Enrico Magenes" (IMATI-CNR)
RP Cammarasana, S (corresponding author), CNR, IMATI, Via Marini 6, Genoa, Italy.
EM simone.cammarasana@ge.imati.cnr.it; patane@ge.imati.cnr.it
RI Cammarasana, Simone/HTM-7042-2023; Patane', Giuseppe/O-1322-2013
OI Cammarasana, Simone/0000-0002-2549-8330
CR [Anonymous], 2013, Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013 Fourth National Conference on
   Aubry Mathieu, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P122, DOI 10.1007/978-3-642-23123-0_13
   Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303
   Chaaraoui Alexandros Andre, 2012, Human Behavior Understanding. Proceedings of the Third International Workshop, HBU 2012, P29, DOI 10.1007/978-3-642-34014-7_3
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Georgiou T, 2020, INT J MULTIMED INF R, V9, P135, DOI 10.1007/s13735-019-00183-w
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Holte M. B., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P342, DOI 10.1109/3DIMPVT.2011.50
   Holte MB, 2012, IEEE J-STSP, V6, P553, DOI 10.1109/JSTSP.2012.2193556
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Huang P, 2010, INT J COMPUT VISION, V89, P362, DOI 10.1007/s11263-010-0319-9
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jolliffe I., 2011, International Encyclopedia of Statistical Science, P1094, DOI [DOI 10.1007/978-3-642-04898-2_455, 10.1007/978-3-642-04898-2_455]
   Kapsouras I, 2014, J VIS COMMUN IMAGE R, V25, P1432, DOI 10.1016/j.jvcir.2014.04.007
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kelley R., 2010, UNDERSTANDING ACTIVI, DOI [10.5772/8127, DOI 10.5772/8127]
   Khokhlova M, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P161, DOI 10.5220/0006541801610168
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lillo I, 2016, PROC CVPR IEEE, P1981, DOI 10.1109/CVPR.2016.218
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Maron H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073616
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Mihradi S., 2011, 2011 2nd International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering, P386, DOI 10.1109/ICICI-BME.2011.6108632
   Muller M., 2007, Information retrieval for music and motion, P69, DOI [10.1007/978-3-540-74048-3_4, DOI 10.1007/978-3-540-74048-3_4]
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Pirk R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083725
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Rao YB, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082731
   Rusu RB, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P267, DOI 10.1109/ROMAN.2008.4600677
   Sevilla-Lara Laura, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P281, DOI 10.1007/978-3-030-12939-2_20
   Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312
   Silambarasi R, 2017, 2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P1833, DOI 10.1109/ICCSP.2017.8286712
   Slama R, 2014, IMAGE VISION COMPUT, V32, P131, DOI 10.1016/j.imavis.2013.12.011
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68
   Suda R, 2002, MATH COMPUT, V71, P703, DOI 10.1090/S0025-5718-01-01386-2
   Tsai DM, 2009, IEEE T IMAGE PROCESS, V18, P158, DOI 10.1109/TIP.2008.2007558
   Tu JH, 2018, IEEE INT CON MULTI
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Veinidis C, 2019, MULTIMED TOOLS APPL, V78, P2789, DOI 10.1007/s11042-018-5855-2
   Vieira A. W., 2012, PROGR PATTERN RECOGN, P252, DOI [DOI 10.1007/978-3-642-33275-3, DOI 10.1007/978-3-642-33275]
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang H, 2019, COMPUT GRAPH FORUM, V38, P367, DOI 10.1111/cgf.13644
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Weinmann M., 2013, ISPRS ANN PHOTOGRAMM, VII-5/W2, P313, DOI DOI 10.5194/ISPRSANNALS-II-5-W2-313-2013
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xie ZY, 2020, NEUROCOMPUTING, V402, P245, DOI 10.1016/j.neucom.2020.03.086
   Yan GL, 2021, COMPUT AIDED GEOM D, V86, DOI 10.1016/j.cagd.2021.101964
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zhang JX, 2019, PATTERN RECOGN, V90, P196, DOI 10.1016/j.patcog.2019.01.027
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang Yujia, 2018, BMVC
   Zhao X, 2017, COMPUT GRAPH FORUM, V36, P119, DOI 10.1111/cgf.13112
NR 61
TC 2
Z9 2
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1335
EP 1350
DI 10.1007/s00371-022-02409-1
EA APR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000777240600001
OA hybrid
DA 2024-07-18
ER

PT J
AU Hu, Q
   Zhang, Y
   Zhu, Y
   Jiang, Y
   Song, MG
AF Hu, Qing
   Zhang, Yu
   Zhu, Yue
   Jiang, Yi
   Song, Mengen
TI Single image dehazing algorithm based on sky segmentation and optimal
   transmission maps
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Sky segmentation; Adaptive estimation of atmospheric
   light; Transmission map
ID RESTORATION; VISIBILITY
AB The majority of existing physical model-based dehazing algorithms have problems, including color distortion and halo effects, when restoring hazy outdoor scenes that often contain large areas of the sky. This paper proposes a single image dehazing algorithm based on sky segmentation and an optimal transmission map to improve the quality of dehazed images containing sky regions. The proposed algorithm acquires the sky region of a hazy image by using the mean shift technique and prior information of sky color rules and estimates the atmospheric light by introducing adaptive threshold constraints based on the sky region. Next, a hazy image feature-based objective function is designed, and a transmission map is accurately estimated by introducing gradient domain-guided filtering. On this basis, images are restored with the atmospheric scattering model, and the final dehazed images are obtained through tone adjustment. The experimental results demonstrate that the proposed algorithm is robust and can effectively eliminate haze and enrich the edge details of images. Compared to other algorithms, the saturated pixel ratio of the present algorithm is approximately zero, indicating the preferable color saturation of the restored images.
C1 [Hu, Qing; Zhang, Yu; Zhu, Yue; Jiang, Yi; Song, Mengen] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian, Peoples R China.
C3 Dalian Maritime University
RP Zhang, Y (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian, Peoples R China.
EM hq0518@dlmu.edu.cn; 240961678@qq.com; zy36572@163.com; j_y@dlmu.edu.cn;
   sme2457@126.com
RI HU, Qing/GWQ-8711-2022; Hu, Qing/K-1400-2014
OI Hu, Qing/0000-0001-8569-044X; Zhang, Yu/0000-0002-1631-0551
FU Research on Key Technologies and Equipment of Next Generation Marine
   Broadband Communication [2019020090-JH2/101]; Key Technologies of Ship
   Perception and Network Support in a complex environment [017210332]
FX This work was supported by the Research on Key Technologies and
   Equipment of Next Generation Marine Broadband Communication (No.
   2019020090-JH2/101) and the Key Technologies of Ship Perception and
   Network Support in a complex environment (No. 017210332).
CR Agrawal SC, 2022, VISUAL COMPUT, V38, P781, DOI 10.1007/s00371-020-02049-3
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Battiato S, 2004, PROC SPIE, V5302, P95, DOI 10.1117/12.526634
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Comaniciu D, 1997, PROC CVPR IEEE, P750, DOI 10.1109/CVPR.1997.609410
   Ngo D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185170
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Fang ZY, 2022, VISUAL COMPUT, V38, P3563, DOI 10.1007/s00371-021-02184-5
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He LY, 2017, IEEE T IMAGE PROCESS, V26, P1063, DOI 10.1109/TIP.2016.2644267
   Jin JC, 2018, ACTA OCEANOL SIN, V37, P99, DOI 10.1007/s13131-018-1269-2
   Khmag A, 2018, VISUAL COMPUT, V34, P675, DOI 10.1007/s00371-017-1406-5
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Kumari A, 2021, IEEE ACCESS, V9, P48131, DOI 10.1109/ACCESS.2021.3068446
   Lei T, 2020, IEEE T FUZZY SYST, V28, P2078, DOI 10.1109/TFUZZ.2019.2930030
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   [刘杰平 Liu Jieping], 2017, [电子学报, Acta Electronica Sinica], V45, P1896
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Ngo D, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142233
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Salazar-Colores S, 2020, IEEE ACCESS, V8, P149176, DOI 10.1109/ACCESS.2020.3015724
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Shi ZW, 2014, OPTIK, V125, P3868, DOI 10.1016/j.ijleo.2014.01.170
   Shiau YH, 2019, IEEE T CIRC SYST VID, V29, P238, DOI 10.1109/TCSVT.2017.2777140
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Silva K.G., 2018, Yugoslav Journal of Operations Research, V28, P153, DOI DOI 10.2298/YJOR180120014G
   Tiejun Zhang, 2013, Information Technology Journal, V12, P2342, DOI 10.3923/itj.2013.2342.2349
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yang ai-ping, 2021, Journal of Northeastern University (Natural Science), V42, P180, DOI 10.12068/j.issn.1005-3026.2021.02.005
   Yuan KL, 2019, IEEE ACCESS, V7, P181348, DOI 10.1109/ACCESS.2019.2958607
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang TJ, 2012, PROC SPIE, V8334, DOI 10.1117/12.956472
   Zhao D, 2019, SIGNAL PROCESS-IMAGE, V74, P253, DOI 10.1016/j.image.2019.02.004
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu YY, 2018, NEUROCOMPUTING, V275, P499, DOI 10.1016/j.neucom.2017.08.055
NR 45
TC 14
Z9 15
U1 7
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 997
EP 1013
DI 10.1007/s00371-021-02380-3
EA JAN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741931800002
DA 2024-07-18
ER

PT J
AU Besenic, K
   Ahlberg, J
   Pandzic, IS
AF Besenic, Kresimir
   Ahlberg, Jorgen
   Pandzic, Igor S.
TI Picking out the bad apples: unsupervised biometric data filtering for
   refined age estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Filtering; Biometric; Unsupervised; Web scraping; Age estimation;
   Dataset design
AB Introduction of large training datasets was essential for the recent advancement and success of deep learning methods. Due to the difficulties related to biometric data collection, facial image datasets with biometric trait labels are scarce and usually limited in terms of size and sample diversity. Web-scraping approaches for automatic data collection can produce large amounts of weakly labeled and noisy data. This work is focused on picking out the bad apples from web-scraped facial datasets by automatically removing erroneous samples that impair their usability. The unsupervised facial biometric data filtering method presented in this work greatly reduces label noise levels in web-scraped facial biometric data. Experiments on two large state-of-the-art web-scraped datasets demonstrate the effectiveness of the proposed method with respect to real and apparent age estimation based on five different age estimation methods. Furthermore, we apply the proposed method, together with a newly devised strategy for merging multiple datasets, to data collected from three major web-based data sources (i.e., IMDb, Wikipedia, Google) and derive the new Biometrically Filtered Famous Figure Dataset or B3FD. The proposed dataset, which is made publicly available, enables considerable performance gains for all tested age estimation methods and age estimation tasks. This work highlights the importance of training data quality compared to data quantity and selection of the estimation method.
C1 [Besenic, Kresimir; Pandzic, Igor S.] Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
   [Ahlberg, Jorgen] Linkoping Univ, Comp Vis Lab, S-58183 Linkoping, Sweden.
C3 University of Zagreb; Linkoping University
RP Besenic, K (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
EM kresimir.besenic@fer.hr; jorgen.ahlberg@liu.se; igor.pandzic@fer.hr
OI Besenic, Kresimir/0000-0002-5861-7076
FU company Visage Technologies
FX The author K. Besenic receives Ph.D. scholarship from the company Visage
   Technologies.
CR Agustsson E, 2017, IEEE INT CONF AUTOMA, P87, DOI 10.1109/FG.2017.20
   Antipov G, 2016, IEEE COMPUT SOC CONF, P801, DOI 10.1109/CVPRW.2016.105
   Besenic K, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P209, DOI 10.5220/0007257202090217
   Biemann C., 2006, Proceedings of TextGraphs: The first workshop on graph based methods for natural language processing, P73
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Chen SX, 2017, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2017.86
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93
   Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Gao BB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P712
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Golomb B.A., 1990, Advances in Neural Information Processing Systems (NIPS), P572
   Han H, 2013, INT CONF BIOMETR
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3846, DOI 10.1109/TIP.2017.2655445
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Jia S, 2015, PATTERN RECOGN LETT, V58, P35, DOI 10.1016/j.patrec.2015.02.006
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li PP, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107178
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   NI K, 2015, ARXIV150203409
   Niu ZX, 2016, PROC CVPR IEEE, P4920, DOI 10.1109/CVPR.2016.532
   Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554
   Panis G, 2015, LECT NOTES COMPUT SC, V8926, P737, DOI 10.1007/978-3-319-16181-5_56
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R., 2016, International Journal of Computer Vision, P1
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Yang X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P344, DOI 10.1109/ICCVW.2015.53
   Yi D, 2015, LECT NOTES COMPUT SC, V9005, P144, DOI 10.1007/978-3-319-16811-1_10
   Zeiler M. D., 2012, CoRR
   Zhang Y., 2017, ARXIV PREPRINT ARXIV
NR 47
TC 1
Z9 1
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 219
EP 237
DI 10.1007/s00371-021-02323-y
EA JAN 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000740610100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, DD
   Tan, JQ
   Zhang, L
   Ge, XY
AF Hu, Dandan
   Tan, Jieqing
   Zhang, Li
   Ge, Xianyu
TI Image deblurring based on enhanced salient edge selection
SO VISUAL COMPUTER
LA English
DT Article
DE Salient edges; Image sharpening; Kernel estimation; Image deblurring
ID KERNEL ESTIMATION
AB Blind image deblurring is a severely ill-posed problem in low-level vision. The success of blind image deblurring relies on statistical priors and well-designed regularizers to obtain a clear image. However, the prior-based method is time-consuming due to a lot of nonlinear calculations. To improve efficiency, this work proposes an enhanced salient edge selection for blind image deblurring. Different from the previous methods that only focus on the salient edge and ignore the image structure, the image sharpening operator is adopted to guide the finer image structure when salient edges provide strong edge information for blur kernel estimation. We find that the latent image restoration can be enhanced by joining the salient edge selection and an image sharpening operator, and the quality of the recovered kernel is hereby improved. By imposing L-2 constraints on the salient edge and sharpening operator terms, a new energy function is introduced and an effective alternating optimization strategy is explored. Extensive experiments have been conducted, which shows that our method is more effective compared with state-of-the-art methods.
C1 [Hu, Dandan; Tan, Jieqing; Zhang, Li] Hefei Univ Technol, Sch Math, Hefei 230009, Peoples R China.
   [Tan, Jieqing; Ge, Xianyu] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Tan, JQ (corresponding author), Hefei Univ Technol, Sch Math, Hefei 230009, Peoples R China.
EM hdd1232020@163.com; jieqingtan@126.com; lizhang@hfut.edu.cn;
   shuxuegxy@126.com
RI Tan, Jie/IVV-5250-2023
FU National Natural Science Foundation of China [61972131]; National Key
   Research and Development Program [2018YFB2100301]
FX We would like to thank the reviewers for their helpful suggestions which
   greatly improve the clarity of the paper. This work is supported by the
   National Natural Science Foundation of China (62172135); National Key
   Research and Development Program (2018YFB2100301); and National Natural
   Science Foundation of China (61972131).
CR ALVAREZ L, 1994, SIAM J NUMER ANAL, V31, P590, DOI 10.1137/0731032
   Cai JF, 2012, IEEE T IMAGE PROCESS, V21, P562, DOI 10.1109/TIP.2011.2164413
   Cai JR, 2020, IEEE T IMAGE PROCESS, V29, P6885, DOI 10.1109/TIP.2020.2995048
   Calder J, 2010, SIAM J IMAGING SCI, V3, P981, DOI 10.1137/090771260
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Cho TS, 2011, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2011.5995479
   Evans L.C., 1998, PARTIAL DIFFERENTIAL, V19
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Feng Q, 2019, VISUAL COMPUT, V35, P1081, DOI 10.1007/s00371-019-01697-4
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   Han Y, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9163274
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Hu Z, 2012, LECT NOTES COMPUT SC, V7576, P59, DOI 10.1007/978-3-642-33715-4_5
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Khan A, 2021, VISUAL COMPUT, V37, P1661, DOI 10.1007/s00371-020-01930-5
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Lai WS, 2015, PROC CVPR IEEE, P64, DOI 10.1109/CVPR.2015.7298601
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li LRH, 2019, INT J COMPUT VISION, V127, P1025, DOI 10.1007/s11263-018-01146-0
   Liu J., 2020, INVERSE PROBL
   Lou YF, 2013, INVERSE PROBL IMAG, V7, P839, DOI 10.3934/ipi.2013.7.839
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Sun LB, 2013, IEEE INT CONF COMPUT
   Sundaramoorthi G, 2009, INT J COMPUT VISION, V84, P113, DOI 10.1007/s11263-008-0133-9
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wen F, 2021, IEEE T CIRC SYST VID, V31, P2923, DOI 10.1109/TCSVT.2020.3034137
   Whyte Oliver, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P745
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yan YY, 2017, PROC CVPR IEEE, P6978, DOI 10.1109/CVPR.2017.738
   Yang H, 2019, VISUAL COMPUT, V35, P1627, DOI 10.1007/s00371-018-1562-2
   Zhang HC, 2011, IEEE I CONF COMP VIS, P770, DOI 10.1109/ICCV.2011.6126315
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 58
TC 7
Z9 7
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 281
EP 296
DI 10.1007/s00371-021-02329-6
EA NOV 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000713568800001
DA 2024-07-18
ER

PT J
AU Wang, C
   Xu, YH
   Wang, L
   Li, CM
AF Wang, Chen
   Xu, Yuhua
   Wang, Lin
   Li, Chunming
TI Fast structural global registration of indoor colored point cloud
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud registration; Global registration; Pairwise rigid
   registration; Indoor scene; Manhattan world
ID ROBUST
AB The indoor scenes usually exhibit strong structural regularity; on the other hand, the local geometric features are scarce and difficult to be distinguished. The properties make the solutions of the existing global registration algorithms ambiguous. In addition, finding global optimal solution is usually computationally expensive. These problems make the global registration of indoor point clouds a challenging task. Based on the Manhattan-world assumption and color information, we propose a fast and effective algorithm for the global registration of indoor colored point clouds in this paper. We introduce the Manhattan-world assumption into the global registration of indoor point cloud to limit the number of all possible rotation solutions. As a result, our algorithm solves the problem very fast. Furthermore, we utilize the color information hierarchically to eliminate the ambiguity induced by the structural models. The limited rotation set is filtered by the global color histograms, and the final solution is evaluated by the local features. The experiments demonstrate that our algorithm can complete the global registration for the point clouds with hundreds of thousands of points in 0.1 second with comparable accuracy with others.
C1 [Wang, Chen; Li, Chunming] Univ Elect Sci & Technol China, Chengdu, Sichuan, Peoples R China.
   [Xu, Yuhua; Wang, Lin] Orbbec Inc, Shenzhen, Guangdong, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Wang, C (corresponding author), Univ Elect Sci & Technol China, Chengdu, Sichuan, Peoples R China.
EM cwang@std.uestc.edu.cn; xyh_nudt@163.com; wanglin193@hotmail.com;
   cmli@ieee.org
RI Li, Chunming/JDD-4682-2023
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Altantsetseg E, 2018, VISUAL COMPUT, V34, P1021, DOI 10.1007/s00371-018-1534-6
   Antonante P, 2022, IEEE T ROBOT, V38, P281, DOI 10.1109/TRO.2021.3094984
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Bhattacharya U, 2017, INT CONF 3D VISION, P548, DOI 10.1109/3DV.2017.00068
   Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905
   Coughlan J. M., 1999, P IEEE INT C COMPUTE, DOI DOI 10.1109/ICCV.1999.790349
   Cui YM, 2021, NEUROCOMPUTING, V432, P300, DOI 10.1016/j.neucom.2020.12.067
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   Dong K, 2022, VISUAL COMPUT, V38, P51, DOI 10.1007/s00371-020-01999-y
   Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319
   Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867
   Glocker B, 2013, INT SYM MIX AUGMENT, P173, DOI 10.1109/ISMAR.2013.6671777
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Hartley RI, 2007, IEEE I CONF COMP VIS, P534
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Leitner J, 2009, AT-EQUAL 2009: 2009 ECSIS SYMPOSIUM ON ADVANCED TECHNOLOGIES FOR ENHANCED QUALITY OF LIFE: LAB-RS AND ARTIPED 2009, P144, DOI 10.1109/AT-EQUAL.2009.37
   Lin CC, 2017, EURASIP J ADV SIG PR, DOI 10.1186/s13634-016-0435-y
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma LN, 2016, IEEE INT CONF ROBOT, P1285, DOI 10.1109/ICRA.2016.7487260
   Magnusson M., 2009, 3 DIMENSIONAL NORMAL
   Magnusson M, 2007, J FIELD ROBOT, V24, P803, DOI 10.1002/rob.20204
   Magnusson M, 2009, IEEE INT CONF ROBOT, P2263
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Park J, 2017, IEEE I CONF COMP VIS, P143, DOI 10.1109/ICCV.2017.25
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   SUN LC, 2020, IEEE C EVOL COMPUTAT, DOI DOI 10.1109/cec48606.2020.9185718
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Zeisl B, 2013, IEEE I CONF COMP VIS, P2808, DOI 10.1109/ICCV.2013.349
   Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 50
TC 3
Z9 3
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4279
EP 4290
DI 10.1007/s00371-021-02295-z
EA SEP 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000692956800001
DA 2024-07-18
ER

PT J
AU Challa, SK
   Kumar, A
   Semwal, VB
AF Challa, Sravan Kumar
   Kumar, Akhilesh
   Semwal, Vijay Bhaskar
TI A multibranch CNN-BiLSTM model for human activity recognition using
   wearable sensor data
SO VISUAL COMPUTER
LA English
DT Article
DE Bidirectional LSTM; HAR; Wearable sensor data; CNN; Deep neural networks
ID CLASSIFICATION; NETWORKS
AB Human activity recognition (HAR) has become a significant area of research in human behavior analysis, human-computer interaction, and pervasive computing. Recently, deep learning (DL)-based methods have been applied successfully to time-series data generated from smartphones and wearable sensors to predict various activities of humans. Even though DL-based approaches performed very well in activity recognition, they are still facing challenges in handling time series data. Several issues persist with time-series data, such as difficulties in feature extraction, heavily biased data, etc. Moreover, most of the HAR approaches rely on manual feature engineering. In this paper, to design a robust classification model for HAR using wearable sensor data, a hybrid of convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) is used. The proposed multibranch CNN-BiLSTM network does automatic feature extraction from the raw sensor data with minimal data pre-processing. The use of CNN and BiLSTM makes the model capable of learning local features as well as long-term dependencies in sequential data. The different filter sizes used in the proposed model can capture various temporal local dependencies and thus helps to improve the feature extraction process. To evaluate the model performance, three benchmark datasets, i.e., WISDM, UCI-HAR, and PAMAP2, are utilized. The proposed model has achieved 96.05%, 96.37%, and 94.29% accuracies on WISDM, UCI-HAR, and PAMAP2 datasets, respectively. The obtained experimental results demonstrate that the proposed model outperforms the other compared approaches.
C1 [Challa, Sravan Kumar; Kumar, Akhilesh] NIT Jamshedpur, Dept Elect & Commun Engn, Jamshedpur, Bihar, India.
   [Semwal, Vijay Bhaskar] MANIT Bhopal, Dept Comp Sci Engn, Bhopal, Madhya Pradesh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Jamshedpur; National Institute of Technology (NIT System);
   Maulana Azad National Institute of Technology Bhopal
RP Challa, SK (corresponding author), NIT Jamshedpur, Dept Elect & Commun Engn, Jamshedpur, Bihar, India.
EM 2016rsec002@nitjsr.ac.in; akumar.ece@nitjsr.ac.in; vsemwal@gmail.com
RI Challa, Sravan Kumar/AAY-4159-2021; Semwal, Vijay Bhaskar/B-5628-2017;
   Kumar, Akhilesh/GZL-9961-2022
OI Challa, Sravan Kumar/0000-0002-3105-7937; Semwal, Vijay
   Bhaskar/0000-0003-0767-6057; Kumar, Akhilesh/0000-0002-6990-138X
FU SERB, DST, Government of India [ECR/2018/000203]
FX This research work is supported by SERB, DST, Government of India
   through the Early Career Research (ECR) award (ECR/2018/000203).
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Anguita D., 2012, INT WORKSH AMB ASS L, P216
   Anguita D, 2013, ESANN, P437, DOI DOI 10.3390/S20082200
   [Anonymous], HDB BRAIN THEORY NEU
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Catal C, 2015, APPL SOFT COMPUT, V37, P1018, DOI 10.1016/j.asoc.2015.01.025
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Chen Z, 2019, IEEE T INSTRUM MEAS
   Cheng X., 2020, REAL TIME HUMAN ACTI
   Damirchi H, 2020, ARC NET ACTIVITY REC
   Dewangan DK, 2021, COMPUTING, V103, P2867, DOI 10.1007/s00607-021-00974-2
   Dewangan DK, 2021, INTEL SERV ROBOT, V14, P199, DOI 10.1007/s11370-020-00343-6
   Dewangan DK, 2021, IEEE SENS J, V21, P3570, DOI 10.1109/JSEN.2020.3027097
   Dewangan DK, 2021, ELECTRON LETT, V57, P53, DOI 10.1049/ell2.12062
   Feng ZT, 2015, IEEE ENG MED BIO, P5074, DOI 10.1109/EMBC.2015.7319532
   Gupta A., 2020, EMOTION INFORM PROCE, DOI [10.1007/978-3-030-48849-9_12, DOI 10.1007/978-3-030-48849-9_12]
   Hammerla N.Y., 2016, P 25 INT JOINT C ART
   Hernández F, 2019, SYMP IMAG SIG PROC A, DOI 10.1109/stsiva.2019.8730249
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ignatov A, 2018, APPL SOFT COMPUT, V62, P915, DOI 10.1016/j.asoc.2017.09.027
   Ignatov AD, 2016, MULTIMED TOOLS APPL, V75, P7257, DOI 10.1007/s11042-015-2643-0
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Karim F, 2018, IEEE ACCESS, V6, P1662, DOI 10.1109/ACCESS.2017.2779939
   Kwapisz JR., 2011, ACM SIGKDD EXPLORATI, V12, P74, DOI [DOI 10.1145/1964897.1964918, 10.1145/1964897.1964918]
   Li T, 2021, P IEEE CVF C COMP VI, P16266
   Madhuranga D, 2021, VISUAL COMPUT, V37, P1263, DOI 10.1007/s00371-020-01864-y
   Mishra AK, 2021, NEURAL COMPUT APPL, V33, P1435, DOI 10.1007/s00521-020-05027-x
   Mousse MA, 2017, VISUAL COMPUT, V33, P1529, DOI 10.1007/s00371-016-1296-y
   Mutegeki Ronald, 2020, 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), P362, DOI 10.1109/ICAIIC48513.2020.9065078
   Nair N, 2018, 5TH INTERNATIONAL WORKSHOP ON SENSOR-BASED ACTIVITY RECOGNITION AND INTERACTION (IWOAR 2018), DOI 10.1145/3266157.3266221
   Nweke HF, 2018, EXPERT SYST APPL, V105, P233, DOI 10.1016/j.eswa.2018.03.056
   Ordóñez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115
   Panwar M, 2017, IEEE ENG MED BIO, P2438, DOI 10.1109/EMBC.2017.8037349
   Pedersoli F, 2014, VISUAL COMPUT, V30, P1107, DOI 10.1007/s00371-014-0921-x
   Pienaar S.W., 2019, 2019 IEEE 2 WIR AFR, P1
   Reiss A, 2012, IEEE INT SYM WRBL CO, P108, DOI 10.1109/ISWC.2012.13
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Semwal VB, 2017, NEURAL COMPUT APPL, V28, P565, DOI 10.1007/s00521-015-2089-3
   Sun Z, 2020, HUMAN ACTION RECOGNI
   Teng Q, 2020, IEEE SENS J, V20, P7265, DOI 10.1109/JSEN.2020.2978772
   Ullah M, 2019, EUR W VIS INF PROCES, P175, DOI [10.1109/euvip47703.2019.8946180, 10.1109/EUVIP47703.2019.8946180]
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Wan SH, 2020, MOBILE NETW APPL, V25, P743, DOI 10.1007/s11036-019-01445-x
   Wang K., 2020, SEQUENTIAL WEAKLY LA
   Wang K, 2019, IEEE SENS J, V19, P7598, DOI 10.1109/JSEN.2019.2917225
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Xia K, 2020, IEEE ACCESS, V8, P56855, DOI 10.1109/ACCESS.2020.2982225
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Yao L, 2018, COMPUTING, V100, P369, DOI 10.1007/s00607-018-0603-z
   Yu Guan, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3090076
   Zeng M, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING, APPLICATIONS AND SERVICES (MOBICASE), P197, DOI 10.4108/icst.mobicase.2014.257786
   Zhao Y, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/7316954
   Zhu R., 2020, Deep Learning Techniques for Biomedical and Health Informatics, P257, DOI [DOI 10.1016/10.1007/978-3-030-33966-113, 10.1007/978-3-030-33966-1_13]
NR 53
TC 115
Z9 117
U1 5
U2 37
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4095
EP 4109
DI 10.1007/s00371-021-02283-3
EA AUG 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000686854900001
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, XF
   Wang, W
   Bi, HB
   Wang, K
AF Wang, Xiufang
   Wang, Wei
   Bi, Hongbo
   Wang, Kang
TI Reverse collaborative fusion model for co-saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Co-saliency detection; Reverse message fusion; Collaborative consistency
   learning; Collaborative information
ID OBJECT DETECTION; SEGMENTATION; DEEP
AB The purpose of co-saliency detection is to find out the salient and common objects of related images. This paper proposes a novel reverse collaborative fusion model (RCFM) for co-saliency detection. The model is mainly composed of two parts: reverse message fusion module (RMFM) and collaborative consistency learning module (CCLM). Specifically, we first aggregate the features in high-level layers as global guidance by using the cascaded decoder (CD). Then, we propose repeated RMFMs on each side output to complete the complementary fusion of deep and shallow information. Then, we fuse multi-scale feature maps as initial co-saliency maps. Finally, the CCLM extracts the collaborative information between images to improve the quality of the initial co-saliency map to obtain the final co-saliency map. The model fully considers the semantic features of high-level and the boundary features of low-level, thereby correcting some deviation predictions and improving the accuracy of co-saliency detection. Compared to the state-of-the-art approaches, experimental results demonstrate that our proposed approach achieves the best performance on four evaluation indicators of three datasets.
C1 [Wang, Xiufang; Wang, Wei; Wang, Kang] Northeast Petr Univ, Daqing, Peoples R China.
   [Bi, Hongbo] Northeast Petr Univ, Sch Elect Informat Engn, Daqing, Peoples R China.
C3 Northeast Petroleum University; Northeast Petroleum University
RP Bi, HB (corresponding author), Northeast Petr Univ, Sch Elect Informat Engn, Daqing, Peoples R China.
EM wxfdqpi@163.com; 15776556110@163.com; bhbdq@126.com; kangwangwww@163.com
OI Bi, Hongbo/0000-0003-2442-330X
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bai C, 2018, J VIS COMMUN IMAGE R, V50, P199, DOI 10.1016/j.jvcir.2017.11.021
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bi HB, 2021, VISUAL COMPUT, V37, P911, DOI 10.1007/s00371-020-01842-4
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen JN, 2017, COMM COM INF SC, V772, P126, DOI 10.1007/978-981-10-7302-1_11
   Chen YL, 2014, INT C PATT RECOG, P2305, DOI 10.1109/ICPR.2014.400
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan D.-P., ARXIV200703380
   Fan D-P, ARXIV180510421
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR42600.2020.00299
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fu HZ, 2015, IEEE T IMAGE PROCESS, V24, P3415, DOI 10.1109/TIP.2015.2442915
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fu KR, 2015, IEEE T IMAGE PROCESS, V24, P5671, DOI 10.1109/TIP.2015.2485782
   Gao S.-H., ARXIV200305643
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   Hsu KJ, 2018, LECT NOTES COMPUT SC, V11209, P502, DOI 10.1007/978-3-030-01228-1_30
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jeong DJ, 2018, IEEE T IMAGE PROCESS, V27, P5866, DOI 10.1109/TIP.2018.2859752
   Jiang B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1375, DOI 10.1145/3343031.3350860
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Kong WC, 2014, INT CONF SYST SCI EN, P13, DOI 10.1109/ICSSE.2014.6887895
   Lee G, 2018, IEEE T PATTERN ANAL, V40, P1599, DOI 10.1109/TPAMI.2017.2737631
   Li B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li HY, 2017, NEUROCOMPUTING, V226, P212, DOI 10.1016/j.neucom.2016.11.056
   Li LN, 2018, NEUROCOMPUTING, V275, P1650, DOI 10.1016/j.neucom.2017.10.002
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Toshev A, 2007, PROC CVPR IEEE, P33
   Wei LN, 2019, IEEE T IMAGE PROCESS, V28, P5052, DOI 10.1109/TIP.2019.2909649
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Zha ZJ, 2020, IEEE T NEUR NET LEAR, V31, P2398, DOI 10.1109/TNNLS.2020.2967471
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang YH, 2018, IEEE INT CON MULTI
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhou XF, 2017, MULTIMED TOOLS APPL, V76, P23187, DOI 10.1007/s11042-016-4093-8
   Zhou XF, 2016, IEEE SIGNAL PROC LET, V23, P517, DOI 10.1109/LSP.2016.2536743
NR 77
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2022
VL 38
IS 11
BP 3911
EP 3921
DI 10.1007/s00371-021-02231-1
EA JUL 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5X0BS
UT WOS:000678086600001
DA 2024-07-18
ER

PT J
AU Wang, K
   Yang, JF
   Yuan, S
   Li, MA
AF Wang, Kang
   Yang, Jinfu
   Yuan, Shuai
   Li, Mingai
TI A lightweight network with attention decoder for real-time semantic
   segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Encoder&#8211; decoder structure; Depth-wise
   separable asymmetric convolution; Dilated convolution; Attention
   mechanism
AB As an important task in scene understanding, semantic segmentation requires a large amount of computation to achieve high performance. In recent years, with the rise of autonomous systems, it is crucial to make a trade-off in terms of accuracy and speed. In this paper, we propose a novel asymmetric encoder-decoder network structure to address this problem. In the encoder, we design a Separable Asymmetric Module, which combines depth-wise separable asymmetric convolution with dilated convolution to greatly reduce computation cost while maintaining accuracy. On the other hand, an attention mechanism is also used in the decoder to further improve segmentation performance. Experimental results on CityScapes and CamVid datasets show that the proposed method can achieve a better balance between segmentation precision and speed compared with state-of-the-art semantic segmentation methods. Specifically, our model obtains mean IoU of 72.5% and 66.3% on CityScapes and CamVid test dataset, respectively, with less than 1M parameters.
C1 [Wang, Kang; Yang, Jinfu; Yuan, Shuai; Li, Mingai] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Wang, K (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM 201861365@emails.bjut.edu.cn
FU National Natural Science Foundation of China [61973009]; Beijing Natural
   Science Foundation [4182009]
FX This work is partly supported by the National Natural Science Foundation
   of China Grant no.61973009 and Beijing Natural Science Foundation under
   Grant no.4182009.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Holschneider M, 1990, Wavelets, P286, DOI DOI 10.1007/978-3-642-75988-828
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mehta Sachin, 2018, P EUR C COMP VIS ECC, P552, DOI DOI 10.1007/978-3-030-01249-6_34
   Paszke A., 2016, ARXIV160602147
   Poudel R. P., 2018, ARXIV180504554
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Siam M, 2018, IEEE IMAGE PROC, P1603, DOI 10.1109/ICIP.2018.8451495
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yu F., 2015, ARXIV
   Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng CX, 2018, VISUAL COMPUT, V34, P735, DOI 10.1007/s00371-017-1411-8
   Zhou Q, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106682
   Zhou Q, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-019-2685-1
   Zhou Q, 2019, WORLD WIDE WEB, V22, P555, DOI 10.1007/s11280-018-0556-3
NR 40
TC 12
Z9 12
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2329
EP 2339
DI 10.1007/s00371-021-02115-4
EA MAY 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000648249600001
DA 2024-07-18
ER

PT J
AU Basly, H
   Ouarda, W
   Sayadi, FE
   Ouni, B
   Alimi, AM
AF Basly, Hend
   Ouarda, Wael
   Sayadi, Fatma Ezahra
   Ouni, Bouraoui
   Alimi, Adel M.
TI DTR-HAR: deep temporal residual representation for human activity
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Daily living activity recognition; Convolutional neural network (CNN);
   Long short-term memory (LSTM); Video surveillance
AB Human activity recognition (HAR) is a highly prized application in the pattern recognition and the computer vision fields. Up till now, deep neural networks have acquired big attention in computer studies and image processing fields, and have generated significant results. In this paper, we propose a deep temporal residual system for daily living activity recognition that aims to enhance spatiotemporal feature representation in order to improve the HAR system performance. To this end, we adopt a deep residual convolutional neural network (RCN) to retain discriminative visual features relayed to appearance and long short-term memory neural network to capture the long-term temporal evolution of actions. The latter was considered to implement time dependencies occurring when carrying out the activity to enhance features extracted from the RCN network by adding time information to address the dynamic activity recognition problem as a sequence labeling job. The deep temporal residual model for human activity recognition system is performed on two benchmark publicly available datasets: MSRDailyActivity3D and CAD-60. the proposed system achieves very competitive results when compared to others from the state of the art.
C1 [Basly, Hend; Ouni, Bouraoui] Univ Sousse, NOCCS Lab, Networked Objects Control & Commun Syst Lab, Natl Engn Sch Sousse ENISO, BP 264, Erriadh 4023, Sousse, Tunisia.
   [Ouarda, Wael; Alimi, Adel M.] Univ Sfax, REGIM Lab, REs Grp Intelligent Machines, Natl Engn Sch Sfax ENIS, BP 1173, Sfax 3038, Tunisia.
   [Sayadi, Fatma Ezahra] Univ Monastir, EE Lab, Elect & Microelect Lab, Fac Sci Monastir FSM, Environm Ave, Monastir 5019, Tunisia.
C3 Universite de Sousse; Universite de Sfax; Ecole Nationale dIngenieurs de
   Sfax (ENIS); Universite de Monastir
RP Basly, H (corresponding author), Univ Sousse, NOCCS Lab, Networked Objects Control & Commun Syst Lab, Natl Engn Sch Sousse ENISO, BP 264, Erriadh 4023, Sousse, Tunisia.
EM basly.hend@gmail.com; wael.ouarda@ieee.org; sayadi_fatma@yahoo.fr;
   ouni_bouraoui@yahoo.fr; adel.alimi@ieee.org
RI Alimi, Adel M./A-5697-2012; Ouarda, Wael/GQP-6480-2022; sayadi,
   fatma/AHC-0214-2022
OI Alimi, Adel M./0000-0002-0642-3384; Ouarda, Wael/0000-0002-6338-7092;
   Hend, Basly/0000-0002-2632-8544
CR Arif S, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11020042
   Asadi-Aghbolaghi M, 2018, MULTIMED TOOLS APPL, V77, P14115, DOI 10.1007/s11042-017-5017-y
   Baradel Fabien., 2018, Proc. Brit. Mach. Vis. Conf, P1
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bhattacharya S, 2014, PERVASIVE MOB COMPUT, V15, P242, DOI 10.1016/j.pmcj.2014.05.006
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Das S, 2018, INDIA, EMPIRE, AND FIRST WORLD WAR CULTURE, P1, DOI 10.1017/ 9781139963244
   Das S, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Ding LY, 2018, AUTOMAT CONSTR, V86, P118, DOI 10.1016/j.autcon.2017.11.002
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ercolano G, 2017, IEEE ROMAN, P877, DOI 10.1109/ROMAN.2017.8172406
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hu YX, 2009, IEEE I CONF COMP VIS, P128, DOI 10.1109/ICCV.2009.5459153
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khaire P, 2018, PATTERN RECOGN LETT, V115, P107, DOI 10.1016/j.patrec.2018.04.035
   Kim JH, 2018, DISPLAYS, V55, P38, DOI 10.1016/j.displa.2018.08.001
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Kong Y, 2015, PROC CVPR IEEE, P1054, DOI 10.1109/CVPR.2015.7298708
   Koperski M, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P44, DOI 10.1109/AVSS.2016.7738023
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JJ, 2014, PATTERN RECOGN LETT, V50, P139, DOI 10.1016/j.patrec.2014.03.024
   Ma CY, 2019, SIGNAL PROCESS-IMAGE, V71, P76, DOI 10.1016/j.image.2018.09.003
   Madhuranga D, 2021, VISUAL COMPUT, V37, P1263, DOI 10.1007/s00371-020-01864-y
   Molyneux D, 2013, PROCEEDINGS OF THE ASME 32ND INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING - 2013 - VOL 6
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ni BB, 2013, IEEE T CYBERNETICS, V43, P1383, DOI 10.1109/TCYB.2013.2276433
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Simonyan K., 2014, ARXIV14091556
   Simonyan K, 2014, ADV NEUR IN, V27
   Srihari D, 2020, MULTIMED TOOLS APPL, V79, P11723, DOI 10.1007/s11042-019-08588-9
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Ullah A, 2018, IEEE ACCESS, V6, P1155, DOI 10.1109/ACCESS.2017.2778011
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu Z., 2015, P 23 ACM INT C MULT, P461, DOI DOI 10.1145/2733373.2806222
   Yilmaz A, 2005, PROC CVPR IEEE, P984
   Zaremba W., 2014, CORR
   Zhang M, 2013, IEEE J BIOMED HEALTH, V17, P553, DOI 10.1109/JBHI.2013.2253613
   Zhao R, 2017, IEEE INT C INT ROBOT, P4260, DOI 10.1109/IROS.2017.8206288
   Zhou TF, 2020, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR42600.2020.00432
   Zhou Y, 2015, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR.2015.7298953
   Zhu Y, 2014, IMAGE VISION COMPUT, V32, P453, DOI 10.1016/j.imavis.2014.04.005
NR 67
TC 18
Z9 18
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 993
EP 1013
DI 10.1007/s00371-021-02064-y
EA FEB 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000618136000002
DA 2024-07-18
ER

PT J
AU Dai, LC
   Zhang, K
   Zheng, XS
   Martin, RR
   Li, YN
   Yu, JH
AF Dai, Lingchen
   Zhang, Kang
   Zheng, Xianjun Sam
   Martin, Ralph R.
   Li, Yina
   Yu, Jinhui
TI Visual complexity of shapes: a hierarchical perceptual learning model
SO VISUAL COMPUTER
LA English
DT Article
DE Neurons; Hierarchical; Shape; Visual complexity; Regression analysis
ID FUNCTIONAL ARCHITECTURE; OBJECT RECOGNITION; FEATURES; PERFORMANCE;
   INFORMATION
AB Understanding how people perceive the visual complexity of shapes has important theoretical as well as practical implications. One school of thought, driven by information theory, focuses on studying the local features that contribute to the perception of visual complexity. Another school, in contrast, emphasizes the impact of global characteristics of shapes on perceived complexity. Inspired by recent discoveries in neuroscience, our model considers both local features of shapes: edge lengths and vertex angles, and global features: concaveness, and is in 92% agreement with human subjective ratings of shape complexity. The model is also consistent with the hierarchical perceptual learning theory, which explains how different layers of neurons in the visual system act together to yield a perception of visual shape complexity.
C1 [Dai, Lingchen; Yu, Jinhui] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Dallas, TX USA.
   [Zheng, Xianjun Sam] Tsinghua Univ, Dept Psychol, Beijing, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
   [Li, Yina] Univ Sci & Technol China, Sch Management, Hefei, Peoples R China.
C3 Zhejiang University; University of Texas System; University of Texas
   Dallas; Tsinghua University; Cardiff University; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Yu, JH (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM jhyu@cad.zju.edu.cn
RI Martin, Ralph R/D-2366-2010; Li, Yi-Na/JGB-4336-2023
OI Li, Yi-Na/0000-0002-5479-5174; Martin, Ralph/0000-0002-8495-8536
CR Ahissar M, 1999, CURR DIR PSYCHOL SCI, V8, P124, DOI 10.1111/1467-8721.00029
   ATTNEAVE F, 1957, J EXP PSYCHOL, V53, P221, DOI 10.1037/h0043921
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BIRKHOFF GEORGE DAVID, 1933, Aesthetic measure, DOI DOI 10.4159/HARVARD.9780674734470
   Brehar R, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113085
   Brinkhoff T., 1995, PROC 3 ACM INT WORKS, P109
   BROWN DR, 1967, PSYCHOL BULL, V68, P243, DOI 10.1037/h0025037
   Carballal A, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22040488
   Diaconescu A.O., 2017, ARXIV170902323
   Donderi DC, 2006, PSYCHOL BULL, V132, P73, DOI 10.1037/0033-2909.132.1.73
   Dou Q, 2019, INT J HUM-COMPUT ST, V124, P56, DOI 10.1016/j.ijhcs.2018.11.006
   Dubitzky, 2007, FUNDAMENTALS DATA MI, DOI DOI 10.1007/978-0-387-47509-7
   Dutt M, 2017, LECT NOTES COMPUT SC, V10149, P105, DOI 10.1007/978-3-319-54609-4_8
   Everitt B.S., 2002, CAMBRIDGE DICT STAT
   Feldman J, 2001, PERCEPT PSYCHOPHYS, V63, P1171, DOI 10.3758/BF03194532
   Feldman J, 2005, PSYCHOL REV, V112, P243, DOI 10.1037/0033-295X.112.1.243
   Gartus A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185276
   Gell-Mann M., 1996, Complexity, V2, P44, DOI 10.1002/(SICI)1099-0526(199609/10)2:1<44::AID-CPLX10>3.0.CO;2-X
   Graham L., 2008, Journal of Humanities Social Sciences, V2, P1
   Haddad K, 2013, J HYDROL, V482, P119, DOI 10.1016/j.jhydrol.2012.12.041
   Harper S, 2013, BEHAV INFORM TECHNOL, V32, P491, DOI 10.1080/0144929X.2012.726647
   Hawkins DM, 2017, J APPL STAT, V44, P1319, DOI 10.1080/02664763.2016.1202217
   Hedenfalk I, 2002, ADV ANAT PATHOL, V9, P1
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Kanwisher N, 2010, P NATL ACAD SCI USA, V107, P11163, DOI 10.1073/pnas.1005062107
   Kayaert G, 2009, VISION RES, V49, P708, DOI 10.1016/j.visres.2009.01.002
   Kim SH, 2016, NEUROIMAGE, V135, P163, DOI 10.1016/j.neuroimage.2016.04.053
   Koffka K, 1935, PRINCIPLES GESTALT P, DOI DOI 10.4324/9781315009292
   Konovalov DA, 2008, J CHEM INF MODEL, V48, P370, DOI 10.1021/ci700283s
   Kwon S., 2017, INT J ADV MANUFACTUR, V88, P1
   Matsumoto T, 2019, COMPUT GRAPH-UK, V78, P108, DOI 10.1016/j.cag.2018.10.009
   MAUNSELL JHR, 1987, ANNU REV NEUROSCI, V10, P363, DOI 10.1146/annurev.ne.10.030187.002051
   MAVRIDES CM, 1969, PERCEPT PSYCHOPHYS, V6, P276, DOI 10.3758/BF03210098
   McCormack Jon, 2020, Artificial Intelligence in Music, Sound, Art and Design. 9th International Conference, EvoMUSART 2020. Held as Part of EvoStar 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12103), P118, DOI 10.1007/978-3-030-43859-3_9
   McDougall SJP, 2000, J EXP PSYCHOL-APPL, V6, P291, DOI 10.1037/1076-898X.6.4.291
   Murray SO, 2002, P NATL ACAD SCI USA, V99, P15164, DOI 10.1073/pnas.192579399
   Page DL, 2003, IEEE IMAGE PROC, P229
   Perkiö J, 2009, LECT NOTES COMPUT SC, V5769, P704, DOI 10.1007/978-3-642-04277-5_71
   Psarra, 2001, P 3 INT SPAC SYNT S, P1
   Rashid M, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12125037
   Rigau Jaume., 2005, Proceedings of the First Eurographics conference on Computational Aesthetics in Graphics, Visualization and Imaging, P177, DOI [10.2312/COMPAESTH/COMPAESTH05/177-184, DOI 10.2312/COMPAESTH/COMPAESTH05/177-184, DOI 10.2312/COMPAESTH/COMPAESTH05/177184]
   Rossignac J, 2005, VISUAL COMPUT, V21, P985, DOI 10.1007/s00371-005-0362-7
   Serre T, 2005, PROC CVPR IEEE, P994
   Severeyn E., 2019, 2019 22 S IM SIGN PR, P1
   Sieu B, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041133
   Su H, 2006, INT C PATT RECOG, P134
   Swindale NV, 1998, BIOL CYBERN, V78, P45, DOI 10.1007/s004220050411
   TREISMAN A, 1982, J EXP PSYCHOL HUMAN, V8, P194, DOI 10.1037/0096-1523.8.2.194
   Tuch AN, 2009, INT J HUM-COMPUT ST, V67, P703, DOI 10.1016/j.ijhcs.2009.04.002
   Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870
   van der Helm PA, 2000, PSYCHOL BULL, V126, P770, DOI 10.1037//0033-2909.126.5.770
   vanderHelm PA, 1996, PSYCHOL REV, V103, P429, DOI 10.1037/0033-295X.103.3.429
   Xu QS, 2004, J CHEMOMETR, V18, P112, DOI 10.1002/cem.858
   Zheng XJS, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1
NR 55
TC 7
Z9 9
U1 7
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 419
EP 432
DI 10.1007/s00371-020-02023-z
EA JAN 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ZE7IA
UT WOS:000605494100004
DA 2024-07-18
ER

PT J
AU Hruda, L
   Kolingerová, I
   Vása, L
AF Hruda, Lukas
   Kolingerova, Ivana
   Vasa, Libor
TI Robust, fast and flexible symmetry plane detection based on
   differentiable symmetry measure
SO VISUAL COMPUTER
LA English
DT Article
DE Symmetry; Plane; Symmetry measure; Differentiable; Symmetry detection
ID REFLECTION SYMMETRY; 3D; SHAPE
AB Reflectional symmetry is a potentially very useful feature which many real-world objects exhibit. It is instrumental in a variety of applications such as object alignment, compression, symmetrical editing or reconstruction of incomplete objects. In this paper, we propose a novel differentiable symmetry measure, which allows using gradient-based optimization to find symmetry in geometric objects. We further propose a new method for symmetry plane detection in 3D objects based on this idea. The method performs well on perfectly as well as approximately symmetrical objects, it is robust to noise and to missing parts. Furthermore, it works on discrete point sets and therefore puts virtually no constraints on the input data. Due to flexibility of the symmetry measure, the method is also easily extensible, e.g., by adding more information about the input object and using it to further improve its performance. The proposed method was tested with very good results on many objects, including incomplete objects and noisy objects, and was compared to other state-of-the-art methods which it outperformed in most aspects.
C1 [Hruda, Lukas; Kolingerova, Ivana] Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Plzen, Czech Republic.
   [Vasa, Libor] Univ West Bohemia, Fac Appl Sci, NTIS, Plzen, Czech Republic.
C3 University of West Bohemia Pilsen; University of West Bohemia Pilsen
RP Hruda, L (corresponding author), Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Plzen, Czech Republic.
EM hrudalu@kiv.zcu.cz; kolinger@kiv.zcu.cz; lvasa@kiv.zcu.cz
RI Hruda, Lukáš/AAR-8966-2020; Vasa, Libor/F-6706-2011
OI Hruda, Lukáš/0000-0002-9477-7464; Vasa, Libor/0000-0002-0213-3769;
   Kolingerova, Ivana/0000-0003-4556-2771
FU Ministry of Education, Youth and Sports of the Czech Republic, project
   PUNTIS under the program NPU I [LO1506];  [SGS-2019-016]
FX This work was supported by Ministry of Education, Youth and Sports of
   the Czech Republic, project PUNTIS (LO1506) under the program NPU I and
   University specific research project SGS-2019-016 Synthesis and Analysis
   of Geometric and Computing Models. We would also like to thank the
   authors of [35] for providing us the implementation of their method
   together with all the information needed for its comparison to our
   method. Furthermore, we want to thank the authors of [27] for providing
   us the results of their method shown in Table 4.
CR [Anonymous], 2017, ICCV 17 WORKSH CHALL
   [Anonymous], 2014, P 20 WORLD M INT ASS
   Cailliere D, 2008, IEEE IMAGE PROC, P1772, DOI 10.1109/ICIP.2008.4712119
   Cicconet M, 2017, IEEE INT CONF COMP V, P1759, DOI 10.1109/ICCVW.2017.207
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Combes B., 2008, IEEE C COMPUTER VISI, P1
   Ecins A, 2017, IEEE INT CONF COMP V, P1779, DOI 10.1109/ICCVW.2017.210
   Fang R, 2008, LECT NOTES COMPUT SC, V5358, P381, DOI 10.1007/978-3-540-89639-5_37
   Funk C, 2017, IEEE INT CONF COMP V, P1692, DOI 10.1109/ICCVW.2017.198
   Hruda L, 2019, COMPUT GRAPH FORUM, V38, P175, DOI 10.1111/cgf.13798
   Hruda L., ROBUST FAST FLEXIBLE
   Hruda L, 2020, LECT NOTES COMPUT SC, V12141, P509, DOI 10.1007/978-3-030-50426-7_38
   Ji PL, 2019, MULTIMED TOOLS APPL, V78, P35471, DOI 10.1007/s11042-019-08043-9
   Kakarala R, 2013, PROC CVPR IEEE, P249, DOI 10.1109/CVPR.2013.39
   Korman S, 2015, COMPUT GRAPH FORUM, V34, P2, DOI 10.1111/cgf.12454
   Kubaskova K, 2016, P CENTR EUR SEM COMP
   Levoy M, 2005, The Stanford 3D scanning repository
   Li B, BO LI HOMEPAGE
   Li B, 2016, GRAPH MODELS, V83, P2, DOI 10.1016/j.gmod.2015.09.003
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778840
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Martinet A, 2006, ACM T GRAPHIC, V25, P439, DOI 10.1145/1138450.1138462
   Mitra NJ, 2006, ACM T GRAPHIC, V25, P560, DOI 10.1145/1141911.1141924
   Nagar R, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107483
   Nagar R, 2019, IEEE T SIGNAL PROCES, V67, P1582, DOI 10.1109/TSP.2019.2893835
   Nagar R, 2018, LECT NOTES COMPUT SC, V11205, P433, DOI 10.1007/978-3-030-01246-5_26
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Panozzo D, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185607
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2
   Prantl M, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP), VOL 1, P185, DOI 10.5220/0007254801850192
   Rieke-Zapp D, 2013, PRESIOUS 3D CULTURAL
   Schiebener D, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P74, DOI 10.1109/IROS.2016.7759037
   Shi ZY, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.12978
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Simari P., 2006, Proceedings of the fourth Eurographics symposium on Geometry processing (SGP '06), P111
   Sipiran I, 2014, COMPUT GRAPH FORUM, V33, P131, DOI 10.1111/cgf.12481
   Speciale P, 2016, LECT NOTES COMPUT SC, V9912, P313, DOI 10.1007/978-3-319-46484-8_19
   Sun CM, 1997, IEEE T PATTERN ANAL, V19, P164, DOI 10.1109/34.574800
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Sun YF, 2018, LECT NOTES COMPUT SC, V11209, P257, DOI 10.1007/978-3-030-01228-1_16
   Sung M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818094
   Thrun S, 2005, IEEE I CONF COMP VIS, P1824
   Wang H, 2017, COMPUT GRAPH FORUM, V36, P51, DOI 10.1111/cgf.13271
   Wang WC, 2019, COMPUT GRAPH FORUM, V38, P617, DOI 10.1111/cgf.13865
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   ZABRODSKY H, 1995, IEEE T PATTERN ANAL, V17, P1154, DOI 10.1109/34.476508
   Zhou Qingnan, 2016, arXiv preprint arXiv:1605.04797
NR 49
TC 11
Z9 11
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 555
EP 571
DI 10.1007/s00371-020-02034-w
EA JAN 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605494100001
DA 2024-07-18
ER

PT J
AU Harvey, C
   Selmanovic, E
   O'Connor, J
   Chahin, M
AF Harvey, Carlo
   Selmanovic, Elmedin
   O'Connor, Jake
   Chahin, Malek
TI A comparison between expert and beginner learning for motor skill
   development in a virtual reality serious game
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Virtual Worlds and Games for Serious
   Applications (VS-Games)
CY SEP 05-07, 2018
CL Wurzburg, GERMANY
DE Virtual reality; Training; Learning; Serious game
AB In order to be used for skill development and skill maintenance, virtual environments require accurate simulation of the physical phenomena involved in the process of the task being trained. The accuracy needs to be conveyed in a multimodal fashion with varying parameterisations still being quantified, and these are a function of task, prior knowledge, sensory efficacy and human perception. Virtual reality (VR) has been integrated from a didactic perspective in many serious games and shown to be effective in the pedological process. This paper interrogates whether didactic processes introduced into a VR serious game, by taking advantage of augmented virtuality to modify game attributes, can be effective for both beginners and experts to a task. The task in question is subjective performance in a clay pigeon shooting simulation. The investigation covers whether modified game attributes influence skill and learning in a complex motor task and also investigates whether this process is applicable to experts as well as beginners to the task. VR offers designers and developers of serious games the ability to provide information in the virtual world in a fashion that is impossible in the real world. This introduces the question of whether this is effective and transfers skill adoption into the real world and also if a-priori knowledge influences the practical nature of this information in the pedagogic process. Analysis is conducted via a between-subjects repeated measure ANOVA using a 2x2 factorial design to address these questions. The results show that the different training provided affects the performance in this task (N=57The skill improvement is still evidenced in repeated measures when information and guidance is removed. This effect does not exist under a control condition. Additionally, we separate by an expert and non-expert group to deduce if a-priori knowledge influences the effect of the presented information, it is shown that it does not.
C1 [Harvey, Carlo; O'Connor, Jake] Birmingham City Univ, Birmingham B4 7XG, W Midlands, England.
   [Selmanovic, Elmedin; Chahin, Malek] Univ Sarajevo, B&H, Zmaja Bosne 35, Sarajevo, Bosnia & Herceg.
C3 Birmingham City University; University of Sarajevo
RP Harvey, C (corresponding author), Birmingham City Univ, Birmingham B4 7XG, W Midlands, England.
EM carlo.harvey@bcu.ac.uk; eselmanovic@pmf.unsa.ba; jake.oconnor@bcu.ac.uk;
   mchahin@pmf.unsa.ba
RI Harvey, Carlo/HKW-8479-2023; Selmanović, Elmedin/KIE-9609-2024
OI Harvey, Carlo/0000-0002-4809-1592; Selmanović,
   Elmedin/0000-0003-4245-3588
CR Armbrüster C, 2008, CYBERPSYCHOL BEHAV, V11, P9, DOI 10.1089/cpb.2007.9935
   Arnab S, 2013, SERIOUS GAMES FOR HEALTHCARE: APPLICATIONS AND IMPLICATIONS, P1, DOI 10.4018/978-1-4666-1903-6
   Asadipour A, 2017, VISUAL COMPUT, V33, P401, DOI 10.1007/s00371-016-1275-3
   Asadipour A, 2015, 2015 IEEE 7TH INTERNATIONAL CONFERENCE ON GAMES AND VIRTUAL WORLDS FOR SERIOUS APPLICATIONS (VS-GAMES), P16
   BAILEY AB, 1976, AIAA J, V14, P1631, DOI 10.2514/3.7262
   BAILEY AB, 1972, AIAA J, V10, P1436, DOI 10.2514/3.50387
   Bedwell WL, 2012, SIMULAT GAMING, V43, P729, DOI 10.1177/1046878112439444
   Bradley C, 2002, VISUALIATION SHOTGUN
   Braun W., 1973, 1630 BALL RES LAB
   Brindle J, 1995, SHOTGUN SHOOTING TEC
   Burke JW, 2009, PROCEEDINGS OF THE IEEE VIRTUAL WORLDS FOR SERIOUS APPLICATIONS, P103, DOI 10.1109/VS-GAMES.2009.17
   Buzzard G, 1985, MODERN SHOTGUN
   CHARTERS AC, 1945, J AERONAUT SCI, V12, P468, DOI 10.2514/8.11287
   Compton D, 1996, THESIS DEP ELECT ELE
   Coulson S, 2003, REAL TIME POSITIONIN
   Covaci A., 2014, VRST
   Covaci A, 2015, IEEE COMPUT GRAPH, V35, P55, DOI 10.1109/MCG.2015.95
   Cowan B, 2015, VISUAL COMPUT, V31, P1207, DOI 10.1007/s00371-014-1006-6
   de Mestre N., 1990, The Mathematics of Projectiles in Sport, DOI [10.1017/CBO9780511624032, DOI 10.1017/CBO9780511624032]
   Denton A, 2003, 3D MODELLING VISUALI
   Field A., 2013, Discovering statistics using IBM SPSS statistics, V4th, P591
   Giblin R., 1996, BALLISTICS MEASUREME
   Gotsis M, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.94
   Harvey C., 2018, 2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games), P1, DOI 10.1109/VS-Games.2018.8493447
   Harvey C, 2017, COMPUT GRAPH FORUM, V36, P172, DOI 10.1111/cgf.12793
   Hulusic V, 2012, COMPUT GRAPH FORUM, V31, P102, DOI 10.1111/j.1467-8659.2011.02086.x
   Landers RN, 2014, SIMULAT GAMING, V45, P752, DOI 10.1177/1046878114563660
   LEWIS JR, 1995, INT J HUM-COMPUT INT, V7, P57, DOI 10.1080/10447319509526110
   Liarokapis F, 2009, VISUAL COMPUT, V25, P1109, DOI 10.1007/s00371-009-0388-3
   Oberfell G, 1960, MYSTERIES SHOTGUN PA
   Qualye P.P, 1925, AM RIFLEM
   Rajanen M., 2015, INFORM SYSTEMS RES, V8
   Rilling S, 2011, VISUAL COMPUT, V27, P287, DOI 10.1007/s00371-011-0550-6
   ROOS FW, 1971, AIAA J, V9, P285, DOI 10.2514/3.6164
   Saini S., 2012, 2012 Proceedings of International Conference on Computer & Information Science (ICCIS 2012), P55, DOI 10.1109/ICCISci.2012.6297212
   Scarle S., 2011, Proceedings of the 2011 3rd International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES 2011), P178, DOI 10.1109/VS-GAMES.2011.48
   Schonauer C., 2011, Virtual Rehabilitation (ICVR), 2011 International Conference on, P1, DOI DOI 10.1109/ICVR.2011.5971855
   Shooting Facts, VALUE SHOOTING EC EN
   Shotpro, SHOTPR 2000 SHOOT SI
   Susi T., 2007, SERIOUS GAMES OVER
   Thomas G, 1987, SHOTGUNS CARTRIDGES
   Wortley D, 2013, SIMULAT GAMING, V44, P452, DOI 10.1177/1046878113488850
NR 42
TC 11
Z9 11
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 3
EP 17
DI 10.1007/s00371-019-01702-w
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA QE3UO
UT WOS:000616134400002
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Xu, JY
   Xue, XY
   Wu, YT
   Mao, XY
AF Xu, Jiayi
   Xue, Xinying
   Wu, Yitiao
   Mao, Xiaoyang
TI Matching a composite sketch to a photographed face using fused HOG and
   deep feature models
SO VISUAL COMPUTER
LA English
DT Article
DE Composite sketch; HOG feature; VGG-face feature; Adaptive feature weight
ID SYSTEM
AB In this paper, we focus on the research of matching a computer-generated composite face sketch to a photograph. This is of great importance in the field of criminal investigation. To blend the different facial representation modalities, we propose a robust feature model by combining pixel-level features extracted from multi-scale key face patches and high-level features learned from a pre-trained deep learning-based model. At first, texture features are captured by a two-level histogram of oriented gradient descriptor, considering both the overall structure and local details. The semantic-level facial characteristics are analyzed through the high-level features of the Visual Geometry Group-Face (VGG-Face) network. Next, feature similarities between each sketch/photograph pair are measured by feature distance. Then, adaptive weights are assigned to each feature similarity, and score level fused according to their visual saliency contribution. Finally, the fused feature similarity is evaluated for matching purposes. After experimenting on the Pattern Recognition and Image Processing-Viewed Software-Generated Composite (PRIP-VSGC) database and the expanded University of Malta Composite Face Sketch (UoM-SGFS) database, it is found that this framework could achieve more satisfying results compared to the existing methods.
C1 [Xu, Jiayi; Xue, Xinying; Wu, Yitiao] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, Kofu, Yamanashi, Japan.
C3 Hangzhou Dianzi University; University of Yamanashi
RP Mao, XY (corresponding author), Univ Yamanashi, Kofu, Yamanashi, Japan.
EM xujiayi@hdu.edu.cn; xxy1202@hdu.edu.cn; uuii210@163.com;
   mao@yamanashi.ac.jp
FU Public Welfare Research Project of Zhejiang Province, China
   [LGF18F020015]; JSPS, Japan [17H00737]; Opening Foundation of Key
   Laboratory of Fundamental Science for National Defense on Vision
   Synthetization, Sichuan University, China [2020SCUVS007]; Grants-in-Aid
   for Scientific Research [17H00737] Funding Source: KAKEN
FX This paper was sponsored by the Public Welfare Research Project of
   Zhejiang Province, China (Grant No. LGF18F020015), JSPS Grants-in-Aid
   for Scientific Research, Japan (Grant No. 17H00737), and Opening
   Foundation of Key Laboratory of Fundamental Science for National Defense
   on Vision Synthetization, Sichuan University, China (Grant No.
   2020SCUVS007).
CR Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Berg TL, 2004, PROC CVPR IEEE, P848
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chugh T., 2017, P IEEE C COMP VIS PA, P117
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Galea C., 2016, 2016 INT C BIOM SPEC, P1
   Galea C, 2018, IEEE T INF FOREN SEC, V13, P1421, DOI 10.1109/TIFS.2017.2788002
   Galea C, 2016, EUR SIGNAL PR CONF, P2240, DOI 10.1109/EUSIPCO.2016.7760647
   Gao XB, 2012, IEEE T CIRC SYST VID, V22, P1213, DOI 10.1109/TCSVT.2012.2198090
   Han H, 2013, IEEE T INF FOREN SEC, V8, P191, DOI 10.1109/TIFS.2012.2228856
   Harada K, 2019, ICAROB 2019: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND ROBOTICS, P1
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kazemi H, 2018, IEEE COMPUT SOC CONF, P612, DOI 10.1109/CVPRW.2018.00091
   Kazemi Hadi, 2018, 2018 International Conference of the Biometrics Special Interest Group (BIOSIG), P1
   Klare B, 2010, PROC SPIE, V7667, DOI 10.1117/12.849821
   Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229
   Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180
   Klum SJ, 2014, IEEE T INF FOREN SEC, V9, P2248, DOI 10.1109/TIFS.2014.2360825
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Martinez A., 1998, 24 CVC BARC
   Mittal P, 2017, INFORM FUSION, V33, P86, DOI 10.1016/j.inffus.2016.04.003
   Mittal P, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Mittal P, 2015, INT CONF BIOMETR, P251, DOI 10.1109/ICB.2015.7139092
   Ouyang S., 2018, P IEEE C COMPUTER VI, P5571
   Pallavi S, 2018, 2018 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P460, DOI 10.1109/ICACCI.2018.8554564
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng CL, 2019, PATTERN RECOGN, V90, P161, DOI 10.1016/j.patcog.2019.01.041
   Peng CL, 2019, SIGNAL PROCESS, V156, P46, DOI 10.1016/j.sigpro.2018.10.015
   Peng CL, 2017, IEEE T PATTERN ANAL, V39, P301, DOI 10.1109/TPAMI.2016.2542816
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang X, 2002, IEEE IMAGE PROC, P257
   Wang NN, 2018, PATTERN RECOGN, V76, P215, DOI 10.1016/j.patcog.2017.11.008
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang ZP, 2018, COMPLEXITY, DOI 10.1155/2018/9356451
   Zhang LL, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P627, DOI 10.1145/2671188.2749321
NR 38
TC 7
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 765
EP 776
DI 10.1007/s00371-020-01976-5
EA SEP 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000574081000001
DA 2024-07-18
ER

PT J
AU Zhu, LF
   Ren, RB
   Chen, DP
   Song, AG
   Liu, J
   Ye, N
   Yang, Y
AF Zhu, Lifeng
   Ren, Rubin
   Chen, Dapeng
   Song, Aiguo
   Liu, Jia
   Ye, Ning
   Yang, Yin
TI Feel the inside: A haptic interface for navigating stress distribution
   inside objects
SO VISUAL COMPUTER
LA English
DT Article
DE Haptics; User interface; Stress field
ID TENSOR FIELD VISUALIZATION; DESIGN
AB Understanding stress distributions over 3D models is a highly desired feature in many scientific and engineering fields. The stress is mathematically a second-order tensor, and it is typically visualized using either color maps, tensor glyphs, or streamlines. However, neither of these methods is physically intuitive to the end user, and they become even more awkward when dealing with the volumetric tensor field over a complicated 3D shape. In this paper, we present a virtual perception system, which leverages a multi-finger haptic interface to help users intuitively perceive 3D stress fields. Our system allows the user to navigate the interior of the 3D model freely and maps the stress tensor to the haptic rendering along the direction of the finger's trajectory. Doing so provides user a natural and straightforward understanding of the stress distribution without interacting with the parameters in the mapped visual representations. Experimental results show that our system is preferred in navigating stress fields inside an object and is applicable for different design tasks.
C1 [Zhu, Lifeng; Ren, Rubin; Chen, Dapeng; Song, Aiguo] Southeast Univ, Sch Instrument Sci & Engn, State Key Lab Bioelect, Jiangsu Key Lab Remote Measurement & Control, Nanjing, Peoples R China.
   [Zhu, Lifeng; Chen, Dapeng; Song, Aiguo; Liu, Jia] Nanjing Univ Informat Sci & Technol, CICAEET, Nanjing, Peoples R China.
   [Ye, Ning] Nanjing Forestry Univ, Sch Informat Technol, Nanjing, Peoples R China.
   [Yang, Yin] Clemson Univ, Sch Comp, Clemson, SC USA.
C3 Southeast University - China; Nanjing University of Information Science
   & Technology; Nanjing Forestry University; Clemson University
RP Zhu, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, State Key Lab Bioelect, Jiangsu Key Lab Remote Measurement & Control, Nanjing, Peoples R China.; Zhu, LF (corresponding author), Nanjing Univ Informat Sci & Technol, CICAEET, Nanjing, Peoples R China.
EM lfzhulf@gmail.com
RI Ye, Ning/O-3246-2015; zhu, lifeng/IST-2069-2023; yang,
   zhou/KBB-6972-2024
OI Ye, Ning/0000-0001-7249-8352; 
FU National Key Technologies RD Program [2019YFC-0119303]; NSFC [61773219,
   61673114]; Zhishan Youth Scholar Program of SEU
FX The authors would like to thank anonymous reviewers for their
   constructive comments. This work has been supported by the National Key
   Technologies R&D Program under Grants No. 2019YFC-0119303, the NSFC
   under Grants No. 61773219, 61673114, and the Zhishan Youth Scholar
   Program of SEU.
CR Achibet M, 2015, P IEEE VIRT REAL ANN, P63, DOI 10.1109/VR.2015.7223325
   [Anonymous], 1965, Foundations of Solid Mechanics
   Avila RS, 1996, IEEE VISUAL, P197, DOI 10.1109/VISUAL.1996.568108
   Bhalerao A, 2003, LECT NOTES COMPUT SC, V2879, P294
   Chen DP, 2019, IEEE T HAPTICS, V12, P281, DOI 10.1109/TOH.2019.2920349
   Chen GN, 2011, IEEE T VIS COMPUT GR, V17, P1979, DOI 10.1109/TVCG.2011.170
   DELMARCELLE T, 1993, IEEE COMPUT GRAPH, V13, P25, DOI 10.1109/38.219447
   Dick C, 2009, IEEE T VIS COMPUT GR, V15, P1399, DOI 10.1109/TVCG.2009.184
   Faeth A, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P213
   Gerrits T, 2017, IEEE T VIS COMPUT GR, V23, P980, DOI 10.1109/TVCG.2016.2598998
   Gescheider G. A., 2013, Psychophysics: the fundamentals
   Gleeson BT, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P172, DOI 10.1109/WHC.2009.4810804
   Hinchet R, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P901, DOI 10.1145/3242587.3242657
   Hirota K, 2016, P IEEE VIRT REAL ANN, P49, DOI 10.1109/VR.2016.7504687
   Hlawitschka M., 2014, VISUALIZATION PROCES, P3
   Jacobs J, 2011, P IEEE VIRT REAL ANN, P11, DOI 10.1109/VR.2011.5759430
   Kim M, 2017, IEEE T VIS COMPUT GR, V23, P1379, DOI 10.1109/TVCG.2017.2657139
   Kindlmann G, 2000, IEEE T VIS COMPUT GR, V6, P124, DOI 10.1109/2945.856994
   Kratz A, 2014, IEEE PAC VIS SYMP, P145, DOI 10.1109/PacificVis.2014.51
   Laha B, 2012, IEEE T VIS COMPUT GR, V18, P597, DOI 10.1109/TVCG.2012.42
   LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375
   Li M, 2014, SENSOR ACTUAT A-PHYS, V218, P132, DOI 10.1016/j.sna.2014.08.003
   Maciel A., 2004, EUR WORKSH VIRT ENV
   Mueller CG, 2006, PROC MONOGR ENG WATE, P3
   Pacchierotti C, 2016, IEEE HAPTICS SYM, P134, DOI 10.1109/HAPTICS.2016.7463167
   Sagardia M, 2017, P IEEE VIRT REAL ANN, P64, DOI 10.1109/VR.2017.7892232
   Sagardia M, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P23, DOI 10.1109/VR.2012.6180872
   Salada M., 2004, P 12 INT S HAPT INT
   Salisbury K, 2004, IEEE COMPUT GRAPH, V24, P24, DOI 10.1109/MCG.2004.1274058
   Serina ER, 1998, J BIOMECH, V31, P639, DOI 10.1016/S0021-9290(98)00067-0
   Sifakis E, 2012, ACM SIGGRAPH 2012 CO, DOI [10.1145/2343483.2343501, DOI 10.1145/2343483.2343501]
   Weichert F, 2013, SENSORS-BASEL, V13, P6380, DOI 10.3390/s130506380
   Xia PJ, 2016, IEEE T HAPTICS, V9, P358, DOI 10.1109/TOH.2016.2554551
   Yang ZX, 2011, COMPUT VIS SCI, V14, P207, DOI 10.1007/s00791-012-0175-y
   Zehner B, 2006, COMPUT GEOSCI-UK, V32, P73, DOI 10.1016/j.cageo.2005.05.008
   Zhang E, 2007, IEEE T VIS COMPUT GR, V13, P94, DOI 10.1109/TVCG.2007.16
NR 36
TC 0
Z9 0
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2445
EP 2456
DI 10.1007/s00371-020-01891-9
EA SEP 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000565126800001
DA 2024-07-18
ER

PT J
AU Zhou, YJ
   Zhao, JN
   Luo, C
AF Zhou, Yijun
   Zhao, Jianan
   Luo, Chen
TI A novel method for reconstructing general 3D curves from stereo images
SO VISUAL COMPUTER
LA English
DT Article
DE Curve reconstruction; Stereo vision; B-spline curve fitting;
   Multi-parameter iteration algorithm
ID PROJECTIVE RECONSTRUCTION; SURFACE RECONSTRUCTION; SHAPE; MOTION;
   OBJECTS; POINTS
AB Three-dimensional (3D) reconstruction of objects and scenes from camera images is of great interests due to its wide applications. Reconstruction based on feature point correspondence is an established approach. Existing research on curve-based reconstruction is limited to certain type of curves and constrained by case-dependent reconstruction accuracy. In view of that, this paper developed a new method to reconstruct general 3D curves from stereo images. Under proposal, a B-spline curve fitting is applied to sets of 2D edge points extracted from acquired stereo images. Derived approximating parametric curves are then used to construct conic surfaces. Further, robust iterative algorithms are developed to get intersection of corresponding conic surfaces to recover 3D curve. Due to the method design, proposed approach can reconstruct general 3D curves including both open and closed curves. The curve fitting technique and developed robust algorithms can meet accuracy requirement of many real applications. Validity of the proposed method is verified through experiments on a cylinder and teacup in laboratory and a real forging within a workshop.
C1 [Zhou, Yijun; Zhao, Jianan; Luo, Chen] Southeast Univ, Sch Mech Engn, Nanjing 211189, Peoples R China.
   [Zhou, Yijun] Southeast Univ, Chengxian Coll, Nanjing 210088, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Luo, C (corresponding author), Southeast Univ, Sch Mech Engn, Nanjing 211189, Peoples R China.
EM zhouyj@seu.edu.cn; zhaojn@seu.edu.cn; chenluo@seu.edu.cn
OI Luo, Chen/0000-0003-4339-0355
FU National Natural Science Foundation of China [51975119, 51575107]
FX This work was supported by the National Natural Science Foundation of
   China under Grant Nos. 51975119 and 51575107. This financial support is
   gratefully acknowledged. We also thank Shanghai Xinmin Heavy-duty
   Forging Limited for providing the workshop in Dongtai that enabled our
   third experiment.
CR Abbena E., 2017, Modern differential geometry of curves and surfaces with Mathematica
   AbdelMalek K, 1997, COMPUT AIDED DESIGN, V29, P21, DOI 10.1016/S0010-4485(96)00046-2
   Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   Aigner M, 2009, VISUAL COMPUT, V25, P731, DOI 10.1007/s00371-009-0361-1
   [Anonymous], 1995, COMPUT VIS IMAGE UND
   [Anonymous], 2006, VISION MODELLING VIS
   Bajaj C. L., 1988, Computer-Aided Geometric Design, V5, P285, DOI 10.1016/0167-8396(88)90010-6
   Baldacci A, 2016, VISUAL COMPUT, V32, P1605, DOI 10.1007/s00371-015-1144-5
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Berthilsson R, 1997, PROC CVPR IEEE, P476, DOI 10.1109/CVPR.1997.609368
   Berthilsson R, 1997, PROC CVPR IEEE, P444, DOI 10.1109/CVPR.1997.609363
   Berthilsson R., 2001, 7 IEEE INT C COMP VI
   Berthilsson R., 1997, P SCAND C IM AN, P963
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chen Y, 2011, COMPUT AIDED GEOM D, V28, P114, DOI 10.1016/j.cagd.2010.11.002
   Cross G, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P25, DOI 10.1109/ICCV.1998.710697
   Fan B, 2012, PATTERN RECOGN, V45, P794, DOI 10.1016/j.patcog.2011.08.004
   Farin G., 2014, Curves and Surfaces for Computer-Aided Geometric Design: A Practical Guide
   Fusiello A, 2000, MACH VISION APPL, V12, P16, DOI 10.1007/s001380050120
   Gay P, 2017, IEEE I CONF COMP VIS, P3094, DOI 10.1109/ICCV.2017.334
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hartley R. I., 1994, Proceedings 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.94CH3405-8), P903, DOI 10.1109/CVPR.1994.323922
   Hartley RI, 1997, INT J COMPUT VISION, V22, P125, DOI 10.1023/A:1007936012022
   Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949
   Heyden A, 1997, SCIA '97 - PROCEEDINGS OF THE 10TH SCANDINAVIAN CONFERENCE ON IMAGE ANALYSIS, VOLS 1 AND 2, P963
   Heyden A, 1999, IMAGE VISION COMPUT, V17, P981, DOI 10.1016/S0262-8856(99)00002-5
   Hofer M., 2014, INT C 3D VIS
   Hu MX, 2010, VISUAL COMPUT, V26, P801, DOI 10.1007/s00371-010-0476-4
   Kaminski JY, 2004, INT J COMPUT VISION, V56, P195, DOI 10.1023/B:VISI.0000011204.89453.4d
   Kaminski JY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P181, DOI 10.1109/ICCV.2001.937622
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   Lieu D.K., 2015, VISUALIZATION MODELI
   Ma S. D., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P344, DOI 10.1109/ICPR.1996.546046
   Mai F, 2010, PATTERN RECOGN, V43, P545, DOI 10.1016/j.patcog.2009.07.003
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Pumarola A., 2015, IEEE INT C ROB AUT, P4503
   Quan L, 1996, IEEE T PATTERN ANAL, V18, P151, DOI 10.1109/34.481540
   Schöps T, 2017, COMPUT VIS IMAGE UND, V157, P151, DOI 10.1016/j.cviu.2016.09.007
   Shashua A, 1997, INT J COMPUT VISION, V23, P185, DOI 10.1023/A:1007962930529
   Shrivakshan G., 2012, INT J COMPUT SCI ISS, V9, P269
   Sparr G., 2001, P INT C PATT REC, V1, P328
   Sturm P., 1996, EUROPEAN C COMPUTER, P709, DOI DOI 10.1007/3-540-61123-1
   Szeliski R, 2011, TEXTS COMPUT SCI, P1
   Tang AWK, 2006, PATTERN RECOGN, V39, P889, DOI 10.1016/j.patcog.2005.10.019
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Umbaugh S. E., 2010, Digital Image Processing and Analysis: Human and Computer Vision Applications with CVIPtools, DOI [10.1201/9781439802069, DOI 10.1201/9781439802069]
   Wang WP, 2003, COMPUT AIDED GEOM D, V20, P401, DOI 10.1016/S0167-8396(03)00081-5
   Wang ZH, 2009, PATTERN RECOGN, V42, P941, DOI 10.1016/j.patcog.2008.08.035
   Weyrich T., 2004, S POINT BASED GRAPHI, P85
   Wu H, 2005, VISUAL COMPUT, V21, P203, DOI 10.1007/s00371-005-0281-7
   Yang ZW, 2005, VISUAL COMPUT, V21, P831, DOI 10.1007/s00371-005-0340-0
   Zhang G, 2015, IEEE T ROBOT, V31, P1364, DOI 10.1109/TRO.2015.2489498
   Zhang J., 2012, 4 INT C DIG HOM
   Zhang S, 2018, OPT LASER ENG, V106, P119, DOI 10.1016/j.optlaseng.2018.02.017
   Zhang XB, 2010, LECT NOTES COMPUT SC, V6454, P21, DOI 10.1007/978-3-642-17274-8_3
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao CS, 1996, COMPUT VIS IMAGE UND, V64, P62, DOI 10.1006/cviu.1996.0046
   Zhu C, 2013, VISUAL COMPUT, V29, P609, DOI 10.1007/s00371-013-0827-z
NR 59
TC 3
Z9 3
U1 5
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 2009
EP 2021
DI 10.1007/s00371-020-01959-6
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000562713200001
DA 2024-07-18
ER

PT J
AU Chen, ZH
   Hu, ZL
   Sheng, B
   Li, P
   Kim, JM
   Wu, EH
AF Chen, Zhihua
   Hu, Zhuoliang
   Sheng, Bin
   Li, Ping
   Kim, Jinman
   Wu, Enhua
TI Simplified non-locally dense network for single-image dehazing
SO VISUAL COMPUTER
LA English
DT Article
DE Single-image dehazing; Dense; Non-local
ID VISION; ALGORITHM
AB Single-image dehazing is an ill-posed problem. Most previous methods focused on estimating intermediate parameters for input hazy images. In this paper, we propose a novel end-to-end Simplified Non-locally Dense Network (SNDN) which does not rely on intermediate parameters. To capture long-range dependencies, we propose a Simplified Non-local Dense Block (SNDB) which is lightweight and outperforms traditional non-local method. Our SNDB will be embedded into a densely connected encoder-decoder network. To avoid gradients vanishing problem, we propose a simple branch network which only have five convolution layers. The effectiveness of our proposed network is proved through ablation experiment. In addition, we enhanced our training set by synthesizing colored hazy images, which helps restore the original color of the hazy image. The experimental results demonstrate that our network have better performance than most of the pervious state-of-the-art methods.
C1 [Chen, Zhihua; Hu, Zhuoliang] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Kim, Jinman] Univ Sydney, Sch Informat Technol, Biomed & Multimedia Informat Technol Res Grp, Sydney, NSW, Australia.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Macau, Peoples R China.
C3 East China University of Science & Technology; Shanghai Jiao Tong
   University; Hong Kong Polytechnic University; University of Sydney;
   Chinese Academy of Sciences; Institute of Software, CAS; University of
   Macau
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.; Wu, EH (corresponding author), Univ Macau, Fac Sci & Technol, Macau, Peoples R China.
EM czh@ecust.edu.cn; 540726646@qq.com; shengbin@sjtu.edu.cn;
   p.li@polyu.edu.hk; jinman.kim@sydney.edu.au; ehwu@um.edu.mo
RI Li, Ping/AAO-2019-2020; Kim, Jin/AAS-5810-2021; Kim, Jin
   Man/HJO-8987-2023
OI Li, Ping/0000-0002-1503-0240; Kim, Jin/0000-0002-7667-9588; Sheng,
   Bin/0000-0001-8678-2784; Sheng, Bin/0000-0001-8510-2556
FU National Natural Science Foundation of China [61672228, 61872241,
   61572316, 61370174]; Shanghai Automotive Industry Science and Technology
   Development Foundation [1837]; Hong Kong Polytechnic University
   [P0030419, P0030929]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672228, Grant 61872241, Grant
   61572316, and Grant 61370174, in part by the Shanghai Automotive
   Industry Science and Technology Development Foundation under Grant 1837,
   and in part by The Hong Kong Polytechnic University under Grant P0030419
   and Grant P0030929.
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti CO, 2019, IEEE SIGNAL PROC LET, V26, P1413, DOI 10.1109/LSP.2019.2932189
   [Anonymous], 2014, IEEE International Conference on Computational Photography (ICCP)
   Berman D., 2018, IEEE Transactions on Pattern Analysis and Machine Intelligence, P1
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cao Y., 2019, CORR, P1, DOI [DOI 10.1109/ICCVW.2019.00246, 10.1109/ICCVW.2019.00246]
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Gandelsman Y., 2018, ARXIV181200467
   Golts A., 2019, IEEE T IMAGE PROCESS
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Ju MY, 2017, NEUROCOMPUTING, V260, P180, DOI 10.1016/j.neucom.2017.04.034
   Kingma D. P., 2014, arXiv
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mei K., 2018, P AS C COMP VIS
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Shi LF, 2018, IEEE T MULTIMEDIA, V20, P2503, DOI 10.1109/TMM.2018.2807593
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yang YZ, 2019, IEEE INT CON MULTI, P1378, DOI 10.1109/ICME.2019.00239
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhao D, 2019, SIGNAL PROCESS-IMAGE, V74, P253, DOI 10.1016/j.image.2019.02.004
   Zhen-Zhao L, 2008, IEEE C EVOL COMPUTAT, P702, DOI 10.1109/CEC.2008.4630872
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 43
TC 15
Z9 15
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2189
EP 2200
DI 10.1007/s00371-020-01929-y
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000551368400004
DA 2024-07-18
ER

PT J
AU Kang, H
   Han, J
AF Kang, HyeongYeop
   Han, JungHyun
TI SafeXR: alerting walking persons to obstacles in mobile XR environments
SO VISUAL COMPUTER
LA English
DT Article
DE Mixed reality; User safety; Human-computer interaction
ID AUDITORY WARNINGS; PEDESTRIAN SAFETY; CELL PHONES; ATTENTION; BEHAVIOR;
   REALITY; TALKING; URGENCY; MODEL
AB The increasing performance of smartphones has made it possible for people to enjoy various mobile XR applications as they walk. Due to the limit of human capability of multitasking, however, it may compromise mobile users' safety. In this paper, we propose a safety assistance system for walking smartphone users, which utilizes only the smartphone's built-in sensors. Our solution detects obstacles by analyzing the feature points extracted from the input camera images and then alerts the users to the danger ahead. In typical smartphone game, AR game, and mobile VR game environments, we made two experiments. The first experiment identifies the desirable and preferred distances to alert the obstacles, and the second one identifies what kinds of interfaces are appropriate for alerting walking persons while least distracting them from gaming. The results obtained from the experiments are integrated into a system, which we name SafeXR. Then, solid empirical evaluations are made on the effects of SafeXR on user's safety, task performances, and user experiences.
C1 [Kang, HyeongYeop] Kyung Hee Univ, Dept Software Convergence, Yongin, South Korea.
   [Han, JungHyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Kyung Hee University; Korea University
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM siamiz@khu.ac.kr; jhan@korea.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU Institute of Information and Communications Technology Planning and
   Evaluation (IITP) - Korea government (MSIT) [2020-0-00861]; National
   Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF-2017M3C4A7066316]
FX This work was supported by Institute of Information and Communications
   Technology Planning and Evaluation (IITP) Grant funded by the Korea
   government (MSIT) (No. 2020-0-00861) and National Research Foundation of
   Korea (NRF) Grant funded by the Korea government (MSIT) (No.
   NRF-2017M3C4A7066316).
CR Alsaleh R, 2018, TRANSPORT RES REC, V2672, P46, DOI 10.1177/0361198118780708
   [Anonymous], 2013, P SIGCHI C HUM FACT, P3385, DOI [DOI 10.1145/2470654.2466463, 10.1145/]
   [Anonymous], ARCORE GOOGL DEV DOC
   Baldwin CL., 2012, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V56, P1431, DOI [DOI 10.1177/1071181312561404, https://doi.org/10.1177/1071181312561404]
   Bliss J.P., 2010, Proceedings of the Human Factors and Ergonomics Society 54th Annual Meeting, P2373, DOI [10.1177/154193121005402803, DOI 10.1177/154193121005402803]
   BROADBENT DE, 1957, PSYCHOL REV, V64, P205, DOI 10.1037/h0047313
   Cirio G., 2009, Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology, P155, DOI [DOI 10.1145/1643928.1643965, 10.1145/1643928.1643965]
   Cirio G, 2012, IEEE T VIS COMPUT GR, V18, P546, DOI 10.1109/TVCG.2012.60
   de Souza GA, 2018, SYMP VIRTUAL AUGMENT, P163, DOI 10.1109/SVR.2018.00033
   Edworthy J., 1996, Warning design: A research prospective
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Foerster K.T., 2014, P 13 INT C MOB UB MU, P68, DOI [10.1145/2677972.2677973, DOI 10.1145/2677972.2677973]
   Fruin J.J, 1971, DESIGNING PEDESTRIAN, V999
   Gérin-Lajoie M, 2008, GAIT POSTURE, V27, P239, DOI 10.1016/j.gaitpost.2007.03.015
   Haas EC, 1996, COMPUT CONTROL ENG J, V7, P193, DOI 10.1049/cce:19960407
   Haga S, 2015, PROCEDIA MANUF, V3, P2574, DOI 10.1016/j.promfg.2015.07.564
   Hartmann J, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300577
   Hatfield J, 2007, ACCIDENT ANAL PREV, V39, P197, DOI 10.1016/j.aap.2006.07.001
   Hyman IE, 2010, APPL COGNITIVE PSYCH, V24, P597, DOI 10.1002/acp.1638
   Jain S, 2015, 2015 INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND INTERNET OF THINGS (ICGCIOT), P225, DOI 10.1109/ICGCIoT.2015.7380462
   Jain S, 2019, IEEE T MOBILE COMPUT, V18, P1911, DOI 10.1109/TMC.2018.2868659
   Jung J, 2018, INT SYM MIX AUGMENT, P70, DOI 10.1109/ISMAR.2018.00032
   Kanamori K, 2019, 2019 12TH ASIA PACIFIC WORKSHOP ON MIXED AND AUGMENTED REALITY (APMAR), P15, DOI 10.1109/APMAR.2019.8709270
   Kanamori K, 2018, INT SYM MIX AUGMENT, P80, DOI 10.1109/ISMAR.2018.00033
   Kang H, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI [10.1145/3359996.3364256, 10.1109/IVEC.2019.8744769]
   Klingberg T, 2000, PROG BRAIN RES, V126, P95
   Lamberg EM, 2012, GAIT POSTURE, V35, P688, DOI 10.1016/j.gaitpost.2011.12.005
   Lee JC, 2004, PROCEEDINGS OF THE 2004 CHINA-JAPAN JOINT MEETING ON MICROWAVES, P65, DOI 10.1145/985692.985701
   Lewis Bridget A., 2012, P HUM FACT ERG SOC A, V56, P1307, DOI [DOI 10.1177/1071181312561379, 10.1177/1071181312561379]
   Liu XF, 2017, IEEE T MOBILE COMPUT, V16, P394, DOI 10.1109/TMC.2016.2550447
   Mack Arien, 1998, Inattentional Blindness
   Marois R, 2005, TRENDS COGN SCI, V9, P296, DOI 10.1016/j.tics.2005.04.010
   Marshall DC, 2007, HUM FACTORS, V49, P145, DOI 10.1518/001872007779598145
   Nasar J, 2008, ACCIDENT ANAL PREV, V40, P69, DOI 10.1016/j.aap.2007.04.005
   Nasar JL, 2013, ACCIDENT ANAL PREV, V57, P91, DOI 10.1016/j.aap.2013.03.021
   Peon AR, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P657, DOI 10.1109/WHC.2013.6548486
   REITAN R. M., 1958, PERCEPT MOT SKILLS, V8, P271
   Sarter NB, 2002, ADV HUM PER, V2, P13, DOI 10.1016/S1479-3601(02)02004-0
   Schwebel DC, 2008, ACCIDENT ANAL PREV, V40, P1394, DOI 10.1016/j.aap.2008.03.005
   Schwebel DC, 2012, ACCIDENT ANAL PREV, V45, P266, DOI 10.1016/j.aap.2011.07.011
   Scott JJ, 2008, HUM FACTORS, V50, P264, DOI 10.1518/001872008X250674
   Simons DJ, 2000, TRENDS COGN SCI, V4, P147, DOI 10.1016/S1364-6613(00)01455-8
   Stavrinos D, 2011, J SAFETY RES, V42, P101, DOI 10.1016/j.jsr.2011.01.004
   Tang MZ, 2016, IEEE INT CONF MOB, P84, DOI [10.1109/MASS.2016.021, 10.1109/MASS.2016.55]
   Tönnis M, 2006, IEEE SYMPOSIUM ON 3D USER INTERFACES 2006, PROCEEDINGS, P127, DOI 10.1109/TRIDUI.2006.1618282
   Tung YC, 2018, IEEE T MOBILE COMPUT, V17, P1469, DOI 10.1109/TMC.2017.2764909
   Valve Corporation, 2015, US Patent, Patent No. 86558185
   Wang Tianyu., 2012, P 12 WORKSHOP MOBILE, P1
   Warnock D, 2011, LECT NOTES COMPUT SC, V6947, P572, DOI 10.1007/978-3-642-23771-3_43
   Wen JQ, 2015, INT CONF PERVAS COMP, P105, DOI 10.1109/PERCOM.2015.7146516
   Wichmann FA., 2018, Stevens' handbook of experimental psychology and cognitive neuroscience, V5, P1, DOI DOI 10.1002/9781119170174.EPCN507
NR 51
TC 12
Z9 13
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2065
EP 2077
DI 10.1007/s00371-020-01907-4
EA JUL 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA NW1CX
UT WOS:000547242400004
DA 2024-07-18
ER

PT J
AU Huang, K
   Gao, SH
AF Huang, Kun
   Gao, Shenghua
TI Image saliency detection via multi-scale iterative CNN
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Deep convolutional neural networks; Iterative error
   correction
ID OBJECT DETECTION; FEATURES; MODEL
AB Salient object detection has received increasingly more attention and achieved significant progress lately due to the powerful features learned by deep convolutional neural networks (CNNs). In this work, we propose a multi-scale iterative CNN for salient object detection, which has two complementary subnetworks at different spatial scales. For each subnetwork, we augment the CNN structures with an iterative learning process to learn the saliency map, where early stages of the CNN give a rough estimate of the saliency map and the remaining errors are gradually learned to refine the saliency map. By merging predictions of the two subnetworks, the training error can be reduced significantly and the estimated saliency map becomes more accurate. Unlike some previous CNN-based methods which often rely on superpixel segmentations, the proposed model is fully CNN and hence can estimate the saliency map much more efficiently. Extensive experiments on standard benchmarks demonstrate that our method outperforms some of the state-of-the-art methods in terms of both accuracy and speed and achieves as good performance as some recent state-of-the-art end-to-end methods under fair settings.
C1 [Huang, Kun] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China.
   [Huang, Kun; Gao, Shenghua] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
   [Huang, Kun] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Shanghai Institute of Microsystem &
   Information Technology, CAS; ShanghaiTech University; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Huang, K (corresponding author), Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China.; Huang, K (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.; Huang, K (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM huangkun@shanghaitech.edu.cn
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   [Anonymous], 2015, CVPR, DOI DOI 10.1109/CVPR.2015.7298642
   Bharath R, 2013, IEEE INT CONF CON AU, P1503
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen T., 2016, IEEE T NEURAL NETW L, VPP, P1
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   HU X, 2019, ARXIV190310152
   Hu XJ, 2018, AAAI CONF ARTIF INTE, P1330
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Z., 2019, ARXIV190108362
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Yun, 2019, arXiv preprint arXiv:1903.12476
   Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751
   Ma HQ, 2014, ZOOKEYS, P1, DOI 10.3897/zookeys.459.8169
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava R. K., 2015, Advances in Neural Information Processing Systems, P2377
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wu RB, 2013, PROC CVPR IEEE, P867, DOI 10.1109/CVPR.2013.117
   Xiao Lv, 2014, WSEAS Transactions on Computers, V13, P266
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao K, 2018, INT CONF NANO MICRO, P234, DOI 10.1109/NEMS.2018.8556873
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao TY, 2019, PROC CVPR IEEE, P12118, DOI 10.1109/CVPR.2019.01240
   Zhou S., 2019, ARXIV190400048
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zou WB, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.78
NR 61
TC 13
Z9 13
U1 0
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1355
EP 1367
DI 10.1007/s00371-019-01734-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000005
DA 2024-07-18
ER

PT J
AU Zhao, DL
   Du, FH
AF Zhao, Delong
   Du, Fuzhou
TI A novel approach for scale and rotation adaptive estimation based on
   time series alignment
SO VISUAL COMPUTER
LA English
DT Article
DE Template matching; Time series alignment; Scale adaptive estimation;
   Brightness-contrast reconstruction; Rotation invariance
ID PATTERN-RECOGNITION; INVARIANT
AB This paper proposes a novel approach for target scale and rotation adaptive estimation based on template matching, which is robust to undergo brightness, contrast invariance, and noise corruption. Improved features based on ring projection transform are extracted, which can not only improve the matching ability of some special scenes by taking into account changes of pixel intensity and structure information, but also automatically recommend the sampling rings involved in the angle estimation. Moreover, treating image features from the perspective of signal time series, we have designed a hierarchical adaptive estimation strategy to solve the problem of scale invariance while reconstructing the transformation of brightness and contrast. Eliminating the limitations of the pre-prepared fixed-scale vertex template, the proposed approach implements an adaptive estimation of the scale. Additionally, rotation angle calculation based on the normalization cross-correlation can be used as the secondary verification of the candidate solution to further improve the matching accuracy. Numerical evaluation shows that the method enjoys attractive results.
C1 [Zhao, Delong; Du, Fuzhou] Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.
C3 Beihang University
RP Du, FH (corresponding author), Beihang Univ, Sch Mech Engn & Automat, Beijing 100191, Peoples R China.
EM du_fuzhou@163.com
OI Zhao, Delong/0000-0003-2773-2263
CR Alhwarin F, 2010, LECT NOTES COMPUT SC, V6376, P222
   Amiri M, 2010, PATTERN RECOGN, V43, P2485, DOI 10.1016/j.patcog.2009.12.014
   [Anonymous], IEEE T PATTERN ANAL
   Attene M, 2011, VISUAL COMPUT, V27, P991, DOI 10.1007/s00371-011-0622-7
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   [陈彬彬 Chen Binbin], 2015, [计算机应用, Journal of Computer Applications], V35, P2619
   Chen CH, 2013, TAIWAN J OBSTET GYNE, V52, P3, DOI 10.1016/j.tjog.2013.01.002
   Chen GY, 2009, PATTERN RECOGN, V42, P2013, DOI 10.1016/j.patcog.2008.10.008
   Cho HJ, 2010, INT J ADV MANUF TECH, V50, P1033, DOI 10.1007/s00170-010-2567-9
   Chu Guangli, 2014, Applied Mechanics and Materials, V644-650, P1104, DOI 10.4028/www.scientific.net/AMM.644-650.1104
   Dekel T, 2015, PROC CVPR IEEE, P2021, DOI 10.1109/CVPR.2015.7298813
   Fraundorfer F., 2003, AFFINE INVARIANT REG
   Fu G, 2013, J INDIAN SOC REMOTE, V41, P819, DOI 10.1007/s12524-013-0295-y
   Ge J, 2017, IEEE T AUTOM SCI ENG, V99, P1
   Hast A., 2015, INT S IM SIGN PROC A, P107
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Jafri R, 2014, VISUAL COMPUT, V30, P1197, DOI 10.1007/s00371-013-0886-1
   Jia BZ, 2015, VISUAL COMPUT, V31, P281, DOI 10.1007/s00371-014-0918-5
   Jia D, 2016, ELECTRON LETT, V52, P1220, DOI 10.1049/el.2016.1331
   Kanezaki A, 2010, VISUAL COMPUT, V26, P1269, DOI 10.1007/s00371-010-0521-3
   Kawano H, 2010, APPL MECH MATER, V36, P413, DOI 10.4028/www.scientific.net/AMM.36.413
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Kim HY, 2007, LECT NOTES COMPUT SC, V4872, P100
   Kim HY, 2010, PATTERN RECOGN, V43, P859, DOI 10.1016/j.patcog.2009.08.005
   Kong H, 2017, IEEE T IND INFORM, V99, P1
   Korman S, 2013, PROC CVPR IEEE, P2331, DOI 10.1109/CVPR.2013.302
   Kruse D, 2015, IEEE T AUTOM SCI ENG, V12, P4, DOI 10.1109/TASE.2014.2333754
   Li C.K., 2012, INT J REMOTE SENS AP, V2, P41
   Li JJ, 2016, IEEE T AUTOM SCI ENG, V13, P757, DOI 10.1109/TASE.2015.2403836
   Li T, 2014, VISUAL COMPUT, V30, P59, DOI 10.1007/s00371-013-0780-x
   Lin YH, 2008, TEMPLATE MATCHING US
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luhandjula T, 2012, VISUAL COMPUT, V4, P98
   Moravec H. P., 1980, OBSTACLE AVOIDANCE N
   Nandi C.S., 2014, SENSING TECHNOLOGY C, VII
   Quellec G., 2016, ENG MED BIOL SOC 200, P2618
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shih HC, 2015, PATTERN RECOGN, V48, P1707, DOI 10.1016/j.patcog.2014.11.004
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Sit A, 2014, IEEE T IMAGE PROCESS, V23, P2369, DOI 10.1109/TIP.2014.2315923
   Su JD, 2013, INT CONF INFO SCI, P858, DOI 10.1109/ICIST.2013.6747676
   Tanaka K, 2000, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2000.855877
   Tang YY, 1997, IEEE T PATTERN ANAL, V19, P921, DOI 10.1109/34.608296
   Tsai DM, 2002, PATTERN RECOGN, V35, P131, DOI 10.1016/S0031-3203(00)00180-1
   Ullah F, 2004, PATTERN RECOGN, V37, P201, DOI 10.1016/S0031-3203(03)00184-5
   Wang X, 2008, SCI TECHNOL INF, V40, P3503
   Wei L, 2015, INTERNATIONAL CONFERENCE ON SOCIAL SCIENCE AND DEVELOPMENT (ICSSD 2015), P192
   Wiskott L, 1999, INT SER COMPUTAT INT, P355
   Yang H, 2016, IEEE T AUTOM SCI ENG, V13, P1367, DOI 10.1109/TASE.2016.2569558
   Yang YX, 2016, SIGNAL PROCESS, V124, P54, DOI 10.1016/j.sigpro.2015.10.028
   Zhang C, 2015, BRIT MACH VIS C
   Zhang H, 2010, IMAGE VISION COMPUT, V28, P38, DOI 10.1016/j.imavis.2009.04.004
NR 54
TC 7
Z9 7
U1 2
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2020
VL 36
IS 1
BP 175
EP 189
DI 10.1007/s00371-018-1598-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ3OJ
UT WOS:000511966800014
DA 2024-07-18
ER

PT J
AU Liang, Y
   Liu, HZ
   Ma, N
AF Liang, Ye
   Liu, Hongzhe
   Ma, Nan
TI A novel deep network and aggregation model for saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Multi-scale network; Feature pyramid; Saliency
   aggregation
ID OBJECT DETECTION; CONTRAST
AB Recent deep learning-based methods for saliency detection have proved the effectiveness of integrating features with different scales. They usually design various complex architectures of network, e.g., multiple networks, to explore the multi-scale information of images, which is expensive in computation and memory. Feature maps produced with different subsampling convolutional layers have different spatial resolutions; therefore, they can be used as the multi-scale features to reduce the costs. In this paper, by exploiting the in-network feature hierarchy of convolutional networks, we propose a novel multi-scale network for saliency detection (MSNSD) consisting of three modules, i.e., bottom-up feature extraction, top-down feature connection and multi-scale saliency prediction. Moreover, to further boost the performance of MSNSD, an input image-aware saliency aggregation method is proposed based on the ridge regression, which combines MSNSD with some well-performed handcrafted shallow models. Extensive experiments on several benchmarks show that the proposed MSNSD outperforms the state-of-the-art saliency methods with less computational and memory complexity. Meanwhile, our aggregation method for saliency detection is effective and efficient to combine deep and shallow models and make them complementary to each other.
C1 [Liang, Ye; Liu, Hongzhe] Beijing Union Univ, Beijing Key Lab Informat Serv Engn, Beijing, Peoples R China.
   [Ma, Nan] Beijing Union Univ, Coll Robot, Beijing, Peoples R China.
C3 Beijing Union University; Beijing Union University
RP Liang, Y (corresponding author), Beijing Union Univ, Beijing Key Lab Informat Serv Engn, Beijing, Peoples R China.
EM liangye@buu.edu.cn; liuhongzhe@buu.edu.cn; xxtmanan@buu.edu.cn
OI Liang, Ye/0000-0002-1715-4538; Liu, Hongzhe/0000-0003-2314-5272
FU National Natural Science Foundation of China [61871038, 61871039];
   Beijing Natural Science Foundation [4182022]
FX This work was supported in part by the National Natural Science
   Foundation of China (61871038, 61871039) and Beijing Natural Science
   Foundation (4182022).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2013, ICML
   [Anonymous], 2018, MAR GEORESOUR GEOTEC
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen XW, 2017, IEEE I CONF COMP VIS, P1050, DOI 10.1109/ICCV.2017.119
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fuller N., P IEEE C COMP VIS PA
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI DOI 10.1109/CVPR.2010.5539929
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Guo JF, 2017, IEEE INT CON MULTI, P325, DOI 10.1109/ICME.2017.8019389
   HU X, 2018, 32 AAAI C ART INT
   HU X, 2019, ARXIV190310152
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kapoor A, 2017, VISUAL COMPUT, V33, P665, DOI 10.1007/s00371-016-1216-1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li RH, 2017, VISUAL COMPUT, V33, P1155, DOI 10.1007/s00371-016-1278-0
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   LIN TY, 2016, ARXIV161203144 CORR
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Y, 2019, VISUAL COMPUT, V35, P1, DOI [10.1007/s00371-018-01620-3, DOI 10.1007/S00371-018-01620-3]
   MAI L, 2013, PROC CVPR IEEE, P1131, DOI DOI 10.1109/CVPR.2013.150
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin X., P IEEE C COMP VIS PA
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tikhonov A. N., 1978, MATH COMPUT, V32, P491, DOI [10.2307/2006360, DOI 10.2307/2006360]
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   W W, 2016, IEEE T VIS COMPUT GR, V23, P2014
   Wang BY, 2018, VISUAL COMPUT, V34, P645, DOI 10.1007/s00371-017-1404-7
   Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang Wenguan, 2017, IEEE TPAMI, V40, P20
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wenguan Wang, 2018, IEEE Transactions on Image Processing, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2018, 2018 IEEE 23 INT C D, V1, P1
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang CL, 2017, VISUAL COMPUT, V33, P1415, DOI 10.1007/s00371-016-1288-y
   Yang Z, 2017, VISUAL COMPUT, V33, P1403, DOI 10.1007/s00371-016-1287-z
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zheng JB, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON BIG KNOWLEDGE (IEEE ICBK 2017), P320, DOI 10.1109/ICBK.2017.58
   Zhu CB, 2017, IEEE INT CONF COMP V, P2860, DOI 10.1109/ICCVW.2017.337
   Zhu L, 2020, IEEE T VIS COMPUT GR, V26, P2471, DOI 10.1109/TVCG.2018.2889055
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 74
TC 4
Z9 4
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1883
EP 1895
DI 10.1007/s00371-019-01781-9
EA DEC 2019
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000541644600001
DA 2024-07-18
ER

PT J
AU Liu, JC
   Rosin, PL
   Sun, XF
   Xiao, JG
   Lian, ZH
AF Liu, Juncheng
   Rosin, Paul L.
   Sun, Xianfang
   Xiao, Jianguo
   Lian, Zhouhui
TI Image-driven unsupervised 3D model co-segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D segmentation; Image-driven; View estimation
ID SHAPE SEGMENTATION
AB Segmentation of 3D models is a fundamental task in computer graphics and vision. Geometric methods usually lead to non-semantic and fragmentary segmentations. Learning techniques require a large amount of labeled training data. In this paper, we explore the feasibility of 3D model segmentation by taking advantage of the huge number of easy-to-obtain 2D realistic images available on the Internet. The regional color exhibited in images provides information that is valuable for segmentation. To transfer the segmentations, we first filter out inappropriate images with several criteria. The views of these images are estimated by our proposed texture-invariant view estimation Siamese network. The training samples are generated by rendering-based synthesis without laborious labeling. Subsequently, we transfer and merge the segmentations produced by each individual image by applying registration and a graph-based aggregation strategy. The final result is obtained by combining all segmentations within the 3D model set. Our qualitative and quantitative experimental results on several model categories validate effectiveness of our proposed method.
C1 [Liu, Juncheng; Xiao, Jianguo; Lian, Zhouhui] Peking Univ, Inst Comp Sci & Technol, Beijing, Peoples R China.
   [Liu, Juncheng; Rosin, Paul L.; Sun, Xianfang] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 Peking University; Cardiff University
RP Lian, ZH (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing, Peoples R China.
EM liujuncheng@pku.edu.cn; lianzhouhui@pku.edu.cn
RI Xiao, Jian/GYU-4351-2022; Sun, Xianfang/ABG-8970-2021; Lian,
   Zhouhui/D-8432-2012
FU National Natural Science Foundation of China [61672043, 61672056];
   National Key Research and Development Program of China [2017YFB1002601];
   Key Laboratory of Science, Technology and Standard in Press Industry
   (Key Laboratory of Intelligent Press Media Technology); China
   Scholarship Council (CSC)
FX This work was supported by National Natural Science Foundation of China
   (Grant Nos.: 61672043 and 61672056), National Key Research and
   Development Program of China (2017YFB1002601) and Key Laboratory of
   Science, Technology and Standard in Press Industry (Key Laboratory of
   Intelligent Press Media Technology). The support provided by China
   Scholarship Council (CSC) during a visit of Juncheng Liu to Cardiff
   University is acknowledged.
CR ANDRIY M, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI DOI 10.1109/TPAMI.2010.46
   [Anonymous], 2010, ACM SIGGRAPH 2010 papers
   [Anonymous], 2017, P CVPR
   [Anonymous], 2014, P IEEE C COMP VIS PA
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Fish N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982409
   Guo K, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2835487
   Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890
   Huang QX, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024159
   Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai YK, 2008, SPM 2008: PROCEEDINGS OF THE ACM SOLID AND PHYSICAL MODELING SYMPOSIUM, P183
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shu ZY, 2016, COMPUT AIDED GEOM D, V43, P39, DOI 10.1016/j.cagd.2016.02.015
   Sidi O., 2011, UNSUPERVISED COSEGME
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Kaick O, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2611811
   Wang YH, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508393
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Xie ZG, 2014, COMPUT GRAPH FORUM, V33, P85, DOI 10.1111/cgf.12434
   Xu WW, 2014, COMPUT GRAPH FORUM, V33, P107, DOI 10.1111/cgf.12436
NR 30
TC 6
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 909
EP 920
DI 10.1007/s00371-019-01679-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200012
OA Bronze
DA 2024-07-18
ER

PT J
AU Dobashi, Y
   Iwasaki, K
   Okabe, M
   Ijiri, T
   Todo, H
AF Dobashi, Yoshinori
   Iwasaki, Kei
   Okabe, Makoto
   Ijiri, Takashi
   Todo, Hideki
TI Inverse appearance modeling of interwoven cloth
SO VISUAL COMPUTER
LA English
DT Article
DE Cloth rendering; Inverse approach; BRDF
AB This paper proposes an inverse approach for modeling the appearance of interwoven cloth. Creating the desired appearance in cloth is difficult because many factors, such as the type of thread and the weaving pattern, have to be considered. Design tools that enable the desired visual appearance of the cloth to be replicated are therefore beneficial for many computer graphics applications. In this paper, we focus on the design of the appearance of interwoven cloth whose reflectance properties are significantly affected by the weaving patterns. Although there are several systems that support editing of weaving patterns, they lack an inverse design tool that automatically determines the spatially varying bidirectional reflectance distribution function (BRDF) from the weaving patterns required to make the cloth display the desired appearance. We propose a method for computing the cloth BRDFs that can be used to display the desired image provided by the user. We formulate this problem as a cost minimization and solve it by computing the shortest path of a graph. We demonstrate the effectiveness of the method with several examples.
C1 [Dobashi, Yoshinori] Hokkaido Univ, Kita Ku, Kita 14,Nishi 9, Sapporo, Hokkaido, Japan.
   [Iwasaki, Kei] Wakayama Univ, 930 Sakaedani, Wakayama, Japan.
   [Okabe, Makoto] Shizuoka Univ, Naka Ku, 3-5-1 Johoku, Hamamatsu, Shizuoka, Japan.
   [Ijiri, Takashi] Shibaura Inst Technol, Koto Ku, 3-7-5 Toyosu, Tokyo, Japan.
   [Todo, Hideki] Chuo Gakuin Univ, 451 Kujike, Chiba, Japan.
C3 Hokkaido University; Wakayama University; Shizuoka University; Shibaura
   Institute of Technology
RP Dobashi, Y (corresponding author), Hokkaido Univ, Kita Ku, Kita 14,Nishi 9, Sapporo, Hokkaido, Japan.
EM doba@ime.ist.hokudai.ac.jp; iwasaki@sys.wakayama-u.ac.jp; m.o@acm.org;
   ijiri@shibaura-it.ac.jp; todo@fla.cgu.ac.jp
RI Iwasaki, Kei/GNH-6504-2022
OI Iwasaki, Kei/0000-0002-5235-536X
FU JSPS KAKENHI [JP15H05924]; Grants-in-Aid for Scientific Research
   [15H05924] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Number JP15H05924.
CR Adabala N., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P178
   Adabala Neeharika, 2003, P ACM S VIRTUAL REAL, P41
   ALEXA M, 2010, ACM T GRAPHIC, V29, P7
   Aliaga C, 2017, COMPUT GRAPH FORUM, V36, P35, DOI 10.1111/cgf.13222
   [Anonymous], 2013, TEXT LIGHT IND SCI T
   [Anonymous], SIGGRAPH ASIA 2012 C
   [Anonymous], 2011, ACM T GRAPH
   Ashikhmin M, 2000, COMP GRAPH, P65, DOI 10.1145/344779.344814
   Bermano A, 2012, COMPUT GRAPH FORUM, V31, P593, DOI 10.1111/j.1467-8659.2012.03038.x
   Breen D. E., 1992, Visual Computer, V8, P264, DOI 10.1007/BF01897114
   Chen M, 2010, VISUAL COMPUT, V26, P853, DOI 10.1007/s00371-010-0467-5
   Chen YJ, 2012, VISUAL COMPUT, V28, P765, DOI 10.1007/s00371-012-0687-y
   Cirio G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661279
   Cormen T. H., 2009, INTRO ALGORITHMS
   Das S, 2013, FIBER POLYM, V14, P1562, DOI 10.1007/s12221-013-1562-9
   Haan M., 2013, ACM T GRAPHIC, V32
   Huh SB, 2006, VISUAL COMPUT, V22, P434, DOI 10.1007/s00371-006-0019-1
   Irawan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077352
   Iwasaki K, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366163
   Jakob W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778790
   Jeong S, 2011, VISUAL COMPUT, V27, P417, DOI 10.1007/s00371-011-0575-x
   Kaldor JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778842
   Kang YM, 2001, VISUAL COMPUT, V17, P147, DOI 10.1007/s003710100103
   Khungurn P, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2818648
   Lin JJ, 2008, J INF SCI ENG, V24, P949
   Luan FJ, 2017, COMPUT GRAPH FORUM, V36, P123, DOI 10.1111/cgf.13230
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P506, DOI 10.1007/s00371-005-0347-6
   Mitra N. J., 2009, ACM T GRAPHIC, V28, DOI DOI 10.1145/1618452.1618502
   Papas M, 2011, COMPUT GRAPH FORUM, V30, P503, DOI 10.1111/j.1467-8659.2011.01876.x
   Sadeghi I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451240
   Schröder K, 2011, COMPUT GRAPH FORUM, V30, P1277, DOI 10.1111/j.1467-8659.2011.01987.x
   Schröder K, 2015, IEEE T VIS COMPUT GR, V21, P188, DOI 10.1109/TVCG.2014.2339831
   Schwartzburg Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601200
   Velinov Z., 2016, VISION MODELING VISU
   Wang JP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360640
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Wu HZ, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508394
   Wu HZ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024179
   Yue YH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2580946
   Yue YH, 2012, COMPUT GRAPH FORUM, V31, P575, DOI 10.1111/j.1467-8659.2012.03036.x
   Yuksel C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185533
   Zhang R, 2016, RES J TEXT APPAR, V20, P37, DOI 10.1108/RJTA-08-2015-0022
   Zhao S, 2016, ACM T GRAPHIC, V35
   Zhao S., 2013, ACM T GRAPHIC, V32
   Zhao S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185571
NR 45
TC 4
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 175
EP 190
DI 10.1007/s00371-017-1455-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600003
DA 2024-07-18
ER

PT J
AU Zhou, JY
   Wu, HT
   Liu, ZC
   Tong, X
   Guo, BN
AF Zhou, Jingyong
   Wu, Hsiang-Tao
   Liu, Zicheng
   Tong, Xin
   Guo, Baining
TI 3D cartoon face rigging from sparse examples
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Cartoon face animation; Data-driven method; Deformation gradient;
   Blendshape model
AB We present a data-driven method for automatically constructing cartoonized 3D blendshapes of a subject's face. Given a pre-defined blendshape template of the real facial expressions and corresponding cartoonized blendshape template created by an artist, we represent the blendshapes of an identity in the real and cartoon face spaces with the deformations of the blendshape template in each space and learn a mapping between the deformations in the two spaces. To this end, our method decomposes the deformations in each space into two parts: an identity-independent part that is represented with the deformation gradient of the blendshape template, and an identity-dependent part that is modeled by a low-rank linear model. We regress the linear model for the real expressions from a 3D facial expression dataset. An algorithm is then introduced to regress the mapping between the linear models in the two spaces from a small set of real expressions and their cartoonized counterparts. At run time, given the blendshapes of a subject's real face and her 3D cartoon neutral face, our method automatically constructs the cartoonized blendshapes of the subject with the help of the cartoonized blendshape template and the learned mapping. Our method is user-independent and only requires a small set of 3D cartoonized expressions modeled by the artist for cartoon face rigging. We evaluate our method by creating cartoonized 3D facial animations for variant identities in two different artistic styles. The rigging results demonstrate that our method successfully preserves both artistic styles and personalized expressions of different identities.
C1 [Zhou, Jingyong] Tsinghua Univ, Microsoft Res Asia, Beijing, Peoples R China.
   [Wu, Hsiang-Tao; Tong, Xin] Microsoft Res Asia, Internet Graph Grp, Beijing, Peoples R China.
   [Guo, Baining] Microsoft Res Asia, Beijing, Peoples R China.
   [Liu, Zicheng] Microsoft Res, Redmond, WA USA.
C3 Microsoft Research Asia; Microsoft; Tsinghua University; Microsoft;
   Microsoft Research Asia; Microsoft Research Asia; Microsoft; Microsoft
RP Zhou, JY (corresponding author), Tsinghua Univ, Microsoft Res Asia, Beijing, Peoples R China.
EM mikezjy@gmail.com; musclewu@microsoft.com; zliu@microsoft.com;
   xtong@microsoft.com; bainguo@microsoft.com
OI Zhou, Jingyong/0000-0002-3756-1851; Tong, Xin/0000-0001-8788-2453
CR Akleman E., 1997, ACM SIGGRAPH, DOI DOI 10.1145/259081.259231
   Akleman E., 2004, ACM SIGGRAPH 2004 SK, DOI 10.1145/1186223.1186299
   ALEXANDER O., 2009, ACM SIGGRAPH 2009 CO, P12
   [Anonymous], 2010, ACM SIGGRAPH 2010 Papers. SIGGRAPH'10, DOI [DOI 10.1145/1833349.1778769, DOI 10.1145/1778765.1778769]
   [Anonymous], 2011, ACM transactions on graphics (TOG), DOI DOI 10.1145/1964921.1964972
   [Anonymous], ACM T GRAPHICS P SIG
   [Anonymous], 2016, P 29 INT C COMPUTER, DOI DOI 10.1145/2915926.2915936
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bouaziz S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461976
   Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Cosker D, 2011, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2011.6126510
   Fujiwara T., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P490, DOI 10.1109/IM.1999.805381
   Fujiwara T, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P385, DOI 10.1109/IM.2001.924483
   Ichim A., 2015, ACM T GRAPHIC, V45, P1
   Lewis J. P., 2014, EUROGRAPHICS
   Li PF, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P941, DOI 10.1109/ICME.2008.4607591
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu JF, 2009, COMPUT GRAPH FORUM, V28, P2104, DOI 10.1111/j.1467-8659.2009.01418.x
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Waltz RA, 2006, MATH PROGRAM, V107, P391, DOI 10.1007/s10107-004-0560-5
   Xie J., 2009, Proceedings of the ACM International Conference on Multimedia, DOI 10.1145/1631272.1631403
   Zhou JY, 2016, VISUAL COMPUT, V32, P717, DOI 10.1007/s00371-016-1265-5
NR 27
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1177
EP 1187
DI 10.1007/s00371-018-1553-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000005
DA 2024-07-18
ER

PT J
AU Liu, Y
   Zheng, CW
   Zheng, Q
   Yuan, HL
AF Liu, Yu
   Zheng, Changwen
   Zheng, Quan
   Yuan, Hongliang
TI Removing Monte Carlo noise using a Sobel operator and a guided image
   filter
SO VISUAL COMPUTER
LA English
DT Article
DE Adaptive sampling and reconstruction; Guided image filter; Sobel
   operator; Ray tracing
ID RECONSTRUCTION; REGRESSION
AB In this study, a novel adaptive rendering approach is proposed to remove Monte Carlo noise while preserving image details through a feature-based reconstruction. First, noise in the additional features is removed using a guided image filter that reduces the impact of noisy features involving strong motion blur or depth of field. The Sobel operator is then employed to recognize the geometric structures by robustly computing a gradient buffer for each feature. Given the gradient information for high-dimensional features, we compute the optimal filter parameters using a data-driven method. Finally, an error analysis is derived through a two-step smoothing strategy to produce a smooth image and guide the adaptive sampling process. Experimental results indicate that our approach outperforms state-of-the-art methods in terms of visual image quality and numerical error.
C1 [Liu, Yu] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Liu, Yu; Zheng, Changwen; Zheng, Quan; Yuan, Hongliang] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Software, CAS
RP Liu, Y (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Liu, Y (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.
EM lyiscas@163.com; 243434171@qq.com
RI Zheng, Quan/HZH-4993-2023
OI Zheng, Quan/0000-0001-5053-5511
CR [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], BIOGEOSCIENCES
   Bauszat P, 2015, COMPUT GRAPH FORUM, V34, P265, DOI 10.1111/cgf.12511
   Bauszat P, 2015, COMPUT GRAPH FORUM, V34, P597, DOI 10.1111/cgf.12587
   Bauszat P, 2011, COMPUT GRAPH FORUM, V30, P1361, DOI 10.1111/j.1467-8659.2011.01996.x
   Belcour l, 2011, ACM T GRAPHIC, V32
   Ben-Artzi A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356686
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Delbracio M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532708
   Durand F, 2005, ACM T GRAPHIC, V24, P1115, DOI 10.1145/1073204.1073320
   Egan K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944849
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kalantari NK, 2013, COMPUT GRAPH FORUM, V32, P93, DOI 10.1111/cgf.12029
   Kettunen M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766997
   Lehtinen J., 2011, ACM T GRAPHIC, V30
   Lehtinen J., 2012, ACM T GRAPHIC, V31
   Liu XD, 2015, VISUAL COMPUT, V31, P105, DOI 10.1007/s00371-013-0914-1
   Liu XD, 2013, VISUAL COMPUT, V29, P501, DOI 10.1007/s00371-013-0814-4
   Liu XD, 2012, VISUAL COMPUT, V28, P613, DOI 10.1007/s00371-012-0709-9
   Manzi M, 2016, COMPUT GRAPH FORUM, V35, P263, DOI 10.1111/cgf.12829
   Moon B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925936
   Moon B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766992
   Moon B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2641762
   Moon B, 2013, COMPUT GRAPH FORUM, V32, P139, DOI 10.1111/cgf.12004
   Overbeck RS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618486
   Pharr M., 2010, PHYS BASED RENDERING
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632
   Sen P., 2012, ACM T GRAPHIC, V31
   Sen P, 2011, IEEE T VIS COMPUT GR, V17, P487, DOI 10.1109/TVCG.2010.46
   Soler C., 2009, ACM T GRAPHIC, V28
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wu FK, 2015, GRAPH MODELS, V78, P26, DOI 10.1016/j.gmod.2014.12.003
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 40
TC 14
Z9 18
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 589
EP 601
DI 10.1007/s00371-017-1363-z
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400011
DA 2024-07-18
ER

PT J
AU Yuan, HL
   Zheng, CW
AF Yuan, Hongliang
   Zheng, Changwen
TI Adaptive rendering based on robust principal component analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Adaptive rendering; Robust principal component analysis; Propagation
   filter; Monte Carlo ray tracing; Mean squared error
ID RECONSTRUCTION; REGRESSION
AB We propose an adaptive sampling and reconstruction method based on the robust principal component analysis (PCA) to denoise Monte Carlo renderings. Addressing spike noise is a challenging problem in adaptive rendering methods. We adopt the robust PCA as a pre-processing step to efficiently decompose spike noise from rendered image after the image space is sampled. Then we leverage patch-based propagation filter for feature prefiltering and apply the robust PCA to reduce dimensionality in high-dimensional feature space. After that, we estimate a per-pixel pilot bandwidth derived from kernel density estimation and construct the multivariate local linear estimator in the reduced feature space to estimate the value of each pixel. Finally, we distribute additional ray samples in the regions with higher estimated mean squared error if sampling budget remains. We demonstrate that our method makes significant improvement in terms of both numerical error and visual quality compared to the state-of-the-art.
C1 [Yuan, Hongliang; Zheng, Changwen] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.
   [Yuan, Hongliang] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Yuan, HL (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing, Peoples R China.; Yuan, HL (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM 11488336@qq.com; changwen@iscas.ac.cn
FU National High Technology Research and Development Program of China (863
   Program) [2012AA011206]
FX Funding was provided by The National High Technology Research and
   Development Program of China (863 Program) (Grant No. 2012AA011206).
CR [Anonymous], ARXIV E PRINTS
   [Anonymous], 1987, P 14 ANN C COMP GRAP, DOI [DOI 10.1145/37401.37410, DOI 10.1145/37402.37410]
   [Anonymous], BIOGEOSCIENCES
   Belcour L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487239
   Bitterli B, 2016, COMPUT GRAPH FORUM, V35, P107, DOI 10.1111/cgf.12954
   Chang JHR, 2015, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2015.7298595
   Cheng MY, 2006, J MULTIVARIATE ANAL, V97, P1501, DOI 10.1016/j.jmva.2005.05.006
   Dabov Kostadin, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P145
   DeCoro C, 2010, COMPUT GRAPH FORUM, V29, P2119, DOI 10.1111/j.1467-8659.2010.01799.x
   Delbracio M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532708
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Farrugia JP, 2004, COMPUT GRAPH FORUM, V23, P605, DOI 10.1111/j.1467-8659.2004.00792.x
   Hachisuka T., ACM SIGGRAPH 2008 PA
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kalantari NK, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766977
   Kalantari NK, 2013, COMPUT GRAPH FORUM, V32, P93, DOI 10.1111/cgf.12029
   Kristan M, 2011, PATTERN RECOGN, V44, P2630, DOI 10.1016/j.patcog.2011.03.019
   Liu XD, 2015, VISUAL COMPUT, V31, P105, DOI 10.1007/s00371-013-0914-1
   Liu XD, 2013, VISUAL COMPUT, V29, P501, DOI 10.1007/s00371-013-0814-4
   Liu XD, 2012, VISUAL COMPUT, V28, P613, DOI 10.1007/s00371-012-0709-9
   Moon B, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925936
   Moon B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766992
   Moon B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2641762
   Overbeck R. S, 2009, ACM SIGGRAPH ASIA 20
   Pharr M., 2010, PHYS BASED RENDERING
   Rigau J., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P260
   Rousselle F, 2013, COMPUT GRAPH FORUM, V32, P121, DOI 10.1111/cgf.12219
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Wand M.P., 1995, Kernel smoothing. Monographs on statistics and applied probability
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan LQ, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2816814
   Zwicker M, 2015, COMPUT GRAPH FORUM, V34, P667, DOI 10.1111/cgf.12592
NR 35
TC 2
Z9 2
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 551
EP 562
DI 10.1007/s00371-017-1360-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400008
DA 2024-07-18
ER

PT J
AU Cai, JP
   Lin, F
   Lee, YT
   Qian, KM
   Seah, HS
AF Cai, Jianping
   Lin, Feng
   Lee, Yong Tsui
   Qian, Kemao
   Seah, Hock Soon
TI Modeling and dynamics simulation for deformable objects of orthotropic
   materials
SO VISUAL COMPUTER
LA English
DT Article
DE Orthotropic materials; Physically based deformable objects; Finite
   element method; Dynamics simulation
ID REAL-TIME SIMULATION
AB Simulation of physically based deformable models is a hot topic in computer graphics. Most of the existing models focus on isotropic materials and some on transversely isotropic materials. We propose a modeling method for orthotropic materials, which exhibits different mechanical behaviors along three orthogonal directions. First, constraints for the strain energy density in linear elastic models are analyzed, and a positive-definite elasticity tensor is derived for an orthotropic material. Second, an orthotropic deformation controlling frame-field is conceptualized and a frame construction tool is developed for users to define the desired material properties. A quaternion Laplacian smoothing algorithm is proposed, and several user-defined rotation minimizing frames are propagated into the entire body of the deformable object, which forms a smooth frame-field. Third, the corotational linear FEM model coupled with the orthonormal frame-field is formulated to realize a dynamics system, which can deal with large deformations. All the algorithms have been implemented in a comprehensive modeling and simulation system, and a GUI is provided to design the orthotropic model. Experiments on real-time deformation simulation and analytical comparisons are presented.
C1 [Cai, Jianping; Lin, Feng; Qian, Kemao; Seah, Hock Soon] Nanyang Technol Univ, SCE, Singapore, Singapore.
   [Lee, Yong Tsui] Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore.
   [Lin, Feng] Foshan Univ, Foshan Nanyang Inst, Foshan, Guangdong, Peoples R China.
C3 Nanyang Technological University; Nanyang Technological University;
   Foshan University
RP Lin, F (corresponding author), Nanyang Technol Univ, SCE, Singapore, Singapore.
EM kkxiaocai@gmail.com; ASFLIN@ntu.edu.sg; MYTLEE@ntu.edu.sg;
   MKMQIAN@ntu.edu.sg; ASHSSEAH@ntu.edu.sg
RI Qian, Kemao/A-3662-2011; Seah, Hock Soon/AAK-9900-2020
OI Qian, Kemao/0000-0001-6988-3321; Seah, Hock Soon/0000-0003-2699-7147
FU MOE [ARC 4/12, RG139/14]; Singapore National Research Foundation under
   its IDM Futures Funding Initiative
FX Tetrahedral meshes of the fish model and the hosta model are generated
   by the PhysXViewer included in NVIDIA PhysX SDK (v2.7.3) [30]. The
   raptor model (both the surface and tetrahedral mesh) is obtained from
   the demo of the paper [1]. The implementation is based on VegaFEM. This
   work is partially supported by the grants, MOE ARC 4/12 and RG139/14,
   and by the Singapore National Research Foundation under its IDM Futures
   Funding Initiative and administered by the Interactive & Digital Media
   Programme Office, Media Development Authority.
CR Allard J., 2011, GPU COMPUTING GEMS J, P281
   An SS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409118
   [Anonymous], 2006, Technical Report TCDCS- 2006-46
   [Anonymous], 2005, P 2005 S INTERACTIVE, DOI [DOI 10.1145/1053427.10534294, DOI 10.1145/1053427.1053429]
   [Anonymous], 2013, LECT NOTES COMPUTATI, DOI [10.1007/978-94-007-5446-1_6, DOI 10.1007/978-94-007-5446-1_6]
   [Anonymous], 2005, ACMEUROGRAPHICS S CO
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   [Anonymous], 1985, ACM SIGGRAPH COMPUTE
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Cai J., 2014, PACIFIC GRAPHICS SHO, DOI [10.2312/pgs.20141243, DOI 10.2312/PGS.20141243]
   Choi HP, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066605
   Choi MG, 2005, IEEE T VIS COMPUT GR, V11, P91
   Cook R.D., 2003, CONCEPTS APPL FINITE
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Huang J, 2006, VISUAL COMPUT, V22, P740, DOI 10.1007/s00371-006-0058-7
   Kim T, 2012, IEEE T VIS COMPUT GR, V18, P1228, DOI 10.1109/TVCG.2012.78
   Li Yan Li Yan, 2014, Sugar Crops of China, P41
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971
   Liu N., 2012, P GRAPHICS INTERFACE, P193
   Magnenat-Thalmann N., 1988, P GRAPH INT 88, V88
   Martin S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964967
   McDonnell KT, 2007, VISUAL COMPUT, V23, P285, DOI 10.1007/s00371-007-0096-9
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Müller M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964987
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Müller M, 2004, PROC GRAPH INTERF, P239
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   Nvidia D.Z, 2011, NVIDIA PHYSX SDK
   Picinbono G, 2003, GRAPH MODELS, V65, P305, DOI 10.1016/S1524-0703(03)00045-6
   Picinbono G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P596, DOI 10.1109/ROBOT.2000.844118
   Stomakhin Alexey., 2012, Proc. Symp. Comp. Anim, P25
   Takayama K, 2007, LECT NOTES COMPUT SC, V4569, P1, DOI 10.1007/978-3-540-73214-3_1
   ten Thije RHW, 2007, COMPUT METHOD APPL M, V196, P3141, DOI 10.1016/j.cma.2007.02.010
   Terzopoulos D., 1988, Visual Computer, V4, P306, DOI 10.1007/BF01908877
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   Teschner M, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P312, DOI 10.1109/CGI.2004.1309227
   Ting TC, 1996, J APPL MECH, V63, P1056
   Volino P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559762
   Wang HM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964966
   Wang WP, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330513
   Wood W, 1990, PRACTICAL TIME STEPP, V6
   Xu HY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766917
NR 46
TC 10
Z9 11
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1307
EP 1318
DI 10.1007/s00371-016-1221-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000008
DA 2024-07-18
ER

PT J
AU Zhang, SY
   Han, ZZ
   Martin, RR
   Zhang, H
AF Zhang, Suiyun
   Han, Zhizhong
   Martin, Ralph R.
   Zhang, Hui
TI Semantic 3D indoor scene enhancement using guide words
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE Scene enhancement; 3D indoor scene; Scene semantics; Scene decoration;
   Interior design; Submodular set function
AB We propose a novel framework for semantically enhancing a 3D indoor scene in agreement with a user-provided guide word. To do so, we make changes to furniture colors and place small objects in the scene. The relevance of specific furniture colors and small objects to each guide word is learned from a database of annotated images, taking into account both their frequency and specificity to that guide word. Enhancement suggestions are generated by optimizing a scoring function, which combines the relevance of both enhancement factors, i.e., furniture colors and small objects. During optimization, a submodular set function is adopted to ensure that a diverse set of enhancement suggestions is produced. Our experiments show that this framework can generate enhancement suggestions that are both compatible with the input guide word, and comparable to ones designed by humans.
C1 [Zhang, Suiyun; Zhang, Hui] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Han, Zhizhong] Northwestern Polytech Univ, Xian, Shaanxi, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
C3 Tsinghua University; Northwestern Polytechnical University; Cardiff
   University
RP Zhang, H (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM zhangsuiyun13@mails.tsinghua.edu.cn; h312h@mail.nwpu.edu.cn;
   MartinRR@cardiff.ac.uk; huizhang@tsinghua.edu.cn
RI Martin, Ralph R/D-2366-2010; Han, Zhizhong/AAW-4044-2021
OI Martin, Ralph/0000-0002-8495-8536
FU National Natural Science Foundation of China [61373070]; National Key
   Technologies R&D Program of China [2015BAF23B03]; EPSRC
FX This work was supported by the National Natural Science Foundation of
   China (61373070), the National Key Technologies R&D Program of China
   (2015BAF23B03), and an EPSRC Travel Grant.
CR [Anonymous], VISUAL COMPUT
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2366145.2366155
   [Anonymous], 2011, P ADV NEURAL INFORM
   Barinova O, 2010, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2010.5539905
   Chang A.X., 2014, P WORKSH INT LANG LE
   Chen GM, 2016, COMPUT GRAPH-UK, V60, P34, DOI 10.1016/j.cag.2016.08.009
   Chen K., 2015, ACM T GRAPHIC, V34
   Chen XW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P321, DOI 10.1145/2733373.2806274
   Gilks W.R., 1999, Markov Chain Monte Carlo In Practice
   Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Majerowicz L, 2014, IEEE T VIS COMPUT GR, V20, P1507, DOI 10.1109/TVCG.2013.245
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22
   Xu K, 2002, PROC GRAPH INTERF, P25
   Yu LF, 2016, IEEE T VIS COMPUT GR, V22, P1138, DOI 10.1109/TVCG.2015.2417575
   Zhang SY, 2016, PROCEEDINGS VRCAI 2016: 15TH ACM SIGGRAPH CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY, P353, DOI 10.1145/3013971.3014002
NR 19
TC 8
Z9 8
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 925
EP 935
DI 10.1007/s00371-017-1394-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800023
DA 2024-07-18
ER

PT J
AU Dal Corso, A
   Frisvad, JR
   Mosegaard, J
   Baerentzen, JA
AF Dal Corso, Alessandro
   Frisvad, Jeppe Revall
   Mosegaard, Jesper
   Baerentzen, J. Andreas
TI Interactive directional subsurface scattering and transport of emergent
   light
SO VISUAL COMPUTER
LA English
DT Article
DE Subsurface scattering; Global illumination; Interactive rendering;
   Translucent objects; Turbid media
ID TRANSLUCENT OBJECTS
AB Existing techniques for interactive rendering of deformable translucent objects can accurately compute diffuse but not directional subsurface scattering effects. It is currently a common practice to gain efficiency by storing maps of transmitted irradiance. This is, however, not efficient if we need to store elements of irradiance from specific directions. To include changes in subsurface scattering due to changes in the direction of the incident light, we instead sample incident radiance and store scattered radiosity. This enables us to accommodate not only the common distance-based analytical models for subsurface scattering but also directional models. In addition, our method enables easy extraction of virtual point lights for transporting emergent light to the rest of the scene. Our method requires neither preprocessing nor texture parameterization of the translucent objects. To build our maps of scattered radiosity, we progressively render the model from different directions using an importance sampling pattern based on the optical properties of the material. We obtain interactive frame rates, our subsurface scattering results are close to ground truth, and our technique is the first to include interactive transport of emergent light from deformable translucent objects.
C1 [Dal Corso, Alessandro; Frisvad, Jeppe Revall; Baerentzen, J. Andreas] Tech Univ Denmark, Lyngby, Denmark.
   [Mosegaard, Jesper] Alexandra Inst, Aarhus, Denmark.
C3 Technical University of Denmark
RP Dal Corso, A (corresponding author), Tech Univ Denmark, Lyngby, Denmark.
EM alcor@dtu.dk
RI Bærentzen, Jakob Andreas/IZD-8251-2023; Frisvad, Jeppe
   Revall/I-4679-2013; Bærentzen, Jakob Andreas/AAP-9876-2020
OI Bærentzen, Jakob Andreas/0000-0003-2583-0660; Frisvad, Jeppe
   Revall/0000-0002-0603-3669; Bærentzen, Jakob
   Andreas/0000-0003-2583-0660; Dal Corso, Alessandro/0000-0002-4005-7365
CR [Anonymous], 2018, Real-Time Rendering
   Bernabei D, 2012, IEEE COMPUT GRAPH, V32, P34, DOI 10.1109/MCG.2011.106
   Borlum J., 2011, Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics, P7
   CARR N.A., 2003, HWWS 03, P51
   Chang CW, 2008, COMPUT GRAPH FORUM, V27, P517, DOI 10.1111/j.1467-8659.2008.01149.x
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   Christensen P. H., 2012, TECH REP
   d'Eon E., 2014, ACM SIGGRAPH 2014 TA
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dachsbacher C, 2014, COMPUT GRAPH FORUM, V33, P88, DOI 10.1111/cgf.12256
   dEon E., 2007, P EUR S REND TECHN, P147
   Frisvad JR, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682629
   Gerasimov P., 2004, GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics, V20, P193
   Gibson SFF, 1998, LECT NOTES COMPUT SC, V1496, P888, DOI 10.1007/BFb0056277
   Gkioulekas Ioannis, 2013, ACM T GRAPHIC, V32, P13
   Gosselin D., 2004, SHADERX3 ADV RENDERI, V3, P171
   Habel R, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/cgf.12148
   Hable J., 2009, SHADER X7, V7, P161
   HALTON JH, 1964, COMMUN ACM, V7, P701, DOI 10.1145/355588.365104
   HAO X., 2003, I3D 03, P75
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529
   Jimenez J, 2010, IEEE COMPUT GRAPH, V30, P32, DOI 10.1109/MCG.2010.39
   Jimenez J, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609970
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Keller A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P49, DOI 10.1145/258734.258769
   Kniss J, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P109, DOI 10.1109/VISUAL.2002.1183764
   Koa MD, 2014, VISUAL COMPUT, V30, P821, DOI 10.1007/s00371-014-0952-3
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   Li DP, 2013, IEEE T VIS COMPUT GR, V19, P484, DOI 10.1109/TVCG.2012.127
   Mertens T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P130
   Mertens T, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P51, DOI 10.1109/PCCGA.2003.1238246
   Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 2010, PHYS BASED RENDERING
   RUSHMEIER HE, 1990, ACM T GRAPHIC, V9, P1, DOI 10.1145/77635.77636
   Shah MA, 2009, IEEE COMPUT GRAPH, V29, P66, DOI 10.1109/MCG.2009.11
   Sheng Y., 2013, P ACM SIGGRAPH S INT, P63
   Simon G., 2004, GPU GEMS, V1, P263
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Velho L., 2002, IMPLICIT OBJECTS COM, DOI DOI 10.1007/B97350
   Wang John, 2008, International Journal of Information and Decision Sciences, V1, P1
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
   Wang R, 2008, VISUAL COMPUT, V24, P565, DOI 10.1007/s00371-008-0237-9
   Wang YJ, 2010, COMPUT GRAPH FORUM, V29, P497, DOI 10.1111/j.1467-8659.2009.01619.x
   Xu K, 2007, COMPUT GRAPH FORUM, V26, P545, DOI 10.1111/j.1467-8659.2007.01077.x
   Yan LQ, 2012, COMPUT GRAPH FORUM, V31, P2267, DOI 10.1111/j.1467-8659.2012.03220.x
   Zhang F., 2007, GPU GEMS, V10
NR 51
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 371
EP 383
DI 10.1007/s00371-016-1207-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300010
DA 2024-07-18
ER

PT J
AU Yang, L
   Yan, QG
   Xiao, CX
AF Yang, Long
   Yan, Qingan
   Xiao, Chunxia
TI Shape-controllable geometry completion for point cloud models
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud model; Geometry completion; Sharp features; Normal
   propagation; Position sampling
ID APPEARANCE; HOLES
AB Geometry completion is an important operation for generating a complete model. In this paper, we present a novel geometry completion algorithm for point cloud models, which is capable of filling holes on either smooth models or surfaces with sharp features. Our method is built on the physical diffusion pattern. We first decompose each pass hole-boundary contraction into two steps, namely normal propagation and position sampling. Then the normal dissimilarity constraint is incorporated into these two steps to fill holes with sharp features. Our algorithm implements these two steps alternately and terminates until generating no new hole boundary. Experimental results demonstrate its feasibility and validity of recovering the potential geometry shapes.
C1 [Yang, Long; Yan, Qingan; Xiao, Chunxia] Wuhan Univ, Sch Comp, Wuhan, Peoples R China.
   [Yang, Long] Northwest A&F Univ, Coll Informat Engn, Yangling, Peoples R China.
C3 Wuhan University; Northwest A&F University - China
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan, Peoples R China.
EM cxxiao@whu.edu.cn
RI Yan, Qingan/L-8433-2019
OI Yan, Qingan/0000-0002-0257-8004
FU National Basic Research Program of China [2012CB725303]; NSFC
   [61472288]; NCET [NCET-13-0441]; Fundamental Research Funds for the
   Central Universities [2042015kf0181]; State Key Lab of Software
   Engineering [SKLSE-2015-A-05]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and insightful suggestions. This work was partly
   supported by the National Basic Research Program of China (No.
   2012CB725303), the NSFC (No. 61472288), NCET (NCET-13-0441), the
   Fundamental Research Funds for the Central Universities (2042015kf0181),
   and the State Key Lab of Software Engineering (SKLSE-2015-A-05).
CR Adamson A, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P243, DOI 10.1109/SMI.2004.1314511
   Attene M, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431214
   Bac A, 2008, LECT NOTES COMPUT SC, V4975, P272
   Bendels GH, 2006, JOURNAL WSCG, V14, P89
   Berger M., 2014, EUROGRAPHICS 2014 ST, P161
   Campen M., 2012, P 2012 EUR CAGL IT, pt4
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Chalmoviansky P, 2003, LECT NOTES COMPUT SC, V2768, P196
   Chen CY, 2008, IEEE T VIS COMPUT GR, V14, P200, DOI 10.1109/TVCG.2007.70625
   Davis J, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P428, DOI 10.1109/TDPVT.2002.1024098
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gross M., 2011, Point-based graphics
   Harary G, 2014, COMPUT GRAPH FORUM, V33, P45, DOI 10.1111/cgf.12430
   Harary G, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2532548
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Ju T, 2009, J COMPUT SCI TECH-CH, V24, P19, DOI 10.1007/s11390-009-9206-7
   Kazhdan M., 2006, P 4 EUROGRAPHICS S G
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kobbelt L, 2004, COMPUT GRAPH-UK, V28, P801, DOI 10.1016/j.cag.2004.08.009
   [Краевский В.В. Kraevsky V.V.], 2005, [Педагогика, Pedagogika], P13
   Lévy B, 2003, ACM T GRAPHIC, V22, P364, DOI 10.1145/882262.882277
   Ngo H.T.M., 2012, VISIGRAPP, P53
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Ohtake Y, 2005, ACM SIGGRAPH 2005 CO, P173
   Park S, 2005, IEEE I CONF COMP VIS, P1260
   Pernot JP, 2006, COMPUT GRAPH-UK, V30, P892, DOI 10.1016/j.cag.2006.08.020
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Weyrich T., 2004, SPBG 04 S POINT BASE, DOI [10.2312/SPBG/SPBG04/085-094, DOI 10.2312/SPBG/SPBG04/085-094]
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
   Yin KX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661241
   Zhao W, 2007, VISUAL COMPUT, V23, P987, DOI 10.1007/s00371-007-0167-y
NR 33
TC 20
Z9 23
U1 1
U2 44
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 385
EP 398
DI 10.1007/s00371-016-1208-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300011
DA 2024-07-18
ER

PT J
AU Cai, LL
   Nguyen, BP
   Chui, CK
   Ong, SH
AF Cai, Lile
   Nguyen, Binh P.
   Chui, Chee-Kong
   Ong, Sim-Heng
TI A two-level clustering approach for multidimensional transfer function
   specification in volume visualization
SO VISUAL COMPUTER
LA English
DT Article
DE Volume visualization; Volume exploration; Transfer function
   specification; Normalized cut; Self-organizing map
ID TRANSFER-FUNCTION DESIGN; EXPLORATION; CLASSIFICATION; EXTRACTION;
   SYSTEM
AB Multidimensional transfer functions can perform more sophisticated classification of volumetric objects compared to 1-D transfer functions. However, visualizing and manipulating the transfer function space is non-intuitive when its dimension goes beyond 3-D, thus making user interaction difficult. In this paper, we propose to address the multidimensional transfer function design problem by taking a two-level clustering approach, where the first-level clustering by the self-organizing map (SOM) projects high-dimensional feature data to a 2-D topology preserving map, and the second-level clustering on the SOM neurons reduces the design freedom from a large number of SOM neurons to a manageable number of clusters. Based on the two-level clustering results, we propose a novel volume exploration scheme that provides top-down navigation to users exploring the volume. Guided by an informative volume overview, interesting structures in the volume are discovered interactively by the user selecting clusters to visualize and modifying the clustering results when necessary. Our interface keeps track of each interesting structure discovered, which not only enables users to inspect individual structures closely, but also allows them to compose the final visualization by fusing the structures deemed important.
C1 [Cai, Lile; Ong, Sim-Heng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Nguyen, Binh P.; Chui, Chee-Kong] Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore.
C3 National University of Singapore; National University of Singapore
RP Cai, LL (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
EM cailile@u.nus.edu; phubinh@ieee.org; mpecck@nus.edu.sg;
   eleongsh@nus.edu.sg
RI Nguyen, Binh P./N-8193-2013; Ong, Sim-Heng/R-9244-2019
OI Nguyen, Binh P./0000-0001-6203-6664; Ong, Sim-Heng/0000-0003-2766-8150
FU National University of Singapore [WBS: R265-000-446-112]
FX The authors would like to thank all the anonymous reviewers for their
   insightful comments. This work is supported in part by National
   University of Singapore FRC Tier 1 Grant (WBS: R265-000-446-112).
CR [Anonymous], 2003, P WORKSH SELF ORG MA
   Bajaj CL, 1997, VISUALIZATION '97 - PROCEEDINGS, P167, DOI 10.1109/VISUAL.1997.663875
   Bevk M, 2002, COMP MED SY, P239, DOI 10.1109/CBMS.2002.1011383
   Bogdan M., 2001, 9th European Symposium on Artificial Neural Networks. ESANN'2001. Proceedings, P131
   Brugger D, 2008, IEEE T NEURAL NETWOR, V19, P442, DOI 10.1109/TNN.2007.909556
   Caban JJ, 2008, IEEE T VIS COMPUT GR, V14, P1364, DOI 10.1109/TVCG.2008.169
   Cai L, 2013, COMPUT MED IMAG GRAP, V37, P450, DOI 10.1016/j.compmedimag.2013.08.008
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Correa CD, 2009, IEEE T VIS COMPUT GR, V15, P1465, DOI 10.1109/TVCG.2009.189
   Fujishiro I., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P467, DOI 10.1109/VISUAL.1999.809932
   Gajdos P., 2011, ADV INTELLIGENT SYST, V179, P231
   Guo HQ, 2011, IEEE PAC VIS SYMP, P19, DOI 10.1109/PACIFICVIS.2011.5742368
   Haidacher M, 2010, IEEE PAC VIS SYMP, P17, DOI 10.1109/PACIFICVIS.2010.5429615
   HLADUVKA J, 2000, P SPRING C COMP GRAP, V16, P58
   Hsieh TJ, 2014, VISUAL COMPUT, V30, P33, DOI 10.1007/s00371-013-0777-5
   Ip CY, 2012, IEEE T VIS COMPUT GR, V18, P2355, DOI 10.1109/TVCG.2012.231
   Khan N. M., 2013, ADV SELF ORG MAPS, P75
   Kiang MY, 2001, COMPUT STAT DATA AN, V38, P161, DOI 10.1016/S0167-9473(01)00040-8
   Kim HS, 2010, INFORM VISUAL, V9, P167, DOI 10.1057/ivs.2010.6
   Kindlmann G, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P79, DOI 10.1109/SVV.1998.729588
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Lau D. P., 2014, HEAD NECK
   Linsen L, 2008, IEEE T VIS COMPUT GR, V14, P1483, DOI 10.1109/TVCG.2008.167
   Maciejewski R, 2009, IEEE T VIS COMPUT GR, V15, P1473, DOI 10.1109/TVCG.2009.185
   Moutarde F., 2005, P WORKSH SELF ORG MA
   Mu-Chun Su, 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P1906, DOI 10.1109/IJCNN.1999.832672
   Nguyen BP, 2012, VISUAL COMPUT, V28, P181, DOI 10.1007/s00371-011-0634-3
   Opolon D., 2004, P EUR S ART NEUR NET
   Petrilis D, 2008, NEURAL PROCESS LETT, V27, P85, DOI 10.1007/s11063-007-9061-x
   Pinto Franciscode Moura., 2007, Eurographics IEEE-VGTC Symposium on Visualization, P131, DOI DOI 10.2312/VISSYM/EUROVIS07/131-1382
   Sereda P, 2006, IEEE T VIS COMPUT GR, V12, P208, DOI 10.1109/TVCG.2006.39
   Sereda P., 2006, Proceedings of Eurographics/IEEE VGTC Symp on Visualization, P243
   Shi J., 2004, NORMALIZED CUT SEGME
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tzeng F.-Y., 2004, S DATA VISUALISATION, P17, DOI DOI 10.2312/VISSYM/VISSYM04/017-024
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   Ultsch A., 2003, U MATRIX TOOL VISUAL
   Ultsch A., 1993, INFORM CLASSIFICATIO, P307
   Vesanto J, 2000, IEEE T NEURAL NETWOR, V11, P586, DOI 10.1109/72.846731
   Wang L, 2012, IEEE T VIS COMPUT GR, V18, P121, DOI 10.1109/TVCG.2011.23
   Wang Y, 2012, PROCEEDINGS OF THE ASME/JSME/KSME JOINT FLUIDS ENGINEERING CONFERENCE 2011, VOL 1, PTS A-D, P1295
   Wang YH, 2011, IEEE T VIS COMPUT GR, V17, P1560, DOI 10.1109/TVCG.2011.97
   Weber GH, 2007, IEEE T VIS COMPUT GR, V13, P330, DOI 10.1109/TVCG.2007.47
   Wesarg Stefan, 2009, 2009 Second International Conference in Visualisation (VIZ), P153, DOI 10.1109/VIZ.2009.30
   Wu ST, 2004, PATTERN RECOGN, V37, P175, DOI 10.1016/S0031-3203(03)00237-1
   Zhao X, 2010, Vol Graph, P69
NR 48
TC 9
Z9 10
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 163
EP 177
DI 10.1007/s00371-015-1167-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400005
DA 2024-07-18
ER

PT J
AU Omidvar, M
   Ribardière, M
   Carré, S
   Méneveaux, D
   Bouatouch, K
AF Omidvar, Mahmoud
   Ribardiere, Mickael
   Carre, Samuel
   Meneveaux, Daniel
   Bouatouch, Kadi
TI A radiance cache method for highly glossy surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Rendering; Global illumination; Radiance caching; Glossy surfaces
ID GLOBAL ILLUMINATION
AB Radiance caching methods have proven to be efficient for global illumination. Their goal is to compute precisely illumination values (incident radiance or irradiance) at a reasonable number of points lying on the scene surfaces. These points, called records, are stored in a cache used for estimating illumination at other points in the scene. Unfortunately, with records lying on glossy surfaces, the irradiance value alone is not sufficient to evaluate the reflected radiance; each record should also store the incident radiance for all incident directions. Memory storage can be reduced with projection techniques using spherical harmonics or other basis functions. These techniques provide good results for low shininess BRDFs. However, they get impractical for shininess of even moderate value, since the number of projection coefficients increases drastically. In this paper, we propose a new radiance caching method that handles highly glossy surfaces while requiring a low memory storage. Each cache record stores a coarse representation of the incident illumination thanks to a new data structure, called Equivalent Area light Sources, capable of handling fuzzy mirror surfaces. In addition, our method proposes a new simplification of the interpolation process, since it avoids the need for expressing and evaluating complex gradients.
C1 [Omidvar, Mahmoud] Univ Rennes 1, CSTB Nantes, Rennes, France.
   [Ribardiere, Mickael; Meneveaux, Daniel] Univ Poitiers, XLIM SIC, Poitiers, France.
   [Carre, Samuel] CSTB Nantes, Nantes, France.
   [Bouatouch, Kadi] Univ Rennes 1, IRISA, Rennes, France.
C3 Universite de Rennes; Universite de Poitiers; Universite de Rennes
RP Ribardière, M (corresponding author), Univ Poitiers, XLIM SIC, Poitiers, France.
EM mahmoud.omidvar@univ-poitiers.fr; mickael.ribardiere@univ-poitiers.fr;
   samuel.carre@cstb.fr; daniel.meneveaux@univ-poitiers.fr;
   kadi.bouatouch@irisa.fr
OI Meneveaux, Daniel/0000-0001-7160-3026; Ribardiere,
   Mickael/0000-0003-2964-2608
CR Brady A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601193
   Gassenbauer V, 2009, COMPUT GRAPH FORUM, V28, P1189, DOI 10.1111/j.1467-8659.2009.01496.x
   Gautron P., 2004, RENDERING TECHNIQUES, P321
   Herzog R, 2009, COMPUT GRAPH FORUM, V28, P259, DOI 10.1111/j.1467-8659.2009.01365.x
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen H. W., 1996, Rendering Techniques '96. Proceedings of the Eurographics Workshop. Eurographics, P21
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Krivánek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83
   Krivanek J., 2006, P EUROGRAPHICS S REN, V2006, P127, DOI DOI 10.2312/EGWR/EGSR06/127-138
   Lafortune E. P., 1993, EDUGRAPHICS '93. First International Conference on Graphics Education. COMPUGRAPHICS '93. Third International Conference on Computational Graphics and Visualization Techniques. Combined Proceedings, P145
   Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801
   Lewis Robert R., 1994, Fourth Eurographics Workshop on Rendering, P47
   Meunier S, 2010, COMPUT GRAPH-UK, V34, P767, DOI 10.1016/j.cag.2010.08.002
   Ribardière M, 2011, COMPUT GRAPH FORUM, V30, P1603, DOI 10.1111/j.1467-8659.2010.01846.x
   Scherzer D, 2012, COMPUT GRAPH FORUM, V31, P1391, DOI 10.1111/j.1467-8659.2012.03134.x
   Schwarzhaupt J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366212
   Serpaggi X, 2001, COMPUT GRAPH FORUM, V20, pC278, DOI 10.1111/1467-8659.00520
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Veach E., 1998, Robust Monte Carlo Methods for Light Transport Simulation
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Zaninetti J, 1998, COMPUT GRAPH FORUM, V17, pC149, DOI 10.1111/1467-8659.00262
   [No title captured]
NR 24
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1239
EP 1250
DI 10.1007/s00371-015-1159-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800003
DA 2024-07-18
ER

PT J
AU Liu, DW
   Klette, R
AF Liu, Dongwei
   Klette, Reinhard
TI Fog effect for photography using stereo vision
SO VISUAL COMPUTER
LA English
DT Article
DE Stereo vision; Computational photography; Image-based rendering;
   Stereo-map refinement; Fog effect
ID IMAGES; DEPTH
AB Fog is an important factor in photography with a special aesthetic, emotional, or compositional meaning. We present a fog-simulation method for photo editing using binocular stereo vision. Given a stereo pair, we estimate the depth information by stereo matching followed by a process to refine depth results for the given photo editing purpose. Then, depth-aware fog effects can be applied on the base image, with optional interaction for control purposes. Besides homogeneous fog, we provide three tools to control the density of the fog media. Thus, various kinds of heterogeneous atmospheric effects can also been simulated. Experiments show that the proposed method can achieve more natural-looking results than manually drawn fog, our results are very close to the appearance of fog in the real world.
C1 [Liu, Dongwei] Univ Auckland, Auckland 1, New Zealand.
   [Klette, Reinhard] Auckland Univ Technol, Auckland, New Zealand.
C3 University of Auckland; Auckland University of Technology
RP Liu, DW (corresponding author), Univ Auckland, Auckland 1, New Zealand.
EM dliu697@aucklanduni.ac.nz; rklette@aut.ac.nz
RI Klette, Reinhard/B-7018-2012
OI Klette, Reinhard/0000-0001-8818-7145; Liu, Dongwei/0000-0002-0697-0548
FU China Scholarship Council
FX The authors thank Simon Hermann for providing a lib of the iSGM matcher.
   This project is supported by the China Scholarship Council.
CR Abbott J, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P263, DOI 10.1109/3DV.2013.42
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Biri V., 2002, P EUR WORKSH REND, P1
   Blais F, 2004, J ELECTRON IMAGING, V13, P231, DOI 10.1117/1.1631921
   Carr P, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P103, DOI 10.1109/DICTA.2009.25
   Cerezo E, 2005, VISUAL COMPUT, V21, P303, DOI 10.1007/s00371-005-0287-1
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Engelhardt T., 2010, Proceedings_of_the_2010_ ACM_SIGGRAPH_symposium_on_Interactive_3D_Graphics_and_Games, P119
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Gardner G. Y., 1985, Computer Graphics, V19, P297, DOI 10.1145/325165.325248
   Grewe L, 1998, P SOC PHOTO-OPT INS, V3376, P102, DOI 10.1117/12.303670
   Hachisuka T., 2013, SIGGRAPH ASIA COURSE, V15
   Hermann Simon, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P465, DOI 10.1007/978-3-642-37431-9_36
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Hu W., 2010, Proceedings of the 2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games, I3D 2010, P109
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Klette R., 2014, CONCISE COMPUTER VIS
   Klette R., 2004, DIGITAL GEOMETRY GEO
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Matsuo Takuya, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P300
   Ng R., 2005, [31] R. Ng, M. Levoy, M. Bredif, G. Duval, M. Horowitz, and P. Hanrahan, "Light field photography with a hand-held plenoptic camera," Stanford University Com- puter Science Tech Report CSTR 2005-02, 2005.
   Oh BM, 2001, COMP GRAPH, P433
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Schaul L, 2009, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2009.5413700
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Schechner YY, 2000, INT J COMPUT VISION, V39, P141, DOI 10.1023/A:1008175127327
   Shum HY, 2000, PROC SPIE, V4067, P2, DOI 10.1117/12.386541
   Xu Z., 2009, 2009 INT C COMP INT, P1
   Zhou K, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P116, DOI 10.1109/PG.2007.48
NR 36
TC 5
Z9 5
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2016
VL 32
IS 1
BP 99
EP 109
DI 10.1007/s00371-014-1058-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DF9FQ
UT WOS:000371666800009
DA 2024-07-18
ER

PT J
AU Huang, RJ
   Ye, M
   Xu, P
   Li, T
   Dou, YM
AF Huang, Renjie
   Ye, Mao
   Xu, Pei
   Li, Tao
   Dou, Yumin
TI Learning to pool high-level features for face representation
SO VISUAL COMPUTER
LA English
DT Article
DE Learning pooling scheme; High-level facial features; Sparse coding; Face
   recognition
ID RECOGNITION; VERIFICATION
AB The available face descriptors are always generated by a hand-designed pooling scheme or without a pooling process. We propose to learn a pooling scheme for high-level features. First, we obtain the local features on the densely sampled points on a face image. Then, a weighted-sum pooling is used to obtain the high-level feature of a block of this face image. By learning the pooling weights, the structure information of local features is integrated into the high-level feature of the block. At the same time, a linear transformation is learned to reduce the dimension of this high-level feature. Our main contribution is the method of learning the pooling scheme, which can capture the structure information between the local features in a block. This structure information includes the facial structures and contours. The experiments on multiple face datasets confirm the efficiency and effectiveness of our method.
C1 [Huang, Renjie; Ye, Mao; Xu, Pei; Li, Tao; Dou, Yumin] Univ Elect Sci & Technol China, Minist Educ, Key Lab NeuroInformat, Ctr Robot,Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Huang, Renjie] Southwest Univ, Sch Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Li, Tao] HeNan Radio & Televis Univ, Dept Informat Engn, Zhengzhou 450046, Peoples R China.
C3 University of Electronic Science & Technology of China; Southwest
   University - China
RP Ye, M (corresponding author), Univ Elect Sci & Technol China, Minist Educ, Key Lab NeuroInformat, Ctr Robot,Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM huangrj@swu.edu.cn; cvlab.uestc@gmail.com; xupei268@sina.com;
   cvlablitao@gmail.com; dymcdut@sina.com
RI Huang, Ren/HJG-5652-2022; Ye, Mao/K-3012-2019
OI Huang, Ren/0000-0002-3068-8331; Ye, Mao/0000-0001-9253-1332; Ye,
   Mao/0000-0003-4760-8702; Huang, Renjie/0000-0002-9996-3354
FU National Natural Science Foundation of China [61375038]; Fundamental
   Research Funds for the Central Universities [XDJK2013C122]; Engineering
   and Technological Research Center of Intelligent Instrument and
   Controlling Device of ChongQing
FX This work was supported in part by the National Natural Science
   Foundation of China (61375038) and the Fundamental Research Funds for
   the Central Universities (XDJK2013C122). This work was also partly
   supported by Engineering and Technological Research Center of
   Intelligent Instrument and Controlling Device of ChongQing.
CR [Anonymous], 2010, P 27 INT C MACHINE L
   [Anonymous], 2007, 0749 U MASS
   [Anonymous], 2014, CVPR
   [Anonymous], 2003, INT C INT C MACH LEA, DOI DOI 10.1016/0026-2714(92)90278-S
   Arigbabu OA, 2015, VISUAL COMPUT, V31, P513, DOI 10.1007/s00371-014-0990-x
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Cox D., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   Davis J. V., 2007, ICML, P209
   de Campos T, 2012, COMPUT VIS IMAGE UND, V116, P68, DOI 10.1016/j.cviu.2011.07.011
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Hoyer PO, 2002, NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS, P557, DOI 10.1109/NNSP.2002.1030067
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Huang RJ, 2014, INT J CONTROL AUTOM, V12, P833, DOI 10.1007/s12555-013-0294-3
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1
   Jolliffe T., 2005, PRINCIPAL COMPONENT, P11
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leey C., 2014, CVPR
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Lin Dahua., 2014, CVPR
   Lu CY, 2013, J VIS COMMUN IMAGE R, V24, P111, DOI 10.1016/j.jvcir.2012.05.003
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Seo HJ, 2011, IEEE T INF FOREN SEC, V6, P1275, DOI 10.1109/TIFS.2011.2159205
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Singh C, 2012, VISUAL COMPUT, V28, P1085, DOI 10.1007/s00371-011-0659-7
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Vanderbei RJ, 1999, COMPUT OPTIM APPL, V13, P231, DOI 10.1023/A:1008677427361
   Wang HY, 2013, MACH VISION APPL, V24, P1121, DOI 10.1007/s00138-013-0488-y
   WOLF L., 2009, ACCV
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Xu Can., 2014, CVPR
   Xu P., 2013, OPTICAL ENG, V52
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang W., 2005, ICCV
   Zhen Cui, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P149, DOI 10.1109/FG.2011.5771389
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 41
TC 4
Z9 4
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1683
EP 1695
DI 10.1007/s00371-014-1049-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900008
DA 2024-07-18
ER

PT J
AU Procházka, A
   Kasparová, M
   Yadollahi, M
   Vysata, O
   Grajciarová, L
AF Prochazka, Ales
   Kasparova, Magdalena
   Yadollahi, Mohammadreza
   Vysata, Oldrich
   Grajciarova, Lucie
TI Multi-camera systems use for dental arch shape measurement
SO VISUAL COMPUTER
LA English
DT Article
DE Image acquisition; Multi-camera systems; Geometric modeling; Digital
   signal processing; Computational intelligence; Cleft palate
ID VIRTUAL-REALITY; SURGERY; LIP
AB This paper presents the analysis of dental cast images and new mathematical methods for their evaluation. While depth sensors of sophisticated scanners allow precise spatial data processing, it is possible to use a multi-camera system for direct data acquisition and recognition of selected objects as well. The main goal of this paper is the analysis of three-dimensional objects to enable numerical evaluation of measurements important for proposals of the appropriate treatment after dental operations. Methods presented include (1) computer analysis of a single de-noised and thresholded image used for detection of its components and (2) presentation of tools for three-dimensional modeling using a double camera system. The proposed algorithms allow semi-automatic evaluation of measurements between selected objects. The resulting general algorithm can be used both in biomedical applications and engineering to detect measurements in static image frames or videosequences.
C1 [Prochazka, Ales; Yadollahi, Mohammadreza; Grajciarova, Lucie] Inst Chem Technol, Dept Comp & Control Engn, Tech 5, CR-16628 Prague 6, Czech Republic.
   [Vysata, Oldrich] Charles Univ Prague, Fac Med Hradec Kralove, Dept Neurol, Hradec Kralove 50005, Czech Republic.
   [Kasparova, Magdalena] Charles Univ Prague, Fac Med 2, Dept Stomatol, Prague 15006 5, Czech Republic.
C3 University of Chemistry & Technology, Prague; Charles University Prague;
   University Hospital Hradec Kralove; Charles University Prague
RP Procházka, A (corresponding author), Inst Chem Technol, Dept Comp & Control Engn, Tech 5, CR-16628 Prague 6, Czech Republic.
EM A.Prochazka@ieee.org; Magdalena.Kasparova@fnmotol.cz; Yadollao@vscht.cz;
   Vysatao@gmail.com; Lucie.Grajciarova@vscht.cz
RI Vysata, Oldrich/O-6699-2017
OI Vysata, Oldrich/0000-0003-3649-6688
CR [Anonymous], C MATLAB 2005
   [Anonymous], 48 INT S ELMAR 2006
   [Anonymous], THESIS I CHEM TECHNO
   [Anonymous], SBORN K MATLAB 2004
   [Anonymous], DAT COMPR C DCC 02
   [Anonymous], INT J ORTHOD ORAL SU
   [Anonymous], SIGNAL ANAL PREDICTI
   [Anonymous], 1904, CLEFT PALATE HARE LI
   [Anonymous], 2007, ANN C TECHN COMP
   [Anonymous], APPL CLIFFORD ALGEBR
   [Anonymous], PROCRRD COMP VIS PAT
   BOHN A, 1963, ACTA ODONTOL SCA S38, V21, P98
   Bracewell R., 2003, FOURIER ANAL IMAGING
   Chang JS, 2005, LECT NOTES ARTIF INT, V3681, P582
   Chang YB, 2010, IEEE T MED IMAGING, V29, P1652, DOI 10.1109/TMI.2010.2049526
   Chapuis J, 2007, IEEE T INF TECHNOL B, V11, P274, DOI 10.1109/TITB.2006.884372
   Dostálová T, 2006, METHOD INFORM MED, V45, P191
   Dugelay Jean-Luc., 2008, 3D Object Processing: Compression, Indexing and Watermarking
   Franken T, 2005, VISUAL COMPUT, V21, P619, DOI 10.1007/s00371-005-0309-z
   Fujimoto I, 2012, MEAS SCI TECHNOL, V23, DOI 10.1088/0957-0233/23/11/115102
   Gao K, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.2.027201
   Gonzales Rafael C., 2009, Digital Image Processing Using MATLAB
   Gráfová L, 2013, DENTOMAXILLOFAC RAD, V42, DOI 10.1259/dmfr.20120391
   Harrell WE, 2002, AM J ORTHOD DENTOFAC, V122, P325, DOI 10.1067/mod.2002.126147
   Hashimoto T, 2013, ACTA POLYTECH HUNG, V10, P139
   Huang JH, 2012, MEAS SCI TECHNOL, V23, DOI 10.1088/0957-0233/23/12/125402
   Jiang Y, 1998, PERCEPT PSYCHOPHYS, V60, P275, DOI 10.3758/BF03206036
   Kasparová M, 2014, BIOMED ENG ONLINE, V13, DOI 10.1186/1475-925X-13-68
   Kasparova M, 2013, BIOMED ENG ONLINE, V12, DOI 10.1186/1475-925X-12-49
   Keating AP, 2008, J ORTHOD, V35, P191, DOI 10.1179/146531207225022626
   Kim L, 2006, VISUAL COMPUT, V22, P90, DOI 10.1007/s00371-006-0369-8
   Kljuno E, 2008, J INTELL ROBOT SYST, V52, P79, DOI 10.1007/s10846-008-9204-y
   Kummer AW, 2014, CLEFT PALATE CRANIOF
   Ma Yaqi, 2010, 2010 International Conference on Image Analysis and Signal Processing (IASP 2010), P336, DOI 10.1109/IASP.2010.5476100
   Nixon Mark S, 2012, FEATURE EXTRACTION I, DOI DOI 10.1016/B978-0-12-396549-3.00007-0
   Ogodescu A.E., 2011, International Journal of Biology and Biomedical Engineering, V1, P32
   Popielski P, 2014, ADV INTELL SYST, V283, P209, DOI 10.1007/978-3-319-06593-9_19
   Procházka A, 2004, 8TH WORLD MULTI-CONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL VI, PROCEEDINGS, P82
   Quan W, 2012, MEAS SCI TECHNOL, V23, DOI 10.1088/0957-0233/23/12/125407
   Ramalingam S, 2013, INT J COMPUT VISION, V102, P73, DOI 10.1007/s11263-012-0576-x
   Ringer M., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P172
   RINGER M, 2002, LECT NOTES COMPUTER, V2350
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   SEMB G, 1991, CLEFT PALATE-CRAN J, V28, P1, DOI 10.1597/1545-1569(1991)028<0001:ASOFGI>2.3.CO;2
   Song XB, 2014, VISUAL COMPUT, V30, P855, DOI 10.1007/s00371-014-0965-y
   Trotman CA, 2010, J DENT RES, V89, P728, DOI 10.1177/0022034510365485
   Vaseghi S., 2000, ADV SIGNAL PROCESSIN
   Xia PJ, 2013, VISUAL COMPUT, V29, P433, DOI 10.1007/s00371-012-0748-2
   YAMAMOTO K, 1991, IEEE T BIO-MED ENG, V38, P360, DOI 10.1109/10.133232
   Zhang ZY, 2012, APPL OPTICS, V51, P1638, DOI 10.1364/AO.51.001638
NR 50
TC 5
Z9 5
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2015
VL 31
IS 11
BP 1501
EP 1509
DI 10.1007/s00371-014-1029-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CT3BJ
UT WOS:000362680600006
DA 2024-07-18
ER

PT J
AU Hua, M
   Wang, WC
AF Hua, Miao
   Wang, Wencheng
TI Effective structure restoration for image completion using internet
   resources
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Image completion; Structure retrieval; Iterative closest point
ID REGISTRATION
AB Structure restoration plays an important role in image completion. Though much progress has been made, existing techniques are ineffective to restore plausible complex structures, due to the high complexity of structure registration between neighboring regions. We address this challenge by taking two measures. The first is to get the images whose structures are potential to be well merged with the structures around the hole. This can be achieved by effectively retrieving the mass images on the internet, which provide enough candidates. The second is to coherently blend the retrieved structures with the structures around the hole, which is by suppressing the connection differences between these structures. Experimental results show that we can easily complete images, especially with complex structures or for filling large holes.
C1 [Hua, Miao; Wang, Wencheng] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Hua, Miao] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Wang, WC (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
EM huam@ios.ac.cn; whn@ios.ac.cn
RI Wang, Wencheng/A-3828-2009
FU National Natural Science Foundation of China [61379087]; Chinese Academy
   of Sciences; European Union [612627]
FX This work is partially supported by National Natural Science Foundation
   of China (61379087), the Knowledge Innovation Program of the Chinese
   Academy of Sciences and the European Unions Seventh Framework Programme
   (FP7/2007-2013) under grant agreement (612627).
CR [Anonymous], 2011, IEEE T VIS COMPUT GR, DOI DOI 10.1109/TVCG.2010.266
   [Anonymous], 2006, COMPUTER VISION PATT
   [Anonymous], 2007, CVPR
   [Anonymous], ACM T GRAPH
   [Anonymous], CVPR
   [Anonymous], ACM SIGGRAPH
   Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Goldberg C, 2012, COMPUT GRAPH FORUM, V31, P265, DOI 10.1111/j.1467-8659.2012.03005.x
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   Ho J, 2009, IEEE I CONF COMP VIS, P1335, DOI 10.1109/ICCV.2009.5459309
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373
   Liu MY, 2010, PROC CVPR IEEE, P1696, DOI 10.1109/CVPR.2010.5539837
   Mansfield A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.121
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   NILL NB, 1992, OPT ENG, V31, P813, DOI 10.1117/12.56114
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Wexler Y, 2004, PROC CVPR IEEE, P120
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
NR 32
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 1113
EP 1122
DI 10.1007/s00371-015-1126-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500035
DA 2024-07-18
ER

PT J
AU Mu, TJ
   Sun, JJ
   Martin, RR
   Hu, SM
AF Mu, Tai-Jiang
   Sun, Jia-Jia
   Martin, Ralph R.
   Hu, Shi-Min
TI A response time model for abrupt changes in binocular disparity
SO VISUAL COMPUTER
LA English
DT Article
DE Stereoscopy; Perception; Response time; Visual comfort
ID OCULAR VERGENCE; ACCOMMODATION; CONVERGENCE; DEPTH; ADAPTATION
AB We propose a novel depth perception model to determine the time taken by the human visual system (HVS) to adapt to an abrupt change in stereoscopic disparity, such as can occur in a scene cut. A series of carefully designed perceptual experiments on successive disparity contrast were used to build our model. Factors such as disparity, changes in disparity, and the spatial frequency of luminance contrast were taken into account. We further give a computational method to predict the response time during scene cuts in stereoscopic cinematography, which has been validated in user studies. We also consider various applications of our model.
C1 [Mu, Tai-Jiang; Sun, Jia-Jia; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing 100084, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales.
C3 Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing 100084, Peoples R China.
EM ralph@cs.cf.ac.uk; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Martin, Ralph R/D-2366-2010; Zhang,
   Shuo/IUO-8909-2023; Li, Ye/JBS-2949-2023; Mu, Tai-Jiang/JWO-1381-2024;
   sun, jia/IAM-5263-2023
OI Mu, Tai-Jiang/0000-0002-9197-346X; Martin, Ralph/0000-0002-8495-8536
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61272226, 61120106007]; National High Technology
   Research and Development Program of China [2013AA013903]; Tsinghua
   University Initiative Scientific Research Program; EPSRC [EP/J009830/1]
   Funding Source: UKRI
FX We would like to thank the anonymous reviewers for their insightful
   comments. This work was supported by the National Basic Research Project
   of China (No. 2011CB302205), the Natural Science Foundation of China
   (No. 61272226 and 61120106007), the National High Technology Research
   and Development Program of China (No. 2013AA013903) and Tsinghua
   University Initiative Scientific Research Program.
CR Akeley K, 2004, ACM T GRAPHIC, V23, P804, DOI 10.1145/1015706.1015804
   Basha T, 2011, IEEE I CONF COMP VIS, P1816, DOI 10.1109/ICCV.2011.6126448
   BURT P, 1980, SCIENCE, V208, P615, DOI 10.1126/science.7367885
   Celikcan U, 2013, VISUAL COMPUT, V29, P685, DOI 10.1007/s00371-013-0804-6
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Choi J, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.1.017206
   Choi J, 2010, IEEE IMAGE PROC, P2981, DOI 10.1109/ICIP.2010.5653389
   CUMMING BG, 1986, J NEUROPHYSIOL, V55, P896, DOI 10.1152/jn.1986.55.5.896
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Didyk P., 2012, ACM T GRAPHIC, V31
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Du SP, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508387
   Du SP, 2013, IEEE T VIS COMPUT GR, V19, P1288, DOI 10.1109/TVCG.2013.14
   Emoto M, 2005, J DISP TECHNOL, V1, P328, DOI 10.1109/JDT.2005.858938
   ERKELENS CJ, 1986, J PHYSIOL-LONDON, V379, P145, DOI 10.1113/jphysiol.1986.sp016245
   ERKELENS CJ, 1989, PROC R SOC SER B-BIO, V236, P441, DOI 10.1098/rspb.1989.0031
   Heinzle S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964989
   Hess RF, 1999, VISION RES, V39, P559, DOI 10.1016/S0042-6989(98)00127-8
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Howard I. P., 2012, PERCEIVING IN DEPTH, V2
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Hulusic V, 2013, VISUAL COMPUT, V29, P1159, DOI 10.1007/s00371-012-0760-6
   Ide S, 2002, P SOC PHOTO-OPT INS, V4660, P38, DOI 10.1117/12.468065
   Inoue T, 1997, APPL OPTICS, V36, P4509, DOI 10.1364/AO.36.004509
   Kim D, 2011, IEEE T CIRC SYST VID, V21, P231, DOI 10.1109/TCSVT.2011.2106275
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee B, 1997, VISION RES, V37, P1769, DOI 10.1016/S0042-6989(96)00274-X
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Lee S, 2014, VISUAL COMPUT, V30, P455, DOI 10.1007/s00371-013-0868-3
   Liao M, 2012, IEEE T VIS COMPUT GR, V18, P1079, DOI 10.1109/TVCG.2011.114
   Liu C., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), P253, DOI DOI 10.1145/2072298.2072332
   Lo WY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866173
   Luo SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366201
   MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029
   Masia B, 2013, COMPUT GRAPH-UK, V37, P983, DOI 10.1016/j.cag.2013.06.004
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Mu TJ, 2014, VISUAL COMPUT, V30, P833, DOI 10.1007/s00371-014-0961-2
   Neri P, 2004, J NEUROPHYSIOL, V92, P1880, DOI 10.1152/jn.01042.2003
   Niu N, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047362
   Oskam T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024223
   Palmer S., 1999, VISION SCI PHOTONS P
   Pan N, 2013, VISUAL COMPUT, V29, P277, DOI 10.1007/s00371-012-0773-1
   Pollock B, 2012, IEEE T VIS COMPUT GR, V18, P581, DOI 10.1109/TVCG.2012.58
   RASHBASS C, 1961, J PHYSIOL-LONDON, V159, P339, DOI 10.1113/jphysiol.1961.sp006812
   Schor C, 1999, OPHTHAL PHYSL OPT, V19, P134, DOI 10.1046/j.1475-1313.1999.00409.x
   SCHOR CM, 1992, OPTOMETRY VISION SCI, V69, P258, DOI 10.1097/00006324-199204000-00002
   SCHOR CM, 1986, VISION RES, V26, P927, DOI 10.1016/0042-6989(86)90151-3
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Smith BM, 2009, PROC CVPR IEEE, P485, DOI 10.1109/CVPRW.2009.5206793
   Takagi M, 2001, INVEST OPHTH VIS SCI, V42, P1479
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Yang S., 2013, SPIE, V8648
   Yong J. J., 2012, J ELECTRON IMAGING, V21
   Zhang GF, 2007, IEEE T VIS COMPUT GR, V13, P686, DOI 10.1109/TVCG.2007.1032
NR 56
TC 10
Z9 13
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 675
EP 687
DI 10.1007/s00371-014-0994-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400013
DA 2024-07-18
ER

PT J
AU Tian, XL
   Jiao, LC
   Zheng, XL
   Zhang, XH
AF Tian, Xiaolin
   Jiao, Licheng
   Zheng, Xiaoli
   Zhang, Xiaohua
TI Inter-frame constrained coding based on superpixel for tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Tracking; Superpixels; Coding
ID ROBUST
AB A video tracking method based on superpixel with inter-frame constrained coding is proposed in this paper. The 3-D CIE Lab color feature is extracted at each superpixel to characterize local image information. Based on the color feature, the superpixel-based coding model is achieved between the adjacent frames for correctly tracking object. The proposed tracking method considers the interaction of corresponding superpixels between the adjacent frames of complex scenes, which enhances the stability of encoding. Due to the update of codebook and classifier parameter, the proposed method is robust for long-term object tracking. We test the proposed method on 15 challenging sequences involving drastic illumination change, partial or full occlusion, and large pose variation. The proposed method shows excellent performance in comparison with eight previously proposed trackers.
C1 [Tian, Xiaolin; Jiao, Licheng; Zheng, Xiaoli; Zhang, Xiaohua] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Int Res Ctr Intelligent Percept & Computat, Minist Educ,Inst Intelligent Informat Proc, Xian 710071, Peoples R China.
C3 Xidian University
RP Tian, XL (corresponding author), Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Int Res Ctr Intelligent Percept & Computat, Minist Educ,Inst Intelligent Informat Proc, POB 224, Xian 710071, Peoples R China.
EM xltian@mail.xidian.edu.cn
RI Jiao, Licheng/JOZ-0842-2023; Zhang, Xiaohua/D-5348-2012
OI Jiao, Licheng/0000-0003-3354-9617; 
FU Natural Science Basic Research Plan in Shaanxi Province of China
   [2014JM8301]; Fundamental Research Funds for the Central Universities;
   National Natural Science Foundation of China [60972148, 61072106,
   61173092, 61271302, 61272282, 61001206, 61202176, 61271298]; Fund for
   Foreign Scholars in University Research and Teaching Programs (the 111
   Project) [B07048]; Program for Cheung Kong Scholars and Innovative
   Research Team in University [IRT1170]
FX This work is supported by the Project Supported by Natural Science Basic
   Research Plan in Shaanxi Province of China (Program No. 2014JM8301); The
   Fundamental Research Funds for the Central Universities; the National
   Natural Science Foundation of China under grant No. 60972148, 61072106,
   61173092, 61271302, 61272282, 61001206, 61202176, 61271298; The Fund for
   Foreign Scholars in University Research and Teaching Programs (the 111
   Project): No. B07048; the Program for Cheung Kong Scholars and
   Innovative Research Team in University: IRT1170.
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], COMP VIS ICCV 2011 I
   [Anonymous], 2009, P ADV NEUR INF PROC
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Lim J., 2005, NIPS
   Liu BY, 2010, LECT NOTES COMPUT SC, V6314, P624
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2012, IEEE T IMAGE PROCESS, V21, P2824, DOI 10.1109/TIP.2011.2182521
   Wu Y, 2011, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2011.6126357
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
NR 29
TC 1
Z9 1
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 701
EP 715
DI 10.1007/s00371-014-0996-4
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400015
DA 2024-07-18
ER

PT J
AU Zheng, JW
   Huang, QF
   Chen, SY
   Wang, WL
AF Zheng, Jianwei
   Huang, Qiongfang
   Chen, Shengyong
   Wang, Wanliang
TI Efficient kernel discriminative common vectors for classification
SO VISUAL COMPUTER
LA English
DT Article
DE Feature extraction; Kernel discriminative common vector; Cholesky
   decomposition; Kernel trick
ID DIMENSION REDUCTION; FEATURE-EXTRACTION
AB Kernel discriminant analysis (KDA) which operates in the reproducing kernel Hilbert space (RKHS) is a very popular approach to dimensionality reduction. Kernel discriminative common vectors (KDCV) shares the same modified Fisher linear discriminant criterion with KDA and guarantees a 100 % recognition rate for the training set samples as well as favorable generalization performance. However, KDCV has the disadvantage of high computational complexity in both the training and the testing stage. This paper attempts to improve the computation efficiency of KDCV by two strategies. First, the Cholesky decomposition is introduced to obtain the projection matrix instead of eigen-decomposition. Second, we replace the matrix operation with vector operation in the testing process which reduces the computational complexity. Extensive experiments on COIL images dataset, ORL faces dataset, PIE faces dataset, and USPS handwritten digits dataset demonstrate that the proposed algorithm is more efficient than the traditional KDCV algorithm without loss of accuracy.
C1 [Zheng, Jianwei; Huang, Qiongfang; Chen, Shengyong; Wang, Wanliang] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Zheng, JW (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, 288 Liuhe Rd, Hangzhou 310023, Zhejiang, Peoples R China.
EM zjw@zjut.edu.cn; huangqf_zjut@163.com; csy@zjut.edu.cn; wwl@zjut.edu.cn
RI Chen, S./H-3083-2011; Wang, Wanliang/G-5024-2011
OI Chen, S.Y./0000-0002-6705-3831
FU Provincial Science Foundation of Zhejiang [LQ12F03011, LQ14F030003];
   National Natural Science Foundation of China [61379123]; National
   Science and Technology Support Plan [2012BAD10B0101]
FX The authors would like to thank the anonymous reviewers for their
   constructive comments and suggestions. This project was supported in
   part by the Provincial Science Foundation of Zhejiang (LQ12F03011,
   LQ14F030003), National Natural Science Foundation of China (61379123),
   and National Science and Technology Support Plan (2012BAD10B0101).
CR Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Berbar MA, 2014, VISUAL COMPUT, V30, P19, DOI 10.1007/s00371-013-0774-8
   Cai D, 2011, VLDB J, V20, P21, DOI 10.1007/s00778-010-0189-3
   Cevikalp H, 2005, IEEE T PATTERN ANAL, V27, P4, DOI 10.1109/TPAMI.2005.9
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Ching WK, 2012, PATTERN RECOGN, V45, P2719, DOI 10.1016/j.patcog.2012.01.007
   Du P, 2012, VISUAL COMPUT, V28, P493, DOI 10.1007/s00371-011-0646-z
   Gan HT, 2014, J OPT SOC AM A, V31, P1, DOI 10.1364/JOSAA.31.000001
   Guo YQ, 2013, NEUROCOMPUTING, V118, P33, DOI 10.1016/j.neucom.2013.02.011
   Hakan C., 2006, IEEE T NEURAL NETWOR, V17, P1550
   Jin XH, 2014, IEEE T IND ELECTRON, V61, P2441, DOI 10.1109/TIE.2013.2273471
   Jing XY, 2008, NEUROCOMPUTING, V71, P3044, DOI 10.1016/j.neucom.2007.08.027
   Jing XY, 2006, PATTERN RECOGN, V39, P707, DOI 10.1016/j.patcog.2005.10.020
   Kim H, 2007, PATTERN RECOGN, V40, P2939, DOI 10.1016/j.patcog.2007.03.002
   Li W, 2013, IEEE GEOSCI REMOTE S, V10, P1374, DOI 10.1109/LGRS.2013.2242042
   Loog M, 2014, PATTERN RECOGN LETT, V37, P24, DOI 10.1016/j.patrec.2013.03.004
   Lu GF, 2013, NEURAL NETWORKS, V46, P165, DOI 10.1016/j.neunet.2013.05.010
   Lu GF, 2012, INFORM SCIENCES, V193, P72, DOI 10.1016/j.ins.2012.01.015
   Xiong T., 2004, P ADV NEURAL INFORM, V17
   Xu Y, 2006, PATTERN RECOGN, V39, P1026, DOI 10.1016/j.patcog.2005.10.029
   Ye JP, 2004, PATTERN RECOGN, V37, P851, DOI 10.1016/j.patcog.2003.08.006
NR 21
TC 3
Z9 3
U1 2
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 643
EP 655
DI 10.1007/s00371-014-0991-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400011
DA 2024-07-18
ER

PT J
AU Wu, YX
   Jia, N
   Sun, JP
AF Wu, Yunxia
   Jia, Ni
   Sun, Jiping
TI Real-time multi-scale tracking based on compressive sensing
SO VISUAL COMPUTER
LA English
DT Article
DE Tracking-by-detection; Multi-scale; Compressive tracking; Bootstrap
   filter; Real time
ID ROBUST VISUAL TRACKING; OBJECT TRACKING
AB Tracking-by-detection methods have been widely studied and some promising results have been obtained. These methods use discriminative appearance models to train and update online classifiers. They also use a sliding window to detect samples which will then be classified. Then, the location of the sample with the maximum classifier response will be selected as the new location. Compressive tracking was recently proposed with an appearance model based on features extracted in the compressed domain. However, CT uses a fixed-size tracking box to detect samples, and this is unsuitable for practice applications. CT detects samples around the selected region of the previous frame within a fixed radius. Here, the classifier may become inaccurate if the selected region drifts. The fixed radius is also not suitable for tracking targets that experience abrupt acceleration changes. Furthermore, CT updates the classifier parameters with constant learning rate. If the target is fully occluded for an extended period, the classifier will instead learn the features of the cover object and the target will ultimately be lost. In this paper, we present a multi-scale compressive tracker. This tracker integrates an improved appearance model based on normalized rectangle features extracted in the adaptive compressive domain into the bootstrap filter. This type of feature extraction is efficient, and the computation complexity does not increase as the tracking regions become larger. The classifier response is utilized to generate particle importance weight and a re-sample procedure preserves samples according to weight. A 2-order transition model considers the target velocity to estimate the current position and scale status. In this way, the sampling is not limited to a fixed range. Here, feedback strategies are adopted to adjust learning rate for occlusion. Experimental results on various benchmark challenging sequences have demonstrated the superior performance of our tracker when compared with several state-of-the-art tracking algorithms.
C1 [Wu, Yunxia; Jia, Ni; Sun, Jiping] China Univ Min & Technol, Sch Mech Elect & Informat Engn, Beijing 100083, Peoples R China.
C3 China University of Mining & Technology
RP Jia, N (corresponding author), China Univ Min & Technol, Sch Mech Elect & Informat Engn, Beijing 100083, Peoples R China.
EM jiani19881@163.com
FU National Natural Science Foundation of China [51074169, 51134024];
   National High Technology Research and Development Program of China (863
   Program) [2012AA062203]
FX This research work was supported by the National Natural Science
   Foundation of China (Grant Nos. 51074169 and 51134024), the National
   High Technology Research and Development Program of China (863 Program)
   (Grant No. 2012AA062203). The authors would like to thank the anonymous
   reviewers for their helpful comments and suggestions.
CR Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao C.L., 2012, IEEE C COMP VIS PATT, P16
   Black M. J., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P329, DOI 10.1007/BFb0015548
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Isard M., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P893, DOI 10.1007/BFb0055711
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu BY, 2010, LECT NOTES COMPUT SC, V6314, P624
   MacCormick J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P572, DOI 10.1109/ICCV.1999.791275
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Pang Y., IEEE INT C COMP VIS
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Wu Y, 2004, INT J COMPUT VISION, V58, P55, DOI 10.1023/B:VISI.0000016147.97880.cd
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang CJ, 2005, IEEE I CONF COMP VIS, P212
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 35
TC 26
Z9 33
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 471
EP 484
DI 10.1007/s00371-014-0942-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600008
DA 2024-07-18
ER

PT J
AU Jia, BZ
   Liu, R
   Zhu, M
AF Jia, Baozhi
   Liu, Rui
   Zhu, Ming
TI Real-time obstacle detection with motion features using monocular vision
SO VISUAL COMPUTER
LA English
DT Article
DE Autonomous navigation; Driver-assistance systems; Real-time obstacle
   detect; Feature points; Monocular vision
ID VANISHING POINTS
AB Real-time obstacle detection by monocular vision is a challenging problem in autonomous navigation of vehicles and driver-assistance systems. In this paper, we introduce an approach of real-time obstacle detection which can effectively tell apart obstacles from shadows and road surface markings. We propose the followings: (1) a two consecutive frames (TCF) model to find the differences between obstacles and the ground plane by motion features; (2) a filter to increase probabilities of obstacle regions; (3) an updating process to reduce false positives and update the algorithm when the vehicle moves on. We perform experiments on two datasets and our autonomous vehicle. The results show that our method is effective in various conditions and meets the real-time requirement.
C1 [Jia, Baozhi; Liu, Rui; Zhu, Ming] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Jia, BZ (corresponding author), Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
EM bobby32@mail.ustc.edu.cn; liuruin@mail.ustc.edu.cn; mzhu@ustc.edu.cn
FU "Strategic Priority Research Program-Network Video Communication and
   Control" of the Chinese Academy of Sciences [XDA06030900]; Applications
   & Demonstrations of New Complex Forms of TV Business [2012BAH73F02]
FX This research was supported by the "Strategic Priority Research
   Program-Network Video Communication and Control" of the Chinese Academy
   of Sciences (Grant No. XDA06030900), and by the Applications &
   Demonstrations of New Complex Forms of TV Business (Grant No.
   2012BAH73F02).
CR Alvarez JM, 2012, LECT NOTES COMPUT SC, V7578, P376, DOI 10.1007/978-3-642-33786-4_28
   [Anonymous], 2001, Pyramidal implementation of the Lucas Kanade Feature Tracker Description of the Algorithm
   BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237
   Bosse M, 2003, VISUAL COMPUT, V19, P417, DOI 10.1007/s00371-003-0205-3
   Braillon C, 2006, 2006 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P468
   Broggi A., 2005, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)-Workshops, P65
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Geiger A, Vision Meets Robotics: The KITTI Dataset
   Geiger A., 2012, CVPR
   Goldman R., 1991, Graphics Gems II, P338
   Ince E, 2011, TURK J ELECTR ENG CO, V19, P607, DOI 10.3906/elk-0910-266
   Jung BY, 2010, INT J SOC ROBOT, V2, P63, DOI 10.1007/s12369-009-0038-y
   Kong H, 2009, PROC CVPR IEEE, P96, DOI 10.1109/CVPRW.2009.5206787
   Kühnl T, 2011, IEEE INT VEH SYM, P800, DOI 10.1109/IVS.2011.5940416
   Labayrade R, 2002, IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS, P646
   Crespo J, 2009, VISUAL COMPUT, V25, P309, DOI 10.1007/s00371-008-0270-8
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   STRAFORINI M, 1993, IMAGE VISION COMPUT, V11, P91, DOI 10.1016/0262-8856(93)90075-R
   Tomasi Carlo, 1991, Image Rochester NY, DOI DOI 10.1016/S0031-3203(03)00234-6
   Turk M. A., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V727, P136, DOI 10.1117/12.937792
NR 20
TC 19
Z9 26
U1 0
U2 49
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 281
EP 293
DI 10.1007/s00371-014-0918-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900004
DA 2024-07-18
ER

PT J
AU Xu, M
   Zhang, HL
AF Xu, Min
   Zhang, Hanling
TI Saliency detection with color contrast based on boundary information and
   neighbors
SO VISUAL COMPUTER
LA English
DT Article
DE Visual saliency; Color contrast; Energy function; Saliency map
ID MODEL; ATTENTION; IMAGE
AB Object-level saliency detection is significant in many computer vision tasks. In this paper, we propose a novel saliency detection model based on color contrast and image boundaries. The saliency of an image is defined as the contrast between the image elements (regions) and image boundaries elements (regions). We consider the saliency in two-stage procedure rather than in one stage. First of all, according to the definition of saliency, we take four boundaries of image into consideration respectively to obtain a combination coarse saliency map. Furthermore, a new energy function based on the coarse saliency map is proposed, which takes the coarse saliency map as input to yield the final full resolution saliency map. Experimental results on two public datasets demonstrate that the proposed model performs better than the state-of-the-art methods.
C1 [Xu, Min; Zhang, Hanling] Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
C3 Hunan University
RP Xu, M (corresponding author), Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
EM xumin19891030@163.com; jt_hlzhang@hnu.edu.cn
FU Hunan Province Science and Technology Planning Project, China
   [2014G2012]
FX This work was supported by the Key Project of Hunan Province Science and
   Technology Planning Project, China (2014G2012).
CR Achanta R., 2008, ICVS
   Alpert S., 2007, CVPR
   [Anonymous], 2003, ACM MULTIMEDIA
   [Anonymous], 2006, NIPS
   [Anonymous], 2011, ICCV
   [Anonymous], IEEE TIP
   [Anonymous], 2010, Technical Report
   [Anonymous], 2012, CVPR
   [Anonymous], 2011, BMVC
   [Anonymous], 2012, ECCV
   [Anonymous], 2010, CVPR
   [Anonymous], 2005, NIPS
   [Anonymous], 2007, CVPR
   [Anonymous], 2012, CVPR
   [Anonymous], 2009, CVPR
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Goferman S., 2010, CVPR
   Gopalakrishnan V., 2010, IEEE TIP
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li J., 2013, IEEE SIGNAL PROCESS, V20
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Liu T., 2011, IEEE PAMI
   Neisser U, 1967, COGNITIVE PSYCHOL
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tsotsos JK, 2008, INT C DEVEL LEARN, P55, DOI 10.1109/DEVLRN.2008.4640805
   Wang L., 2011, ICCV
   Yarbus A. L., 1967, Eye Movements and Vision
   Zhai Y., 2006, ACM MULTIMEDIA
NR 34
TC 19
Z9 21
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 355
EP 364
DI 10.1007/s00371-014-0930-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900009
DA 2024-07-18
ER

PT J
AU Pedersoli, F
   Benini, S
   Adami, N
   Leonardi, R
AF Pedersoli, Fabrizio
   Benini, Sergio
   Adami, Nicola
   Leonardi, Riccardo
TI XKin: an open source framework for hand pose and gesture recognition
   using kinect
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Kinect; Hand pose; Gesture recognition; Open-source; XKin; Human
   computer interaction
ID SPACE
AB This work targets real-time recognition of both static hand-poses and dynamic hand-gestures in a unified open-source framework. The developed solution enables natural and intuitive hand-pose recognition of American Sign Language (ASL), extending the recognition to ambiguous letters not challenged by previous work. While hand-pose recognition exploits techniques working on depth information using texture-based descriptors, gesture recognition evaluates hand trajectories in the depth stream using angular features and hidden Markov models (HMM). Although classifiers come already trained on ASL alphabet and 16 uni-stroke dynamic gestures, users are able to extend these default sets by adding their personalized poses and gestures. The accuracy and robustness of the recognition system have been evaluated using a publicly available database and across many users. The XKin open project is available online (Pedersoli, XKin libraries.) under FreeBSD License for researchers in human-machine interaction.
C1 [Pedersoli, Fabrizio; Benini, Sergio; Adami, Nicola; Leonardi, Riccardo] Univ Brescia, Dept Informat Engn, I-25125 Brescia, Italy.
C3 University of Brescia
RP Pedersoli, F (corresponding author), Univ Brescia, Dept Informat Engn, Via Branze 38, I-25125 Brescia, Italy.
EM fabrizio.pedersoli@ing.unibs.it
RI Leonardi, Riccardo/F-5666-2010
OI Leonardi, Riccardo/0000-0003-0755-1924; Adami,
   Nicola/0000-0002-8879-9456
CR [Anonymous], 2011, P 5 IEEE INT C AUTOM
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Doliotis P., 2012, INT S VIS COMP ISVC
   Doliotis P., 2011, Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments-PETRA '11, P1
   Escalera S., 2013, P 15 ACM INT C MULT
   Guyon Isabelle., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Work- shops (CVPRW), P1
   Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   LIDDELL SK, 1986, NAT LANG LINGUIST TH, V4, P445, DOI 10.1007/BF00134470
   Mihail R. P., 2012, INT C IM PROC COMP V
   Myers CoryS., 1981, BELL SYST TECH J, V60
   Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pedersoli F, 2012, P ACM C MULT 2012 OP
   Peris M., 2012, IEEE CYB
   Pugeault N, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130290
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Ren Z., 2011, P 19 ACM INT C MULT, P759
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   RUBINE D, 1991, COMP GRAPH, V25, P329, DOI 10.1145/127719.122753
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Le TL, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P369, DOI 10.1109/ComManTel.2013.6482422
   Ting Wan, 2012, 2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet), P1063, DOI 10.1109/CECNet.2012.6201837
   Uebersax D, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130267
   Wachs JP, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1897816.1897838
   Wang R., 2011, Proceedings of the 24th annual ACM symposium on User interface software and technology. UIST '11, P549
   Wobbrock JO, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P159
   Yi Li, 2012, Proceedings of the 2012 IEEE 3rd International Conference on Software Engineering and Service Science (ICSESS), P196, DOI 10.1109/ICSESS.2012.6269439
NR 30
TC 51
Z9 53
U1 0
U2 44
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1107
EP 1122
DI 10.1007/s00371-014-0921-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800005
DA 2024-07-18
ER

PT J
AU Hu, KX
   Liu, YL
   Dong, Q
   Liu, H
   Xing, GY
AF Hu, Kexin
   Liu, Yanli
   Dong, Qi
   Liu, Hao
   Xing, Guanyu
TI Color face image decomposition under complex lighting conditions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Intrinsic image decomposition; Skin color model; Face relighting;
   Complex lighting conditions
ID INTRINSIC IMAGES; SINGLE IMAGE; RECOGNITION; ILLUMINATION; SHAPE;
   REPRESENTATION; REFLECTANCE
AB In this paper, we proposed a method to recover the reflectance and shading images of face images, such as portraits, captured under complex lighting conditions. Under such lighting conditions, traditional intrinsic image decomposition can hardly be applied on face images especially with cast shadows. Inspired by the fact that the face symmetry is an inherent property of reflectance image for human face, face symmetry is firstly exploited as a constraint into traditional intrinsic decomposition framework to obtain the initial reflectance image. To remove the artifacts of the initial reflectance image caused by cast shadows, we use the skin color model to refine the reflectance image. Experiments show that our method is effective for face images under complex lighting conditions.
C1 [Hu, Kexin; Liu, Yanli; Dong, Qi; Liu, Hao] Sichuan Univ, Coll Comp Sci, Chengdu 610064, Peoples R China.
   [Hu, Kexin; Liu, Yanli; Dong, Qi; Liu, Hao] Sichuan Univ, Natl Key Lab Fundamental Sci Synthet Vis, Chengdu 610064, Peoples R China.
   [Xing, Guanyu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
C3 Sichuan University; Sichuan University; University of Electronic Science
   & Technology of China
RP Liu, YL (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610064, Peoples R China.
EM yanliliu@scu.edu.cn
FU NSFC of China [61103137]; 863 Program of China [2013AA013902]; Key
   Projects in the National Science and Technology Pillar Program of China
   [2012BAH62F02]
FX We would like to thank the anonymous reviewers for helping us to improve
   this paper. The work is supported by NSFC of China (No. 61103137), 863
   Program of China (No. 2013AA013902) and Key Projects in the National
   Science and Technology Pillar Program of China (No. 2012BAH62F02).
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   Ankur Patel, 2009, BMVC, P1
   [Anonymous], 1978, COMPUTER VISION SYST
   [Anonymous], SHAPE ILLUMINATION R
   [Anonymous], 2003, PROC GRAPHICON
   Barron JT, 2012, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2012.6247693
   Barron JT, 2012, LECT NOTES COMPUT SC, V7575, P57, DOI 10.1007/978-3-642-33765-9_5
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Casati J.P.B., 2013, 9 WORKSH VIS COMP
   Chen JS, 2010, LECT NOTES COMPUT SC, V6314, P44, DOI 10.1007/978-3-642-15561-1_4
   Chen X, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P106
   Chen XW, 2011, PROC CVPR IEEE, P281, DOI 10.1109/CVPR.2011.5995473
   Deng WH, 2013, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2013.58
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Gehler P.V., 2011, NIPS, V2, P4
   Han H, 2010, LECT NOTES COMPUT SC, V6312, P308, DOI 10.1007/978-3-642-15552-9_23
   Harguess J., 2011, CVPR 2011 WORKSH IEE, P66
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li Q, 2010, VISUAL COMPUT, V26, P41, DOI 10.1007/s00371-009-0375-8
   Luo Y, 2008, INT J PATTERN RECOGN, V22, P555, DOI 10.1142/S0218001408006399
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Peers P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239503
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Phung SL, 2005, IEEE T PATTERN ANAL, V27, P148, DOI 10.1109/TPAMI.2005.17
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964
   Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738
   Smith W.A.P., 2005, P IEEE INT C IM PROC, pIII
   Struc A. V., 2010, CITESEER
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185
   Wang HT, 2004, PROC CVPR IEEE, P498
   Wang Y. H., 2007, EC RES J, V1, P28
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang L., 1996, APPL COMP VIS 1996 W, P142
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhao WY, 2000, PROC CVPR IEEE, P286, DOI 10.1109/CVPR.2000.855831
   Zhao X, 2012, LECT NOTES COMPUT SC, V7584, P220, DOI 10.1007/978-3-642-33868-7_22
NR 42
TC 4
Z9 6
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 685
EP 695
DI 10.1007/s00371-014-0962-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700012
DA 2024-07-18
ER

PT J
AU LeBlanc, S
   Boyer, P
   Joslin, C
AF LeBlanc, Sean
   Boyer, Philip
   Joslin, Chris
TI Modelling and animation of impact and damage with Smoothed Particle
   Hydrodynamics
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Smoothed Particle Hydrodynamics (SPH); Elastic solid; Plasticity; CUDA
ID SPH; DEFORMATIONS
AB In this paper we present a unique procedure for extending the method of Smoothed Particle Hydrodynamics (SPH) to allow for the simulation of permanently deformable soft-solid materials. SPH has previously been shown to be a computationally and visually effective method for fluid dynamics simulations and recently for modelling the deformation of elastic soft solids. By incorporating new parameters into the procedure, the current research is able to demonstrate a range of visually appealing plasticity and damage scenarios. As SPH is a mesh-free Lagrangian method, data can be manipulated to allow for parallelisation using nVidia's CUDA platform, as is shown here where simulations are capable of performing at real-time rates.
C1 [LeBlanc, Sean; Joslin, Chris] Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
   [Boyer, Philip] Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
C3 Carleton University; Carleton University
RP LeBlanc, S (corresponding author), Carleton Univ, Sch Informat Technol, 1125 Colonal By Dr, Ottawa, ON K1S 5B6, Canada.
EM seanleblanc@cmail.carleton.ca; philip.boyer@carleton.ca;
   chris.joslin@carleton.ca
FU Natural Science and Engineering Research Council of Canada
   [CHRP-398837-2011]; Ontario Research Fund: Research Excellent Project
   (MESSAGES)
FX The work presented in this paper is supported by the Natural Science and
   Engineering Research Council of Canada, Collaborative Health Research
   Project (CHRP-398837-2011) and the Ontario Research Fund: Research
   Excellent Project (MESSAGES).
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Clavet S., 2005, SCA '05, P219, DOI DOI 10.1145/1073368.1073400
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   GINGOLD RA, 1977, MON NOT R ASTRON SOC, V181, P375, DOI 10.1093/mnras/181.3.375
   Goswami P., 2010, P 2010 ACM SIGGRAPHE, P55
   Harada T, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P55
   Hutchinson D., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P31
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Khoei AR, 2008, COMPUT METHOD APPL M, V197, P1100, DOI 10.1016/j.cma.2007.10.006
   Liu MB, 2010, ARCH COMPUT METHOD E, V17, P25, DOI 10.1007/s11831-010-9040-7
   Markus Becker, 2009, NPH, V9, P27, DOI [10.2312/EG/DL/conf/EG2009/nph/027-034, DOI 10.2312/EG/DL/CONF/EG2009/NPH/027-034]
   Medina DF, 2000, COMPOS PART A-APPL S, V31, P853, DOI 10.1016/S1359-835X(00)00031-2
   Monaghan JJ, 2005, REP PROG PHYS, V68, P1703, DOI 10.1088/0034-4885/68/8/R01
   MONAGHAN JJ, 1994, J COMPUT PHYS, V110, P399, DOI 10.1006/jcph.1994.1034
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   O'Brien JF, 2002, ACM T GRAPHIC, V21, P291, DOI 10.1145/566570.566579
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Oh S, 2009, COMPUT ANIMAT VIRT W, V20, P215, DOI 10.1002/cav.290
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   SWEGLE JW, 1995, J COMPUT PHYS, V116, P123, DOI 10.1006/jcph.1995.1010
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
NR 23
TC 4
Z9 4
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 909
EP 917
DI 10.1007/s00371-014-0981-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700032
DA 2024-07-18
ER

PT J
AU Yang, WW
   Wang, X
   Wang, GZ
AF Yang, Wenwu
   Wang, Xun
   Wang, Guozheng
TI Part-to-part morphing for planar curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Shape morphing; 2D animation; Curve generation
AB This paper presents a shape-morphing technique that interpolates a pair of 2D polygons or curves. Firstly, a user-guided feature point correspondence is introduced to associate similar parts between the source and target shapes, which allows user to control the correspondence results effectively and flexibly. Secondly, to fully capture the global and local motions of the shapes, we define a simple structure called a part figure to represent these movements in terms of the shape parts. The part figure representation is general and applicable to various part associations between the shapes. By interpolating the part figures of the shapes, the smooth transition of the global and local movements from the source to the target shape is generated, which results in in-between shape parts with least distortion. Then, the coherent intermediate shapes of the morphing sequence are formed from these intermediate shape parts. Experimental results show that the method can transform the source shape into the target shape as expected and generate natural and visually pleasing effects.
C1 [Yang, Wenwu; Wang, Xun; Wang, Guozheng] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang Gongshang University
RP Yang, WW (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou, Zhejiang, Peoples R China.
EM wwyang@mail.zjgsu.edu.cn
FU Natural Science Foundation of China [61003189, 61170098]; Natural
   Science Foundation of Zhejiang Province [LY12F02025]; Science and
   Technology Agency projects of Zhejiang Province [2012C33074,
   2012R10041-16]
FX This research was partially funded by the Natural Science Foundation of
   China (Nos. 61003189, 61170098), the Natural Science Foundation of
   Zhejiang Province (No. LY12F02025), the Science and Technology Agency
   projects of Zhejiang Province (Nos. 2012C33074, 2012R10041-16).
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   Baxter W., 2008, NPAR'08 Proceedings of the 6th international symposium on Non-photorealistic animation and rendering, P59, DOI [http://doi.acm.org/10.1145/1377980.1377993. endereco, DOI 10.1145/1377980.1377993.ENDERE]
   Baxter WV, 2009, IEEE T VIS COMPUT GR, V15, P867, DOI 10.1109/TVCG.2009.38
   Carmel E, 1997, VISUAL COMPUT, V13, P465, DOI 10.1007/s003710050118
   Chen Q, 2006, COMPUT ANIMAT VIRT W, V17, P189, DOI 10.1002/cav.122
   Chetverikov D., 1999, 23 WORKSHOP AUSTRIAN, P175
   Fekete J.-D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P79, DOI 10.1145/218380.218417
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Hahmann S, 2007, COMPUTING, V79, P197, DOI 10.1007/s00607-006-0198-7
   Kaul A., 1991, EUROGRAPHICS '91. Proceedings of the European Computer Graphics Conference and Exhibition, P493
   Kort A., 2002, NPAR 02, P125, DOI DOI 10.1145/508530.508552
   Liu D., 2011, COMPUT GRAPH FORUM, V30, P25
   Liu LG, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P111
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951
   Sederberg T. W., 1993, Computer Graphics Proceedings, P15, DOI 10.1145/166117.166118
   SEDERBERG TW, 1992, COMP GRAPH, V26, P25, DOI 10.1145/142920.134001
   SHAPIRA M, 1995, IEEE COMPUT GRAPH, V15, P44, DOI 10.1109/38.365005
   Siu Chi Hsu, 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P109
   Surazhsky V., 2003, International Journal of Shape Modeling, V9, P191, DOI 10.1142/S0218654303000115
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Whited B, 2010, COMPUT GRAPH FORUM, V29, P605, DOI 10.1111/j.1467-8659.2009.01630.x
   Wolberg G, 1998, VISUAL COMPUT, V14, P360, DOI 10.1007/s003710050148
   Yang WW, 2009, COMPUT GRAPH-UK, V33, P414, DOI 10.1016/j.cag.2009.03.007
NR 24
TC 2
Z9 2
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 919
EP 928
DI 10.1007/s00371-014-0955-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700033
DA 2024-07-18
ER

PT J
AU Mirzaei, MR
   Ghorshi, S
   Mortazavi, M
AF Mirzaei, Mohammad Reza
   Ghorshi, Seyed
   Mortazavi, Mohammad
TI Audio-visual speech recognition techniques in augmented reality
   environments
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented reality; Audio-visual speech recognition; Augmented reality
   environments; Communication; Deaf people
ID VIRTUAL-REALITY
AB Many recent studies show that Augmented Reality (AR) and Automatic Speech Recognition (ASR) technologies can be used to help people with disabilities. Many of these studies have been performed only in their specialized field. Audio-Visual Speech Recognition (AVSR) is one of the advances in ASR technology that combines audio, video, and facial expressions to capture a narrator's voice. In this paper, we combine AR and AVSR technologies to make a new system to help deaf and hard-of-hearing people. Our proposed system can take a narrator's speech instantly and convert it into a readable text and show the text directly on an AR display. Therefore, in this system, deaf people can read the narrator's speech easily. In addition, people do not need to learn sign-language to communicate with deaf people. The evaluation results show that this system has lower word error rate compared to ASR and VSR in different noisy conditions. Furthermore, the results of using AVSR techniques show that the recognition accuracy of the system has been improved in noisy places. Also, the results of a survey that was conducted with 100 deaf people show that more than 80 % of deaf people are very interested in using our system as an assistant in portable devices to communicate with people.
C1 [Mirzaei, Mohammad Reza; Ghorshi, Seyed; Mortazavi, Mohammad] Sharif Univ Technol, Sch Sci & Engn, Kish Isl, Iran.
C3 Sharif University of Technology
RP Mirzaei, MR (corresponding author), Sharif Univ Technol, Sch Sci & Engn, Int Campus, Kish Isl, Iran.
EM mrmirzaie@gmail.com; ghorshi@sharif.edu; mortazavi@sharif.edu
OI Mirzaei, Mohammadreza/0000-0002-1291-5510
CR Adobe Systems Inc, 2011, AD FLASH BUILD
   [Anonymous], P INT C SPOK LANG PR
   [Anonymous], 2006, 2006 2 TECHNOLOGIES
   Bailly G., 2004, ISSUES VISUAL AUDIO
   Bradski G., 2008, LEARNING OPENCV
   Braunstein R., 2007, ACTIONSCRIPT 3 0 BIB
   Cawood S., 2008, AUGMENTED REALITY PR
   Cerf VG, 2014, COMMUN ACM, V57, P7, DOI 10.1145/2656433
   Chin S.W., 2008, INT S INT SIGN PROC
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Goose S, 2003, IEEE PERVAS COMPUT, V2, P65, DOI 10.1109/MPRV.2003.1186727
   Hanlon N., 2009, P 20 IR C ART COGN S, P134
   Hello Enjoy Company, 2011, PAP 3D
   Hohl W., 2008, INT ENV OP SOURC SOF
   Irawati S., 2006, 2006 IEEEACM INT S M, P183
   Kaiser E., 2003, ICMI 03, P12
   Kalra A, 2010, INT J COMPUT SCI NET, V10, P216
   Lange BS, 2010, PHYS MED REH CLIN N, V21, P339, DOI 10.1016/j.pmr.2009.12.007
   Lipovic I., 2011, SPEECH LANGUAGE TECH
   López-Ludeña V, 2011, IEEE LAT AM T, V9, P565, DOI 10.1109/TLA.2011.5993744
   Mihelic F., 2008, Speech Recognition: Technologies and Applications
   Navarathna R., 2010, 2010 10th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA 2010), P598, DOI 10.1109/ISSPA.2010.5605429
   Open Computer Vision Library, 2011, OP AVSR ALPH 1
   Passig D, 2000, AM ANN DEAF, V145, P286, DOI 10.1353/aad.2012.0102
   Shen P., 2010, INT C AUD VIS SPEECH
   Sherman WilliamR., 2003, UNDERSTANDING VIRTUA
   Silva R., 2003, Introduction to augmented reality
   Spark Project Team, 2011, FLARTOOLKIT
   Spark Project Team, 2011, MAR FAC DET
   Transmote, 2011, FLARMANAGER AUGM REA
   Wagner D, 2007, 6 IEEE ACM INT S MIX
   Zainuddin N.M., 2009, REG C SPEC NEEDS ED
   Zainuddin N M. M., 2010, IEEE icerisinde. 2010 Uluslarasi Bilgi Sistemleri Sempozyumu, V1, P1, DOI DOI 10.1109/ITSIM.2010.5561325
   Zayed H.S., 2010, INT J COMPUT ED, V56, P1045
NR 34
TC 13
Z9 13
U1 3
U2 56
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2014
VL 30
IS 3
BP 245
EP 257
DI 10.1007/s00371-013-0841-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB3LU
UT WOS:000331693100001
DA 2024-07-18
ER

PT J
AU Schwenk, K
   Voss, G
   Behr, J
   Jung, Y
   Limper, M
   Herzig, P
   Kuijper, A
AF Schwenk, Karsten
   Voss, Gerrit
   Behr, Johannes
   Jung, Yvonne
   Limper, Max
   Herzig, Pasquale
   Kuijper, Arjan
TI Extending a distributed virtual reality system with exchangeable
   rendering back-ends
SO VISUAL COMPUTER
LA English
DT Article
DE Distributed/network graphics; Virtual reality
AB We present an approach to integrate multiple rendering back-ends under a common application layer for distributed systems. The primary goal was to find a practical and nonintrusive way to use potentially very different renderers in heterogeneous computing environments without impairing their strengths and without burdening the back-ends or the application with details of the cluster environment. Our approach is based on a mediator layer that handles multithreading, clustering, and the synchronization between the application's and the back-end's scene. We analyze the proposed approach with an implementation for a state-of-the-art distributed VR/AR system. In particular, we present two case studies and an example application.
C1 [Schwenk, Karsten; Behr, Johannes; Jung, Yvonne] Fraunhofer IGD, Visual Comp Syst Technol Dept, Darmstadt, Germany.
   [Limper, Max; Herzig, Pasquale] Fraunhofer IGD, Visual Comp Syst Technol Grp, Darmstadt, Germany.
   [Kuijper, Arjan] Fraunhofer IGD, Darmstadt, Germany.
   [Schwenk, Karsten; Kuijper, Arjan] Tech Univ Darmstadt, Darmstadt, Germany.
   [Limper, Max] Tech Univ Darmstadt, Interact Graph Syst Grp, Darmstadt, Germany.
   [Voss, Gerrit] Nanyang Technol Univ, Fraunhofer IDM NTU, Singapore, Singapore.
C3 Technical University of Darmstadt; Technical University of Darmstadt;
   Nanyang Technological University
RP Schwenk, K (corresponding author), Fraunhofer IGD, Visual Comp Syst Technol Dept, Darmstadt, Germany.
EM karsten.schwenk@igd.fraunhofer.de; vossg@camtech.ntu.edu.sg;
   johannes.behr@igd.fraunhofer.de; yvonne.jung@igd.fraunhofer.de;
   max.limper@igd.fraunhofer.de; pasquale.herzig@igd.fraunhofer.de;
   arjan.kuijper@igd.fraunhofer.de
RI Kuijper, Arjan/A-7814-2012
OI Kuijper, Arjan/0000-0002-6413-0061
CR [Anonymous], 1999, OpenGL programming guide: the official guide to learning OpenGL
   Arcila T., 2006, JOURN AFRV
   Behr J., 2010, Proceedings of the 15th International Conference on Web 3D Technology-Web3D '10 p, P185, DOI [10.1145/1836049.1836077, DOI 10.1145/1836049.1836077]
   Behr J., 2004, P 9 INT C 3D WEB TEC, P71
   Berthelot R. B., 2011, P 16 INT C 3D WEB TE, P21
   Brüderlin B, 2007, IEEE COMPUT GRAPH, V27, P48, DOI 10.1109/MCG.2007.153
   Döllner J, 2002, IEEE T VIS COMPUT GR, V8, P99, DOI 10.1109/2945.998664
   Duval T., 2011, ICAT 2011
   Fraunhofer IGD, 2012, INSTANT REALITY FRAM
   Frey S., 2010, Proceedings of the 10th Eurographics conference on Parallel Graphics and Visualization, EG PGV'10, P131
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   REINERS D, 2002, 1 OPENSG S
   Rubinstein D., 2009, P 14 INT C 3D WEB TE, P43, DOI [10.1145/1559764.1559771, DOI 10.1145/1559764.1559771]
   Schwenk K, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P7, DOI 10.1109/CW.2012.9
   Staadt O. G., 2008, SIGGRAPH ASIA 2008 C, P41
   STEINICKE F, 2005, ICAT 05 P, P220
   Vo G., 2006, GRAPHITE 06 P, P303
   Voss G., 2002, Fourth Eurographics Workshop on Parallel Graphics and Visualization, P33
NR 20
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2013
VL 29
IS 10
SI SI
BP 1039
EP 1049
DI 10.1007/s00371-013-0836-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 227LV
UT WOS:000325115400006
DA 2024-07-18
ER

PT J
AU Herholz, S
   Hahn, JU
   Schilling, A
AF Herholz, Sebastian
   Hahn, Jens-Uwe
   Schilling, Andreas
TI Dual space directional occlusion
SO VISUAL COMPUTER
LA English
DT Article
DE Ambient occlusion; Real-time; Directional occlusion; Spherical
   harmonics; Shadows; Voxelization
AB Current real-time ambient or directional occlusion approximation methods are either screen space or object space based. Both methods suffer from drawbacks such as time incoherence and occlusion popping for screen space methods or loss of detailed occlusion effects caused by geometry simplification for object space methods. We present an algorithm that combines both methods to overcome these drawbacks. To avoid over or underestimations during the combination, we use the Spherical Harmonics representation of the directional occlusion information. We therefore combine "Screen Space Spherical Harmonics Occlusion" (Herholz et al. in VMV, 2012) with "Interactive Voxel Cone Tracing" (CT) (Crassin et al. in Comput. Graph. Forum, 2011). The result is a directional occlusion approximation including both occlusions from distant or not directly visible objects and detailed occlusions effects from fine geometrical structures. To increase the quality of CT for occlusion sampling, we also present several extensions such as view dependent cascaded voxelization and a method for voxel coverage estimation.
C1 [Herholz, Sebastian; Schilling, Andreas] Univ Tubingen, D-72076 Tubingen, Germany.
   [Hahn, Jens-Uwe] Stuttgart Media Univ, D-70569 Stuttgart, Germany.
C3 Eberhard Karls University of Tubingen; University of Stuttgart
RP Herholz, S (corresponding author), Univ Tubingen, Sand 14, D-72076 Tubingen, Germany.
EM Sebastian.Herholz@gmail.com
OI Schilling, Andreas/0000-0002-5339-2023
CR [Anonymous], 1998, P 6 INT C COMP VIS
   Bavoil L, 2008, ACM SIGGRAPH
   Christensen P, 2010, ACM SIGGRAPH
   Crassin C, 2012, OPENGL INSIGHTS, P303
   Crassin C, 2011, COMPUT GRAPH FORUM, V30, P1921, DOI 10.1111/j.1467-8659.2011.02063.x
   Engel W.F., 2007, SHADERX5 ADV RENDERI
   Favera E.C.D, 2012, SIBGRAPI
   Herholz S, 2012, VMV
   Hoang TD, 2012, VISUAL COMPUT, V28, P289, DOI 10.1007/s00371-011-0639-y
   Kajiya J.T., 1986, ACM SIGGRAPH
   Keller A, 2001, P EUR WORKSH REND TE
   Langer MS, 2000, PERCEPTION, V29, P649, DOI 10.1068/p3060
   McGuire M, 2012, P ACM HIGH PERF GRAP
   McGuire M, 2011, P ACM HIGH PERF GRAP
   Miller G., 1994, P C COMP GRAPH INT T
   Mittring M, 2007, ACM SIGGRAPH
   Ramamoorthi R, 2001, P C COMP GRAPH INT T
   Reinbothe C, 2009, EUROGRAPHICS
   Ritschel T, 2009, P INT 3D GRAPH GAM
   Schilling A, 1991, ACM SIGGRAPH
   Shanmugam P, 2007, P INT 3D GRAPH GAM
   Sloan P.P, 2007, P PAC C COMP GRAPH A
   Sloan Peter-Pike, 2008, GAM DEV C
   Sumner R.W, 2004, ACM T GRAPHIC, DOI [10.1145/1186562. 1015736, DOI 10.1145/1186562.1015736]
   SUTHERLAND IE, 1974, COMMUN ACM, V17, P32, DOI 10.1145/360767.360802
NR 25
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 917
EP 926
DI 10.1007/s00371-013-0856-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100007
DA 2024-07-18
ER

PT J
AU Bie, XH
   Wang, WC
   Sun, HQ
   Huang, HD
   Zhang, MY
AF Bie, Xiaohui
   Wang, Wencheng
   Sun, Hanqiu
   Huang, Haoda
   Zhang, Minying
TI Intent-aware image cloning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Intent representation; Image cloning; Image composition
AB Currently, gradient domain methods are popular for producing seamless cloning of a source image patch into a target image. However, structure conflicts between the source image patch and the target image may generate artifacts, preventing the general practices. In this paper, we tackle the challenge by incorporating the users' intent in outlining the source patch, where the boundary drawn generally has different appearances from the objects of interest. We first reveal that artifacts exist in the over-included region, the region outside the objects of interest in the source patch. Then we use the diversity from the boundary to approximately distinguish the objects from the over-included region, and design a new algorithm to make the target image adaptively take effects in blending. So the structure conflicts can be efficiently suppressed to remove the artifacts around the objects of interest in the composite result. Moreover, we develop an interpolation measure to composite the final image rather than solving a Poisson equation, and speed up the interpolation by treating pixels in clusters and using hierarchical sampling techniques. Our method is simple to use for instant and high-quality image cloning, in which users only need to outline a region of interested objects to process. Our experimental results have demonstrated the effectiveness of our cloning method.
C1 [Bie, Xiaohui; Wang, Wencheng; Zhang, Minying] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Sun, Hanqiu] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Huang, Haoda] Google Inc, Mountain View, CA 94043 USA.
   [Bie, Xiaohui; Zhang, Minying] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese
   University of Hong Kong; Google Incorporated; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Bie, XH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
EM xiaohui@ios.ac.cn; whn@ios.ac.cn
RI Wang, Wencheng/A-3828-2009
FU Chinese Academy of Sciences; National Social Science Foundation of China
   [12AZD118]; RGC [416311, 416212]; UGC [2050485]
FX The work is supported by the Knowledge Innovation Program of the Chinese
   Academy of Sciences, the National Social Science Foundation of China
   (Project No. 12AZD118), RGC research grants (ref. 416311, 416212) and
   UGC direct grant for research (no. 2050485).
CR Agarwala A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239545, 10.1145/1276377.1276495]
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Darabi S., 2012, ACM T GRAPH, V31, P82
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Du H, 2013, VISUAL COMPUT, V29, P217, DOI 10.1007/s00371-012-0722-z
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Jeschke S., 2009, ACM Transactions on Graphics (TOG), V28, P1
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Kazhdan M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360620
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   McGuire M, 2005, ACM T GRAPHIC, V24, P567, DOI 10.1145/1073204.1073231
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sunkavalli K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778862
   Szeliski R, 2006, ACM T GRAPHIC, V25, P1135, DOI 10.1145/1141911.1142005
   Wang J., 2006, ACM SIGGRAPH 2006 SK
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
NR 22
TC 5
Z9 7
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 599
EP 608
DI 10.1007/s00371-013-0826-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400014
DA 2024-07-18
ER

PT J
AU Doidge, IC
   Jones, MW
AF Doidge, Ian C.
   Jones, Mark W.
TI Probabilistic illumination-aware filtering for Monte Carlo rendering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Global illumination; Monte Carlo; Path tracing; Noise reduction; Poisson
   probability distribution
ID PHOTOGRAPHY; FLASH; NOISE
AB Noise removal for Monte Carlo global illumination rendering is a well known problem, and has seen significant attention from image-based filtering methods. However, many state of the art methods breakdown in the presence of high frequency features, complex lighting and materials. In this work we present a probabilistic image based noise removal and irradiance filtering framework that preserves this high frequency detail such as hard shadows and glossy reflections, and imposes no restrictions on the characteristics of the light transport or materials. We maintain per-pixel clusters of the path traced samples and, using statistics from these clusters, derive an illumination aware filtering scheme based on the discrete Poisson probability distribution. Furthermore, we filter the incident radiance of the samples, allowing us to preserve and filter across high frequency and complex textures without limiting the effectiveness of the filter.
C1 [Doidge, Ian C.; Jones, Mark W.] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
C3 Swansea University
RP Doidge, IC (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
EM csiand@swansea.ac.uk; m.w.jones@swansea.ac.uk
RI Jones, Mark W./F-1114-2015
OI Jones, Mark W./0000-0001-8991-1190
FU EPSRC [EP/I031243/1]; RIVIC; EPSRC [EP/I031243/1] Funding Source: UKRI
FX We would like to thank the reviewers for their helpful suggestions to
   improve the paper, and Jotero.com for donation of the Ajax model to the
   community. The work presented in this paper was funded by an EPSRC
   doctoral training grant, EPSRC grant EP/I031243/1 and RIVIC. We would
   like to be notified about any adoption of this method into rendering
   software since our funding council seeks to record the impact of its
   funded research.
CR Cline D, 2005, ACM T GRAPHIC, V24, P1186, DOI 10.1145/1073204.1073330
   Dammertz H., 2010, P C HIGH PERF GRAPH, P67, DOI DOI 10.5555/1921479.1921491
   DeCoro C, 2010, COMPUT GRAPH FORUM, V29, P2119, DOI 10.1111/j.1467-8659.2010.01799.x
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Gassenbauer V, 2009, COMPUT GRAPH FORUM, V28, P1189, DOI 10.1111/j.1467-8659.2009.01496.x
   Hachisuka T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360632
   Kontkanen J, 2006, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2004, P259, DOI 10.1007/3-540-31186-6_16
   Krivánek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83
   Pajot Anthony., 2011, P GRAPHICS INTERFACE, P159
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ragan-Kelley J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966396
   Rousselle F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366214
   Schwenk K, 2012, IEEE COMPUT GRAPH, V32, P46, DOI 10.1109/MCG.2012.30
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167083
   Suykens F., 2000, 8th International Conference in Central Europe on Computer Graphics, Visualization and Interactive Digital Media'2000. Under the Auspices of the Lord Mayor of the City of Pilsen in cooperation with EUROGRAPHOCS and IFIP WG 5.10. WSCG'2000. Conference Proceedings, P220
   Veach E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P65, DOI 10.1145/258734.258775
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
NR 17
TC 4
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 707
EP 716
DI 10.1007/s00371-013-0807-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400024
OA hybrid
DA 2024-07-18
ER

PT J
AU Liang, H
   Yuan, JS
   Thalmann, D
   Zhang, ZY
AF Liang, Hui
   Yuan, Junsong
   Thalmann, Daniel
   Zhang, Zhengyou
TI Model-based hand pose estimation via spatial-temporal hand parsing and
   3D fingertip localization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Fingertip detection; Geodesic distance; Hand pose estimation; Human
   computer interaction
ID TRACKING
AB In this paper we present a novel vision-based markerless hand pose estimation scheme with the input of depth image sequences. The proposed scheme exploits both temporal constraints and spatial features of the input sequence, and focuses on hand parsing and 3D fingertip localization for hand pose estimation. The hand parsing algorithm incorporates a novel spatial-temporal feature into a Bayesian inference framework to assign the correct label to each image pixel. The 3D fingertip localization algorithm adapts a recently developed geodesic extrema extraction method to fingertip detection with the hand parsing algorithm, a novel path-reweighting method and K-means clustering in metric space. The detected 3D fingertip locations are finally used for hand pose estimation with an inverse kinematics solver. Quantitative experiments on synthetic data show the proposed hand pose estimation scheme can accurately capture the natural hand motion. A simulated water-oscillator application is also built to demonstrate the effectiveness of the proposed method in human-computer interaction scenarios.
C1 [Liang, Hui; Thalmann, Daniel] Nanyang Technol Univ, Inst Media Innovat, Singapore 637553, Singapore.
   [Liang, Hui] Nanyang Technol Univ, Sch EEE, Singapore 637553, Singapore.
   [Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Zhang, Zhengyou] Microsoft Res, Redmond, WA 98052 USA.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University; Microsoft
RP Liang, H (corresponding author), Nanyang Technol Univ, Inst Media Innovat, 50 Nanyang Dr, Singapore 637553, Singapore.
EM hliang1@e.ntu.edu.sg; jsyuan@ntu.edu.sg; danielthalmann@ntu.edu.sg;
   zhang@microsoft.com
RI zhang, zheng/HCH-9684-2022; Yuan, Junsong/R-4352-2019; Thalmann,
   Daniel/AAL-1097-2020; Yuan, Junsong/A-5171-2011; Thalmann,
   Daniel/A-4347-2008; zhang, ZY/HJH-6535-2023; Zhang, Zhang/JAX-2097-2023
OI Thalmann, Daniel/0000-0002-0451-7491; Yuan, Junsong/0000-0002-7901-8793
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore
FX This research, which is carried out at BeingThere Centre, is supported
   by the Singapore National Research Foundation under its International
   Research Centre @ Singapore Funding Initiative and administered by the
   IDM Programme Office.
CR [Anonymous], 2011, P BRIT MACH VIS C
   Aristidou A., 2010, Proceedings of the 4th International Symposium on Communications, Control and Signal Processing, IEEE-ISCCSP '10, P1, DOI DOI 10.1109/ISCCSP.2010.5463419
   Baak A., 2011, P IEEE INT C COMP VI
   Ballan L, 2012, LECT NOTES COMPUT SC, V7577, P640, DOI 10.1007/978-3-642-33783-3_46
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chua CS, 2002, IMAGE VISION COMPUT, V20, P191, DOI 10.1016/S0262-8856(01)00094-4
   Doliotis P, 2012, LECT NOTES COMPUT SC, V7431, P148, DOI 10.1007/978-3-642-33179-4_15
   Erol A., 2005, P IEEE WORKSH VIS HU, P75, DOI [DOI 10.1109/CVPR.2005.395, 10.1109/CVPR.2005.395]
   Henia O. B., 2010, DROP REF
   Ho MF, 2011, PATTERN RECOGN, V44, P443, DOI 10.1016/j.patcog.2010.08.012
   Keskin C, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130391
   Kölsch M, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P614, DOI 10.1109/AFGR.2004.1301601
   Kolsch M., 2005, CVPR
   Liang H., 2012, VRCAI, P87
   Lin J, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P121, DOI 10.1109/HUMO.2000.897381
   Lin JY, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P693, DOI 10.1109/AFGR.2004.1301615
   Panin G, 2009, LECT NOTES COMPUT SC, V5876, P1131, DOI 10.1007/978-3-642-10520-3_108
   PELLEGRINI S, 2008, P BRIT MACH VIS C
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Romero Javier, 2009, 2009 9th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2009), P87, DOI 10.1109/ICHR.2009.5379596
   Schwarz L. A., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P700, DOI 10.1109/FG.2011.5771333
   Stenger B, 2001, PROC CVPR IEEE, P310
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Thalmann Daniel., 2012, P 20 ACM INT C MULTI, P785, DOI DOI 10.1145/2393347.2396312
   Toyama K, 2002, INT J COMPUT VISION, V48, P9, DOI 10.1023/A:1014899027014
   Ulrich Neumann ZhenyaoMoand., 2006, IEEE Conference on Computer Vision and Pattern Recognition, P1499, DOI DOI 10.1109/CVPR.2006.237
   WANG LCT, 1991, IEEE T ROBOTIC AUTOM, V7, P489, DOI 10.1109/70.86079
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Xu JA, 2010, IEEE IMAGE PROC, P3257, DOI 10.1109/ICIP.2010.5651179
   Ying Wu, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P606, DOI 10.1109/ICCV.1999.791280
NR 30
TC 37
Z9 42
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 837
EP 848
DI 10.1007/s00371-013-0822-4
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400036
DA 2024-07-18
ER

PT J
AU Serin, E
   Sumengen, S
   Balcisoy, S
AF Serin, Ekrem
   Sumengen, Selcuk
   Balcisoy, Selim
TI Representational image generation for 3D objects
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Mesh saliency; Representational images; Information theory
ID VIEWPOINT ENTROPY; VIEW SELECTION
AB Finding good representational images for 3D object exploration is a highly subjective problem in the cognitive field. The "best" or "good" definitions do not depend on any metric. We have explained the VKL distance concept and introduced a novel view descriptor called vSKL distance for finding "good" representational images. The image generation is done by projecting the surfaces of 3D objects onto the screen or any planar surface. The projection process depends on parameters such as camera position, camera vector, up vector, and clipping plane positions. In this work we present a technique to find such camera positions that the 3D object is projected in "good" or "best" way where those subjective definitions are mapped to Information Theoretical distances. We compared greedy view selection integrated vSKL with two well known techniques: VKL and VMI. vSKL performs very close to the other two, hence face coverage perturbation is minimal, but it is 3 to 4 times faster. Furthermore, the saliency information is conveyed to users with generated images.
C1 [Serin, Ekrem; Balcisoy, Selim] Sabanci Univ, Istanbul, Turkey.
   [Sumengen, Selcuk] Sabanci Univ, Comp Graph Lab, Istanbul, Turkey.
C3 Sabanci University; Sabanci University
RP Serin, E (corresponding author), Sabanci Univ, Istanbul, Turkey.
EM eserin@su.sabanciuniv.edu; selcuk@sabanciuniv.edu;
   balcisoy@sabanciuniv.edu
RI , Selim/Y-3196-2019
OI Balcisoy, Selim/0000-0002-6495-7341
CR BARRAL P, 2000, P EUR 2000
   Bordoloi U., 2005, P IEEE VIS C VIS 200
   Bulbul A., 2010, P 7 S APPL PERC GRAP, P81, DOI DOI 10.1145/1836248.1836263
   Castelló P, 2006, LECT NOTES COMPUT SC, V3992, P263, DOI 10.1007/11758525_35
   Chen X., 2012, ACM T GRAPH, V31
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji GF, 2006, IEEE T VIS COMPUT GR, V12, P1109, DOI 10.1109/TVCG.2006.137
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Muhler K., 2007, EUROVIS, P267
   Sbert M., 2005, P COMPAESTH EG, P185, DOI [10.2312/COMPAESTH/COMPAESTH05/185-192, DOI 10.2312/COMPAESTH/COMPAESTH05/185-192]
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Serin E., 2011, ISCIS, P299
   Serin E, 2012, COMPUT GRAPH-UK, V36, P1013, DOI 10.1016/j.cag.2012.08.006
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Sokolov D., 2006, J VIRTUAL REAL BROAD, V3
   Takahashi S., 2005, P IEEE VIS C VIS 200
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   Viola I, 2006, IEEE T VIS COMPUT GR, V12, P933, DOI 10.1109/TVCG.2006.152
NR 24
TC 8
Z9 8
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 675
EP 684
DI 10.1007/s00371-013-0805-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400021
DA 2024-07-18
ER

PT J
AU Martínez, J
   García, AS
   Molina, JP
   Martínez, D
   González, P
AF Martinez, Jonatan
   Garcia, Arturo S.
   Molina, Jose P.
   Martinez, Diego
   Gonzalez, Pascual
TI An empirical evaluation of different haptic feedback for shape and
   texture recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Force feedback; Vibrotactile; Textures; Dataglove
ID TACTILE
AB The scope of this research is to evaluate three different haptic feedback methods for texture discrimination in virtual environments. In particular, a Phantom force feedback device, a custom-made vibrotactile dataglove and paper palpable prototypes have been used. This paper describes a new study which corroborates the results of an initial experiment (Martinez et al. in 2011 International Conference on Cyberworlds, pp. 62-68, 2011) and performs a more in-depth evaluation of some results of interest and, in particular, those based on gender. In the experiment expansion, the number of users has been increased, so both genders are even, and the texture identification strategies have been analyzed. Finally, statistical analyses have been conducted to assess the differences between both genders, showing a new path which could be explored with new experiments. In addition, the vibrotactile dataglove has proved to have a notable behavior in the detection of varying grating textures, and it is even useful to identify shapes.
C1 [Martinez, Jonatan; Molina, Jose P.; Martinez, Diego; Gonzalez, Pascual] Univ Castilla La Mancha, Inst Invest Informat Albacete I3A, Albacete, Spain.
   [Garcia, Arturo S.] SymbiaIT, Albacete, Spain.
   [Martinez, Jonatan; Martinez, Diego] Univ Castilla La Mancha, Lab User Interact & Software Engn LoUISE Lab, Albacete, Spain.
C3 Universidad de Castilla-La Mancha; Universidad de Castilla-La Mancha
RP Martínez, J (corresponding author), Univ Castilla La Mancha, Inst Invest Informat Albacete I3A, Albacete, Spain.
EM jonatan.martinez@uclm.es; arturo@symbiait.es; jpmolina@dsi.uclm.es;
   diegomp1982@dsi.uclm.es; pgonzalez@dsi.uclm.es
RI González, Pascual/E-3693-2016; Jimenez, Arturo S. Garcia/ABH-3849-2020
OI González, Pascual/0000-0003-3549-5712; Jimenez, Arturo S.
   Garcia/0000-0003-0671-324X; Molina Masso, Jose
   Pascual/0000-0001-6832-3250; Martinez Plasencia,
   Diego/0000-0002-5815-8236
FU  [PEII09-0054-9581];  [TIN2008-06596-C02-01]
FX This work has been supported by the projects PEII09-0054-9581 and
   TIN2008-06596-C02-01. Thanks to the users who have participated
   voluntarily in the experiments.
CR Allerkamp D, 2007, VISUAL COMPUT, V23, P97, DOI 10.1007/s00371-006-0031-5
   Gabbard JL, 1999, IEEE COMPUT GRAPH, V19, P51, DOI 10.1109/38.799740
   Gurari N, 2009, INT C REHAB ROBOT, P398
   Hollins M, 2002, BEHAV BRAIN RES, V135, P51, DOI 10.1016/S0166-4328(02)00154-7
   JOHANSSON RS, 1978, J PHYSIOL-LONDON, V281, P101, DOI 10.1113/jphysiol.1978.sp012411
   Kuchenbecker KJ, 2006, IEEE T VIS COMPUT GR, V12, P219, DOI 10.1109/TVCG.2006.32
   Kyung KU, 2007, LECT NOTES COMPUT SC, V4813, P34
   Martinez D, 2010, VISUAL COMPUT, V26, P619, DOI 10.1007/s00371-010-0499-x
   Martínez J, 2011, 2011 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P62, DOI 10.1109/CW.2011.23
   Minsky M., 1990, ACM T COMPUT-HUM INT, V24, P235, DOI DOI 10.1145/91385.91451
   Okamoto S, 2008, IEEE INT CONF ROBOT, P220, DOI 10.1109/ROBOT.2008.4543212
   Okamura AM, 1998, IEEE INT CONF ROBOT, P674, DOI 10.1109/ROBOT.1998.677050
   Robles-De-La-Torre G, 2006, IEEE MULTIMEDIA, V13, P24, DOI 10.1109/MMUL.2006.69
   Ryu J., 2004, Proc. ACM Symp. Virtual Real. Softw. Technol. VRST 04, P89, DOI DOI 10.1145/1077534.1077551
   Sestras R. E., 2009, LEONARDO J SCI, V15, P71
   Shirali-Shahreza M, 2009, 11TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS I-III, PROCEEDINGS,, P1997
NR 16
TC 12
Z9 13
U1 0
U2 21
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2013
VL 29
IS 2
SI SI
BP 111
EP 121
DI 10.1007/s00371-012-0716-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 072FM
UT WOS:000313654700003
DA 2024-07-18
ER

PT J
AU Singh, C
   Walia, E
   Mittal, N
AF Singh, Chandan
   Walia, Ekta
   Mittal, Neerja
TI Robust two-stage face recognition approach using global and local
   features
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Global features; Local features; Zernike moments
   (ZMs); Weber's law; Histogram; Dense descriptors
ID PSEUDO ZERNIKE MOMENTS; IMAGE-ANALYSIS; INFORMATION; PZM
AB This paper presents a robust two-stage face recognition approach that combines the traits of global features in first stage and the local features in second stage. The global features are extracted from Zernike moments (ZMs) method that encompasses the useful characteristics of being invariant to image rotation, scale, and noise. The local features are obtained from the histogram-based Weber Law Descriptor (WLD) having tremendous qualities like invariance to scale, change in image intensities, rotation, and noise. The novelty of this paper is twofold: (1) an efficient approach is used for combining the global and local features which is based on the human psychology to trace and memorize the known persons, i.e., locate some similar faces from the overall appearance of different persons and later identify from this the specific individual on the basis of their interior differences like shape of eyes, nose, etc.; (2) a method is used for providing the weights to individual face patches in extraction of local features, which is based on the averaged discrimination competence of features within a patch. The performance of proposed two-stage face recognition approach is analyzed against some major hurdles of this system, i.e., illumination, expression, scale, pose, occlusion, and noise variations. The proposed method achieves the highest recognition rate of 98.0% and 94.1% on ORL and Yale databases, respectively. The experimental results on these well-known face databases demonstrate that the proposed method is highly robust to illumination variation and also generates superior results against other variations.
C1 [Mittal, Neerja] Rayat & Bahra Coll Engg & BioTech, Dept CSE&IT, Kharar, Distt Mohali, India.
   [Singh, Chandan] Punjabi Univ, Dept Comp Sci, Patiala, Punjab, India.
   [Walia, Ekta] S Asian Univ, Dept Comp Sci, New Delhi, India.
C3 Punjabi University
RP Mittal, N (corresponding author), Rayat & Bahra Coll Engg & BioTech, Dept CSE&IT, Kharar, Distt Mohali, India.
EM chandan.csp@gmail.com; wekta@yahoo.com; neerjamittal_2k1@yahoo.com
RI Garg, Neerja Mittal/ABD-2404-2020
OI Garg, Neerja Mittal/0000-0002-2877-816X
FU All India Council for Technical Education (AICTE), Govt. of India, New
   Delhi, India [8013/RID/BOR/RPS-77/2005-06]
FX The authors are grateful to All India Council for Technical Education
   (AICTE), Govt. of India, New Delhi, India, for supporting the research
   work vide their file number 8013/RID/BOR/RPS-77/2005-06. We are also
   thankful to anonymous reviewers whose comments helped us to improve the
   quality of the paper.
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Dabbaghchian S, 2010, PATTERN RECOGN, V43, P1431, DOI 10.1016/j.patcog.2009.11.001
   Faez K, 2006, IEEE SYS MAN CYBERN, P4197, DOI 10.1109/ICSMC.2006.384793
   Fang YC, 2002, INT C PATT RECOG, P382, DOI 10.1109/ICPR.2002.1048319
   Haddadnia J, 2001, IEEE IMAGE PROC, P1018, DOI 10.1109/ICIP.2001.959221
   HADDANDNIA J, 2003, EURASIP J APPL SIG P, V9, P890, DOI DOI 10.1155/S1110865703305128
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Hjelmås E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Huang LL, 2005, PATTERN RECOGN LETT, V26, P1641, DOI 10.1016/j.patrec.2005.01.015
   Jun B, 2011, PATTERN RECOGN, V44, P532, DOI 10.1016/j.patcog.2010.10.008
   Kanan HR, 2008, PATTERN RECOGN, V41, P3799, DOI 10.1016/j.patcog.2008.05.024
   Kim C, 2005, IEEE IJCNN, P2030
   Lai JH, 2001, PATTERN RECOGN, V34, P95, DOI 10.1016/S0031-3203(99)00200-9
   Lajevardi SM, 2010, DIGIT SIGNAL PROCESS, V20, P1771, DOI 10.1016/j.dsp.2010.03.004
   Liu ZM, 2010, PATTERN RECOGN, V43, P2882, DOI 10.1016/j.patcog.2010.03.003
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Nabatchian A, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 3, PROCEEDINGS, P661, DOI 10.1109/CISP.2008.479
   Neerja, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P554, DOI 10.1109/CISP.2008.144
   Nor'aini AJ, 2006, IEEE ST CONF RES DEV, P120, DOI 10.1109/SCORED.2006.4339322
   Nor'aini A.J., 2006, P 3 KUAL LUMP INT C, P37
   Pang YH, 2006, J RES PRACT INF TECH, V38, P197
   Pang YH, 2005, IEICE ELECTRON EXPR, V2, P70, DOI 10.1587/elex.2.70
   Singh Chandan, 2011, Pattern Recognition and Image Analysis, V21, P71, DOI 10.1134/S1054661811010044
   Singh C, 2010, PATTERN RECOGN, V43, P2497, DOI 10.1016/j.patcog.2010.02.005
   Soundar KR, 2010, IET COMPUT VIS, V4, P173, DOI 10.1049/iet-cvi.2008.0065
   Soyel H, 2010, ELECTRON LETT, V46, P343, DOI 10.1049/el.2010.0092
   Su Y, 2009, IEEE T IMAGE PROCESS, V18, P1885, DOI 10.1109/TIP.2009.2021737
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913
   Turk M, 2001, IEICE T INF SYST, VE84D, P1586
   Wang JZ, 2011, PATTERN RECOGN LETT, V32, P494, DOI 10.1016/j.patrec.2010.11.014
   Wang Y, 2010, PATTERN RECOGN, V43, P3580, DOI 10.1016/j.patcog.2010.05.021
   Wee CY, 2007, IMAGE VISION COMPUT, V25, P967, DOI 10.1016/j.imavis.2006.07.010
   Xu Y, 2008, NEUROCOMPUTING, V71, P1857, DOI 10.1016/j.neucom.2007.09.021
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
   Zhang X, 2009, PATTERN RECOGN, V42, P2876, DOI 10.1016/j.patcog.2009.04.017
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhi RC, 2008, NEUROCOMPUTING, V71, P3607, DOI 10.1016/j.neucom.2008.04.047
   Zhou D, 2006, PATTERN RECOGN LETT, V27, P536, DOI 10.1016/j.patrec.2005.09.015
NR 43
TC 16
Z9 17
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2012
VL 28
IS 11
BP 1085
EP 1098
DI 10.1007/s00371-011-0659-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021JO
UT WOS:000309881300003
DA 2024-07-18
ER

PT J
AU Joia, P
   Gomez-Nieto, E
   Neto, JB
   Casaca, W
   Botelho, G
   Paiva, A
   Nonato, LG
AF Joia, Paulo
   Gomez-Nieto, Erick
   Neto, Joao Batista
   Casaca, Wallace
   Botelho, Glenda
   Paiva, Afonso
   Nonato, Luis Gustavo
TI Class-specific metrics for multidimensional data projection applied to
   CBIR
SO VISUAL COMPUTER
LA English
DT Article
DE Multidimensional projection; Content-Based Image Retrieval
ID IMAGE RETRIEVAL; DIMENSIONALITY REDUCTION
AB Content-based image retrieval is still a challenging issue due to the inherent complexity of images and choice of the most discriminant descriptors. Recent developments in the field have introduced multidimensional projections to burst accuracy in the retrieval process, but many issues such as introduction of pattern recognition tasks and deeper user intervention to assist the process of choosing the most discriminant features still remain unaddressed. In this paper, we present a novel framework to CBIR that combines pattern recognition tasks, class-specific metrics, and multidimensional projection to devise an effective and interactive image retrieval system. User interaction plays an essential role in the computation of the final multidimensional projection from which image retrieval will be attained. Results have shown that the proposed approach outperforms existing methods, turning out to be a very attractive alternative for managing image data sets.
C1 [Joia, Paulo; Gomez-Nieto, Erick] Univ Sao Paulo, ICMC, Dept Comp Sci, Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo
RP Joia, P (corresponding author), Univ Sao Paulo, ICMC, Dept Comp Sci, Sao Carlos, SP, Brazil.
EM pjoia@icmc.usp.br; egomezn@icmc.usp.br; jbatista@icmc.usp.br;
   wallace@icmc.usp.br; glenda@icmc.usp.br; apneto@icmc.usp.br;
   gnonato@icmc.usp.br
RI Paiva, Afonso/E-2593-2011; Nonato, Luis Gustavo/D-5782-2011; Joia,
   Paulo/D-3248-2016; Nieto, Erick Gomez/X-1606-2019; Casaca,
   Wallace/D-1823-2013
OI Paiva, Afonso/0000-0001-8229-3385; Joia, Paulo/0000-0001-6198-6713;
   Nieto, Erick Gomez/0000-0001-7481-1656; Casaca,
   Wallace/0000-0002-1073-9939
FU FAPESP; CAPES-Brazil
FX We thank the anonymous reviewers for their useful and constructive
   comments. This work was supported by FAPESP and CAPES-Brazil.
CR [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Arivazhagan S, 2006, PATTERN RECOGN LETT, V27, P1976, DOI 10.1016/j.patrec.2006.05.008
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Brandes U, 2007, LECT NOTES COMPUT SC, V4372, P42
   Campello RJGB, 2009, J HEURISTICS, V15, P43, DOI 10.1007/s10732-007-9059-6
   Chalmers M, 1996, IEEE VISUAL, P127, DOI 10.1109/VISUAL.1996.567787
   da Silva SF, 2007, PROC INT C TOOLS ART, P557, DOI 10.1109/ICTAI.2007.142
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Eler DM, 2009, VISUAL COMPUT, V25, P923, DOI 10.1007/s00371-009-0368-7
   Faloutsos C., 1995, SIGMOD Record, V24, P163, DOI 10.1145/568271.223812
   Gütlein M, 2009, 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING, P332, DOI 10.1109/CIDM.2009.4938668
   Joia P., 2011, P SIGBRAPI 2011
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Jourdan F, 2004, IEEE INFOR VIS, P388, DOI 10.1109/IV.2004.1320173
   Koren Y, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P137, DOI 10.1109/INFVIS.2002.1173159
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Krzanowski Wojtek J, 2000, PRINCIPLES MULTIVARI
   Kumar D. A., 2011, INT J COMPUTER APPL, V17, P37, DOI DOI 10.5120/2199-2793
   Landwehr N, 2005, MACH LEARN, V59, P161, DOI 10.1007/s10994-005-0466-3
   Maheswary P, 2008, INT C COMP ELEC ENG, P821, DOI 10.1109/ICCEE.2008.114
   Morrison A, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P152, DOI 10.1109/INFVIS.2002.1173161
   Müller H, 2004, INT J MED INFORM, V73, P1, DOI 10.1016/j.ijmedinf.2003.11.024
   Paulovich FV, 2011, COMPUT GRAPH FORUM, V30, P1091, DOI 10.1111/j.1467-8659.2011.01958.x
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Paulovich FV, 2010, IEEE T VIS COMPUT GR, V16, P1281, DOI 10.1109/TVCG.2010.207
   Pekalska E., 1999, P 5 ANN C ADV SCH CO, P221
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Seber G A., 2009, Multivariate observations, DOI DOI 10.1002/9780470316641
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1
   TORGERSON WS, 1965, PSYCHOMETRIKA, V30, P379, DOI 10.1007/BF02289530
   Xu K, 2009, COMPUT GRAPH-UK, V33, P391, DOI 10.1016/j.cag.2009.03.022
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhou X., 2003, MULTIMEDIA SYST, V40, P262
NR 36
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 1027
EP 1037
DI 10.1007/s00371-012-0730-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900008
DA 2024-07-18
ER

PT J
AU Nguyen, KT
   Jang, H
   Han, J
AF Nguyen, Kien T.
   Jang, Hanyoung
   Han, JungHyun
TI Image-space hierarchical coherence buffer
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Image-based algorithm; Indirect lighting; Global illumination
ID GLOBAL ILLUMINATION
AB Indirect illumination plays an important role in global illumination. However, computing indirect illumination is a time-consuming process and needs to be approximated to achieve interactive performance. Indirect illumination varies rather slowly across the surface. This leads to the idea of computing indirect illumination sparsely in the scene and interpolating the result. This paper presents a hierarchical structure, which enables efficient sampling. The hierarchy is constructed in the image space by exploiting coherences among the screen-space pixels. From the hierarchy, samples are chosen, each of which represents a group of coherent pixels. This paper presents two methods of utilizing the samples for indirect lighting computation. The methods produce plausible lighting results and show high performances. The proposed algorithms run entirely in the image space and are easy to implement in contemporary graphic hardware.
C1 [Han, JungHyun] Korea Univ, Interact Media Lab 3D, Seoul, South Korea.
   [Han, JungHyun] Korea Univ, Game Res Ctr, Seoul, South Korea.
C3 Korea University; Korea University
RP Han, J (corresponding author), Korea Univ, Interact Media Lab 3D, Seoul, South Korea.
EM jhan@korea.ac.kr
CR Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   Dachsbacher C., 2006, Proc. Symp. Interactive 3D Graph. and Games, Redwood City, P93, DOI DOI 10.1145/1111411.1111428
   Debattista K, 2009, COMPUT GRAPH FORUM, V28, P2216, DOI 10.1111/j.1467-8659.2009.01435.x
   Gassenbauer V, 2009, COMPUT GRAPH FORUM, V28, P1189, DOI 10.1111/j.1467-8659.2009.01496.x
   GAUTRON P., 2005, SIGGRAPH 05, P36
   Gautron P, 2007, IEEE T VIS COMPUT GR, V13, P891, DOI 10.1109/TVCG.2007.1061
   Herzog R, 2009, COMPUT GRAPH FORUM, V28, P259, DOI 10.1111/j.1467-8659.2009.01365.x
   KELLER A, 1997, SIGGRAPH 97, P49
   Krivánek J, 2005, IEEE T VIS COMPUT GR, V11, P550, DOI 10.1109/TVCG.2005.83
   LAINE S, 2010, I3D 10, P55
   Nichols G., 2009, P 2009 S INTERACTIVE, P83, DOI DOI 10.1145/1507149.1507162
   Nichols G, 2010, COMPUT GRAPH FORUM, V29, P1279, DOI 10.1111/j.1467-8659.2010.01723.x
   Nichols G, 2009, COMPUT GRAPH FORUM, V28, P1141, DOI 10.1111/j.1467-8659.2009.01491.x
   Ritschel T., 2008, ACM T GRAPHIC, V27, P1
   RITSCHEL T, 2009, SIGGRAPH AS 09, P1
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Tabellion E, 2004, ACM T GRAPHIC, V23, P469, DOI 10.1145/1015706.1015748
   Tawara T, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P110, DOI 10.1109/CGI.2004.1309199
   Walter B, 2005, ACM T GRAPHIC, V24, P1098, DOI 10.1145/1073204.1073318
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
NR 20
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 759
EP 768
DI 10.1007/s00371-011-0562-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600034
DA 2024-07-18
ER

PT J
AU Gobron, S
   Çöltekin, A
   Bonafos, H
   Thalmann, D
AF Gobron, Stephane
   Coeltekin, Arzu
   Bonafos, Herve
   Thalmann, Daniel
TI GPGPU computation and visualization of three-dimensional cellular
   automata
SO VISUAL COMPUTER
LA English
DT Article
DE Cellular automata; GPGPU; Simulation of natural phenomena; Emerging
   behavior; Volume graphics; Information visualization; Real-time
   rendering; Medical visualization
AB This paper presents a general-purpose simulation approach integrating a set of technological developments and algorithmic methods in cellular automata (CA) domain. The approach provides a general-purpose computing on graphics processor units (GPGPU) implementation for computing and multiple rendering of any direct-neighbor three-dimensional (3D) CA. The major contributions of this paper are: the CA processing and the visualization of large 3D matrices computed in real time; the proposal of an original method to encode and transmit large CA functions to the graphics processor units in real time; and clarification of the notion of top-down and bottom-up approaches to CA that non-CA experts often confuse. Additionally a practical technique to simplify the finding of CA functions is implemented using a 3D symmetric configuration on an interactive user interface with simultaneous inside and surface visualizations. The interactive user interface allows for testing the system with different project ideas and serves as a test bed for performance evaluation. To illustrate the flexibility of the proposed method, visual outputs from diverse areas are demonstrated. Computational performance data are also provided to demonstrate the method's efficiency. Results indicate that when large matrices are processed, computations using GPU are two to three hundred times faster than the identical algorithms using CPU.
C1 [Gobron, Stephane; Thalmann, Daniel] Ecole Polytech Fed Lausanne, IC, ISIM, VRLAB, CH-1015 Lausanne, Switzerland.
   [Coeltekin, Arzu] Univ Zurich, Dept Geog, GIVA, CH-8057 Zurich, Switzerland.
   [Bonafos, Herve] Tecnomade, F-54000 Nancy, France.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; University of Zurich
RP Gobron, S (corresponding author), Ecole Polytech Fed Lausanne, IC, ISIM, VRLAB, Stn 14, CH-1015 Lausanne, Switzerland.
EM stephane.gobron@epfl.ch
RI Çöltekin, Arzu/ACY-8666-2022; Thalmann, Daniel/A-4347-2008; Thalmann,
   Daniel/AAL-1097-2020
OI Çöltekin, Arzu/0000-0002-3178-3509; Thalmann, Daniel/0000-0002-0451-7491
FU SNF [120434, 122696]; European Union COSI-ICT [IST FP7 231323]
FX We are grateful to Dr. Helena Grillon, Dr. Junghyun Ahn, and Mr. Patrick
   Salamin for their suggestions, which greatly improved the manuscript. We
   also would like to express our gratitude to the three anonymous
   reviewers for their valuable feedback. This work was supported by the
   following grants: SNF grant "GeoF" (No. 120434), SNF grant
   "AERIALCROWDS" (No. 122696), and the European Union COSI-ICT
   "CYBER-EMOTIONS" (IST FP7 231323).
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   Coltekin A., 2006, PHOTOGRAM J FINL, V20
   Coutinho B.B.S., 2008, LNCC REPORTS 2008
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Durbeck LJK, 2001, NANOTECHNOLOGY, V12, P217, DOI 10.1088/0957-4484/12/3/305
   Ganguly Niloy., 2003, SURVEY CELLULAR AUTO
   GARDNER M, 1970, SCI AM, V223, P120, DOI 10.1038/scientificamerican1070-120
   GARDNER M, 1971, SCI AM
   GARDNER M, 1970, SCI AM
   GERHARDT M, 1990, SCIENCE, V247, P1563, DOI 10.1126/science.2321017
   Gobron S, 2008, LECT NOTES COMPUT SC, V5191, P512, DOI 10.1007/978-3-540-79992-4_67
   HARRIS MJ, 2002, HWWS 02, P109
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   Phillips W., 1981, INTRO MINERALOGY GEO
   Pinto N., 2007, ARCHIT CITY ENV, V1, P3
   PRUSINKIEWICZ P, 1993, GRAPH INTER, P128
   Rost R.J., 2006, OpenGL Shading Language, V2nd
   Shreiner D., 2005, OPENGL PROGRAMMING G, V5th
   Singler J., 2004, ACM WORKSH GEN PURP
   Smith M., 1994, THESIS I NATL POLYTE
   Tatarchuk N, 2008, J PARALLEL DISTR COM, V68, P1319, DOI 10.1016/j.jpdc.2008.06.011
   Thalmann D., 1986, Visual Computer, V2, P384, DOI 10.1007/BF01952423
   Tomassini M, 2000, IEEE T COMPUT, V49, P1146, DOI 10.1109/12.888056
   Tran J., 2003, New challenges for cellular automata simulation on the GPU
   Von Neumann J., 1970, ESSAYS CELLULAR AUTO
   WOLFRAM S, 1984, PHYSICA D, V10, pR7, DOI 10.1016/0167-2789(84)90244-6
   Wolfram S., 2002, A new kind of science
   Wu T.-C., 2010, 2 INT C COMP MOD SIM, P513
   Zaloudek L, 2009, 2009 COMPUTATION WORLD: FUTURE COMPUTING, SERVICE COMPUTATION, COGNITIVE, ADAPTIVE, CONTENT, PATTERNS, P533, DOI 10.1109/ComputationWorld.2009.49
   Zhao Y, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P19, DOI 10.1109/SMI.2008.4547942
NR 30
TC 14
Z9 15
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2011
VL 27
IS 1
BP 67
EP 81
DI 10.1007/s00371-010-0515-1
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 700YI
UT WOS:000285781100006
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Benhabiles, H
   Vandeborre, JP
   Lavoué, G
   Daoudi, M
AF Benhabiles, Halim
   Vandeborre, Jean-Philippe
   Lavoue, Guillaume
   Daoudi, Mohamed
TI A comparative study of existing metrics for 3D-mesh segmentation
   evaluation
SO VISUAL COMPUTER
LA English
DT Article
DE 3D-mesh segmentation; Ground-truth; Similarity metric; Subjective tests;
   Evaluation
AB In this paper, we present an extensive experimental comparison of existing similarity metrics addressing the quality assessment problem of mesh segmentation. We introduce a new metric, named the 3D Normalized Probabilistic Rand Index (3D-NPRI), which outperforms the others in terms of properties and discriminative power. This comparative study includes a subjective experiment with human observers and is based on a corpus of manually segmented models. This corpus is an improved version of our previous one (Benhabiles et al. in IEEE International Conference on Shape Modeling and Application (SMI), 2009). It is composed of a set of 3D-mesh models grouped in different classes associated with several manual ground-truth segmentations. Finally the 3D-NPRI is applied to evaluate six recent segmentation algorithms using our corpus and the Chen et al.'s (ACM Trans. Graph. (SIGGRAPH), 28(3), 2009) corpus.
C1 [Benhabiles, Halim; Vandeborre, Jean-Philippe; Daoudi, Mohamed] Univ Lille, UMR USTL CNRS 8022, Comp Sci Lab, LIFL, Lille, France.
   [Vandeborre, Jean-Philippe; Daoudi, Mohamed] TELECOM Lille 1, Inst TELECOM, Lille, France.
   [Lavoue, Guillaume] Univ Lyon, CNRS, UMR 5205, INSA Lyon,LIRIS, F-69621 Lyon, France.
C3 Universite de Lille; IMT - Institut Mines-Telecom; IMT Atlantique;
   Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS)
RP Benhabiles, H (corresponding author), Univ Lille, UMR USTL CNRS 8022, Comp Sci Lab, LIFL, Lille, France.
EM halim.benhabiles@lifl.fr; jean-philippe.vandeborre@lifl.fr;
   glavoue@liris.cnrs.fr; mohamed.daoudi@lifl.fr
RI ; Daoudi, Mohammed/H-5935-2013
OI Benhabiles, Halim/0000-0003-2950-5312; Daoudi,
   Mohammed/0000-0003-4219-7860
FU ANR (Agence Nationale de la Recherche, France) [ANR-07-MDCO-015]
FX This work is supported by the ANR (Agence Nationale de la Recherche,
   France) through MADRAS project (ANR-07-MDCO-015).
CR [Anonymous], IEEE INT C SHAP MOD
   ANTINI G, 2005, IEEE INT C MULT EXP
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Berretti S, 2009, IMAGE VISION COMPUT, V27, P1540, DOI 10.1016/j.imavis.2009.02.004
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   DANIEL WW, 1999, FDN ANAL HLTH SCI BO
   FOWLKES EB, 1983, J AM STAT ASSOC, V78, P553, DOI 10.2307/2288117
   FUNKHOUSER T, 2004, ACM T GRAPH P SIGGRA
   Gerig G., 2001, Medical Image Computing and Computer-Assisted InterventionMICCAI 2001, Proceedings of the 4th International Conference Utrecht, The Netherlands, 1417 October 2001, P516
   Giorgi D., 2007, Shrec: shape retrieval contest: Watertight models track
   Golovinskiy A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409098
   Ji ZP, 2006, COMPUT GRAPH FORUM, V25, P283, DOI 10.1111/j.1467-8659.2006.00947.x
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   LAI YK, 2008, SPM 08
   Lavoué G, 2005, COMPUT AIDED DESIGN, V37, P975, DOI 10.1016/j.cad.2004.09.001
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   SHEFFER A, 2007, FDN TRENDS COMPUTER, V2, P64
   Tierny J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P215, DOI 10.1109/SMI.2007.38
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Zhang H, 2008, COMPUT VIS IMAGE UND, V110, P260, DOI 10.1016/j.cviu.2007.08.003
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 30
TC 27
Z9 29
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2010
VL 26
IS 12
BP 1451
EP 1466
DI 10.1007/s00371-010-0494-2
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 678WA
UT WOS:000284112400003
DA 2024-07-18
ER

PT J
AU Bandeira, D
   Walter, M
AF Bandeira, Djalma
   Walter, Marcelo
TI Highlights on weathering effects
SO VISUAL COMPUTER
LA English
DT Article
DE Weathering; Imperfections; Highlight removal
ID REFLECTION COMPONENTS
AB Current techniques for image-based weathering and deweathering operations are not efficient in cases where the images have strong specular effects. Highlights lack chroma information, and therefore special treatment for them is important for correct appearance modeling. We introduce techniques to establish efficient control of weathering and deweathering operations on images with high levels of specular highlights. Our solution adjusts the highlight information so that a more coherent result when transferring material properties is achieved. We also present a solution for transferring of material features between objects with very different reflectance properties, and explore normal mapping applications to improve the appearance modeling of materials with geometric variations.
C1 [Bandeira, Djalma] Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
   [Walter, Marcelo] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
C3 Universidade Federal de Pernambuco; Universidade Federal do Rio Grande
   do Sul
RP Bandeira, D (corresponding author), Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
EM dbs@cin.ufpe.br; marcelo.walter@inf.ufrgs.br
RI Walter, Marcelo/A-1964-2013; Walter, Marcelo/O-7526-2019
OI Walter, Marcelo/0000-0002-5634-8765
FU FACEPE [IBPG-0216-1.03/08, APQ-0203-1.03/06]; CNPq [483356/2007]
FX Work partially supported by FACEPE through grants IBPG-0216-1.03/08 and
   APQ-0203-1.03/06 and CNPq through grant 483356/2007.
CR Aoki K, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P467, DOI 10.1109/PCCGA.2002.1167903
   BADLER NI, 1990, J VISUAL COMP ANIMAT, V1, P26
   BANDEIRA D, 2009, BRAZ S COMPUT GRAPH, V22, P32
   Ben-Artzi A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356686
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Bosch C, 2004, COMPUT GRAPH FORUM, V23, P361, DOI 10.1111/j.1467-8659.2004.00767.x
   Chang YX, 2003, VISUAL COMPUT, V19, P50, DOI 10.1007/s00371-002-0172-0
   CHEN Y, 2005, SIGGRAPH 05, P1127
   Desbenoit B, 2004, COMPUT GRAPH FORUM, V23, P341, DOI 10.1111/j.1467-8659.2004.00765.x
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Dorsey J., 2007, DIGITAL MODELING MAT
   FANG H., 2004, SIGGRAPH 04, P354
   Gu JW, 2006, ACM T GRAPHIC, V25, P762, DOI 10.1145/1141911.1141952
   Hirota K, 1998, VISUAL COMPUT, V14, P126, DOI 10.1007/s003710050128
   HSU SC, 1995, IEEE COMPUT GRAPH, V15, P18, DOI 10.1109/38.364957
   Iben H.N., 2006, P ACM SIGGRAPHEUROGR, P177
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Mérillou S, 2008, COMPUT GRAPH-UK, V32, P159, DOI 10.1016/j.cag.2008.01.003
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   Paquette E, 2002, PROC GRAPH INTERF, P59
   Paquette Eric., 2001, GRAPHICS INTERFACE, P175
   Shen HL, 2008, PATTERN RECOGN, V41, P2461, DOI 10.1016/j.patcog.2008.01.026
   Spencer B, 2009, IEEE T VIS COMPUT GR, V15, P49, DOI 10.1109/TVCG.2008.67
   Sun B, 2007, IEEE T VIS COMPUT GR, V13, P595, DOI 10.1109/TVCG.2007.1013
   Tan P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P164, DOI 10.1109/ICCV.2003.1238333
   Tan RT, 2005, IEEE T PATTERN ANAL, V27, P178, DOI 10.1109/TPAMI.2005.36
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Valette G, 2008, COMPUT GRAPH FORUM, V27, P47, DOI 10.1111/j.1467-8659.2007.01042.x
   Wang JP, 2006, ACM T GRAPHIC, V25, P754, DOI 10.1145/1141911.1141951
   Xue S, 2008, COMPUT GRAPH FORUM, V27, P617
   ZELINKA S., 2005, Proceedings of Graphics Interface 2005, P227
NR 33
TC 3
Z9 3
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 965
EP 974
DI 10.1007/s00371-010-0495-1
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800056
DA 2024-07-18
ER

PT J
AU ElHakim, R
   ElHelw, M
AF ElHakim, Reda
   ElHelw, Mohamed
TI Interactive 3D visualization for wireless sensor networks
SO VISUAL COMPUTER
LA English
DT Article
DE Three-dimensional computer graphics; Interactive visualization; Wireless
   sensor networks; Ubiquitous computing
AB Wireless sensor networks open up a new realm of ubiquitous computing applications based on distributed large-scale data collection by embedded sensor nodes that are wirelessly connected and seamlessly integrated within the environment. 3D visualization of sensory data is a challenging issue, however, due to the large number of sensors used in typical deployments, continuous data streams, and constantly varying network topology. This paper describes a practical approach for interactive 3D visualization of wireless sensor network data. A regular 3D grid is reconstructed using scattered sensor data points and used to generate view-dependent 2D slices that are consequently rendered with commodity graphics hardware leading to smooth visualization over space and time. Furthermore, the use of efficient space partitioning data structures and the independent processing of sensor data points facilitates interactive rendering for large number of sensors while accommodating constantly changing network topology. The practical value of the proposed method is demonstrated and results obtained for visualizing time-varying temperature distributions in an urban area are presented.
C1 [ElHakim, Reda; ElHelw, Mohamed] Nile Univ, Ctr Informat Sci, Ubiquitous Comp Grp, Cairo, Egypt.
C3 Egyptian Knowledge Bank (EKB); Nile University
RP ElHakim, R (corresponding author), Nile Univ, Ctr Informat Sci, Ubiquitous Comp Grp, Cairo, Egypt.
EM reda.mostafa@nileu.edu.eg; melhelw@nileuniversity.edu.eg
OI Elhakim, Reda/0000-0003-2263-2116
CR Akyildiz IF, 2002, COMPUT NETW, V38, P393, DOI 10.1016/S1389-1286(01)00302-4
   Amidror I, 2002, J ELECTRON IMAGING, V11, P157, DOI 10.1117/1.1455013
   BERNARDONA FF, 2007, PARALLEL COMPUT, V33
   BUONADONNA P, 2005, P 2 EUR WORKSH WIR S
   BUSCHMANN C., 2005, SIGBED Review, V2, P1
   FRANKE R, 1991, GEOMETRIC MODELLING
   Grosky WI, 2007, IEEE MULTIMEDIA, V14, P8, DOI 10.1109/MMUL.2007.82
   HANSEN C.D., 2005, The visualization handbook
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MA KL, 2005, VISUALIZATION HDB
   McReynolds T., 2005, ADV GRAPHICS PROGRAM
   PARK W, 2005, P 10 INT FALL WORKSH
   ROSENTHAL P, 2006, P EUR IEEE VGTC S VI
   Samet H, 2006, FDN MULTIDIMENSIONAL
   SELAVO L, 2006, P 3 INT C BROADB COM
   Shepard D., 1968, P 1968 ACM NAT C
   SHU L, 2008, NETTOPO FRAMEWORK SI
   Turon M, 2005, P 2 IEEE WORKSH EMB
   YANG Y, 2006, P 1 IEEE C IND EL AP
NR 19
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1071
EP 1077
DI 10.1007/s00371-010-0451-0
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800066
DA 2024-07-18
ER

PT J
AU Niessner, M
   Schäfer, H
   Stamminger, M
AF Niessner, Matthias
   Schaefer, Henry
   Stamminger, Marc
TI Fast indirect illumination using Layered Depth Images
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time global illumination; Hybrid rendering
ID SOFT; SHADOWS
AB We present a novel hybrid rendering method for diffuse and glossy indirect illumination. A scene is rendered using standard rasterization on a GPU. In a shader, secondary ray queries are used to sample incident light and to compute indirect lighting. We observe that it is more important to cast many rays than to have precise results for each ray. Thus, we approximate secondary rays by intersecting them with precomputed layered depth images of the scene. We achieve interactive to real-time frame rates including indirect diffuse and glossy effects.
C1 [Niessner, Matthias; Schaefer, Henry; Stamminger, Marc] Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Niessner, M (corresponding author), Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
EM Matthias.Niessner@cs.fau.de; Henry.Schaefer@cs.fau.de;
   Marc.Stamminger@cs.fau.de
CR Agrawala M, 2000, COMP GRAPH, P375, DOI 10.1145/344779.344954
   AILA T, 2009, P C HIGH PERF GRAPH, P145, DOI DOI 10.1145/1572769.1572792
   Assarsson U, 2003, ACM T GRAPHIC, V22, P511, DOI 10.1145/882262.882300
   Bavoil Louis, 2008, Journal of Graphics Tools, V13, P19
   Burger K., 2007, VISION MODELING VISU, pEurographics
   DACHSBACHER C, 2006, P 2006 S INT 3D GRAP, P100
   Forest V, 2008, COMPUT GRAPH FORUM, V27, P663, DOI 10.1111/j.1467-8659.2008.01164.x
   Guennebaud G, 2007, COMPUT GRAPH FORUM, V26, P525, DOI 10.1111/j.1467-8659.2007.01075.x
   Hachisuka T., 2005, GPU GEMS
   Havran V, 2003, COMPUT GRAPH-UK, V27, P593, DOI 10.1016/S0097-8493(03)00103-1
   HERTEL S, 2009, HYBRID GPU RENDERING, P59
   KELLER A, 1997, SIGGRAPH 97, P49
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   *NVIDIA, 2001, INT ORD IND TRANSP
   Purcell TimothyJ., 2005, ACM SIGGRAPH Courses, P268
   Ritschel T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409082
   RITSCHEL T, 2007, P EUR S REND CIT
   RITSCHEL T, 2009, ACM SIGGRAPH AS 2009, P1
   Roger David., 2007, Symposium on Rendering, Rendering Techniques, P99, DOI DOI 10.2312/EGWR/EGSR07/099-110
   Schwarz M, 2007, COMPUT GRAPH FORUM, V26, P515, DOI 10.1111/j.1467-8659.2007.01074.x
   SEGOVIA B, 2006, P 21 ACM SIGGRAPH EU, P60
   Sengupta S., 2007, COMPUTING, P106
   SHADE J, 1925, P 25 ANN C COMP GRAP, P231
   Sintorn E, 2008, COMPUT GRAPH FORUM, V27, P1285, DOI 10.1111/j.1467-8659.2008.01267.x
   Wald I., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P15
   Wald I, 2006, ACM T GRAPHIC, V25, P485, DOI 10.1145/1141911.1141913
   XIE F, 2007, COMPUT GRAPH FORUM
NR 27
TC 11
Z9 12
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 679
EP 686
DI 10.1007/s00371-010-0486-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tanuwijaya, S
   Ohno, Y
AF Tanuwijaya, Sindharta
   Ohno, Yoshio
TI TF-DF indexing for mocap data segments in measuring relevance based on
   textual search queries
SO VISUAL COMPUTER
LA English
DT Article
DE Motion capture; Mocap; Retrieval
ID CONTENT-BASED RETRIEVAL; MOTION CAPTURE
AB Many techniques have been proposed to address the problem of mocap data retrieval by using a short motion as input, and they are commonly categorized as content-based retrieval. However, it is difficult for users who do not have equipments to create mocap data samples to take advantage of them. On the contrary, simple retrieval methods which only require text as input can be used by everyone. Nevertheless, not only that it is not clear how to measure mocap data relevance in regard to textual search queries, but the search results will also be limited to the mocap data samples, the annotations of which contain the words in the search query.
   In this paper, the authors propose a novel method that builds on the TF (term frequency) and IDF (inverse document frequency) weights, commonly used in text document retrieval, to measure mocap data relevance in regard to textual search queries. We extract segments from mocap data samples and regard these segments as words in text documents. However, instead of using IDF which prioritizes infrequent segments, we opt to use DF (document frequency) to prioritize frequent segments. Since motions are not required as input, everybody will be able to take advantage of our approach, and we believe that our work also opens up possibilities for applying developed text retrieval methods in mocap data retrieval.
C1 [Tanuwijaya, Sindharta] Keio Univ, Grad Sch Sci & Technol, Kohoku Ku, Yokohama, Kanagawa 223, Japan.
   [Ohno, Yoshio] Keio Univ, Grad Sch, Kohoku Ku, Yokohama, Kanagawa 223, Japan.
C3 Keio University; Keio University
RP Tanuwijaya, S (corresponding author), Keio Univ, Grad Sch Sci & Technol, Kohoku Ku, 3-14-1 Hiyoshi, Yokohama, Kanagawa 223, Japan.
EM sin@on.ics.keio.ac.jp; ohno@on.ics.keio.ac.jp
CR Agarwal S, 2003, ACM T GRAPHIC, V22, P605, DOI 10.1145/882262.882314
   [Anonymous], I3D 09
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Assa J, 2005, ACM T GRAPHIC, V24, P667, DOI 10.1145/1073204.1073246
   BEAUDOIN P, 2008, SCA 08, P117
   Cardle M., 2003, P SIGGRAPH 2003 SKET
   Chiu CY, 2004, J VIS COMMUN IMAGE R, V15, P446, DOI 10.1016/j.jvcir.2004.04.004
   *CMU, 2007, CMU GRAPH LAB MOT CA
   DYER ME, 1985, OPER RES LETT, V3, P285, DOI 10.1016/0167-6377(85)90002-1
   Forbes K., 2005, Dans SCA '05, P67
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   LEVENE M, 2006, INTRO SEARCH ENG WEB
   Liu G., 2005, P 2005 ACM SIGMOD IN, P924
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Shen H, 2005, FOOD AGR IMMUNOL, V16, P273, DOI 10.1080/09540100500399668
   Tanuwijaya S, 2009, GRAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P266
   Vlachos M, 2002, PROC INT CONF DATA, P673, DOI 10.1109/ICDE.2002.994784
   VLACHOS M, 2003, ACM KDD
   Vlasic D, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276421, 10.1145/1239451.1239486]
NR 21
TC 3
Z9 4
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1091
EP 1100
DI 10.1007/s00371-010-0463-9
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800068
DA 2024-07-18
ER

PT J
AU Wang, XP
   Meng, YQ
   Wang, ZG
   Zhang, LY
AF Wang, Xiaoping
   Meng, Yaqin
   Wang, Zhiguo
   Zhang, Liyan
TI Constructing up to G <SUP>2</SUP> continuous curve on freeform surface
SO VISUAL COMPUTER
LA English
DT Article
DE Hermite interpolation; Curve projection; Freeform surface; G(2)
   continuity; Ordinary differential equation
ID SPLINE CURVES; INTERPOLATION; MANIFOLDS; APPROXIMATION; COMPUTATION;
   DESIGN
AB This paper presents new methods for G (1) and G (2) continuous interpolation of an arbitrary sequence of points on an implicit or parametric surface with prescribed tangent direction and both tangent direction and curvature vector, respectively, at every point. We design a G (1) or G (2) continuous curve in three-dimensional space, construct a so-called directrix vector field using the space curve and then project a special straight line segment onto the given surface along the directrix vector field. With the techniques in classical differential geometry, we derive a system of differential equations for the projection curve. The desired interpolation curve is just the projection curve, which can be obtained by numerically solving the initial-value problems for a system of first-order ordinary differential equations in the parametric domain associated to the surface representation for the parametric case or in three-dimensional space for the implicit case. Several shape parameters are introduced into the resulting curve, which can be used in subsequent interactive modification such that the shape of the resulting curve meets our demand. The presented method is independent of the geometry and parameterization of the base surface, and numerical experiments demonstrate that it is effective and potentially useful in patterns design on surface.
C1 [Wang, Xiaoping; Wang, Zhiguo; Zhang, Liyan] Nanjing Univ Aeronaut & Astronaut, Res Ctr CAD CAM Engn, Nanjing 210016, Peoples R China.
   [Meng, Yaqin] E China Univ Sci & Technol, Coll Sci, Shanghai 200237, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; East China University
   of Science & Technology
RP Wang, XP (corresponding author), Nanjing Univ Aeronaut & Astronaut, Res Ctr CAD CAM Engn, Nanjing 210016, Peoples R China.
EM harayano@hotmail.com; ellenmyq@163.com
RI Yu, ZH/KBC-6889-2024; Wang, Xiao-Ping/G-7394-2011; liu,
   sha/JXL-6600-2024
FU National Natural Science Foundation of China [60673026, 50875130,
   50805075, 50875126]
FX The authors would like to thank the anonymous reviewers for valuable
   comments. The work reported in this paper was supported by National
   Natural Science Foundation of China under grant (Nos. 60673026,
   50875130, 50805075 and 50875126).
CR [Anonymous], 1992, SMR
   BOHL H, 1999, THESIS U STUTTGART S
   CARNARINHA M, 1995, IMA J MATH CONTROL I, V12, P399
   Crouch P., 1995, Journal of Dynamical and Control Systems, V1, P177, DOI 10.1007/BF02254638
   DIETZ R, 1993, COMPUT AIDED GEOM D, V10, P211, DOI 10.1016/0167-8396(93)90037-4
   Do Carmo Manfredo P, 2016, Dover Books on Mathematics, Vsecond
   Flöry S, 2008, COMPUT AIDED DESIGN, V40, P25, DOI 10.1016/j.cad.2007.01.012
   Gu XF, 2006, GRAPH MODELS, V68, P237, DOI 10.1016/j.gmod.2006.03.004
   Hartmann E, 1996, VISUAL COMPUT, V12, P181
   Hofer M, 2004, ACM T GRAPHIC, V23, P284, DOI 10.1145/1015706.1015716
   Hughes J. F., 1999, Journal of Graphics Tools, V4, P33, DOI 10.1080/10867651.1999.10487513
   Jinggong Li, 1990, Computer-Aided Geometric Design, V7, P209, DOI 10.1016/0167-8396(90)90032-M
   Lamnii A, 2008, J COMPUT APPL MATH, V213, P439, DOI 10.1016/j.cam.2007.01.017
   *MATH WORKS INC, 2000, US MATLAB VERS 6
   NOAKES L, 1989, IMA J MATH CONTROL I, V6, P456
   PARK FC, 1995, J MECH DESIGN, V117, P36
   Pegna J, 1996, J MECH DESIGN, V118, P45, DOI 10.1115/1.2826855
   Popiel T, 2006, COMPUT AIDED GEOM D, V23, P261, DOI 10.1016/j.cagd.2005.11.003
   Popiel T, 2007, J APPROX THEORY, V148, P111, DOI 10.1016/j.jat.2007.03.002
   Pottmann H, 2005, COMPUT AIDED GEOM D, V22, P693, DOI 10.1016/j.cagd.2005.06.006
   Puig-Pey J, 2004, LECT NOTES COMPUT SC, V3044, P771
   Qu J, 2004, ENG COMPUT-GERMANY, V20, P22, DOI 10.1007/s00366-004-0275-5
   Renner G, 2004, COMPUT AIDED DESIGN, V36, P351, DOI 10.1016/S0010-4485(03)00100-3
   Shoemaker K., 1985, Computer Graphics, V19, P245, DOI 10.1145/325165.325242
   SPROTT K, 1997, P 1997 ASME DES ENG, P14
   Wang Xiao-Ping, 2004, Journal of Software, V15, P451
   Wang XP, 2007, ADV ENG SOFTW, V38, P150, DOI 10.1016/j.advengsoft.2006.08.015
   Wang XP, 2010, INT J COMPUT MATH, V87, P2291, DOI 10.1080/00207160802624349
   WOLTER FE, 1992, ENG COMPUT, V8, P61, DOI 10.1007/BF01200103
NR 29
TC 5
Z9 9
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 813
EP 822
DI 10.1007/s00371-010-0462-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800041
DA 2024-07-18
ER

PT J
AU Cheng, X
   Liu, GD
   Pan, ZG
   Tang, B
AF Cheng, Xi
   Liu, Gengdai
   Pan, Zhigeng
   Tang, Bing
TI Fragment-based responsive character motion for interactive games
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Motion fragment; Character control; Motion graph; Responsive character
   animation
ID GENERATION; CAPTURE
AB Fragment-based character animation has become popular in recent years. By stringing appropriate motion capture fragments together, the system drives characters responding to the control signals of the user and generates realistic character motions. In this paper, we propose a novel, straightforward and fast method to build the control policy table, which selects the next motion fragment to play based on the current user's input and the previous motion fragment. During the synthesis of the control policy table, we cluster similar fragments together to create several fragment classes. Dynamic programming is employed to generate the training samples based on the control signals of the user. Finally, we use a supervised learning routine to create the tabular control policy. We demonstrate the efficacy of our method by comparing the motions generated by our controller to the optimal controller and other previous controllers. The results indicate that although a reinforcement learning algorithm known as value iteration also creates the tabular control policy, it is more complex and requires more expensive space-time cost in synthesis of the control policy table. Our approach is simple but efficient, and is practical for interactive character games.
C1 [Cheng, Xi; Liu, Gengdai; Pan, Zhigeng; Tang, Bing] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Pan, ZG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM zhigengpan@gmail.com
CR Ahokas H., 2004, On the evolution, spread and names of rutabaga, DOI 10.1145/1077534.1077542
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Da Silva Marco, 2008, COMPUTER GRAPHICS FO, V27, P371
   Gleicher M., 2003, P 2003 S INTERACTIVE, P181
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kry PG, 2006, ACM T GRAPHIC, V25, P872, DOI 10.1145/1141911.1141969
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   McCann J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276385, 10.1145/1239451.1239457]
   PULLEN K, 2002, P 29 ANN C COMP GRAP, P501
   REITSMA PSA, 2004, P 2004 ACM SIGGRAPH, P89
   Safonova A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239557
   SCHODL A, 2000, GITGVU0011
   Tang B, 2006, COMPUT ANIMAT VIRT W, V17, P271, DOI 10.1002/cav.131
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
   Zordan V, 2007, SANDBOX SYMPOSIUM 2007: ACM SIGGRAPH VIDEO GAME SYMPOSIUM, PROCEEDINGS, P9
NR 20
TC 2
Z9 5
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 479
EP 485
DI 10.1007/s00371-009-0343-3
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300012
DA 2024-07-18
ER

PT J
AU Jia, C
   Popescu, V
AF Jia, Chun
   Popescu, Voicu
TI Compact real-time modeling of seated humans by video sprite sequence
   quantization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Image-based modeling and rendering; Video compression; Vector
   quantization
ID TRACKING
AB We propose an image-based method for real-time modeling of seated humans using upper-body video sprites, which is suitable for applications such as teleconferencing and distance learning. A database of representative video sprite sequences is pre-acquired and pre-uploaded to each remote rendering site. At run-time, for each input sprite, a closely matching sprite is located in the database and the index of the matching sprite is sent to the rendering site, which drastically reduces the data rate. Unlike other data compression methods, our method takes advantage of the limited number of significant body positions a participant assumes during a session. Exploiting redundancy between frames with distant time stamps enables aggressive compression rates with high visual and semantic fidelity.
C1 [Jia, Chun; Popescu, Voicu] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Popescu, V (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM chun.jia@gmail.com; popescu@cs.purdue.edu
CR AIZAWA K, 1995, P IEEE, V83, P259, DOI 10.1109/5.364463
   [Anonymous], 144962 ISOIEC
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   CARRANZA J, 2003, P SIGGRAPH 03, P569
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   GIBBS S, 1993, P UIST, P179
   Grammalidis N, 2000, IEEE T CIRC SYST VID, V10, P302, DOI 10.1109/76.825729
   HAKEEM A, 2005, P 13 ANN ACM INT C M, P608
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lee MC, 1997, IEEE T CIRC SYST VID, V7, P130, DOI 10.1109/76.554424
   LENGYEL J, 1997, P SIGGRAPH 97, P233
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   PEARSON DE, 1995, P IEEE, V83, P892, DOI 10.1109/5.387091
   Remondino F, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P116, DOI 10.1109/IM.2003.1240240
   SCHOLL J, 2005, P 13 ANN ACM INT C M, P71
   SHADE J, 1996, P SIGGRAPH 96, P75
   Theobalt C, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P185, DOI 10.1109/PCCGA.2003.1238260
   Theobalt C, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P96, DOI 10.1109/PCCGA.2002.1167843
   TORBORG J, 1996, P SIGGRAPH, P353
   TORRES L, 2000, EUR SIGN PROC C SEPT
   WEN Z, 2004, P IEEE INT C MULT EX, V3, P1631
   WIEGAND T, 2003, H264ISOIEC1449610AVC
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   YANG S, 2004, P 12 ANN ACM INT C M, P676
NR 25
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 565
EP 572
DI 10.1007/s00371-009-0318-4
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, HW
   Tang, K
AF Wang, Huawei
   Tang, Kai
TI Biorthogonal wavelet construction for hybrid quad/triangle meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Biorthogonal wavelet; Quad/triangle subdivision; Lifting scheme; Hybrid
   mesh
ID MULTIRESOLUTION ANALYSIS; SUBDIVISION SCHEME; SURFACES
AB Ever since its introduction by Stam and Loop, the quad/triangle subdivision scheme, which is a generalization of the well-known Catmull-Clark subdivision and Loop subdivision, has attracted a great deal of interest due to its flexibility of allowing both quads and triangles in the same model. In this paper, we present a novel biorthogonal wavelet-constructed through the lifting scheme-that accommodates the quad/triangle subdivision. The introduced wavelet smoothly unifies the Catmull-Clark subdivision wavelet (for quadrilateral meshes) and the Loop subdivision wavelet (for triangular meshes) in a single framework. It can be used to flexibly and efficiently process any complicated semi-regular hybrid meshes containing both quadrilateral and triangular regions. Because the analysis and synthesis algorithms of the wavelet are composed of only local lifting operations allowing fully in-place calculations, they can be performed in linear time. The experiments demonstrate sufficient stability and fine fitting quality of the presented wavelet, which are similar to those of the Catmull-Clark subdivision wavelet and the Loop subdivision wavelet. The wavelet analysis can be used in various applications, such as shape approximation, progressive transmission, data compression and multiresolution edit of complex models.
C1 [Wang, Huawei; Tang, Kai] Hong Kong Univ Sci & Technol, Dept Mech Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Tang, K (corresponding author), Hong Kong Univ Sci & Technol, Dept Mech Engn, Hong Kong, Hong Kong, Peoples R China.
EM hwwang@cityu.edu.hk; mektang@ust.hk
RI Tang, Kai/ABA-9642-2021
OI Tang, Kai/0000-0002-5184-2086
FU Hong Kong RGC [RGC07/08EG.620307]
FX We sincerely thank the Caltech Multi-Resolution Modeling Group for
   publishing the data files on their website. We also sincerely thank the
   anonymous reviewers for their valuable suggestions. This work is
   partially supported by Hong Kong RGC Grant RGC07/08EG.620307.
CR Bajaj C, 2002, VISUAL COMPUT, V18, P343, DOI 10.1007/s003710100150
   BALL AA, 1986, COMPUT AIDED DESIGN, V18, P437, DOI 10.1016/0010-4485(86)90067-9
   Beets K, 2006, LECT NOTES COMPUT SC, V4035, P711, DOI 10.1007/11784203_69
   Bertram M, 2004, COMPUTING, V72, P29, DOI 10.1007/s00607-003-0044-0
   Bertram M, 2004, IEEE T VIS COMPUT GR, V10, P326, DOI 10.1109/TVCG.2004.1272731
   Bertram M, 2000, IEEE VISUAL, P389, DOI 10.1109/VISUAL.2000.885720
   Bonneau GP, 1998, IEEE T VIS COMPUT GR, V4, P365, DOI 10.1109/2945.765329
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Labsik U, 2000, COMPUT GRAPH FORUM, V19, pC131, DOI 10.1111/1467-8659.00405
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Li DG, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P25
   Li GQ, 2004, VISUAL COMPUT, V20, P180, DOI 10.1007/s00371-003-0238-7
   Litke N, 2001, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2001.964527
   Loop C, 1987, THESIS U UTAH
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Peters J, 2004, ACM T GRAPHIC, V23, P980, DOI 10.1145/1027411.1027415
   Samavati FF, 1999, COMPUT GRAPH FORUM, V18, P97, DOI 10.1111/1467-8659.00361
   Samavati FF, 2002, COMPUT GRAPH FORUM, V21, P121, DOI 10.1111/1467-8659.00572
   Schaefer S, 2005, ACM T GRAPHIC, V24, P28, DOI 10.1145/1037957.1037959
   Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439
   Sederberg T. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P387, DOI 10.1145/280814.280942
   Stam J, 2003, COMPUT GRAPH FORUM, V22, P79, DOI 10.1111/1467-8659.t01-2-00647
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P113, DOI 10.1109/TVCG.2004.1260763
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   Velho L, 2001, COMPUT AIDED GEOM D, V18, P397, DOI 10.1016/S0167-8396(01)00039-5
   Wang HW, 2007, IEEE T VIS COMPUT GR, V13, P914, DOI 10.1109/TVCG.2007.1031
   Wang HW, 2006, VISUAL COMPUT, V22, P874, DOI 10.1007/s00371-006-0074-7
   Wang HW, 2008, COMPUT AIDED GEOM D, V25, P816, DOI 10.1016/j.cagd.2007.11.002
   Wu JS, 2003, INT J GEOGR INF SCI, V17, P273, DOI 10.1080/1365881022000016016
   Zorin D, 2001, COMPUT AIDED GEOM D, V18, P429, DOI 10.1016/S0167-8396(01)00040-1
   ZORIN D., 1996, P SIGGRAPH ANN C COM, P189
NR 40
TC 2
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2009
VL 25
IS 4
BP 349
EP 366
DI 10.1007/s00371-008-0300-6
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 413YH
UT WOS:000263830600005
DA 2024-07-18
ER

PT J
AU Long, J
   Mould, D
AF Long, Jeremy
   Mould, David
TI Dendritic stylization
SO VISUAL COMPUTER
LA English
DT Article
DE Non-photorealistic rendering; Modeling; Dendritic structures; Tree
   modeling; Pareidolia
AB Dendritic or branching structures are commonly seen in natural phenomena such as lightning, cracks, and vegetal growth. They are also often used for artistic or decorative purposes. We present a new procedural method for modeling dendritic structures based on a path planning approach. Our method includes the use of a partial non-scalar distance metric that gives us powerful and responsive control over the evolving dendritic structure. We demonstrate the effectiveness of our approach by creating dendritic stylizations of input images. We also show how our approach can be used to model more complex dendritic structures, such as trees; our algorithm allows us to create pareidolia effects, where an image is embedded within the branches of the tree.
C1 [Long, Jeremy] Univ Victoria, Dept Comp Sci, Victoria, BC, Canada.
   [Mould, David] Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 0W0, Canada.
C3 University of Victoria; University of Saskatchewan
RP Long, J (corresponding author), Univ Victoria, Dept Comp Sci, Victoria, BC, Canada.
EM jsl@csc.uvic.ca; mould@cs.usask.ca
CR [Anonymous], 1995, FRACTALS DISORDERED
   [Anonymous], SIGGRAPH, DOI DOI 10.1145/237170.237279
   [Anonymous], 1996, The Algorithmic Beauty of Plants
   [Anonymous], 2002, P 2 INT S NONPH AN R, DOI DOI 10.1145/508535.508537
   [Anonymous], 1995, SIGGRAPH
   Ball P., 2001, The Self-made Tapestry: Pattern Formation in Nature
   BRADY RM, 1984, NATURE, V309, P225, DOI 10.1038/309225a0
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   DALI S, 1996, SALVADOR DALI MUSEUM
   DEMKO S, 1985, SIGGRAPH 85, P271, DOI DOI 10.1145/325334.325245
   Desbenoit B, 2004, COMPUT GRAPH FORUM, V23, P341, DOI 10.1111/j.1467-8659.2004.00765.x
   Gewali L., 1988, Proceedings of the Fourth Annual Symposium on Computational Geometry, P266, DOI 10.1145/73393.73421
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Kim T, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P267, DOI 10.1109/PCCGA.2004.1348357
   KIM T, 2003, SCA 03, P86
   Kim T, 2007, IEEE COMPUT GRAPH, V27, P68, DOI 10.1109/MCG.2007.33
   Long Jeremy, 2007, Proceedings Graphics Interface 2007, P257, DOI 10.1145/1268517.1268559
   MOULD D, 2005, GI 05, P219
   MOULD D, 2007, P COMP AESTH GRAPH V, P44
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Pai DK, 1998, IEEE T ROBOTIC AUTOM, V14, P19, DOI 10.1109/70.660835
   Pedersen H., 2006, NPAR 06, P79, DOI [10.1145/1124728.1124742, DOI 10.1145/1124728.1124742]
   POWER JL, 1999, I3D 99, P175, DOI DOI 10.1145/300523.300548
   PRUSINKIEWICZ P, 1994, SIGGRAPH 94, P351, DOI DOI 10.1145/192161.192254
   REEVES WT, 1985, SIGGRAPH 85, P313, DOI DOI 10.1145/325334.325250
   SALISBURY MP, 1994, SIGGRAPH 94, P101, DOI DOI 10.1145/192161.192185
   SHIRAISHI M, 2000, NPAR 2000, P53, DOI DOI 10.1145/340916.340923
   Streit L, 1998, COMPUT GRAPH FORUM, V17, pC207, DOI 10.1111/1467-8659.00268
   TAN P, 2007, SIGGRAPH 2007, P87, DOI DOI 10.1145/1275808.1276486
   WANG H, 1965, SCI AM, V213, P98, DOI 10.1038/scientificamerican1165-98
   WANG H, 1961, AT&T TECH J, V40, P1
   WEBER J, 1995, SIGGRAPH 95, P119, DOI DOI 10.1145/218380.218427
   Winston P.H., 1992, Artificial Intelligence
   WITTEN TA, 1981, PHYS REV LETT, V47, P1400, DOI 10.1103/PhysRevLett.47.1400
   Wong MichaelT., 1998, SIGGRAPH'98, P423, DOI DOI 10.1145/280814.280948
   XU J, 2007, SIGGRAPH 07, P29, DOI DOI 10.1145/1275808.1276414
   YAPO M, 2007, PREMONITION MOTION P
NR 37
TC 1
Z9 1
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 241
EP 253
DI 10.1007/s00371-008-0217-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200004
DA 2024-07-18
ER

PT J
AU Komodakis, N
   Tziritas, G
AF Komodakis, Nikos
   Tziritas, Georgios
TI Real-time exploration and photorealistic reconstruction of large natural
   environments
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based rendering; Morphable 3D models; Geometric morphing; 3D
   mosaics
AB This paper presents a hybrid (geometry- and image-based) framework suitable for providing photorealistic walkthroughs of large, complex outdoor scenes, based only on a small set of real images from the scene. To this end, a novel data representation of a 3D scene is proposed, which is called morphable 3D panoramas. Motion is assumed to be taking place along a predefined path of the 3D environment and the input to the system is a sparse set of stereoscopic views at certain positions (key positions) along that path (one view per position). An approximate local 3D model is constructed from each view, capable of capturing the photometric and geometric properties of the scene only locally. Then, during the rendering process, a continuous morphing (both photometric as well as geometric) takes place between successive local 3D models, using what we call a 'morphable 3D model'. For the estimation of the photometric morphing, a robust algorithm capable of extracting a dense field of 2D correspondences between wide-baseline images is used, whereas, for the geometric morphing, a novel method of computing 3D correspondences between local models is proposed. In this way, a physically valid morphing is always produced, which is thus kept transparent from the user. Moreover, a highly optimized rendering path is used during morphing. Thanks to the use of appropriate pixel and vertex shaders, this rendering path can be run fully in 3D graphics hardware and thus allows for high frame rates.
   Our system can be extended to handle multiple stereoscopic views (and therefore multiple local models) per key position of the path (related by a camera rotation). In this case, one local 3D panorama (per key position) is constructed, comprising all local 3D models therein, and so a 'morphable 3D panorama' is now used during the rendering process. For handling the geometric consistency of each 3D panorama, a technique which is based on solving a partial differential equation is adopted. The effectiveness of our framework is demonstrated by using it for the 3D visual reconstruction of the Samaria Gorge in Crete.
C1 [Komodakis, Nikos; Tziritas, Georgios] Univ Crete, Dept Comp Sci, Iraklion, Greece.
C3 University of Crete
RP Komodakis, N (corresponding author), Univ Crete, Dept Comp Sci, Iraklion, Greece.
EM komod@csd.uoc.gr; tziritas@csd.uoc.gr
RI Tziritas, Georgios/AAO-5855-2021
FU Reinforcement Programme of Human Research Manpower (PENED); National and
   Community Funds; EU European Social Fund; Greek Ministry of Development
   - General Secretariat of Research and Technology
FX This work is part of the 03ED417 research project, implemented within
   the framework of the Reinforcement Programme of Human Research Manpower
   (PENED) and cofinanced by National and Community Funds (75% from the EU
   European Social Fund and 25% from the Greek Ministry of Development -
   General Secretariat of Research and Technology).
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Aliaga DG, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P331, DOI 10.1109/VISUAL.2002.1183792
   [Anonymous], 1995, Markov random field modeling in computer vision
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Bouguet J., CAMERA CALIBRATION T
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Faugeras O., 2001, The geometry of multiple images: the laws that govern the formation of multiple images of a scene and some of their applications
   Felzenszwalb PR, 2004, PROC CVPR IEEE, P261
   FLEISHMAN S, 1999, P VRST99, P82
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   GRINIAS I, 2002, P INT C DIG SIGN PRO, P679
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   HECKBERT P, 1995, CMUSCS95194
   Heung-Yeung Shum, 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P236
   KOCH R, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P109, DOI 10.1109/ICCV.1995.466799
   Komodakis N, 2005, IEEE I CONF COMP VIS, P1018
   KOMODAKIS N, 2006, VID P CVPR
   KOMODAKIS N, 2004, P 2 INT S 3D PROC VI
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MA Y, 2005, INVITATION 3D VISION
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   McMillan L, 1997, IMAGE BASED APPROACH
   Mikolajczyk K., 2002, P INT C COMPUTER VIS, P128
   Moreels P, 2007, INT J COMPUT VISION, V73, P263, DOI 10.1007/s11263-006-9967-1
   Nistér D, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P438, DOI 10.1109/TDPVT.2004.1335271
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   SCHIRMACHER H, 2001, P EUR 2001, V20, pC165
   Segal M., OPENGL GRAPHICS SYST
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Strecha C, 2004, PROC CVPR IEEE, P552
   Strecha C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1194
   Strecha C, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P416, DOI 10.1109/TDPVT.2002.1024097
   TELLER S, 1998, P IM UND WORKSH, P455
   Tuytelaars T., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P412
   Uyttendaele M, 2004, IEEE COMPUT GRAPH, V24, P52, DOI 10.1109/MCG.2004.1297011
   VEDULA S, 2002, P 13 ACM EUR WORKSH, P65
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
   Zwilliger D., 1997, HDB DIFFERENTIAL EQU
NR 44
TC 1
Z9 1
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 117
EP 137
DI 10.1007/s00371-008-0209-0
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700003
DA 2024-07-18
ER

PT J
AU Yersin, B
   Maïm, J
   Morini, F
   Thalmann, D
AF Yersin, Barbara
   Maiem, Jonathan
   Morini, Fiorenzo
   Thalmann, Daniel
TI Real-time crowd motion planning
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds
CY OCT 24-27, 2007
CL Hannover, GERMANY
SP Welfenlab, Gottfried Wilhelm Leibniz Univ, EuroGraphics, ACM SIGWEB, ACM SIGART
DE crowds; real-time; motion planning; groups
ID FLOW
AB Real-time crowd motion planning requires fast, realistic methods for path planning as well as obstacle avoidance. In a previous work (Morini et al. in Cyberworlds International Conference, pp. 144-151, 2007), we introduced a hybrid architecture to handle real-time motion planning of thousands of pedestrians. In this article, we present an extended version of our architecture, introducing two new features: an improved short-term collision avoidance algorithm, and simple efficient group behavior for crowds. Our approach allows the use of several motion planning algorithms of different precision for regions of varied interest. Pedestrian motion continuity is ensured when switching between such algorithms. To assess our architecture, several performance tests have been conducted, as well as a subjective test demonstrating the impact of using groups. Our results show that the architecture can plan motion in real time for several thousands of characters.
C1 [Yersin, Barbara; Maiem, Jonathan; Morini, Fiorenzo; Thalmann, Daniel] IC ISIM VRLAB, CH-1015 Lausanne, Switzerland.
RP Yersin, B (corresponding author), IC ISIM VRLAB, Stat 14, CH-1015 Lausanne, Switzerland.
EM barbara.yersin@epfl.ch; jonathan.maim@epfl.ch;
   fiorenzo.morini@gmail.com; daniel.thalmann@epfl.ch
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020
OI Thalmann, Daniel/0000-0002-0451-7491
CR [Anonymous], 2006, 900 SIGGRAPH S VID G, P113, DOI [DOI 10.1145/1183316.1183333, 10.1145/1183316.1183333]
   [Anonymous], S COMP AN 05 P NEW Y
   BAYAZIT OB, 2003, ICAL 2003, P362
   Chenney Stephen., 2004, Proceedings of the 2004 ACM SIGGRAPH/Euro- graphics symposium on Computer animation, P233, DOI [10.1145/1028523.1028553, DOI 10.1145/1028523.1028553.]
   DEHERASCIECHOMS.P, 2005, VAST 05, P1
   Heigeas L., 2003, GRAPHICON
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1994, EVOLUTION NATURAL ST, P229
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Hughes RL, 2002, TRANSPORT RES B-METH, V36, P507, DOI 10.1016/S0191-2615(01)00015-7
   KAMPHUIS A, 2004, SCA 04, P19
   Kirchner A, 2002, PHYSICA A, V312, P260, DOI 10.1016/S0378-4371(02)00857-9
   Lamarche F, 2004, COMPUT GRAPH FORUM, V23, P509, DOI 10.1111/j.1467-8659.2004.00782.x
   LAU M, 2006, SCA 06, P299
   LEE KH, 2007, SCA 07
   Loscos C, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P122
   Metoyer RA, 2003, COMP ANIM CONF PROC, P149, DOI 10.1109/CASA.2003.1199318
   Morini F, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P144, DOI 10.1109/CW.2007.23
   MUSSE SR, 1997, EUR WORKSH COMP AN S
   Niederberger C, 2003, COMPUT GRAPH FORUM, V22, P323, DOI 10.1111/1467-8659.00679
   PARIS S, 2007, EUROGRAPHICS 07
   PELECHANO, 2007, SCA 07
   PETTRE J, 2007, ICRA 07
   Pettré J, 2006, COMPUT ANIMAT VIRT W, V17, P445, DOI 10.1002/cav.147
   Reynolds Craig., 1999, STEERING BEHAV AUTON
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   SHAO W, 2005, SCA 05, P19
   Sung M, 2004, COMPUT GRAPH FORUM, V23, P519, DOI 10.1111/j.1467-8659.2004.00783.x
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
NR 29
TC 22
Z9 24
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2008
VL 24
IS 10
BP 859
EP 870
DI 10.1007/s00371-008-0286-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 347IQ
UT WOS:000259134200002
DA 2024-07-18
ER

PT J
AU Kwon, JY
   Lee, IK
AF Kwon, Ji-Yong
   Lee, In-Kwon
TI Determination of camera parameters for character motions using motion
   area
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE camera planning; motion exploration; motion area
AB We propose a method to determine camera parameters for character motion, which considers the motion by itself. The basic idea is to approximately compute the area swept by the motion of the character's links that are orthogonally projected onto the image plane, which we call "motion area". Using the motion area, we can determine good fixed camera parameters and camera paths for a given character motion in the off-line or real-time camera control. In our experimental results, we demonstrate that our camera path generation algorithms can compute a smooth moving camera path while the camera effectively displays the dynamic features of character motion. Our methods can be easily used in combination with the method for generating occlusion-free camera paths. We expect that our methods can also be utilized by the general camera planning method as one of heuristics for measuring the visual quality of the scenes that include dynamically moving characters.
C1 [Kwon, Ji-Yong; Lee, In-Kwon] Yonsei Univ, Seoul 120749, South Korea.
C3 Yonsei University
RP Lee, IK (corresponding author), Yonsei Univ, Seoul 120749, South Korea.
EM mage@cs.yonsei.ac.kr; iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022
OI Lee, In-Kwon/0000-0002-1534-1882
CR Bares W. H., 2000, Smart Graphics. Papers from the 2000 AAAI Symposium, P84
   BLINN J, 1988, IEEE COMPUT GRAPH, V8, P76, DOI 10.1109/38.7751
   Bruderlin Armin., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '95, P97, DOI DOI 10.1145/218380.218421
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Christie M, 2005, COMPUT GRAPH FORUM, V24, P247, DOI 10.1111/j.1467-8659.2005.00849.x
   DRUCKER SM, 1995, SI3D 95, P139
   GLEICHER M, 1992, COMP GRAPH, V26, P331, DOI 10.1145/142920.134088
   Gooch B., 2001, EUR WORKSH REND, P83
   HALPER N., 2000, AAAI Spring Symposium, P92
   HALPER N, 2001, P EUR 2001 C, V20, P174
   HE LW, 1996, P 23 ANN C COMP GRAP, P217
   Kennedy Kevin., 2002, SMARTGRAPH '02: Proceedings of the 2nd International Symposium on Smart Graphics, P1
   Kyung MH, 1996, GRAPH MODEL IM PROC, V58, P262, DOI 10.1006/gmip.1996.0022
   Lay D C., 2002, Linear Algebra and Its Applications, V3rd
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   LIN TC, 2004, SHORT COMM PAP P WSC, P289
   Shoemake Ken, 1985, P 12 ANN C COMP GRAP, P245, DOI [DOI 10.1145/325165.325242, DOI 10.1145/325334.325242]
   SOKOLOV D, 2006, INT C COMP GRAPH APP, P184
   SOKOLOV D, 2005, 6 INT EUR S VIRT REA, P67
   TOMLINSON B, 2000, AGENTS 00 P 4 INT C, P317
NR 20
TC 14
Z9 18
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 475
EP 483
DI 10.1007/s00371-008-0228-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800003
DA 2024-07-18
ER

PT J
AU Wang, R
   Cheslack-Postava, E
   Wang, R
   Luebke, D
   Chen, QY
   Hua, W
   Peng, QS
   Bao, HJ
AF Wang, Rui
   Cheslack-Postava, Ewen
   Wang, Rui
   Luebke, David
   Chen, Qianyong
   Hua, Wei
   Peng, Qunsheng
   Bao, Hujun
TI Real-time editing and relighting of homogeneous translucent materials
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE BSSRDF; precomputed radiance transfer; PCA; Haar wavelets; spatial
   compression
ID PRECOMPUTED RADIANCE TRANSFER
AB Existing techniques for fast, high-quality rendering of translucent materials often fix BSSRDF parameters at precomputation time. We present a novel method for accurate rendering and relighting of translucent materials that also enables real-time editing and manipulation of homogeneous diffuse BSSRDFs. We first apply PCA analysis on diffuse multiple scattering to derive a compact basis set, consisting of only twelve 1D functions. We discovered that this small basis set is accurate enough to approximate a general diffuse scattering profile. For each basis, we then precompute light transport data representing the translucent transfer from a set of local illumination samples to each rendered vertex. This local transfer model allows our system to integrate a variety of lighting models in a single framework, including environment lighting, local area lights, and point lights. To reduce the PRT data size, we compress both the illumination and spatial dimensions using efficient nonlinear wavelets. To edit material properties in real-time, a user-defined diffuse BSSRDF is dynamically projected onto our precomputed basis set, and is then multiplied with the translucent transfer information on the fly. Using our system, we demonstrate realistic, real-time translucent material editing and relighting effects under a variety of complex, dynamic lighting scenarios.
C1 [Wang, Rui; Hua, Wei; Peng, Qunsheng; Bao, Hujun] Zhejiang Univ, Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Cheslack-Postava, Ewen] Stanford Univ, Dept Comp Sci, Graph Lab, Stanford, CA 94305 USA.
   [Wang, Rui; Chen, Qianyong] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
   [Luebke, David] NVIDIA Corp, Santa Clara, CA 95050 USA.
C3 Zhejiang University; Stanford University; University of Massachusetts
   System; University of Massachusetts Amherst; Nvidia Corporation
RP Wang, R (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
EM ruiwang@cs.umass.edu; ewencp@cs.stanford.edu; rwang@cad.zju.edu.cn;
   dave@luebke.us
OI Chen, Qian-Yong/0000-0003-2317-9296
CR Ben-Artzi A, 2006, ACM T GRAPHIC, V25, P945, DOI 10.1145/1141911.1141979
   Cheslack-Postava E, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P81
   Colbert M, 2006, IEEE COMPUT GRAPH, V26, P30, DOI 10.1109/MCG.2006.13
   DACHSBACHER C, 2003, P 14 EUR WORKSH REND, P197
   dEon E., 2007, P EUR S REND TECHN, P147
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Hanrahan P., 1993, P ACM SIGGRAPH, P165
   Hao XJ, 2004, ACM T GRAPHIC, V23, P120, DOI 10.1145/990002.990004
   Hasan M, 2006, ACM T GRAPHIC, V25, P1089, DOI 10.1145/1141911.1141998
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   JENSEN HW, 1998, P SIGGRAPH 98, P311, DOI DOI 10.1145/280814.280925
   Lensch HPA, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P214, DOI 10.1109/PCCGA.2002.1167862
   LIU X, 2004, P EUR S REND, P337
   MERTENS T, 2003, P 14 EUR WORKSH REND, P130
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   Ramantoorthi R, 2002, ACM T GRAPHIC, V21, P517, DOI 10.1145/566570.566611
   Sloan PP, 2005, ACM T GRAPHIC, V24, P1216, DOI 10.1145/1073204.1073335
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Sloan PP, 2002, ACM T GRAPHIC, V21, P527, DOI 10.1145/566570.566612
   Sun X, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239478
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Wang R, 2005, ACM T GRAPHIC, V24, P1202, DOI 10.1145/1073204.1073333
   WANG R, 2004, P EUR S REND, P345
   Xu K, 2007, COMPUT GRAPH FORUM, V26, P545, DOI 10.1111/j.1467-8659.2007.01077.x
   Zhou K, 2005, ACM T GRAPHIC, V24, P1196, DOI 10.1145/1073204.1073332
NR 26
TC 19
Z9 21
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 565
EP 575
DI 10.1007/s00371-008-0237-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800012
DA 2024-07-18
ER

PT J
AU van Nierop, OA
   van der Helm, A
   Overbeeke, KJ
   Djajadiningrat, TJP
AF van Nierop, Onno A.
   van der Helm, Aadjan
   Overbeeke, Kees J.
   Djajadiningrat, Tom J. P.
TI A natural human hand model
SO VISUAL COMPUTER
LA English
DT Article
DE natural hand model; evolution; taxonomy; anatomy; biomechanics; joint
   model
ID JOINT
AB We present a skeletal linked model of the human hand that has natural motion. We show how this can be achieved by introducing a new biology-based joint axis that simulates natural joint motion and a set of constraints that reduce an estimated 150 possible motions to twelve. The model is based on observation and literature.
   To facilitate testing and evaluation, we present a simple low polygon count skin that can stretch and bulge. To evaluate we first introduce a hand-motion taxonomy in a two-dimensional parameter space based on tasks that are evolutionary linked to the environment. Second, we discuss and test the model.
   The appendix shows motion sequences of the model and the real hand. Animations can be fetched from our website.
C1 Delft Univ Technol, Fac Ind Design Engn, ID Studio Lab, NL-2628 CE Delft, Netherlands.
   Eindhoven Univ Technol, Fac Ind Design, NL-5612 AZ Eindhoven, Netherlands.
   Philips NV Eindhoven, DEsign Ctr, Philips Design Dept, Eindhoven, Netherlands.
C3 Delft University of Technology; Eindhoven University of Technology;
   Philips
RP van Nierop, OA (corresponding author), Delft Univ Technol, Fac Ind Design Engn, ID Studio Lab, Landbergstr 15, NL-2628 CE Delft, Netherlands.
EM o.a.vannierop@io.tudelft.nl
OI van der Helm, Aadjan/0000-0003-0449-6579
CR Albrecht Irene., 2003, P 2003 ACM SIGGRAPHE, P98
   AN KN, 1979, J BIOMECH, V12, P775, DOI 10.1016/0021-9290(79)90163-5
   ANDREA P, 1982, NIEUW NEDERLANDS GEB
   [Anonymous], 2005, P 2005 ACM SIGGRAPHE, DOI DOI 10.1145/1073368.1073414
   Bando Y, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P166, DOI 10.1109/PCCGA.2002.1167852
   BUCHHOLZ B, 1992, ERGONOMICS, V35, P261, DOI 10.1080/00140139208967812
   CASOLO F, 1994, NATO ASI SER, P200
   COONEY WP, 1981, J BONE JOINT SURG AM, V63, P1371, DOI 10.2106/00004623-198163090-00002
   Edwards B., 1979, Drawing on the right side of the brain: a course in enhancing creativity and artist confidence
   ElKoura G., 2003, Proc. of ACM SIGGRAPH/Eurographics SCA, P110
   FLEMING J, 2001, COMPUTER ARTS
   Gourret J., 1989, Computer Graphics, V23, P21, DOI 10.1145/74334.74335
   Gribnau MaartenW., 1998, CHI 98 conference summary on Human factors in computing systems - CHI '98, number April, P233
   HOLLISTER A, 1992, J ORTHOPAED RES, V10, P454, DOI 10.1002/jor.1100100319
   HUANG Z, 1995, COMPUTER GRAPHICS DE, P235
   Hummels C, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P591, DOI 10.1109/AFGR.1998.671012
   KAPANDJI IA, 1998, HAND TUBIANA, V1, P404
   Kim J, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P37, DOI 10.1109/CGI.2000.852318
   KUCH JJ, 1994, 1994 C REC 28 AS C S, V2, P1252
   Kurihara Tsuneya, 2004, P 2004 ACM SIGGRAPHE, P355
   Lee J, 2006, COMPUT ANIMAT VIRT W, V17, P479, DOI 10.1002/cav.150
   LEE JT, 1995, IEEE COMPUT GRAPH, V15, P77, DOI 10.1109/38.403831
   MacKenzie CL., 1994, Advances in psychology, V104
   Magnenat-Thalmann Nadia, 1988, P GRAPHICS INTERFACE
   McDonald J, 2001, VISUAL COMPUT, V17, P158, DOI 10.1007/s003710100104
   MOCCOZET L, 1997, COMPUT ANIOM, V97, P93
   NAPIER JR, 1956, J BONE JOINT SURG BR, V38, P902, DOI 10.1302/0301-620X.38B4.902
   Rhee Taehyun., 2006, Human Hand Modeling from Surface Anatomy, DOI DOI 10.1145/1111411.1111417
   RIJPKEMA H, 1991, SIGGRAPH COMPUT GRAP, V25, P359
   ROHLING RN, 1993, PRESENCE, V2, P219
   RUNESON S, 1977, SCAND J PSYCHOL, V18, P172, DOI 10.1111/j.1467-9450.1977.tb00274.x
   SNIJDERS CJ, 2001, BIOMECHANICA SPIER S, P379
   WAGNER C, 1988, ERGONOMICS, V31, P97, DOI 10.1080/00140138808966651
   Williams PL., 1995, GRAYS ANATOMY
   WU Y, 1999, P INT C IM PROC JAP, P6
   Yasumuro Y, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P275, DOI 10.1109/IM.1997.603876
   Yi BF, 2005, COMPUT SCI ENG, V7, P92, DOI 10.1109/MCSE.2005.58
NR 37
TC 32
Z9 41
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2008
VL 24
IS 1
BP 31
EP 44
DI 10.1007/s00371-007-0176-x
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 234GU
UT WOS:000251149500003
OA hybrid
DA 2024-07-18
ER

PT J
AU Yin, XT
   Jin, M
   Gu, XF
AF Yin, Xiaotian
   Jin, Miao
   Gu, Xianfeng
TI Computing shortest cycles using universal covering space
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE shortest cycles; universal covering; homotopy
ID PATHS
AB In this paper we generalize the shortest path algorithm to the shortest cycles in each homotopy class on a surface with arbitrary topology, utilizing the universal covering space (UCS) in algebraic topology. In order to store and handle the UCS, we propose a two-level data structure which is efficient for storage and easy to process. We also pointed several practical applications for our shortest cycle algorithms and the UCS data structure.
C1 SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Yin, XT (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM xyin@cs.sunysb.edu; mjin@cs.sunysb.edu; gu@cs.sunysb.edu
RI yin, xiaotian/E-9642-2014
OI Gu, Xianfeng David/0000-0001-8226-5851
CR DEY TK, 1995, DISCRETE COMPUT GEOM, V14, P93, DOI 10.1007/BF02570697
   Dey TK, 1999, J COMPUT SYST SCI, V58, P297, DOI 10.1006/jcss.1998.1619
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Erickson J, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1038
   FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168
   FREDMAN ML, 1987, J ACM, V34, P596, DOI 10.1145/28869.28874
   GU X, 2003, P 2003 EUR ACM SIGGR, P127
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   GUSKOV I, 2001, TOPOLOGICAL NOISE RE, P19
   HERSBERGER J, 1991, P 3 CAN C COMP GEOM, P157
   HERSHBERGER J, 1994, COMP GEOM-THEOR APPL, V4, P63, DOI 10.1016/0925-7721(94)90010-8
   JASON DB, 1977, J ACM, V24, P1
   Lazarus F., 2001, P 17 ANN S COMPUTATI, P80, DOI DOI 10.1145/378583.378630
   Lee YJ, 2002, COMPUT GRAPH FORUM, V21, P229, DOI 10.1111/1467-8659.t01-1-00582
   Schipper H., 1992, Proceedings of the Eighth Annual Symposium on Computational Geometry, P358, DOI 10.1145/142675.142749
   SHOSHAN A, 1999, P 40 FOCS, P605
   Thorup M, 1999, J ACM, V46, P362, DOI 10.1145/316542.316548
   VEGTER G, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P102, DOI 10.1145/98524.98546
   Wood Z, 2004, ACM T GRAPHIC, V23, P190, DOI 10.1145/990002.990007
NR 19
TC 11
Z9 13
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 999
EP 1004
DI 10.1007/s00371-007-0169-9
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400005
DA 2024-07-18
ER

PT J
AU Lyard, E
   Magnenat-Thalmann, N
AF Lyard, Etienne
   Magnenat-Thalmann, Nadia
TI A simple footskate removal method for virtual reality applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE computer animation; motion retargeting; virtual reality
AB Footskate is a common problem encountered in interactive applications dealing with virtual character animations. It has proven difficult to fix without the use of complex numerical methods, which require expert skills for their implementations, along with a fair amount of user interaction to correct a motion. On the other hand, deformable bodies are being increasingly used in virtual reality (VR) applications, allowing users to customize their avatar as they wish. This introduces the need of adapting motions without any help from a designer, as a random user seldom has the skills required to drive the existing algorithms towards the right solution. In this paper, we present a simple method to remove footskate artifacts in VR applications. Unlike previous algorithms, our approach does not rely on the skeletal animation to perform the correction but rather on the skin. This ensures that the final foot planting really matches the virtual character's motion. The changes are applied to the root joint of the skeleton only so that the resulting animation is as close as possible to the original one. Eventually, thanks to the simplicity of its formulation, it can be quickly and easily added to existing frameworks.
C1 Univ Geneva, MIRA Lab, CH-1227 Carouge, Switzerland.
C3 University of Geneva
RP Lyard, E (corresponding author), Univ Geneva, MIRA Lab, Batiment A 7 Route Drize, CH-1227 Carouge, Switzerland.
EM Lyard@miralab.unige.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
CR ABE Y, 2004, P ACM S COMP AN 2004
   Bindiganavale R, 1998, LECT NOTES ARTIF INT, V1537, P70
   BOULIC R, 2003, P EUROGRAPHICS 2003
   Catmull E, 1974, inCom-puter Aided Geometric Design, P317, DOI [DOI 10.1016/B978-0-12-079050-0.50020-5, 10.1016/B978-0-12-079050-0.50020-5]
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   Egges A, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P121, DOI 10.1109/PCCGA.2004.1348342
   Glardon P, 2006, VISUAL COMPUT, V22, P194, DOI 10.1007/s00371-006-0376-9
   GLEICHER M, 1998, P SIGGRAPH 1998 COMP
   GLEICHER M, 1997, P S INT 3D GRAPH ACM
   IKDEMOTO L, 2006, SI3D 06 P 2006 S INT, P49
   JUGGLER VR, 2006, VR JUGGLER OPERN SOU
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   KOVAR L, 2003, SCA 03, P214
   KOVAR L, 2002, P ACM S COMP AN 2002
   LAM W, 2005, P SIGCHI INT C ADV C, P337
   LEE J, 1999, P SIGGRAPH 1999 COMP
   Nickalls R. W. D., 1993, The Mathematical Gazette, V77, P354, DOI DOI 10.2307/3619777
   Ponder M, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P96
   Seo H, 2004, GRAPH MODELS, V66, P1, DOI 10.1016/j.gmod.2003.07.004
   Shin HJ, 2001, ACM T GRAPHIC, V20, P67, DOI 10.1145/502122.502123
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   WITKIN A, 1988, P SIGGRAPH 88, P159
NR 23
TC 7
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 689
EP 695
DI 10.1007/s00371-007-0135-6
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600009
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hafez, M
AF Hafez, Moustapha
TI Tactile interfaces: technologies, applications and challenges
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2006 HAPTEX Workshop
CY NOV 21-23, 2006
CL Natl Commun Assoc, Helsinki, FINLAND
HO Natl Commun Assoc
DE vibrotactile; active materials; thermal rendering; tactile vocabulary
AB Tactile interfaces are used to communicate information through the sense of touch, which is an area of growing interest in the research community. Potential applications include virtual training for surgeons, remotely touching materials via the Internet, automotive industry, active interfaces for blind persons, and sensory substitution devices.
C1 CEA LIST, Sensory Interfaces Lab, F-92265 Fontenay Aux Roses, France.
C3 Universite Paris Saclay; CEA
RP Hafez, M (corresponding author), CEA LIST, Sensory Interfaces Lab, 18 Route Panorama, F-92265 Fontenay Aux Roses, France.
EM Moustapha.hafez@cea.fr
CR CALDWELL DG, 1993, P IEEE RSJ YOK JAP 2
   FUKUDA T, 1997, MICRORESONATOR USING
   HAFEZ M, 2004, 9 INT C NEW ACT ATUA
   KENALEY GL, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P132, DOI 10.1109/ROBOT.1989.99979
   KHOUDJA MB, 2004, IEEE INT C ROB AUT I
   KHOUDJA MB, 2004, INT S ROB ISR 2004 P
   KHOUDJA MB, 2003, INT S MICR HUM SCI M
   Pasquero J., 2003, P EUROHAPTICS, P94
   TAYLOR PM, 1998, 64 ELEMENT TACTILE D, P163
   VUJIC N, 2004, 9 INT C NEW ACT ATUA
   WAGNER CR, 2002, 10 S HAPT INT VIRT E
NR 11
TC 21
Z9 22
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2007
VL 23
IS 4
BP 267
EP 272
DI 10.1007/s00371-007-0102-2
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 172ZW
UT WOS:000246844300005
DA 2024-07-18
ER

PT J
AU Keles, HY
   Es, A
   Isler, V
AF Yalim Keles, Hacer
   Es, Alphan
   Isler, Veysi
TI Acceleration of direct volume rendering with programmable graphics
   hardware
SO VISUAL COMPUTER
LA English
DT Article
DE volume rendering; GPU programming; empty space skipping; early ray
   termination
AB We propose a method to accelerate direct volume rendering using programmable graphics hardware (GPU). In the method, texture slices are grouped together to form a texture slab. Rendering non-empty slabs from front to back viewing order generates the resultant image. Considering each pixel of the image as a ray, slab silhouette maps (SSMs) are used to skip empty spaces along the ray direction per pixel basis. Additionally, SSMs contain terminated ray information. The method relies on hardware z-occlusion culling and hardware occlusion queries to accelerate ray traversals. The advantage of this method is that SSMs are created on the fly by the GPU without any pre-processing. The cost of generating the acceleration structure is very small with respect to the total rendering time.
C1 METU, Tubitak Uzay, Ankara, Turkey.
   METU, Dept Comp Engn, Ankara, Turkey.
C3 Middle East Technical University; Turkiye Bilimsel ve Teknolojik
   Arastirma Kurumu (TUBITAK); Middle East Technical University
RP Keles, HY (corresponding author), METU, Tubitak Uzay, Ankara, Turkey.
EM hacer.yalim@bilten.metu.edu.tr; alphan.es@bilten.metu.edu.tr;
   isler@ceng.metu.edu.tr
RI Keles, Hacer Yalim/W-7934-2018; Isler, Veysi/A-6801-2016
OI Keles, Hacer Yalim/0000-0002-1671-4126; Isler, Veysi/0000-0003-0174-7600
CR Akeley K., 1993, Computer Graphics Proceedings, P109, DOI 10.1145/166117.166131
   Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   COHEN D, 1994, VISUAL COMPUT, V11, P27, DOI 10.1007/BF01900824
   CULLIP TJ, 1993, TR93027 U N CAR DEP
   Engel Klaus, 2001, P ACM SIGGRAPH EUROG, P9, DOI [DOI 10.1145/383507.383515, 10.1145/383507.383515]
   FOLEY T., 2005, HWWS 05, P15
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   LAUR D, 1991, COMP GRAPH, V25, P285, DOI 10.1145/127719.122748
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Li W, 2003, PROC GRAPH INTERF, P81
   Li W, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P317, DOI 10.1109/VISUAL.2003.1250388
   Mark WR, 2003, ACM T GRAPHIC, V22, P896, DOI 10.1145/882262.882362
   Meissner M., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P207, DOI 10.1109/VISUAL.1999.809889
   Neophytou N, 2005, VOLUME GRAPHICS 2005, P197
   REZK-SALAMA C., 2000, EGSIGGRAPH WORKSHOP, P109, DOI DOI 10.1145/346876.348238
   ROETTGER S, 2003, P EG IEEE TCVG S VIS, P231
   SUBRAMANIAN KR, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P150, DOI 10.1109/VISUAL.1990.146377
   UDUPA JK, 1993, IEEE COMPUT GRAPH, V13, P58, DOI 10.1109/38.252558
   VanGelder A, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P23, DOI 10.1109/SVV.1996.558039
   WESTERMANN R, 1998, P SIGGRAPH 98, P169
   [No title captured]
NR 21
TC 5
Z9 6
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2007
VL 23
IS 1
BP 15
EP 24
DI 10.1007/s00371-006-0084-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 113RB
UT WOS:000242613900003
DA 2024-07-18
ER

PT J
AU Cañas, GD
   Gortler, SJ
AF Canas, Guillermo D.
   Gortler, Steven J.
TI Surface remeshing in arbitrary codimensions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE geometric algorithms; surface; remeshing; approximation
ID SIMPLIFICATION
AB We present a method for remeshing surfaces that is both general and efficient. Existing efficient methods are restrictive in the type of remeshings they produce, while methods that are able to produce general types of remeshings are generally based on iteration, which prevents them from producing remeshes at interactive rates. In our method, the input surface is directly mapped to an arbitrary (possibly high-dimensional) range space, and uniformly remeshed in this space. Because the mesh is uniform in the range space, all the quantities encoded in the mapping are bounded, resulting in a mesh that is simultaneously adapted to all criteria encoded in the map, and thus we can obtain remeshings of arbitrary characteristics. Because the core operation is a uniform remeshing of a surface embedded in range space, and this operation is direct and local, this remeshing is efficient and can run at interactive rates.
C1 Harvard Univ, Cambridge, MA 02138 USA.
C3 Harvard University
RP Gortler, SJ (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.
EM gdiez@cs.harvard.edu; sjg@cs.harvard.edu
CR Alliez P, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P49
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Alliez P, 2002, ACM T GRAPHIC, V21, P347, DOI 10.1145/566570.566588
   Andújar C, 2002, ACM T GRAPHIC, V21, P88, DOI 10.1145/508357.508359
   Balmelli L, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P467, DOI 10.1109/VISUAL.2002.1183809
   BHANIRAMKA P, 2000, IEEE VISUALIZATION
   Cañas GD, 2006, PROCEEDINGS OF THE 15TH INTERNATIONAL MESHING ROUNDTABLE, P289, DOI 10.1007/978-3-540-34958-7_17
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   DAZEVEDO EF, 1991, NUMER MATH, V59, P321, DOI 10.1007/BF01385784
   DONG S, 2006, ACM T GRAPHICS
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Edelsbrunner H., 1988, S COMPUTATIONAL GEOM, P118
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Ju T., 2004, SIGGRAPH 04, P888, DOI DOI 10.1145/1015706.1015815
   KOBBELT JP, 2001, SIGGRAPH 01, P57
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Praun E, 2003, ACM T GRAPHIC, V22, P340, DOI 10.1145/882262.882274
   Rocchini C, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P296, DOI 10.1109/SMA.2001.923401
   Schaefer S, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P70, DOI 10.1109/PCCGA.2004.1348336
   SEONG JK, 2005, SHAPE MODELING APPL
   SURAZHSKY V, 2003, S GEOM PROC, P17
   Taubin G, 2002, GRAPH MODELS, V64, P94, DOI 10.1006/gmod.2002.0571
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
NR 27
TC 15
Z9 20
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 885
EP 895
DI 10.1007/s00371-006-0073-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000031
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Han, JW
   Zhou, K
   Wei, LY
   Gong, MM
   Bao, HJ
   Zhang, XM
   Guo, BN
AF Han, Jianwei
   Zhou, Kun
   Wei, Li-Yi
   Gong, Minmin
   Bao, Hujun
   Zhang, Xinming
   Guo, Baining
TI Fast example-based surface texture synthesis via discrete optimization
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE texture synthesis; texture mapping; flow visualization; texture
   animation; energy minimization
AB We synthesize and animate general texture patterns over arbitrary 3D mesh surfaces. The animation is controlled by flow fields over the target mesh, and the texture can be arbitrary user input as long it satisfies the Markov-Random-Field assumptions. We achieve this by extending the texture optimization framework over 3D mesh surfaces. We propose an efficient discrete solver inspired by k-coherence search, allowing interactive flow texture animation while avoiding the blurry blending problem for the least square solver in previous work. Our technique has potential applications ranging from simulation, visualization, and special effects.
C1 Zhejiang Univ, Hangzhou 310027, Peoples R China.
   Microsoft Res Asia, Graph Grp, Beijing, Peoples R China.
   Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Zhejiang University; Microsoft Research Asia; Microsoft; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Han, JW (corresponding author), Zhejiang Univ, Hangzhou 310027, Peoples R China.
EM hanjianwei@cad.zju.edu.cn; kunzhou@microsoft.com; lywei@microsoft.com;
   gongminmin@msn.com; bao@cad.zju.edu.cn; xinming@ustc.edu.cn;
   bainguo@microsoft.com
RI Zhou, Kun/ABF-4071-2020; wu, wenjian/AAO-8800-2021; Zhou,
   Kun/AAH-9290-2019; Han, Jian/ABZ-1060-2022; Wei, Li-Yi/F-4469-2011;
   Zhang, Xinming/H-6389-2019
OI Zhou, Kun/0000-0003-2320-3655; wu, wenjian/0000-0003-3752-6324; Han,
   Jian/0000-0002-0647-4050; Zhang, Xinming/0000-0002-8136-6834
CR Cabral B., 1993, P 20 ANN C COMP GRAP, P263, DOI DOI 10.1145/166117.166151
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Interrante V, 1998, IEEE COMPUT GRAPH, V18, P49, DOI 10.1109/38.689664
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   LEFEBVRE S, 2006, IN PRESS ACM T GRAPH
   MOUNT DM, 2005, ANN LIB APPROXIMATE
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Soler C, 2002, ACM T GRAPHIC, V21, P673, DOI 10.1145/566570.566635
   Stam J, 2003, ACM T GRAPHIC, V22, P724, DOI 10.1145/882262.882338
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Tong YY, 2003, ACM T GRAPHIC, V22, P445, DOI 10.1145/882262.882290
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Turk G, 2001, COMP GRAPH, P347, DOI 10.1145/383259.383297
   van Wijk JJ, 2002, ACM T GRAPHIC, V21, P745, DOI 10.1145/566570.566646
   WEI L.-Y., 2002, Order-independent texture synthesis
   Wei LY, 2001, COMP GRAPH, P355, DOI 10.1145/383259.383298
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Zhang W, 2002, INT J NONLINEAR SCI, V3, P295, DOI 10.1515/IJNSNS.2002.3.3-4.295
   Zhou K, 2005, IEEE T VIS COMPUT GR, V11, P519, DOI 10.1109/TVCG.2005.78
NR 20
TC 45
Z9 64
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 918
EP 925
DI 10.1007/s00371-006-0078-3
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Soon, A
   Lee, WS
AF Soon, Andrew
   Lee, Won-Sook
TI Shape-based detail-preserving exaggeration of extremely accurate 3D
   faces
SO VISUAL COMPUTER
LA English
DT Article
DE exaggeration; high resolution; 3D faces; morphable model; detail
   reconstruction
AB We present an approach to automatically exaggerate the distinctive features of extremely detailed 3D faces. These representations comprise several million triangles and capture skin detail down to the pores. Despite their high level of realism, their size makes visualization difficult and real-time mesh manipulation infeasible. The premise of our methodology is to first remove the detail to obtain low resolution shape information, then perform shape-based exaggeration on a low resolution model and finally reapply the detail onto the exaggeration to recover the original resolution. We also present the results of applying this methodology to a small set of faces.
C1 Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
   Univ Ottawa, Sch Informat Technol & Engn, Ottawa, ON K1N 6N5, Canada.
C3 Carleton University; University of Ottawa
RP Soon, A (corresponding author), Carleton Univ, Dept Syst & Comp Engn, 1125 Colonel Dr, Ottawa, ON K1S 5B6, Canada.
EM asoon@sce.carleton.ca
CR Blanz Volker., 1999, P 26 ANN C COMPUTER, P187, DOI DOI 10.1145/311535.311556
   Borshukov G., 2003, ACM SIGGRAPH Sketches and Applications, page, P1
   BRENNAN SE, 1985, LEONARDO, V18, P170, DOI 10.2307/1578048
   BUI TD, 2003, P 6 IASTED INT C COM, P19
   CHIANG PY, 2005, P AS C COMP VIS ACCV
   Cohen J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P119, DOI 10.1145/237170.237220
   COOK RL, 1984, P 11 ANN C COMP GRAP, P223, DOI DOI 10.1145/800031.808602
   Eck M., 1991, CAD COMPUTERGRAPHIK, V13, P109
   Fujiwara T, 2001, VSMM 2001: SEVENTH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA, PROCEEDINGS, P625, DOI 10.1109/VSMM.2001.969721
   Goto T, 2002, SIGNAL PROCESS-IMAGE, V17, P243, DOI 10.1016/S0923-5965(01)00021-2
   Hilton A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P246, DOI 10.1109/TDPVT.2002.1024069
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Lee WS, 2000, IMAGE VISION COMPUT, V18, P355, DOI 10.1016/S0262-8856(99)00057-8
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Loop C, 1987, THESIS U UTAH
   NOH JY, 2000, P ACM S VIRT REAL SO, P166
   Sun W, 2001, VISUAL COMPUT, V17, P457, DOI 10.1007/s003710100121
   TAYLOR J, 2002, P 1 INT WORKSH 3D VI, P70
   Zhang Y, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P518, DOI 10.1109/CGI.2004.1309257
NR 19
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 478
EP 492
DI 10.1007/s00371-006-0023-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300004
DA 2024-07-18
ER

PT J
AU Yang, RG
   Guinnip, D
   Wang, L
AF Yang, Ruigang
   Guinnip, David
   Wang, Liang
TI View-dependent textured splatting
SO VISUAL COMPUTER
LA English
DT Article
DE point rendering; picture/image generation; multi-texture; real-time
ID 3D VIDEO
AB We present a novel approach to render low resolution point clouds with multiple high resolution textures - the type of data typical from passive vision systems. The low precision, noisy, and sometimes incomplete nature of such data sets is not suitable for existing point-based rendering techniques that are designed to work with high precision and high density point clouds. Our new algorithm - view-dependent textured splatting (VDTS) - combines traditional splatting with a view-dependent texturing strategy to reduce rendering artifacts caused by imprecision or noise in the input data.
   VDTS requires no pre-processing of input data, addresses texture aliasing, and most importantly, processes texture visibility on-the-fly. The combination of these characteristics makes VDTS well-suited for interactive rendering of dynamic scenes. Towards this end, we present a real-time view acquisition and rendering system to demonstrate the effectiveness of VDTS. In addition, we show that VDTS can produce high quality rendering when the texture images are augmented with per-pixel depth. In this scenario, VDTS is a reasonable alternative for interactive rendering of large CG models.
C1 Univ Kentucky, Ctr Visualizat & Virtual Environm, Lexington, KY 40507 USA.
C3 University of Kentucky
RP Yang, RG (corresponding author), Univ Kentucky, Ctr Visualizat & Virtual Environm, Lexington, KY 40507 USA.
EM ryang@cs.uky.edu; dguinnip@cs.uky.edu; lwangd@cs.uky.edu
OI Yang, Ruigang/0000-0001-5296-6307
CR *ATI TECHN INC, 2003, SUPERBUFFER OPENGL
   BAKER HH, 2003, P 11 ACM INT C MULT, P470
   BOTSCH M, 2002, 13 EUR WORKSH REND, P53
   BUEHLER C, 2001, P SIGGRAPH 2001 LOS, P405
   Crawfis R. A., 1993, Proceedings Visualization '93. (Cat. No.93CH3354-8), P261, DOI 10.1109/VISUAL.1993.398877
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   DEBEVEC PE, 1998, 9 EUR REND WORKSH VI
   Gross M, 2003, ACM T GRAPHIC, V22, P819, DOI 10.1145/882262.882350
   JANG J, 2002, EUR IEEE VIS S 2002, P125
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy Marc, 1985, The use of points as a display primitive
   Matusik W, 2002, ACM T GRAPHIC, V21, P427, DOI 10.1145/566570.566599
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MUCKE EP, 1993, UIUCDCSR931836
   Mueller K, 1998, IEEE T VIS COMPUT GR, V4, P178, DOI 10.1109/2945.694987
   *NVIDIA, 2001, HARDW SHAD MAPP
   *NVIDIA CORP, 2005, GEFORCE 7800
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Ren L, 2002, COMPUT GRAPH FORUM, V21, P461, DOI 10.1111/1467-8659.00606
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Stamminger M, 2001, SPRING EUROGRAP, P151
   Waschbüsch M, 2005, VISUAL COMPUT, V21, P629, DOI 10.1007/s00371-005-0346-7
   Würmlin S, 2004, COMPUT GRAPH-UK, V28, P3, DOI 10.1016/j.cag.2003.10.015
   Yang RG, 2003, PROC CVPR IEEE, P211, DOI 10.1109/ISCS.2003.1239980
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 25
TC 6
Z9 7
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 456
EP 467
DI 10.1007/s00371-006-0015-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300002
DA 2024-07-18
ER

PT J
AU Murakami, K
   Tsuruno, R
   Genda, E
AF Murakami, Kyoko
   Tsuruno, Reiji
   Genda, Etsuo
TI Natural-looking strokes for drawing applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE NPR; stroke; drawing; multiple paper texture
AB This paper presents an algorithm for generating realistic drawing strokes in real-time that can take on the appearance of pastels, charcoals, or crayons. The similarity between the pigment deposit patterns on paper surfaces produced by pastel strokes and the shadows/shades on illuminated paper surfaces have been investigated. Multiple paper textures have been prepared and the paper surfaces have been ascertained by illumination from various directions to represent strokes in arbitrary directions. These textures have been processed as if they could be used as a height field, and pigments deposited on the paper have been calculated using the height field and tablet inputs.
C1 Kyushu Univ, Fac Design, Tsuruno Lab, Fukuoka 812, Japan.
C3 Kyushu University
RP Murakami, K (corresponding author), Kyushu Univ, Fac Design, Tsuruno Lab, Fukuoka 812, Japan.
EM kyoko@verygood.aid.design.kyushu-u.ac.jp; tsuruno@design.kyushu-u.ac.jp;
   genda@design.kyushu-u.ac.jp
CR TAKAYANAGI I, 1999, TECH REP IEICE, V99, P7
NR 1
TC 4
Z9 5
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 415
EP 423
DI 10.1007/s00371-006-0021-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500005
DA 2024-07-18
ER

PT J
AU Liu, YS
   Zhang, H
   Yong, JH
   Yu, PQ
   Sun, JG
AF Liu, YS
   Zhang, H
   Yong, JH
   Yu, PQ
   Sun, JG
TI Mesh blending
SO VISUAL COMPUTER
LA English
DT Article
DE mesh blending; smoothing; parameterization; rolling ball
ID SURFACE; CURVES
AB A new method for smoothly connecting different patches on triangle meshes with arbitrary connectivity, called mesh blending, is presented. A major feature of mesh blending is to move vertices of the blending region to a virtual blending surface by choosing an appropriate parameterization of those vertices. Once blending is completed, the parameterization optimization is performed to perfect the final meshes. Combining mesh blending with multiresolution techniques, an effective blending technique for meshes is obtained. Our method has several advantages: (1) the user can intuitively control the blending result using different blending radii, (2) the shape of cross-section curves can be adjusted to flexibly design complex models, and (3) the resulting mesh has the same connectivity as the original mesh. In this paper, some examples about smoothing, sharpening, and mesh editing show the efficiency of the method.
C1 Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   Beijing Jiaotong Univ, Sch Comp Informat & Technol, Beijing, Peoples R China.
C3 Tsinghua University; Tsinghua University; Beijing Jiaotong University
RP Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM huizhang@mail.tsinghua.edu.cn; yongjh@mail.tsinghua.edu.cn;
   yupiqiang@tsinghua.org.cn
CR ALLIEZ P, 2002, P SIGGRAPH 02, P347
   Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   [Anonymous], 1994, Computational Geometry in C
   BARNHILL RE, 1993, GEOMETRIE MODELING, P1
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Biermann H, 2001, COMP GRAPH, P185, DOI 10.1145/383259.383280
   BOTSCH M, 2001, P EUR 01, P402
   CHOI BK, 1989, COMPUT AIDED DESIGN, V21, P213, DOI 10.1016/0010-4485(89)90046-8
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Desbrun M, 2000, PROC GRAPH INTERF, P145
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hartmann E, 2001, VISUAL COMPUT, V17, P1, DOI 10.1007/PL00013398
   HERMANN T, 1992, CURVES SURFACES COMP, V3, P204
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   KOBBELT LP, 2000, P EUROGRAPHICS 00, P479
   Levin A, 1999, COMPUT AIDED GEOM D, V16, P345, DOI 10.1016/S0167-8396(98)00051-X
   LEVY B, 2003, P SIGGRAPH 2003 COMP, P364
   Lukacs G, 1998, COMPUT AIDED GEOM D, V15, P585, DOI 10.1016/S0167-8396(98)00006-5
   Maekawa T, 1999, COMPUT AIDED DESIGN, V31, P165, DOI 10.1016/S0010-4485(99)00013-5
   Moller T., 1997, J. Graph. Tools, V2, P25, DOI [DOI 10.1080/10867651.1997.10487472, 10.1080/10867651.1997.10487472]
   MUSETH K, 2002, P SIGGRAPH, P330
   Nomura N, 2001, P SOC PHOTO-OPT INS, V4476, P34, DOI 10.1117/12.447285
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   Park H, 2000, COMPUT AIDED DESIGN, V32, P237, DOI 10.1016/S0010-4485(99)00088-3
   Piegl L., 1997, The Nurbs Book, Vsecond
   ROSSIGNAC JR, 1984, P COMP MECH ENG 3 JU, P65
   Schneider R, 2001, COMPUT AIDED GEOM D, V18, P359, DOI 10.1016/S0167-8396(01)00036-X
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   VARADY T, 1989, MATH SURFACES, V3, P171
   VIDA J, 1994, COMPUT AIDED DESIGN, V26, P341, DOI 10.1016/0010-4485(94)90023-X
   Wu TR, 2000, COMPUT AIDED GEOM D, V17, P759, DOI 10.1016/S0167-8396(00)00023-6
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
NR 37
TC 8
Z9 13
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2005
VL 21
IS 11
BP 915
EP 927
DI 10.1007/s00371-005-0306-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 974RH
UT WOS:000232608600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, YQ
   Zhu, HB
   Liu, XH
   Wu, EH
AF Liu, YQ
   Zhu, HB
   Liu, XH
   Wu, EH
TI Real-time simulation of physically based on-surface flow
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE on-surface flow; Navier-Stokes equations; wetting; real time
AB Although many papers have been published in the field of fluid simulation, little attention has been paid to on-surface flow involving wetting and stain transportation as well as erosion and deposition phenomena. In this paper, we introduce nonzero divergence in the mass equation of Navier-Stokes equations to simulate water penetration from on-surface flow into substrate material. Also, the volume of fluid method is adopted to track the free surface. With a computation of the actual amount of absorbed water we render the wetting effects with fully dry and fully wet texture images simultaneously. Using our model, the on-surface flow that accompanies water absorption can be simulated realistically in real time with OpenGL preview rendering. Experimental results illustrate that our model can be widely applied to solve various problems related to on-surface flow.
C1 Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
   Chinese Acad Sci, Inst Software, Comp Sci Lab, Beijing, Peoples R China.
   Chinese Acad Sci, Grad Sch, Beijing 100864, Peoples R China.
C3 University of Macau; Chinese Academy of Sciences; Institute of Software,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
EM lyq@ios.ac.cn; zhuhb@ios.ac.cn; lxh@ios.ac.cn; ehwu@umac.mo
CR Baxter W, 2004, COMPUT ANIMAT VIRT W, V15, P433, DOI 10.1002/cav.47
   CHU NSH, 2005, IN PRESS P SIGGRAPH, V24
   Curtis C. J., 1997, Proceedings of the 24th annual conference on Computer graphics and interactive techniques, P421
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Dorsey Julie., 1996, Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, P411, DOI [10.1145/237170. 237280, DOI 10.1145/237170.237280]
   Enright D, 2002, ACM T GRAPHIC, V21, P736, DOI [10.1145/566570.566581, 10.1145/566570.566645]
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   HIRT CW, 2000, SIMULATING WETTING D
   HONG JM, IN PRESS P ACM SIGGR
   Jensen HW, 1999, SPRING EUROGRAP, P273
   JONSSON M, 2002, SIGRAD
   Stam J, 2003, ACM T GRAPHIC, V22, P724, DOI 10.1145/882262.882338
   Stam J., 1999, The Proceedings of SIGGRAPH, P121, DOI DOI 10.1145/311535.311548
   Wang HM, 2005, ACM T GRAPHIC, V24, P921, DOI 10.1145/1073204.1073284
   Wu EH, 2004, COMPUT ANIMAT VIRT W, V15, P139, DOI 10.1002/cav.16
NR 16
TC 13
Z9 16
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 727
EP 734
DI 10.1007/s00371-005-0314-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400025
DA 2024-07-18
ER

PT J
AU Wu, JH
   Kobbelt, L
AF Wu, JH
   Kobbelt, L
TI Efficient spectral watermarking of large meshes with orthogonal basis
   functions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE digital watermarking; large-mesh watermarking; radial basis functions;
   spectral decomposition
ID SURFACES
AB Allowing for copyright protection and ownership assertion, digital watermarking techniques, which have been successfully applied to classical media types like audio, images, and video, have recently been adapted for the newly emerged multimedia data type of 3D geometry models. In particular, the widely used spread-spectrum methods can be generalized for 3D datasets by transforming the original model to a frequency domain and perturbing the coefficients of the most dominant basis functions. Previous approaches employing this kind of spectral watermarking are mainly based on multiresolution mesh analysis, wavelet domain transformation, or spectral mesh analysis. Though they already exhibit good resistance to many types of real-world attacks, they are often far too slow to cope with very large meshes due to their complicated numerical computations. In this paper, we present a novel spectral watermarking scheme using new orthogonal basis functions based on radial basis functions. With our proposed fast basis function orthogonalization, while observing a persistence with respect to various attacks that is similar to that of other related approaches, our scheme runs faster by two orders of magnitude and thus can efficiently watermark very large models.
C1 Rhein Westfal TH Aachen, Comp Graph Grp, Aachen, Germany.
C3 RWTH Aachen University
RP Rhein Westfal TH Aachen, Comp Graph Grp, Aachen, Germany.
EM wu@cs.rwth-aachen.de; kobbelt@cs.rwth-aachen.de
OI Kobbelt, Leif/0000-0002-7880-9470
CR ALEKSEYEV S, 2003, THESIS RWTH AACHEN U
   Ben-Chen M, 2005, ACM T GRAPHIC, V24, P60, DOI 10.1145/1037957.1037961
   Benedens O, 1999, IEEE COMPUT GRAPH, V19, P46, DOI 10.1109/38.736468
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Botsch M, 2004, ACM T GRAPHIC, V23, P630, DOI 10.1145/1015706.1015772
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Cotting D, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P233, DOI 10.1109/SMI.2004.1314510
   Cox I., 2001, Digital Watermarking
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Gotsman C, 2002, TUTORIALS ON MULTIRESOLUTION IN GEOMETRIC MODELLING, P319
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Harte T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P661, DOI 10.1109/ICIP.2002.1039057
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   KATZENBEISSER S, 2000, INFORMATION HIDING T
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohtake Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P31, DOI 10.1109/SMI.2004.1314491
   Ohtake Y, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P153
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   Press W. H., 1995, Numerical Recipes in C: The Art of Scientific Computing
   Sorkine O, 2005, IEEE T VIS COMPUT GR, V11, P171, DOI 10.1109/TVCG.2005.33
   SORKINE O, 2003, P EUR ACM SIGGRAPH S, P42
   TAUBIN G, 1995, COMPUTER GRAPHICS, V29, P351
   Tobor I, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P19, DOI 10.1109/SMI.2004.1314490
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Turk G., 1999, VARIATIONAL IMPLICIT
   UCCHEDDU F, 2004, P 2004 MULT SEC WORK, P143
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
   ZAYER R, 2005, P EUR 2005
NR 34
TC 33
Z9 40
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 848
EP 857
DI 10.1007/s00371-005-0311-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400038
DA 2024-07-18
ER

PT J
AU Arvo, J
   Hirvikorpi, M
AF Arvo, J
   Hirvikorpi, M
TI Compressed shadow maps
SO VISUAL COMPUTER
LA English
DT Article
DE rendering; shadow algorithms
AB Shadow mapping has been subject to extensive investigation, but previous shadow map algorithms cannot usually generate high-quality shadows with a small memory footprint. In this paper, we present compressed shadow maps as a solution to this problem. A compressed shadow map reduces memory consumption by representing lit surfaces with endpoints of intermediate line segments as opposed to the conventional array-based pixel structures. Compressed shadow maps are only discretized in the vertical direction while the horizontal direction is represented by floating-point accuracy. The compression also helps with the shadow map self-shadowing problems. We compare our algorithm against all of the most popular shadow map algorithms and show, on average, order of magnitude improvements in storage requirements in our test scenes. The algorithm is simple to implement, can be added easily to existing software renderers, and lets us use graphics hardware for shadow visualization.
C1 Univ Turku, Turku Ctr Comp Sci, FIN-20520 Turku, Finland.
   Univ Turku, Dept Informat Technol, FIN-20520 Turku, Finland.
C3 University of Turku; University of Turku
RP Univ Turku, Turku Ctr Comp Sci, Lemminkaisenkatu 14 A, FIN-20520 Turku, Finland.
CR Akenine-Moller T., 2019, Real-time rendering
   [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   BRABEC S, 2002, J GRAPHICS TOOLS, V7, P9
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Fernando R, 2001, COMP GRAPH, P387, DOI 10.1145/383259.383302
   GRANT C, 1992, THESIS U CALIFORNIA
   LANE JM, 1980, COMMUN ACM, V23, P23, DOI 10.1145/358808.358815
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   Reeves W. T., 1987, SIGGRAPH Comput. Graph., P283
   Sen P, 2003, ACM T GRAPHIC, V22, P521, DOI 10.1145/882262.882301
   Stamminger M, 2002, ACM T GRAPHIC, V21, P557, DOI 10.1145/566570.566616
   Tadamura K, 2001, VISUAL COMPUT, V17, P76, DOI 10.1007/PL00013400
   TANAKA T, 1994, COMPUT GRAPH FORUM, V13, pC487, DOI 10.1111/1467-8659.1330467
   White C. A., 1881, REPORT CARBONIFERO S, V3, P1
   WOO A, 1992, SHADOW DEPTH MAP REV, V3
NR 15
TC 8
Z9 8
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2005
VL 21
IS 3
BP 125
EP 138
DI 10.1007/s00371-004-0276-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 921UV
UT WOS:000228791700001
DA 2024-07-18
ER

PT J
AU Wu, ZK
   Lin, F
   Soon, SH
AF Wu, ZK
   Lin, F
   Soon, SH
TI Tunnel-free voxelisation of rational Bezier surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE algorithm; free-form modeling; rational Bezier; volume graphics;
   voxelisation
ID SCAN-CONVERSION; VOXELIZATION; POLYGONS; REGIONS; CURVES
AB To synthesize natural and artificial objects into a hybrid graphics scene represented by a set of voxels, voxelisation of geometric models is necessary. Rational parametric surfaces have been widely used in the representation of free-form surfaces. Voxelisation of these surfaces is therefore of great importance in the development of a voxel-based modeling system. A key issue is to develop a tunnel-free voxelisation algorithm for these continuous surfaces. In this paper, we propose such an algorithm for a rational Bezier surface. We derive the bound of the parametric steps to ensure that the voxelised rational Bezier surface is, by our algorithm, 6-tunnel-free, and we give the mathematical proof of this property. For efficient computation, we employ the forward difference technique in homogeneous form in the implementation of the algorithm. For more general applications, we show that voxelisation of a NURBS surface can be realised by first converting it into a piecewise rational Bezier surface and then voxelising each of the rational Bezier surfaces. We indicate the advantages carrying out this procedure.
C1 Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Comp Engn, Nanyang Ave, Singapore 639798, Singapore.
EM asflin@nut.edu.sg
RI Seah, Hock Soon/AAK-9900-2020
OI Seah, Hock Soon/0000-0003-2699-7147
CR Andres E, 1997, COMPUT GRAPH FORUM, V16, pC3, DOI 10.1111/1467-8659.00137
   [Anonymous], TOPOLOGICAL ALGORITH
   AVILA R, 1994, IEEE VISUAL, P31, DOI 10.1109/VISUAL.1994.346340
   CHANG SL, 1989, COMPUT GRAPH, V23, P157
   COHEN D, 1994, VISUAL COMPUT, V11, P27, DOI 10.1007/BF01900824
   COHEN D, 1991, VOLUME VISUALIZATION, P280
   CohenOr D, 1997, IEEE COMPUT GRAPH, V17, P80, DOI 10.1109/38.626973
   COHENOR D, 1995, GRAPH MODEL IM PROC, V57, P453, DOI 10.1006/gmip.1995.1039
   Farin GeraldE., 1991, NURBS for Curve and Surface Design, DOI [10.5555/531858, DOI 10.5555/531858]
   Feng L, 1998, COMPUT GRAPH-UK, V22, P641, DOI 10.1016/S0097-8493(98)00073-9
   Floater M. S., 1992, Computer-Aided Geometric Design, V9, P161, DOI 10.1016/0167-8396(92)90014-G
   HERSCH RD, 1986, IEEE COMPUT GRAPH, V6, P61, DOI 10.1109/MCG.1986.276819
   Huang LX, 1998, ACTA PHARMACOL SIN, V19, P20
   Jones MW, 1996, COMPUT GRAPH FORUM, V15, P311, DOI 10.1111/1467-8659.1550311
   KAUFMAN A, 1993, COMPUTER, V26, P51, DOI 10.1109/MC.1993.274942
   KAUFMAN A, 1987, IEEE COMPUT GRAPH, V0021, P00171
   KAUFMAN A, 1987, EUROGRAPHICS 87, P197
   Kumar S, 1996, IEEE T VIS COMPUT GR, V2, P323, DOI 10.1109/2945.556501
   LIN F, 1992, COMPUT GRAPH, V16, P79, DOI 10.1016/0097-8493(92)90074-6
   PAVLIDIS T, 1985, IEEE COMPUT GRAPH, V5, P47, DOI 10.1109/MCG.1985.276499
   PIEGLE L, 1997, NURBS BOOK
   ROCKWOOD A, 1987, IEEE COMPUT GRAPH, V7, P15, DOI 10.1109/MCG.1987.276916
   SAITO T, 1995, COMPUT AIDED GEOM D, V12, P417, DOI 10.1016/0167-8396(94)00023-L
   Sederberg T. W., 1989, Computer Graphics, V23, P147, DOI 10.1145/74334.74348
   STOLTE N, 1997, VISUALIZATION MODELI, P191
   TAUBIN G, 1994, IEEE COMPUT GRAPH, V14, P14, DOI 10.1109/38.267467
   VANWYK CJ, 1984, COMPUT VISION GRAPH, V25, P383, DOI 10.1016/0734-189X(84)90202-0
   Wang GJ, 1997, COMPUT AIDED GEOM D, V14, P377, DOI 10.1016/S0167-8396(96)00033-7
   Wu ZK, 2000, VOLUME GRAPHICS, P159
   Yao CF, 1997, COMPUT GRAPH FORUM, V16, P101, DOI 10.1111/1467-8659.00126
   ZHOU JW, 2001, INT WORKSH VOL GRAPH, P63
NR 31
TC 3
Z9 3
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2003
VL 19
IS 7-8
BP 505
EP 512
DI 10.1007/s00371-003-0215-1
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 749YL
UT WOS:000186957600007
DA 2024-07-18
ER

PT J
AU Bosse, M
   Rikoski, R
   Leonard, J
   Teller, S
AF Bosse, M
   Rikoski, R
   Leonard, J
   Teller, S
TI Vanishing points and three-dimensional lines from omni-directional video
SO VISUAL COMPUTER
LA English
DT Article
DE omni-directional video; vanishing points; structure from motion; visual
   navigation; image line tracking
ID SIMULTANEOUS LOCALIZATION; ALGORITHM; MOTION
AB This paper describes a system for structure from motion using vanishing points and three-dimensional lines extracted from omni-directional video sequences. To track lines, we use a novel dynamic programming approach to improve ambiguity resolution, and we use delayed states to aid in the initialization of landmarks. By reobserving vanishing points we get direct measurements of the robot's three-dimensional attitude that are independent of its position. Using vanishing points simplifies the representation since parallel lines share the same direction states. We show the performance of the system in various indoor and outdoor environments and include comparisons with independent two-dimensional reference maps for each experiment.
C1 MIT, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM ifni@mit.edu; rikoski@mit.edu; jleonard@mit.edu; seth@mit.edu
RI Bosse, Michael C/B-7719-2011
OI /0000-0002-8863-6550
CR [Anonymous], 1990, Autonomous Robot Vehicles, DOI 10.1007/978-1-4613-8997-2{}14
   [Anonymous], INT S ROB RES ISRR
   ANTONE M, 2001, THESIS MIT
   Antone ME, 2000, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.2000.854809
   Bar-Shalom Y., 1988, TRACKING DATA ASS
   BOSSE MC, 2003, IN PRESS P IEEE INT
   Castellanos JA, 1999, IEEE T ROBOTIC AUTOM, V15, P948, DOI 10.1109/70.795798
   Chiuso A, 2002, IEEE T PATTERN ANAL, V24, P523, DOI 10.1109/34.993559
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   DAVISON AJ, 1998, THESIS U OXFORD
   DISSANAYAKE MWM, 1999, 6 INT S EXP ROB MARC, P265
   Faugeras O., 2001, The geometry of multiple images: the laws that govern the formation of multiple images of a scene and some of their applications
   Guivant JE, 2001, IEEE T ROBOTIC AUTOM, V17, P242, DOI 10.1109/70.938382
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Leonard JJ, 2002, INT J ROBOT RES, V21, P943, DOI 10.1177/0278364902021010889
   LOZANOPEREZ T, 1989, AUTONOMOUS ROBOT VEH
   Marr D., 1982, Vision
   MCLAUCHLAN PF, 2000, INT C COMP VIS PATT, V2, P738
   MOUTARLIER P, 1989, 1 INT S EXP ROB MONT
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   TAYLOR CJ, 1995, IEEE T PATTERN ANAL, V17, P1021, DOI 10.1109/34.473228
   TAYLOR CJ, 2000, 2 WORKSH 3D STRUCT M, P187
   TELLER S, 2001, INT C COMP VIS PATT, V1, P813
   Thrun S, 2001, INT J ROBOT RES, V20, P335, DOI 10.1177/02783640122067435
   TRIGGS B, 1999, INT WORKSH VIS BERL
NR 25
TC 18
Z9 21
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2003
VL 19
IS 6
BP 417
EP 430
DI 10.1007/s00371-003-0205-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745PC
UT WOS:000186696200006
DA 2024-07-18
ER

PT J
AU Wang, ZC
   Yang, HJ
AF Wang, Zhicheng
   Yang, Huijun
TI Local entropy-based feature-preserving simplification and evaluation for
   large field point cloud
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Large-scale point cloud; Farmland scene data; Point cloud
   simplification; Information entropy
AB With the development of point cloud-based telemetry technology in recent years, the point cloud data of large field scenes acquired by various sensors have been applied to farmland boundary division, crop growth monitor, area surveying, etc. However, the large field point cloud will cost huge amounts of computational resources in the following transmission, storage and processing, which make it more important to simplify the field point cloud appropriately. In light of limitation of existing algorithms in point cloud simplification of large field scenes, we propose a novel feature-preserved simplification algorithm for large field point cloud data. By introducing the average local entropy as the threshold for area division, our algorithm effectively solves the problem of fuzzy boundary division, as well as preserving the field features and reducing the simplification errors. In view of the problem that the evaluation of current simplification algorithm is mainly focused on qualitative assessment, a quantitative evaluation index for the point cloud simplification is proposed by employing the point-average local entropy, which takes both model retention and simplification efficiency into account. Finally, comparable experiments are performed on four sets of point clouds. The results show that, compared with the statistics of six typical algorithms, the proposed algorithm increases the local entropy by 0.029%, 0.146% and 0.088% on our datasets, and increases 0.029% on the public dataset. The method accurately evaluates the simplification effect. Additionally, the surface area change rate is also used to further evaluate the performance of proposed algorithm, and the quantitative evaluation index is lower than others, which verify the advantages of proposed algorithm in feature protection and large field simplification.
C1 [Wang, Zhicheng; Yang, Huijun] Northwest A&F Univ, Coll Informat Engn, Xianyang 712100, Shaanxi, Peoples R China.
C3 Northwest A&F University - China
RP Yang, HJ (corresponding author), Northwest A&F Univ, Coll Informat Engn, Xianyang 712100, Shaanxi, Peoples R China.
EM wangzhicheng@nwafu.edu.cn; yhj740225@nwafu.edu.cn
RI Wang, Zhigang/O-1984-2013
OI Wang, Zhigang/0000-0002-3028-5196
FU Key Research and Development Program of Shaanxi province
FX No Statement Available
CR [陈达枭 Chen Daxiao], 2016, [计算机应用研究, Application Research of Computers], V33, P2841
   Chen ZW., 2013, Acta Optica Sinica, V33, P161
   Cheng M., 2020, Trans. Chin. Soc. Agric. Mach, V51, P314
   Dong JM., 2020, Mod. Electron. Tech, V43, p20
   DU X., 2012, Comput. Eng. Appl, V48, p182
   Gao HW., 2014, Mach. Electron, V06, P22
   Ge Y.K., 2012, Research on Data Pre-Processing Technology of Scattered Point Cloud Based on Curvatures Feature
   Ge Yuan-kun, 2012, Application Research of Computers, V29, P1997, DOI 10.3969/j.issn.1001-3695.2012.05.106
   Guo Y, 2018, VISUAL COMPUT, V34, P1325, DOI 10.1007/s00371-017-1416-3
   Han HY, 2015, OPTIK, V126, P2157, DOI 10.1016/j.ijleo.2015.05.092
   Hu QY, 2021, PROC CVPR IEEE, P4975, DOI 10.1109/CVPR46437.2021.00494
   Huang XM., 2018, J JISHOU UNIV, V39, P19, DOI DOI 10.13438/j.cnki.jdzk.2018.05.005
   Levin D, 2004, MATH VISUAL, P37
   Li RB., 2020, Urban Geotech. Invest. Surv, V02, P70
   Li RZ, 2018, LASER OPTOELECTRON P, V55, DOI 10.3788/LOP55.011008
   Liu De-ping, 2008, Journal of Xidian University, V35, P334
   Liu XD, 2012, VISUAL COMPUT, V28, P613, DOI 10.1007/s00371-012-0709-9
   Lu AM., 2020, Eng. Technol. Res, V5, P231
   Lu JH., 2018, Bull. Surv. Mapp, V12, P101
   Ma HJ., 2019, World Nonferr. Met, V12, P231
   Ma WF., 2015, Eng. Surv. Mapp, V24, p13
   Mahdaoui A, 2020, ADV MULTIMED, V2020, DOI 10.1155/2020/8825205
   Sankaran S, 2015, COMPUT ELECTRON AGR, V118, P372, DOI 10.1016/j.compag.2015.09.001
   Sun W, 2001, COMPUT AIDED DESIGN, V33, P183, DOI 10.1016/S0010-4485(00)00088-9
   Tian YZ., 2021, Ind. Control Comput, V34, p80
   Wang CF, 2019, LASER OPTOELECTRON P, V56, DOI 10.3788/LOP56.111004
   Wang GL, 2021, MEAS SCI TECHNOL, V32, DOI 10.1088/1361-6501/abd497
   Wang X., 2022, Electron. Measur. Technol, V45, P119, DOI [10.19651/j.cnki.emt.2209934, DOI 10.19651/J.CNKI.EMT.2209934]
   Weir DJ, 1996, P I MECH ENG B-J ENG, V210, P147, DOI 10.1243/PIME_PROC_1996_210_100_02
   Wu J.J., 2004, Research of Point-Based Techniques on Unorganized Point Cloud
   [吴禄慎 Wu Lushen], 2016, [计算机应用与软件, Computer Applications and Software], V33, P42
   Xijiang C, 2015, China J. Lasers, V42, DOI 10.3788/CJL201542.0814003
   Xuan W, 2018, J INDIAN SOC REMOTE, V46, P581, DOI 10.1007/s12524-017-0730-6
   [杨玲 YANG Ling], 2009, [地理与地理信息科学, Geography and Geo-information Science], V25, P25
   Yang R.H., 2011, Research on Point Cloud Angular Resolution and Processing Model of Terrestrial Laser Scanning
   [叶珉吕 Ye Minlu], 2015, [大地测量与地球动力学, Journal of Geodesy and Geodynamics], V35, P424
   Zhao CJ., 2021, China Agric. Abstr.-Agric. Eng, V33, P4
   Zhao FQ., 2022, Laser Optoelectron. Progress, V59
   Zhao PC, 2016, INT GEOSCI REMOTE SE, P5581, DOI 10.1109/IGARSS.2016.7730457
   [周波 Zhou Bo], 2008, [现代制造工程, Modern Manufacturing Engineering], P64
   Zhou L., 2004, Mie China, V33, P102
NR 41
TC 0
Z9 0
U1 8
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 29
PY 2023
DI 10.1007/s00371-023-03194-1
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DM5K2
UT WOS:001132471200002
DA 2024-07-18
ER

PT J
AU Liu, GH
   Ren, JW
AF Liu, Guohua
   Ren, Jiawei
TI Feature purification fusion structure for fabric defect detection
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect; Defect detection; Deep learning; Small target; Feature
   fusion
AB Fabric defect detection is an important part of the textile industry to ensure product quality. To solve the problems such as the difficulty of detecting small defects and the coexistence of multi-scale defects in fabric defect detection, a fabric defect detection method based on the feature purification fusion structure is proposed in this paper. Specifically, we improve the feature extraction network to enhance the network's ability to focus on small defective features and effectively reduce the model parameters. The existing methods use direct fusion between multi-level feature maps, which will lead to feature confusion. Therefore, we propose the feature purification fusion structure (FPF), which includes the semantic information supplementation strategy (SIS) and the detail information supplementation strategy (DIS). SIS extracts valid information from the deep feature map and supplements it to the shallow feature map, weakening feature information irrelevant to the shallow feature map detection task. DIS adaptively supplements the feature information required by the deep feature map detection task from the shallow feature map. FPF improves the ability of the network to detect small defects and effectively mitigates the aliasing effect generated during feature fusion. The experimental results show that compared to the baseline model YOLOv5s algorithm, our model achieved a 6.8% improvement in detection accuracy, with an average inference frame rate of 37.6 FPS, demonstrating better detection performance in fabric defect detection. Furthermore, extending this model to aluminum profile defect datasets also demonstrates strong performance.
C1 [Liu, Guohua; Ren, Jiawei] Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.
   [Liu, Guohua] Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
C3 Tiangong University; Tiangong University
RP Liu, GH (corresponding author), Tiangong Univ, Sch Mech Engn, Tianjin 300387, Peoples R China.; Liu, GH (corresponding author), Tiangong Univ, Adv Mechatron Equipment Technol Tianjin Major Lab, Tianjin 300387, Peoples R China.
EM guohualiumail@163.com
RI King, William/HRA-2631-2023
OI King, William/0000-0003-4104-9647
FU This work is supported by the science and technology program project of
   Tianjin. (No. 21YFFCYS00080).; science and technology program project of
   Tianjin;  [21YFFCYS00080]
FX This work is supported by the science and technology program project of
   Tianjin. (No. 21YFFCYS00080).
CR Bao XY, 2022, VISUAL COMPUT, V38, P2707, DOI 10.1007/s00371-021-02148-9
   Cao YY, 2023, COMPUT ELECTRON AGR, V206, DOI 10.1016/j.compag.2023.107696
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Fu HX, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13040623
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   He Z., 2022, IEEE T GEOSCI ELECT, V61, P1
   Jia Z, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22249662
   Jiaxu L., 2021, ARXIV
   Jing J., 2016, Int. J. Multimed. Ubiquitous Eng, V11, P93, DOI [DOI 10.14257/IJMUE.2016.11.6.09, https://doi.org/10.14257/ijmue.2016.11.6.09]
   Jing JF, 2020, J ENG FIBER FABR, V15, DOI 10.1177/1558925020908268
   Jocher G. J., 2021, Ultralytics/Yolov5: V6. 0-YOLOv5nNanomodels, Roboflow Integration, TensorFlow Export, OpenCV DNN Support
   Karlekar VV, 2015, 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION ICCUBEA 2015, P712, DOI 10.1109/ICCUBEA.2015.145
   Lin GJ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23010097
   Liu GH, 2022, VISUAL COMPUT, V38, P639, DOI 10.1007/s00371-020-02040-y
   Liu Q, 2022, IEEE ACCESS, V10, P4284, DOI 10.1109/ACCESS.2021.3140118
   Luo X, 2023, TEXT RES J, V93, P2342, DOI 10.1177/00405175221143742
   Meier R., 1999, TEXT MON, V34, P36
   Nawaz M, 2023, VISUAL COMPUT, V39, P6323, DOI 10.1007/s00371-022-02732-7
   Qin WN, 2023, VISUAL COMPUT, V39, P5971, DOI 10.1007/s00371-022-02706-9
   Qu JS, 2020, IEEE ACCESS, V8, P82832, DOI 10.1109/ACCESS.2020.2991439
   Runtian Qin, 2021, Journal of Physics: Conference Series, V1907, DOI 10.1088/1742-6596/1907/1/012057
   Sun P, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE 2020), P2191, DOI 10.1109/ICMCCE51767.2020.00475
   Tian C., 2020, SMART DIAGNOSIS CLOT
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Yang MM, 2022, VISUAL COMPUT, V38, P2661, DOI 10.1007/s00371-021-02144-z
   Yazhao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13346, DOI 10.1109/CVPR42600.2020.01336
   Ying Wang, 2021, Journal of Physics: Conference Series, DOI 10.1088/1742-6596/2010/1/012191
   Yue X, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12136823
   Zhang SH, 2022, FIBER POLYM, V23, P3655, DOI 10.1007/s12221-022-4241-x
   Zhao BJ, 2019, J SYST ENG ELECTRON, V30, P1, DOI 10.21629/JSEE.2019.01.01
   Zhou S, 2023, INT J CLOTH SCI TECH, V35, P88, DOI 10.1108/IJCST-11-2021-0165
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
NR 34
TC 0
Z9 0
U1 21
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3825
EP 3842
DI 10.1007/s00371-023-03066-8
EA SEP 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001058970800003
DA 2024-07-18
ER

PT J
AU Lan, SK
   Li, J
   Hu, SQ
   Fan, HC
   Pan, ZB
AF Lan, Shaokun
   Li, Jie
   Hu, Shiqi
   Fan, Hongcheng
   Pan, Zhibin
TI A neighbourhood feature-based local binary pattern for texture
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Local binary pattern (LBP); Feature extraction; Neighbourhood feature
   (NF) pattern; Neighbourhood feature-based local binary pattern (NF-LBP);
   Texture classification
AB The CNN framework has gained widespread attention in texture feature analysis; however, handcrafted features still remain advantageous if computational cost needs to take precedence and in cases where textures are easily extracted with few intra-class variation. Among the handcrafted features, the local binary pattern (LBP) is extensively applied for analysing texture due to its robustness and low computational complexity. However, in local difference vector, it only utilizes the sign component, resulting in unsatisfactory classification capability. To improve classification performance, most LBP variants employ multi-feature fusion. Nevertheless, this can lead to redundant and low-discriminative sub-features and high computational complexity. To address these issues, we propose the neighbourhood feature-based local binary pattern (NF-LBP). Inspired by gradient's definition, we extract the neighbourhood feature in a local region by simply using the first-order difference and 2-norm. Next, we introduce the neighbourhood feature (NF) pattern to describe intensity changes in the neighbourhood. Finally, we combine the NF pattern with the local sign component and the centre pixel component to create the NF-LBP descriptor. This approach provides better complementary texture information to traditional local sign pattern and is less sensitive to noise. Additionally, we use an adaptive local threshold in the encoding scheme. Our experimental results of classification accuracy and F1 score on five texture databases demonstrate that our proposed NF-LBP method attains outstanding texture classification performance, outperforming existing state-of-the-art approaches. Furthermore, extensive experimental results reveal that NF-LBP is strongly robust to Gaussian noise and salt-and-pepper noise.
C1 [Lan, Shaokun; Li, Jie; Pan, Zhibin] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Hu, Shiqi] AVIC Xian Flight Automatic Control Res Inst, Xian 710076, Peoples R China.
   [Fan, Hongcheng] Air Force Engn Univ, Inst Informat & Nav, Xian 710077, Peoples R China.
   [Pan, Zhibin] Chinese Acad Sci, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
C3 Xi'an Jiaotong University; Air Force Engineering University; State Key
   Laboratory of Transient Optics & Photonics; Chinese Academy of Sciences
RP Pan, ZB (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.; Pan, ZB (corresponding author), Chinese Acad Sci, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
EM l261985436@stu.xjtu.edu.cn; jielixjtu@mail.xjtu.edu.cn;
   565489693@qq.com; hongcheng_fan@126.com; zbpan@mail.xjtu.edu.cn
OI Hu, Shiqi/0000-0003-1838-7517
FU National Natural Science Foundation of China [62275211, 61675161,
   U1903213]; Open Research Fund of State Key Laboratory of Transient
   Opticsand Photonics;  [SKLST202212]
FX This work is supported in part by the National Natural Science
   Foundation of China (Grant No. 62275211, 61675161, U1903213) and the
   Open Research Fund of State Key Laboratory of Transient Opticsand
   Photonics (Grant No. SKLST202212).
CR Al-wajih E, 2023, KNOWL-BASED SYST, V259, DOI 10.1016/j.knosys.2022.110079
   Alpaslan N, 2020, IEEE ACCESS, V8, P54415, DOI 10.1109/ACCESS.2020.2981720
   Bai RY, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.107240
   Bansal M, 2021, ARCH COMPUT METHOD E, V28, P1147, DOI 10.1007/s11831-020-09409-1
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bello-Cerezo R, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9040738
   Chen C, 2016, SIGNAL IMAGE VIDEO P, V10, P745, DOI 10.1007/s11760-015-0804-2
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Elahi GMME, 2020, COMPUT MED IMAG GRAP, V79, DOI 10.1016/j.compmedimag.2019.101659
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Gyimah NK, 2021, IEEE SYS MAN CYBERN, P1927, DOI 10.1109/SMC52423.2021.9659140
   HAZGUI M, 2022, VISUAL COMPUT, P1
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Kang LW, 2011, IEEE T MULTIMEDIA, V13, P1019, DOI 10.1109/TMM.2011.2159197
   Karanwal S, 2021, DIGIT SIGNAL PROCESS, V110, DOI 10.1016/j.dsp.2020.102948
   Karanwal S, 2021, PATTERN ANAL APPL, V24, P741, DOI 10.1007/s10044-020-00948-8
   Khan UA, 2021, MULTIMED TOOLS APPL, V80, P26911, DOI 10.1007/s11042-021-10530-x
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan SK, 2023, EXPERT SYST APPL, V221, DOI 10.1016/j.eswa.2023.119763
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li J, 2020, NEUROCOMPUTING, V411, P340, DOI 10.1016/j.neucom.2020.06.014
   Liu L, 2014, IEEE T IMAGE PROCESS, V23, P3071, DOI 10.1109/TIP.2014.2325777
   Liu QW, 2024, VEHICLE SYST DYN, V62, P1565, DOI 10.1080/00423114.2023.2276761
   Ludwig O, 2009, 2009 12TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC 2009), P432
   Melekoodappattu JG, 2022, J AMB INTEL HUM COMP, DOI 10.1007/s12652-022-03713-3
   Mukherjee M., 2021, 2021 IEEE Power Energy Society General Meeting (PESGM), P1
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, INT C PATT RECOG, P701, DOI 10.1109/ICPR.2002.1044854
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan ZB, 2021, EXPERT SYST APPL, V180, DOI 10.1016/j.eswa.2021.115123
   Pan ZB, 2020, MULTIMED TOOLS APPL, V79, P5477, DOI 10.1007/s11042-019-08205-9
   Pan ZB, 2017, EXPERT SYST APPL, V88, P238, DOI 10.1016/j.eswa.2017.07.007
   Peng JC, 2023, DIGIT SIGNAL PROCESS, V133, DOI 10.1016/j.dsp.2022.103844
   Shahid AR, 2023, KNOWL-BASED SYST, V269, DOI 10.1016/j.knosys.2023.110451
   Shahrezaei IH, 2020, IEEE ACCESS, V8, P40198, DOI 10.1109/ACCESS.2020.2976815
   Song TC, 2021, IEEE T CIRC SYST VID, V31, P189, DOI 10.1109/TCSVT.2020.2972155
   Song TC, 2018, IEEE SIGNAL PROC LET, V25, P625, DOI 10.1109/LSP.2018.2809607
   Song TC, 2018, IEEE T CIRC SYST VID, V28, P1565, DOI 10.1109/TCSVT.2017.2671899
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Targhi AT, 2008, INT C PATT RECOG, P717
   Tuncer T, 2020, CHEMOMETR INTELL LAB, V203, DOI 10.1016/j.chemolab.2020.104054
   Verma M, 2018, MULTIMED TOOLS APPL, V77, P11843, DOI 10.1007/s11042-017-4834-3
   Wang K, 2017, PATTERN RECOGN, V67, P213, DOI 10.1016/j.patcog.2017.01.034
   Wang K, 2013, IEEE SIGNAL PROC LET, V20, P853, DOI 10.1109/LSP.2013.2270405
   Wang P, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103400
   Wei JS, 2021, NEUROCOMPUTING, V449, P159, DOI 10.1016/j.neucom.2021.03.063
   Wu XS, 2017, VISUAL COMPUT, V33, P317, DOI 10.1007/s00371-015-1202-z
   Xu XC, 2021, DIGIT SIGNAL PROCESS, V114, DOI 10.1016/j.dsp.2021.103081
   Xu Yong, 2006, 2006 IEEE COMPUTER S, V2, P1932
   Zhang J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3105424
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 52
TC 1
Z9 1
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3385
EP 3409
DI 10.1007/s00371-023-03041-3
EA AUG 2023
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001050556700001
DA 2024-07-18
ER

PT J
AU Mures, OA
   Taibo, J
   Padron, EJ
   Iglesias-Guitian, JA
AF Mures, Omar A.
   Taibo, Javier
   Padron, Emilio J.
   Iglesias-Guitian, Jose A.
TI PlayNet: real-time handball play classification with Kalman embeddings
   and neural networks
SO VISUAL COMPUTER
LA English
DT Article
DE Handball play classification; Real-time multimedia; Neural networks;
   Kalman filtering; Dimensionality reduction
AB Real-time play recognition and classification algorithms are crucial for automating video production and live broadcasts of sporting events. However, current methods relying on human pose estimation and deep neural networks introduce high latency on commodity hardware, limiting their usability in low-cost real-time applications. We present PlayNet, a novel approach to real-time handball play classification. Our method is based on Kalman embeddings, a new low-dimensional representation for game states that enables efficient operation on commodity hardware and customized camera layouts. Firstly, we leverage Kalman filtering to detect and track the main agents in the playing field, allowing us to represent them in a single normalized coordinate space. Secondly, we utilize a neural network trained in nonlinear dimensionality reduction through fuzzy topological data structure analysis. As a result, PlayNet achieves real-time play classification with under 55 ms of latency on commodity hardware, making it a promising addition to automated live broadcasting and game analysis pipelines.
C1 [Mures, Omar A.; Taibo, Javier; Padron, Emilio J.; Iglesias-Guitian, Jose A.] Univ A Coruna, La Coruna, Spain.
   [Mures, Omar A.] CINFO, La Coruna, Spain.
   [Padron, Emilio J.; Iglesias-Guitian, Jose A.] CITIC Ctr ICT Res, La Coruna, Spain.
C3 Universidade da Coruna
RP Mures, OA (corresponding author), Univ A Coruna, La Coruna, Spain.; Mures, OA (corresponding author), CINFO, La Coruna, Spain.
EM omar.alvarez@udc.es; javier.taibo@udc.es; emilio.padron@udc.es;
   j.iglesias.guitian@udc.es
RI Padron, Emilio/KHC-6769-2024; Taibo, Javier/D-1171-2010; Padron
   Gonzalez, Emilio J./F-1241-2016
OI Padron Gonzalez, Emilio J./0000-0002-6864-3737; Taibo,
   Javier/0000-0003-1309-7086; Mures, Omar A./0000-0002-6042-3588
FU CRUE-CSIC agreement with Springer Nature; Xunta de Galicia [ED431F
   2021/11, ED431G 2019/01]; UDC-Inditex InTalent programme
   [PID2019-104184RB-I00, MCIN/AEI/10.13039/501100011033]; Spanish Ministry
   of Science and Innovation [ED431C 2021/30];  [AEI/RYC2018-025385-I]
FX Open Access funding provided thanks to the CRUE-CSICagreement with
   Springer Nature. This work has been developed under the European
   Innovation Council Pilot No 954040. This work was supported also by
   ED431F 2021/11 and ED431G 2019/01 funded by Xunta de Galicia. Emilio J.
   Padron's work was also partially supported through the research projects
   PID2019-104184RB-I00 funded by MCIN/AEI/10.13039/501100011033, and
   ED431C 2021/30. Jose A. Iglesias-Guitian also acknowledges the
   UDC-Inditex InTalent programme and the Spanish Ministry of Science and
   Innovation (AEI/RYC2018-025385-I).
CR Agrawal A, 2021, FOUND TRENDS MACH LE, V14, P211, DOI 10.1561/2200000090
   Ali M, 2019, VISUAL COMPUT, V35, P1013, DOI 10.1007/s00371-019-01673-y
   Biermann Henrik, 2021, MMSports'21: Proceedings of the 4th International Workshop on Multimedia Content Analysis in Sports, P1, DOI 10.1145/3475722.3482792
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Carr Peter., 2013, Proceedings of the 21st ACM International Conference on Multimedia, MM '13, P193, DOI DOI 10.1145/2502081.2502086
   Carrillo H, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01283-0
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Choukroun Y, 2019, IEEE INT CONF COMP V, P3009, DOI 10.1109/ICCVW.2019.00363
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Criminisi A, 1999, IMAGE VISION COMPUT, V17, P625, DOI 10.1016/S0262-8856(98)00183-8
   Cuevas C, 2020, MULTIMED TOOLS APPL, V79, P29685, DOI 10.1007/s11042-020-09409-0
   Deliege A, 2021, IEEE COMPUT SOC CONF, P4503, DOI 10.1109/CVPRW53098.2021.00508
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Guntuboina C., 2021, ELECT LETT COMPUT VI, V20, P99, DOI DOI 10.5565/REV/ELCVIA.1286
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ivasic-Kos M., 2021, DEEP LEARNING APPL, DOI [10.5772/intechopen.96308, DOI 10.5772/INTECHOPEN.96308]
   Kalman R.E., 1961, Trans. ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]
   Karim F, 2018, IEEE ACCESS, V6, P1662, DOI 10.1109/ACCESS.2017.2779939
   Ke GL, 2017, ADV NEUR IN, V30
   Leng L, 2017, MULTIMED TOOLS APPL, V76, P333, DOI 10.1007/s11042-015-3058-7
   Leng L, 2011, LECT NOTES COMPUT SC, V6786, P458, DOI 10.1007/978-3-642-21934-4_37
   Liu T, 2006, J MACH LEARN RES, V7, P1135
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.48550/ARXIV.1802.03426, 10.21105/joss.00861]
   Mendi E, 2013, COMPUT ELECTR ENG, V39, P790, DOI 10.1016/j.compeleceng.2012.11.020
   Morra L., 2020, INT C IM AN REC SPRI, DOI DOI 10.1007/978-3-030-50347-5_11
   Muller Oliver, 2022, Machine Learning and Data Mining for Sports Analytics: 8th International Workshop, MLSA 2021, Virtual Event, Revised Selected Papers. Communications in Computer and Information Science (1571), P116, DOI 10.1007/978-3-031-02044-5_10
   Oytun M, 2020, IEEE ACCESS, V8, P116321, DOI 10.1109/ACCESS.2020.3004182
   Policar Pavlin G, 2019, bioRxiv, DOI DOI 10.1101/731877
   Quiroga J, 2020, IEEE COMPUT SOC CONF, P3911, DOI 10.1109/CVPRW50498.2020.00455
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rongved OAN, 2020, IEEE INT SYM MULTIM, P135, DOI 10.1109/ISM.2020.00030
   Schlipsing M, 2017, J REAL-TIME IMAGE PR, V13, P345, DOI 10.1007/s11554-014-0406-1
   Shih HC, 2018, IEEE T CIRC SYST VID, V28, P1212, DOI 10.1109/TCSVT.2017.2655624
   Shingrakhia H, 2022, VISUAL COMPUT, V38, P2285, DOI 10.1007/s00371-021-02111-8
   Szubert B, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-45301-0
   Taud H., 2018, Geomatic approaches for modeling land change scenarios, DOI [DOI 10.1007/978-3-319-60801-3_27, DOI 10.1007/978-3-319-60801-327]
   Tavassolipour M, 2014, IEEE T CIRC SYST VID, V24, P291, DOI 10.1109/TCSVT.2013.2243640
   Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994
   van den Tillaar R, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072288
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Verucchi M, 2020, IEEE INT C EMERG, P937, DOI [10.1109/etfa46521.2020.9212130, 10.1109/ETFA46521.2020.9212130]
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Zhao BD, 2017, J SYST ENG ELECTRON, V28, P162, DOI 10.21629/JSEE.2017.01.18
   Zolfaghari M, 2020, VISUAL COMPUT, V36, P701, DOI 10.1007/s00371-019-01652-3
NR 45
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2695
EP 2711
DI 10.1007/s00371-023-02972-1
EA AUG 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001043657500002
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, XC
   Chao, WT
   Wang, L
   Duan, FQ
AF Wang, Xuechun
   Chao, Wentao
   Wang, Liang
   Duan, Fuqing
TI Light field depth estimation using occlusion-aware consistency analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Light field; Depth estimation; Occlusion detection; Data cost
ID DISPARITY ESTIMATION
AB Occlusion modeling is critical for light field depth estimation, since occlusion destroys the photo-consistency assumption, which most depth estimation methods hold. Previous works always detect the occlusion points on the basis of Canny detector, which can leave some occlusion points out. Occlusion handling, especially for multi-occluder occlusion, is still challenging. In this paper, we propose a novel occlusion-aware depth estimation method, which can better solve the occlusion problem. We design two novel consistency costs based on the photo-consistency for depth estimation. According to the consistency costs, we analyze the influence of the occlusion and propose an occlusion detection technique based on depth consistency, which can detect the occlusion points more accurately. For the occlusion point, we adopt a new data cost to select the un-occluded views, which are used to determine the depth. Experimental results demonstrate that the proposed method is superior to the other compared algorithms, especially in multi-occluder occlusions.
C1 [Wang, Xuechun; Chao, Wentao; Duan, Fuqing] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Wang, Liang] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing Normal University; Beijing University of Technology
RP Duan, FQ (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
EM fqduan@bnu.edu.cn
RI Wang, Liang/IZD-6629-2023; Wang, Xuechun/JRX-6509-2023
OI Wang, Xuechun/0000-0001-9463-1134; Wang, Liang/0000-0002-3247-1080
FU National Key Research and Development Project Grant [2018AAA0100802];
   Opening Foundation of National Engineering Laboratory for Intelligent
   Video Analysis and Application
FX AcknowledgementsThis work was supported by National Key Research and
   Development Project Grant, Grant/Award Number: 2018AAA0100802, Opening
   Foundation of National Engineering Laboratory for Intelligent Video
   Analysis and Application.
CR Chen YL, 2022, IEEE T COMPUT IMAG, V8, P397, DOI 10.1109/TCI.2022.3169699
   Guo ZH, 2019, IEEE ACCESS, V7, P169123, DOI 10.1109/ACCESS.2019.2954892
   Han K., 2021, IEEE S VLSI TECHN, P1
   Honauer K, 2017, LECT NOTES COMPUT SC, V10113, P19, DOI 10.1007/978-3-319-54187-7_2
   Huang CT, 2019, IEEE T PATTERN ANAL, V41, P552, DOI 10.1109/TPAMI.2018.2809502
   Huang ZC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6300, DOI 10.1109/ICCV48922.2021.00626
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Mousnier A., 2015, COMPUT SCI
   Raj A.S., 2016, Stanford light field archives.
   Ren NgMarc Levoy., 2005, Light field photography with a hand-held plenoptic camera
   Sheng H, 2017, IEEE T IMAGE PROCESS, V26, P5758, DOI 10.1109/TIP.2017.2745100
   Shi JL, 2019, IEEE T IMAGE PROCESS, V28, P5867, DOI 10.1109/TIP.2019.2923323
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89
   Tran TH, 2021, IEEE T CIRC SYST VID, V31, P2562, DOI 10.1109/TCSVT.2020.3028258
   Tsai YJ, 2020, AAAI CONF ARTIF INTE, V34, P12095
   Wang TC, 2016, IEEE T PATTERN ANAL, V38, P2170, DOI 10.1109/TPAMI.2016.2515615
   Wang Yulin, 2022, P IEEE CVF C COMP VI
   Wang YL, 2020, IEEE T COMPUT IMAG, V6, P830, DOI 10.1109/TCI.2020.2986092
   Wanner S., 2013, INT S VIS MOD VIS, P225
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Williem, 2018, IEEE T PATTERN ANAL, V40, P2484, DOI 10.1109/TPAMI.2017.2746858
   Yu JY, 2017, IEEE MULTIMEDIA, V24, P104, DOI 10.1109/MMUL.2017.24
   Zhang S, 2016, COMPUT VIS IMAGE UND, V145, P148, DOI 10.1016/j.cviu.2015.12.007
   Zhang YB, 2017, IEEE T CIRC SYST VID, V27, P739, DOI 10.1109/TCSVT.2016.2555778
   Zhang YC, 2020, IEEE T CIRC SYST VID, V30, P4269, DOI 10.1109/TCSVT.2019.2954948
   Zhu H, 2017, IEEE J-STSP, V11, P965, DOI 10.1109/JSTSP.2017.2730818
NR 29
TC 0
Z9 0
U1 7
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3441
EP 3454
DI 10.1007/s00371-023-03027-1
EA AUG 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001042487800001
DA 2024-07-18
ER

PT J
AU Kravec, J
   Kacerik, M
   Bittner, J
AF Kravec, Jaroslav
   Kacerik, Martin
   Bittner, Jiri
TI PVLI: potentially visible layered image for real-time ray tracing
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Ray tracing; Remote rendering
ID VISIBILITY; DEPTH
AB Novel view synthesis is frequently employed in video streaming, temporal upsampling, or virtual reality. We propose a new representation, potentially visible layered image (PVLI), that uses a combination of a potentially visible set of the scene geometry and layered color images. PVLI encodes the depth implicitly and enables cheap run-time reconstruction. Furthermore, PVLI can also be used to reconstruct pixel and layer connectivities, which is crucial for filtering and post-processing of the rendered images. We use PVLIs to achieve local and server-based real-time ray tracing. In the first case, PVLIs are used as a basis for temporal and spatial upsampling of ray-traced illumination. In the second case, PVLIs are compressed, streamed over the network, and then used by a thin client to perform temporal and spatial upsampling and to hide latency. To shade the view, we use path tracing, accounting for effects such as soft shadows, global illumination, and physically based refraction. Our method supports dynamic lighting, and up to a limited extent, it also handles view-dependent surface interactions.
C1 [Kravec, Jaroslav; Kacerik, Martin; Bittner, Jiri] Czech Tech Univ, Prague, Czech Republic.
C3 Czech Technical University Prague
RP Kacerik, M (corresponding author), Czech Tech Univ, Prague, Czech Republic.
EM kravejar@fel.cvut.cz; kacerma2@fel.cvut.cz; bittner@fel.cvut.cz
OI Kacerik, Martin/0000-0002-5221-3513
FU Czech Science Foundation [GA18-20374S]; Research Center for Informatics
   [CZ.02.1.01/0.0/0.0/16_019/0000765]; Grant Agency of the Czech Technical
   University in Prague [SGS22/173/OHK3/3T/13]
FX AcknowledgementsThis work was supported by the Czech Science Foundation
   (GA18-20374S), Research Center for Informatics
   (CZ.02.1.01/0.0/0.0/16_019/0000765), and by the Grant Agency of the
   Czech Technical University in Prague, No. SGS22/173/OHK3/3T/13.
CR Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   [Anonymous], 2003, Level of detail for 3D graphics
   Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Cohen-Or D, 2003, IEEE T VIS COMPUT GR, V9, P412, DOI 10.1109/TVCG.2003.1207447
   Didyk P., 2010, P VIS MOD VIS WORKSH, P299
   Didyk P, 2010, COMPUT GRAPH FORUM, V29, P713, DOI 10.1111/j.1467-8659.2009.01641.x
   Gribble C., 2019, RAY TRACING GEMS
   Hladky J, 2021, COMPUT GRAPH FORUM, V40, P475, DOI 10.1111/cgf.142648
   Hladky J, 2019, COMPUT GRAPH FORUM, V38, P171, DOI 10.1111/cgf.13780
   Hladky J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356530
   Hoppe H., 2011, P 2011 SIGGRAPH ASIA, P1
   Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398
   Isik M, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476580, 10.1145/3450626.3459793]
   Koch T, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3451266
   Kondapaneni I, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323009
   Koskela M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269978
   Mara M., 2013, LIGHTING DEEP G BUFF
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Muddala SM, 2016, J VIS COMMUN IMAGE R, V38, P351, DOI 10.1016/j.jvcir.2016.02.017
   Mueller JH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3446790
   Mueller JH, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275087
   Müller T, 2017, COMPUT GRAPH FORUM, V36, P91, DOI 10.1111/cgf.13227
   Noimark Y, 2003, IEEE COMPUT GRAPH, V23, P58, DOI 10.1109/MCG.2003.1159614
   OculusVR, 2020, REND OC RIFT
   Ouyang Y, 2021, COMPUT GRAPH FORUM, V40, P17, DOI 10.1111/cgf.14378
   Pajak D, 2011, COMPUT GRAPH FORUM, V30, P415, DOI 10.1111/j.1467-8659.2011.01871.x
   Park SY, 2010, IEEE SIGNAL PROC LET, V17, P839, DOI 10.1109/LSP.2010.2060329
   Reinert B, 2016, COMPUT GRAPH FORUM, V35, P353, DOI 10.1111/cgf.13032
   SAMET H, 1984, COMPUT SURV, V16, P187, DOI 10.1145/356924.356930
   Schied C, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3233301
   Schied C, 2017, HPG '17: PROCEEDINGS OF HIGH PERFORMANCE GRAPHICS, DOI 10.1145/3105762.3105770
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Shi S, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2719921
   Shi S, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2348816.2348825
   Strengert M., 2006, P VIS MOD VIS 2006, P169
   Tauber Z, 2007, IEEE T SYST MAN CY C, V37, P527, DOI 10.1109/TSMCC.2006.886967
   Walter B, 1999, SPRING EUROGRAP, P19
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wonka P, 2006, ACM T GRAPHIC, V25, P494, DOI 10.1145/1141911.1141914
   Zheng SK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480510
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 42
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3359
EP 3372
DI 10.1007/s00371-023-03007-5
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001039828800003
OA Bronze
DA 2024-07-18
ER

PT J
AU Tang, PZ
   Guo, Y
   Zheng, GG
   Zheng, LL
   Pu, J
   Wang, J
   Chen, ZF
AF Tang, Pengzhou
   Guo, Yu
   Zheng, Guanguan
   Zheng, Liangliang
   Pu, Jun
   Wang, Jian
   Chen, Zifan
TI Two-stage filtering method to improve the performance of object
   detection trained by synthetic dataset in heavily cluttered industry
   scenes
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Synthetic image; CAD model; Cluttered scenes;
   Industrial manufacturing; Two-stage
AB Object detection (OD) networks trained with CAD-based synthetic datasets still face significant challenges in detecting real mechanical parts in heavily cluttered industry scenes. This paper proposes a novel two-stage filtering method to improve detection performance. In the first stage, in order to increase the precision and recall of OD, a novel method for optimizing the position of pasted parts is discussed to increase the variety of synthetic datasets, and a novel DoG-MS module is designed and seamlessly integrated into the original networks. In the second stage, high-performance image classification networks, subsequently used as a filter, are designed based on yolov5s and transfer learning. Then an effective filtering strategy is designed to improve the precision of object detection further. Extensive experimental results show that compared to the original OD networks, the two-stage filtering method can improve the mean precision, mean recall, and mAP by 7.5%, 3.5%, and 3.9%, respectively. The proposed method has the potential to expand the application range of CAD-based synthetic datasets in the field of industrial manufacturing.
C1 [Tang, Pengzhou; Guo, Yu; Zheng, Guanguan; Zheng, Liangliang; Pu, Jun; Wang, Jian; Chen, Zifan] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Jiangsu, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Guo, Y (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing, Jiangsu, Peoples R China.
EM guoyu@nuaa.edu.cn
FU Jiangsu Province Frontier Leading Technology Basic Research Special
   Project of China [BK20202007]
FX AcknowledgementsThis work was supported by the Jiangsu Province Frontier
   Leading Technology Basic Research Special Project of China (Grant No.
   BK20202007).
CR Sampaio IGB, 2021, PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS (ICEIS 2021), VOL 1, P75, DOI 10.5220/0010451100750082
   Bang S, 2020, AUTOMAT CONSTR, V115, DOI 10.1016/j.autcon.2020.103198
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Cohen J, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 5: VISAPP, P644, DOI 10.5220/0008975506440651
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Eversberg L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21237901
   Georgakis G, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII
   Gu Y, 2022, AGRICULTURE-BASEL, V12, DOI 10.3390/agriculture12040485
   [郭磊 Guo Lei], 2022, [电子科技大学学报, Journal of University of Electronic Science and Technology of China], V51, P251
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   Hendry, 2019, IMAGE VISION COMPUT, V87, P47, DOI 10.1016/j.imavis.2019.04.007
   Hinterstoisser S, 2019, LECT NOTES COMPUT SC, V11129, P682, DOI 10.1007/978-3-030-11009-3_42
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu WF, 2023, J INTELL MANUF, V34, P2943, DOI 10.1007/s10845-022-01971-8
   Hu WH, 2022, VISUAL COMPUT, V38, P3731, DOI 10.1007/s00371-021-02210-6
   Huang YB, 2020, VISUAL COMPUT, V36, P85, DOI 10.1007/s00371-018-1588-5
   Jo H, 2017, INT C CONTR AUTOMAT, P1035, DOI 10.23919/ICCAS.2017.8204369
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lai ZH, 2020, J MANUF SYST, V55, P69, DOI 10.1016/j.jmsy.2020.02.010
   Lee WC, 2021, PROC IEEE INT SYMP, DOI 10.1109/ISIE45552.2021.9576247
   Leng Z, 2022, ARXIV
   Lenn J., 2020, ULTRALYTICS YOLOV5 I
   Li CHG, 2020, INT J ADV MANUF TECH, V111, P2259, DOI 10.1007/s00170-020-06185-x
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu D, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207086
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu P, 2020, INT J ROBOT AUTOM, V35, P460, DOI [10.2316/Journal.206.2020.6.206-0479, 10.2316/J.2020.206-0479]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Peddireddy D, 2021, J MANUF PROCESS, V64, P1336, DOI 10.1016/j.jmapro.2021.02.034
   Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarkar K, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P130, DOI 10.5220/0006272901300137
   Sharif M, 2020, IEEE ACCESS, V8, P167448, DOI 10.1109/ACCESS.2020.3021660
   Sun J, 2023, VISUAL COMPUT, V39, P4391, DOI 10.1007/s00371-022-02597-w
   Tang PZ, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01237-y
   Tobin Josh, 2017, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P23, DOI 10.1109/IROS.2017.8202133
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   Tsirikoglou A, 2020, COMPUT GRAPH FORUM, V39, P426, DOI 10.1111/cgf.14047
   Wen H, 2021, MATERIALS, V14, DOI 10.3390/ma14102575
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu DH, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105742
   Wu ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5557, DOI 10.1145/3503161.3547930
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Yang X, 2022, IEEE ROBOT AUTOM LET, V7, P7201, DOI 10.1109/LRA.2022.3180403
   Zhang HR, 2017, ROBOT AUTON SYST, V95, P64, DOI 10.1016/j.robot.2017.06.003
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
   Zubizarreta J, 2019, INT J ADV MANUF TECH, V102, P4095, DOI 10.1007/s00170-019-03527-2
NR 58
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2015
EP 2034
DI 10.1007/s00371-023-02899-7
EA JUL 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001023990600001
DA 2024-07-18
ER

PT J
AU Zhao, X
   Li, HD
   Wang, HR
AF Zhao, Xi
   Li, Haodong
   Wang, Haoran
TI Learning shape abstraction by cropping positive cuboid primitives with
   negative ones
SO VISUAL COMPUTER
LA English
DT Article
DE Shape abstraction; Primitive fitting; Iterative error feedback
AB High-quality 3D model abstraction is needed in many graphics or 3D vision tasks to improve the rendering efficiency, increase transmission speed or reduce space occupation. Traditional simplification algorithms for 3D models rely heavily on the mesh topology and ignore objects' overall structure during optimization. Learning-based methods are then proposed to form an end-to-end regression system for abstraction. However, existing learning-based methods have difficulty representing shapes with hollow or concave structures. We propose a self-supervised learning-based abstraction method for 3D meshes to solve this problem. Our system predicts the positive and negative primitives, where positive primitives are to match the inside part of the shape, and negative primitives represent the hollow region of the shape. More specifically, the bool difference between positive primitives and the object is fed to a network using Iteration error feedback mechanism to predict the negative primitives, which crop the positive primitives to create hollow or concave structures. In addition, we design a new separation loss to prevent a negative primitive from overlapping the object too much. We evaluate the proposed method on the ShapeNetCore dataset by Chamfer Distance and Intersection over Union. The results show that our positive-negative abstraction schema outperforms the baselines.
C1 [Zhao, Xi; Li, Haodong; Wang, Haoran] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, 28 Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Zhao, X (corresponding author), Xi An Jiao Tong Univ, Sch Comp Sci & Technol, 28 Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.
EM xi.zhao@mail.xjtu.edu.cn; greyishsong@stu.xjtu.edu.cn;
   wanghr_xjtu@foxmail.com
RI Wang, Haoran/GYD-5712-2022
OI Wang, Haoran/0000-0002-6367-7583
FU National Natural Science Foundation of China [62072366]; Key Ramp;D
   project of Shaanxi Province [2021QFY01-03HZ]
FX This work was supported by National Natural Science Foundation of China
   (62072366) and Key R & D project of Shaanxi Province (2021QFY01-03HZ).
CR Birdal T, 2018, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2018.00372
   Chang A., ARXIV
   Chen X., 2017, SPIE, V10443, P205
   Garland M., 1997, P 24 ANN C COMP GRAP, V1997, P209, DOI DOI 10.1145/258734.258849
   Guo Jianwei, 2024, IEEE Trans Vis Comput Graph, V30, P3283, DOI 10.1109/TVCG.2022.3230369
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Huang J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602141
   Huang JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15323, DOI 10.1109/ICCV48922.2021.01506
   Lê ET, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7437, DOI 10.1109/ICCV48922.2021.00736
   Li LX, 2019, PROC CVPR IEEE, P2647, DOI 10.1109/CVPR.2019.00276
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Liu HTD, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356495
   Liu ZJ, 2018, LECT NOTES COMPUT SC, V11216, P3, DOI 10.1007/978-3-030-01258-8_1
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Paschalidou D, 2019, PROC CVPR IEEE, P10336, DOI 10.1109/CVPR.2019.01059
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Sharma Gopal, 2020, COMPUTER VISION ECCV, P261, DOI DOI 10.1007/978-3-030-58571-6_16
   Sun CY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356529
   Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160
   Wu Q, 2018, COMPUT GRAPH FORUM, V37, P221, DOI 10.1111/cgf.13504
   Yan SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2733, DOI 10.1109/ICCV48922.2021.00275
   Yang KZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459873
   Yin Kangxue, 2021, P IEEE CVF INT C COM, P12456
   Yumer ME, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366185
   Zhao X, 2021, COMPUT AIDED DESIGN, V141, DOI 10.1016/j.cad.2021.103092
NR 25
TC 0
Z9 0
U1 3
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3585
EP 3595
DI 10.1007/s00371-023-02943-6
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001023823500002
DA 2024-07-18
ER

PT J
AU Ma, ZH
   Li, W
   Zhang, MY
   Meng, WL
   Xu, SB
   Zhang, XP
AF Ma, Zhihao
   Li, Wei
   Zhang, Muyang
   Meng, Weiliang
   Xu, Shibiao
   Zhang, Xiaopeng
TI HTCViT: an effective network for image classification and segmentation
   based on natural disaster datasets
SO VISUAL COMPUTER
LA English
DT Article
DE Natural disaster image analysis; Vision transformer; Convolution;
   Hierarchical
AB Classifying and segmenting natural disaster images are crucial for predicting and responding to disasters. However, current convolutional networks perform poorly in processing natural disaster images, and there are few proprietary networks for this task. To address the varying scales of the region of interest (ROI) in these images, we propose the Hierarchical TSAM-CB-ViT (HTCViT) network, which builds on the ViT network's attention mechanism to better process natural disaster images. Considering that ViT excels at extracting global context but struggles with local features, our method combines the strengths of ViT and convolution, and can capture overall contextual information within each patch using the Triple-Strip Attention Mechanism (TSAM) structure. Experiments validate that our HTCViT can improve the classification task with 3 - 4% and the segmentation task with 1 - 2% on natural disaster datasets compared to the vanilla ViT network.
C1 [Ma, Zhihao; Li, Wei; Zhang, Muyang; Meng, Weiliang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Meng, Weiliang; Zhang, Xiaopeng] Zhejiang Lab, Hangzhou, Peoples R China.
   [Ma, Zhihao; Li, Wei; Zhang, Muyang; Meng, Weiliang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Xu, Shibiao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Zhejiang Laboratory; Chinese Academy of Sciences; Institute of
   Automation, CAS; Beijing University of Posts & Telecommunications
RP Meng, WL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Meng, WL (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.; Meng, WL (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.; Xu, SB (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.
EM weiliang.meng@ia.ac.cn; shibiaoxu@bupt.edu.cn
OI meng, wei liang/0000-0002-3221-4981
FU National Natural Science Foundation of China [U21A20515, 61972459,
   62172416, 62102414, U2003109, 62071157, 62171321, 62162044]; Open
   Research Projects of ZhejiangLab [2021KE0AB07];  [TC210H00L/42]
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. U21A20515, 61972459, 62172416, 62102414,
   U2003109, 62071157, 62171321, and 62162044), in part by Open Research
   Projects of ZhejiangLab (No.2021KE0AB07) and the Project TC210H00L/42.
CR Balestriero R., 2021, arXiv
   Barz B, 2019, Arxiv, DOI arXiv:1908.03361
   Bellotti F, 2014, LECT N EARTH SYST SC, P287, DOI 10.1007/978-3-642-32408-6_64
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cruden D.M., 1991, B INT ASS ENG GEOLOG, DOI [DOI 10.1007/BF02590167, 10.1007/BF02590167]
   Ding AZ, 2016, 2016 31ST YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P444, DOI 10.1109/YAC.2016.7804935
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji SP, 2020, LANDSLIDES, V17, P1337, DOI 10.1007/s10346-020-01353-2
   Jiang Yifan, 2021, ARXIV210207074
   Jonkman SN, 2008, J FLOOD RISK MANAG, V1, P43, DOI 10.1111/j.1753-318X.2008.00006.x
   Lendering KT, 2016, J FLOOD RISK MANAG, V9, P320, DOI 10.1111/jfr3.12185
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W., 2016, INT C LEARNING REPRE
   Ma ZH, 2022, VISUAL COMPUT, V38, P3163, DOI 10.1007/s00371-022-02535-w
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Panboonyuen T, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13245100
   Park J., 2018, BRIT MACHINE VISION
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosi A, 2018, LANDSLIDES, V15, P5, DOI 10.1007/s10346-017-0861-4
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sato Hiroshi P., 2005, LANDSLIDES, V4, P2007
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 39
TC 0
Z9 0
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3285
EP 3297
DI 10.1007/s00371-023-02954-3
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001025049000002
DA 2024-07-18
ER

PT J
AU Smajdek, U
   Lesar, Z
   Marolt, M
   Bohak, C
AF Smajdek, Uros
   Lesar, Ziga
   Marolt, Matija
   Bohak, Ciril
TI Combined volume and surface rendering with global illumination caching
SO VISUAL COMPUTER
LA English
DT Article
DE Irradiance caching; Global illumination; Volumetric data
ID OCCLUSION SHADING MODEL; TRANSPORT
AB We present a combined volume and surface rendering technique with global illumination caching. Our approach uses volumetric path tracing to compute the global illumination volume and local shading models for rendering the isosurface. By joining both visualization approaches, we have enhanced the display and illumination of the surfaces while preserving physically realistic illumination of the participating media. To achieve real-time performance and avoid recomputing the image when the camera view changes, we compute the global illumination volume incrementally and defer the projection to a later step. We evaluated our technique by comparing different local shading models for isosurface rendering with the result of full volumetric path tracing and with the non-caching variant of our technique. Results show that the caching and non-caching variants perform comparably well, while the caching variant has the added benefit of being camera-view-independent. Additionally, we show that our approach emphasizes the surfaces within volumes better than volumetric path tracing.
C1 [Smajdek, Uros; Lesar, Ziga; Marolt, Matija; Bohak, Ciril] Univ Ljubljana, Fac Comp & Informat Sci, Vecna pot 113, Ljubljana 1000, Slovenia.
   [Bohak, Ciril] King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
C3 University of Ljubljana; King Abdullah University of Science &
   Technology
RP Smajdek, U (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Vecna pot 113, Ljubljana 1000, Slovenia.
EM us6796@student.uni-lj.si
OI Bohak, Ciril/0000-0002-9015-2897; Smajdek, Uros/0000-0002-8127-7700
CR Andersen TG, 2016, VISUAL COMPUT, V32, P739, DOI 10.1007/s00371-016-1252-x
   [Anonymous], 1984, Prog Nucl Med, V8, P1
   [Anonymous], 2005, Posi- tron Emission Tomography
   [Anonymous], 2017, ACM SIGGRAPH, DOI [10.1145/3084873.3084907, DOI 10.1145/3084873.3084907]
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   Bloomenthal J., 1994, IMPLICIT SURFACE POL, P324, DOI [10.1016/B978-0-12-336156-1.50040-9, DOI 10.1016/B978-0-12-336156-1.50040-9]
   Bosma MK, 1998, P SOC PHOTO-OPT INS, V3335, P10, DOI 10.1117/12.312490
   Bruckner S, 2007, COMPUT GRAPH FORUM, V26, P715, DOI 10.1111/j.1467-8659.2007.01095.x
   Burley B., 2012, ACM SIGGRAPH 2012 CO, V2, P3
   Chandrasekhar S., 1950, RAD TRANSFER
   DOI A, 1991, IEICE TRANS COMMUN, V74, P214
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Engel K, 2006, Real-Time Volume Graphics
   Galtier M, 2013, J QUANT SPECTROSC RA, V125, P57, DOI 10.1016/j.jqsrt.2013.04.001
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Iglesias-Guitian JA, 2022, IEEE T VIS COMPUT GR, V28, P2734, DOI 10.1109/TVCG.2020.3037680
   Isenberg T., 2006, Non-photorealistic rendering in context: an observational study, P115
   Jarosz W, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330518
   Kajiya J. T., 1984, Computers & Graphics, V18, P165
   Kajiya J. T., 1986, Computer Graphics, V20, P143, DOI 10.1145/15886.15902
   KALENDER WA, 1990, RADIOLOGY, V176, P181, DOI 10.1148/radiology.176.1.2353088
   KAWATA S, 1986, ACTA HISTOCHEM CYTOC, V19, P73, DOI 10.1267/ahc.19.73
   Khlebnikov R, 2014, COMPUT GRAPH FORUM, V33, P61, DOI 10.1111/cgf.12362
   Kirk D. B., 1987, Visual Computer, V3, P63, DOI 10.1007/BF02153662
   Koning RI, 2009, ANN ANAT, V191, P427, DOI 10.1016/j.aanat.2009.04.003
   Kroes T, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038586
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Li Z., 2016, NETFLIX TECH BLOG, V6
   Ljung P, 2016, COMPUT GRAPH FORUM, V35, P669, DOI 10.1111/cgf.12934
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mildenhall Ben, 2020, EUR C COMP VIS ECCV
   Novák J, 2018, COMPUT GRAPH FORUM, V37, P551, DOI 10.1111/cgf.13383
   Ohtake Y, 2005, ACM SIGGRAPH 2005 CO, P173
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   Parker S, 1999, IEEE T VIS COMPUT GR, V5, P238, DOI 10.1109/2945.795215
   Ribardière M, 2011, VISUAL COMPUT, V27, P655, DOI 10.1007/s00371-011-0573-z
   Ruiz M, 2010, VISUAL COMPUT, V26, P687, DOI 10.1007/s00371-010-0497-z
   Rushmeier H.E., 1987, P SIGGRAPH, P293, DOI 10.1145/37402.37436
   Schott M, 2009, COMPUT GRAPH FORUM, V28, P855, DOI 10.1111/j.1467-8659.2009.01464.x
   Sobierajski LisaM., 1994, Proceedings of the 1994 symposium on Volume visualization, P11, DOI 10.1145/197938.197949
   Soltészová V, 2010, COMPUT GRAPH FORUM, V29, P883, DOI 10.1111/j.1467-8659.2009.01695.x
   Taylor R, 2015, ASTRON COMPUT, V13, P67, DOI 10.1016/j.ascom.2015.10.002
   TIEDE U, 1990, IEEE COMPUT GRAPH, V10, P41, DOI 10.1109/38.50672
   Tietjen Christian., 2005, Proc. of the Eurographics/IEEE-VGTC Symp. on Visualization (EuroVis), P303, DOI [DOI 10.2312/VISSYM/EUROVIS05/303-310, 10.2312/VisSym/EuroVis05/303-310]
   Treece GM, 1999, COMPUT GRAPH-UK, V23, P583, DOI 10.1016/S0097-8493(99)00076-X
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber C., 2013, Vision, Modeling & Visualization, DOI [10.2312/PE.VMV.VMV13.195-202, DOI 10.2312/PE.VMV.VMV13.195-202]
   Weiss S, 2021, IEEE T VIS COMPUT GR, V27, P3064, DOI 10.1109/TVCG.2019.2956697
   Woodcock E.R., 1965, PROC C APPL COMPUTIN, P557
   Xu CQ, 2021, VIS INFORM, V5, P70, DOI 10.1016/j.visinf.2021.08.001
   YUE Y, 2010, ACM T GRAPHIC, V177, P1, DOI DOI 10.1145/1882261.1866199
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng XJ, 2017, BIOMED RES INT, V2017, DOI 10.1155/2017/1962181
NR 55
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2491
EP 2503
DI 10.1007/s00371-023-02932-9
EA JUN 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001016490300003
OA hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Sun, B
   Hao, Z
   Yu, LJ
   He, J
AF Sun, Bo
   Hao, Zhuo
   Yu, Lejun
   He, Jun
TI Unbiased scene graph generation using the self-distillation method
SO VISUAL COMPUTER
LA English
DT Article
DE Scene graph generation; Long-tail; Self-distillation; Causal inference
AB Scene graph generation (SGG) aims to build a structural representation for the image with the object instance and the relations between object pairs. Due to the long-tail distribution of the dataset labeling, scene graph generation models must adopt the debiasing method during the learning process. In this paper, we propose to integrating a novel self-distillation method into the existing SGG models and the experimental results have shown competitive debiasing performance. Further analysis of its effectiveness with causal inference theory has indicated that our method can be considered as a new intervention method.
C1 [Sun, Bo; Hao, Zhuo; Yu, Lejun; He, Jun] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Sun, Bo; He, Jun] Beijing Normal Univ, Coll Educ Future, Zhuhai 519087, Guangdong, Peoples R China.
C3 Beijing Normal University; Beijing Normal University
RP He, J (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.; He, J (corresponding author), Beijing Normal Univ, Coll Educ Future, Zhuhai 519087, Guangdong, Peoples R China.
EM hejun@bnu.edu.cn
FU National Natural Science Foundation of China [62077009, 62177006];
   Guangdong Provincial Natural Science Foundation [2214050002868]; Zhuhai
   Science and Technology Planning Project [ZH22036201210161PWC]
FX AcknowledgementsThis work is supported by the National Natural Science
   Foundation of China (Grant No. 62077009, 62177006) and partially
   supported by the Guangdong Provincial Natural Science Foundation (Grant
   No. 2214050002868) and Zhuhai Science and Technology Planning
   Project(Grant No. ZH22036201210161PWC.
CR Chang Y, 2023, VISUAL COMPUT, V39, P2583, DOI 10.1007/s00371-022-02480-8
   Chen DQ, 2019, IEEE WINT CONF APPL, P1118, DOI 10.1109/WACV.2019.00124
   Chen XY, 2021, IEEE WINT CONF APPL, P545, DOI 10.1109/WACV48630.2021.00059
   Chiou MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1581, DOI 10.1145/3474085.3475297
   Dean J., 2015, NIPS DEEP LEARNING R
   Fang ZY, 2022, VISUAL COMPUT, V38, P3563, DOI 10.1007/s00371-021-02184-5
   Ge Y., 2021, ARXIV
   Guo H, 2021, PROC CVPR IEEE, P15084, DOI 10.1109/CVPR46437.2021.01484
   Li TH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P610, DOI 10.1109/ICCV48922.2021.00067
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Pearl J., 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161
   Reichenbach H., 1956, The Direction of Time
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tang K., A scene graph generation codebase in pytorch
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Wang HX, 2023, VISUAL COMPUT, V39, P639, DOI 10.1007/s00371-021-02363-4
   Wang WB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15880, DOI 10.1109/ICCV48922.2021.01560
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yan ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P265, DOI 10.1145/3394171.3413722
   Yu J., 2021, IJCAI
   Yu J, 2020, VISUAL COMPUT, V36, P1457, DOI 10.1007/s00371-019-01751-1
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
NR 28
TC 0
Z9 0
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2024
VL 40
IS 4
BP 2381
EP 2390
DI 10.1007/s00371-023-02924-9
EA JUN 2023
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ2U3
UT WOS:001014470900004
DA 2024-07-18
ER

PT J
AU Shah, B
   Bhavsar, H
AF Shah, Bhoomi
   Bhavsar, Hetal
TI Depth-restricted convolutional neural network-a model for Gujarati food
   image classification
SO VISUAL COMPUTER
LA English
DT Article
DE Image classification; Machine learning; Optimization; Predictive models;
   Supervised learning; Transfer Learning
AB For an effective dietary assessment system, it is necessary to keep track of the amount of food consumed. Food recognition is the first step to calorie estimation, and image processing technique is useful to achieve this. With the use of food image classification, people can count the amount of food taken and control the calories taken, which helps to reduce the risk of serious health conditions like hypertension, chronic diseases, and heart disease. The nature of food is very diverse, which makes the food image classification task more challenging. Deep learning methods for image classification give more accurate and efficient results as compared to traditional methods. This research work focuses on classifying Gujarati food images as no efforts have been made till now to classify Gujarati food images. A new dataset named "Traditional Gujarati Food Images Dataset (TGFD)" has been created. The dataset contains 1764 images belonging to five food classes and famous food items in Gujarat. The experiments start by implementing transfer learning on models, namely VGG16, VGG19, Resnet50, Inceptionv3, and Alexnet. Fine-tuning has been implemented on all models in order to increase accuracy. After fine-tuning all the models, the maximum accuracy achieved was "89.36%" on the Inception v3 model, but the loss was very high. Certain parameters, like the number of convolutional layers, number of neurons in fully connected layers, number of filters, and filter size, directly affect the model's accuracy. Taking these parameters into consideration to improve accuracy and reduce loss, this research work proposes a model named "depth-restricted convolutional neural network (DRCNN)" which achieves "95.48%" accuracy, which is remarkable. The DRCNN model contains 482,069 parameters, which is 48 times less than the parameters of the Inceptionv3 model, and the validation loss is only 0.8041. Introducing batch normalization in the proposed model drastically improves performance with a lower number of parameters. DRCNN has been tested on an increasing number of classes in the dataset and on different types of food datasets. In both cases, the model performs outstandingly, proving its versatility.
C1 [Shah, Bhoomi] Maharaja Sayajirao Univ Baroda, Fac Sci, Dept Comp Applicat, Vadodara 390001, Gujarat, India.
   [Bhavsar, Hetal] Maharaja Sayajirao Univ Baroda, Fac Technol & Engn, Dept Comp Sci & Engn, Vadodara 390001, Gujarat, India.
C3 Maharaja Sayajirao University Baroda; Maharaja Sayajirao University
   Baroda
RP Shah, B (corresponding author), Maharaja Sayajirao Univ Baroda, Fac Sci, Dept Comp Applicat, Vadodara 390001, Gujarat, India.
EM bhoomi-compapp@msubaroda.ac.in
OI Shah, Bhoomi/0000-0001-6881-2919
CR Aguilar E, 2017, IMAGE ANAL PROCESSIN, P1
   Attokaren D, 2017, P IEEE REG 10 C
   Attokaren DJ, 2017, TENCON IEEE REGION, P2801, DOI 10.1109/TENCON.2017.8228338
   Bansal A, 2017, IEEE INT CONF COMP V, P2545, DOI 10.1109/ICCVW.2017.299
   Basha SHS, 2020, NEUROCOMPUTING, V378, P112, DOI 10.1016/j.neucom.2019.10.008
   Bera S, 2020, IET IMAGE PROCESS, V14, P480, DOI 10.1049/iet-ipr.2019.0561
   Bird Jordan J., 2020, 2020 IEEE 10th International Conference on Intelligent Systems (IS), P619, DOI 10.1109/IS48319.2020.9199968
   Bylander T, 2006, P 19 INT FLOR ART IN, P11
   Chawla NV, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P853, DOI 10.1007/0-387-25465-X_40
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fahira P, 2020, IEEE 4 INT C INF COM
   Fatourechi M, 2008, IEEE 7 INT C MACH LE
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   Hnoohom N, 2018, 2018 1ST INTERNATIONAL ECTI NORTHERN SECTION CONFERENCE ON ELECTRICAL, ELECTRONICS, COMPUTER AND TELECOMMUNICATIONS ENGINEERING (ECTI-NCON, P116, DOI 10.1109/ECTI-NCON.2018.8378293
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Hu Peng, 2023, IEEE T PATTERN ANAL, P1
   Huilgol P., 2019, ACCURACY VS F1 SCORE
   Hussain M, 2019, ADV INTELL SYST, V840, P191, DOI 10.1007/978-3-319-97982-3_16
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Islam K T, 2019, INT C DIG IM COMP TE
   Joshi S, 2021, IEEE INT C BIG DAT B
   Kandel I, 2020, ICT EXPRESS, V6, P312
   keras, KER KERASTUNER
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Kingma D. P., 2014, arXiv
   Kiourt C., 2020, DEEP LEARNING APPROA, P83
   Lei FY, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-019-1903-4
   Li XH, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107049
   Lu Y., 2019, COMPUTER VISION PATT
   Maitra S, 2018, 2018 4TH INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT)
   Meng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P557, DOI 10.1145/3343031.3350870
   Mohammed S, 2019, IECBES P, P284
   Nayak J, 2020, COMPUT SCI REV, V38, DOI 10.1016/j.cosrev.2020.100297
   Ng YS, 2019, MADIMA'19: PROCEEDINGS OF THE 5TH INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P33, DOI 10.1145/3347448.3357168
   Owais Mujtaba Khanday SD., 2021, IAES INT J ARTIF INT, V10, P872, DOI [DOI 10.11591/IJAI.V10.I4.PP872-878, 10.11591/ijai.v10.i4.pp872-878]
   Pandey P, 2017, IEEE SIGNAL PROC LET, V24, P1758, DOI 10.1109/LSP.2017.2758862
   Phiphiphatphaisit Sirawan, 2020, ICISS 2020: Proceedings of the 2020 The 3rd International Conference on Information Science and System, P51, DOI 10.1145/3388176.3388179
   Pon M.Z. A., 2021, Sparklinglight Transactions on Artificial Intelligence and Quantum Computing (STAIQC), V1, P36, DOI DOI 10.55011/STAIQC.2021.1104
   Rajayogi J., 2019, 2019 4 INT C COMPUTA, P1, DOI [10.1109/CSITSS47250.2019.9031051, DOI 10.1109/CSITSS47250.2019.9031051]
   Sengür A, 2019, 2019 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND DATA PROCESSING (IDAP 2019), DOI 10.1109/idap.2019.8875946
   Shah B, 2021, INTELLIGENT SYSTEMS, P265
   Siddiqi R, 2019, ICDLT 2019: 2019 3RD INTERNATIONAL CONFERENCE ON DEEP LEARNING TECHNOLOGIES, P91, DOI 10.1145/3342999.3343002
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
   Sun J, 2019, 16 INT C MACH VIS AP
   Sun Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2097, DOI 10.1145/3503161.3547876
   Teng JN, 2019, MULTIMED TOOLS APPL, V78, P11155, DOI 10.1007/s11042-018-6695-9
   Uddin AM, 2021, IEEE REGION 10 SYMP, DOI 10.1109/TENSYMP52854.2021.9550874
   VijayaKumari G., 2022, Global Transit. Proc, V3, P225, DOI [10.1016/j.gltp.2022.03.027, DOI 10.1016/J.GLTP.2022.03.027]
   Wei PC, 2022, MULTIMEDIA SYST, V28, P2053, DOI 10.1007/s00530-020-00673-6
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Yadav S, 2021, IEEE 7 INT C ADV COM
   Yanai K, 2015, IEEE INT C MULTIMEDI
   Yigit GÖ, 2018, J INFORM TELECOMMUN, V2, P347, DOI 10.1080/24751839.2018.1446236
   Yin C., 2018, P 8 INT S COMP INT I, P1
   Ying X, 2019, J PHYS CONF SER, V1168, DOI 10.1088/1742-6596/1168/2/022022
   Zhidong Shen, 2020, Procedia Computer Science, V174, P448, DOI 10.1016/j.procs.2020.06.113
NR 56
TC 1
Z9 1
U1 4
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1931
EP 1946
DI 10.1007/s00371-023-02893-z
EA JUN 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000999829400001
DA 2024-07-18
ER

PT J
AU Chen, C
   Wang, YS
   Chen, HH
   Yan, XF
   Ren, DY
   Guo, YW
   Xie, HR
   Wang, FL
   Wei, MQ
AF Chen, Chen
   Wang, Yisen
   Chen, Honghua
   Yan, Xuefeng
   Ren, Dayong
   Guo, Yanwen
   Xie, Haoran
   Wang, Fu Lee
   Wei, Mingqiang
TI GeoSegNet: point cloud semantic segmentation via geometric
   encoder-decoder modeling
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE GeoSegNet; Point cloud semantic segmentation; Residual geometry module;
   Contrastive boundary learning
AB Semantic segmentation of point clouds, aiming to assign each point a semantic category, is critical to 3D scene understanding. Although significant advances in recent years, most of the existing methods still suffer from either the object-level mis-classification or the boundary-level ambiguity. In this paper, we present a robust semantic segmentation network by deeply exploring the geometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a multi-geometry-based encoder and a boundary-guided decoder. In the encoder, we develop a new residual geometry module from multi-geometry perspectives to extract object-level features. In the decoder, we introduce a contrastive boundary learning module to enhance the geometric representation of boundary points. Benefiting from the geometric encoder-decoder modeling, GeoSegNet infers the segmentation of objects effectively while making the intersections (boundaries) of two or more objects clear. GeoSegNet achieves a significant performance with 64.9% mIoU on the challenging S3DIS dataset (Area 5) and 70.2% mIoU on S3DIS sixfold. Experiments show obvious improvements of GeoSegNet over its competitors in terms of the overall segmentation accuracy and object boundary clearness. Code is available at https://github.com/Chen-yuiyui/GeoSegNet.
C1 [Chen, Chen; Wang, Yisen; Chen, Honghua; Yan, Xuefeng; Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Ren, Dayong; Guo, Yanwen] Nanjing Univ, Dept Comp Sci, Nanjing 210023, Peoples R China.
   [Xie, Haoran] Lingnan Univ, Dept Comp & Decis Sci, Hong Kong 999077, Peoples R China.
   [Wang, Fu Lee] Hong Kong Metropolitan Univ, Sch Sci & Technol, Hong Kong 999077, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University;
   Lingnan University; Hong Kong Metropolitan University
RP Yan, XF (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM chenxx@nuaa.edu.cn; eason@nuaa.edu.cn; chenhonghuacn@gmail.com;
   yxf@nuaa.edu.cn; rdyedu@gmail.com; yw.guo@nju.edu.cn; hrxie@ln.edu.hk;
   pwang@hkmu.edu.hk; mqwei@nuaa.edu.cn
RI Li, Kunpeng/KFS-6306-2024; wang, liangyu/KHD-1769-2024; ZHANG,
   JING/KHY-1073-2024; zhang, zheng/KHY-8870-2024; Yan,
   Xuefeng/JGL-6667-2023; Wang, Fu Lee/AAD-9782-2021; Xie,
   Haoran/AFS-3515-2022; Yan, Xin/KGL-5903-2024; Li, Bo/KHX-7246-2024; Yin,
   Jing/KDO-6274-2024
OI Wang, Fu Lee/0000-0002-3976-0053; Xie, Haoran/0000-0003-0965-3617; 
FU Basic Research for National Defense [JCKY2020605C003]
FX This work is supported by the Basic Research for National Defense under
   Grant Nos. JCKY2020605C003.
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Boulch A., 2017, 3DOR EUROGRAPHICS
   Boulch A, 2020, COMPUT GRAPH-UK, V88, P24, DOI 10.1016/j.cag.2020.02.005
   Boulch Alexandre, 2020, P AS C COMP VIS
   Cao Z., 2022, EUR C COMP VIS, P737
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fan SQ, 2021, PROC CVPR IEEE, P14499, DOI 10.1109/CVPR46437.2021.01427
   Gong JY, 2021, Arxiv, DOI arXiv:2101.02381
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Jiang MY, 2018, Arxiv, DOI [arXiv:1807.00652, 10.48550/arXiv.1807.00652]
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lawin FJ, 2017, LECT NOTES COMPUT SC, V10424, P95, DOI 10.1007/978-3-319-64689-3_8
   Li Y., 2018, Adv. Neural Inf. Process. Syst., V31
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Qi CR, 2017, ADV NEUR IN, V30
   Qian GC, 2022, Arxiv, DOI [arXiv:2206.04670, 10.48550/ARXIV.2206.04670]
   Ran H., 2022, P IEEECVF C COMPUTER, P18942
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Rosa S., 2020, 2020 P IEEECVF C COM, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Tang L., 2022, ARXIV, DOI 10.48550/arXiv.2203.05272
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu MY, 2021, AAAI CONF ARTIF INTE, V35, P3047
   Xu MY, 2020, AAAI CONF ARTIF INTE, V34, P12500
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Yan LQ, 2022, Arxiv, DOI arXiv:2205.10706
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20
   Zeyu Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P222, DOI 10.1007/978-3-030-58565-5_14
   Zhang DJ, 2020, INTEGR COMPUT-AID E, V27, P57, DOI 10.3233/ICA-190608
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
NR 44
TC 0
Z9 0
U1 4
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 MAY 29
PY 2023
DI 10.1007/s00371-023-02853-7
EA MAY 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H8LE4
UT WOS:000998403200001
DA 2024-07-18
ER

PT J
AU Liu, H
   Tian, SH
AF Liu, Hui
   Tian, Shuaihua
TI Deep 3D point cloud classification and segmentation network based on
   GateNet
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Machine vision; 3D point cloud; GateNet; SENet; Attention
   mechanism
AB With the gradual growth of deep learning in machine vision, efficient extraction of 3D point clouds becomes significant. The raw data of the 3D point cloud are sparse, disordered, and immersed in noise, which makes it difficult to classify and segment. Whether 3D point clouds can be classified and segmented or not, the local feature is an essential ingredient. Therefore, this paper proposes a GateNet-based PointNet++ network (G-PointNet++). G-PointNet++ extracts local features more accurately than PointNet++ by suppressing irrelevant features and emphasizing important features. Meanwhile, it refines the feature adaptively. Besides, the SENet and attention mechanism are introduced into PointNet++. G-PointNet++ was evaluated on the public ModelNet dataset, ShapeNet dataset, and S3DIS dataset, and its effectiveness in classification and segmentation tasks was verified. In the classification task, G-PointNet++ achieves an overall classification accuracy (OA) of 95.5% on ModelNet10 and 93.3% on ModelNet40. In the segmentation task, the mIoU of G-PointNet++ reaches 85.5% on ShapeNet. These experiments show that G-PointNet++ achieves better performance and saves more time than PointNet++, and its overall accuracy is higher than that of PCT network on ModeNet40.
C1 [Liu, Hui; Tian, Shuaihua] Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Dept Automat, Beijing 100044, Peoples R China.
C3 Beijing University of Civil Engineering & Architecture
RP Liu, H (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Dept Automat, Beijing 100044, Peoples R China.
EM liuhui@bucea.edu.cn; shuaihuatian@163.com
FU National Natural Science Foundation of China [62176018]
FX This work was funded by the National Natural Science Foundation of China
   (Under Grant: 62176018).
CR Abbasi R, 2023, IEEE T INTELL TRANSP, V24, P962, DOI 10.1109/TITS.2022.3167957
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109
   Chang A. X., 2015, ARXIV
   Chen LF, 2023, VISUAL COMPUT, V39, P863, DOI 10.1007/s00371-021-02351-8
   Cheraghian A, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), DOI [10.23919/MVA.2019.8758063, 10.23919/mva.2019.8758063]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hurtado J, 2023, VISUAL COMPUT, V39, P5823, DOI 10.1007/s00371-022-02698-6
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Jiang M., 2018, ARXIV180700652
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li X., 2021, INTERSPEECH 2021
   Li Y., 32 C NEUR INF PROC S
   Mahdaoui A., 2021, PROC SPIE
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Qi CR, 2017, ADV NEUR IN, V30
   Song YN, 2023, VISUAL COMPUT, V39, P1109, DOI 10.1007/s00371-021-02391-0
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Wang H., 2021, 2021 CHIN AUT C CAC
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xiao BL, 2022, VISUAL COMPUT, V38, P4373, DOI 10.1007/s00371-021-02301-4
   Xu S, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3141073
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Zhang QS, 2020, ELECTRON LETT, V56, P291, DOI 10.1049/el.2019.2856
   Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279, DOI 10.1109/ICASSP.2018.8462291
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhao MH, 2020, IEEE T IND INFORM, V16, P4681, DOI 10.1109/TII.2019.2943898
NR 33
TC 4
Z9 4
U1 33
U2 69
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 971
EP 981
DI 10.1007/s00371-023-02826-w
EA MAR 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000955719200001
DA 2024-07-18
ER

PT J
AU Chen, XL
   Lu, YB
   Cao, BN
   Lin, DM
   Ahmad, I
AF Chen, Xiaolei
   Lu, Yubing
   Cao, Baoning
   Lin, Dongmei
   Ahmad, Ishfaq
TI Lightweight head pose estimation without keypoints based on multi-scale
   lightweight neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Head pose estimation; Deep learning; Lightweight network; Convolutional
   neural networks
ID FORESTS
AB Head pose estimation methods without facial key points have emerged as a promising research field. However, there remain several unsolved challenges. For example, the current methods incur a computational cost, require large memory, and are difficult to deploy in practical applications. We propose a lightweight high-precision head pose estimation method based on a dual-stream convolutional neural network for overcoming these issues. The network comprises a dual-stream lightweight backbone network, external attention module, and soft stagewise regression (SSR) module. Dual-stream lightweight backbone network can extract original image features more effectively while keeping low computational overhead. External attention module can enhance the feature map extraction from the backbone network and improve the feature attention. SSR module calculates the probability of the head in each direction and predicts the head pose by regression. Extensive experiments on Annotated Facial Landmarks in the Wild (AFLW2000) and Biwi Kinect Head Pose Database (BIWI) datasets demonstrate that the model proposed in this paper has fewer parameters and lower estimation errors than the state-of-the-art methods in the field of head pose estimation in recent years.
C1 [Chen, Xiaolei; Lu, Yubing; Cao, Baoning; Lin, Dongmei] Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730000, Peoples R China.
   [Ahmad, Ishfaq] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX USA.
C3 Lanzhou University of Technology; University of Texas System; University
   of Texas Arlington
RP Chen, XL (corresponding author), Lanzhou Univ Technol, Coll Elect & Informat Engn, Lanzhou 730000, Peoples R China.
EM chenxl703@lut.edu.cn
RI Chen, Xiaolei/ABD-6837-2021
OI Chen, Xiaolei/0000-0001-9060-5369
FU National Natural Science Foundation of China [61967012, 61866022,
   61861027]; Science and Technology Program of Gansu Province [20JR5RA459]
FX AcknowledgementsThis study was supported by the National Natural Science
   Foundation of China (Nos. 61967012, 61866022, and 61861027) and the
   Science and Technology Program of Gansu Province (Grant No. 20JR5RA459).
CR Anisimov D, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Asad Syed Muhammad., 2020, 2020 International Conference on UK-China Emerging Technologies (UCET), P1
   BAHROUN S, 2021, VISUAL COMPUT
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao ZW, 2022, LECT NOTES COMPUT SC, V13672, P737, DOI 10.1007/978-3-031-19775-8_43
   Cao ZW, 2021, IEEE WINT CONF APPL, P1187, DOI 10.1109/WACV48630.2021.00123
   Chang FJ, 2017, IEEE INT CONF COMP V, P1599, DOI 10.1109/ICCVW.2017.188
   Chuan T., 2019, Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference, P123, DOI DOI 10.1145/3341069.3342979
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He L, 2015, NEUROCOMPUTING, V164, P210, DOI 10.1016/j.neucom.2015.02.068
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang B, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.005
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Khan K, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116479
   Kumar A, 2017, IEEE INT CONF AUTOMA, P258, DOI 10.1109/FG.2017.149
   Lee S, 2018, LECT NOTES ELECTR EN, V449, P164, DOI 10.1007/978-981-10-6451-7_20
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang D, 2022, MULTIMED TOOLS APPL, V81, P35411, DOI 10.1007/s11042-022-12319-y
   Liang XC, 2023, VISUAL COMPUT, V39, P2277, DOI 10.1007/s00371-022-02413-5
   Liu HB, 2021, INTERVIROLOGY, V64, P126, DOI [10.1109/TPWRD.2021.3054889, 10.1159/000513687, 10.1109/TMM.2021.3081873]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma X., 2020, arXiv
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Stergiou A, 2021, ARXIV
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Tan MX, 2019, PR MACH LEARN RES, V97
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xin M, 2021, IEEE COMPUT SOC CONF, P1462, DOI 10.1109/CVPRW53098.2021.00162
   Xu LH, 2019, NEUROCOMPUTING, V337, P339, DOI 10.1016/j.neucom.2018.12.074
   Xu X, 2017, IEEE INT CONF AUTOMA, P642, DOI 10.1109/FG.2017.81
   Yang S, 2023, VISUAL COMPUT, V39, P6015, DOI 10.1007/s00371-022-02709-6
   Yang TY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1078
   Yang TY, 2019, PROC CVPR IEEE, P1087, DOI 10.1109/CVPR.2019.00118
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12789
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 45
TC 0
Z9 0
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2455
EP 2469
DI 10.1007/s00371-023-02781-6
EA FEB 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J1YY4
UT WOS:000924564400001
DA 2024-07-18
ER

PT J
AU Han, L
   Shi, X
   He, JH
   Ma, HW
   Dou, F
   Zhao, HK
AF Han, Li
   Shi, Xue
   He, Jinhai
   Ma, Huiwen
   Dou, Feng
   Zhao, Hongkai
TI Generalized unsupervised functional map learning for dense
   correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Shape correspondence; Functional map; Unsupervised
   learning; Point clouds
ID UNIQUE SIGNATURES; HISTOGRAMS; SURFACE; SHAPE
AB Inspired by deep functional map methods, we present a generalized unsupervised functional map learning approach for arbitrary 3D shape correspondence. Unlike prior methods, they either require extensive data training or rely on input features; our model directly operates on point clouds and learns both deep features and optimized basis function without the constraint of geometric connectivity and the assumption of isometry. We propose a novel scheme that combines structural embedding based on Mahalanobis distance and locally linear embedding to learn the optimized feature basis. Furthermore, the constructed shape descriptors effectively optimize the estimation of functional map and dense correspondence through a tri-level regularization mechanism that enforces penalties on global structural properties, representation error and pair-wise Mahalanobis distance distortion, which significantly improves the performance of unsupervised learning. Extensive experiments in shape matching show that our method can learn from less training data and has better generalization ability compared with the state-of-the-art supervised and unsupervised methods.
C1 [Han, Li; Shi, Xue; He, Jinhai; Ma, Huiwen; Dou, Feng] Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian 116021, Peoples R China.
   [Zhao, Hongkai] Univ Calif Irvine, Dept Math, Irvine, CA USA.
C3 Liaoning Normal University; University of California System; University
   of California Irvine
RP Han, L (corresponding author), Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian 116021, Peoples R China.
EM lhan98@lnnu.edu.cn
RI Dou, Feng/JQJ-6686-2023
FU NSFC [61702246]; Research projects of Liaoning province [2019lsktyb-084,
   LJ2020015, 2020JH4/10100045]; Fund of Dalian Science and Technology
   [2019J12GX038]
FX We would like to thank the anonymous reviewers for their helpful
   comments. The research presented in this paper is supported by a grant
   from NSFC (61702246), grants from research projects of Liaoning province
   (2019lsktyb-084, LJ2020015, 2020JH4/10100045) and a fund of Dalian
   Science and Technology (2019J12GX038).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Aflalo Y, 2016, INT J COMPUT VISION, V118, P380, DOI 10.1007/s11263-016-0883-8
   Aflalo Y, 2015, SIAM J IMAGING SCI, V8, P1141, DOI 10.1137/140977680
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Aygun M., 2020, UNSUPERVISED DENSE S
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Burghard O, 2017, COMPUT GRAPH-UK, V68, P1, DOI 10.1016/j.cag.2017.06.004
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Donati Nicolas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8589, DOI 10.1109/CVPR42600.2020.00862
   Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P165, DOI 10.1111/cgf.13254
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Groueix T., 2018, 3D CODED 3D CORRES D
   Halimi O, 2019, PROC CVPR IEEE, P4365, DOI 10.1109/CVPR.2019.00450
   Huang QX, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601111
   KIM V, 2012, ACM T GRAPHIC, V31, P4, DOI DOI 10.1145/2185520.2185550
   Kovnatsky A, 2013, COMPUT GRAPH FORUM, V32, P439, DOI 10.1111/cgf.12064
   Kovnatsky A, 2015, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2015.7298692
   Küpçü E, 2019, COMPUT VIS IMAGE UND, V189, DOI 10.1016/j.cviu.2019.102808
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Muruganathan S., 2014, Journal of Computer Science, V10, P552, DOI 10.3844/jcssp.2014.552.562
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Qi CR, 2017, ADV NEUR IN, V30
   Ren J., 2020, STRUCTURED REGULARIZ
   Ren J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275040
   Rodola E., 2017, Computer Graphics Forum, V36, P222, DOI 10.1111/cgf.12797
   Rodola E., 2015, COMPUT SCI
   Roufosse JM, 2019, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2019.00170
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Vestner M, 2017, PROC CVPR IEEE, P6681, DOI 10.1109/CVPR.2017.707
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Ye JB, 2016, VISUAL COMPUT, V32, P553, DOI 10.1007/s00371-015-1071-5
   Yi L., 2019, P SIGGRAPH ASIA
   Zeng YM, 2021, PROC CVPR IEEE, P6048, DOI 10.1109/CVPR46437.2021.00599
NR 44
TC 0
Z9 0
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6625
EP 6638
DI 10.1007/s00371-022-02752-3
EA JAN 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000920933400001
DA 2024-07-18
ER

PT J
AU Zhang, L
   Yao, HL
   Tan, JQ
AF Zhang, Li
   Yao, Hongli
   Tan, Jieqing
TI A class of nonstationary interproximate subdivision algorithm for
   interpolating feature data points
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Feature points; Nonstationary; Binary; Interproximate
   subdivision
ID SCHEME; SPLINES; C-2
AB Subdivision algorithm is an efficient way to reconstruct curves and surfaces for discrete data points which can reconstruct feature and edge contour for edge extraction. By introducing local pushback operation, a brand-new interproximate subdivision framework is established which bridges the gap between interpolating schemes and approximating ones. This subdivision framework not only contains most existing schemes, but also includes some new schemes. Then, a class of nonstationary interproximate subdivision schemes are proposed. Properties of the schemes such as continuity and convexity are further discussed. With the help of the framework, limit curves can freely interpolate given points and approximate the other ones if data points have been divided into interpolating points and approximating ones by certain rules. Numerical examples show that the proposed approach can faithfully interpolate feature data points and obtain better visual effects than the ones obtained via other known schemes.
C1 [Zhang, Li; Yao, Hongli; Tan, Jieqing] Hefei Univ Technol, Sch Math, Hefei 230601, Peoples R China.
C3 Hefei University of Technology
RP Zhang, L (corresponding author), Hefei Univ Technol, Sch Math, Hefei 230601, Peoples R China.
EM lizhang@hfut.edu.cn
RI Tan, Jie/IVV-5250-2023
FU National Key Research and Development Program [2018YFB2100301]; National
   Natural Science Foundation of China [61972131]
FX This work is supported by the National Key Research and Development
   Program (2018YFB2100301); National Natural Science Foundation of China
   (61972131).
CR Akram G, 2017, J COMPUT APPL MATH, V319, P480, DOI 10.1016/j.cam.2017.01.026
   Bazazian D, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P358
   Beccari C, 2007, COMPUT AIDED GEOM D, V24, P1, DOI 10.1016/j.cagd.2006.10.003
   Beccari CV, 2011, BIT, V51, P781, DOI 10.1007/s10543-011-0328-2
   Bello SA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12111729
   Chaikin GM., 1974, COMPUT GRAPHICS IMAG, V3, P346, DOI DOI 10.1016/0146-664X(74)90028-8
   CHENG F, 1991, COMPUT AIDED DESIGN, V23, P700, DOI 10.1016/0010-4485(91)90023-P
   COHEN E, 1980, COMPUT VISION GRAPH, V14, P87, DOI 10.1016/0146-664X(80)90040-4
   Conti C, 2010, BIT, V50, P269, DOI 10.1007/s10543-010-0263-7
   Conti C, 2009, LINEAR ALGEBRA APPL, V431, P1971, DOI 10.1016/j.laa.2009.06.037
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   do Carmo MP., 1992, RIEMANNIAN GEOMETRY
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1995, J MATH ANAL APPL, V193, P594, DOI 10.1006/jmaa.1995.1256
   Dyn N., 1987, Computer-Aided Geometric Design, V4, P257, DOI 10.1016/0167-8396(87)90001-X
   Dyn N., 1992, Advances in Numerical Analysis, VII, P36
   Dyn N., 2004, Mathematical Methods for Curves and Surfaces: Tromso 2005, P145
   Floater MS, 2011, J COMPUT APPL MATH, V236, P476, DOI 10.1016/j.cam.2011.03.018
   Hassan MF, 2002, COMPUT AIDED GEOM D, V19, P1, DOI 10.1016/S0167-8396(01)00084-X
   He XC, 2008, OPT ENG, V47, DOI 10.1117/1.2931681
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   Li G, 2007, COMPUT GRAPH FORUM, V26, P185, DOI 10.1111/j.1467-8659.2007.01015.x
   Li X, 2013, J COMPUT APPL MATH, V244, P36, DOI 10.1016/j.cam.2012.11.012
   Maillot J, 2001, COMPUT GRAPH FORUM, V20, pC471, DOI 10.1111/1467-8659.00540
   Mao AH, 2016, VISUAL COMPUT, V32, P1085, DOI 10.1007/s00371-015-1175-y
   Novara P, 2016, APPL MATH LETT, V62, P84, DOI 10.1016/j.aml.2016.07.004
   Romani L, 2009, J COMPUT APPL MATH, V224, P383, DOI 10.1016/j.cam.2008.05.013
   Romani L, 2016, IMA J NUMER ANAL, V36, P380, DOI 10.1093/imanum/drv008
   Romani L, 2015, COMPUT AIDED GEOM D, V32, P22, DOI 10.1016/j.cagd.2014.11.002
   Tan JQ, 2015, APPL MATH COMPUT, V265, P819, DOI 10.1016/j.amc.2015.05.107
   Tan JQ, 2016, APPL MATH COMPUT, V276, P37, DOI 10.1016/j.amc.2015.12.002
   Wang Z., 2021, Proc., P1, DOI DOI 10.1109/VTC2021-FALL52928.2021.9625159
   Zhang L, 2019, J COMPUT APPL MATH, V349, P563, DOI 10.1016/j.cam.2018.09.014
   Zheng HC, 2017, APPL MATH COMPUT, V313, P209, DOI 10.1016/j.amc.2017.05.066
NR 34
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2441
EP 2454
DI 10.1007/s00371-022-02722-9
EA NOV 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J1YY4
UT WOS:000888681400001
DA 2024-07-18
ER

PT J
AU Zhong, JY
   Cao, Y
   Zhu, YN
   Gong, J
   Chen, QS
AF Zhong, Jingyue
   Cao, Yang
   Zhu, Yina
   Gong, Jie
   Chen, Qiaosen
TI Multi-channel weighted fusion for image captioning
SO VISUAL COMPUTER
LA English
DT Article
DE Image captioning; Multi-channel encoder; Weighted fusion; Dimension
   reduction
ID ATTENTION
AB Automatically describing the detail and content of the image is a meaningful but difficult task. In this paper, we propose a variety of optimization improvements to enhance the encoder and decoder for image captioning, called multi-channel weighted fusion. In the presented model, we propose multi-channel encoder which is able to extract different features of the same image by combining various models and algorithms. In order to avoid dimensional explosion caused by multi-channel encoder, we employ the reducing multilayer perceptron to reduce the dimension and discuss how to train the reducing multilayer perceptron. For the decoder part, we discuss how the decoder receives features from different channels and propose a technique for fusing independent and identically typed decoders. To get a better description generated by the decoder, we exploit the voting weight strategy for decoder fusion and explore the entropy function to choose the best distribution. The experiment on datasets Flickr-8k, Flickr-30k and MS COCO demonstrates that the proposed model is compatible with most features with low error rate. For instance, our model is specifically outstanding on METEOR score.
C1 [Zhong, Jingyue; Cao, Yang; Zhu, Yina; Gong, Jie; Chen, Qiaosen] South China Normal Univ, Sch Comp Sci, Guangzhou 510630, Guangdong, Peoples R China.
C3 South China Normal University
RP Cao, Y (corresponding author), South China Normal Univ, Sch Comp Sci, Guangzhou 510630, Guangdong, Peoples R China.
EM yangcao@scnu.edu.cn
OI Cao, Yang/0000-0002-4758-6091
FU Science and Technology on Information System Engineering Laboratory
   [WDZC20205250410]; Key-Area Research and Development Program of
   Guangdong Province [2019B111101001]
FX This work was supported by Science and Technology on Information System
   Engineering Laboratory [WDZC20205250410] and Key-Area Research and
   Development Program of Guangdong Province under Grant [2019B111101001].
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   [Anonymous], 2005, P 2 WORKSH STAT MACH
   Asad M, 2021, VISUAL COMPUT, V37, P1415, DOI 10.1007/s00371-020-01878-6
   Barlas G, 2021, VISUAL COMPUT, V37, P1309, DOI 10.1007/s00371-020-01867-9
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Cao PF, 2019, NEURAL PROCESS LETT, V50, P103, DOI 10.1007/s11063-018-09973-5
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chung J., 2014, ARXIV PREPRINT ARXIV, P1412
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Dosovitskiy A., 2021, INT C LEARNING REPRE, P2010
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Hazgui M, 2022, VISUAL COMPUT, V38, P457, DOI 10.1007/s00371-020-02028-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang T, 2019, VISUAL COMPUT, V35, P1655, DOI 10.1007/s00371-018-1565-z
   Kingma D. P., 2015, 3 INT C LEARN REPR I
   Leng L, 2017, MULTIMED TOOLS APPL, V76, P333, DOI 10.1007/s11042-015-3058-7
   Leng L, 2013, NEUROCOMPUTING, V108, P1, DOI 10.1016/j.neucom.2012.08.028
   Li X, 2022, IET COMPUT VIS, V16, P280, DOI 10.1049/cvi2.12087
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao J., 2015, INT C LEARNING REPRE, P1412
   Mason R, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P592
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Nogueira TDC, 2020, MULTIMED TOOLS APPL, V79, P30615, DOI 10.1007/s11042-020-09539-5
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang QZ, 2022, IEEE T PATTERN ANAL, V44, P1035, DOI 10.1109/TPAMI.2020.3013834
   Wei HY, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103068
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu Y., 2019, DETECTRON2
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P2313, DOI 10.1109/TPAMI.2020.3042192
   Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang J, 2021, EXPERT SYST APPL, V184, DOI 10.1016/j.eswa.2021.115462
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou L, 2020, IEEE T IMAGE PROCESS, V29, P694, DOI 10.1109/TIP.2019.2928144
NR 56
TC 1
Z9 1
U1 5
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6115
EP 6132
DI 10.1007/s00371-022-02716-7
EA NOV 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000883235000001
DA 2024-07-18
ER

PT J
AU Luo, L
   Weng, DD
   Ding, N
   Hao, J
   Tu, ZQ
AF Luo, Le
   Weng, Dongdong
   Ding, Ni
   Hao, Jie
   Tu, Ziqi
TI The effect of avatar facial expressions on trust building in social
   virtual reality
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Face expression; Trust game; Avatar
ID EMOTION; COMMUNICATION
AB In immersive virtual reality (VR) applications, the facial expressions and lip-syncing of an avatar can have a significant impact on a user's experience. In this paper, we designed a VR "trust game" scene to evaluate the effects of four expression conditions (positive facial expressions, neutral facial expressions, negative facial expressions and no expressions and lip-syncing) on participants in an immersive VR scene. We measured the participants with both objective and subjective measures. The two objective behavioral measures were the level of investment in the "trust game" and the users' eye-movement data, and the subjective measures included social presence, emotional awareness level, and user preferences. We found that the participants were generally less trusting of the avatars with negative expressions, while the avatars with positive expressions made the participants feel comfortable and thus increased their willingness to cooperate with the avatars. In conclusion, avatars with facial expressions, whether positive or negative, were more effective in influencing the participants' trust levels and decision-making behaviors than those without facial expressions. These findings provide novel ideas and suggestions for improving the level of human-computer interaction in VR and enhancing user experience in VR scenes.
C1 [Luo, Le; Weng, Dongdong; Hao, Jie; Tu, Ziqi] Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100081, Peoples R China.
   [Ding, Ni] Beijing Normal Univ, Sch Arts & Commun, Beijing 100875, Peoples R China.
C3 Beijing Institute of Technology; Beijing Normal University
RP Weng, DD (corresponding author), Beijing Inst Technol, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100081, Peoples R China.; Ding, N (corresponding author), Beijing Normal Univ, Sch Arts & Commun, Beijing 100875, Peoples R China.
EM l_luo@bit.edu.cn; crgj@bit.edu.cn; dingni@bnu.edu.cn; haoj@bit.edu.cn;
   tzq1991@bit.edu.cn
OI Hao, Jie/0000-0003-1730-4847
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010149001]; National Natural Science Foundation of China
   [62072036]; 111 Project [B18005]
FX This work was supported by the Key-Area Research and Development Program
   of Guangdong Province (No. 2019B010149001) and the National Natural
   Science Foundation of China (No. 62072036) and the 111 Project (B18005).
CR Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   Alexander O, 2010, IEEE COMPUT GRAPH, V30, P20, DOI 10.1109/MCG.2010.65
   [Anonymous], 2003, P ACM SIGCHI C HUM F, DOI [DOI 10.1145/642611.642703, 10.1145/642611.642703]
   [Anonymous], 2021, METAHUMAN CREATOR
   Aseeri S, 2021, IEEE T VIS COMPUT GR, V27, P2608, DOI 10.1109/TVCG.2021.3067783
   Baker Steven, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359251
   BERG J, 1995, GAME ECON BEHAV, V10, P122, DOI 10.1006/game.1995.1027
   Biocca P.F., 2003, GUIDE NETWORKED MIND
   BOYLE GJ, 1984, PERS INDIV DIFFER, V5, P747, DOI 10.1016/0191-8869(84)90124-7
   Burgos-Artizzu Xavier P, 2015, Kobe, Japan) 15). SIGGRAPH Asia 2015 Technical Briefs, P1, DOI [DOI 10.1145/2820903.2820910, 10.1145/2820903.2820910]
   Meneses JAC, 2017, PSICOLOGICA, V38, P1
   Cho S, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P26, DOI [10.1109/VR46266.2020.1581170537418, 10.1109/VR46266.2020.00-84]
   Debevec P., 2002, ACM Transactions on Graphics, V21, P547, DOI 10.1145/566570.566614
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   EKMAN P, 1979, ANNU REV PSYCHOL, V30, P527, DOI 10.1146/annurev.ps.30.020179.002523
   Friesen E., 1978, Environmental Psychology & Nonverbal Behavior, V3, P5, DOI 10.1037/t27734-000
   Games, 2021, RECORDING FACIAL ANI
   George C, 2018, COMPANION OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'18), DOI 10.1145/3180308.3180355
   Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163
   Gonzalez-Franco M, 2020, IEEE T VIS COMPUT GR, V26, P2023, DOI 10.1109/TVCG.2020.2973075
   Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722
   Harjunen VJ, 2018, COMPUT HUM BEHAV, V87, P384, DOI 10.1016/j.chb.2018.06.012
   Harms P.C., 2004, INTERNAL CONSISTENCY
   Hart JD, 2018, PROCEEDINGS OF THE 2018 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY COMPANION EXTENDED ABSTRACTS (CHI PLAY 2018), P453, DOI 10.1145/3270316.3271543
   Heidicker P, 2017, IEEE SYMP 3D USER, P233, DOI 10.1109/3DUI.2017.7893357
   Izard C E, 1978, Nebr Symp Motiv, V26, P163
   Jaeger B, 2019, J EXP PSYCHOL GEN, V148, P1008, DOI 10.1037/xge0000591
   Kampouris C., 2018, EGSR (EI&I), P1
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Krach S, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0002597
   Krumhuber E, 2007, EMOTION, V7, P730, DOI 10.1037/1528-3542.7.4.730
   Kugler T, 2020, SOC PSYCHOL PERS SCI, V11, P317, DOI 10.1177/1948550619856302
   Latoschik ME, 2017, VRST'17: PROCEEDINGS OF THE 23RD ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, DOI 10.1145/3139131.3139156
   Mathur M. B., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P313
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Mussel P, 2013, JUDGM DECIS MAK, V8, P381
   Olszewski K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980252
   Pan Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0189078
   Paradeda RB, 2016, LECT NOTES ARTIF INT, V9979, P169, DOI 10.1007/978-3-319-47437-3_17
   Pauw LS, 2022, COMPUT HUM BEHAV, V136, DOI 10.1016/j.chb.2022.107368
   Russell J A., 1997, The psychology of facial expression, DOI [DOI 10.1017/CBO9780511659911, 10.1017/CB09780511659911]
   Scharlemann JPW, 2001, J ECON PSYCHOL, V22, P617, DOI 10.1016/S0167-4870(01)00059-9
   Shao D, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12229345
   Smith HJ, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173863
   Stouten J, 2010, J BEHAV DECIS MAKING, V23, P271, DOI 10.1002/bdm.659
   Suzuki K, 2017, P IEEE VIRT REAL ANN, P177, DOI 10.1109/VR.2017.7892245
   Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699
   Tortosa MI, 2013, PSICOLOGICA, V34, P179
   Tu ZQ, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P217, DOI 10.1109/ISMAR-Adjunct.2019.00-43
   Wei SE, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323030
   Wenger A, 2005, ACM T GRAPHIC, V24, P756, DOI 10.1145/1073204.1073258
   Wu J., 2016, Trust and cooperation in humanrobot decision making
   Yoon B, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P547, DOI [10.1109/vr.2019.8797719, 10.1109/VR.2019.8797719]
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 54
TC 2
Z9 2
U1 8
U2 44
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5869
EP 5882
DI 10.1007/s00371-022-02700-1
EA OCT 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000870666700001
DA 2024-07-18
ER

PT J
AU Zhang, Y
AF Zhang, Yong
TI A unified image cryptography based on a perceptron-like network
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; Information security; Unified cryptography;
   Perceptron-like network; Neural network
ID HENON-SINE MAP; CRYPTANALYSIS; COMBINATION
AB Image encryption is the core technology of secure storage and transmission of image information. Based on a perceptron-like network of integer domain, a new unified image encryption algorithm is proposed. Its encryption algorithm and decryption algorithm are the same, both including an exclusive-or module, two types of perceptron-like networks and a sequence-flip module. The image data are diffused and encrypted by the forward propagation algorithm of neural network, and the historical data of the image are memorized and updated by the backward propagation algorithm of the network. The unified image cryptosystem uses a 512-bit key with the encryption speed up to 13.794Mbps on the computer equipped with an i7-9750H CPU. The maximum relative error of each system sensitivity index is less than 0.1%. The system requires the pseudorandom numbers by only half the size of the image. Simulated tests and comparative analyses show that the unified image cryptosystem possesses excellent characteristics of high speed and strong security strength. The proposed system can be used as a candidate safety method for network image information security.
C1 [Zhang, Yong] Jiangxi Univ Finance & Econ, Sch Software & Internet Things Engn, Nanchang 330013, Jiangxi, Peoples R China.
C3 Jiangxi University of Finance & Economics
RP Zhang, Y (corresponding author), Jiangxi Univ Finance & Econ, Sch Software & Internet Things Engn, Nanchang 330013, Jiangxi, Peoples R China.
EM zhangyong@jxufe.edu.cn
OI Zhang, Yong/0000-0002-7428-1816
FU National Natural Science Foundation of China [61762043]; Natural Science
   Foundation of Jiangxi Province [20192BAB207022]; Scientific Research
   Foundation of Jiangxi Provincial Education Department [GJJ190249,
   GJJ210507]
FX This work was fully supported by National Natural Science Foundation of
   China (No. 61762043), Natural Science Foundation of Jiangxi Province
   (No. 20192BAB207022), and Scientific Research Foundation of Jiangxi
   Provincial Education Department (No. GJJ190249, GJJ210507).
CR Cao C, 2018, SIGNAL PROCESS, V143, P122, DOI 10.1016/j.sigpro.2017.08.020
   Chai XL, 2019, NEURAL COMPUT APPL, V31, P219, DOI 10.1007/s00521-017-2993-9
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Chen JX, 2018, NONLINEAR DYNAM, V93, P2399, DOI 10.1007/s11071-018-4332-9
   Daoui A, 2022, EXPERT SYST APPL, V190, DOI 10.1016/j.eswa.2021.116193
   Fang PF, 2023, VISUAL COMPUT, V39, P1975, DOI 10.1007/s00371-022-02459-5
   Feng W, 2019, IEEE ACCESS, V7, P12584, DOI 10.1109/ACCESS.2019.2893760
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Hu GZ, 2021, NONLINEAR DYNAM, V103, P2819, DOI 10.1007/s11071-021-06228-2
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2016, INFORM SCIENCES, V339, P237, DOI 10.1016/j.ins.2016.01.017
   Huang XL, 2018, MULTIMED TOOLS APPL, V77, P2611, DOI 10.1007/s11042-017-4455-x
   Jiang N, 2019, INT J THEOR PHYS, V58, P979, DOI 10.1007/s10773-018-3989-7
   Li M, 2019, IEEE ACCESS, V7, P63336, DOI 10.1109/ACCESS.2019.2916402
   LORENZ EN, 1976, QUATERNARY RES, V6, P495, DOI 10.1016/0033-5894(76)90022-3
   Murali P, 2023, VISUAL COMPUT, V39, P1057, DOI 10.1007/s00371-021-02384-z
   Nkandeu YPK, 2019, MULTIMED TOOLS APPL, V78, P10013, DOI 10.1007/s11042-018-6612-2
   Pak C, 2017, SIGNAL PROCESS, V138, P129, DOI 10.1016/j.sigpro.2017.03.011
   Panwar K, 2019, INT J BIFURCAT CHAOS, V29, DOI 10.1142/S0218127419501037
   Rakheja P, 2020, OPT QUANT ELECTRON, V52, DOI 10.1007/s11082-020-2219-8
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Wang XY, 2021, OPT LASER TECHNOL, V138, DOI 10.1016/j.optlastec.2020.106837
   Wu JH, 2018, SIGNAL PROCESS, V153, P11, DOI 10.1016/j.sigpro.2018.06.008
   Wu XJ, 2015, APPL SOFT COMPUT, V37, P24, DOI 10.1016/j.asoc.2015.08.008
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Zhang Y, 2020, INFORM SCIENCES, V526, P180, DOI 10.1016/j.ins.2020.03.054
   Zhang Y, 2018, KSII T INTERNET INF, V12, P4487, DOI 10.3837/tiis.2018.09.020
   Zhang Y, 2018, INFORM SCIENCES, V450, P361, DOI 10.1016/j.ins.2018.03.055
NR 29
TC 1
Z9 1
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4985
EP 5000
DI 10.1007/s00371-022-02641-9
EA AUG 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000840387400001
DA 2024-07-18
ER

PT J
AU Zhang, JX
   Xie, W
   Wang, C
   Tu, RD
   Tu, ZG
AF Zhang, Jiaxu
   Xie, Wei
   Wang, Chao
   Tu, Ruide
   Tu, Zhigang
TI Graph-aware transformer for skeleton-based action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Skeleton action recognition; Visual transformer; Graph-aware
   transformer; Velocity information of human body joints; Graph neural
   network
AB Recently, graph convolutional networks (GCNs) play a critical role in skeleton-based human action recognition. However, most GCN-based methods still have two main limitations: (1) The semantic-level adjacency matrix of the skeleton graph is difficult to be manually defined, which restricts the perception field of GCN and limits its ability to extract the spatial-temporal features. (2) The velocity information of human body joints cannot be efficiently used and fully exploited by GCN, because GCN does not represent the correlation between the velocity vectors explicitly. To address these issues, we propose a graph-aware transformer (GAT), which can make full use of the velocity information and learn discriminative spatial-temporal motion features from the sequence of the skeleton graphs in a data-driven way. Besides, similar to the GCN-based model, our GAT also considers the prior structures of the human body including the link-aware structure and the part-aware structure. Extensive experiments on three large-scale datasets, i.e., NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-Skeleton, demonstrated that the proposed GAT obtains significant improvement compared to the GCN-based baseline for skeleton action recognition.
C1 [Zhang, Jiaxu; Wang, Chao; Tu, Zhigang] Wuhan Univ, State Key Lab Informat Engn Surveying, Wuhan 430072, Hubei, Peoples R China.
   [Xie, Wei] Cent China Normal Univ, Sch Comp, Wuhan 430079, Hubei, Peoples R China.
   [Tu, Ruide] Cent China Normal Univ, Sch Informat Management, Wuhan 430079, Hubei, Peoples R China.
C3 Wuhan University; Central China Normal University; Central China Normal
   University
RP Wang, C (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying, Wuhan 430072, Hubei, Peoples R China.; Tu, RD (corresponding author), Cent China Normal Univ, Sch Informat Management, Wuhan 430079, Hubei, Peoples R China.
EM zjiaxu@whu.edu.cn; XW@mail.ccnu.edu.cn; c.wang@whu.edu.cn;
   turuide@mails.ccnu.edu.cn; zhigangtu@whu.edu.cn
RI hu, xin/KHT-2406-2024; Tu, Zhigang/AAG-2255-2020
OI Tu, Zhigang/0000-0001-5003-2260
FU National Natural Science Foundation of China [62106177]; Joint Fund of
   the Ministry of Education of China [8091B032156]
FX Y This work was supported by the National Natural Science Foundation of
   China under Grant No. 62106177. It was also supported by the Joint Fund
   of the Ministry of Education of China under Grant No. 8091B032156. The
   numerical calculation was supported by the supercomputing system in the
   Super-computing Center of Wuhan University.
CR Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Caetano C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909840
   Caetano C, 2019, SIBGRAPI, P16, DOI 10.1109/SIBGRAPI.2019.00011
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen H, 2020, ARXIV
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chenyang Si, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P35, DOI 10.1007/978-3-030-58571-6_3
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Dai Z., ARXIV
   Demisse GG, 2018, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2018.00056
   Devlin J., 2018, BERT PRE TRAINING DE
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Duan H., ARXIV
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu Y., 2017, BMVC
   Kay W., 2017, CORR ABS170506950
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu Juncheng., 2017, P IEEE C COMPUTER VI, P792, DOI DOI 10.1109/CVPR.2017.391
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Parmar N., 2020, ARXIV
   Paszke A, 2019, ADV NEUR IN, V32
   Peng GZ, 2019, AAAI CONF ARTIF INTE, P8827
   Peng WT, 2020, IEEE INT C ELECTR TA, DOI 10.1109/icce-taiwan49838.2020.9258245
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Straka M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.69
   Sun Z, 2020, ARXIV
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang Y, 2020, ARXIV
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Wu B., 2020, Visual transformers: Token-based image representation and processing for computer vision
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Xu ZM, 2019, IEEE T NEUR NET LEAR, V30, P2951, DOI 10.1109/TNNLS.2018.2886008
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yuan XH, 2017, IEEE-CAA J AUTOMATIC, V4, P677, DOI 10.1109/JAS.2017.7510625
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zengeler N, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010059
   Zhang DJ, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107312
   Zhang JG, 2017, IEEE T CYBERNETICS, V47, P960, DOI 10.1109/TCYB.2016.2535122
   Zhang JX, 2022, CAAI T INTELL TECHNO, V7, P46, DOI 10.1049/cit2.12012
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang X., 2020, P IEEE CVF C COMP VI, P14333, DOI DOI 10.1109/CVPR42600202001434
   Zhang XQ, 2021, IEEE T NEUR NET LEAR, V32, P3005, DOI 10.1109/TNNLS.2020.3009209
   Zhang XK, 2020, IEEE T NEUR NET LEAR, V31, P3047, DOI 10.1109/TNNLS.2019.2935173
   Zhao H., 2020, ARXIV
   Zhao R, 2019, IEEE I CONF COMP VIS, P6881, DOI 10.1109/ICCV.2019.00698
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zheng W, 2019, IEEE INT CON MULTI, P826, DOI 10.1109/ICME.2019.00147
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 79
TC 6
Z9 8
U1 11
U2 59
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4501
EP 4512
DI 10.1007/s00371-022-02603-1
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000830367500001
DA 2024-07-18
ER

PT J
AU Yang, PQ
   Wang, MJ
   Yuan, H
   He, C
   Cong, L
AF Yang, Peiqi
   Wang, Mingjun
   Yuan, Hao
   He, Ci
   Cong, Li
TI Using contour loss constraining residual attention U-net on optical
   remote sensing interpretation
SO VISUAL COMPUTER
LA English
DT Article
DE Remote sensing; Image interpretation; Loss function; Semantic
   segmentation; U-Net; Residual attention mechanism
ID SEMANTIC SEGMENTATION; NETWORKS
AB Using deep learning in remote sensing interpretation could reduce a lot of human and material costs. Semantic segmentation is the main method for this task. It can automatically outline the objects and it has recently achieved great success in remote sensing images. However, in the appliance of remote sensing interpretation, the accuracy of contour largely determines the evaluation of remote sensing interpretation. Though the current loss functions reflect the segmentation performance, they could not guide the model to optimize itself toward a more precise contour. This paper proposed an exactly defined contour loss (CL) for remote sensing interpretation with Residual Attention U-Net (RA U-Net) as the main framework. The RA U-Net uses the residual attention module as the skip connection layer. It enhances the judgment of U-Net. In CL, image processing methods are used to extract the contours of the foreground. And elements-sum and elements-subtract operations are used to transfer the contour information to a matrix of the same size as label images. Then, these matrices would be the weights for CE. By assigning different weights for different elements in different regions, this function will guide the model to reach a balance between accurate segmentation results and precise contours. The experiment on open datasets shows a good performance. The proposed model was also trained on the Construction Disturbance Dataset collected from Jiang Xi Province, China. The dataset was labeled manually. The evaluation enhanced a lot on the Construction Disturbance Dataset and the IoU on two datasets increased 1% to 2% when using CL as the loss function. This paper also compared the proposed method with other state-of-the-art methods and the results showed extensive effectiveness.
C1 [Yang, Peiqi; Wang, Mingjun] Xian Univ Technol, Sch Automat & Informat Engn, Xian 710048, Peoples R China.
   [Yuan, Hao] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [He, Ci] Sci & Technol Commun Networks Lab, Shijiazhuang 050000, Hebei, Peoples R China.
   [He, Ci] 54th Res Inst China Elect Technol Grp Corp, Shijiazhuang 050000, Hebei, Peoples R China.
   [Cong, Li] State Grid JiLin Prov Elect Power Co Ltd, Informat Commun Co, Changchun 130000, Peoples R China.
C3 Xi'an University of Technology; Xidian University; China Electronics
   Technology Group; State Grid Corporation of China
RP Yuan, H (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM yuanhao08@126.com
RI Yuan, Hao/HPD-3071-2023
OI Yuan, Hao/0000-0002-9061-9805
FU National Key Research and Development Program of China [2019YFE0196600];
   National Natural Science Foundation of China [62072360, 61902292,
   62001357, 62072359, 62172438]; key research and development plan of
   Shaanxi Province [2019ZDLGY13-07, 2019ZDLGY13-04, 2020JQ-844]; Natural
   Science Foundation of Guangdong Province of China [2022A1515010988];
   Xi'an Science and Technology Plan [20RGZN0005]; Xi'an Key Laboratory of
   Mobile Edge Computing and Security [201805052-ZD3CG36]
FX This work was supported by the National Key Research and Development
   Program of China (2019YFE0196600), the National Natural Science
   Foundation of China (62072360, 61902292, 62001357, 62072359, 62172438),
   the key research and development plan of Shaanxi Province
   (2019ZDLGY13-07, 2019ZDLGY13-04, 2020JQ-844), the Natural Science
   Foundation of Guangdong Province of China (2022A1515010988), the Xi'an
   Science and Technology Plan (20RGZN0005), and the Xi'an Key Laboratory
   of Mobile Edge Computing and Security (201805052-ZD3CG36).
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen C, 2022, J PARALLEL DISTR COM, V165, P66, DOI 10.1016/j.jpdc.2022.03.010
   Chen C, 2022, IEEE T INTELL TRANSP, V23, P19655, DOI 10.1109/TITS.2021.3128012
   Chen C, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107108
   Chen L.C., 2018, P EUR C COMP VIS ECC, P801, DOI [DOI 10.1007/978-3-030-01234-2_49, DOI 10.48550/ARXIV.1802.02611]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Z., 2019, ARXIV190801975
   Cheng L., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508819
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Di Martino T., 2021, P IEEE INT GEOSC REM, P1847, DOI 10.1109/IGARSS47720.2021.9554286
   Farhangfar S, 2019, IRAN CONF ELECTR ENG, P1864, DOI [10.1109/IranianCEE.2019.8786455, 10.1109/iraniancee.2019.8786455]
   Feng CS, 2022, IEEE T IND INFORM, V18, P3582, DOI 10.1109/TII.2021.3116132
   Gang Xu, 2012, 2012 International Conference on Computer Science and Service System (CSSS), P2261, DOI 10.1109/CSSS.2012.561
   Goel A, 2019, IEEE GEOSCI REMOTE S, V16, P952, DOI 10.1109/LGRS.2018.2884675
   GOLDBERG M, 1978, IEEE T SYST MAN CYB, V8, P86, DOI 10.1109/TSMC.1978.4309905
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hui Zhang, 2012, 2012 International Conference on Computer Science and Service System (CSSS), P238, DOI 10.1109/CSSS.2012.67
   Iandola F.N., 2014, CORR ARXIV14041869
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Jinna Hu, 2021, IEEE Communications Standards Magazine, V5, P12, DOI 10.1109/MCOMSTD.001.2000017
   Kai, 2017, OPTICAL SENSING IMAG
   Karimi D, 2020, IEEE T MED IMAGING, V39, P499, DOI 10.1109/TMI.2019.2930068
   Li XH, 2021, ISPRS J PHOTOGRAMM, V179, P14, DOI 10.1016/j.isprsjprs.2021.07.007
   Liu YC, 2018, ISPRS J PHOTOGRAMM, V145, P78, DOI 10.1016/j.isprsjprs.2017.12.007
   Lv N, 2021, IEEE J-STARS, V14, P9318, DOI 10.1109/JSTARS.2021.3110842
   Ma J., 2020, Towards efficient covid-19 ct annotation: A benchmark for lung and infection segmentation
   Malik, 2015, 2015 1 INT C NEW TEC, P1, DOI [10.1109/NTIC.2015.7368750, DOI 10.1109/NTIC.2015.7368750]
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Ming DP, 2012, IEEE GEOSCI REMOTE S, V9, P813, DOI 10.1109/LGRS.2011.2182604
   Mnih V., 2013, THESIS, DOI DOI 10.1109/ICCV.2015.178
   Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1_22
   Rekkas VP, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10222786
   Ren J., 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P4380, DOI [10.1109/IGARSS47720.2021.9553666, DOI 10.1109/IGARSS47720.2021.9553666]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saxena N, 2020, IEEE INT C SEMANT CO, P154, DOI 10.1109/ICSC.2020.00030
   Taghanaki SA, 2019, COMPUT MED IMAG GRAP, V75, P24, DOI 10.1016/j.compmedimag.2019.04.005
   Tan M., 2020, INT C MACH LEARN, DOI DOI 10.48550/ARXIV.1905.11946
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wan SH, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108146
   Wang C, 2023, IEEE T MOBILE COMPUT, V22, P3137, DOI 10.1109/TMC.2021.3137219
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wu GM, 2019, INT GEOSCI REMOTE SE, P158, DOI [10.1109/igarss.2019.8900475, 10.1109/IGARSS.2019.8900475]
   Wu Z., 2016, ARXIV160506885
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661
   Xiang DL, 2014, IEEE GEOSCI REMOTE S, V11, P1290, DOI 10.1109/LGRS.2013.2292820
   Xiaohui Zeng, 2021, 2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering (CSAIEE), P231, DOI 10.1109/CSAIEE54046.2021.9543336
   Yintu Bao, 2021, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P1858, DOI 10.1109/IMCEC51613.2021.9482266
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zhun Li, 2020, 2020 7th International Conference on Information Science and Control Engineering (ICISCE), P1269, DOI 10.1109/ICISCE50968.2020.00256
NR 49
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4279
EP 4291
DI 10.1007/s00371-022-02590-3
EA JUL 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000827347700001
DA 2024-07-18
ER

PT J
AU Baluja, S
AF Baluja, Shumeet
TI A natural representation of colors with textures
SO VISUAL COMPUTER
LA English
DT Article
DE Color encoding; Decolorization; Invertible binary; Invertible grayscale;
   Texture Generation; Texture Representation
ID THRESHOLDING TECHNIQUES; IMAGE; ENTROPY; PERFORMANCE; ALGORITHM; QUALITY
AB We present an automated method to transform an image to a set of binary textures that represent the intensities and colors of a full color original image. The foundation of our method is information preservation: creating a set of textures that allows for the reconstruction of the original image's colors solely from the binarized representation. We present techniques to ensure that the textures created are not visually distracting, preserve the intensity profile of the images, and are natural in that they map sets of colors that are perceptually similar to patterns that are similar. The textures instantiated are fully reversible; the original images' colors can be recreated from the single-bit per-pixel images. The approach uses deep-neural networks and is entirely self-supervised. The system yields aesthetically pleasing binary images when tested on a variety of image sources, including color and black and white photographs, clip art, and paintings.
C1 [Baluja, Shumeet] Google Res, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Baluja, S (corresponding author), Google Res, Mountain View, CA 94043 USA.
EM shumeet@google.com
CR [Anonymous], 2020, CRICUT CRICUT
   [Anonymous], 2017, RESCIENCE J GITHUB
   [Anonymous], 2020, GLOWFORGE GLOWFORGE
   Bai Y, 2001, P SOC PHOTO-OPT INS, V4300, P444
   Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877
   Bernsen J., 1986, P 8 INT C PATT REC
   Bradski G, 2000, DR DOBBS J, V25, P120
   Calvo-Zaragoza J., 2017, CORR ARXIVABS1706102
   Chaki N, 2014, STUD COMPUT INTELL, V560, P1, DOI [10.1007/978-81-322-1907-1, 10.1007/978-81-322-1907-1_2]
   Chang CI, 2006, IEE P-VIS IMAGE SIGN, V153, P837, DOI 10.1049/ip-vis:20050032
   Chang JH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618508
   Deng J., 2009, IEEE C COMP VIS PATT
   DOYLE W, 1962, J ACM, V9, P259, DOI 10.1145/321119.321123
   Fisher R., 2003, GAUSSIAN SMOOTHING
   FLOYD RW, 1976, P SID, V17, P75
   Funkhouser T, 2008, IMAGE QUANTIZATION H
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   GLASBEY CA, 1993, CVGIP-GRAPH MODEL IM, V55, P532, DOI 10.1006/cgip.1993.1040
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Harrington S.J., 1994, COLOR HARD COPY GRAP, V2171, P305
   Helland T, 2012, IMAGE DITHERING 11 A
   Hinton Geoffrey, 2012, COURSERA VIDEO LECT
   Hu DH, 2018, IEEE ACCESS, V6, P38303, DOI 10.1109/ACCESS.2018.2852771
   HUANG LK, 1995, PATTERN RECOGN, V28, P41, DOI 10.1016/0031-3203(94)E0043-K
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Isumi R, 2021, VISUAL COMPUT, V37, P3103, DOI 10.1007/s00371-021-02268-2
   Jiang J, 1999, SIGNAL PROCESS-IMAGE, V14, P737, DOI 10.1016/S0923-5965(98)00041-1
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   KITTLER J, 1986, PATTERN RECOGN, V19, P41, DOI 10.1016/0031-3203(86)90030-0
   KOVASZNAY LSG, 1955, P IRE, V43, P560
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   LEE SU, 1990, COMPUT VISION GRAPH, V52, P171, DOI 10.1016/0734-189X(90)90053-X
   Li CH, 1998, PATTERN RECOGN LETT, V19, P771, DOI 10.1016/S0167-8655(98)00057-9
   Luo WJ, 2016, ADV NEUR IN, V29
   Makhzani A., 2015, ARXIV
   Minnen D., 2018, ARXIV
   Niblack W., 1986, An Introduction to Digital Image Processing
   Ostromoukhov V, 2001, COMP GRAPH, P567, DOI 10.1145/383259.383326
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pang WM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360688
   Phansalkar Neerad, 2011, 2011 International Conference on Communications and Signal Processing (ICCSP), P218, DOI 10.1109/ICCSP.2011.5739305
   Praun E, 2001, COMP GRAPH, P581, DOI 10.1145/383259.383328
   PREWITT JMS, 1966, ANN NY ACAD SCI, V128, P1035
   Qu YG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409108
   RIDLER TW, 1978, IEEE T SYST MAN CYB, V8, P630, DOI 10.1109/tsmc.1978.4310039
   Roetling P. G., 1981, Proceedings of the SPIE - The International Society for Optical Engineering, V310, P133, DOI 10.1117/12.932860
   Sauvola J, 2000, PATTERN RECOGN, V33, P225, DOI 10.1016/S0031-3203(99)00055-2
   Schneider CA, 2012, NAT METHODS, V9, P671, DOI 10.1038/nmeth.2089
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   SHANBHAG AG, 1994, CVGIP-GRAPH MODEL IM, V56, P414, DOI 10.1006/cgip.1994.1037
   Soille P., 2013, MORPHOLOGICAL IMAGE
   Stathis P, 2008, INT C PATT RECOG, P1953
   Theis L., 2017, ICLR
   TSAI WH, 1985, COMPUT VISION GRAPH, V29, P377, DOI 10.1016/0734-189X(85)90133-1
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   wikipedia, 2020, WIKIPEDIA INK
   Xia MH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275080
   YEN JC, 1995, IEEE T IMAGE PROCESS, V4, P370, DOI 10.1109/83.366472
   ZACK GW, 1977, J HISTOCHEM CYTOCHEM, V25, P741, DOI 10.1177/25.7.70454
   Zander J, 2004, COMPUT GRAPH FORUM, V23, P421, DOI 10.1111/j.1467-8659.2004.00773.x
   Zhou BF, 2003, ACM T GRAPHIC, V22, P437, DOI 10.1145/882262.882289
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 64
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3267
EP 3278
DI 10.1007/s00371-022-02568-1
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819903900001
DA 2024-07-18
ER

PT J
AU Mortazavi, M
   Gachpazan, M
   Amintoosi, M
   Salahshour, S
AF Mortazavi, M.
   Gachpazan, M.
   Amintoosi, M.
   Salahshour, S.
TI Fractional derivative approach to sparse super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Image super-resolution; Fractional order differential; Sparse
   representation; Dictionary learning
ID IMAGE SUPERRESOLUTION
AB Fractional calculus is an important branch of mathematical analysis and played a fundamental role in different fields including signal processing and image processing. In this paper, we proposed a sparse super-resolution (SR) technique by using the Grunwald-Letnikov (G-L) fractional differential operator, which aims to reconstruct high-resolution images by recovering pixel information from low-resolution images. We suggested a modified fractional derivative mask based on G-L for image enhancement. The proposed method suppresses block artifacts, staircase edges, and false edges near the edges. The proposed method is very flexible and preserves detailed features. The obtained experiments demonstrated that fractional operator can nonlinearly preserve the low-frequency contour information in the smooth region and also nonlinearly improve well the high-frequency edge and texture of the image. In sparse SR images, the dictionary is constructed by texture information of the images by using integer derivatives. In this work, we computed dictionary matrices by applying the fractional masks. In fact, the extracted features based on fractional derivatives are utilized in the dictionary training procedure and sparse coding. The experimental results and analysis on natural images indicated that the proposed method achieved much better results than other algorithms in terms of both quantitative measures and visual perception.
C1 [Mortazavi, M.; Gachpazan, M.] Ferdowsi Univ Mashhad, Fac Math Sci, Dept Appl Math, Mashhad, Razavi Khorasan, Iran.
   [Amintoosi, M.] Hakim Sabzevari Univ, Fac Math & Comp Sci, Sabzevar, Iran.
   [Salahshour, S.] Bahcesehir Univ, Fac Engn & Nat Sci, Istanbul, Turkey.
C3 Ferdowsi University Mashhad; Bahcesehir University
RP Gachpazan, M (corresponding author), Ferdowsi Univ Mashhad, Fac Math Sci, Dept Appl Math, Mashhad, Razavi Khorasan, Iran.
EM minamortazavi5@gmail.com; gachpazan@um.ac.ir; m.amintoosi@hsu.ac.ir;
   soheil.salahshour@eng.bau.edu.tr
RI Amintoosi, Mahmood/R-1819-2018; Salahshour, Soheil/K-4817-2019
OI Amintoosi, Mahmood/0000-0001-9640-6475; Salahshour,
   Soheil/0000-0003-1390-3551
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2002, ADV NEURAL INF PROCE
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen DL, 2013, ABSTR APPL ANAL, DOI 10.1155/2013/585310
   Diethelm K, 2002, J MATH ANAL APPL, V265, P229, DOI 10.1006/jmaa.2000.7194
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Ghanbari B, 2020, ADV DIFFER EQU-NY, V2020, DOI [10.1186/s13662-020-03040-x, 10.1186/s13662-020-02993-3, 10.1186/s13662-020-02890-9]
   Ghanbari B, 2020, PHYSICA A, V542, DOI 10.1016/j.physa.2019.123516
   Gorenflo R, 1997, Fractional calculus: Integral and differential equations of fractional order, DOI DOI 10.1007/978-3-7091-2664-6_5
   Guan JL, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S021800141857001X
   He N, 2015, SIGNAL PROCESS, V112, P180, DOI 10.1016/j.sigpro.2014.08.025
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   JALAB HA, 2013, DISCRETE DYN NAT SOC
   Jung MY, 2011, IEEE T IMAGE PROCESS, V20, P1583, DOI 10.1109/TIP.2010.2092433
   Kim Hyun-Ho, 2017, ELECT IMAGING, V2017, P81
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Ma W, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11212578
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Podlubny I, 1998, FRACTIONAL DIFFERENT
   Pu Y., 2006, Invention Patent of China, Patent No. [ZL2006100217023, 2006100217023]
   Pu YF, 2007, J ALGORITHMS COMPUT, V1, P357, DOI 10.1260/174830107782424075
   Pu Yi-Fei, 2007, Acta Automatica Sinica, V33, P1128
   Pu YF, 2010, IEEE T IMAGE PROCESS, V19, P491, DOI 10.1109/TIP.2009.2035980
   [蒲亦非 Pu Yifei], 2005, [四川大学学报. 工程科学版, Journal of Sichuan University. Engineering Science Edition], V37, P118
   Ren ZM, 2013, SIGNAL PROCESS, V93, P2408, DOI 10.1016/j.sigpro.2013.02.015
   Shukla AK, 2021, MULTIMED TOOLS APPL, V80, P30213, DOI 10.1007/s11042-020-08968-6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   UR H, 1992, CVGIP-GRAPH MODEL IM, V54, P181, DOI 10.1016/1049-9652(92)90065-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang XM, 2019, VISUAL COMPUT, V35, P1755, DOI 10.1007/s00371-018-1570-2
   [杨柱中 YANG Zhuzhong], 2008, [四川大学学报. 工程科学版, Journal of Sichuan University. Engineering Science Edition], V40, P152
   Yi-Fei P., 2007, J SICHUAN U ENG SCI, V3, P022
   Yi-fei P., 2008, SCI CHINA SER E, V38, P2252
   Yin HT, 2013, INFORM FUSION, V14, P229, DOI 10.1016/j.inffus.2012.01.008
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang HP, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143234
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SY, 2014, TRANSP LETT, V6, P197, DOI 10.1179/1942787514Y.0000000025
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang Y, 2011, COMPUT MATH METHOD M, V2011, DOI 10.1155/2011/173748
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 48
TC 3
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3011
EP 3028
DI 10.1007/s00371-022-02509-y
EA MAY 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000802898700001
DA 2024-07-18
ER

PT J
AU Santos, RLC
   Baranoski, GVG
AF Santos, Rebecca L. C.
   Baranoski, Gladimir V. G.
TI Observation-driven generation of texture maps depicting dust
   accumulation over time
SO VISUAL COMPUTER
LA English
DT Article
DE Computer graphics; Realism; Texture; Material appearance; Weathering
   phenomena; Dust
ID SIMULATION
AB The perception of realism in computer-generated images can be significantly enhanced by subtle visual cues. Among those, one can highlight the presence of dust on synthetic objects, which is often subject to temporal variations in real settings. In this paper, we present a framework for the generation of textures representing the accumulation of this ubiquitous material over time in indoor settings. It employs a physically inspired approach to portray the effects of different levels of accumulated dust roughness on the appearance of substrate surfaces and to modulate these effects according to the different illumination and viewing geometries. The development of its core algorithms was guided by empirical insights and data obtained from observational experiments which are also described. To illustrate its applicability to the rendering of visually plausible depictions of time-dependent changes in dusty scenes, we provide sequences of images obtained considering distinct dust accumulation scenarios.
C1 [Santos, Rebecca L. C.] Univ Waterloo, Waterloo, ON, Canada.
   [Baranoski, Gladimir V. G.] Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada.
C3 University of Waterloo; University of Waterloo
RP Baranoski, GVG (corresponding author), Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada.
EM rlsantos@uwaterloo.ca; gvgbaran@uwaterloo.ca
OI Baranoski, Gladimir/0000-0002-9215-2137
CR Adachi S., 2013, ACM SIGGRAPH 2013
   [Anonymous], 1996, Computer graphics: principles and practice
   Ashraf G, 1999, J VISUAL COMP ANIMAT, V10, P193, DOI 10.1002/(SICI)1099-1778(199910/12)10:4<193::AID-VIS210>3.0.CO;2-M
   Bandeira D, 2010, VISUAL COMPUT, V26, P965, DOI 10.1007/s00371-010-0495-1
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Burden R., 1988, NUMERICAL ANAL
   Carr DB, 2004, PAIN, V108, P17, DOI 10.1016/j.pain.2003.07.001
   Chen B, 2021, VISUAL COMPUT, V37, P2975, DOI 10.1007/s00371-021-02227-x
   Chen J. X., 1999, ACM Transactions on Modeling and Computer Simulation, V9, P81, DOI 10.1145/333296.333366
   Chen JX, 1999, COMPUT SCI ENG, V1, P12, DOI 10.1109/5992.743611
   Chen X, 2013, LECT NOTES COMPUT SC, V8033, P437, DOI 10.1007/978-3-642-41914-0_43
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Dagenais F, 2016, VISUAL COMPUT, V32, P881, DOI 10.1007/s00371-016-1261-9
   FARIN G, 1983, COMPUT AIDED DESIGN, V15, P73, DOI 10.1016/0010-4485(83)90171-9
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Greenberg D. P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P477, DOI 10.1145/258734.258914
   Guo J, 2014, VISUAL COMPUT, V30, P797, DOI 10.1007/s00371-014-0967-9
   Hanselman Duane., 2001, Mastering MATLAB 6: a comprehensive tutorial and reference
   HOOCK DW, 1991, P SOC PHOTO-OPT INS, V1486, P164
   HSU SC, 1995, IEEE COMPUT GRAPH, V15, P18, DOI 10.1109/38.364957
   Imagire T, 2009, VISUAL COMPUT, V25, P719, DOI 10.1007/s00371-009-0319-3
   Inc. A., PHOT
   Judd DB., 1975, Color in business, science, and industry /
   Kimmel BW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421646
   Kimmel BW, 2010, COMPUT GRAPH-UK, V34, P441, DOI 10.1016/j.cag.2010.04.002
   Kimmel B, 2007, OPT EXPRESS, V15, P9755, DOI 10.1364/OE.15.009755
   Kubelka P., 1931, ZURICH TECH PHYS, V12, P543
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Méndez-Feliu A, 2009, VISUAL COMPUT, V25, P181, DOI 10.1007/s00371-008-0213-4
   Meng J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766949
   Mérillou S, 2008, COMPUT GRAPH-UK, V32, P159, DOI 10.1016/j.cag.2008.01.003
   Bajo JM, 2021, VISUAL COMPUT, V37, P2053, DOI 10.1007/s00371-020-01963-w
   Muñoz-Pandiella I, 2018, IEEE T VIS COMPUT GR, V24, P3239, DOI 10.1109/TVCG.2018.2794526
   Nilsson Jim, 2019, RAY TRACING GEMS, P321
   Ofek E, 1997, IEEE COMPUT GRAPH, V17, P18, DOI 10.1109/38.574667
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Schwarzler M., 2007, TR18620709 VIENN U T
   Shahidi S., 2012, INT C COMP GRAPH THE, P7
   Summer R., 1999, GRAPHICS INTERFACE, P125
   Sun B, 2007, IEEE T VIS COMPUT GR, V13, P595, DOI 10.1109/TVCG.2007.1013
   van Bronswijk JEMH., 1976, HOUSE DUST BIOL ALLE
   WESCHLER CJ, 1983, JAPCA J AIR WASTE MA, V33, P624, DOI 10.1080/00022470.1983.10465619
   YUEN HK, 1990, IMAGE VISION COMPUT, V8, P71, DOI 10.1016/0262-8856(90)90059-E
   Yun J, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1760
NR 46
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1943
EP 1957
DI 10.1007/s00371-022-02457-7
EA APR 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000777427600001
PM 37091062
OA Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Mestetskiy, LM
   Guru, DS
   Benifa, JVB
   Nagendraswamy, HS
   Chola, C
AF Mestetskiy, Leonid M.
   Guru, D. S.
   Benifa, J. V. Bibal
   Nagendraswamy, H. S.
   Chola, Channabasava
TI Gender identification of <i>Drosophila melanogaster</i> based on
   morphological analysis of microscopic images
SO VISUAL COMPUTER
LA English
DT Article
DE Drosophila melanogaster; Body shape; Morphological model; Skeleton;
   Gender recognition
ID CLASSIFICATION; INFORMATION; SHAPE; FEATURES; MODEL; FLY
AB Drosophila melanogaster (D. melanogaster) is an imperative genomic model organism that is employed widely in healthcare and biological research works. Roughly 61% of recognized human genes have a perceptible similarity with the genetic code of D. melanogaster flies, besides 50% of its protein structures have mammalian equivalents. In recent times, numerous studies have been done in D. melanogaster to investigate the functions of particular genes that are available in its central nervous system, including the major organs like the heart, liver and kidney. The findings of these research works through D. melanogaster are utilized as a key mechanism to explore human interrelated diseases. However, it is essential to recognize the male and female Drosophila flies for the better understanding of human disease related studies, and it is a tricky job. This paper describes a unique programmed system to categorize the gender of D. melanogaster from the ventral view portraits captured through microscope. The proposed method includes image segmentation of the body of D. melanogaster in the form of a binary image and the construction of a continuous morphological model based on its skeleton. An analysis of the skeleton makes it possible to assess the sharpness of the caudal end of the D. melanogaster abdomen through a detailed assessment of the curvature. Based on this assessment, a Drosophila melanogaster Gender (DMG) classifier is constructed for the gender determination of D. melanogaster flies. The accuracy of the DMG classifier is about 98% in proportion to the existing state-of-the-art shape-based classifiers with optimal computing time.
C1 [Mestetskiy, Leonid M.] Moscow MV Lomonosov State Univ, Dept Computat Math & Cybernet, Moscow, Russia.
   [Benifa, J. V. Bibal; Chola, Channabasava] Indian Inst Informat Technol, Dept Comp Sci & Engn, Kottayam, Kerala, India.
   [Guru, D. S.; Nagendraswamy, H. S.] Univ Mysore, Dept Studies Comp Sci, Mysore, Karnataka, India.
C3 Lomonosov Moscow State University; University of Mysore
RP Benifa, JVB (corresponding author), Indian Inst Informat Technol, Dept Comp Sci & Engn, Kottayam, Kerala, India.
EM benifa.john@gmail.com
RI Chola, Channabasava/AFC-6168-2022
OI Chola, Channabasava/0000-0002-7509-9354
FU Russian Foundation for Basic Research Grant; RFBR [2001-00664]
FX This material is based upon work supported by Russian Foundation for
   Basic Research Grant under the Grant No. RFBR 2001-00664.
CR Abuzneid MA, 2018, IEEE ACCESS, V6, P20641, DOI 10.1109/ACCESS.2018.2825310
   Ahmad Foysal, 2014, International Journal of Machine Learning and Computing, V4, P82, DOI 10.7763/IJMLC.2014.V4.425
   Al-antari MA, 2020, ADV EXP MED BIOL, V1213, P59, DOI 10.1007/978-3-030-33128-3_4
   [Anonymous], 1985, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-1-4612-1098-6
   Asano T., 1987, DISCRETE ALGORITHMS, DOI [10.1016/b978-0-12-386870-1.50010-1, DOI 10.1016/B978-0-12-386870-1.50010-1]
   Baker N, 2020, VISION RES, V172, P46, DOI 10.1016/j.visres.2020.04.003
   Barker, 2019, ADAM INCEPTION V3 DE
   Bilder D, 2017, GENETICS, V206, P1227, DOI 10.1534/genetics.117.202390
   Chen P, 2021, SIGNAL IMAGE VIDEO P, V15, P1003, DOI 10.1007/s11760-020-01825-x
   Chola C, 2022, COMPUT MATH METHOD M, V2022, DOI 10.1155/2022/4593330
   Dubey T, 2019, ACS OMEGA, V4, P18793, DOI 10.1021/acsomega.9b02792
   Gil-Jimenez P., 2005, P 8 INT C ART NEUR N, DOI [10.1007/11494669_107, DOI 10.1007/11494669_107]
   Gosciewska K, 2015, LECT NOTES COMPUT SC, V9164, P169, DOI 10.1007/978-3-319-20801-5_18
   Guari Q, 2019, J CANCER, V10, P4876, DOI 10.7150/jca.28769
   Handa J, 2014, J BIOSCIENCES, V39, P609, DOI 10.1007/s12038-014-9452-x
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hendry, 2019, IMAGE VISION COMPUT, V87, P47, DOI 10.1016/j.imavis.2019.04.007
   Iandola Forrest N, 2016, SQUEEZENET ALEXNET L
   Jiang HX, 2020, IEEE ACCESS, V8, P68828, DOI 10.1109/ACCESS.2020.2986946
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li F, 2018, VISUAL COMPUT, V34, P1525, DOI 10.1007/s00371-017-1426-1
   Lin SW, 2007, INT J ADV MANUF TECH, V34, P1164, DOI 10.1007/s00170-006-0667-3
   Mallikarjuna K, 2021, WIRELESS PERS COMMUN, V117, P2495, DOI 10.1007/s11277-020-07991-y
   Neto FGM, 2017, SIBGRAPI, P193, DOI 10.1109/SIBGRAPI.2017.32
   Mestetskiy L, 2011, LECT NOTES COMPUT SC, V6754, P130, DOI 10.1007/978-3-642-21596-4_14
   Motta D, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210829
   Pandey UB, 2011, PHARMACOL REV, V63, P411, DOI 10.1124/pr.110.003293
   Priyanka S, 2018, APPL INTELL, V48, P4960, DOI 10.1007/s10489-018-1251-x
   Ramesh B, 2015, PATTERN RECOGN, V48, P894, DOI 10.1016/j.patcog.2014.09.019
   Rebelo AR, 2020, PROCEEDINGS OF 16TH BRAZILIAN SYMPOSIUM ON INFORMATION SYSTEMS ON DIGITAL TRANSFORMATION AND INNOVATION, SBSI 2020, DOI 10.1145/3411564.3411641
   Roeder T, 2011, BIOL INSPIR SYST, V2, P15, DOI 10.1007/978-90-481-9641-8_2
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   SKLANSKY J, 1972, IEEE T COMPUT, VC 21, P260, DOI 10.1109/TC.1972.5008948
   Srivastava V, 2020, J DIGIT IMAGING, V33, P252, DOI 10.1007/s10278-019-00245-9
   Tuda M, 2020, ECOL INFORM, V60, DOI 10.1016/j.ecoinf.2020.101135
   Tyagi, 2017, CONTENT BASED IMAGE, DOI [10.1007/978-981-10-6759-4_13, DOI 10.1007/978-981-10-6759-4_13]
   Ugur B, 2016, DIS MODEL MECH, V9, P235, DOI 10.1242/dmm.023762
   Vecchio G, 2015, NANOTOXICOLOGY, V9, P135, DOI 10.3109/17435390.2014.911985
   Xie YT, 2018, INFORM FUSION, V42, P102, DOI 10.1016/j.inffus.2017.10.005
NR 39
TC 2
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1815
EP 1827
DI 10.1007/s00371-022-02447-9
EA MAR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000770721300002
DA 2024-07-18
ER

PT J
AU Li, XR
   Pi, JD
   Lou, M
   Qi, YL
   Li, SZ
   Meng, J
   Ma, YD
AF Li, Xiaorong
   Pi, Jiande
   Lou, Meng
   Qi, Yunliang
   Li, Sizheng
   Meng, Jie
   Ma, Yide
TI Multi-level feature fusion network for nuclei segmentation in digital
   histopathological images
SO VISUAL COMPUTER
LA English
DT Article
DE Nuclei segmentation; CNN; CAD; MFFNet
ID COLOR NORMALIZATION; NET
AB Early detection and the classification of cancer in diagnosed patients can improve the prognosis and improve patient outcomes. In the field of histopathology, the assessment of the disease state is based on the morphological characteristics and spatial distribution of the nuclei in the tissue images. Therefore, the purpose of this research is to propose an automatic histopathological images nuclei segmentation method to accurately predict the boundaries of overlapping and multi-size nuclei. Based on the traditional U-Net, we proposed a convolutional neural network (CNN) that includes iterative attention feature fusion (iAFF) and residual modules for overlapping and multi-size nuclei segmentation task, which is essential and challenging for the development of computer-aided diagnosis (CAD) systems. We extensively evaluate this method on the TNBC and TCGA datasets and the experimental results show that our method can obtain better segmentation performance than the most advanced deep learning models. The proposed method has three advantages in the task of nuclei segmentation. First of all, the iAFF module used in the skip connection fully combines the global channel and the local context and overcomes the semantic and scale inconsistency between the input features. Second, the residual module in the decoder further integrates context information. Third, the method proposed in this paper will not increase too much computational overhead on U-Net, but the effect is significantly improved. Therefore, compared with traditional CNN, multi-level feature fusion network (MFFNet) can reduce redundancy and effectively improve the performance of the model without greatly increasing the network parameters.
C1 [Li, Xiaorong; Pi, Jiande; Lou, Meng; Qi, Yunliang; Li, Sizheng; Meng, Jie; Ma, Yide] Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Gansu, Peoples R China.
C3 Lanzhou University
RP Ma, YD (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Lanzhou, Gansu, Peoples R China.
EM ydma01@126.com
RI li, xiao/HKV-8405-2023; Lou, Meng/JYX-3663-2024; LI, XIAO/IQV-9318-2023;
   li, xiao/HJP-5134-2023; lu, yang/IWE-3635-2023; li,
   xuedong/JYP-4367-2024; wang, xicheng/IXX-0974-2023
OI Lou, Meng/0000-0001-5409-0281; 
FU Natural Science Foundation of Gansu Province [18JR3RA288]; Fundamental
   Research Funds for the Central Universities of China
FX This work is jointly supported by the Natural Science Foundation of
   Gansu Province (No.18JR3RA288) and the Fundamental Research Funds for
   the Central Universities of China (No.lzujbky-2017-it72 and
   No.lzujbky-2018-it61).
CR Aatresh AA, 2021, COMPUT MED IMAG GRAP, V93, DOI 10.1016/j.compmedimag.2021.101975
   Chanchal AK, 2021, COMPUT ELECTR ENG, V92, DOI 10.1016/j.compeleceng.2021.107177
   Chen H, 2016, PROC CVPR IEEE, P2487, DOI 10.1109/CVPR.2016.273
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng ZM, 2022, VISUAL COMPUT, V38, P749, DOI 10.1007/s00371-021-02075-9
   Chidester B., 2019, P IEEECVF C COMP VIS
   Cruz-Roa A, 2017, SCI REP-UK, V7, DOI 10.1038/srep46450
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Fang W., 2020, P AS C COMP VIS 2020
   Fei Ding, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P253, DOI 10.1007/978-3-030-59710-8_25
   Garcia Rojo Marcial, 2012, Stud Health Technol Inform, V179, P15
   Hayakawa T, 2021, ARCH COMPUT METHOD E, V28, P1, DOI 10.1007/s11831-019-09366-4
   Kang QB, 2019, LECT NOTES COMPUT SC, V11764, P703, DOI 10.1007/978-3-030-32239-7_78
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Li XL, 2019, IEEE ACCESS, V7, P84040, DOI 10.1109/ACCESS.2019.2924744
   Liu DN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P861
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mahmood F, 2020, IEEE T MED IMAGING, V39, P3257, DOI 10.1109/TMI.2019.2927182
   Naylor P, 2019, IEEE T MED IMAGING, V38, P448, DOI 10.1109/TMI.2018.2865709
   Naylor P, 2017, I S BIOMED IMAGING, P933, DOI 10.1109/ISBI.2017.7950669
   Oda H, 2018, LECT NOTES COMPUT SC, V11071, P228, DOI 10.1007/978-3-030-00934-2_26
   Piórkowski A, 2019, ADV INTELL SYST, V762, P393, DOI 10.1007/978-3-319-91211-0_35
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy S, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03308-4
   Shu J, 2013, IEEE ENG MED BIO, P5445, DOI 10.1109/EMBC.2013.6610781
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Song J, 2019, KNOWL-BASED SYST, V176, P40, DOI 10.1016/j.knosys.2019.03.031
   Song J, 2017, IEEE J BIOMED HEALTH, V21, P451, DOI 10.1109/JBHI.2015.2504422
   Vahadane A, 2016, IEEE T MED IMAGING, V35, P1962, DOI 10.1109/TMI.2016.2529665
   Veta M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070221
   Vuola AO, 2019, I S BIOMED IMAGING, P208, DOI [10.1109/isbi.2019.8759574, 10.1109/ISBI.2019.8759574]
   Wan T, 2020, NEUROCOMPUTING, V408, P144, DOI 10.1016/j.neucom.2019.08.103
   Win KY, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL ARTS, MEDIA AND TECHNOLOGY (ICDAMT): DIGITAL ECONOMY FOR SUSTAINABLE GROWTH, P14, DOI 10.1109/ICDAMT.2017.7904925
   Yang LF, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008193
   Zeng ZT, 2019, IEEE ACCESS, V7, P21420, DOI 10.1109/ACCESS.2019.2896920
   Zhao J, 2020, LECT NOTES COMPUT SC, V11977, P51, DOI 10.1007/978-3-030-37969-8_7
   Zhao M, 2021, FUTURE GENER COMP SY, V114, P185, DOI 10.1016/j.future.2020.07.045
NR 38
TC 13
Z9 14
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1307
EP 1322
DI 10.1007/s00371-022-02407-3
EA MAR 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000764589100001
DA 2024-07-18
ER

PT J
AU Vaidya, SP
AF Vaidya, S. Prasanth
TI Fingerprint-based robust medical image watermarking in hybrid transform
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Medical image watermarking; Lifting wavelet transform (LWT); Discrete
   wavelet transform (DWT); Local binary pattern (LBP); Non-blind
   watermarking; Electronic patient record (EPR)
ID MULTIPLE WATERMARKING; TEXTURE MEASURES; CLASSIFICATION; SCHEME
AB To protect the medical images integrity, digital watermark is embedded into the medical images. A non-blind medical image watermarking scheme based on hybrid transform is propounded. In this paper, fingerprint of the patient is used as watermark for better authentication, identifying the original medical image and privacy of the patients. In this scheme, lifting wavelet transform (LWT) and discrete wavelet transform (DWT) are utilized for amplifying the watermarking algorithm. The scaling and embedding factors are calculated adaptively with the help of Local Binary Pattern values of the host medical image to achieve better imperceptibility and robustness for medical images and fingerprint watermark, respectively. Two-level decomposition is done where for the first level LWT is utilized and for the second level decomposition DWT is utilized. At the extraction side, non-blind recovery of fingerprint watermark is performed which is similar to the embedding process. The propounded design is implemented on various medical images like Chest X-ray, CT scan and so on. The propounded design provides better imperceptibility and robustness with the combination of LWT-DWT. The result analysis proves that the proposed fingerprint watermarking scheme has attained best results in terms of robustness and authentication with different medical image attacks. Peak Signal to Noise Ratio and Normalized Correlation Coefficient metrics are used for evaluating the proposed scheme. Furthermore, superior results are obtained when compared to related medical image watermarking schemes.
C1 [Vaidya, S. Prasanth] Aditya Engn Coll A, Dept Comp Sci & Engn, Surampalem 533437, Andhra Pradesh, India.
C3 Aditya Engineering College, Surampalem
RP Vaidya, SP (corresponding author), Aditya Engn Coll A, Dept Comp Sci & Engn, Surampalem 533437, Andhra Pradesh, India.
EM vaidya269@gmail.com
CR Albahri OS, 2020, J INFECT PUBLIC HEAL, V13, P1381, DOI 10.1016/j.jiph.2020.06.028
   Alshanbari HS, 2021, MULTIMED TOOLS APPL, V80, P16549, DOI 10.1007/s11042-020-08814-9
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   Aparna P, 2019, IET IMAGE PROCESS, V13, P421, DOI 10.1049/iet-ipr.2018.5288
   Cedillo-Hernandez M, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101695
   Daubechies I, 1998, J FOURIER ANAL APPL, V4, P247, DOI 10.1007/BF02476026
   Fares K, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2020.102403
   Hosny KM, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103007
   Hosny KM, 2018, IEEE ACCESS, V6, P77212, DOI 10.1109/ACCESS.2018.2879919
   Hosny KM, 2018, MULTIMED TOOLS APPL, V77, P24727, DOI 10.1007/s11042-018-5670-9
   Hosny KM, 2017, COMPUT ELECTR ENG, V62, P429, DOI 10.1016/j.compeleceng.2017.05.015
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Kahlessenane F, 2021, J AMB INTEL HUM COMP, V12, P2931, DOI 10.1007/s12652-020-02450-9
   Kannammal A, 2014, INT J IMAG SYST TECH, V24, P111, DOI 10.1002/ima.22086
   Khare P, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.3918
   Kumar C, 2020, MULTIMED TOOLS APPL, V79, P11069, DOI 10.1007/s11042-018-6177-0
   Lei BY, 2012, SIGNAL PROCESS, V92, P1985, DOI 10.1016/j.sigpro.2011.12.021
   Li BJ, 2018, OPT LASER ENG, V104, P173, DOI 10.1016/j.optlaseng.2017.08.006
   Liao SC, 2007, LECT NOTES COMPUT SC, V4642, P828
   Lingling Wu, 2009, 2009 1st International Conference on Information Science and Engineering (ICISE 2009), P1164, DOI 10.1109/ICISE.2009.347
   Loukhaoukha K, 2009, 2009 11TH CANADIAN WORKSHOP ON INFORMATION THEORY, P177, DOI 10.1109/CWIT.2009.5069549
   Makhanov S., 2020, MED RECORD DATABASE
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Mehta R, 2017, INT J MACH LEARN CYB, V8, P379, DOI 10.1007/s13042-015-0331-z
   Memon NA, 2011, INT J COMPUT MATH, V88, P2057, DOI 10.1080/00207160.2010.543677
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Parvees MYM, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0611-5
   Sanivarapu PV, 2020, PHYS ENG SCI MED, V43, P213, DOI 10.1007/s13246-019-00838-2
   Sharma S, 2020, ELECTRON LETT, V56, P923, DOI 10.1049/el.2020.1445
   Singh AK, 2017, MULTIMED TOOLS APPL, V76, P8881, DOI 10.1007/s11042-016-3514-z
   Singh AK, 2016, MULTIMED TOOLS APPL, V75, P8381, DOI 10.1007/s11042-015-2754-7
   Singh AK, 2015, WIRELESS PERS COMMUN, V83, P2133, DOI 10.1007/s11277-015-2505-0
   Singh AK, 2015, J MED IMAG HEALTH IN, V5, P607, DOI 10.1166/jmihi.2015.1432
   Singh AK, 2015, WIRELESS PERS COMMUN, V80, P1415, DOI 10.1007/s11277-014-2091-6
   Singh S, 2017, MULTIMED TOOLS APPL, V76, P19113, DOI 10.1007/s11042-017-4570-8
   Starovoitov VV, 2020, EURASIAN J MATH COMP, V8, P76, DOI 10.32523/2306-6172-2020-8-1-76-90
   Thakur S, 2019, MULTIMED TOOLS APPL, V78, P3457, DOI 10.1007/s11042-018-6263-3
   Vaidya SP, 2017, MULTIMED TOOLS APPL, V76, P25623, DOI 10.1007/s11042-017-4355-0
   Vaidya SP, 2015, PROCEDIA COMPUT SCI, V58, P233, DOI 10.1016/j.procs.2015.08.063
   Vaidya S.P., 2018, INT C REC TRENDS IM, P132
   Verma VS, 2015, EXPERT SYST APPL, V42, P8184, DOI 10.1016/j.eswa.2015.06.041
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Xing JinZhu, 2020, FINGERPRINT DATABASE
   Yoo JC, 2012, IET IMAGE PROCESS, V6, P483, DOI 10.1049/iet-ipr.2011.0025
   Yuan ZH, 2020, IET IMAGE PROCESS, V14, P3829, DOI 10.1049/iet-ipr.2019.1740
   Zhou ML, 2019, IEEE T MULTIMEDIA, V21, P1921, DOI 10.1109/TMM.2019.2895281
NR 48
TC 31
Z9 32
U1 4
U2 49
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2245
EP 2260
DI 10.1007/s00371-022-02406-4
EA JAN 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000749149200003
PM 35125576
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Peng, JBA
   Zou, BJ
   Zhu, CZ
AF Peng, Jianbiao
   Zou, Beiji
   Zhu, Chengzhang
TI Combining external attention GAN with deep convolutional neural networks
   for real-fake identification of luxury handbags
SO VISUAL COMPUTER
LA English
DT Article
DE Luxury handbags; External attention mechanism; Generative adversarial
   networks; Deep convolutional neural networks
AB Identifying the fake luxury handbag from their images is crucial to prevent counterfeit. Current state-of-the-art methods focus on developing professional detection devices and some shallower convolution neural network (CNN) models. Such developments employ some professional appraisers with certain prior knowledge or simple CNN models trained by limited datasets to identify the fake luxury handbag, and little attention has been given to external attention optimization mechanism. In addition, existing methods ignore the class imbalance between real and fake handbag datasets, and their prediction capacity is limited. This paper proposes an innovative hybrid framework for fake luxury handbag identification. The proposed method combines external attention generative adversarial networks (EAGANs) with deep convolutional neural networks (DCNNs) with four improvements: (1) EAGAN employs transformer to encode and decode information in both generator and discriminator for more local feature representation; (2) a new attention module based on external attention mechanism is introduced into generator and discriminator of EAGAN for guiding the network to focus on global feature representation; (3) a simple CNN auxiliary classifier is appended to the EAGAN to automatically and efficiently learn the influence of different features, and then, it combines external attention mechanism module to jointly learn the representable features; (4) a three-stage weighted loss is proposed to train the EAGAN model, and then, EAGAN model is combined with DCNN for real-fake identification of luxury handbags. By integrating the above improvements in series, the models' performances are gradually enhanced. Experimental results show that our framework yields superior results to existing state-of-the-art approaches.
C1 [Peng, Jianbiao; Zou, Beiji; Zhu, Chengzhang] Cent South Univ, Sch Comp Sci & Engn, Changsha, Peoples R China.
   [Peng, Jianbiao; Zou, Beiji; Zhu, Chengzhang] Cent South Univ, Hunan Engn Res Ctr Machine Vis & Intelligent Med, Changsha, Peoples R China.
   [Zou, Beiji; Zhu, Chengzhang] Mobile Hlth Minist Educ, China Mobile Joint Lab, Changsha, Peoples R China.
   [Zhu, Chengzhang] Cent South Univ, Coll Literature & Journalism, Changsha, Peoples R China.
C3 Central South University; Central South University; Central South
   University
RP Peng, JBA (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha, Peoples R China.; Peng, JBA (corresponding author), Cent South Univ, Hunan Engn Res Ctr Machine Vis & Intelligent Med, Changsha, Peoples R China.
EM pengjianbiao@csu.edu.cn
FU Scientific and Technological Innovation Leading Plan of High-tech
   Industry of Hunan Province [2020GK2021]; National Natural Science
   Foundation of China [61902434]; Natural Science Foundation of Hunan
   Province, China [2019JJ50826]
FX This work is supported by the Scientific and Technological Innovation
   Leading Plan of High-tech Industry of Hunan Province (2020GK2021), the
   National Natural Science Foundation of China (No. 61902434), and the
   Natural Science Foundation of Hunan Province, China (No. 2019JJ50826).
CR Antonopoulos GA, 2020, EUR J CRIM POLICY RE, V26, P357, DOI 10.1007/s10610-019-09414-6
   Bae HJ, 2015, ADV MATER, V27, P2083, DOI 10.1002/adma.201405483
   Bove VM, 2021, PHOTONICS-BASEL, V8, DOI 10.3390/photonics8060219
   Cadarso VJ, 2013, LIGHT-SCI APPL, V2, DOI 10.1038/lsa.2013.42
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Davies Jack, 2021, IEEE Engineering Management Review, V49, P116, DOI 10.1109/EMR.2021.3069366
   Durall R., ARXIV PREPRINT ARXIV
   Geng Z., ARXIV PREPRINT ARXIV
   Guo M.-H., ARXIV PREPRINT ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A. G., 2017, PREPRINT
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Kang T, 2020, INT CONF BIG DATA, P102, DOI 10.1109/BigComp48618.2020.00-92
   Kim J., 2019, CORR
   King DB, 2015, ACS SYM SER, V1214, P1
   Kumar A., 2021, HDB RES MACHINE LEAR, P293
   Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Liebel M, 2020, NAT NANOTECHNOL, V15, P1005, DOI 10.1038/s41565-020-0771-9
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   OECD and E. U. I. P. Office, 2019, TRENDS TRAD COUNT PI
   Radford A., 2015, ARXIV
   Rauschnabel PA, 2021, INT J INFORM MANAGE, V57, DOI 10.1016/j.ijinfomgt.2020.102279
   Serban A., 2020, ARXIV PREPRINT ARXIV
   Sharma A, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2011, DOI 10.1145/3097983.3098186
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh DP, 2021, J CONSUM BEHAV, V20, P1078, DOI 10.1002/cb.1917
   Song HT, 2021, VISUAL COMPUT, V37, P2285, DOI 10.1007/s00371-020-01986-3
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tu YJ, 2021, DECIS SUPPORT SYST, V142, DOI 10.1016/j.dss.2020.113471
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Wang KJ, 2017, 2017 IEEE 2ND INTERNATIONAL WORKSHOPS ON FOUNDATIONS AND APPLICATIONS OF SELF* SYSTEMS (FAS*W), P191, DOI 10.1109/FAS-W.2017.147
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZX, 2021, ADV OPT MATER, V9, DOI 10.1002/adom.202001609
   Xie RS, 2015, NEUROCOMPUTING, V167, P625, DOI 10.1016/j.neucom.2015.04.026
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhao H., 2000, P IEEECVF C COMPUTER, P10076
NR 48
TC 4
Z9 4
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 971
EP 982
DI 10.1007/s00371-021-02378-x
EA JAN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000741630200004
DA 2024-07-18
ER

PT J
AU Ramadan, M
   El-Jaroudi, A
AF Ramadan, Mona
   El-Jaroudi, Amro
TI Action detection and classification in kitchen activities videos using
   graph decoding
SO VISUAL COMPUTER
LA English
DT Article
DE Video classification; Action detection; Graph decoding; Two-level
   classification
AB In this work, we propose a hybrid deep network/graph decoding using hidden Markov model system for the classification of kitchen activities for the Actions for Cooking Eggs data set. We use and compare two deep learning architectures, a deep convolutional neural network (CNN) alone and a long short-term memory network built on top of a CNN. We address the video classification problem both on the level of actions performed in certain frames and the full-length video level. Our proposed system detects a sequence of cooking actions and outputs a menu class for the entire video. Our approach achieves the highest reported accuracy on the data set for identifying cooking actions with an overall accuracy of 81% compared to the state of the art of 76% and succeeds in assigning a menu label to a sequence of cooking actions with an accuracy of 100% compared to an accuracy range of 10-30% reported in previous work. We also explore the effects of processing a subset of the available frames and imposing a state occupancy constraint during decoding. Our best reported results are achieved when using a common-sense dictionary grammar expansion when processing one frame out of every 35 frames and when restricting state transitions for at least five consecutive frames.
C1 [Ramadan, Mona; El-Jaroudi, Amro] Univ Pittsburgh, Dept Elect & Comp Engn, 3700 OHara St, Pittsburgh, PA 15213 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh
RP Ramadan, M (corresponding author), Univ Pittsburgh, Dept Elect & Comp Engn, 3700 OHara St, Pittsburgh, PA 15213 USA.
EM mhr23@pitt.edu; amro@pitt.edu
OI Ramadan, Mona/0000-0002-1999-0142
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Baccouche M, 2010, LECT NOTES COMPUT SC, V6353, P154
   Bansal S, 2013, IEEE IMAGE PROC, P3461, DOI 10.1109/ICIP.2013.6738714
   Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013
   Granada R., 2017, AAAI 2017 WORKSH PLA
   Hashimoto Atsushi, 2016, 2016 IEEE International Conference on Multimedia & Expo: Workshops (ICMEW), DOI 10.1109/ICMEW.2016.7574771
   Hung N.T., 2015, Emerging Trends in Image Processing, Computer Vision and Pattern Recognition, P379
   Hussein F, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063532
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jurafsky D., 2014, Speech and Language Processing
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Kojima S, 2017, 2017 6TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS AND VISION & 2017 7TH INTERNATIONAL SYMPOSIUM IN COMPUTATIONAL MEDICAL AND HEALTH TECHNOLOGY (ICIEV-ISCMHT)
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   la TorreFrade F.D., 2008, CMURITR0822
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JY, 2019, IEEE T CIRC SYST VID, V29, P2667, DOI 10.1109/TCSVT.2018.2799968
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P5573, DOI 10.1109/TIP.2021.3086590
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P3168, DOI 10.1109/TIP.2019.2957930
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P2416, DOI 10.1109/TCSVT.2018.2868123
   Liu Y, 2018, IEEE SIGNAL PROC LET, V25, P848, DOI 10.1109/LSP.2018.2823910
   Liu Y, 2018, COMPLEXITY, DOI 10.1155/2018/5345241
   Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154
   Hoai M, 2011, PROC CVPR IEEE
   Monteiro J, 2017, IEEE IJCNN, P2048, DOI 10.1109/IJCNN.2017.7966102
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ni BB, 2016, INT J COMPUT VISION, V120, P28, DOI 10.1007/s11263-016-0891-8
   Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4
   Ohyama W, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P166, DOI 10.1109/ACPR.2015.7486487
   Olson DL., 2008, Advanced data mining techniques, V1st, P138, DOI [10.1007/978-3-540-76917-0, DOI 10.1007/978-3-540-76917-0]
   Pesnel, 2004, CONTEXT AWARE VISION
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shimada Atsushi, 2013, Advances in Depth Image Analysis and Applications. International Workshop, WDIA 2012. Selected and Invited Papers: LNCS 7854, P168, DOI 10.1007/978-3-642-40303-3_18
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 45
TC 2
Z9 2
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 799
EP 812
DI 10.1007/s00371-021-02346-5
EA JAN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000740610100002
DA 2024-07-18
ER

PT J
AU An, SM
   Huang, XX
   Wang, L
   Wang, LL
   Zheng, ZJ
AF An, Shunmin
   Huang, Xixia
   Wang, Le
   Wang, Linling
   Zheng, Zhangjing
TI Semi-Supervised image dehazing network
SO VISUAL COMPUTER
LA English
DT Article
DE Semi-supervised; Supervised branch; Unsupervised branch; Unsupervised
   loss; Reconstruction loss; End-to-end
AB A semi-supervised image dehazing network was proposed which consists of the supervised branch and the unsupervised branch. In the supervision branch, the encoding-decoding neural network is used as the network structure, and the network is constrained by the supervision loss. In the unsupervised branch, two similar sub-networks are used to estimate the transmission map and atmospheric light, and the unsupervised loss is constructed through prior knowledge to constrain the unsupervised branch. In the semi-supervised image dehazing network, the supervised branch and the unsupervised branch will output dehazing result, respectively. Then by minimizing the reconstruction loss between the two images, the supervised and unsupervised branches are constrained to make the network more generalizable. The entire semi-supervised image dehazing network is trained in an end-to-end manner, and the supervised and unsupervised branch shares weights in the encoding part. Extensive experimental results show that the proposed method has good performance in image dehazing compared with six advanced dehazing methods.
C1 [An, Shunmin; Huang, Xixia; Wang, Le; Wang, Linling; Zheng, Zhangjing] Shanghai Maritime Univ, Inst Logist Sci & Engn, 1550 Haigang Ave, Shanghai, Peoples R China.
C3 Shanghai Maritime University
RP An, SM (corresponding author), Shanghai Maritime Univ, Inst Logist Sci & Engn, 1550 Haigang Ave, Shanghai, Peoples R China.
EM shunminan@163.com; 20841520@qq.com; 1208041624@qq.com;
   Wanglinling0229@163.com; 1277446413@qq.com
RI lin, yuan/JXL-9592-2024
OI An, Shunmin/0000-0003-0747-1983
CR An SM, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251337
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Gou Y., 2020, ADV NEUR IN, V33
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   IUBC PRIOR, 2013, J THEOR APPL INFORM, V48, P1
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Liu X, 2017, COMPUT VIS IMAGE UND, V162, P23, DOI 10.1016/j.cviu.2017.08.002
   Liu X, 2016, IET IMAGE PROCESS, V10, P877, DOI 10.1049/iet-ipr.2016.0138
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sidorov O, 2019, IEEE INT CONF COMP V, P3844, DOI 10.1109/ICCVW.2019.00477
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang YT, 2013, SIGNAL PROCESS, V93, P3227, DOI 10.1016/j.sigpro.2013.04.025
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wei, 2021, P IEEE INT C MULT EX, DOI 10.1109/ICME51207.2021.9428285
   Wu H, 2018, IEEE T IMAGE PROCESS, V27, P1259, DOI 10.1109/TIP.2017.2772836
   Xu B, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/832093
   Xu ZM, 2018, INT J ADV ROBOT SYST, V15, DOI [10.1109/INFOCOM.2018.8485853, 10.1177/1729881418763458]
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang JL, 2021, IEEE T INTELL TRANSP, V22, P7004, DOI 10.1109/TITS.2020.3000761
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 39
TC 8
Z9 8
U1 4
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 2041
EP 2055
DI 10.1007/s00371-021-02265-5
EA AUG 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000684532600001
DA 2024-07-18
ER

PT J
AU Chen, B
   Wang, C
   Piovarci, M
   Seidel, HP
   Didyk, P
   Myszkowski, K
   Serrano, A
AF Chen, Bin
   Wang, Chao
   Piovarci, Michal
   Seidel, Hans-Peter
   Didyk, Piotr
   Myszkowski, Karol
   Serrano, Ana
TI The effect of geometry and illumination on appearance perception of
   different material categories
SO VISUAL COMPUTER
LA English
DT Article
DE Perception; Material appearance
ID SURFACE REFLECTANCE; 3-DIMENSIONAL SHAPE; VISUAL-PERCEPTION; GLOSS
AB The understanding of material appearance perception is a complex problem due to interactions between material reflectance, surface geometry, and illumination. Recently, Serrano et al. collected the largest dataset to date with subjective ratings of material appearance attributes, including glossiness, metallicness, sharpness and contrast of reflections. In this work, we make use of their dataset to investigate for the first time the impact of the interactions between illumination, geometry, and eight different material categories in perceived appearance attributes. After an initial analysis, we select for further analysis the four material categories that cover the largest range for all perceptual attributes: fabric, plastic, ceramic, and metal. Using a cumulative link mixed model (CLMM) for robust regression, we discover interactions between these material categories and four representative illuminations and object geometries. We believe that our findings contribute to expanding the knowledge on material appearance perception and can be useful for many applications, such as scene design, where any particular material in a given shape can be aligned with dominant classes of illumination, so that a desired strength of appearance attributes can be achieved.
C1 [Chen, Bin; Wang, Chao; Seidel, Hans-Peter; Myszkowski, Karol; Serrano, Ana] Max Planck Inst Informat, Saarbrucken, Germany.
   [Piovarci, Michal] IST Austria, Klosterneuburg, Austria.
   [Didyk, Piotr] Univ Svizzera Italiana, Lugano, Switzerland.
   [Serrano, Ana] Ctr Univ Def, Zaragoza, Spain.
C3 Max Planck Society; Institute of Science & Technology - Austria;
   Universita della Svizzera Italiana
RP Chen, B (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.
EM binchen@mpi-inf.mpg.de; chaowang@mpi-inf.mpg.de;
   michael.piovarci@ist.ac.at; hpseidel@mpi-sb.mpg.de; piotr.didyk@usi.ch;
   karol@mpi-inf.mpg.de; anase@unizar.es
RI Wang, Chao/JXM-2578-2024; Serrano, Ana/ABC-3358-2021
OI Serrano, Ana/0000-0002-7796-3177; Didyk, Piotr/0000-0003-0768-8939;
   CHEN, Bin/0000-0003-3022-1931
FU European Union [765911]; European Research Council (ERC) [804226];
   European Research Council (ERC) [804226] Funding Source: European
   Research Council (ERC)
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under theMarie Sklodowska-Curie, grant
   agreement N. 765911 (RealVision) and from the European Research Council
   (ERC), grant agreement N. 804226 (PERDY).
CR Adams WJ, 2018, J VISION, V18, DOI 10.1167/18.13.4
   Anderson BL, 2011, CURR BIOL, V21, pR978, DOI 10.1016/j.cub.2011.11.022
   Awaja F, 2005, EUR POLYM J, V41, P1453, DOI 10.1016/j.eurpolymj.2005.02.005
   BECK J, 1981, PERCEPT PSYCHOPHYS, V30, P407, DOI 10.3758/BF03206160
   Burley B., 2012, ACM SIGGRAPH 2012 CO, V2, P3
   Chadwick AC, 2015, VISION RES, V109, P221, DOI 10.1016/j.visres.2014.10.026
   Dror R, 2004, J VISION, V4, P821, DOI 10.1167/4.9.11
   Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059
   Filip J, 2014, COMPUT GRAPH FORUM, V33, P91, DOI 10.1111/cgf.12477
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   Fleming RW, 2017, ANNU REV VIS SCI, V3, P365, DOI 10.1146/annurev-vision-102016-061429
   Fleming RW, 2015, VISION RES, V115, P157, DOI 10.1016/j.visres.2015.08.006
   Fleming RW, 2014, VISION RES, V94, P62, DOI 10.1016/j.visres.2013.11.004
   Fleming RW, 2003, J VISION, V3, P347, DOI 10.1167/3.5.3
   Gkioulekas I, 2015, PROC CVPR IEEE, P5528, DOI 10.1109/CVPR.2015.7299192
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Havran V, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12944
   Ho YX, 2008, PSYCHOL SCI, V19, P196, DOI 10.1111/j.1467-9280.2008.02067.x
   Hu BY, 2020, COMPUT GRAPH FORUM, V39, P157, DOI 10.1111/cgf.13920
   Hunter R., 1937, J. Res. Natl. Bur. Stand, V18, P19, DOI [DOI 10.6028/JRES.018.006, 10.6028/jres.018.006]
   Kim J, 2016, I-PERCEPTION, V7, DOI 10.1177/2041669516658047
   Kim J, 2012, NAT NEUROSCI, V15, P1590, DOI 10.1038/nn.3221
   Kim J, 2011, J VISION, V11, DOI 10.1167/11.9.4
   Lagunas M, 2021, J VISION, V21, DOI 10.1167/jov.21.2.2
   Marlow P, 2011, J VISION, V11, DOI 10.1167/11.9.16
   Marlow PJ, 2015, VISION RES, V115, P199, DOI 10.1016/j.visres.2015.05.003
   Marlow PJ, 2015, CURR BIOL, V25, pR221, DOI 10.1016/j.cub.2015.01.062
   Marlow PJ, 2013, J VISION, V13, DOI 10.1167/13.14.2
   Marlow PJ, 2012, CURR BIOL, V22, P1909, DOI 10.1016/j.cub.2012.08.009
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Motoyoshi I, 2012, VISION RES, V53, P30, DOI 10.1016/j.visres.2011.11.010
   Nunnally JC, 1978, PSYCHOMETRIC THEORY, V2nd
   Olkkonen M, 2010, J VISION, V10, DOI 10.1167/10.9.5
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Pont SC, 2006, PERCEPTION, V35, P1331, DOI 10.1068/p5440
   Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11
   Schmid A.C., 2020, MAT CATEGORY DETERMI
   Serrano A, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459813
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Todd JT, 2018, J VISION, V18, DOI 10.1167/18.3.9
   Toscani M, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380741
   Vangorp P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276473, 10.1145/1239451.1239528]
   Wills J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559760
   Zhang F, 2019, J VISION, V19, DOI 10.1167/19.4.11
NR 45
TC 2
Z9 2
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2021
VL 37
IS 12
SI SI
BP 2975
EP 2987
DI 10.1007/s00371-021-02227-x
EA JUL 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP1DP
UT WOS:000673536600003
OA hybrid
DA 2024-07-18
ER

PT J
AU Ding, R
   Liu, SG
AF Ding, Rui
   Liu, Shiguang
TI Underwater sound propagation for virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE Sound propagation; Underwater; Ray tracing; Normal mode; Virtual reality
ID SCATTERING
AB Realistic sound effect synchronized with visual rendering can greatly improve the immersive sense of a user in virtual reality (VR). Prior work focuses on sound propagation in air without considering the characteristics of underwater, thus cannot be directly extended for underwater scenarios. This paper proposes a novel method for simulating sound propagation in underwater scenes. We combine the normal mode method in oceanography with an improved ray tracing method to effectively calculate underwater sound propagation. A normal mode method is adapted for sound pressure calculation in the low-frequency domain. In the high-frequency domain, by considering the characteristics of underwater, we propose a threshold-based improved ray tracing method to compute the impulse response, bringing results closer to real values at higher efficiency. We sample the possible listener positions and use backward ray tracing to perform interpolation and extrapolation at runtime. Our simulation results are realistic at interactive rendering rates for scenes with moving sources. To the best of our knowledge, this is the first time that a sound propagation model tailored for underwater environment is presented in the field of VR. Various experiments in underwater scenes validated our method.
C1 [Ding, Rui; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
EM lsg@tju.edu.cn
RI DING, RUI/P-3018-2019; Ding, Rui/JSL-0652-2023
OI DING, RUI/0000-0002-3542-3965; 
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   Antani L, 2013, IEEE T VIS COMPUT GR, V19, P567, DOI 10.1109/TVCG.2013.27
   Aretz M., 2012, THESIS RWTH AACHEN U
   Cao CX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982431
   Drumm IA, 2000, J ACOUST SOC AM, V107, P1405, DOI 10.1121/1.428427
   ELLIS DD, 1991, J ACOUST SOC AM, V89, P2207, DOI 10.1121/1.400913
   Evans RB, 2006, J ACOUST SOC AM, V119, P161, DOI 10.1121/1.2133240
   FORNBERG B, 1988, GEOPHYSICS, V53, P625, DOI 10.1190/1.1442497
   Gensane M, 2002, ACTA ACUST UNITED AC, V88, P630
   Gumerov NA, 2009, J ACOUST SOC AM, V125, P191, DOI 10.1121/1.3021297
   Hampel S, 2008, INT J NUMER METH ENG, V73, P427, DOI 10.1002/nme.2080
   James DL, 2006, ACM T GRAPHIC, V25, P987, DOI 10.1145/1141911.1141983
   Knobles DP, 2003, J ACOUST SOC AM, V113, P781, DOI 10.1121/1.1534847
   Laine S, 2009, APPL ACOUST, V70, P172, DOI 10.1016/j.apacoust.2007.11.011
   Liu S., 2020, ARXIV201105538
   Liu SG, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P859, DOI [10.1109/VR46266.2020.1580539900203, 10.1109/VR46266.2020.00013]
   Manocha, 2018, ACM MULTIMEDIA, P1
   Markovic D, 2016, IEEE T VIS COMPUT GR, V22, P2262, DOI 10.1109/TVCG.2016.2515612
   Mechel FP, 2002, J SOUND VIB, V256, P873, DOI [10.1006/jsvi.2002.5025, 10.1006/jsvi.5025]
   Mehra R, 2014, IEEE T VIS COMPUT GR, V20, P495, DOI 10.1109/TVCG.2014.38
   Mehra R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451245
   Mo Q, 2016, IEEE T VIS COMPUT GR, V22, P2493, DOI 10.1109/TVCG.2015.2509996
   Porter M.B., 1992, KRAKEN NORMAL MODE P
   Raghuvanshi N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201339
   Raghuvanshi N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601184
   Raghuvanshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778805
   Raghuvanshi N, 2009, IEEE T VIS COMPUT GR, V15, P789, DOI [10.1109/TVCG.2009.27, 10.1109/TVCG.2009.28]
   Raghuvanshi Nikunj, 2010, THESIS U N CAROLINA
   Rojas D, 2015, INT WORK QUAL MULTIM
   RYCROFT M, 2005, J ATMOS TERR PHYS, V5, P629
   Sakamoto S., 2006, J Acoust Soc Am, V120, P3008
   Schissler C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601216
   Taylor M, 2012, IEEE T VIS COMPUT GR, V18, P1797, DOI 10.1109/TVCG.2012.27
   Wang JH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322976
   Wang Y, 2000, IEEE T ANTENN PROPAG, V48, P743, DOI 10.1109/8.855493
   [杨燕明 YANG Yanming], 2007, [海洋学报, Acta Oceanologica Sinica], V29, P33
   YEE KS, 1966, IEEE T ANTENN PROPAG, VAP14, P302
   Yeh HC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508420
NR 38
TC 5
Z9 5
U1 1
U2 22
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2797
EP 2807
DI 10.1007/s00371-021-02175-6
EA JUN 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000659034300005
DA 2024-07-18
ER

PT J
AU Seo, M
   Kang, H
AF Seo, MinYeong
   Kang, HyeongYeop
TI Toward virtual stair walking
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Locomotion; Visual gain
ID VISUAL DOMINANCE; SYSTEM; TOUCH
AB This paper presents a motion remapping-based locomotion technique. Our technique can provide a realistic sensation of climbing and descending stairs when users navigate the virtual environment on foot. The main contribution is to provide users a realistic experience of walking up and down virtual stairs while in reality, they are walking on a flat surface. When a user lifts their real foot, our technique controls the position of virtual foot in order to match the timing of real foot touching the floor with that of virtual foot touching the stairs. The avatar's head and waist are also controlled to mimic the height change movements of stair walking. To achieve this, we collected the actual motion data beforehand and then designed our locomotion technique using the data. Then, we conducted an experiment and an application test. In the experiment, we identified how much visual gain should be applied to foot motion to induce a realistic sensation of stair walking. The results demonstrated that applying visual gains of 1.193 and 0.822 to motions of climbing and descending the stairs were accepted as the most realistic, respectively. In the application test, we investigated whether the proposed technique successfully increases the user's perceived presence and provides a positive user experience. The results demonstrated that the user's perceived presence was significantly enhanced when we applied visual gains. The results also showed that participants felt as if they were walking on the stairs in the virtual environment without experiencing discomfort or postural instability. As the proposed technique only needs visual cue control, we expect that it can easily be applied to commercial applications .
C1 [Seo, MinYeong; Kang, HyeongYeop] Kyung Hee Univ, Dept Software Convergence, 1732 Deogyeong Daero, Yongin, Gyeonggi Do, South Korea.
C3 Kyung Hee University
RP Kang, H (corresponding author), Kyung Hee Univ, Dept Software Convergence, 1732 Deogyeong Daero, Yongin, Gyeonggi Do, South Korea.
EM siamiz@khu.ac.kr
RI Kang, HyeongYeop/AAJ-2471-2020
OI Kang, HyeongYeop/0000-0001-5292-4342
FU Basic Science Research Program through the National Research Foundation
   ofKorea (NRF) - Ministry of Education [NRF-2020R1F1A1076528]
FX The authors declare that they have no conflict of interest. This
   research was supported by Basic Science Research Program through the
   National Research Foundation ofKorea (NRF) funded by the Ministry of
   Education (NRF-2020R1F1A1076528).
CR Abtahi P, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173724
   [Anonymous], 2006, Human body perception from the inside out: advances in visual cognition
   Asjad NS, 2018, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2018), DOI 10.1145/3225153.3225171
   Azmandian M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1968, DOI 10.1145/2858036.2858226
   Bergström J, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P1175, DOI 10.1145/3332165.3347939
   Bhandari J., 2018, P 44 GRAPHICS INTERF, P162, DOI [DOI 10.20380/GI2018.22, 10.20380/GI2018.223, DOI 10.20380/GI2018.223]
   Biocca F, 2001, PRESENCE-VIRTUAL AUG, V10, P247, DOI 10.1162/105474601300343595
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Bouguila L, 2002, P IEEE VIRT REAL ANN, P291, DOI 10.1109/VR.2002.996544
   Bouguila L., 2002, P SIGGRAPH, P63
   Bozgeyikli E, 2016, CHI PLAY 2016: PROCEEDINGS OF THE 2016 ANNUAL SYMPOSIUM ON COMPUTER-HUMAN INTERACTION IN PLAY, P205, DOI 10.1145/2967934.2968105
   Bruder G, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P75, DOI 10.1109/3DUI.2009.4811208
   Burns E, 2005, P IEEE VIRT REAL ANN, P3
   Chengyuan Lai, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P15, DOI 10.1109/3DUI.2015.7131719
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Dominjon L, 2005, P IEEE VIRT REAL ANN, P19
   EA MCMANUS., 2011, P ACM SIGGRAPH S APP, P37, DOI DOI 10.1145/2077451.2077458
   Festl F, 2012, J VISION, V12, DOI 10.1167/12.7.10
   Freitag S, 2014, 2014 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P119, DOI 10.1109/3DUI.2014.6798852
   Hollerbach J.M., 2000, HAPTICS S P ASME DYN, P1293
   Iwata H, 2005, IEEE COMPUT GRAPH, V25, P64, DOI 10.1109/MCG.2005.5
   Iwata H, 2001, P IEEE VIRT REAL ANN, P131, DOI 10.1109/VR.2001.913779
   Iwata H., 2006, ACM SIGGRAPH 2006 Emerging technologies, P28, DOI DOI 10.1145/1179133.1179162
   Kang H, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174206
   Kang H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P699, DOI [10.1109/vr.2019.8798251, 10.1109/VR.2019.8798251]
   Kang H, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P38, DOI [10.1109/VR.2019.8798300, 10.1109/vr.2019.8798300]
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Kim M, 2017, IEEE T VIS COMPUT GR, V23, P1379, DOI 10.1109/TVCG.2017.2657139
   Kohli L., 2013, REDIRECTED TOUCHING
   Langbehn E, 2018, PROCEEDINGS OF THE VIRTUAL REALITY INTERNATIONAL CONFERENCE - LAVAL VIRTUAL (ACM VRIC 2018), DOI 10.1145/3234253.3234291
   Marchal M, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P19, DOI 10.1109/3DUI.2010.5446238
   Matsumoto K, 2019, 17TH ACM SIGGRAPH INTERNATIONAL CONFERENCE ON VIRTUAL-REALITY CONTINUUM AND ITS APPLICATIONS IN INDUSTRY (VRCAI 2019), DOI 10.1145/3359997.3365705
   Matsumoto K, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P105, DOI 10.1109/3DUI.2016.7460038
   Nagao R., 2017, ACM SIGGRAPH 2017 Posters, P1, DOI DOI 10.1145/3084822.3084838
   Nagao R, 2018, IEEE T VIS COMPUT GR, V24, P1584, DOI 10.1109/TVCG.2018.2793038
   Nilsson Niels C., 2012, Haptics: Perception, Devices, Mobility, and Communication. Proceedings International Conference (EuroHaptics 2012), P349, DOI 10.1007/978-3-642-31401-8_32
   Nilsson NC, 2014, IEEE T VIS COMPUT GR, V20, P569, DOI 10.1109/TVCG.2014.21
   Noma H, 2003, P IEEE VIRT REAL ANN, P217, DOI 10.1109/VR.2003.1191142
   Nordahl R., 2012, 2012 IEEE VR Workshop on Perceptual Illusions in Virtual Environments, P21, DOI 10.1109/PIVE.2012.6229796
   POSNER MI, 1976, PSYCHOL REV, V83, P157, DOI 10.1037/0033-295X.83.2.157
   Razzaque S., 2002, Virtual Environments 2002. Eurographics Workshop Proceedings, P123
   Razzaque S., Redirected Walking
   Rietzler Michael., 2018, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI'18, p128:1
   Schmidt D, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2157, DOI 10.1145/2702123.2702253
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schwaiger M, 2007, LECT NOTES COMPUT SC, V4551, P926
   Souman JL, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/2043603.2043607
   Steinicke F., 2009, Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, P19, DOI 10.1145/1620993.1620998
   Steinicke F, 2010, IEEE T VIS COMPUT GR, V16, P17, DOI 10.1109/TVCG.2009.62
   Stoakley R., 1995, P SIGCHI C HUM FACT, P265, DOI [10.1145/223904.223938, DOI 10.1145/223904.223938]
   Tan D. S., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P418, DOI 10.1145/365024.365307
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Vasylevska K, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P388, DOI [10.1109/VR46266.2020.00-45, 10.1109/VR46266.2020.1581099885115]
   Xin Tong, 2016, 2016 IEEE International Workshop on Mixed Reality Art (MRA), P5, DOI 10.1109/MIXRA.2016.7858996
   Zenner A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P47, DOI [10.1109/vr.2019.8798143, 10.1109/VR.2019.8798143]
   Zhang R., 2013, Proc. Proceedings of the ACM Symposium on Applied Perception (SAP), P71
NR 56
TC 3
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2783
EP 2795
DI 10.1007/s00371-021-02179-2
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000659034300002
DA 2024-07-18
ER

PT J
AU Liu, XL
   Yin, JQ
   Liu, HP
   Yin, YL
AF Liu, Xiaoli
   Yin, Jianqin
   Liu, Huaping
   Yin, Yilong
TI PISEP<SUP>2</SUP>: pseudo-image sequence evolution-based 3D pose
   prediction
SO VISUAL COMPUTER
LA English
DT Article
DE 3D pose prediction; Video prediction; 3D skeleton; Convolutional neural
   networks
AB Pose prediction is to predict future poses given a window of previous poses. In this paper, we propose a new problem that predicts poses using 3D positions of skeletal sequences.Different from the traditional pose prediction based on mocap frames, this problem is convenient to use in real applications due to its simple sensors to capture data. We also present a new framework, pseudo-image sequence evolution-based 3D pose prediction, to address this new problem. Specifically, a skeletal representation is proposed by transforming a 3D skeletal sequence into an image sequence, which can model different correlations among different joints. With this image-based skeletal representation, we model the pose prediction as the evolution of an image sequence. Moreover, a novel inference network is proposed to predict multiple future poses in a non-recursive manner using decoders with independent parameters. In contrast to the recursive sequence-to-sequence model, we can improve the computational efficiency and avoid error accumulations significantly. Extensive experiments are carried out on two benchmark datasets (e.g., G3D and FNTU). The proposed method achieves state-of-the-art performance on both datasets, which demonstrates the effectiveness of our proposed method.
C1 [Liu, Xiaoli; Yin, Jianqin] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Liu, Huaping] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Yin, Yilong] Shandong Univ, Sch Comp Sci & Technol, Jinan 250022, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Tsinghua University;
   Shandong University
RP Yin, JQ (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
EM Liuxiaoli134@bupt.edu.cn; jqyin@bupt.edu.cn
OI Liu, Xiaoli/0000-0002-9224-8649
FU National Natural Science Foundation of China [61673192]; Fundamental
   Research Funds for the Central Universities [2020XD-A04-1, 2019RC27];
   BUPT Excellent Ph.D. Students Foundation [CX2019111]
FX This work was supported partly by the National Natural Science
   Foundation of China (Grant No. 61673192), the Fundamental Research Funds
   for the Central Universities (Grant No. 2020XD-A04-1, 2019RC27), and
   BUPT Excellent Ph.D. Students Foundation (CX2019111). The research in
   this paper used the NTU RGB+D Action Recognition Dataset made available
   by the ROSE Lab at the Nanyang Technological University, Singapore.
CR [Anonymous], 2019, VISUAL COMPUT
   BABAEIZADEH M, 2017, ICLR
   Bloom V., 2012, 2012 IEEE COMP SOC C, P7, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Bors AG, 2000, IEEE T IMAGE PROCESS, V9, P1441, DOI 10.1109/83.855440
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Chiu HK, 2019, IEEE WINT CONF APPL, P1423, DOI 10.1109/WACV.2019.00156
   Cox D., 2017, INT C LEARNING REPRE
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Finn C, 2016, ADV NEUR IN, V29
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   Gui LY, 2018, LECT NOTES COMPUT SC, V11212, P441, DOI 10.1007/978-3-030-01237-3_27
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hsieh JT, 2018, ADV NEUR IN, V31
   Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573
   Kalchbrenner Nal, 2017, INT C MACHINE LEARNI, P1771
   Kamisli F, 2015, IEEE T IMAGE PROCESS, V24, P1247, DOI 10.1109/TIP.2015.2400818
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kong Y., 2018, ARXIV180611230
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Lu CC, 2017, PROC CVPR IEEE, P2137, DOI 10.1109/CVPR.2017.230
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Nie Q, 2019, IEEE T IMAGE PROCESS, V28, P3959, DOI 10.1109/TIP.2019.2907048
   Oh J., 2015, Advances in neural information processing systems, P2863
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi XJ, 2015, ADV NEUR IN, V28
   Tang, 2018, ARXIV PREPRINT ARXIV
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416
   Wang YB, 2017, ADV NEUR IN, V30
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu ZR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2940
   Yu, 2018, INT C MACH LEARN 201
   Zhang H, 2015, IEEE INT CONF ROBOT, P3053, DOI 10.1109/ICRA.2015.7139618
   Zhang JK, 2017, AAAI CONF ARTIF INTE, P2891
NR 47
TC 4
Z9 4
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2603
EP 2616
DI 10.1007/s00371-021-02135-0
EA APR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000643168400001
DA 2024-07-18
ER

PT J
AU Xie, JH
   Ge, YX
   Zhang, JY
   Huang, S
   Chen, FY
   Wang, HX
AF Xie, Jiahong
   Ge, Yongxin
   Zhang, Junyin
   Huang, Sheng
   Chen, Feiyu
   Wang, Hongxing
TI Low-resolution assisted three-stream network for person
   re-identification
SO VISUAL COMPUTER
LA English
DT Article
DE Person re-identification; Image quality; Multi-stream network; Low
   resolution
AB In the commonly used datasets of person re-identification, the image quality is not uniform. Most existing methods on person re-identification mainly focus on the challenges caused by occlusion, view and pose variations, ignoring the diversity of person image quality. In this paper, we provide an intuitive solution to address this problem. Specifically, we generate low-resolution images by reducing the resolution of original person images and propose a low-resolution assisted three-stream network (LRAN) to fuse the extracted person features from original RGB images, low-resolution images and greyscale images into a more robust feature as the final person representation. In this way, the model eliminates the impact of image quality differences to some extent. Experimental results demonstrate that the proposed method achieves the state-of-the-art results on Market-1501, DukeMTMC-reID and CUHK03-NP datasets.
C1 [Ge, Yongxin; Huang, Sheng; Chen, Feiyu; Wang, Hongxing] Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
   [Xie, Jiahong; Ge, Yongxin; Zhang, Junyin; Huang, Sheng; Chen, Feiyu; Wang, Hongxing] Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China.
C3 Chongqing University; Chongqing University
RP Ge, YX (corresponding author), Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.; Ge, YX (corresponding author), Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China.
EM yongxinge@cqu.edu.cn
RI Wang, Hongxing/F-4670-2011
FU National Natural Science Foundation of China [61772093]; Graduate
   Research and Innovation Foundation of Chongqing, China [CYS18065];
   Chongqing Research Program of Basic Science & Frontier Technology
   [cstc2018jcyjAX0 410]; Chongqing Major Theme Program
   [cstc2017zdcyzdzxX0002]; Fundamental Research Funds for the Central
   Universities [2019CDYGYB014, 2019CDCGRJ217, 2019CDXYRJ0011]
FX The work described in this paper was partially supported by the National
   Natural Science Foundation of China (Grant No. 61772093), Graduate
   Research and Innovation Foundation of Chongqing, China (Grant No.
   CYS18065), Chongqing Research Program of Basic Science & Frontier
   Technology (Grant No. cstc2018jcyjAX0 410), the Chongqing Major Theme
   Program (Grant No. cstc2017zdcyzdzxX0002) and the Fundamental Research
   Funds for the Central Universities (Grant No. 2019CDYGYB014, Grant No.
   2019CDCGRJ217 and 2019CDXYRJ0011).
CR Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen YC, 2020, IEEE T NETW SCI ENG, V7, P3279, DOI 10.1109/TNSE.2020.3024723
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   Fan B, 2021, IEEE T MULTIMEDIA, V23, P2770, DOI 10.1109/TMM.2020.3016122
   Fan X, 2022, VISUAL COMPUT, V38, P279, DOI 10.1007/s00371-020-02015-z
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo YL, 2018, PROC CVPR IEEE, P2335, DOI 10.1109/CVPR.2018.00248
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li D, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209602
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu HM, 2021, NEUROCOMPUTING, V423, P57, DOI 10.1016/j.neucom.2020.10.019
   Mochizuki I, 2018, VISUAL COMPUT, V34, P1031, DOI 10.1007/s00371-018-1518-6
   Qi Lei, 2019, ARXIV190800862
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen YT, 2018, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR.2018.00720
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   ZHONG Z, 2018, PROC CVPR IEEE, P5157, DOI DOI 10.1109/CVPR.2018.00541
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou XZ, 2020, IEEE T AFFECT COMPUT, V11, P542, DOI 10.1109/TAFFC.2018.2828819
NR 42
TC 10
Z9 10
U1 1
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2515
EP 2525
DI 10.1007/s00371-021-02127-0
EA APR 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000640468600001
DA 2024-07-18
ER

PT J
AU Dhengre, N
   Sinha, S
AF Dhengre, Nikhil
   Sinha, Saugata
TI K sparse autoencoder-based accelerated reconstruction of magnetic
   resonance imaging
SO VISUAL COMPUTER
LA English
DT Article
DE Magnetic resonance imaging; Compressive sensing magnetic resonance
   imaging reconstruction; K sparse autoencoder; Cascaded reconstruction
ID MRI RECONSTRUCTION; ALGORITHM; NETWORKS
AB Owing to the sequential collection of phase encoded data in k-space, magnetic resonance (MR) imaging suffers from long acquisition time. One possible measure to reduce the long acquisition time is to reconstruct MR image using a subset of k-space MR data rather than the complete set. In this work, we propose to implement a K sparse autoencoder model for reconstruction of MR image from undersampled k-space data. Autoencoder models, which have shown great ability in capturing the complex features of input data, can be used to reconstruct high-quality MR image. The reconstruction process involved solving an optimization problem whose solution was expected to satisfy the data consistency and also lie in close proximity to the output space of trained K sparse autoencoder. Observing the effect of sparsity value enforced by K sparse autoencoder on the reconstructed output, we implemented the cascaded form of reconstruction, incorporating three K sparse autoencoders with three different K values. Using MR-PD and MR-T1 images, reconstruction performance of the proposed approach was compared with those of the conventional reconstruction approaches. The quantitative as well as the qualitative analysis of the reconstructed images, obtained using the proposed approach, validates the efficiency of the proposed approach.
C1 [Dhengre, Nikhil; Sinha, Saugata] Visvesvaraya Natl Inst Technol, Dept Elect & Commun Engn, Nagpur, Maharashtra, India.
C3 National Institute of Technology (NIT System); Visvesvaraya National
   Institute of Technology, Nagpur
RP Dhengre, N (corresponding author), Visvesvaraya Natl Inst Technol, Dept Elect & Commun Engn, Nagpur, Maharashtra, India.
EM nikits40@gmail.com; saugata.sinha@ece.vnit.ac.in
RI Sinha, Saugata/AAA-7696-2022; Dhengre, Nikhil/JXN-6260-2024
CR Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bakas Spyridon, 2018, ARXIV181102629, DOI DOI 10.17863/CAM.38755
   Baldi P., 2012, P ICML WORKSH UNS TR, P37, DOI [10.5555/3045796.3045801., DOI 10.1561/2200000006, 10.1561/2200000006]
   Chen C., 2012, P INT C ADV NEUR INF, V25, P1115
   Chen SS, 2017, INT J BIOMED IMAGING, V2017, DOI 10.1155/2017/9604178
   Chen YM, 2010, INVERSE PROBL IMAG, V4, P223, DOI 10.3934/ipi.2010.4.223
   Deshmane A, 2012, J MAGN RESON IMAGING, V36, P55, DOI 10.1002/jmri.23639
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan XY, 2018, APPL MAGN RESON, V49, P465, DOI 10.1007/s00723-018-0988-z
   Guerquin-Kern M, 2009, I S BIOMED IMAGING, P193, DOI 10.1109/ISBI.2009.5193016
   Huang J., 2019, MATH PROBL ENG, V2019, P14, DOI [10.1155/2019/3694604, DOI 10.1155/2019/3694604]
   Huang JP, 2017, APPL MAGN RESON, V48, P749, DOI 10.1007/s00723-017-0910-0
   Huang JZ, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P968, DOI 10.1109/ISBI.2012.6235718
   Huang JZ, 2011, MED IMAGE ANAL, V15, P670, DOI 10.1016/j.media.2011.06.001
   Jiang CH, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-27261-z
   Lai ZY, 2016, MED IMAGE ANAL, V27, P93, DOI 10.1016/j.media.2015.05.012
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Liang Z-P, 2000, PRINCIPLES MAGNETIC
   Liu RW, 2019, MULTIMED TOOLS APPL, V78, P12749, DOI 10.1007/s11042-018-6028-z
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Ma SQ, 2008, PROC CVPR IEEE, P389
   Makhzani A, 2013, ARXIV13125663, P5663
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Qu XB, 2014, MED IMAGE ANAL, V18, P843, DOI 10.1016/j.media.2013.09.007
   Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019
   Qu XB, 2010, INVERSE PROBL SCI EN, V18, P737, DOI 10.1080/17415977.2010.492509
   Qu XB, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON IT IN MEDICINE AND EDUCATION, VOLS 1 AND 2, PROCEEDINGS, P693, DOI 10.1109/ITME.2008.4743955
   Ragab M, 2020, NEURAL COMPUT APPL, V32, P2705, DOI 10.1007/s00521-018-3812-7
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   Schlemper J, 2017, LECT NOTES COMPUT SC, V10265, P647, DOI 10.1007/978-3-319-59050-9_51
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Quan TM, 2018, IEEE T MED IMAGING, V37, P1488, DOI 10.1109/TMI.2018.2820120
   Wang SS, 2016, I S BIOMED IMAGING, P514, DOI 10.1109/ISBI.2016.7493320
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu DF, 2017, IEEE T MED IMAGING, V36, P2479, DOI 10.1109/TMI.2017.2753138
   Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879
   Zhu Z.Q., 2011, Electrical Machines and Systems (ICEMS), 2011 International Conference on, P1
   Zhuang PX, 2019, SIGNAL PROCESS, V155, P346, DOI 10.1016/j.sigpro.2018.10.005
NR 39
TC 6
Z9 6
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 837
EP 847
DI 10.1007/s00371-020-02054-6
EA JAN 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000613039600001
DA 2024-07-18
ER

PT J
AU Wu, ZY
   Meng, K
   Wo, Y
   Zhong, XD
AF Wu, Zhangyong
   Meng, Ke
   Wo, Yan
   Zhong, Xudong
TI Masquerade attack on biometric hashing via BiohashGAN
SO VISUAL COMPUTER
LA English
DT Article
DE Masquerade attack; Biometric hashing; Image reconstruction; Generative
   adversarial network
AB Masquerade attack on biometric hashing, which reconstructs the original biometric image from the given hashcode, has been given much attention recently. It is mainly used to validate the security of biometric recognition system or expand existing biometric databases like face or iris. However, an existing state-of-the-art method tends to ignore the perceptual quality of synthesized biometric images in the attack, and consequently, the synthetic images can be easily differentiated from real images. To obtain the high-perceptual-quality image which can simultaneously pass the validation of recognition system, we introduce a new target combining semantic invariability in hashing space and perceptual similarity in biometric space. In order to simulate the mapping from images to hashcodes and tackle the derivative problem related to discrete hashcodes in hashing space, we propose a DNN-based network named SimHashNet. Then we incorporate the SimHashNet into a generative adversarial network as our model named BiohashGAN to generate synthetic images form hashcodes. Experiment result on dataset CASIA-IrisV4.0-Interval and CMU PIE demonstrates that the synthetic images obtained from our model can pass the validation of recognition system and simultaneously maintain high perceptual quality.
C1 [Wu, Zhangyong; Meng, Ke; Wo, Yan; Zhong, Xudong] South China Univ Technol, Guangzhou, Peoples R China.
C3 South China University of Technology
RP Wo, Y (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.
EM angelorlover@hotmail.com; shadowkm@163.com; woyan@scut.edu.cn;
   491910429@qq.com
OI Wo, Yan/0000-0003-1001-4425
FU National Natural Science Foundation of Guangdong [2018A030313994,
   2017A030312008]; Guangzhou science and technology plan project
   [202002030298]
FX This work is supported by the National Natural Science Foundation of
   Guangdong [Grant No.2018A030313994, Grant No. 2017A030312008] and the
   Guangzhou science and technology plan project [Grant No.202002030298].
CR Adler A, 2003, CCECE 2003: CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3, PROCEEDINGS, P1163
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Feng YC, 2014, PATTERN RECOGN, V47, P3019, DOI 10.1016/j.patcog.2014.03.003
   Galbally J, 2013, COMPUT VIS IMAGE UND, V117, P1512, DOI 10.1016/j.cviu.2013.06.003
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jain AK, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/579416
   Jin ATB, 2004, PATTERN RECOGN, V37, P2245, DOI 10.1016/j.patcog.2004.04.011
   Kaplan E, 2020, IEEE T DEPEND SECURE, V17, P443, DOI 10.1109/TDSC.2017.2759732
   Lacharme P, 2013, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY (SECRYPT 2013), P363
   Lee YC, 2009, CCGRID: 2009 9TH IEEE INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, P92, DOI 10.1109/CCGRID.2009.16
   Nagar A, 2010, PROC SPIE, V7541, DOI 10.1117/12.839976
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Ross A, 2005, PROC SPIE, V5779, P68, DOI 10.1117/12.604477
   Venugopalan S, 2011, IEEE T INF FOREN SEC, V6, P385, DOI 10.1109/TIFS.2011.2108288
   Wang YW, 2019, IEEE SIGNAL PROC LET, V26, P1295, DOI 10.1109/LSP.2019.2917073
   Wang YW, 2018, SIGNAL PROCESS-IMAGE, V68, P68, DOI 10.1016/j.image.2018.06.018
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
NR 24
TC 0
Z9 0
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 821
EP 835
DI 10.1007/s00371-020-02053-7
EA JAN 2021
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000612910400001
DA 2024-07-18
ER

PT J
AU Agrawal, SC
   Jalal, AS
AF Agrawal, Subhash Chand
   Jalal, Anand Singh
TI Distortion-free image dehazing by superpixels and ensemble neural
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Superpixel; Ensemble network; Artifacts; Blurring
   effect; Saturation
ID CONTRAST ENHANCEMENT; SINGLE; ARCHITECTURE; FRAMEWORK; HAZE
AB Single image dehazing is a technique used to remove the effect of haze from an image captured in poor weather conditions. Due to the scattering of particles, a captured image suffers from low visibility and contrast. Besides, scattering also adds nonlinear noise to the captured image. Existing image dehazing methods improve the visibility of the hazy image. However, these methods significantly generate artifacts such as halo at the depth discontinuities, blocking, and color aliasing in the sky regions. Some methods addressed this problem, but these methods introduce other issues such as loss of details, blurring effects, and oversaturation in the dehazed image. This paper proposes a method using superpixels and ensemble nonlinear regression to estimate the transmission that improves the visibility of a hazy image without any artifact. Conventional machine learning methods require a vast amount of haze-free and hazy images of different haze concentrations to train the model. The use of superpixels offers less number of training examples and also helps in reducing halo artifacts. The ensemble nonlinear regression predicts the transmission for a superpixel in such a way that the recovered image looks more natural, especially in the sky regions. The proposed method is evaluated by the various distortion parameters on real-world challenging and synthetic hazy images. The qualitative and quantitative analysis in experimental results proves that the proposed method is superior to that of state-of-the-art dehazing methods.
C1 [Agrawal, Subhash Chand; Jalal, Anand Singh] GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
C3 GLA University
RP Agrawal, SC (corresponding author), GLA Univ, Dept Comp Engn & Applicat, Mathura 281406, UP, India.
EM subhash.agrawal@gla.ac.in; asjalal@gla.ac.in
OI Jalal, Anand/0000-0002-7469-6608; Agrawal, Subhash
   Chand/0000-0002-5115-4873
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Amini K, 2015, J COMPUT APPL MATH, V288, P341, DOI 10.1016/j.cam.2015.04.040
   Ancuti Codruta O., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P501, DOI 10.1007/978-3-642-19309-5_39
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cadena C, 2014, IEEE INT CONF ROBOT, P2639, DOI 10.1109/ICRA.2014.6907237
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Cozman F, 1997, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.1997.609419
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Dai Tang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P765, DOI 10.1109/ICME.2012.184
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fan X, 2017, IEEE T CIRC SYST VID, V27, P2505, DOI 10.1109/TCSVT.2016.2592328
   FATTAL R, 2008, ACM T GRAPHIC, V27
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Kumar R, 2021, IEEE T INTELL TRANSP, V22, P6536, DOI 10.1109/TITS.2020.2993906
   Kumar R, 2019, IEEE T VLSI SYST, V27, P2693, DOI 10.1109/TVLSI.2019.2932033
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Qian W, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/4945214
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Salazar-Colores S, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.4.043022
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Singh D, 2018, MULTIMED TOOLS APPL, V77, P9595, DOI 10.1007/s11042-017-5321-6
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Van den Bergh M, 2013, IEEE WORK APP COMP, P363, DOI 10.1109/WACV.2013.6475041
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhang H., 2013, J COMPUTLNF SYST, V9, P1623
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 48
TC 9
Z9 9
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 781
EP 796
DI 10.1007/s00371-020-02049-3
EA JAN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000604815100002
DA 2024-07-18
ER

PT J
AU Huang, LY
   Liu, F
AF Huang, Linyuan
   Liu, Feng
TI Retinal vessel segmentation using simple SPCNN model and line connector
SO VISUAL COMPUTER
LA English
DT Article
DE Retinal vessel; Image segmentation; Pulse coupled neural network; Line
   connector
ID BLOOD-VESSELS; GRAY-LEVEL; IMAGES; NETWORK
AB The effective segmentation of retinal blood vessels is essential for the medical diagnosis of ophthalmology diseases. In this paper, a novel approach is presented to segment retinal vessels accurately and efficiently. Firstly, we propose a simple simplified pulse coupled neural network utilizing the similarity of adjacent neurons to acquire the basic structure of blood vessels. Then we apply a line connector to solve the problem of broken vessels occurring in the segmentation, in order to present a complete structure of the blood vessels and improve the accuracy of vessel identification. Experimental analyses on two publicly available databases show that the proposed methods with or without the line connector outperform the most existing methods in terms of average accuracy and have a fast response time. It is of great importance for medical diagnosis with high accuracy and short time consumption. Our methods are practicable either for retinal vessel segmentation, or for other applications of clinical research.
C1 [Huang, Linyuan; Liu, Feng] Shanghai Maritime Univ, Coll Informat Engn, Shanghai 201306, Peoples R China.
C3 Shanghai Maritime University
RP Liu, F (corresponding author), Shanghai Maritime Univ, Coll Informat Engn, Shanghai 201306, Peoples R China.
EM liufeng@shmtu.edu.cn
OI Huang, Linyuan/0000-0002-6756-5271
FU National Natural Science Foundation of China [U1701265]
FX The authors acknowledge the funding support from the National Natural
   Science Foundation of China under Grant U1701265.
CR Araújo RJ, 2019, LECT NOTES COMPUT SC, V11752, P59, DOI 10.1007/978-3-030-30645-8_6
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Chen Y, 2011, IEEE T NEURAL NETWOR, V22, P880, DOI 10.1109/TNN.2011.2128880
   Dixon R.N., 1979, I PHYS C SERIES, V44, P178
   Ekblad U, 2004, NUCL INSTRUM METH A, V525, P392, DOI 10.1016/j.nima.2004.03.102
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P600, DOI 10.1016/j.cmpb.2011.08.009
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P407, DOI 10.1016/j.cmpb.2012.03.009
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Gonzalez R.C., 2009, DIGITAL IMAGE PROCES, V9, P468
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Huang Y, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.6.061603
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   JOHNSON JL, 1993, OPT LETT, V18, P1253, DOI 10.1364/OL.18.001253
   Khomri B, 2018, IET IMAGE PROCESS, V12, P2163, DOI 10.1049/iet-ipr.2018.5425
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Marín D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Nguyen UTV, 2013, PATTERN RECOGN, V46, P703, DOI 10.1016/j.patcog.2012.08.009
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ranganath H. S., 1995, Proceedings IEEE Southeastcon '95. Visualize the Future (Cat. No.95CH35793), P37, DOI 10.1109/SECON.1995.513053
   Remeseiro B, 2021, VISUAL COMPUT, V37, P1247, DOI 10.1007/s00371-020-01863-z
   Ricci E, 2007, IEEE T MED IMAGING, V26, P1357, DOI 10.1109/TMI.2007.898551
   Shah SAA, 2019, IEEE ACCESS, V7, P167221, DOI 10.1109/ACCESS.2019.2954314
   Sheng B, 2019, IEEE T CYBERNETICS, V49, P2707, DOI 10.1109/TCYB.2018.2833963
   Shukla AK, 2020, BIOMED SIGNAL PROCES, V59, DOI 10.1016/j.bspc.2020.101883
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Walter T, 2007, MED IMAGE ANAL, V11, P555, DOI 10.1016/j.media.2007.05.001
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang WH, 2019, MED BIOL ENG COMPUT, V57, P1481, DOI 10.1007/s11517-019-01967-2
   Yang Y, 2019, SIGNAL IMAGE VIDEO P, V13, P1529, DOI 10.1007/s11760-019-01501-9
   Yang Z, 2018, NEUROCOMPUTING, V285, P196, DOI 10.1016/j.neucom.2018.01.044
   Yao JC, 2017, VISUAL COMPUT, V33, P179, DOI 10.1007/s00371-015-1171-2
   Yin BJ, 2015, MED IMAGE ANAL, V26, P232, DOI 10.1016/j.media.2015.09.002
   You XG, 2011, PATTERN RECOGN, V44, P2314, DOI 10.1016/j.patcog.2011.01.007
   Zhan K, 2009, IEEE T NEURAL NETWOR, V20, P1980, DOI 10.1109/TNN.2009.2030585
   Zhou C, 2020, COMPUT METH PROG BIO, V187, DOI 10.1016/j.cmpb.2019.105231
   Zuiderveld K, 1994, GRAPH GEMS
   Zwiggelaar R, 2004, IEEE T MED IMAGING, V23, P1077, DOI 10.1109/TMI.2004.828675
   Zwiggelaar R., 1996, Proceedings of the 7th British Machine Vision Conference, P715, DOI DOI 10.5244/C.10.70
NR 39
TC 6
Z9 7
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 135
EP 148
DI 10.1007/s00371-020-02008-y
EA NOV 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000587944700001
DA 2024-07-18
ER

PT J
AU Jiang, XB
   Jin, Y
   Yao, Y
AF Jiang, Xiaoben
   Jin, Yan
   Yao, Yu
TI Low-dose CT lung images denoising based on multiscale parallel
   convolution neural network
SO VISUAL COMPUTER
LA English
DT Article
DE CT lung image denoising; Multiscale parallel; Convolution; Neural
   network; Dilated convolution; Residual learning
ID TOMOGRAPHY; ALGORITHMS
AB The continuous development and wide application of CT in medical practice have raised public concern over the associated radiation dose to the patient. However, reducing the radiation dose may result in increasing the noise and artifacts, which may adversely interfere with the judgment and belief of radiologists. Therefore, we propose a low-dose CT denoising model based on multiscale parallel convolution neural network to improve the visual effect. Residual learning is utilized to reduce the difficulty of network learning, and batch normalization is adopted to solve the problem of performance degradation due to the increase in neural network layers. Specifically, we introduce the dilated convolution to expand the receptive field by inserting weights of zero in the standard convolution kernel, while not increasing the extra parameters. Furthermore, the multiscale parallel method is utilized to extract multiscale detail features from lung images. Compared to the traditional methods such as Wiener filter, NLM, and models based on CNN, e.g., SCNN, DnCNN, our extensive experimental results demonstrate that our proposed model (CT-ReCNN) can not only reduce the LDCT lung images noise level, but also retain more exact information as well.
C1 [Jiang, Xiaoben; Jin, Yan; Yao, Yu] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Jin, Y (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
EM jy@zjut.edu.cn
OI Jiang, Xiaoben/0000-0002-9454-0037; Jin, Yan/0000-0001-8956-7684
FU Zhejiang Provincial Natural Science Foundation of China [LY17F010015]
FX This research was supported by Zhejiang Provincial Natural Science
   Foundation of China under Grant No. LY17F010015.
CR Aapm, 2017, Low Dose CT Grand Challenge
   Bhongade S., 2013, IEEE INT C MACH INT
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Chen H, 2017, IEEE T MED IMAGING, V36, P2524, DOI 10.1109/TMI.2017.2715284
   Ding Y, 2019, ELECTRON LETT, V55, P174, DOI 10.1049/el.2018.6449
   Giachetti A, 2016, VISUAL COMPUT, V32, P693, DOI 10.1007/s00371-016-1234-z
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hsieh J, 1998, MED PHYS, V25, P2139, DOI 10.1118/1.598410
   Isogawa K, 2018, IEEE SIGNAL PROC LET, V25, P224, DOI 10.1109/LSP.2017.2782270
   Jia LN, 2019, J MED IMAG HEALTH IN, V9, P140, DOI 10.1166/jmihi.2019.2552
   Jifara W, 2019, J SUPERCOMPUT, V75, P704, DOI 10.1007/s11227-017-2080-0
   Jin Y, 2019, IET IMAGE PROCESS, V13, P1970, DOI 10.1049/iet-ipr.2019.0241
   Kachelriess M, 2001, MED PHYS, V28, P475, DOI 10.1118/1.1358303
   Kingma D.P., 2015, NEURAL INFORM PROCES
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li H, 2020, VISUAL COMPUT, V36, P1693, DOI 10.1007/s00371-019-01769-5
   Li ZB, 2014, MED PHYS, V41, DOI 10.1118/1.4851635
   Liu Y, 2018, NEUROCOMPUTING, V284, P80, DOI 10.1016/j.neucom.2018.01.015
   Manduca A, 2009, MED PHYS, V36, P4911, DOI 10.1118/1.3232004
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   NAIDICH DP, 1990, RADIOLOGY, V175, P729, DOI 10.1148/radiology.175.3.2343122
   Peters TM, 2001, J PHYS MATH NUCL GEN, V6
   Ramírez RR, 2011, NEUROIMAGE, V56, P78, DOI 10.1016/j.neuroimage.2011.02.002
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Salimans T, 2016, ADV NEUR IN, V29
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   SCHULZ TJ, 1991, J OPT SOC AM A, V8, P801, DOI 10.1364/JOSAA.8.000801
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith-Bindman R, 2009, ARCH INTERN MED, V169, P2078, DOI 10.1001/archinternmed.2009.427
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   XIE CG, 1992, IEE PROC-G, V139, P89, DOI 10.1049/ip-g-2.1992.0015
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yu F., 2016, INT C LEARN REPR COM
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 36
TC 22
Z9 23
U1 1
U2 32
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2419
EP 2431
DI 10.1007/s00371-020-01996-1
EA NOV 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000587098900001
DA 2024-07-18
ER

PT J
AU Xu, NY
   Zhu, Q
   Xu, XY
   Zhang, DQ
AF Xu, Nuoya
   Zhu, Qi
   Xu, Xiangyu
   Zhang, Daoqiang
TI An effective recognition approach for contactless palmprint
SO VISUAL COMPUTER
LA English
DT Article
DE Biometrics; Contactless palmprint; Spatial transformer networks; ResNet
AB The biometrics character has been widely used for individual identification and verification. Palmprint as one of biological features contains abundant discriminative features, which has already attracted a lot of interest. In this work, we focus on the identification and verification of contactless palmprint images. Considering the main differences between contact and contactless images, including orientation and deformation, we use a deep network combined with image alignment to further improve the recognition performance of contactless palmprint images. Recently, convolutional neural networks can well solve many classification problems, and researchers have proposed many networks with different architectures. We exploit the residual network in our framework, which achieves promising performance on the image classification problem. In order to improve the accuracy of verification, the spatial transformation network is used to align the image. The proposed method is tested on two public palmprint databases CASIA, GPDS. Extensive experiments are carried out with several state-of-the-art approaches as comparison, and the results demonstrated the effectiveness of our method.
C1 [Xu, Nuoya; Zhu, Qi; Xu, Xiangyu; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Xu, Nuoya; Zhu, Qi] Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210093, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Zhu, Q (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.; Zhu, Q (corresponding author), Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210093, Peoples R China.
EM zhuqi@nuaa.edu.cn
FU National Natural Science Foundation of China [61501230, 61732006,
   61876082]; National Key R&D Program of China [2018YFC2001600,
   2018YFC2001602]; National Science and Technology Major Project
   [2018ZX10201002]; Fundamental Research Funds for the Central
   Universities [NJ2019010]
FX This work was supported in part by National Natural Science Foundation
   of China (Nos. 61501230, 61732006 and 61876082), the National Key R&D
   Program of China (Grant Nos. 2018YFC2001600, 2018YFC2001602), National
   Science and Technology Major Project (No. 2018ZX10201002), and the
   Fundamental Research Funds for the Central Universities (No. NJ2019010).
CR Aberni Y, 2017, 2017 40TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P793, DOI 10.1109/TSP.2017.8076097
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 2011, PALMPR IM DAT
   [Anonymous], 2005, PALMPR IM DAT
   Buko B., 2022, ABS151203385 CORR, V22, P8878
   Connie T, 2005, IMAGE VISION COMPUT, V23, P501, DOI 10.1016/j.imavis.2005.01.002
   Cui JR, 2014, NEURAL COMPUT APPL, V24, P497, DOI 10.1007/s00521-012-1265-y
   Fei LK, 2019, IEEE T SYST MAN CY-S, V49, P346, DOI 10.1109/TSMC.2018.2795609
   Fei LK, 2016, NEUROCOMPUTING, V218, P264, DOI 10.1016/j.neucom.2016.08.048
   Fei LK, 2016, PATTERN RECOGN LETT, V69, P35, DOI 10.1016/j.patrec.2015.10.003
   Fei LK, 2016, PATTERN RECOGN, V49, P89, DOI 10.1016/j.patcog.2015.08.001
   Hong L, 1998, IEEE T PATTERN ANAL, V20, P777, DOI 10.1109/34.709565
   Huang XJ, 2018, PATTERN RECOGN, V81, P388, DOI 10.1016/j.patcog.2018.03.014
   Jaderberg M., 2015, SPATIAL TRANSFORMER, P1
   Jaswal G, 2017, LECT NOTES COMPUT SC, V10597, P233, DOI 10.1007/978-3-319-69900-4_30
   Kong A, 2006, PATTERN RECOGN, V39, P478, DOI 10.1016/j.patcog.2005.08.014
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, IEEE INT CONF MULTI
   Kumar A, 2019, IEEE T INF FOREN SEC, V14, P34, DOI 10.1109/TIFS.2018.2837669
   Laadjel M, 2013, J REAL-TIME IMAGE PR, V8, P253, DOI 10.1007/s11554-011-0230-9
   Li W, 2011, IEEE T SYST MAN CY C, V41, P274, DOI 10.1109/TSMCC.2010.2055849
   Lu GM, 2003, PATTERN RECOGN LETT, V24, P1463, DOI 10.1016/S0167-8655(02)00386-0
   Lunke Fei, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P213, DOI 10.1007/978-3-319-69923-3_23
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025
   Phromsuthirak C., 2014, SIGN INF PROC ASS AN, P1
   Raghavendr R, 2018, INT CONF BIOMETR, P209, DOI 10.1109/ICB2018.2018.00040
   Rida I, 2018, IEEE ACCESS, V6, P3241, DOI 10.1109/ACCESS.2017.2787666
   Satya V, 2019, INT ARAB J INF TECHN, V1, P1
   Simonyan K., 2015, ICLR, V9
   Sun ZN, 2005, PROC CVPR IEEE, P279
   Wang K, 2019, PATTERN RECOGN, V86, P85, DOI 10.1016/j.patcog.2018.08.010
   Wu XQ, 2003, PATTERN RECOGN LETT, V24, P2829, DOI 10.1016/S0167-8655(03)00141-7
   Xu XP, 2012, SENSORS-BASEL, V12, P4633, DOI 10.3390/s120404633
   Xu XB, 2016, NEURAL COMPUT APPL, V27, P143, DOI 10.1007/s00521-014-1570-8
   Yang WK, 2016, NEUROCOMPUTING, V213, P183, DOI 10.1016/j.neucom.2015.11.134
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhang D, 2009, ADV INFORM COMMUNICA
   Zhang D, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071391
   Zhang D, 2010, IEEE T INSTRUM MEAS, V59, P480, DOI 10.1109/TIM.2009.2028772
   Zhang L, 2004, IEEE T SYST MAN CY B, V34, P1335, DOI 10.1109/TSMCB.2004.824521
   Zhang L, 2017, PATTERN RECOGN, V69, P199, DOI 10.1016/j.patcog.2017.04.016
   Zheng Q, 2016, IEEE T INF FOREN SEC, V11, P633, DOI 10.1109/TIFS.2015.2503265
   Zhou YB, 2011, IEEE T INF FOREN SEC, V6, P1259, DOI 10.1109/TIFS.2011.2158423
NR 44
TC 9
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 695
EP 705
DI 10.1007/s00371-020-01962-x
EA AUG 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000563612200003
DA 2024-07-18
ER

PT J
AU Mikaeli, E
   Aghagolzadeh, A
   Azghani, M
AF Mikaeli, Elhameh
   Aghagolzadeh, Ali
   Azghani, Masoumeh
TI Single-image super-resolution via patch-based and group-based local
   smoothness modeling
SO VISUAL COMPUTER
LA English
DT Article
DE Image super-resolution; Sparse representation; Nonlocal self-similarity;
   Local smoothness; Split Bergman iteration
ID SPARSE REPRESENTATION; ALGORITHMS; REGULARIZATION; INTERPOLATION;
   RECOVERY
AB Local smoothness and nonlocal self-similarity of natural images are two main priors in the image restoration (IR) problem. Many IR methods have widely used patch-based modeling. Recently, the concept of grouping-based technique, the nonlocal patches with similar structures, has been introduced as the basic unit of sparse representation. In the group-based methods, the nonlocal self-similarity and the local sparsity properties are combined in a unified framework using the sparsity-based techniques. In this paper, a new model is proposed which utilizes both the patch and the group as the basic units of image modeling, called patch-based and group-based local smoothness modeling (PGLSM). More, precisely, in the proposed PGLSM scheme, the local smoothness in the patch-based unit is exploited by an isotropic total variation method and the local smoothness in the group-based unit is exploited by group-based sparse representation method. In this way, a novel technique for high-fidelity single-image super-resolution (SISR) via PGLSM is proposed, called SR-PGLSM. By adding nonlocal means (NLM) as the complementary regularization term to PGLSM, another technique for SISR is modeled, called SR_PGLSM_NLM. In order to efficiently solve the above variational problems, the split Bergman iterative technique has been leveraged. Extensive experimental results validate the effectiveness and robustness of the proposed methods. Our proposed schemes can recover more fine structures and achieve better results than the competing methods with the scaling factor of 2 and 3 and for noisy images both subjectively and objectively in most cases.
C1 [Mikaeli, Elhameh; Aghagolzadeh, Ali] Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
   [Azghani, Masoumeh] Sahand Univ Technol, Fac Elect & Comp Engn, Tabriz, Iran.
C3 Babol Noshirvani University of Technology; Sahand University of
   Technology
RP Aghagolzadeh, A (corresponding author), Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
EM e.mikaiili@stu.nit.ac.ir; aghagol@nit.ac.ir; mazghani@sut.ac.ir
RI Aghagolzadeh, Ali/AAA-7757-2021
OI Aghagolzadeh, Ali/0000-0002-6999-3464
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candocia FM, 1999, IEEE T NEURAL NETWOR, V10, P372, DOI 10.1109/72.750566
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, J VIS COMMUN IMAGE R, V24, P1055, DOI 10.1016/j.jvcir.2013.06.019
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Elad M, 2009, IEEE T INFORM THEORY, V55, P4701, DOI 10.1109/TIT.2009.2027565
   Eslahi N, 2016, NEUROCOMPUTING, V200, P88, DOI 10.1016/j.neucom.2016.03.013
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Hughes NP, 2003, P SOC PHOTO-OPT INS, V5207, P763, DOI 10.1117/12.506045
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402
   KWOK W, 1993, IEEE T CONSUM ELECTR, V39, P455, DOI 10.1109/30.234620
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li J, 2016, NEUROCOMPUTING, V184, P196, DOI 10.1016/j.neucom.2015.07.139
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Moreau Jean-Jacques, 1965, B SOC MATH FRANCE, V93, P273, DOI DOI 10.24033/BSMF.1625
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Rakotomamonjy A, 2011, SIGNAL PROCESS, V91, P1505, DOI 10.1016/j.sigpro.2011.01.012
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Soltani-Farani A, 2015, IEEE T GEOSCI REMOTE, V53, P527, DOI 10.1109/TGRS.2014.2325067
   Song Q, 2018, IEEE T IMAGE PROCESS, V27, P1966, DOI 10.1109/TIP.2017.2789323
   STARK H, 1989, J OPT SOC AM A, V6, P1715, DOI 10.1364/JOSAA.6.001715
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tao DP, 2015, INFORM SCIENCES, V320, P383, DOI 10.1016/j.ins.2015.03.031
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tropp JA, 2006, SIGNAL PROCESS, V86, P589, DOI 10.1016/j.sigpro.2005.05.031
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JH, 2008, CRYST RES TECHNOL, V43, P999, DOI 10.1002/crat.200800010
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang XM, 2019, VISUAL COMPUT, V35, P1755, DOI 10.1007/s00371-018-1570-2
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2016, IEEE SIGNAL PROC LET, V23, P102, DOI 10.1109/LSP.2015.2504121
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
NR 63
TC 10
Z9 10
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1573
EP 1589
DI 10.1007/s00371-019-01756-w
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ2RH
UT WOS:000565894000001
DA 2024-07-18
ER

PT J
AU Scheidegger, F
   Istrate, R
   Mariani, G
   Benini, L
   Bekas, C
   Malossi, C
AF Scheidegger, Florian
   Istrate, Roxana
   Mariani, Giovanni
   Benini, Luca
   Bekas, Costas
   Malossi, Cristiano
TI Efficient image dataset classification difficulty estimation for
   predicting deep-learning accuracy
SO VISUAL COMPUTER
LA English
DT Article
DE Dataset characterization; Classification difficulty; Deep learning;
   Image classification
AB In the deep-learning community, new algorithms are published at a very fast pace. Therefore, solving an image classification problem for new datasets becomes a challenging task, as it requires to re-evaluate published algorithms and their different configurations in order to find a close to optimal classifier. To facilitate this process, before biasing our decision toward a class of neural networks or running an expensive search over the network space, we propose to estimate the classification difficulty of the dataset. Our method computes a single number that characterizes the dataset difficulty 97x faster than training state-of-the-art networks. The proposed method can be used in combination with network topology and hyper-parameter search optimizers to efficiently drive the search toward promising neural network configurations.
C1 [Scheidegger, Florian; Benini, Luca] Swiss Fed Inst Technol, Ramistr 101, CH-8092 Zurich, Switzerland.
   [Scheidegger, Florian; Istrate, Roxana; Mariani, Giovanni; Bekas, Costas; Malossi, Cristiano] IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
   [Istrate, Roxana] Queens Univ Belfast, Univ Rd, Belfast BT7 1NN, Antrim, North Ireland.
   [Benini, Luca] Univ Bologna, Via Zamboni 33, I-40126 Bologna, Italy.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; International
   Business Machines (IBM); Queens University Belfast; University of
   Bologna
RP Scheidegger, F (corresponding author), Swiss Fed Inst Technol, Ramistr 101, CH-8092 Zurich, Switzerland.; Scheidegger, F (corresponding author), IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM eid@zurich.ibm.com; roi@zurich.ibm.com; ova@zurich.ibm.com;
   lbenini@iis.ee.ethz.ch; bek@zurich.ibm.com; acm@zurich.ibm.com
OI Scheidegger, Florian/0000-0003-0430-3634
FU Swiss Federal Institute of Technology Zurich
FX Open access funding provided by Swiss Federal Institute of Technology
   Zurich. We would like to thank Dr. Dario Garcia Gasulla from the
   Barcelona Supercomputing Center for discussion and advise.
CR [Anonymous], 2016, ARXIV160202830
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Baker B., 2017, ICLR
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cai H, 2018, AAAI CONF ARTIF INTE, P2787
   Chen L, 2019, VISUAL COMPUT, V35, P1361, DOI 10.1007/s00371-018-01615-0
   Chen WJ, 2021, VISUAL COMPUT, V37, P805, DOI 10.1007/s00371-020-01831-7
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Coates A., 2011, P 14 INT C ART INT S, P215
   Deadman E, 2013, LECT NOTES COMPUT SC, V7782, P171, DOI 10.1007/978-3-642-36803-5_12
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Ganin Y, 2016, J MACH LEARN RES, V17
   Griffin G., 2007, CALTECH 256 OBJECT C
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Hanzhang H, 2017, ANYTIME NEURAL NETWO
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Ho TK, 2002, IEEE T PATTERN ANAL, V24, P289, DOI 10.1109/34.990132
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Ionescu RT, 2016, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2016.237
   Istrate Roxana, 2018, TAPAS TRAIN LESS ACC
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kumar A, 2018, ADV NEUR IN, V31
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li L., 2016, HYPERBAND BANDIT BAS
   Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13
   Luciano L, 2019, VISUAL COMPUT, V35, P1171, DOI 10.1007/s00371-019-01668-9
   Lucic M., 2018, ADV NEURAL INF PROCE, P698
   Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3
   Mundhenk TN, 2016, LECT NOTES COMPUT SC, V9907, P785, DOI 10.1007/978-3-319-46487-9_48
   Netzer Yuval, 2011, ADV NEUR INF PROC SY
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pham H, 2018, PR MACH LEARN RES, V80
   Pourashraf P, 2015, LECT NOTES COMPUT SC, V9475, P609, DOI 10.1007/978-3-319-27863-6_56
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Real E., 2017, Large-scale evolution of image classifiers, V70, P2902
   Rosenberg A., 2007, EMNLP CONLL, P410, DOI DOI 10.7916/D80V8N84
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Scheidegger F., 2018, ABS18030958 CORR
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snoek J., 2012, Advances in Neural Information Processing Systems, V25, DOI DOI 10.48550/ARXIV.1206.2944
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tieleman Tijmen, 2012, LECT 6 5 RMSPROP COU
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao H., 2017, ARXIV170807747
   Xie LX, 2017, IEEE I CONF COMP VIS, P1388, DOI 10.1109/ICCV.2017.154
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zoph B., 2016, INT C LEARN REPR
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 58
TC 17
Z9 17
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1593
EP 1610
DI 10.1007/s00371-020-01922-5
EA JUL 2020
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000554044000001
OA Green Submitted, Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Monika, R
   Samiappan, D
   Kumar, R
AF Monika, R.
   Samiappan, Dhanalakshmi
   Kumar, R.
TI Underwater image compression using energy based adaptive block
   compressive sensing for IoUT applications
SO VISUAL COMPUTER
LA English
DT Article
DE Internet of underwater things (IoUT); Adaptive block compressed sensing
   (ABCS); Energy based ABCS (EABCS); Orthogonal matching pursuit (OMP);
   Sparse binary random matrix
ID RECOVERY; INTERNET
AB Internet of Underwater Things (IoUT) consists of a large number of interconnected resource-constrained underwater devices that are capable of monitoring vast unexplored water bodies. Specifically, these devices are equipped with cameras to capture the underwater scenes and communicate them with each other and also with the cloud. However the data generated is very high which limits the performance of the IoUT devices in terms of computational capabilities and battery lifetime. Block Compressed Sensing technique which performs block by block fixed sampling can be utilized to achieve data compression however it ends up in image distortions after reconstruction. To unravel this issue, Adaptive Block Compressive Sensing technique is used. In this paper, Energy based Adaptive Block Compressive Sensing (EABCS) with Orthogonal Matching Pursuit reconstruction algorithm is proposed to improve the sampling performance and visual quality of the reconstructed image. Sparse binary random matrix is used as measurement matrix as it is highly sparse. With this energy based adaptive strategy, higher measurements are assigned to blocks with higher energy and vice versa. The proposed EABCS technique has achieved better compression with approximately 25-30% of measurements/samples with an increase in Peak signal to noise ratio of about 3-5 dB and structural similarity Index of around 0.1-0.3 with respect to other adaptive strategies. Percentage of space saving is also about 60-70%.
C1 [Monika, R.; Samiappan, Dhanalakshmi; Kumar, R.] SRM Inst Sci & Technol, Dept ECE, Kattankulathur, India.
C3 SRM Institute of Science & Technology Chennai
RP Samiappan, D (corresponding author), SRM Inst Sci & Technol, Dept ECE, Kattankulathur, India.
EM monikar@srmist.edu.in; dhanalas@srmist.edu.in; kumarr@srmist.edu.in
RI R, Monika/AAE-7469-2021; KUMAR, R./ABE-6523-2021; Dhanalakshmi,
   S./J-2073-2018
OI R, Monika/0000-0002-7814-6611; Dhanalakshmi, S./0000-0002-6970-2719
CR Akbari A, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574688
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   CANH TN, 2014, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICME.2014.6890251
   Domingo MC, 2012, J NETW COMPUT APPL, V35, P584, DOI 10.1016/j.jnca.2011.10.015
   Charette Paul G., 2015, 2015 Photonics North, DOI 10.1109/PN.2015.7292530
   Djelouat H, 2018, J SENS ACTUAT NETW, V7, DOI 10.3390/jsan7040045
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Fang Wang, 2012, 2012 IEEE/ACIS 11th International Conference on Computer and Information Science (ICIS), P351, DOI 10.1109/ICIS.2012.83
   Fayed S, 2016, MULTIMED TOOLS APPL, V75, P6347, DOI 10.1007/s11042-015-2575-8
   Feng WZ, 2018, J SENSORS, V2018, DOI 10.1155/2018/3238140
   Fragkiadakis A, 2014, INT J DISTRIB SENS N, DOI 10.1155/2014/393248
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao XW, 2015, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2015.47
   Gao ZR, 2013, J VIS COMMUN IMAGE R, V24, P885, DOI 10.1016/j.jvcir.2013.06.006
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Indyk P, 2010, LECT NOTES COMPUT SC, V6034, P157, DOI 10.1007/978-3-642-12200-2_15
   Krishnaraj N, 2020, J REAL-TIME IMAGE PR, V17, P2097, DOI 10.1007/s11554-019-00879-6
   Li R, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718781751
   Li R, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041231
   Li RQ, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/6758147
   Liu GH, 2021, VISUAL COMPUT, V37, P515, DOI 10.1007/s00371-020-01820-w
   Liu WD, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9112276
   Monika R., 2018, ADV ELECT COMMUNICAT, P529
   Rippel O, 2017, PR MACH LEARN RES, V70
   Sehgal A, 2012, IEEE COMMUN MAG, V50, P144, DOI 10.1109/MCOM.2012.6384464
   Shen Y, 2015, INVERSE PROBL IMAG, V9, P231, DOI 10.3934/ipi.2015.9.231
   Sun F, 2017, INT J DIGIT MULTIMED, V2017, DOI 10.1155/2017/3902543
   Tong FH, 2020, APPL MATH COMPUT, V371, DOI 10.1016/j.amc.2019.124965
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiaomeng Duan, 2018, Cloud Computing and Security. 4th International Conference, ICCCS 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11063), P110, DOI 10.1007/978-3-030-00006-6_10
   Xin L., 2015, OPEN CYBERN SYST J, V9, P683, DOI [10.2174/1874110X01509010683, DOI 10.2174/1874110X01509010683]
   Xu J, 2016, IEICE T INF SYST, VE99D, P1702, DOI 10.1587/transinf.2015EDL8230
   Yu Y, 2010, IEEE SIGNAL PROC LET, V17, P973, DOI 10.1109/LSP.2010.2080673
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang JG, 2017, MULTIMED TOOLS APPL, V76, P4227, DOI 10.1007/s11042-016-3496-x
   Zhang Shu-fang, 2012, Journal of Tianjin University, V45, P319
   Zhang Z., 2019, MULTIMEDIA TOOLS APP, P1
   Zhao H.H., 2018, INT S ARTIFICIAL INT, P389
   Zhao HH, 2019, MULTIMED TOOLS APPL, V79, P1
   Zhu SY, 2014, IEEE INT SYMP CIRC S, P1, DOI 10.1109/ISCAS.2014.6865050
NR 42
TC 14
Z9 15
U1 2
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1499
EP 1515
DI 10.1007/s00371-020-01884-8
EA JUN 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000543677800002
DA 2024-07-18
ER

PT J
AU Remeseiro, B
   Mendonça, AM
   Campilho, A
AF Remeseiro, Beatriz
   Mendonca, Ana Maria
   Campilho, Aurelio
TI Automatic classification of retinal blood vessels based on multilevel
   thresholding and graph propagation
SO VISUAL COMPUTER
LA English
DT Article
DE Retinal images; Artery; vein classification; Arteriolar-to-venular
   ratio; Multilevel thresholding; Graph propagation
ID ARTERY/VEIN CLASSIFICATION; SEGMENTATION; HYPERTENSION; CALIBER
AB Several systemic diseases affect the retinal blood vessels, and thus, their assessment allows an accurate clinical diagnosis. This assessment entails the estimation of the arteriolar-to-venular ratio (AVR), a predictive biomarker of cerebral atrophy and cardiovascular events in adults. In this context, different automatic and semiautomatic image-based approaches for artery/vein (A/V) classification and AVR estimation have been proposed in the literature, to the point of having become a hot research topic in the last decades. Most of these approaches use a wide variety of image properties, often redundant and/or irrelevant, requiring a training process that limits their generalization ability when applied to other datasets. This paper presents a new automatic method for A/V classification that just uses the local contrast between blood vessels and their surrounding background, computes a graph that represents the vascular structure, and applies a multilevel thresholding to obtain a preliminary classification. Next, a novel graph propagation approach was developed to obtain the final A/V classification and to compute the AVR. Our approach has been tested on two public datasets (INSPIRE and DRIVE), obtaining high classification accuracy rates, especially in the main vessels, and AVR ratios very similar to those provided by human experts. Therefore, our fully automatic method provides the reliable results without any training step, which makes it suitable for use with different retinal image datasets and as part of any clinical routine.
C1 [Remeseiro, Beatriz; Mendonca, Ana Maria; Campilho, Aurelio] INESC TEC INESC Technol & Sci, Campus FEUP,Rua Dr Roberto Frias, P-4200465 Porto, Portugal.
   [Remeseiro, Beatriz] Univ Oviedo, Dept Comp Sci, Campus Gijon S-N, Gijon 33203, Spain.
   [Mendonca, Ana Maria; Campilho, Aurelio] Univ Porto, Fac Engn, Campus FEUP,Rua Dr Roberto Frias, P-4200465 Porto, Portugal.
C3 INESC TEC; Universidade do Porto; University of Oviedo; Universidade do
   Porto
RP Remeseiro, B (corresponding author), INESC TEC INESC Technol & Sci, Campus FEUP,Rua Dr Roberto Frias, P-4200465 Porto, Portugal.; Remeseiro, B (corresponding author), Univ Oviedo, Dept Comp Sci, Campus Gijon S-N, Gijon 33203, Spain.
EM bremeseiro@uniovi.es; amendon@fe.up.pt; campilho@fe.up.pt
RI Remeseiro, Beatriz/N-3791-2014; Mendonça, Ana Maria/M-7629-2013;
   Campilho, Aurelio/M-4869-2013
OI Remeseiro, Beatriz/0000-0001-9265-253X; Mendonça, Ana
   Maria/0000-0002-4319-738X; Campilho, Aurelio/0000-0002-5317-6275
FU Portuguese funding agency, FCT - Fundacao para a Ciencia e a Tecnologia
   [SFRH/BPD/111177/2015]; Fundação para a Ciência e a Tecnologia
   [SFRH/BPD/111177/2015] Funding Source: FCT
FX Beatriz Remeseiro acknowledges the support of the Portuguese funding
   agency, FCT - FundacAo para a Ciencia e a Tecnologia, under
   Post-doctoral Fellowship program (ref. SFRH/BPD/111177/2015).
CR Akbar S, 2018, ARTIF INTELL MED, V90, P15, DOI 10.1016/j.artmed.2018.06.004
   [Anonymous], 2015, THESIS
   Cheung CY, 2015, DIABETOLOGIA, V58, P871, DOI 10.1007/s00125-015-3511-1
   Daien V, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071089
   Dashtbozorg B, 2015, LECT NOTES COMPUT SC, V9164, P335, DOI 10.1007/978-3-319-20801-5_36
   Dashtbozorg B, 2015, COMPUT BIOL MED, V56, P1, DOI 10.1016/j.compbiomed.2014.10.009
   Dashtbozorg B, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2263809
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Ding J, 2014, J HYPERTENS, V32, P207, DOI 10.1097/HJH.0b013e32836586f4
   Estrada R, 2015, IEEE T MED IMAGING, V34, P2518, DOI 10.1109/TMI.2015.2443117
   Foracchia M, 2005, MED IMAGE ANAL, V9, P179, DOI 10.1016/j.media.2004.07.001
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P407, DOI 10.1016/j.cmpb.2012.03.009
   Galdran A, 2019, I S BIOMED IMAGING, P556, DOI [10.1109/isbi.2019.8759380, 10.1109/ISBI.2019.8759380]
   Heitmar R, 2017, CARDIOVASC DIABETOL, V16, DOI 10.1186/s12933-017-0534-6
   Hu Q., 2015, RITE RETINAL IMAGES
   Hu ZH, 2015, J MED IMAGING, V2, DOI 10.1117/1.JMI.2.1.014501
   Huang F, 2018, COMPUT METH PROG BIO, V161, P197, DOI 10.1016/j.cmpb.2018.04.016
   Huang F, 2018, MACH VISION APPL, V29, P23, DOI 10.1007/s00138-017-0867-x
   Irshad S, 2015, LECT NOTES COMPUT SC, V9164, P411, DOI 10.1007/978-3-319-20801-5_45
   Joshi VS, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088061
   Knudtson MD, 2003, CURR EYE RES, V27, P143, DOI 10.1076/ceyr.27.3.143.16049
   Lyu XZ, 2016, IEEE INT C BIOINFORM, P375, DOI 10.1109/BIBM.2016.7822548
   Mendonca AM, 2017, MED IMAGING 2017 COM, P402
   Mendonça AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955
   Meyer MI, 2018, LECT NOTES COMPUT SC, V10882, P622, DOI 10.1007/978-3-319-93000-8_71
   Mirsharif Q, 2013, COMPUT MED IMAG GRAP, V37, P607, DOI 10.1016/j.compmedimag.2013.06.003
   Montoro A, 2014, 2014 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI), P404, DOI 10.1109/BHI.2014.6864388
   Muraoka Y, 2013, AM J OPHTHALMOL, V156, P706, DOI 10.1016/j.ajo.2013.05.021
   Mustafa A, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON WIRELESS AND TELEMATICS (ICWT), P1, DOI 10.1109/ICWT.2017.8284127
   Niemeijer M, 2011, INSPIRE AVR IOWA NOR
   Niemeijer M., 2004, DRIVE: Digital retinal Images for Vessel Extraction
   Niemeijer M, 2011, IEEE T MED IMAGING, V30, P1941, DOI 10.1109/TMI.2011.2159619
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pellegrini E, 2018, IEEE T MED IMAGING, V37, P516, DOI 10.1109/TMI.2017.2762963
   Relan D, 2016, ADV INTELL SYST, V390, P77, DOI 10.1007/978-81-322-2625-3_7
   Schneiderman H., 1990, FUNDUSCOPIC EXAMINAT
   Seidelmann SB, 2016, J AM COLL CARDIOL, V67, P1893, DOI 10.1016/S0735-1097(16)31894-0
   Varnousfaderani ES, 2016, MED IMAGING 2016 IMA, P966
   Vázquez SG, 2013, COMPUT MED IMAG GRAP, V37, P337, DOI 10.1016/j.compmedimag.2013.10.001
   Xu XY, 2017, COMPUT METH PROG BIO, V141, P3, DOI 10.1016/j.cmpb.2017.01.007
   Zhao YT, 2020, IEEE T MED IMAGING, V39, P341, DOI 10.1109/TMI.2019.2926492
NR 41
TC 11
Z9 11
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1247
EP 1261
DI 10.1007/s00371-020-01863-z
EA JUN 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539878400002
DA 2024-07-18
ER

PT J
AU Khwildi, R
   Zaid, AO
AF Khwildi, Raoua
   Zaid, Azza Ouled
TI HDR image retrieval by using color-based descriptor and tone mapping
   operator
SO VISUAL COMPUTER
LA English
DT Article
DE High Dynamic Range; Image retrieval; Tone mapping; HSV color histogram;
   Color moments
ID DYNAMIC-RANGE EXPANSION; REPRODUCTION
AB Various methods have been performed for the purpose of Low Dynamic Range (LDR) image retrieval. However, no major work concerning the High Dynamic Range (HDR) image indexing has been widely diffused yet. We therefore propose a method that tackles the problem of efficiently and accurately retrieving HDR images. The proposed system is based on a hybrid descriptor which combines two color features. The first one is histogram based on the hue-saturation-value (HSV) color space that approaches the perception of human vision, whereas the second comprises the first- and second-order moments of the color bands. As a dissimilarity measure, we retained the Manhattan distance. In the second part of our work, we proposed an automatic tone mapping operator (TMO) to get an overview on the result images by using Standard Dynamic Range (SDR) devices. Comparisons with recent state-of-the-art TMOs have shown that our TM method produces LDR images with adequate quality while maintaining low complexity. Finally, to test our retrieval system, we have created two databases. Experimental evaluation showed that our system supports HDR images while achieving satisfying results in terms of accuracy and computational cost.
C1 [Khwildi, Raoua; Zaid, Azza Ouled] Univ Tunis El Manar, SysCom Lab, Natl Engn Sch Tunis ENIT, BP 37, Tunis 1002, Tunisia.
C3 Universite de Tunis-El-Manar; Ecole Nationale d'Ingenieurs de Tunis
   (ENIT)
RP Khwildi, R (corresponding author), Univ Tunis El Manar, SysCom Lab, Natl Engn Sch Tunis ENIT, BP 37, Tunis 1002, Tunisia.
EM khwildi.raoua135@gmail.com; azza.ouledzaid@isi.rnu.tn
OI Ouled Zaid, Azza/0000-0002-3264-5933
CR Adams A, 1981, PRINT
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   [Anonymous], 2003, IND LIGHT MAG
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F., 2012, ACM S APPL PERC 2012, P39
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Bruce NDB, 2014, COMPUT GRAPH-UK, V39, P12, DOI 10.1016/j.cag.2013.10.001
   Burger W., 2009, Principles of Digital Image Processing
   Debattista K, 2015, VISUAL COMPUT, V31, P1089, DOI 10.1007/s00371-015-1121-z
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eidenberger H., 2003, VISUAL COMMUNICATION
   Hristova H, 2017, VISUAL COMPUT, V33, P725, DOI 10.1007/s00371-017-1399-0
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Khwildi R, 2016, INT C MACH VIS
   Khwildi R, 2018, IEEE INT WORKSH MULT
   Korshunov P., 2012, P SPIE APPL DIG IM P, P8499
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Kovaleski RP, 2009, VISUAL COMPUT, V25, P539, DOI 10.1007/s00371-009-0327-3
   Krawczyk G, 2005, COMPUT GRAPH FORUM, V24, P635, DOI 10.1111/j.1467-8659.2005.00888.x
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Lin CH, 2014, EXPERT SYST APPL, V41, P3276, DOI 10.1016/j.eswa.2013.11.017
   Liu GH, 2013, PATTERN RECOGN, V46, P188, DOI 10.1016/j.patcog.2012.06.001
   Masia B, 2017, MULTIMED TOOLS APPL, V76, P631, DOI 10.1007/s11042-015-3036-0
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Pass G., 1996, P 4 ACM INT C MULT, V96, P65, DOI DOI 10.1145/244130.244148
   Poularakis A, 2000, TRANSFORMS APPL HDB
   Qi XJ, 2007, PATTERN RECOGN, V40, P728, DOI 10.1016/j.patcog.2006.04.042
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Shrivastava N, 2015, COMPUT ELECTR ENG, V46, P314, DOI 10.1016/j.compeleceng.2014.11.009
   Singh S. R., 2015, INT J COMPUT SYST, V2, P161
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Ward Greg, 2005, SIGGRAPH 05 ACM SIGG, P2
   Ward Greg., 1991, GRAPHICS GEMS, V2, P15
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P57, DOI 10.1109/MCG.2005.133
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820
NR 44
TC 10
Z9 11
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1111
EP 1126
DI 10.1007/s00371-019-01719-1
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400003
DA 2024-07-18
ER

PT J
AU Zhang, YX
   Guo, Q
   Zhang, CM
AF Zhang, Yongxia
   Guo, Qiang
   Zhang, Caiming
TI Simple and fast image superpixels generation with color and boundary
   probability
SO VISUAL COMPUTER
LA English
DT Article
DE Superpixel; Image segmentaion; Efficient; Real time; Accuracy
ID SEGMENTATION
AB As one of the most popular preprocessing steps in computer vision fields, superpixel generation algorithm has been extensively studied in recent years. Researchers have to find a way to produce superpixels with both accuracy and computationally efficiency. Inspired by the real-time superpixel segmentation method using density-based spatial clustering of applications with noise (DBSCAN), we propose a two-stage, non-iterative superpixel segmentation approach. In the first stage, we produce the initial regions. To make the superpixels attach to most object boundaries well, we define an adaptive parameter based on the boundary probability map in the distance measurement. At the same time, we adopt the averaging colors of region to represent the cluster center feature. In the second stage, we merge small regions to produce superpixels. To make them have uniform sizes, we take the initial region size into consideration and define a new distance measurement between the two neighboring regions. In the whole framework, we process all the pixels only once. We test the proposed method on the public data sets. The experimental results show that our proposed algorithm outperforms the most compared approaches with accuracy and has competitive speed with the real-time methods (e.g., DBSCAN).
C1 [Zhang, Yongxia; Guo, Qiang] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
   [Zhang, Yongxia; Guo, Qiang] Digital Media Technol Key Lab Shandong Prov, Jinan 250014, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Software, Jinan 250010, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University
RP Zhang, YX (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.; Zhang, YX (corresponding author), Digital Media Technol Key Lab Shandong Prov, Jinan 250014, Peoples R China.
EM sdu_zyx@hotmail.com
RI Cheng, Lin/KFQ-3111-2024; Guo, Qiang/I-2949-2019
OI Guo, Qiang/0000-0003-4219-3528
FU National Natural Science Foundation of China [61802229, 61873145]; NSFC
   [U1609218]; Natural Science Foundation of Shandong Province
   [ZR2018BF007, ZR2017JL029]; Fostering Project of Dominant Discipline and
   Talent Team of Shandong Province Higher Education Institutions
FX This work was supported by the National Natural Science Foundation of
   China (61802229, 61873145), NSFC Joint Fund with Zhejiang under Key
   Project (U1609218), Natural Science Foundation of Shandong Province
   (ZR2018BF007, ZR2017JL029), and Fostering Project of Dominant Discipline
   and Talent Team of Shandong Province Higher Education Institutions.
CR Achanta R, 2017, PROC CVPR IEEE, P4895, DOI 10.1109/CVPR.2017.520
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2008, 2008 IEEE C COMP VIS
   Ban ZH, 2018, IEEE T IMAGE PROCESS, V27, P4105, DOI 10.1109/TIP.2018.2836306
   Beucher S., 2018, Mathematical Morphology in Image Processing, P433
   Chen JS, 2017, IEEE T IMAGE PROCESS, V26, P3317, DOI 10.1109/TIP.2017.2651389
   Cheng J, 2013, IEEE T MED IMAGING, V32, P1019, DOI 10.1109/TMI.2013.2247770
   Choi KS, 2016, COMPUT VIS IMAGE UND, V146, P1, DOI 10.1016/j.cviu.2016.02.018
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dong XP, 2017, IEEE T CIRC SYST VID, V27, P2518, DOI 10.1109/TCSVT.2016.2595321
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   Giraud R, 2018, COMPUT VIS IMAGE UND, V170, P1, DOI 10.1016/j.cviu.2018.01.006
   Giraud R, 2016, INT C PATT RECOG, P2374, DOI 10.1109/ICPR.2016.7899991
   Hartley T., 2019, COMPUTER VISION PATT
   Jampani V., 2018, P EUR C COMP VIS ECC, P352
   Lee SH, 2017, PROC CVPR IEEE, P5863, DOI 10.1109/CVPR.2017.621
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Liang YL, 2016, IEEE T CIRC SYST VID, V26, P928, DOI 10.1109/TCSVT.2015.2406232
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu B, 2013, IEEE T GEOSCI REMOTE, V51, P907, DOI 10.1109/TGRS.2012.2203358
   Liu YJ, 2018, IEEE T PATTERN ANAL, V40, P653, DOI 10.1109/TPAMI.2017.2686857
   Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77
   Machairas V, 2015, IEEE T IMAGE PROCESS, V24, P3707, DOI 10.1109/TIP.2015.2451011
   Machairas V, 2015, LECT NOTES COMPUT SC, V9082, P194, DOI 10.1007/978-3-319-18720-4_17
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meyer Fernand., 1991, Dans 8me congres de reconnaissance des formes et intelligence artificielle, V2, P847
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Oh KW, 2019, J REAL-TIME IMAGE PR, V16, P945, DOI 10.1007/s11554-016-0583-1
   Peng JT, 2016, IEEE T CIRC SYST VID, V26, P917, DOI 10.1109/TCSVT.2015.2430631
   Qian X, 2019, VISUAL COMPUT, V35, P985, DOI 10.1007/s00371-019-01682-x
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Reyes A, 2017, MEX INT CONF ARTIF I, P46, DOI 10.1109/MICAI-2017.2017.00016
   Schuurmans M., 2018, COMPUTER VISION PATT
   Shen JB, 2019, IEEE T NEUR NET LEAR, V30, P2637, DOI 10.1109/TNNLS.2018.2885591
   Shen JB, 2017, IEEE T IMAGE PROCESS, V26, P4911, DOI 10.1109/TIP.2017.2722691
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Stutz D, 2015, LECT NOTES COMPUT SC, V9358, P555, DOI 10.1007/978-3-319-24947-6_46
   Tian XL, 2015, VISUAL COMPUT, V31, P701, DOI 10.1007/s00371-014-0996-4
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Verelst T., 2019, COMPUTER VISION PATT
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang C, 2018, VISUAL COMPUT, V34, P67, DOI 10.1007/s00371-016-1312-2
   Wang H, 2020, IEEE T CIRC SYST VID, V30, P822, DOI 10.1109/TCSVT.2019.2896438
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Zhang YX, 2017, IEEE T CIRC SYST VID, V27, P1502, DOI 10.1109/TCSVT.2016.2539839
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
NR 52
TC 9
Z9 9
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1061
EP 1074
DI 10.1007/s00371-020-01852-2
EA MAY 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000532644900001
DA 2024-07-18
ER

PT J
AU Yang, H
   Li, YN
   Zhang, K
AF Yang, Huan X.
   Li, Yi-Na
   Zhang, Kang
TI Interactive influences of color attributes on color perception bias
SO VISUAL COMPUTER
LA English
DT Article
DE Color perception; Perception bias; Simultaneous color contrast;
   Visualization
ID CONTRAST; DIFFERENCE
AB Graphic user interfaces and information visualization use color to represent qualitative or quantitative information. The interaction between adjacent colors leads to perceptual bias, known as simultaneous color contrast, and implicitly distort the understanding of visualized information presentation. To investigate the effect of simultaneous color contrast, we conduct two empirical experiments, in both theoretical and application settings, using a set of random target/proximal combinations of colors in the CIEL*a*b* color space. The perception bias of a target color, induced by its surround, is measured. Linear regression analysis indicates that both a high saturation of the proximal color and a high a*/low b* value of the target color cause a strong simultaneous color contrast (i.e., high perception bias). A moderating effect analysis indicates that a* value/b* value of the target color moderates the influence of the saturation of the proximal color on the perception bias. For example, controlling the saturation of the proximal color, the more reddish/yellowish the target color is, the more alleviated the perceptual bias is.
C1 [Yang, Huan X.] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Li, Yi-Na] Univ Sci & Technol China, Sch Management, Hefei, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.
   [Zhang, Kang] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; University of Texas System; University of
   Texas Dallas; Macau University of Science & Technology
RP Li, YN (corresponding author), Univ Sci & Technol China, Sch Management, Hefei, Peoples R China.
EM yanghuan@tju.edu.cn; yinali@ustc.edu.cn; kzhang@utdallas.edu
RI Li, Yi-Na/JGB-4336-2023
OI Li, Yi-Na/0000-0002-5479-5174
FU National Natural Science Foundation of China [71702080]; Humanities and
   Social Sciences Fund of Ministry of Education [17YJC630071]; Australian
   Government Research Training Program Scholarship
FX This research is supported by National Natural Science Foundation of
   China (71702080), Humanities and Social Sciences Fund of Ministry of
   Education (17YJC630071), and an Australian Government Research Training
   Program Scholarship.
CR Aisch G., 2016, NY TIMES
   Albers J., 2013, Interaction of Color
   [Anonymous], AM J PSYCHOL
   Bosten JM, 2012, VISION RES, V53, P40, DOI 10.1016/j.visres.2011.11.007
   BOWMAKER JK, 1983, TRENDS NEUROSCI, V6, P41, DOI 10.1016/0166-2236(83)90019-X
   BREITMEYER BG, 1994, VISION RES, V34, P1039, DOI 10.1016/0042-6989(94)90008-6
   Chalmers PA, 2003, COMPUT HUM BEHAV, V19, P593, DOI 10.1016/S0747-5632(02)00086-9
   Chevreul E., 1839, De la loi du contraste simultane des couleurs et de l'assortiment des objets colores
   Choudhury AKR, 2014, WOODHEAD PUBL SER TE, P144, DOI 10.1533/9780857099242.144
   Cohen J, 2007, NOUS, V41, P335, DOI 10.1111/j.1468-0068.2007.00650.x
   Csurka G, 2011, VISUAL COMPUT, V27, P1039, DOI 10.1007/s00371-011-0657-9
   Fairchild MD, 2013, COLOR APPEAR MODELS
   GEISLER WS, 1978, VISION RES, V18, P279, DOI 10.1016/0042-6989(78)90162-1
   Ishihara S., 1987, Test for colour-blindness
   KINNEY JAS, 1962, VISION RES, V2, P503, DOI 10.1016/0042-6989(62)90052-4
   Klauke S, 2015, J VISION, V15, DOI 10.1167/15.13.17
   KRAUSKOPF J, 1986, J OPT SOC AM A, V3, P1752, DOI 10.1364/JOSAA.3.001752
   Li BB, 2015, VISUAL COMPUT, V31, P257, DOI 10.1007/s00371-013-0916-z
   Luo MR, 2001, COLOR RES APPL, V26, P340, DOI 10.1002/col.1049
   MALONEY LT, 1986, J OPT SOC AM A, V3, P29, DOI 10.1364/JOSAA.3.000029
   Mittelstädt S, 2014, COMPUT GRAPH FORUM, V33, P231, DOI 10.1111/cgf.12379
   Mullen KT, 2002, VISUAL NEUROSCI, V19, P109, DOI 10.1017/S0952523802191103
   Nijboer TCW, 2011, VISION RES, V51, P43, DOI 10.1016/j.visres.2010.09.030
   Perales E., 2006, CGIV 2006 3 EUR C CO, P414
   Peterson MS, 2001, PSYCHOL SCI, V12, P287, DOI 10.1111/1467-9280.00353
   Ratnasingam S, 2017, J VISION, V17, DOI 10.1167/17.2.13
   Rizzi A, 2012, WOODHEAD PUBL SER TE, P83
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Schloss KB, 2011, ATTEN PERCEPT PSYCHO, V73, P551, DOI 10.3758/s13414-010-0027-0
   Sears A., 2007, HUMAN COMPUTER INTER
   Shepherd AJ, 1997, PERCEPTION, V26, P455, DOI 10.1068/p260455
   Stone M., 2014, COL IM C, V2014, P253
   Szafir DA, 2018, IEEE T VIS COMPUT GR, V24, P392, DOI 10.1109/TVCG.2017.2744359
   Tufte ER, 1990, Envisioning Information
   Wang YW, 2017, VISUAL COMPUT, V33, P235, DOI 10.1007/s00371-015-1189-5
   Ward M. O., 2015, INTERACTIVE DATA VIS
   Williams LA, 2017, COLLABRA-PSYCHOL, V3, DOI 10.1525/collabra.54
   Wuerger S., 2015, ENCY COLOR SCI TECHN, P1
NR 38
TC 1
Z9 3
U1 7
U2 50
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 925
EP 937
DI 10.1007/s00371-019-01706-6
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LJ3BL
UT WOS:000530043100005
DA 2024-07-18
ER

PT J
AU Zolfaghari, M
   Ghanei-Yakhdan, H
   Yazdi, M
AF Zolfaghari, Mohammad
   Ghanei-Yakhdan, Hossein
   Yazdi, Mehran
TI Real-time object tracking based on an adaptive transition model and
   extended Kalman filter to handle full occlusion
SO VISUAL COMPUTER
LA English
DT Article
DE Extended Kalman filter; Adaptive state transition model; Full occlusion;
   Model update; Occlusion detection
ID VISUAL TRACKING; MULTIPLE OBJECTS; ROBUST
AB In this paper, a tracker scheme is proposed that not only can face object tracking challenges but also can estimate object positions over occluded frames. In the proposed scheme, kernelized correlation filter (KCF) is considered as our basic tracker due to its high efficiency in the most object tracking challenges except occlusion and illumination variation. To improve the efficiency of KCF, the proposed method integrates an occlusion detection method, an adaptive model update and a prediction system into the KCF tracker. The occlusion detection method is based on the peak-to-sidelobe ratio of the confidence map to determine the type of occlusion. When an object is partially occluded, the object appearance model is adaptively updated to increase the accuracy of object tracking. When full occlusion occurs, the proposed predictor is run and exploits the available motion information before the occurrence of full occlusion to predict the location of the tracked object. The proposed predictor uses adaptive transition state equations to estimate the acceleration and velocity of the object needed in the extended Kalman filter (EKF) to predict object position. It also uses two quadratic equations to estimate the object trajectory. Finally, a method is proposed that exploits the estimated object positions by both EKF and the object trajectory to predict object positions over fully occluded frames. Experimental results on open datasets show that the proposed method achieved a better performance in comparison with several state-of-the-art trackers.
C1 [Zolfaghari, Mohammad; Ghanei-Yakhdan, Hossein] Yazd Univ, Dept Elect Engn, Yazd, Iran.
   [Yazdi, Mehran] Shiraz Univ, Dept Elect & Telecommun Engn, Shiraz, Iran.
C3 University of Yazd; Shiraz University
RP Ghanei-Yakhdan, H (corresponding author), Yazd Univ, Dept Elect Engn, Yazd, Iran.
EM hghaneiy@yazd.ac.ir
RI Ghanei-Yakhdan, Hossein/JAC-4508-2023; Ghanei-Yakhdan,
   Hossein/D-7382-2018; , Mehran/C-2776-2011
OI , Mehran/0000-0002-8889-7048; Ghanei-Yakhdan, Hosein/0000-0003-4575-1062
CR Adhikari G., 2016, PROC ICRTIT, DOI [10.1109/ICRTIT.2016.7569517, DOI 10.1109/ICRTIT.2016.7569517]
   Ahmed J, 2008, MACH VISION APPL, V19, P1, DOI 10.1007/s00138-007-0072-4
   Aishwarya SNR, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS IN ELECTRONICS, INFORMATION & COMMUNICATION TECHNOLOGY (RTEICT), P587, DOI 10.1109/RTEICT.2017.8256664
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2016, BIOMED RES INT
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Hadi I., 2014, INT J COMPUT APPL, V104, P28, DOI [10.5120/18177-9068, DOI 10.5120/18177-9068]
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Iraei I., 2015, 2015 2 INT C PATT RE, P4799
   Iswanto IA, 2017, PROCEDIA COMPUT SCI, V116, P587, DOI 10.1016/j.procs.2017.10.010
   Jeong JM, 2014, PROC SICE ANN CONF, P941, DOI 10.1109/SICE.2014.6935235
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Li DW, 2017, CHIN CONT DECIS CONF, P3120, DOI 10.1109/CCDC.2017.7979044
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu Q, 2014, IET COMPUT VIS, V8, P419, DOI 10.1049/iet-cvi.2013.0134
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mu XK, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P6, DOI 10.1109/CISP-BMEI.2016.7852673
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Simon D., 2006, OPTIMAL STATE ESTIMA, DOI [10.1002/0470045345, DOI 10.1002/0470045345.CH11]
   Tripathi RP, 2016, PROCEEDINGS OF 2ND IEEE INTERNATIONAL CONFERENCE ON ENGINEERING & TECHNOLOGY ICETECH-2016, P1128, DOI 10.1109/ICETECH.2016.7569426
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang J, 2010, 2ND IEEE INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER CONTROL (ICACC 2010), VOL. 5, P223, DOI 10.1109/ICACC.2010.5487263
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiao JJ, 2016, IEEE T CIRC SYST VID, V26, P304, DOI 10.1109/TCSVT.2015.2406193
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
NR 40
TC 10
Z9 10
U1 4
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 701
EP 715
DI 10.1007/s00371-019-01652-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800004
DA 2024-07-18
ER

PT J
AU Qin, Y
   Mo, LF
   Li, CY
   Luo, JY
AF Qin, Yang
   Mo, Lingfei
   Li, Chenyang
   Luo, Jiayi
TI Skeleton-based action recognition by part-aware graph convolutional
   networks
SO VISUAL COMPUTER
LA English
DT Article
DE Action recognition; Graph convolutional networks; Human skeleton
AB This paper proposes an improved graph convolutional networks to deal with the skeleton-based action recognition. Inspired by splitting skeleton into several parts to feed deep networks, the part-aware convolutions is designed to replace common convolutions which is performed on all the neighboring joints. For scale invariance on multi-scale data, an Inception-like structure is introduced, which can concatenate feature maps from different convolution kernels. In contrast to methods based on LSTMs, the model presented is capable of extracting both temporal and spatial features from input data. Due to full use of spatial structure, the performance is enhanced greatly on various datasets. To evaluate the model, experiments were conducted on three benchmark skeleton-based datasets, including Berkeley MHAD, SBU Kinect Interaction, and NTU RGB-D datasets. The effectiveness and robustness of the model are demonstrated by comparing the experimental results of the proposed model with the state-of-the-art results. In addition, feature maps from different layers of a trained model are explored and the explanation of the part-aware convolutions is also provided.
C1 [Qin, Yang; Mo, Lingfei; Li, Chenyang; Luo, Jiayi] Southeast Univ, Sch Instrument Sci & Engn, Nanjing, Peoples R China.
C3 Southeast University - China
RP Mo, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing, Peoples R China.
EM lfmo@seu.edu.cn
RI LI, Chenyang/ISU-2126-2023; luo, jiayi/KIG-0439-2024; Luo,
   Jeffrey/H-6408-2017; mo, lingfei/ACV-6434-2022
OI LI, Chenyang/0000-0003-4639-8571; Luo, Jeffrey/0000-0002-2719-1025; mo,
   lingfei/0000-0002-8561-9122
FU National Science Foundation of China [61603091]
FX This study was funded by the National Science Foundation of China
   [61603091, Multi-Dimensions Based Physical Activity Assessment for the
   Human Daily Life].
CR [Anonymous], 2018, ARXIV180209834
   [Anonymous], 2014, International Conference on Intelligent Sensors, Sensor Networks and Information Processing
   [Anonymous], 2013, Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013 Fourth National Conference on
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   Bloom V., 2012, 2012 IEEE COMP SOC C, P7, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Defferrard M, 2016, ADV NEUR IN, V29
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   Kapsouras I, 2014, J VIS COMMUN IMAGE R, V25, P1432, DOI 10.1016/j.jvcir.2014.04.007
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339
   Kipf T. N., 2016, SEMISUPERVISED CLASS, P1
   Li C., 2017, ARXIV170702356
   Li Y., 2015, ARXIV
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wu JZ, 2014, VISUAL COMPUT, V30, P1395, DOI 10.1007/s00371-013-0899-9
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 44
TC 32
Z9 34
U1 0
U2 36
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 621
EP 631
DI 10.1007/s00371-019-01644-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500013
DA 2024-07-18
ER

PT J
AU Li, J
   Yang, B
   Yang, WK
   Sun, CY
   Xu, JH
AF Li, Jun
   Yang, Bo
   Yang, Wankou
   Sun, Changyin
   Xu, Jianhua
TI Subspace-based multi-view fusion for instance-level image retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Instance retrieval; Deep neural network; Shallow features; Multi-view
   fusion; Latent representations
ID LOOP CLOSURE DETECTION; FEATURES; RECOGNITION; HISTOGRAMS; WORDS; COLOR;
   SHAPE; BAG
AB In this paper, we address the problem of multiple features fusion in instance-level image retrieval. Achieving tremendous success in recent retrieval task, convolutional neural network (CNN) features are capable of encoding high-level image contents and demonstrate unrivaled superiority to the hand-crafted shallow image signatures. However, the shallow features still play a beneficial role in visual matching particularly when dramatic variances in viewpoint and scale are present, since they inherit certain invariance from the local robust descriptor, e.g., scale-invariant feature transform (SIFT). Thus, it is important to leverage the mutual correlation between these two heterogeneous signatures for effective visual representation. Since it is still an open problem, in this paper, we propose a subspace-based multi-view fusion strategy where a shared subspace is uncovered from the original high-dimensional features yielding a compact latent representation. Experiments on six public benchmark datasets reveal the proposed method works better than other classical fusion approaches and achieve the state-of-the-art performance.
C1 [Li, Jun; Xu, Jianhua] Nanjing Normal Univ, Sch Comp Sci & Technol, Nanjing 210023, Peoples R China.
   [Yang, Bo] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
   [Yang, Wankou; Sun, Changyin] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
C3 Nanjing Normal University; Southeast University - China; Southeast
   University - China
RP Yang, B (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM lijuncst@njnu.edu.cn; ybseu@seu.edu.cn; wkyang@seu.edu.cn;
   cysun@seu.edu.cn; xujianhua@njnu.edu.cn
RI SUN, CHANG/GXM-3680-2022; sun, chang/ITV-6759-2023
FU National Natural Science Foundation of China [61703096, 61921004,
   U1713209, 61773117, 61803212]; Natural Science Foundation of Jiangsu
   Province [BK20170691, BK20180744]; Natural Science Foundation of Jiangsu
   Higher Education Institutions of China [18KJB520034]
FX This work is supported by the National Natural Science Foundation of
   China under Grants 61703096, 61921004, U1713209, 61773117, 61803212, the
   Natural Science Foundation of Jiangsu Province under Grants BK20170691,
   BK20180744 and the Natural Science Foundation of Jiangsu Higher
   Education Institutions of China under Grant 18KJB520034.
CR Alzu'bi A, 2017, NEUROCOMPUTING, V249, P95, DOI 10.1016/j.neucom.2017.03.072
   [Anonymous], 2014, CVPRW
   [Anonymous], 2015, CVPRW
   [Anonymous], 2013, CVPR
   [Anonymous], 2016, IEEE C COMP VIS PATT
   [Anonymous], 2014, CVPR
   [Anonymous], 2016, CVPR
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, CVPRW
   [Anonymous], 2014, CVPR
   [Anonymous], 2004, ELECT IMAGING
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2015, ICCV
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chatzichristofis SA, 2013, IEEE T CYBERNETICS, V43, P192, DOI 10.1109/TSMCB.2012.2203300
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chum O., 2008, BMVC, P812, DOI DOI 10.5244/C.22.50
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gordo A, 2012, PROC CVPR IEEE, P3045, DOI 10.1109/CVPR.2012.6248035
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Haghighat M, 2016, EXPERT SYST APPL, V47, P23, DOI 10.1016/j.eswa.2015.10.047
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Y, 2018, AUTON ROBOT, V42, P1169, DOI 10.1007/s10514-017-9684-3
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Ji Z, 2015, IEEE T IMAGE PROCESS, V24, P4137, DOI 10.1109/TIP.2015.2437198
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Karakasis EG, 2015, PATTERN RECOGN LETT, V55, P22, DOI 10.1016/j.patrec.2015.01.005
   Kumar D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1413
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li J, 2019, IEEE T KNOWL DATA EN, V31, P2393, DOI 10.1109/TKDE.2018.2876834
   Li J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P486, DOI 10.1109/ROBIO.2018.8665207
   Li J, 2017, NEUROCOMPUTING, V257, P47, DOI 10.1016/j.neucom.2016.10.074
   Li J, 2016, NEUROCOMPUTING, V207, P202, DOI 10.1016/j.neucom.2016.04.047
   Liu Y, 2012, IEEE INT C INT ROBOT, P1051, DOI 10.1109/IROS.2012.6386145
   Liu Z, 2016, IEEE T CIRC SYST VID, V26, P375, DOI 10.1109/TCSVT.2015.2409693
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823
   Negrel R, 2012, IEEE IMAGE PROC, P2425, DOI 10.1109/ICIP.2012.6467387
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Pradhan J., 2018, INT C MATH COMPUTING, V834, P84
   Pradhan J, 2019, MULTIMED TOOLS APPL, V78, P1685, DOI 10.1007/s11042-018-6246-4
   Pradhan J, 2018, DIGIT SIGNAL PROCESS, V82, P258, DOI 10.1016/j.dsp.2018.07.016
   Raghuwanshi G, 2019, MULTIMED TOOLS APPL, V78, P1889, DOI 10.1007/s11042-018-6333-6
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shahbazi H, 2011, IEEE INT C INT ROBOT, P1228, DOI 10.1109/IROS.2011.6048862
   Shakeri M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P466, DOI 10.1109/IROS.2016.7759095
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Varish N, 2017, MULTIMED TOOLS APPL, V76, P15885, DOI 10.1007/s11042-016-3882-4
   Varma M., 2009, P 26 ANN INT C MACHI, P1065
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang Zhenhua., 2014, CORR
   Wu JJ, 2014, IEEE INT CONF ROBOT, P861
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng L., 2013, CVPR
NR 80
TC 15
Z9 15
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 619
EP 633
DI 10.1007/s00371-020-01828-2
EA FEB 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000517006400002
DA 2024-07-18
ER

PT J
AU Wang, KK
   Zhang, GF
   Yang, J
   Bao, HJ
AF Wang, Kangkan
   Zhang, Guofeng
   Yang, Jian
   Bao, Hujun
TI Dynamic human body reconstruction and motion tracking with low-cost
   depth cameras
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic 3D human body modeling; Shape correspondence; Skeleton-driven
   deformation; Nonrigid deformation; Data-driven model; Temporal filtering
ID INTERACTING CHARACTERS; CAPTURE; REGISTRATION; DEFORMATION; SPACE; POSE
AB We present a novel approach for dynamic human body reconstruction and motion tracking using low-cost depth cameras. Our reconstruction system is able to produce a sequence of dynamic 3D human body models from the noisy input depth data. To accurately align the template model with noisy input data, we combine skeleton-driven deformation and mesh deformation techniques to enhance the registration robustness to depth missing, occlusions, and severe noise. In addition, a novel data-driven 3D human body model is introduced to efficiently reconstruct human body models with wide shape and pose variations only using a limited number of training databases with standard standing pose. We perform quantitative and qualitative experiments to evaluate our method and compare it with other methods for body reconstruction on both synthetic and real datasets. Experimental results demonstrate the effectiveness of the proposed approach.
C1 [Wang, Kangkan; Yang, Jian] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab, Minist Educ, Nanjing 210094, Peoples R China.
   [Wang, Kangkan; Yang, Jian] Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Socia, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Wang, Kangkan; Zhang, Guofeng; Bao, Hujun] Zhejiang Univ, State Key Lab CAD & CG, Zijingang Campus, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology; Zhejiang University
RP Wang, KK (corresponding author), Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab, Minist Educ, Nanjing 210094, Peoples R China.; Wang, KK (corresponding author), Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Socia, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.; Wang, KK (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Zijingang Campus, Hangzhou 310058, Zhejiang, Peoples R China.
EM wangkangkan@njust.edu.cn; zhangguofeng@cad.zju.edu.cn;
   csjyang@njust.edu.cn; bao@cad.zju.edu.cn
FU Natural Science Foundation of China [61602444]; Fundamental Research
   Funds for the Central Universities [2018FZA5011]
FX This work was partially funded by the Natural Science Foundation of
   China (No. 61602444) and the Fundamental Research Funds for the Central
   Universities (No. 2018FZA5011).
CR Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B., 2006, P 2006 ACM SIGGRAPHE, P147
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Anguelov Dragomir., 2004, NIPS
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265
   Bronstein AM, 2006, SIAM J SCI COMPUT, V28, P1812, DOI 10.1137/050639296
   Carpenter JM, 2004, AM MUS NOVIT, P1, DOI 10.1206/0003-0082(2004)427<0001:AROTGA>2.0.CO;2
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chang W, 2009, COMPUT GRAPH FORUM, V28, P447, DOI 10.1111/j.1467-8659.2009.01384.x
   Chang W, 2008, COMPUT GRAPH FORUM, V27, P1459, DOI 10.1111/j.1467-8659.2008.01286.x
   Chen Jia, 2012, Journal of Computer Aided Design & Computer Graphics, V24, P357
   Chen YP, 2013, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2013.21
   Cui Y., 2013, ACM SIGGRAPH AISA
   Cui Y., 2011, ACCV 2012 WORKSH COL
   Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190
   Dou M., 2016, ACM SIGGRAPH
   Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Guo Kaiwen, 2015, ICCV
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Hodrick RJ, 1997, J MONEY CREDIT BANK, V29, P1, DOI 10.2307/2953682
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Jain Varun., 2007, INT J SHAPE MODELING, V13, P101, DOI DOI 10.1142/S0218654307000968
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kendall D. G., 1989, STAT SCI, V4, P87, DOI DOI 10.1214/SS/1177012582
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Liao M, 2009, IEEE I CONF COMP VIS, P167, DOI 10.1109/ICCV.2009.5459161
   Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Murray R. M., 1994, MATH INTRO ROBOTIC M
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Pauly M., 2005, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P23
   Pekelny Y, 2008, COMPUT GRAPH FORUM, V27, P399, DOI 10.1111/j.1467-8659.2008.01137.x
   Pons-Moll Gerard, 2011, Model-Based Pose Estimation, chapter 9, P139
   Sahillioglu Y, 2010, PROC CVPR IEEE, P453, DOI 10.1109/CVPR.2010.5540178
   Seo H, 2004, GRAPH MODELS, V66, P1, DOI 10.1016/j.gmod.2003.07.004
   Shapiro A, 2014, COMPUT ANIMAT VIRT W, V25, P201, DOI 10.1002/cav.1579
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Wang KK, 2014, IEEE T IMAGE PROCESS, V23, P4893, DOI 10.1109/TIP.2014.2352851
   Wang KK, 2014, IEEE T PATTERN ANAL, V36, P1493, DOI 10.1109/TPAMI.2013.235
   Wang RZ, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P432, DOI 10.1109/3DIMPVT.2012.57
   Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Ye GZ, 2012, LECT NOTES COMPUT SC, V7573, P828, DOI 10.1007/978-3-642-33709-3_59
   Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310
   Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Zeng M, 2013, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2013.26
   Zhang P., 2014, ACM T GRAPHIC, V33, P1
   Zhang Q, 2014, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2014.92
NR 63
TC 8
Z9 9
U1 2
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 603
EP 618
DI 10.1007/s00371-020-01826-4
EA FEB 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000516787000001
DA 2024-07-18
ER

PT J
AU Ahmadi, SBB
   Zhang, GX
   Wei, SJ
   Boukela, L
AF Bagheri Baba Ahmadi, Sajjad
   Zhang, Gongxuan
   Wei, Songjie
   Boukela, Lynda
TI An intelligent and blind image watermarking scheme based on hybrid SVD
   transforms using human visual system characteristics
SO VISUAL COMPUTER
LA English
DT Article
DE Digital image watermarking; Discrete wavelet transform (DWT); Human
   visual system (HVS); Singular values decomposition (SVD); Particle swarm
   optimization (PSO)
AB This paper presents a new intelligent image watermarking scheme based on discrete wavelet transform (DWT) and singular values decomposition (SVD) using human visual system (HVS) and particle swarm optimization (PSO). The cover image is transformed by one-level (DWT) and subsequently the LL sub-band of (DWT) transformed image is chosen for embedding. To achieve the highest possible visual quality, the embedding regions are selected based on (HVS). After applying (SVD) on the selected regions, every two watermark bits are embedded indirectly into the U and Vt components of SVD decomposition of the selected regions, instead of embedding one watermark bit into the U component and compensating on the Vt component that results in twice capacity and reasonable imperceptibility. In addition, for increasing the robustness without losing the transparency, the scaling factors are chosen automatically by (PSO) based on the attacks test results and predefined conditions, instead of using fixed or manually set scaling factors for all different cover images. Experimental and comparative results demonstrated the stability and improved performance of the proposed scheme compared to its parents watermarking schemes. Moreover, the proposed scheme is free of false positive detection error.
C1 [Bagheri Baba Ahmadi, Sajjad; Zhang, Gongxuan; Wei, Songjie; Boukela, Lynda] Nanjing Univ Sci & Technol, Dept Comp Sci & Engn, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology
RP Ahmadi, SBB; Zhang, GX (corresponding author), Nanjing Univ Sci & Technol, Dept Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM s.bagheri@njust.edu.cn; gongxuan@njust.edu.cn; swei@njust.edu.cn;
   Lyndaboukela@njust.edu.cn
RI Bagheri Baba Ahmadi, Sajjad/ABG-5654-2020; Zhang,
   Gongxuan/HKE-1007-2023; Boukela, Lynda/KUC-7909-2024
OI Bagheri Baba Ahmadi, Sajjad/0000-0003-0382-0832; Zhang,
   Gongxuan/0000-0003-2925-5624; Boukela, Lynda/0000-0002-1201-7438
FU National Science Foundation of China [61272420, 61472189]
FX The authors would like to thank the anonymous reviewers for their
   constructive comments that helped us to greatly improved the quality and
   readability of the paper. This work has been supported by the National
   Science Foundation of China under Grant Numbers 61272420 and 61472189.
CR ANDREWS HC, 1976, IEEE T COMMUN, V24, P425, DOI 10.1109/TCOM.1976.1093309
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P783, DOI 10.1109/83.918570
   Chanu OB, 2019, VISUAL COMPUT, V6, P1
   Chung KL, 2007, APPL MATH COMPUT, V188, P54, DOI 10.1016/j.amc.2006.09.117
   Ernawan F, 2017, Journal of Telecommunication, Electronic and Computer Engineering (JTEC), V9, P111
   Ernawan F., 2019, Int. J. Electr. Computer Eng., V9, P2185, DOI [DOI 10.11591/IJECE.V9I3.PP2185-2195, 10.11591/ijece.v9i3.pp2185-2195]
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Ernawan F, 2018, 2018 IEEE 14TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA 2018), P221, DOI 10.1109/CSPA.2018.8368716
   Fan MQ, 2008, APPL MATH COMPUT, V203, P926, DOI 10.1016/j.amc.2008.05.003
   Garbaczewski P., 2005, Entropy, V7, P253, DOI DOI 10.3390/E7040253
   He KF, 2006, IEEE ICMA 2006: PROCEEDING OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS 1-3, PROCEEDINGS, P2352
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Konstantinides K, 1997, IEEE T IMAGE PROCESS, V6, P479, DOI 10.1109/83.557359
   Lai CC, 2011, OPT COMMUN, V284, P938, DOI 10.1016/j.optcom.2010.10.047
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   Loukhaoukha K., 2011, J Inf Hiding Multim Signal Process, V2, P303
   Maity SP, 2010, AEU-INT J ELECTRON C, V64, P243, DOI 10.1016/j.aeue.2008.10.004
   Makbol NM, 2016, IET IMAGE PROCESS, V10, P34, DOI 10.1049/iet-ipr.2014.0965
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Rastegar S, 2011, AEU-INT J ELECTRON C, V65, P658, DOI 10.1016/j.aeue.2010.09.008
   Roy S, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170326
   Run RS, 2012, EXPERT SYST APPL, V39, P673, DOI 10.1016/j.eswa.2011.07.059
   Sadek RA, 2012, INT J ADV COMPUT SC, V3, P26
   Sheskin J.D., 2004, Handbook of Parametric and Nonparametric Statistical Procedures, VThird
   Shih Frank Y, 2017, Digital watermarking and steganography: fundamentals and techniques
   Singh L, 2020, MULTIMED TOOLS APPL, V79, P15901, DOI 10.1007/s11042-018-6407-5
   Verma VS, 2015, IETE TECH REV, V32, P479, DOI 10.1080/02564602.2015.1042927
   Wang MS, 2009, COMPUT STAND INTER, V31, P757, DOI 10.1016/j.csi.2008.09.003
   Zhang XP, 2005, IEEE T MULTIMEDIA, V7, P593, DOI 10.1109/TMM.2005.843357
   Zhang YQ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134381
NR 31
TC 54
Z9 54
U1 3
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 385
EP 409
DI 10.1007/s00371-020-01808-6
EA FEB 2020
PG 25
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000515883500001
DA 2024-07-18
ER

PT J
AU Pan, JJ
   Yang, YH
   Gao, Y
   Qin, H
   Si, YQ
AF Pan, Junjun
   Yang, Yuhan
   Gao, Yang
   Qin, Hong
   Si, Yaqing
TI Real-time simulation of electrocautery procedure using meshfree methods
   in laparoscopic cholecystectomy
SO VISUAL COMPUTER
LA English
DT Article
DE Meshfree; Cholecystectomy; Virtual reality; Thermal transmission model;
   GPU; Electrocautery
ID MODEL
AB Virtual reality (VR) medical simulators with visual and haptic feedback provide an efficient and cost-effective alternative without any risk to the traditional training approaches. Electrocautery is one of the essential training tasks in laparoscopic cholecystectomy. In order to achieve a high fidelity with visual realism in electrocautery simulation, we propose a physical-based solution to handle the soft tissue deformation and topology change which occurs due to the heat generated by electro-hook. The whole computation is built on discrete particles. For cholecystectomy simulation, the first task is to remove the fat tissue which wraps the cystic artery and duct with electro-hook. We use a meshfree method to handle the fat tissue deformation which based on continuum elasticity equations. And a meshfree dissection model is also introduced to handle the thermal transmission when electro-hook touches the fat surface. Both models are implemented on GPU to achieve real-time performance. The visual performance and computational cost of the proposed method are properly evaluated and compared with other popularly used approaches. From the experimental results, our electrocautery simulation can achieve real-time performance with high degree of realism and fidelity. This technique has also been adopted in our developed VR-based laparoscopic surgery simulator, which has been tested and verified by laparoscopic surgeons through a pilot study. Surgeons believed that the visual performance is realistic and helpful to enhance laparoscopic electrocautery skills. Our system exhibits the potentials to improve the surgical skills of medical practitioners during their training sessions and effectively shorten their learning curve.
C1 [Pan, Junjun; Yang, Yuhan; Gao, Yang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Si, Yaqing] Zhengzhou Univ, Affiliated Hosp 1, Zhengzhou, Henan, Peoples R China.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook; Zhengzhou University
RP Pan, JJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM pan_junjun@hotmail.com; qin@cs.stonybrook.edu
RI Gao, Yang/JQV-9627-2023; Pan, Junjun/A-1316-2013
OI Gao, Yang/0000-0002-9149-3554; 
FU National Key R&D Program of China [2018YFC0115102]; National Nature
   Science Foundation of China [61532002, 61672149, 61872020, 61872347];
   NSF [IIS-1715985, IIS-1812606]
FX This research has been supported by National Key R&D Program of China
   (Grant No. 2018YFC0115102), National Nature Science Foundation of China
   (Grant Nos. 61532002, 61672149, 61872020, 61872347), NSF IIS-1715985,
   NSF IIS-1812606.
CR [Anonymous], 2011, ZYGOTE 3D SCI
   [Anonymous], 2013, PARTICLE SIMULATION
   [Anonymous], 2018, SURG SCI
   Babuska I, 2004, COMPUT METHOD APPL M, V193, P4057, DOI 10.1016/j.cma.2004.03.002
   Berndt I, 2017, IEEE COMPUT GRAPH, V37, P24, DOI 10.1109/MCG.2017.45
   Bonjer HJ, 2005, LANCET ONCOL, V6, P477, DOI 10.1016/S1470-2045(05)70221-7
   Cheng QQ, 2017, IEEE ACCESS, V5, P16359, DOI 10.1109/ACCESS.2017.2731990
   Dodde RE, 2007, ASME INT MANUF SCI E, V1, P2
   Gao Y, 2017, GRAPH MODELS, V94, P14, DOI 10.1016/j.gmod.2017.09.001
   Nguyen H, 2007, EUR J PSYCHOL, V3
   Iwasaki K, 2010, COMPUT GRAPH FORUM, V29, P2215, DOI 10.1111/j.1467-8659.2010.01810.x
   Jin X., 2013, COMPUTATIONAL BIOMEC, DOI [10.1007/978-1-4614-6351-1_6, DOI 10.1007/978-1-4614-6351-1_6]
   Jin X, 2014, COMPUT METHOD BIOMEC, V17, P800, DOI 10.1080/10255842.2012.716829
   Joldes GR, 2011, INT J NUMER METH BIO, V27, P173, DOI 10.1002/cnm.1407
   Jugenheimer M, 2009, LAPAROSCOPIC CHOLECY
   Kim Y., 2012, MESH PROCESSING MED, DOI [10.1007/978-3-642-33463-4_8, DOI 10.1007/978-3-642-33463-4_8]
   Kim Y, 2015, VISUAL COMPUT, V31, P485, DOI 10.1007/s00371-014-0944-3
   Lu ZH, 2014, INT J MED ROBOT COMP, V10, P495, DOI 10.1002/rcs.1561
   Müller M, 2007, J VIS COMMUN IMAGE R, V18, P109, DOI 10.1016/j.jvcir.2007.01.005
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Muller M., 2003, ACM SIGGRAPH, V6, DOI [10.2312/sca03/154-159, DOI 10.2312/SCA03/154-159]
   Pan J, 2016, VISUAL COMPUT
   Pan JJ, 2011, INT J MED ROBOT COMP, V7, P304, DOI 10.1002/rcs.399
   Pan JJ, 2015, COMPUT ANIMAT VIRT W, V26, P321, DOI 10.1002/cav.1655
   PrzemyslawKorzeniowski AB, 2016, INT J COMPUT ASSIST, V11, P12
   Schwer L. E, 2009, SMIRT 20
   Simbionix simulation, 2017, 3D SYST
   Wang MN, 2018, INT J MED ROBOT COMP, V14, DOI 10.1002/rcs.1923
   Yuan Sui J, 2017, INT J MED ROBOT COMP
   Yuan ZY, 2010, ADV MATER RES-SWITZ, V121-122, P154, DOI 10.4028/www.scientific.net/AMR.121-122.154
   Zou YN, 2017, COMPUT METH PROG BIO, V148, P113, DOI 10.1016/j.cmpb.2017.06.013
   Zou YN, 2017, IEEE T CYBERNETICS, V47, P3494, DOI 10.1109/TCYB.2016.2560938
NR 32
TC 6
Z9 8
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 861
EP 872
DI 10.1007/s00371-019-01680-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200008
OA Bronze
DA 2024-07-18
ER

PT J
AU Todo, H
   Yatagawa, T
   Sawayama, M
   Dobashi, Y
   Kakimoto, M
AF Todo, Hideki
   Yatagawa, Tatsuya
   Sawayama, Masataka
   Dobashi, Yoshinori
   Kakimoto, Masanori
TI Image-based translucency transfer through correlation analysis over
   multi-scale spatial color distribution
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based material transfer; Multi-scale image analysis; Principal
   components analysis
AB This paper introduces an image-based material transfer framework which only requires single input and reference images as an ordinary color transfer method. In contrast to previous material transfer methods, we focus on transferring the appearances of translucent objects. There are two challenging problems in such material transfer for translucent objects. First, the appearances of translucent materials are characterized by not only colors but also their spatial distribution. Unfortunately, the traditional color transfer methods can hardly handle the translucency because they only consider the colors of the objects. Second, temporal coherency in the transferred results cannot be handled by the traditional methods and furthermore by recent neural style transfer methods, as we demonstrated in this paper. To address these problems, we propose a novel image-based material transfer method based on the analysis of spatial color distribution. We focus on subbands, which represent multi-scale image structures, and find that the correlation between color distribution and subbands is a key feature for reproducing the appearances of translucent materials. Our method relies on a standard principal component analysis, which harmonizes the correlation of input and reference images to reproduce the translucent appearance. Considering the spatial color distribution in the input and reference images, our method can be naturally applied to video sequences in a frame-by-frame manner without any additional pre-/post-process. Through experimental analyses, we demonstrate that the proposed method can be applied to a broad variety of translucent materials, and their resulting appearances are perceptually similar to those of the reference images.
C1 [Todo, Hideki] Chuo Gakuin Univ, Fac Liberal Arts, 451 Kujike, Abiko, Chiba 2701196, Japan.
   [Yatagawa, Tatsuya] Waseda Univ, Tokyo, Japan.
   [Sawayama, Masataka] NTT Commun Sci Labs, Sensory Representat Grp, Human Informat Sci Lab, Yokohama, Kanagawa, Japan.
   [Dobashi, Yoshinori] Hokkaido Univ, Dwango CG Res, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido, Japan.
   [Kakimoto, Masanori] Tokyo Univ Technol, Dwango CG Res, Sch Media Sci, Tokyo, Japan.
C3 Waseda University; Nippon Telegraph & Telephone Corporation; Hokkaido
   University; Tokyo University of Technology
RP Todo, H (corresponding author), Chuo Gakuin Univ, Fac Liberal Arts, 451 Kujike, Abiko, Chiba 2701196, Japan.
EM todo@fla.cgu.ac.jp; tatsy@acm.org; masataka.sawayama.eh@hco.ntt.co.jp;
   doba@ime.ist.hokudai.ac.jp; kakimotoms@stf.teu.ac.jp
RI Yatagawa, Tatsuya/HKV-3976-2023; Sawayama, Masataka/IAQ-4450-2023
OI Yatagawa, Tatsuya/0000-0003-4653-2435; 
FU JSPS KAKENHI [JP15H05924, 16J02280]; Grants-in-Aid for Scientific
   Research [16J02280] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI (Grant Number JP15H05924) and
   Grant-in-Aid for JSPS Fellows (Grant Number 16J02280).
CR Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Boyadzhiev I, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2809796
   CARROLL R, 2001, ACM T GRAPHIC, V30, DOI DOI 10.1145/2010324.1964938
   Chadwick AC, 2018, J VISION, V18, DOI 10.1167/18.11.18
   Chen D., 2017, PROCEEDINGS OF INTER
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Dumoulin V., 2016, P INT C LEARNING REP
   Fier J, 2017, ACM T GRAPH, V36, P155
   Fleming R. W., 2005, ACM Transactions on Applied Perception (TAP), V2, P346, DOI DOI 10.1145/1077399.1077409
   Fleming RW, 2017, ANNU REV VIS SCI, V3, P365, DOI 10.1146/annurev-vision-102016-061429
   Gao C., 2018, ARXIV 1807 01197
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gkioulekas I, 2015, PROC CVPR IEEE, P5528, DOI 10.1109/CVPR.2015.7299192
   Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516972
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Innamorati C, 2017, COMPUT GRAPH FORUM, V36, P15, DOI 10.1111/cgf.13220
   Jakob Wenzel, 2010, Mitsuba renderer
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan EA, 2006, ACM T GRAPHIC, V25, P654, DOI 10.1145/1141911.1141937
   [李承铭 Li Chengming], 2011, [土木工程学报, China Civil Engineering Journal], V44, P1
   Liao J., 2017, ACM Trans. Graph.
   Liu G., 2017, ARXIV170800106
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Motoyoshi I, 2010, J VISION, V10, DOI 10.1167/10.1.3
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ruder M, 2018, INT J COMPUT VISION, V126, P1199, DOI 10.1007/s11263-018-1089-z
   Sawayama M, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006061
   Schmidt TW, 2016, COMPUT GRAPH FORUM, V35, P216, DOI 10.1111/cgf.12721
   Sharan L., 2009, J VISION, V9, P784, DOI 10.1167/9.8.784
   Xiao B, 2014, J VISION, V14, DOI 10.1167/14.3.17
   Zhu J. Y., 2017, P INT C COMP VIS 201
NR 35
TC 6
Z9 7
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 811
EP 822
DI 10.1007/s00371-019-01676-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Tripathi, G
   Singh, K
   Vishwakarma, DK
AF Tripathi, Gaurav
   Singh, Kuldeep
   Vishwakarma, Dinesh Kumar
TI Convolutional neural networks for crowd behaviour analysis: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural networks; Crowd behaviour; Stochastic gradient
   descent; Deep learning; Anomaly detection
ID ANOMALY DETECTION; MOTION; VIDEO; LOCALIZATION; SURVEILLANCE; VISION;
   IMAGES; MODEL
AB Interest in automatic crowd behaviour analysis has grown considerably in the last few years. Crowd behaviour analysis has become an integral part all over the world for ensuring peaceful event organizations and minimum casualties in the places of public and religious interests. Traditionally, the area of crowd analysis was computed using handcrafted features. However, the real-world images and videos consist of nonlinearity that must be used efficiently for gaining accuracies in the results. As in many other computer vision areas, deep learning-based methods have taken giant strides for obtaining state-of-the-art performance in crowd behaviour analysis. This paper presents a comprehensive survey of current convolution neural network (CNN)-based methods for crowd behaviour analysis. We have also surveyed popular software tools for CNN in the recent years. This survey presents detailed attributes of CNN with special emphasis on optimization methods that have been utilized in CNN-based methods. It also reviews fundamental and innovative methodologies, both conventional and latest methods of CNN, reported in the last few years. We introduce a taxonomy that summarizes important aspects of the CNN for approaching crowd behaviour analysis. Details of the proposed architectures, crowd analysis needs and their respective datasets are reviewed. In addition, we summarize and discuss the main works proposed so far with particular interest on CNNs on how they treat the temporal dimension of data, their highlighting features and opportunities and challenges for future research. To the best of our knowledge, this is a unique survey for crowd behaviour analysis using the CNN. We hope that this survey would become a reference in this ever-evolving field of research.
C1 [Tripathi, Gaurav; Singh, Kuldeep] Bharat Elect Ltd, Cent Res Lab, Ghaziabad 201010, India.
   [Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Dept Informat Technol, Delhi 110042, India.
C3 Bharat Electronics Limited; Delhi Technological University
RP Vishwakarma, DK (corresponding author), Delhi Technol Univ, Dept Informat Technol, Delhi 110042, India.
EM gauravtripathy@gmail.com; kuldeep.er@gmail.com; dvishwakarma@gmail.com
RI VISHWAKARMA, DINESH/ABK-7887-2022; VISHWAKARMA, DINESH
   KUMAR/L-3815-2018; Singh, Kuldeep/GZM-9100-2022
OI VISHWAKARMA, DINESH KUMAR/0000-0002-1026-0047; 
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Afsar P, 2015, EXPERT SYST APPL, V42, P6935, DOI 10.1016/j.eswa.2015.05.023
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Ali S, 2007, PROC CVPR IEEE, P65
   Andersson M, 2013, IEEE J-STSP, V7, P102, DOI 10.1109/JSTSP.2013.2237882
   [Anonymous], INT C AUT FAC GEST R
   [Anonymous], IEEE WINT C APPL COM
   [Anonymous], ARXIV170500027
   [Anonymous], 2011, ICCV
   [Anonymous], 2017, 31 AAAI C ART INT AA
   [Anonymous], 2014, ARXIV13114025
   [Anonymous], DETECTION VIDEO ANOM
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], INT C COMP VIS
   [Anonymous], NATURE
   [Anonymous], ARXIV170305530
   [Anonymous], INT C SIGN PROC ICSP
   [Anonymous], WORKSH APPL COMP VIS
   [Anonymous], 2016 IEEE INT C IM P
   [Anonymous], 2010, INT C MACH LEARN
   [Anonymous], 2013, Modeling, Simulation and Visual Analysis of Crowds: A Multidisciplinary Perspective
   [Anonymous], 2011, ADV NEURAL INFORM PR
   [Anonymous], INT J COMPUT VIS
   [Anonymous], IMAGE ANAL PROCESSIN
   [Anonymous], 2012, ICPR WORKSH PATT REC
   [Anonymous], 2014, CVPR
   [Anonymous], 2012, DEEP LEARNING UNSUPE
   [Anonymous], 2014, ARXIV14123409
   [Anonymous], 2008, INT C ADV CONC INT V
   [Anonymous], 1998, P IEEE
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2015, IEEE C COMP VIS PATT
   [Anonymous], INT C PATT REC ICPR
   [Anonymous], 2017 14 IEEE INT C A
   [Anonymous], IEEE INT C ADV VID S
   [Anonymous], 2017, ARXIV170707890
   [Anonymous], INT C IM PROC ICIP H
   [Anonymous], ACM C REC SYST HONG
   [Anonymous], ARXIV161200220
   [Anonymous], INT C FRONT INF TECH
   [Anonymous], IEEE INT C ROB BIOM
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2017, ARXIV170709725
   [Anonymous], 2014, FULLY CONVOLUTIONAL
   [Anonymous], 2015, P IEEE INT C COMP VI
   [Anonymous], ARXIV160403505
   [Anonymous], 2012, BMVC
   [Anonymous], 2010, ADV NEURAL INF PROCE
   [Anonymous], MULTIMED TOOLS APPL
   [Anonymous], 2012, IEEE COMP SOC C COMP
   [Anonymous], 2016, ARXIV160506465
   [Anonymous], 2013, IEEE C COMP VIS PATT
   [Anonymous], 2010, INT C SIGN PROC SYST
   [Anonymous], 2014, P IEEE C COMP VIS PA
   [Anonymous], THESIS
   [Anonymous], 2000, Proc. of Image and Vision Computing
   [Anonymous], 2014, EUR C COMP VIS
   [Anonymous], 2015, ARXIV14114038
   [Anonymous], 2009, IEEE C COMP VIS PATT
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], CONVOLUTION
   [Anonymous], AAAI WORKSH
   [Anonymous], INT WORKSH HUM BEH U
   [Anonymous], 2013, PMLR
   [Anonymous], 2013, J. Postdr. Res
   [Anonymous], 2014, OPEN NN OPEN SOURCE
   [Anonymous], 2015, P 23 ACM INT C MULT
   [Anonymous], SCAND C IM AN
   [Anonymous], 2016, EUR C COMP VIS
   [Anonymous], 1999, IEEE C COMP VIS PATT
   [Anonymous], ARXIV161000307
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2013, RECTIFIER NONLINEARI
   [Anonymous], 2016, INT C LEARN REPR ICL
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2017, IEEE T PATTERN ANAL
   [Anonymous], JOINT EUR C MACH LEA
   [Anonymous], 2010, IEEE C COMP VIS PATT
   [Anonymous], 2015, IEEE C COMP VIS PATT
   [Anonymous], 2013, Maxout networks
   [Anonymous], INT C ADV VID SIGN B
   [Anonymous], 2016, ARXIV160802908
   [Anonymous], ARXIV170909121
   [Anonymous], INT C STAT LANG SPEE
   [Anonymous], IEEE C IEEE COMP VIS
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], BIGLEARN NIPS WORKSH
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2010, NIPS
   [Anonymous], 2010, P ANN C NEUR INF PRO
   [Anonymous], INT C IM PROC ICIP M
   [Anonymous], IEEE ACM INT S COD G
   [Anonymous], 2013, ARXIV201313013557
   [Anonymous], 2010, CVPR
   [Anonymous], ACM MULT C AMST
   [Anonymous], ARXIV170202359
   [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], ACM INT C MULT
   [Anonymous], 2015, Recent Advances in Convolutional Neural Networks
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], INT C INT INF HID MU
   [Anonymous], 2016, EUR C COMP VIS
   [Anonymous], P MEDIAEVAL 2012 WOR
   [Anonymous], 2016, ARXIV160900866
   [Anonymous], 2016, EUR C COMP VIS
   [Anonymous], EUR C COMP VIS
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2010 20 INT C PATT R
   [Anonymous], AS PAC SIGN INF PROC
   [Anonymous], EUR C COMP VIS
   [Anonymous], INT J COMPUT VIS
   [Anonymous], 2010, INT C COMP STAT COMP
   [Anonymous], 2017, P AS C COMP VIS
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bengio Y., 2007, INT C NEUR INF PROC
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bishop Christopher M, 2006, PATTERN RECOGNITION, DOI DOI 10.1117/1.2819119
   Blunsden S., 2010, ANN BMVA, V4, P4, DOI DOI 10.5465/19416521003654160
   Burges C. J, 2010, MNIST HANDWRITTEN DI
   Candamo J, 2010, IEEE T INTELL TRANSP, V11, P206, DOI 10.1109/TITS.2009.2030963
   Cao LJ, 2015, PATTERN RECOGN, V48, P3016, DOI 10.1016/j.patcog.2015.04.001
   Chan AB, 2016, ARXIV161106748
   Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738
   Chen DY, 2013, IEEE SENS J, V13, P2129, DOI 10.1109/JSEN.2013.2245889
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T., 2015, P NEUR INF PROC SYST
   Cheng ZW, 2014, NEUROCOMPUTING, V136, P124, DOI 10.1016/j.neucom.2014.01.019
   Cho SH, 2014, PATTERN RECOGN LETT, V44, P64, DOI 10.1016/j.patrec.2013.11.017
   Clevert D.A, 2015, 4 INT C LEARN REPR I
   Dean G., 2012, NIPS
   Deng L, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/atsip.2013.9
   Dimokranitou Asimenia., 2017, Adversarial autoencoders for anomalous event detection in images
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Fisher L., 2009, The Perfect Swarm: The Science of Complexity in Everyday Life
   Fu M, 2015, ENG APPL ARTIF INTEL, V43, P81, DOI 10.1016/j.engappai.2015.04.006
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Garcia-Bunster G, 2012, IET COMPUT VIS, V6, P296, DOI 10.1049/iet-cvi.2011.0138
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Geoffrey EHinton., 2012, Improving neural networks by preventing co-adaptation of feature detectors
   Ghanem B, 2010, LECT NOTES COMPUT SC, V6312, P223
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gowsikhaa D, 2014, ARTIF INTELL REV, V42, P747, DOI 10.1007/s10462-012-9341-3
   Grant JM, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3052930
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Hu YC, 2016, J VIS COMMUN IMAGE R, V38, P530, DOI 10.1016/j.jvcir.2016.03.021
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Hyvärinen A, 2007, NETWORK-COMP NEURAL, V18, P81, DOI 10.1080/09548980701418942
   Idrees H, 2015, IEEE T PATTERN ANAL, V37, P1986, DOI 10.1109/TPAMI.2015.2396051
   Ihaddadene N, 2008, INT C PATT RECOG, P217
   Johansson A, 2008, ADV COMPLEX SYST, V11, P497, DOI 10.1142/S0219525908001854
   Kalchbrenner Nal, 2016, ARXIV161010099
   Kephart JO, 2003, COMPUTER, V36, P41, DOI 10.1109/MC.2003.1160055
   King DB, 2015, ACS SYM SER, V1214, P1
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kok VJ, 2016, NEUROCOMPUTING, V177, P342, DOI 10.1016/j.neucom.2015.11.021
   Krausz B, 2012, COMPUT VIS IMAGE UND, V116, P307, DOI 10.1016/j.cviu.2011.08.006
   Krisp J.M., 2013, Earth Observation of Global Changes (EOGC), P255, DOI [DOI 10.1007/978-3-642-32714-8, DOI 10.1007/978-3-642-32714-8_17]
   Kumagai S., 2017, arXiv preprint arXiv:1703.09393
   Leggett R., 2004, Real-Time Crowd Simulation: A Review
   Lemley J, 2017, IEEE CONSUM ELECTR M, V6, P48, DOI 10.1109/MCE.2016.2640698
   Leo M, 2017, COMPUT VIS IMAGE UND, V154, P1, DOI 10.1016/j.cviu.2016.09.001
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Liang RH, 2014, NEUROCOMPUTING, V133, P377, DOI 10.1016/j.neucom.2013.12.040
   Liao Honghong., 2011, Image and Graphics (ICIG), 2011 Sixth International Conference on, P731
   Lin M., 2013, P 2 INT C LEARNING R
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lo SCB, 1995, IEEE T MED IMAGING, V14, P711, DOI 10.1109/42.476112
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Moore BE, 2011, COMMUN ACM, V54, P64, DOI 10.1145/2043174.2043192
   Ooi B.C., 2015, ACM INT C MULT
   Oord V.D., 2016, ARXIV160903499
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Rao AS, 2015, VISUAL COMPUT, V31, P1533, DOI 10.1007/s00371-014-1032-4
   Rippel O., 2015, NIPS
   Rodriguez M., 2011, IEEE INT C COMP VIS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Sam D.B., 2017, P IEEE C COMP VIS PA
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sercu T., 2016, NIPS WORKSH
   Shao J, 2017, IEEE T CIRC SYST VID, V27, P613, DOI 10.1109/TCSVT.2016.2593647
   Jacques JCS, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.937394
   Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1
   Simonyan K., 2014, 14091556 ARXIV
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Sjarif N. A., 2012, Int. J. Adv. Software Comput. Appl, V4, P1
   Sodemann AA, 2012, IEEE T SYST MAN CY C, V42, P1257, DOI 10.1109/TSMCC.2012.2215319
   Solmaz B, 2012, IEEE T PATTERN ANAL, V34, P2064, DOI 10.1109/TPAMI.2012.123
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thida M., 2013, Intelligent multimedia surveillance, P17, DOI DOI 10.1007/978-3-642-41512-8_2
   Tieleman T., 2012, Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude, P26
   Vicsek T, 2012, PHYS REP, V517, P71, DOI 10.1016/j.physrep.2012.03.004
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wallach I., 2015, arXiv
   Wang B, 2012, MACH VISION APPL, V23, P501, DOI 10.1007/s00138-011-0341-0
   Wang LA, 2003, PATTERN RECOGN, V36, P585, DOI 10.1016/S0031-3203(02)00100-0
   Wing JM, 2006, COMMUN ACM, V49, P33, DOI 10.1145/1118178.1118215
   Wu S, 2017, INT J COMPUT VISION, V123, P499, DOI 10.1007/s11263-017-1005-y
   Xu D, 2014, NEUROCOMPUTING, V143, P144, DOI 10.1016/j.neucom.2014.06.011
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Yu H., 2016, PROC CVPR IEEE, P952
   Zagoruyko S., 2016, BMVC, P1
   Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4
   Zhang ZX, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.4.047204
   Zhao ZY, 2016, LECT NOTES COMPUT SC, V9912, P712, DOI 10.1007/978-3-319-46484-8_43
   Zhou BY, 2012, COMPUT VIS IMAGE UND, V116, P1014, DOI 10.1016/j.cviu.2012.05.005
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
   Zitouni MS, 2016, NEUROCOMPUTING, V186, P139, DOI 10.1016/j.neucom.2015.12.070
NR 220
TC 92
Z9 98
U1 2
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2019
VL 35
IS 5
BP 753
EP 776
DI 10.1007/s00371-018-1499-5
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HZ0IT
UT WOS:000468524900011
DA 2024-07-18
ER

PT J
AU Fan, X
   Tang, XX
   Hou, MJ
   Luo, ZX
AF Fan, Xin
   Tang, Xianxuan
   Hou, Minjun
   Luo, Zhongxuan
TI Fast example searching for input-adaptive data-driven dehazing with
   Gaussian process regression
SO VISUAL COMPUTER
LA English
DT Article
DE Image dehazing; Example searching; Gaussian process regression; Input
   adaptive
AB Recently, data-driven approaches are prevailing in low-level image processing including single image dehazing. The performance of these methods can behave better when the learning process adapts to the input. This input-adaptive training demands efficiently selecting optimal examples for the input from a large training set. In this paper, we address the issue of input-specific example searching and propose a fast searching strategy on vast image examples to learn a more accurate Gaussian process (GP) regressor for single image dehazing. The GP regression learnt from these optimal examples is able to produce the transmission prediction with lower variance and thus renders high robustness. Extensive experiments on hazy images at various haze levels demonstrate the effectiveness of the proposed example searching compared with the state-of-the-art data-driven dehazing methods.
C1 [Fan, Xin] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Tang, Xianxuan; Hou, Minjun; Luo, Zhongxuan] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Fan, X (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian, Peoples R China.
EM xin.fan@ieee.org
FU Natural Science Foundation of China [61572096, 61432003, 61733002]
FX This work is partially supported by the Natural Science Foundation of
   China under Grant Nos. 61572096, 61432003, and 61733002. The authors are
   grateful to Prof. Ming-Ting Sun at the University of Washington and Dr.
   Jue Wang at Megvii Inc. for their constructive discussions and
   suggestions.
CR Ancuti CO, 2011, LECT NOTES COMPUT SC, V6493, P501
   [Anonymous], INT C DIG HOM
   [Anonymous], AAAI
   [Anonymous], ELECT IMAGING 2007
   [Anonymous], 16348 U WISC MAD
   [Anonymous], PACIFIC GRAPHICS SHO
   [Anonymous], COMPUTER VISION PATT
   [Anonymous], TIP
   [Anonymous], 2016, EUR C COMP VIS
   [Anonymous], INT C COMP VIS
   [Anonymous], IEEE T PATTERN ANAL
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cao Yanshuai., 2013, P ADV NEURAL INFORM, V26, P1097
   Chen LP, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P273
   Domingos P, 2012, COMMUN ACM, V55, P78, DOI 10.1145/2347736.2347755
   Fan X., 2016, IEEE Trans. Circuits Syst. Video Technol
   Fan X, 2016, VISUAL COMPUT, V32, P137, DOI 10.1007/s00371-015-1083-1
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Feng YY, 2017, IEEE INT CON MULTI, P385, DOI 10.1109/ICME.2017.8019469
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Frost R, 2015, TRENDS COGN SCI, V19, P117, DOI 10.1016/j.tics.2014.12.010
   Gibson KB, 2013, IEEE IMAGE PROC, P728, DOI 10.1109/ICIP.2013.6738150
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Kwon Y, 2015, IEEE T PATTERN ANAL, V37, P1792, DOI 10.1109/TPAMI.2015.2389797
   Lázaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Luo GY, 2012, INT J PHOTOENERGY, V2012, DOI 10.1155/2012/275209
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Schmidt U, 2013, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2013.84
   Silpa-Anan C, 2008, PROC CVPR IEEE, P2308
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Yue HJ, 2014, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2014.375
   Zhao XD, 2015, IEEE T CIRC SYST VID, V25, P185, DOI 10.1109/TCSVT.2014.2347513
   Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52
NR 39
TC 3
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 565
EP 577
DI 10.1007/s00371-018-1485-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800008
DA 2024-07-18
ER

PT J
AU Kanojia, G
   Raman, S
AF Kanojia, Gagan
   Raman, Shanmuganathan
TI Patch-based detection of dynamic objects in CrowdCam images
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Dynamic objects; Epipolar geometry
AB A scene can be divided into two parts: static and dynamic. The parts of the scene which do not admit any motion are static regions, while moving objects correspond to dynamic regions. In this work, we tackle the challenging task of identifying dynamic objects present in the CrowdCam images. Our approach exploits the coherency present in the natural images and utilizes the epipolar geometry present between a pair of images to achieve this objective. It does not require a dynamic object to be present in all the given images. We show that the proposed approach obtains state-of-the-art accuracy on standard datasets.
C1 [Kanojia, Gagan] Indian Inst Technol Gandhinagar, Prof Shanmuganathan Raman Elect Engn, Palaj 382355, Gandhinagar, India.
   [Raman, Shanmuganathan] Indian Inst Technol Gandhinagar, Elect Engn & Comp Sci & Engn, Palaj 382355, Gandhinagar, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Gandhinagar; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Gandhinagar
RP Kanojia, G (corresponding author), Indian Inst Technol Gandhinagar, Prof Shanmuganathan Raman Elect Engn, Palaj 382355, Gandhinagar, India.
EM gagan.kanojia@iitgn.ac.in; shanmuga@iitgn.ac.in
RI Raman, Shanmuganathan/AAV-2186-2020
OI Raman, Shanmuganathan/0000-0003-2718-7891
FU TCS Research Scholarship
FX We thank Dr. Arka Chattopadhyay for his assistance in revising the
   manuscript. Gagan kanojia was supported by TCS Research Scholarship.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   [Anonymous], 2016, P IEEE C COMP VIS PA
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Basha T, 2012, LECT NOTES COMPUT SC, V7577, P654, DOI 10.1007/978-3-642-33783-3_47
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4
   Dafni A, 2017, COMPUT VIS IMAGE UND, V160, P36, DOI 10.1016/j.cviu.2017.04.004
   Dar M, 2016, PROC CVPR IEEE, P1220, DOI 10.1109/CVPR.2016.137
   Dekel T, 2013, IEEE I CONF COMP VIS, P977, DOI 10.1109/ICCV.2013.125
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gullapally S. C., 2015, 2015 21 NAT C COMM N, P1
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu X, 2016, COMPUT VIS IMAGE UND, V148, P153, DOI 10.1016/j.cviu.2015.11.015
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Margolin R, 2014, LECT NOTES COMPUT SC, V8695, P377, DOI 10.1007/978-3-319-10584-0_25
   Ni QL, 2015, LECT NOTES COMPUT SC, V9491, P17, DOI 10.1007/978-3-319-26555-1_3
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158
   Peng YA, 2018, SIGNAL IMAGE VIDEO P, V12, P99, DOI 10.1007/s11760-017-1135-2
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Pont-Tuset J., 2017, ARXIV170400675
   Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422
   Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861
   Song HJ, 2011, MULTIMED TOOLS APPL, V52, P121, DOI 10.1007/s11042-010-0464-8
   Tian LC, 2017, IEEE INT CON MULTI, P1542, DOI 10.1109/ICME.2017.8019303
   Tian ZQ, 2016, IEEE T MED IMAGING, V35, P791, DOI 10.1109/TMI.2015.2496296
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang C, 2018, VISUAL COMPUT, V34, P67, DOI 10.1007/s00371-016-1312-2
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Wang TFY, 2015, COMPUT GRAPH FORUM, V34, P177, DOI 10.1111/cgf.12706
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   ZHANG C, 1967, TIP, V26, P4055, DOI DOI 10.1109/TIP.2017.2712279
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   ZONTAK M, 2011, PROC CVPR IEEE, P977, DOI DOI 10.1109/CVPR.2011.5995401
NR 48
TC 1
Z9 1
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2019
VL 35
IS 4
BP 521
EP 534
DI 10.1007/s00371-018-1480-3
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HS2EB
UT WOS:000463672800005
DA 2024-07-18
ER

PT J
AU Choi, JY
AF Choi, Jae Young
TI Spatial pyramid face feature representation and weighted dissimilarity
   matching for improved face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Face feature representation; Spatial pyramid; Overlapped patches;
   Dissimilarity matching; Class-wise discriminant power
ID BLOCK-BASED BAG; DISCRIMINANT-ANALYSIS; GABOR; PATTERNS; MODEL;
   ILLUMINATION; EXPRESSION; HISTOGRAM; WORDS; POSE
AB In this paper, we present a novel face recognition (FR) algorithm based on multiresolution spatial pyramid. In our method, a face is subdivided into increasingly finer subregions (local regions) and represented at multiple levels of histogram representations. To address image misalignment problem, overlapped patch-based local descriptor extraction has been also developed in an effective way. To preserve multiple levels of detail in facial local characteristics and to encode holistic spatial configuration, face features obtained for concatenated histograms (coming from all levels of spatial pyramid) are integrated into a combined feature set, termed spatial pyramid face feature representation (SPFR). In addition, to perform recognition by matching between the pair of probe and gallery SPFR sets, we propose the use of a weighted sum of the dissimilarity scores computed at all spatial pyramid levels. For this purpose, we develop a novel weight determination solution based on class-wise discriminant power estimation for face feature at a specific pyramid level. We incorporate our proposed algorithm into general FR pipeline and achieve encouraging identification results on the CMU-PIE, FERET, and LFW datasets, compared to previously developed methods. In addition, the feasibility of our method has been successfully demonstrated by making comparisons with other state-of-the-art FR methods (including deep CNN based method) under the FERET and FRGC 2.0 evaluation protocols. Based on results, our method is advantageous in terms of high recognition accuracy and low complexity, as well as straightforward implementation.
C1 [Choi, Jae Young] Hankuk Univ Foreign Studies, Pattern Recognit & Machine Intelligence Lab, Div Comp & Elect Syst Engn, 81 Oedae Ro, Yongin 17305, Gyeonggi Do, South Korea.
C3 Hankuk University Foreign Studies
RP Choi, JY (corresponding author), Hankuk Univ Foreign Studies, Pattern Recognit & Machine Intelligence Lab, Div Comp & Elect Syst Engn, 81 Oedae Ro, Yongin 17305, Gyeonggi Do, South Korea.
EM jychoi@hufs.ac.kr
FU Hankuk University of Foreign Studies Research Fund; Basic Science
   Research Program through National Research Foundation of Korea (NRF) -
   Ministry of Education [2015R1D1A1A01057420]
FX This work was supported by Hankuk University of Foreign Studies Research
   Fund. This research was also supported by Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education (No.2015R1D1A1A01057420).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 6 INT C DISTR SMART
   Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54
   Chai ZH, 2014, IEEE T INF FOREN SEC, V9, P14, DOI 10.1109/TIFS.2013.2290064
   Chan CH, 2013, IEEE T PATTERN ANAL, V35, P1164, DOI 10.1109/TPAMI.2012.199
   Cheng Y., 2017, VIS COMPUT
   Choi JY, 2012, IEEE T IMAGE PROCESS, V21, P1366, DOI 10.1109/TIP.2011.2168413
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Deb A, 2016, PR IEEE COMP DESIGN, P17, DOI 10.1109/ICCD.2016.7753256
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ding CX, 2016, IEEE T PATTERN ANAL, V38, P518, DOI 10.1109/TPAMI.2015.2462338
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fukunaga K., 1992, INTRO STAT PATTERN R, V2nd
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Geng C, 2009, IEEE IMAGE PROC, P3313, DOI 10.1109/ICIP.2009.5413956
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Hu JL, 2017, IMAGE VISION COMPUT, V60, P48, DOI 10.1016/j.imavis.2016.08.007
   Hua SG, 2012, EURASIP J IMAGE VIDE, P1, DOI 10.1186/1687-5281-2012-6
   Hwang W, 2011, IEEE T IMAGE PROCESS, V20, P1152, DOI 10.1109/TIP.2010.2083674
   Jiang XD, 2008, IEEE T PATTERN ANAL, V30, P383, DOI 10.1109/TPAMI.2007.70708
   Kawahara T., 2016, P IEEE INT C COMP VI, P16
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lei Z, 2007, LECT NOTES COMPUT SC, V4642, P49
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Li ZS, 2011, IEICE T FUND ELECTR, VE94A, P533, DOI 10.1587/transfun.E94.A.533
   Liao S, 2011, LECT NOTES COMPUT SC, V6963, P1, DOI 10.1007/978-3-642-23944-1_1
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2003, PATTERN RECOGN LETT, V24, P3079, DOI 10.1016/S0167-8655(03)00167-3
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Mantecón T, 2016, IEEE SIGNAL PROC LET, V23, P771, DOI 10.1109/LSP.2016.2553784
   Maturana D., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P470, DOI 10.1109/FG.2011.5771444
   Maturana D, 2011, LECT NOTES COMPUT SC, V6495, P618, DOI 10.1007/978-3-642-19282-1_49
   Meng X, 2006, INT C PATT RECOG, P536
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Najafabadi MM, 2015, Journal of big data, V2, P1, DOI [10.1186/s40537-014-0007-7, DOI 10.1186/S40537-014-0007-7]
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Su Y, 2009, IEEE T IMAGE PROCESS, V18, P1885, DOI 10.1109/TIP.2009.2021737
   Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P235
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang ZF, 2014, VISUAL COMPUT, V30, P359, DOI 10.1007/s00371-013-0861-x
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Xie SF, 2009, SIGNAL PROCESS, V89, P2333, DOI 10.1016/j.sigpro.2009.02.016
   Yang M, 2013, IEEE T NEUR NET LEAR, V24, P900, DOI 10.1109/TNNLS.2013.2245340
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhao CH, 2015, OPTIK, V126, P1761, DOI 10.1016/j.ijleo.2015.04.068
   Zhao L, 2016, VISUAL COMPUT, V32, P1165, DOI 10.1007/s00371-015-1169-9
   Zisheng Li, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1285, DOI 10.1109/ICPR.2010.320
NR 58
TC 7
Z9 7
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2018
VL 34
IS 11
BP 1535
EP 1549
DI 10.1007/s00371-017-1429-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GV9ZR
UT WOS:000446521700006
DA 2024-07-18
ER

PT J
AU Splechtna, R
   Beham, M
   Gracanin, D
   Ganuza, ML
   Bühler, K
   Pandzic, IS
   Matkovic, K
AF Splechtna, Rainer
   Beham, Michael
   Gracanin, Denis
   Lujan Ganuza, Maria
   Buehler, Katja
   Pandzic, Igor Sunday
   Matkovic, Kresimir
TI Cross-table linking and brushing: interactive visual analysis of
   multiple tabular data sets
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Visual analytics; Interactive visual analysis; Multiple data sets
   analysis
ID SYSTEM; MODEL
AB Studying complex problems often requires identifying and exploring connections and dependencies among several, seemingly unrelated, data sets. Those data sets are often represented as data tables. We propose a novel approach to studying such data sets using linking and brushing across multiple data tables in a coordinated multiple views system. We first identify possible mappings from a subset of one data set to a subset of another data set. That collection of mappings is then used to specify linking among data sets and to support brushing across data sets. Brushing in one data set is then mapped to a brush in the destination data set. If the brush is refined in the destination data set, the inverse mapping, or a back-link, is used to determine the refined brush in the original data set. Brushing and back-links make it possible to efficiently create and analyze complex queries interactively in an iterative process. That process is further supported by a user interface that keeps track of the mappings, links and brushes. The proposed approach is evaluated using three data sets.
C1 [Splechtna, Rainer; Beham, Michael; Buehler, Katja; Matkovic, Kresimir] VRVis Res Ctr, Donau City Str 11, A-1220 Vienna, Austria.
   [Gracanin, Denis] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.
   [Lujan Ganuza, Maria] UNS, DCIC, VyGLab Res Lab, San Andres 800, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   [Pandzic, Igor Sunday] Univ Zagreb, Fac Elect Engn & Comp, Dept Telecommun, Unska 3, Zagreb 10000, Croatia.
C3 Virginia Polytechnic Institute & State University; National University
   of the South; University of Zagreb
RP Ganuza, ML (corresponding author), UNS, DCIC, VyGLab Res Lab, San Andres 800, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
EM mlg@cs.uns.edu.ar
RI Matkovic, Kresimir/AAT-8896-2021
OI Ganuza, Maria Lujan/0000-0003-4576-2124; Gracanin,
   Denis/0000-0001-6831-2818; Buhler, Katja/0000-0002-0362-7998; Matkovic,
   Kresimir/0000-0001-9406-8943
FU BMVIT; BMWFW; Styria; SFG; Vienna Business Agency [854174]
FX This study was funded by BMVIT, BMWFW, Styria, SFG and Vienna Business
   Agency in the scope of COMET (854174).
CR BECKER RA, 1987, TECHNOMETRICS, V29, P127, DOI 10.2307/1269768
   Card S K., 1999, READINGS INFORM VISU
   Cavalcanti V. M., 2006, P WORK C ADV VIS INT, P412
   Cerullo C, 2007, DEXA 2007: 18TH INTERNATIONAL CONFERENCE ON DATABASE AND EXPERT SYSTEMS APPLICATIONS, PROCEEDINGS, P109, DOI 10.1109/DEXA.2007.91
   CODD EF, 1970, COMMUN ACM, V13, P377, DOI 10.1145/357980.358007
   Date C.J., 1997, GUIDE SQL STANDARD
   Della Penna G, 2013, J VISUAL LANG COMPUT, V24, P71, DOI 10.1016/j.jvlc.2012.11.002
   Doleisch H., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P239
   Fisherkeller M.A., 1988, DYNAMIC GRAPHICS STA, P91
   Garcia-Molina H., 2009, DATABASE SYSTEMS COM
   Hauser H, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P127, DOI 10.1109/INFVIS.2002.1173157
   Konyha Z, 2006, IEEE T VIS COMPUT GR, V12, P1373, DOI 10.1109/TVCG.2006.99
   Kosara R., 2003, STATE OF THE ART P E, P123
   Lichman M., 2013, UCI MACHINE LEARNING
   LIN S, 2013, P 15 EUR C VIS EUROV, P401, DOI DOI 10.1111/CGF.12127
   Ganuza ML, 2014, IEEE T VIS COMPUT GR, V20, P1913, DOI 10.1109/TVCG.2014.2346754
   Mahalanobis P. C., 1936, P NATL I SCI INDIA, V2, P49
   Martin A.R., P IEEE C VIS VIS 95, P271
   Matkovic K, 2008, IEEE INT CONF INF VI, P215, DOI 10.1109/IV.2008.87
   McLachlan G. J., 1999, Resonance, V4, P20
   NOAA, 2018, LAND BAS DAT PROD
   North C., 2000, Proceedings of the the working conference on Advanced visual interfaces (AVI) 2000, P128, DOI [DOI 10.1145/345513.345282, 10.1145/345513.345282]
   Pace RK, 1997, STAT PROBABIL LETT, V33, P291, DOI 10.1016/s0167-7152(96)00140-x
   Rados S, 2016, COMPUT GRAPH FORUM, V35, P251, DOI 10.1111/cgf.12901
   Shadoan R, 2013, IEEE T VIS COMPUT GR, V19, P2070, DOI 10.1109/TVCG.2013.220
   Splechtna R, 2015, IEEE CONF VIS ANAL, P89, DOI 10.1109/VAST.2015.7347635
   Turkay C, 2011, IEEE T VIS COMPUT GR, V17, P2591, DOI 10.1109/TVCG.2011.178
   Vincenty T, 1975, SURV REV, V23, P88, DOI [10.1179/sre.1975.23.176.88, DOI 10.1179/SRE.1975.23.176.88]
   Weaver C, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P159, DOI 10.1109/INFVIS.2004.12
   Weaver C, 2010, IEEE T VIS COMPUT GR, V16, P192, DOI 10.1109/TVCG.2009.94
   Zhicheng Liu, 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P41, DOI 10.1109/VAST.2011.6102440
NR 31
TC 2
Z9 2
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1087
EP 1098
DI 10.1007/s00371-018-1516-8
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400030
DA 2024-07-18
ER

PT J
AU Ibrahim, AEM
AF Ibrahim, Alaa Eldin M.
TI Multiple-process procedural texture
SO VISUAL COMPUTER
LA English
DT Article
DE Procedural texture synthesis; Parallel Monte Carlo tree search; Genetic
   programming; Multiple objectives; Multiple processes;
   Multiple-generation population
ID ALGORITHMS
AB Our newly developed generation of procedural textures (GPT) system automatically generates procedural textures for the computer graphics industry. The system makes use of hybrid parallel Monte Carlo tree search and gender-based genetic algorithm modules that share a common multiple-generation population of procedural textures and a knowledge database. It also uses a multi-objective fitness function. The parallel Monte Carlo tree search module was inspired by gaming algorithms. To speed up the search, this module is enhanced with knowledge from previous successfully created procedural textures or tree node analyses. The gender-based genetic algorithm module automatically simulates several key features in natural selection and uses a multiple-generation breeding population, the notion of gender, and the concept of aging. This maintains diversity while providing many breeding opportunities for highly successful offspring. A third module selects generated shaders from the multiple-generation population and mutates them by replacing nodes with subtrees using the knowledge database. We evaluated the fitness quality of each module and compared the fitness quality of the system running in both single- and multiple-process mode. The optimal fitness quality was achieved by executing the system in multiple-process mode using a hybrid of these modules. We give examples of the GPT running in interactive mode, where a user directs the search towards the desired look using an esthetic evaluation.
C1 [Ibrahim, Alaa Eldin M.] Univ Sharjah, POB 27272, Sharjah, U Arab Emirates.
C3 University of Sharjah
RP Ibrahim, AEM (corresponding author), Univ Sharjah, POB 27272, Sharjah, U Arab Emirates.
EM aitrade2010@yahoo.com
CR [Anonymous], 2010, THESIS
   [Anonymous], COMPUTER GRAPHICS S
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], ARTIFICIAL EVOLUTION
   [Anonymous], 2013, 9 ART INT INT DIG EN
   [Anonymous], ACM T GRAPH P SIGGRA
   [Anonymous], 2010, P AI GAM S AISB 2010
   [Anonymous], 2012, IEEE T COMP INTEL AI, DOI DOI 10.1109/TCIAIG.2012.2186810
   [Anonymous], 9 INT S VIS COMP
   [Anonymous], GENTROPY EVOLUTIONAR
   [Anonymous], 1991, FDN GENETIC ALGORITH
   Bergen S, 2011, GENET EVOL COMPUT, P227
   Boudt K, 2012, STAT COMPUT, V22, P471, DOI 10.1007/s11222-011-9237-0
   Bourque E, 2004, COMPUT GRAPH FORUM, V23, P461, DOI 10.1111/j.1467-8659.2004.00777.x
   Cantú-Paz E, 2003, LECT NOTES COMPUT SC, V2723, P801
   Cazenave T, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P456
   Chaslot GMJB, 2008, LECT NOTES COMPUT SC, V5131, P60, DOI 10.1007/978-3-540-87608-3_6
   Chen KH, 2008, ICGA J, V31, P67
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   Ebner M, 2005, LECT NOTES COMPUT SC, V3447, P261
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   Han JW, 2006, VISUAL COMPUT, V22, P918, DOI 10.1007/s00371-006-0078-3
   Lagae A, 2010, COMPUT GRAPH FORUM, V29, P2579, DOI 10.1111/j.1467-8659.2010.01827.x
   Lagae A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531360
   Lai YC, 2015, VISUAL COMPUT, V31, P83, DOI 10.1007/s00371-013-0908-z
   McGuire M., 2006, I3D 06, P79
   McLEISH Don., 2005, MONTE CARLO SIMULATI
   Nicoara E.S., 2009, Bulletin of PG University of Ploiesti, Series Mathematics, Informatics, Physics, V61, P87
   Ross BJ, 2004, NEW GENERAT COMPUT, V22, P271, DOI 10.1007/BF03040964
   Selvarajah S., 2011, INT J LATEST TRENDS, V2, P108
   Shen JB, 2007, VISUAL COMPUT, V23, P631, DOI 10.1007/s00371-007-0154-3
   Shen JB, 2006, VISUAL COMPUT, V22, P936, DOI 10.1007/s00371-006-0079-2
   SIMS K, 1991, COMP GRAPH, V25, P319, DOI 10.1145/127719.122752
   Toffolo A, 2003, EVOL COMPUT, V11, P151, DOI 10.1162/106365603766646816
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   Upstill S., 1989, RENDERMAN COMPANION
   Wang JN, 2004, PROC SPIE, V5444, P27, DOI 10.1117/12.561110
   WHITLEY D, 1989, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P116
   WITKIN A, 1991, COMP GRAPH, V25, P299, DOI 10.1145/127719.122750
   Zhang D.S., 2000, PROC 1 IEEE PACIFIC, P392
   Zitzler E, 2000, EVOL COMPUT, V8, P173, DOI 10.1162/106365600568202
NR 41
TC 0
Z9 0
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1511
EP 1528
DI 10.1007/s00371-016-1295-z
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600003
DA 2024-07-18
ER

PT J
AU Wang, JH
   Zheng, CX
   Chen, WH
   Wu, XM
AF Wang, Jianhua
   Zheng, Chuanxia
   Chen, Weihai
   Wu, Xingming
TI Learning aggregated features and optimizing model for semantic labeling
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic scene understanding; Aggregated features; Object attribute;
   Joint optimizing model; Conditional random field
ID OBJECTS
AB Semantic labeling for indoor scenes has been extensively developed with the wide availability of affordable RGB-D sensors. However, it is still a challenging task for multi-class recognition, especially for "small" objects. In this paper, a novel semantic labeling model based on aggregated features and contextual information is proposed. Given an RGB-D image, the proposed model first creates a hierarchical segmentation using an adapted gPb/UCM algorithm. Then, a support vector machine is trained to predict initial labels using aggregated features, which fuse small-scale appearance features, mid-scale geometric features, and large-scale scene features. Finally, a joint multi-label Conditional random field model that exploits both spatial and attributive contextual relations is constructed to optimize the initial semantic and attributive predicted results. The experimental results on the public NYU v2 dataset demonstrate the proposed model outperforms the existing state-of-the-art methods on the challenging 40 dominant classes task, and the model also achieves a good performance on a recent SUN RGB-D dataset. Especially, the prediction accuracy of "small" classes has been improved significantly.
C1 [Wang, Jianhua; Zheng, Chuanxia; Chen, Weihai; Wu, Xingming] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Chen, WH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM whchen@buaa.edu.cn
RI Chen, Wei/GZK-7348-2022; Zheng, Chuanxia/GQI-0645-2022
OI /0009-0006-1337-808X
FU National Natural Science Foundation of China [61573048, 61620106012];
   International Scientific and Technological Cooperation Projects of China
   [2015DFG12650]
FX The work described in this paper was supported by the National Natural
   Science Foundation of China under Grant No. 61573048, 61620106012, and
   the International Scientific and Technological Cooperation Projects of
   China under Grant No. 2015DFG12650.
CR Anand A, 2013, INT J ROBOT RES, V32, P19, DOI 10.1177/0278364912461538
   [Anonymous], ECCV
   [Anonymous], 2012, LECT NOTES COMPUT SC, DOI [10.1007/978-3-642-33715-4_54, DOI 10.1007/978-3-642-33715-4_54]
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970
   Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002
   Bo L., 2010, ADV NEURAL INFORM PR, V23, P244
   Cadena C, 2014, IEEE INT CONF ROBOT, P2639, DOI 10.1109/ICRA.2014.6907237
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chao YW, 2013, LECT NOTES COMPUT SC, V8157, P489, DOI 10.1007/978-3-642-41184-7_50
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Chen K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661239
   Chen WH, 2014, OPT LASER ENG, V55, P69, DOI 10.1016/j.optlaseng.2013.10.025
   Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628
   Couprie C., 2013, ARXIV13013572, P1
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Deng Z, 2015, IEEE I CONF COMP VIS, P1733, DOI 10.1109/ICCV.2015.202
   Ding K, 2014, VISUAL COMPUT, V30, P1311, DOI 10.1007/s00371-013-0888-z
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Gupta A., 2010, ADV NEURAL INFORM PR, P1288
   Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hermans A, 2014, IEEE INT CONF ROBOT, P2631, DOI 10.1109/ICRA.2014.6907236
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Wang AR, 2014, LECT NOTES COMPUT SC, V8693, P453, DOI 10.1007/978-3-319-10602-1_30
   Wolf D, 2015, IEEE INT CONF ROBOT, P4867, DOI 10.1109/ICRA.2015.7139875
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zhang J, 2013, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2013.161
   Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43
NR 42
TC 1
Z9 1
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2017
VL 33
IS 12
BP 1587
EP 1600
DI 10.1007/s00371-016-1302-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FK4JE
UT WOS:000413458600008
DA 2024-07-18
ER

PT J
AU Li, RH
   Cai, JR
   Zhang, HL
   Wang, TH
AF Li, Ruihui
   Cai, Jianrui
   Zhang, Hanling
   Wang, Taihong
TI Aggregating complementary boundary contrast with smoothing for salient
   region detection
SO VISUAL COMPUTER
LA English
DT Article
DE Regional contrast; Boundary prior; Smoothing; Iterative framework;
   Salient object detection
AB Automatic to locate the salient regions in the images are useful for many computer vision and computer graphics tasks. However, the previous techniques prefer to give noisy and fuzzy saliency maps, which will be a crucial limitation for the performance of subsequent image processing. In this paper, we present a novel framework by aggregating various bottom-up cues and bias to enhance visual saliency detection. It can produce high-resolution, full-field saliency map which can be close to binary one and more effective in real-world applications. First, the proposed method concentrates on multiple saliency cues in a global context, such as regional contrast, spatial relationship and color histogram smoothing to produce a coarse saliency map. Second, combining complementary boundary prior with smoothing, we iteratively refine the coarse saliency map to improve the contrast between salient and non-salient regions until a close to binary saliency map is reached. Finally, we evaluate our salient region detection on two publicly available datasets with pixel accurate annotations. The experimental results show that the proposed method performs equally or better than the 12 alternative methods and retains comparable detection accuracy, even in extreme cases. Furthermore, we demonstrate that the saliency map produced by our approach can serve as a good initialization for automatic alpha matting and image retargeting.
C1 [Li, Ruihui; Zhang, Hanling] Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
   [Cai, Jianrui] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   [Wang, Taihong] Hunan Univ, Key Lab Micronano Optoelect Devices, Minist Educ, Changsha 410000, Hunan, Peoples R China.
   [Wang, Taihong] Hunan Univ, State Key Lab Chemo Biosensing & Chemometr, Changsha 410000, Hunan, Peoples R China.
C3 Hunan University; Hong Kong Polytechnic University; Hunan University;
   Hunan University
RP Wang, TH (corresponding author), Hunan Univ, Key Lab Micronano Optoelect Devices, Minist Educ, Changsha 410000, Hunan, Peoples R China.; Wang, TH (corresponding author), Hunan Univ, State Key Lab Chemo Biosensing & Chemometr, Changsha 410000, Hunan, Peoples R China.
EM larch18@hnu.edu.cn; csjcai@comp.polyu.edu.uk; jt_hlzhang@hnu.edu.cn;
   hnuwth@163.com
RI Wang, Taihong/R-8389-2018; Wang, Taihong/K-8968-2012; Li,
   Ruihui/AAA-1369-2022; Wang, Taihong/S-2094-2018; CAI,
   Jianrui/U-3662-2019
OI CAI, Jianrui/0000-0002-7397-7774
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2007, INT C COMP VIS SYST
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Borji A., 2012, EUR C COMP VIS
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Borji Ali, 2015, IEEE C COMP VIS PATT
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koch C, 1999, NAT NEUROSCI, V2, P9, DOI 10.1038/4511
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Margolin R, 2013, VISUAL COMPUT, V29, P381, DOI 10.1007/s00371-012-0740-x
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tavakoli HR., 2011, SCAND C IM AN
   Tong N, 2014, IEEE SIGNAL PROC LET, V21, P1035, DOI 10.1109/LSP.2014.2323407
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wang KZ, 2015, IEEE T IMAGE PROCESS, V24, P3019, DOI 10.1109/TIP.2015.2432712
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu HF, 2014, VISUAL COMPUT, V30, P229, DOI 10.1007/s00371-013-0823-3
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   YANG JM, 2012, PROC CVPR IEEE, P2296, DOI [DOI 10.1109/CVPR.2012.6247940, 10.1109/CVPR.2012.6247940]
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhong GY, 2016, VISUAL COMPUT, V32, P611, DOI 10.1007/s00371-015-1077-z
   Zhu H., 2013, IEEE INT S CIRC SYST, P19
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 45
TC 10
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1155
EP 1167
DI 10.1007/s00371-016-1278-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600006
DA 2024-07-18
ER

PT J
AU Ballester-Ripoll, R
   Pajarola, R
AF Ballester-Ripoll, Rafael
   Pajarola, Renato
TI Lossy volume compression using Tucker truncation and thresholding
SO VISUAL COMPUTER
LA English
DT Article
DE Tensor approximation; Data compression; Higher-order decompositions;
   Tensor rank reduction; Multidimensional data encoding
ID TENSOR APPROXIMATION
AB Tensor decompositions, in particular the Tucker model, are a powerful family of techniques for dimensionality reduction and are being increasingly used for compactly encoding large multidimensional arrays, images and other visual data sets. In interactive applications, volume data often needs to be decompressed and manipulated dynamically; when designing data reduction and reconstruction methods, several parameters must be taken into account, such as the achievable compression ratio, approximation error and reconstruction speed. Weighing these variables in an effective way is challenging, and here we present two main contributions to solve this issue for Tucker tensor decompositions. First, we provide algorithms to efficiently compute, store and retrieve good choices of tensor rank selection and decompression parameters in order to optimize memory usage, approximation quality and computational costs. Second, we propose a Tucker compression alternative based on coefficient thresholding and zigzag traversal, followed by logarithmic quantization on both the transformed tensor core and its factor matrices. In terms of approximation accuracy, this approach is theoretically and empirically better than the commonly used tensor rank truncation method.
C1 [Ballester-Ripoll, Rafael; Pajarola, Renato] Univ Zurich, Dept Informat, Visualizat & MultiMedia Lab, Binzmuhlestr 14, CH-8050 Zurich, Switzerland.
C3 University of Zurich
RP Ballester-Ripoll, R (corresponding author), Univ Zurich, Dept Informat, Visualizat & MultiMedia Lab, Binzmuhlestr 14, CH-8050 Zurich, Switzerland.
EM rballester@ifi.uzh.ch; pajarola@acm.org
OI Pajarola, Renato/0000-0002-6724-526X; Ballester-Ripoll,
   Rafael/0000-0001-5831-2056
CR ANDREWS HC, 1976, IEEE T COMMUN, V24, P425, DOI 10.1109/TCOM.1976.1093309
   Bader B.W., 2012, Matlab tensor toolbox version 2.5
   Ballester-Ripoll R, 2015, COMPUT GRAPH-UK, V47, P34, DOI 10.1016/j.cag.2014.10.002
   Bilgili A, 2011, COMPUT GRAPH FORUM, V30, P2427, DOI 10.1111/j.1467-8659.2011.02072.x
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chan TF, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P391, DOI 10.1109/ICIP.2000.899404
   Chen H, 2012, INT GEOSCI REMOTE SE, P4090, DOI 10.1109/IGARSS.2012.6350833
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Hackbusch W., 2012, TENSOR SPACES NUMERI, V42, DOI [10.1007/978-3-642-28027-6, DOI 10.1007/978-3-642-28027-6]
   *ISO IEC, 1994, 1091811994 ISOIEC
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z
   Kurt M., 2013, TPCG, P85, DOI 10.2312/LocalChapterEvents.TPCG.TPCG13.085-092.URL
   Pajarola R., 2013, EUROGRAPHICS 2013 TU
   Qing Wu, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P49
   Rajwade A, 2013, IEEE T PATTERN ANAL, V35, P849, DOI 10.1109/TPAMI.2012.140
   Rovid Andras, 2011, Recent Researches in Artificial Intelligence, Knowledge Engineering and Data Bases. 10th WSEAS International Conference on Artificial Intelligence, Knowledge Engineering and Data Bases (AIKED 2011), P297
   Suter SK, 2013, COMPUT GRAPH FORUM, V32, P151, DOI 10.1111/cgf.12102
   Suter S.K., 2010, Proceedings Vision, Modeling, and Visualization Workshop, P203
   Suter SK, 2011, IEEE T VIS COMPUT GR, V17, P2135, DOI 10.1109/TVCG.2011.214
   Tan HC, 2013, NEUROCOMPUTING, V119, P144, DOI 10.1016/j.neucom.2012.03.039
   Treib M, 2012, IEEE T VIS COMPUT GR, V18, P2169, DOI 10.1109/TVCG.2012.274
   Tsai YT, 2006, ACM T GRAPHIC, V25, P967, DOI 10.1145/1141911.1141981
   Tsai YT, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167077
   Vannieuwenhoven N, 2012, SIAM J SCI COMPUT, V34, pA1027, DOI 10.1137/110836067
   Vasilescu MAO, 2004, ACM T GRAPHIC, V23, P336, DOI 10.1145/1015706.1015725
   Wang HC, 2005, PROC CVPR IEEE, P346
   Wang HC, 2005, ACM T GRAPHIC, V24, P527, DOI 10.1145/1073204.1073224
   Wu Q, 2008, IEEE T VIS COMPUT GR, V14, P186, DOI 10.1109/TVCG.2007.70406
   Wu Q, 2008, IEEE IMAGE PROC, P2828, DOI 10.1109/ICIP.2008.4712383
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zaid AO, 2001, XIV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P164, DOI 10.1109/SIBGRAPI.2001.963051
NR 34
TC 22
Z9 25
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1433
EP 1446
DI 10.1007/s00371-015-1130-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BS
UT WOS:000386397000007
OA Green Published
DA 2024-07-18
ER

PT J
AU Jeong, Y
   Lee, S
   Kwon, S
   Lee, S
AF Jeong, Yuna
   Lee, Sangmin
   Kwon, Soonhyeon
   Lee, Sungkil
TI Expressive chromatic accumulation buffering for defocus blur
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Chromatic aberration; Defocus blur; Spectral rendering; Optical effect;
   Lens system
AB This article presents a novel parametric model to include expressive chromatic aberrations in defocus blur rendering and its effective implementation using the accumulation buffering. Our model modifies the thin-lens model to adopt the axial and lateral chromatic aberrations, which allows us to easily extend them with nonlinear and artistic appearances beyond physical limits. For the dispersion to be continuous, we employ a novel unified 3D sampling scheme, involving both the lens and spectrum. We further propose a spectral equalizer to emphasize particular dispersion ranges. As a consequence, our approach enables more intuitive and explicit control of chromatic aberrations, unlike the previous physically-based rendering methods.
C1 [Jeong, Yuna; Lee, Sangmin; Kwon, Soonhyeon; Lee, Sungkil] Sungkyunkwan Univ, Suwon, South Korea.
C3 Sungkyunkwan University (SKKU)
RP Lee, S (corresponding author), Sungkyunkwan Univ, Suwon, South Korea.
RI LEE, Sungkil/AAJ-8474-2021
OI LEE, Sungkil/0000-0003-0123-9382
CR Akenine-Möller T, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P7
   [Anonymous], 2000, MODERN OPTICAL ENG
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Gotanda Y., 2015, ACM SIGGRAPH 2015 Courses, P23
   Guy S, 2004, ACM T GRAPHIC, V23, P231, DOI 10.1145/1015706.1015708
   Haeberli P., 1990, Computer Graphics, V24, P309, DOI 10.1145/97880.97913
   HALTON JH, 1964, COMMUN ACM, V7, P701, DOI 10.1145/355588.365104
   Hanika J, 2014, COMPUT GRAPH FORUM, V33, P323, DOI 10.1111/cgf.12301
   Hullin M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1965003
   Hullin MB, 2012, COMPUT GRAPH FORUM, V31, P1375, DOI 10.1111/j.1467-8659.2012.03132.x
   Kolb C., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P317, DOI 10.1145/218380.218463
   Kraus M, 2007, COMPUT GRAPH FORUM, V26, P645, DOI 10.1111/j.1467-8659.2007.01088.x
   Lee S., 2009, ACM T GRAPHIC, V28
   Lee S, 2013, COMPUT GRAPH FORUM, V32, P1, DOI 10.1111/cgf.12145
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P453, DOI 10.1109/TVCG.2008.106
   Lee S, 2008, COMPUT GRAPH FORUM, V27, P1955, DOI 10.1111/j.1467-8659.2008.01344.x
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   McGraw T, 2015, VISUAL COMPUT, V31, P601, DOI 10.1007/s00371-014-0986-6
   Polyanskiy M.N., 2016, Refractive index database
   Potmesil M., 1981, Computer Graphics, V15, P297, DOI 10.1145/965161.806818
   Rokita P, 1996, IEEE COMPUT GRAPH, V16, P18, DOI 10.1109/38.486676
   Schedl D.C., 2012, J WSCG, V20, P239
   Sellmeier W., 1871, Ann. Phys. Chem, V219, P272, DOI [10.1002/andp.18712190612, DOI 10.1002/ANDP.18712190612]
   Smith T., 1932, Transactions of the Optical Society, V33, P73
   Steinert B, 2011, COMPUT GRAPH FORUM, V30, P1643, DOI 10.1111/j.1467-8659.2011.01851.x
   Thomas S.W., 1986, VISUAL COMPUT, V2, P3
   Wong T-T, 1997, J GRAPHICS TOOLS, V2, P9, DOI DOI 10.1080/10867651.1997.10487471
   Wu JZ, 2013, VISUAL COMPUT, V29, P41, DOI 10.1007/s00371-012-0673-4
   Wu JZ, 2010, VISUAL COMPUT, V26, P555, DOI 10.1007/s00371-010-0459-5
   Zernike F., 2006, APPL NONLINEAR OPTIC
NR 30
TC 1
Z9 2
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 1025
EP 1034
DI 10.1007/s00371-016-1244-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600033
DA 2024-07-18
ER

PT J
AU Ye, JB
   Yu, YZ
AF Ye, Jianbo
   Yu, Yizhou
TI A fast modal space transform for robust nonrigid shape retrieval
SO VISUAL COMPUTER
LA English
DT Article
DE Content-based object retrieval; Shape retrieval; Biharmonic distance;
   Functional map; Shape signature
ID DESCRIPTORS; EIGENVALUES; SURFACES; FEATURES
AB Nonrigid or deformable 3D objects are common in many application domains. Retrieval of such objects in large databases based on shape similarity is still a challenging problem. In this paper, we take advantages of functional operators as characterizations of shape deformation, and further propose a framework to design novel shape signatures for encoding nonrigid geometries. Our approach constructs a context-aware integral kernel operator on a manifold, then applies modal analysis to map this operator into a low-frequency functional representation, called fast functional transform, and finally computes its spectrum as the shape signature. In a nutshell, our method is fast, isometry-invariant, discriminative, smooth and numerically stable with respect to multiple types of perturbations. Experimental results demonstrate that our new shape signature for nonrigid objects can outperform all methods participating in the nonrigid track of the SHREC'11 contest. It is also the second best performing method in the real human model track of SHREC'14.
C1 [Ye, Jianbo] Penn State Univ, State Coll, PA USA.
   [Yu, Yizhou] Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Zhejiang University
RP Yu, YZ (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
EM yizhouy@acm.org
RI Ye, Jianbo/Q-6990-2017
OI Ye, Jianbo/0000-0003-4612-6429
CR Aflalo Y, 2013, P NATL ACAD SCI USA, V110, P18052, DOI 10.1073/pnas.1308708110
   Agathos A, 2010, VISUAL COMPUT, V26, P1301, DOI 10.1007/s00371-010-0523-1
   Alfakih AY, 2008, COMPUT APPL MATH, V27, P237, DOI 10.1590/S1807-03022008000300001
   [Anonymous], 1995, PERTURBATION THEORY
   ARNOLDI WE, 1951, Q APPL MATH, V9, P17, DOI 10.1090/qam/42792
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Aubin T., 1998, Some Nonlinear Problems in Riemannian Geometry
   Bogomolny E, 2003, J PHYS A-MATH GEN, V36, P3595, DOI 10.1088/0305-4470/36/12/341
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2011, IEEE T PATTERN ANAL, V33, P1065, DOI 10.1109/TPAMI.2010.210
   Buser P, 1994, Int. Math. Res. Not., P391
   Castro MH, 2012, MATH COMPUT, V81, P2303, DOI 10.1090/S0025-5718-2012-02595-6
   Cignoni P, 2008, ERCIM NEWS, P45
   Elad A, 2001, PROC CVPR IEEE, P168
   Friedrichs K.O., 1965, PERTURBATION SPECTRA, VIII
   Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x
   GORDON C, 1992, INVENT MATH, V110, P1, DOI 10.1007/BF01231320
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Kurtek S, 2013, COMPUT GRAPH FORUM, V32, P429, DOI 10.1111/cgf.12063
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Li CY, 2013, INT J MULTIMED INF R, V2, P261, DOI 10.1007/s13735-013-0041-9
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Lidskii V., 1966, USSR COMP MATH MATH, V6, P73, DOI DOI 10.1016/0041-5553(66)90033-4
   Lipman Y., 2007, ACM T GRAPHICS
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maes C., 2010, BIOMETRICS THEORY AP, DOI [DOI 10.1109/BTAS.2010.5634543, 10.1109/BTAS.2010.5634543]
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pickup David., 2014, PROC 3DOR, V4, P8
   Quarteroni A, 2007, NUMERICAL MATH, V37
   Raviv D, 2015, INT J COMPUT VISION, V111, P1, DOI 10.1007/s11263-014-0728-2
   READE JB, 1983, SIAM J MATH ANAL, V14, P152, DOI 10.1137/0514012
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Smeets D, 2009, LECT NOTES COMPUT SC, V5702, P757, DOI 10.1007/978-3-642-03767-2_92
   Stewart G. W., 2001, SIAM Journal on Matrix Analysis and Applications, V23, P601, DOI 10.1137/S0895479800371529
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   SUNADA T, 1985, ANN MATH, V121, P169, DOI 10.2307/1971195
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Tabia H, 2011, IEEE T PATTERN ANAL, V33, P852, DOI 10.1109/TPAMI.2010.202
   Thakoor N, 2007, IEEE T IMAGE PROCESS, V16, P2707, DOI 10.1109/TIP.2007.908076
   Weyl Wey11 Hermann, 1911, NACHR KONIGL GES WIS, V1911, P110
   Ye J., 2013, Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, P121
   Ying X, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508379
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
   Zienkiewicz OC, 2005, FINITE ELEMENT METHOD FOR FLUID DYNAMICS, 6TH EDITION, P1
NR 50
TC 21
Z9 21
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 553
EP 568
DI 10.1007/s00371-015-1071-5
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800002
DA 2024-07-18
ER

PT J
AU Zhang, YZ
   Zheng, JM
   Magnenat-Thalmann, N
AF Zhang, Yuzhe
   Zheng, Jianmin
   Magnenat-Thalmann, Nadia
TI Example-guided anthropometric human body modeling
SO VISUAL COMPUTER
LA English
DT Article
DE Human body modeling; Anthropometry; Examples; Correlation analysis; RBF
   interpolation; Constrained optimization
ID PARAMETERIZATION
AB This paper presents an example-guided, anthro-pometry-based modeling method for creating 3D human body models from users' input of partial anthropometric measurements with a given example dataset. Rather than directly forming a mapping between the partial measurements and the body model, we first estimate a set of chosen 30 measurements from the input based on the example-oriented measurement analysis. We then create an initial 3D model using the example-oriented radial basis function model that maps the set of 30 measurements to the body shape space and is established based on the given examples. We finally refine the 3D model by constrained optimization to create the target body model. Our method has several advantages: (1) the created model is guaranteed to match the input measurements and reflects the shape characteristics of examples; (2) the input requirement is modest, which makes it useful in practice; and (3) the information of both the measurements and examples is fully utilized. We demonstrate the effectiveness, accuracy, flexibility and extensibility of the method by various experimental evaluations and a Kinect-based body customization application.
C1 [Zhang, Yuzhe; Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Magnenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zheng, JM (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM YZHANG3@e.ntu.edu.sg; asjmzheng@ntu.edu.sg; NADIATHALMANN@ntu.edu.sg
RI Thalmann, Nadia/AAK-5195-2021; Zheng, Jianmin/A-3717-2011
OI Thalmann, Nadia/0000-0002-1459-5960; Zheng, Jianmin/0000-0002-5062-6226
FU MOE Tier-2 grant of Singapore [MOE2011-T2-2-041]; Institute for Media
   Innovation, Nanyang Technological University
FX Yuzhe Zhang receives the PhD scholarship awarded by the Institute for
   Media Innovation, Nanyang Technological University. Jianmin Zheng is
   partially supported by MOE Tier-2 grant (MOE2011-T2-2-041) of Singapore.
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 1996, TECHNICAL REPORT
   Baek SY, 2012, COMPUT AIDED DESIGN, V44, P56, DOI 10.1016/j.cad.2010.12.006
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Chen YP, 2013, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2013.21
   Chu CH, 2010, COMPUT IND, V61, P541, DOI 10.1016/j.compind.2010.03.004
   Deun D.V., 2011, AUTOMATIC GENERATION
   Grosso M. R., 1989, State-of-the-Art in Computer Animation. Proceedings of Computer Animation '89, P83
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Hsu C.-H., 2009, HUM FACTORS MAN, V19
   Izadi S., 2011, KINECTFUSION REAL TI, P23
   Kasap M, 2011, VISUAL COMPUT, V27, P263, DOI 10.1007/s00371-011-0547-1
   Kasap M, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P368, DOI 10.1109/CW.2010.55
   Li J., 2007, Proceedings of computer graphics international, V7, P151
   Lipman Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P181, DOI 10.1109/SMI.2004.1314505
   MagnenatThalmann N, 2010, MODELING AND SIMULATING BODIES AND GARMENTS, P1
   Meyer M, 2002, INT WORKSH VIS MATH
   Nealen A, 2005, ACM T GRAPHIC, V24, P1142, DOI 10.1145/1073204.1073324
   Richter M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P340, DOI 10.1109/3DIMPVT.2012.81
   Seo H, 2004, GRAPH MODELS, V66, P1, DOI 10.1016/j.gmod.2003.07.004
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Wang CCL, 2005, COMPUT AIDED DESIGN, V37, P83, DOI 10.1016/j.cad.2004.05.001
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
NR 25
TC 16
Z9 22
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2015
VL 31
IS 12
BP 1615
EP 1631
DI 10.1007/s00371-014-1043-1
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CV2ZD
UT WOS:000364126900004
DA 2024-07-18
ER

PT J
AU Guo, SH
   Southern, R
   Chang, J
   Greer, D
   Zhang, JJ
AF Guo, Shihui
   Southern, Richard
   Chang, Jian
   Greer, David
   Zhang, Jian Jun
TI Adaptive motion synthesis for virtual characters: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Computer animation; Character motion synthesis
ID OF-THE-ART; BIPEDAL LOCOMOTION; PHYSICS; OPTIMIZATION; CONTROLLERS;
   REPRESENTATION
AB Character motion synthesis is the process of artificially generating natural motion for a virtual character. In film, motion synthesis can be used to generate difficult or dangerous stunts without putting performers at risk. In computer games and virtual reality, motion synthesis enriches the player or participant experience by allowing for unscripted and emergent character behavior. In each of these applications the ability to adapt to changes to environmental conditions or to the character in a smooth and natural manner, while still conforming with user-specified constraints, determines the utility of a method to animators and industry practitioners. This focus on adaptation capability distinguishes our survey from other reviews which focus on general technology developments. Three main methodologies (example-based; simulation-based and hybrid) are summarised and evaluated using compound metrics: adaptivity, naturalness and controllability. By assessing existing techniques according to this classification we are able to determine how well a method corresponds to users' expectations. We discuss optimization strategies commonly used in motion synthesis literature, and also contemporary perspectives from biology which give us a deeper insight into this problem. We also present observations and reflections from industry practitioners to reveal the operational constraints of character motion synthesis techniques. Our discussion and review presents a unique insight into the subject, and provide essential guidance when selecting appropriate methods to design an adaptive motion controller.
C1 [Guo, Shihui; Southern, Richard; Chang, Jian; Greer, David; Zhang, Jian Jun] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Guo, SH (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth BH12 5BB, Dorset, England.
EM sguo@bournemouth.ac.uk
OI Chang, Jian/0000-0003-4118-147X; Zhang, Jian/0000-0002-7069-5771;
   Southern, Richard/0000-0002-1933-3951
FU China Scholarship Council; Bournemouth University; People Programme
   (Marie Curie Actions) of the European Union's Seventh Framework
   Programme FP7/under REA [612627-"AniNex"]
FX The authors would like to thank the helpful comments from reviewers.
   Author Shihui Guo is sponsored by both China Scholarship Council and
   Bournemouth University. The research leading to these results has
   received funding from the People Programme (Marie Curie Actions) of the
   European Union's Seventh Framework Programme FP7/2007-2013/under REA
   grant agreement No. 612627-"AniNex".
CR Abe Y, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P249
   Adam A., 2008, IDENTIFYING HUMANS T
   Al Borno M, 2013, IEEE T VIS COMPUT GR, V19, P1405, DOI 10.1109/TVCG.2012.325
   Al-Asqhar Rami Ali, 2013, P 12 ACM SIGGRAPHEUR, P45
   Alexander R. M., 1996, Optima for animals
   [Anonymous], 2013, P 12 ACM SIGGRAPHEUR, DOI DOI 10.1145/2485895.2485903
   [Anonymous], ACM SIGGRAPH 2011 PA
   [Anonymous], ACM T GRAPH
   [Anonymous], 2010, Animating non-humanoid characters with human motion data
   [Anonymous], 2008, NEUROPHYSIOLOGICAL B
   Autodesk, 2008, NEW ART VIRT MOV
   Bai YF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366175
   Carvalho SR, 2013, VISUAL COMPUT, V29, P171, DOI 10.1007/s00371-012-0678-z
   Chai JX, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239459
   Choi MG, 2011, COMPUT GRAPH FORUM, V30, P445, DOI 10.1111/j.1467-8659.2011.01889.x
   Coros S., 2009, ACM SIGGRAPH AS 2009
   Coros S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964954
   Coros S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781156
   Coros S, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409066
   da Silva M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531388
   de Lasa M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781157
   Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287
   Fang AC, 2003, ACM T GRAPHIC, V22, P417, DOI 10.1145/882262.882286
   FELDMAN AG, 1986, J MOTOR BEHAV, V18, P17
   Feng AndrewW., 2012, I3D, P95, DOI [DOI 10.1145/2159616.2159632, 10.1145/2159616.2159632]
   Geijtenbeek T, 2012, COMPUT GRAPH FORUM, V31, P2492, DOI 10.1111/j.1467-8659.2012.03189.x
   Geijtenbeek T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508399
   Ghahramani Z, 2001, INT J PATTERN RECOGN, V15, P9, DOI 10.1142/S0218001401000836
   Gleicher M., 1998, P 25 ANN C COMPUTER, P42
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Guo SH, 2014, COMPUT GRAPH-UK, V38, P78, DOI 10.1016/j.cag.2013.10.021
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Hertzmann A, 2011, IEEE COMPUT GRAPH, V31, P20, DOI 10.1109/MCG.2011.61
   Herzeg I, 2011, GAM DEV C
   Ho ESL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778770
   Ikemoto Leslie, 2006, P 2006 S INT 3D GRAP, P49
   Jain SK, 2011, B MATH SCI, V1, P1, DOI 10.1007/s13373-011-0012-5
   Jain S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477936
   Jain Sumit., 2009, Proceedings of SCA 2009, P47
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kulpa R, 2005, COMPUT GRAPH FORUM, V24, P343, DOI 10.1111/j.1467-8659.2005.00859.x
   Kwang-Jin Choi, 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P32, DOI 10.1109/PCCGA.1999.803346
   Kwon T, 2008, IEEE T VIS COMPUT GR, V14, P707, DOI 10.1109/TVCG.2008.22
   Lau M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618517
   Lee J., 1999, P 26 ANN C COMP GRAP, P48
   Lee Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866160
   Liu CK, 2002, ACM T GRAPHIC, V21, P408, DOI 10.1145/566570.566596
   Liu FD, 2013, IEEE T CYBERNETICS, V43, P1131, DOI 10.1109/TSMCB.2012.2224920
   Liu LB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366173
   Ma W., 2010, P ACM SIGGRAPH EUR S, P21
   Macchietto A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531386
   McIntyre J, 2001, NAT NEUROSCI, V4, P693, DOI 10.1038/89477
   Min JY, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640452
   Monzani JS, 2000, COMPUT GRAPH FORUM, V19, pC11, DOI 10.1111/1467-8659.00393
   Mordatch I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778808
   Muico U, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966395
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   Pejsa T, 2010, COMPUT GRAPH FORUM, V29, P202, DOI 10.1111/j.1467-8659.2009.01591.x
   Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536
   Pratt J, 2001, INT J ROBOT RES, V20, P129, DOI 10.1177/02783640122067309
   Pronost N., 2009, INT J, V100, P1
   Qinxin Y., 2007, THESIS U TORONTO ONT
   Rother D.D., 2008, THESIS U MINNESOTA M
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   SHADMEHR R, 1994, J NEUROSCI, V14, P3208
   Sharon D, 2005, IEEE INT CONF ROBOT, P2387, DOI 10.1109/robot.2005.1570470
   Shin HJ, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P194
   Shin HJ, 2001, ACM T GRAPHIC, V20, P67, DOI 10.1145/502122.502123
   Shiratori T., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P37
   Shum H.P.H., 2010, IEEE T VIS COMPUT GR, V99
   Sok K.W., 2007, ACM SIGGRAPH 2007 SI
   Sok KwangWon., 2010, P 2010 ACM SIGGRAPHE, P11
   Spong MW, 2005, IEEE T AUTOMAT CONTR, V50, P1025, DOI 10.1109/TAC.2005.851449
   Sun HC, 2001, COMP GRAPH, P261, DOI 10.1145/383259.383288
   TAGA G, 1991, BIOL CYBERN, V65, P147, DOI 10.1007/BF00198086
   Tak S, 2005, ACM T GRAPHIC, V24, P98, DOI 10.1145/1037957.1037963
   Takahashi CD, 2001, J NEUROPHYSIOL, V86, P1047, DOI 10.1152/jn.2001.86.2.1047
   Tsai YY, 2010, IEEE T VIS COMPUT GR, V16, P325, DOI 10.1109/TVCG.2009.76
   Wampler K, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805970
   Wampler K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531366
   Wang J.M., 2009, ACM SIGGRAPH AS
   Wang JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778810
   Wang JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185521
   Wei XL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966398
   Williamson M. M., 1999, THESIS MIT
   Witkin A., 1988, Computer Graphics, V22, P159, DOI 10.1145/378456.378507
   Wu Chun-Chih, 2010, P 2010 ACM SIGGRAPH, P113
   Wu JC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778809
   Yamane K., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P688, DOI 10.1109/ROBOT.2000.844132
   Ye Y., 2008, ACM SIGGRAPH AS 2008
   Ye Y, 2012, CHIN J NAT MEDICINES, V10, P1, DOI [10.3724/SP.J.1009.2012.00001, 10.1016/S1875-5364(12)60001-6]
   Ye YT, 2010, COMPUT GRAPH FORUM, V29, P555, DOI 10.1111/j.1467-8659.2009.01625.x
   Yin K., 2007, ACM SIGGRAPH 2007 SI
   Zehr EP, 1999, PROG NEUROBIOL, V58, P185, DOI 10.1016/S0301-0082(98)00081-1
   Zordan J. K., 2002, P ACM SIGGRAPH EUR S, P89
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
   Zordan V, 2007, SANDBOX SYMPOSIUM 2007: ACM SIGGRAPH VIDEO GAME SYMPOSIUM, PROCEEDINGS, P9
NR 97
TC 19
Z9 24
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 497
EP 512
DI 10.1007/s00371-014-0943-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400001
DA 2024-07-18
ER

PT J
AU Shen, EY
   Xia, JZ
   Cheng, ZQ
   Martin, RR
   Wang, YH
   Li, SK
AF Shen, Enya
   Xia, Jiazhi
   Cheng, Zhiquan
   Martin, Ralph R.
   Wang, Yunhai
   Li, Sikun
TI Model-driven multicomponent volume exploration
SO VISUAL COMPUTER
LA English
DT Article
DE Interactive volume visualization; Volume segmentation; Volume
   correspondence; Knowledge-assisted visualization
ID KNOWLEDGE; SEGMENTATION
AB The current multicomponent volume segmentation and labeling methods are mostly hard to get correct segmentation and labeling results automatically and rely hardly on experts' aids, which make related volume exploration to be time consuming, laborious and prone to errors and omissions. To solve this problem, we present a novel volume exploration method driven by admitted model. We first apply Gaussian mixture models to segment the raw volume. However, different components with similar value are still mixed. To segment these components further, we make use of region-grown principle to produce a fine-grained part segmentation. To label different parts automatically, we found that it is helpful to take advantage of annotated model, like human anatomy model (PlasticboyCC, http://www.plasticboy.co.uk/store/index.html, 2013). However, it is not straightforward to label segmented volume with geometric model automatically. Inspired by electors voting (Au et al., Comput Graph Forum 29:645-654, 2010), we propose a volume-model correspondence schema to overcome this intractable challenge. Moreover, it is essential to exploit intuitive interactive methods for interactive exploration, so we also developed practical precise interaction techniques to assist volume exploration. Our experiments with various data and discussion with specialists show that our method provides an efficient and impactful way to explore volume data.
C1 [Shen, Enya; Cheng, Zhiquan; Li, Sikun] Natl Univ Def Technol, Sch Comp, Changsha, Hunan, Peoples R China.
   [Xia, Jiazhi] Cent S Univ, Sch Informat Sci & Engn, Changsha, Hunan, Peoples R China.
   [Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AX, S Glam, Wales.
   [Wang, Yunhai] Shenzhen VisuCA Key Lab SIAT, Shenzhen, Peoples R China.
C3 National University of Defense Technology - China; Central South
   University; Cardiff University
RP Shen, EY (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha, Hunan, Peoples R China.
EM shenenya.nudt@gmail.com; xiajiazhi@gmail.com; cheng.zhiquan@gmail.com;
   ralph@cs.cf.ac.uk; cloudseawang@gmail.com; Lisikun@263.net.cn
RI Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
FU National Natural Science Foundation of China [61170157]; National Grand
   Fundamental Research 973 Program of China [G2009CB72380]
FX The authors would like to thank anonymous reviewers at TVCJ for their
   comments that helped us to improve the quality of this manuscript. The
   authors would also like to thank J.Y. Huang for checking reading of this
   manuscript. This research is supported by the National Natural Science
   Foundation of China under Grant No. 61170157, and the National Grand
   Fundamental Research 973 Program of China under Grant No. G2009CB72380.
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], 2005, PROC S INTERACTIVE 3
   Attene M, 2009, COMPUT AIDED DESIGN, V41, P756, DOI 10.1016/j.cad.2009.01.003
   Au OKC, 2010, COMPUT GRAPH FORUM, V29, P645, DOI 10.1111/j.1467-8659.2009.01634.x
   Bourguignon D, 2001, COMPUT GRAPH FORUM, V20, pC114, DOI 10.1111/1467-8659.00504
   Bruckner S, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P671
   Cabezas M, 2011, COMPUT METH PROG BIO, V104, pE158, DOI 10.1016/j.cmpb.2011.07.015
   Chan MY, 2008, IEEE T VIS COMPUT GR, V14, P1683, DOI 10.1109/TVCG.2008.159
   Chen H.-L.Jason., 2006, 3 EUROGRAPHICS WORKS, P123, DOI DOI 10.2312/SBM/SBM06/123-129
   Chen M, 2009, IEEE COMPUT GRAPH, V29, P12, DOI 10.1109/MCG.2009.6
   Correa C.D., 2009, ACM SIGGRAPH COMPUT, V43, P5
   Correa CD, 2008, IEEE T VIS COMPUT GR, V14, P1380, DOI 10.1109/TVCG.2008.162
   Correa CD, 2011, IEEE T VIS COMPUT GR, V17, P192, DOI 10.1109/TVCG.2010.35
   Drebin R. A., 1988, Computer Graphics, V22, P65, DOI 10.1145/378456.378484
   Engel K, 2006, Real-Time Volume Graphics
   Fisher M., 2011, ACM SIGGRAPH 2011
   Friese KI, 2011, VISUAL COMPUT, V27, P129, DOI 10.1007/s00371-010-0539-6
   Gerl M, 2012, COMPUT GRAPH-UK, V36, P201, DOI 10.1016/j.cag.2011.10.006
   Guo HQ, 2011, IEEE T VIS COMPUT GR, V17, P2106, DOI 10.1109/TVCG.2011.261
   Hohne K., 2003, REGIONAL SYSTEMIC RA
   Jung Y, 2013, VISUAL COMPUT, V29, P805, DOI 10.1007/s00371-013-0833-1
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Li W, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239482
   Liu WY, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P326
   Muoz-Moreno E., 2013, PLOS ONE, V8
   Nam JE, 2009, COMPUT GRAPH-UK, V33, P607, DOI 10.1016/j.cag.2009.06.006
   Pagare R., 2012, International Journal of Computer Applications, V37, P42, DOI DOI 10.5120/4616-6295
   Papaleo L, 2009, LECT NOTES COMPUT SC, V5716, P103, DOI 10.1007/978-3-642-04146-4_13
   Paraboschi L., 2007, Eurographics Italian Chapter, Trento (Italy), P87
   PlasticboyCC, 2013, PLAST AN MOD STOR
   Prassni JS, 2010, IEEE PAC VIS SYMP, P9, DOI 10.1109/PACIFICVIS.2010.5429624
   Rautek P, 2008, COMPUT GRAPH FORUM, V27, P847, DOI 10.1111/j.1467-8659.2008.01216.x
   Ruiz M, 2011, IEEE T VIS COMPUT GR, V17, P1932, DOI 10.1109/TVCG.2011.173
   Salama CR, 2006, IEEE T VIS COMPUT GR, V12, P1021, DOI 10.1109/TVCG.2006.148
   Schiemann T, 1997, Med Image Anal, V1, P263, DOI 10.1016/S1361-8415(97)85001-3
   Shen E., 2012, COMP VIS MED C, P250
   Super BJ, 2007, PATTERN RECOGN, V40, P2818, DOI 10.1016/j.patcog.2006.12.029
   Tzeng FY, 2005, IEEE T VIS COMPUT GR, V11, P273, DOI 10.1109/TVCG.2005.38
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P553, DOI 10.1111/j.1467-8659.2011.01893.x
   Verbeek JJ, 2003, NEURAL COMPUT, V15, P469, DOI 10.1162/089976603762553004
   Wang YH, 2011, IEEE T VIS COMPUT GR, V17, P1560, DOI 10.1109/TVCG.2011.97
   Yousefi S, 2012, IEEE T BIO-MED ENG, V59, P1808, DOI 10.1109/TBME.2011.2122306
   Yuan XR, 2005, VISUAL COMPUT, V21, P745, DOI 10.1007/s00371-005-0330-2
   Zhou JL, 2009, IEEE T VIS COMPUT GR, V15, P1481, DOI 10.1109/TVCG.2009.120
NR 46
TC 5
Z9 8
U1 3
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 441
EP 454
DI 10.1007/s00371-014-0940-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600006
DA 2024-07-18
ER

PT J
AU Zhu, XQ
   Jin, XG
   You, LH
AF Zhu, Xiaoqiang
   Jin, Xiaogang
   You, Lihua
TI High-quality tree structures modelling using local convolution surface
   approximation
SO VISUAL COMPUTER
LA English
DT Article
DE Tree modelling; Convolution surfaces; CUDA; Subdivision surfaces;
   Quadrilateral meshes
ID DESIGN; SILHOUETTES
AB In this paper, we propose a local convolution surface approximation approach for quickly modelling tree structures with pleasing visual effect. Using our proposed local convolution surface approximation, we present a tree modelling scheme to create the structure of a tree with a single high-quality quad-only mesh. Through combining the strengths of the convolution surfaces, subdivision surfaces and GPU, our tree modelling approach achieves high efficiency and good mesh quality. With our method, we first extract the line skeletons of given tree models by contracting the meshes with the Laplace operator. Then we approximate the original tree mesh with a convolution surface based on the extracted skeletons. Next, we tessellate the tree trunks represented by convolution surfaces into quad-only subdivision surfaces with good edge flow along the skeletal directions. We implement the most time-consuming subdivision and convolution approximation on the GPU with CUDA, and demonstrate applications of our proposed approach in branch editing and tree composition.
C1 [Zhu, Xiaoqiang; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Zhu, Xiaoqiang] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [You, Lihua] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, Dorset, England.
C3 Zhejiang University; Shanghai University; Bournemouth University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM zhuxiaoqiang@zjucadcg.cn; jin@cad.zju.edu.cn; LYou@bournemouth.ac.uk
FU National Natural Science Foundation of China [61272298, 61373084];
   Zhejiang Provincial Natural Science Foundation of China [Z1110154];
   China 863 program [2012AA011503, 2013AA01A603]; Major Science and
   Technology Innovation Team [2010R50040]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. 61272298 and 61373084), Zhejiang Provincial Natural
   Science Foundation of China (Grant No. Z1110154), the China 863 program
   (Grant Nos. 2012AA011503 and 2013AA01A603), and the Major Science and
   Technology Innovation Team (Grant no. 2010R50040).
CR Akkouche S, 2001, COMPUT GRAPH FORUM, V20, P67, DOI 10.1111/1467-8659.00479
   ALEXE A., 2004, P AFRIGRAPH, P25
   Alexe Anca., 2005, Pacific Graphics, Short paper
   Angelidis A, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P137
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], 2005, GPU GEMS
   [Anonymous], 2010, ACM SIGGRAPH 2010 TA
   [Anonymous], IRIT200517R
   [Anonymous], P EINDH IMPL SURF WO
   [Anonymous], ACM SIGGRAPH 2008 SI
   Bernhardt A., 2008, EUROGRAPHICS WORKSH, P57
   BLOOMENTHAL J, 1991, COMP GRAPH, V25, P251, DOI 10.1145/127719.122757
   BLOOMENTHAL J, 1994, GRAPHICS GEMS, V4, P324
   Bucksch A., 2009, Eurographics Workshop on 3D Object Retrieval, P13, DOI DOI 10.2312/3DOR/3DOR09/013-020
   Cao J., 2010, SHAP MOD INT C SMI 2, P187, DOI [DOI 10.1109/SMI.2010.25, 10.1109/SMI.2010.25]
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chen XJ, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1356682.1356684
   Deussen O, 2005, Digital Design of Nature: Computer Generated Plants and Organics
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Fabri A, 2000, SOFTWARE PRACT EXPER, V30, P1167, DOI 10.1002/1097-024X(200009)30:11<1167::AID-SPE337>3.0.CO;2-B
   Galbraitht C, 2004, COMPUT GRAPH FORUM, V23, P351, DOI 10.1111/j.1467-8659.2004.00766.x
   Gourmel O, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451238
   Hart J.C., 1996, P IMPLICIT SURFACES, P143
   Hubert E, 2012, GRAPH MODELS, V74, P1, DOI 10.1016/j.gmod.2011.07.001
   Hubert E, 2012, J SYMB COMPUT, V47, P680, DOI 10.1016/j.jsc.2011.12.026
   Ji ZP, 2010, COMPUT GRAPH FORUM, V29, P2169, DOI 10.1111/j.1467-8659.2010.01805.x
   Jin XG, 2002, VISUAL COMPUT, V18, P530, DOI 10.1007/s00371-002-0161-3
   Jin XG, 2002, COMPUT GRAPH-UK, V26, P437, DOI 10.1016/S0097-8493(02)00087-0
   Jin XG, 2009, VISUAL COMPUT, V25, P279, DOI 10.1007/s00371-008-0267-3
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Lin JC, 2010, VISUAL COMPUT, V26, P1017, DOI 10.1007/s00371-010-0460-z
   LINDENMAYER A, 1968, J THEOR BIOL, V18, P300, DOI 10.1016/0022-5193(68)90080-5
   Lluch J, 2004, GRAPH MODELS, V66, P89, DOI 10.1016/j.gmod.2004.01.002
   Longay S., 2012, The proceedings of the Eurographics Symposium on Sketch-Based Interfaces and Modeling, P107, DOI [DOI 10.2312/SBM/SBM12/107-120, 10.2312/SBM/SBM12/107-120]
   Loop C, 1987, THESIS U UTAH
   Loop C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618497
   Marechal Nicolas, 2010, P GRAPHICS INTERFACE, P217
   McCormack J, 1998, COMPUT GRAPH FORUM, V17, P113, DOI 10.1111/1467-8659.00232
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Okabe M, 2005, COMPUT GRAPH FORUM, V24, P487, DOI 10.1111/j.1467-8659.2005.00874.x
   Palubicki W., 2009, ACM SIGGRAPH 2009 SI, P1
   Patney A., 2009, Proceedings of the Conference on High Performance Graphics 2009, P99
   Pirk S., 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185546
   Reche A, 2004, ACM T GRAPHIC, V23, P720, DOI 10.1145/1015706.1015785
   Schwarz M, 2009, COMPUT GRAPH FORUM, V28, P365, DOI 10.1111/j.1467-8659.2009.01376.x
   Sherstyuk A, 1999, VISUAL COMPUT, V15, P171, DOI 10.1007/s003710050170
   Sovakar A, 2004, COMPUT GRAPH-UK, V28, P67, DOI 10.1016/j.cag.2003.10.005
   Tai CL, 2004, COMPUT GRAPH FORUM, V23, P71, DOI 10.1111/j.1467-8659.2004.00006.x
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Tan P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409061
   Tan P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239538
   Vaillant R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461960
   van Overveld K, 2004, VISUAL COMPUT, V20, P362, DOI 10.1007/s00371-002-0197-4
   Wither J, 2009, COMPUT GRAPH FORUM, V28, P541, DOI 10.1111/j.1467-8659.2009.01394.x
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
   Xia J., 2011, S INTERACTIVE 3D GRA, P151
   Xu H, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289610
   Zanni C, 2013, COMPUT GRAPH FORUM, V32, P219, DOI 10.1111/cgf.12199
   Zhu XQ, 2012, VISUAL COMPUT, V28, P1115, DOI 10.1007/s00371-011-0662-z
NR 59
TC 14
Z9 17
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2015
VL 31
IS 1
BP 69
EP 82
DI 10.1007/s00371-013-0905-2
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AY8ZQ
UT WOS:000347839600006
DA 2024-07-18
ER

PT J
AU Jiang, S
   Sajadi, B
   Ihler, A
   Gopi, M
AF Jiang, Shan
   Sajadi, Behzad
   Ihler, Alexander
   Gopi, M.
TI Optimizing redundant-data clustering for interactive walkthrough
   applications
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Out-of-core rendering; Data layout; Secondary storage devices; Redundant
   data; Linear programming; Walkthrough rendering; Seek time; Transfer
   time
AB In modern walkthrough applications, storing massive datasets has become easy and inexpensive due to the availability of gigantic disk-based storage devices including hard drives, DVDs, and Blu-ray discs. However, fetching data from these devices for processing and rendering in interactive environments remains a bottleneck as data transfer speed has not kept pace with the sizes of both the secondary storage and main memory.Out-of-core algorithms are commonly used as a solution to transfer data efficiently from the secondary storage to main memory. Existing algorithms strongly rely on suitable data layout algorithms to reduce the data fetch time. However, in spite of all commonly used techniques, the total time required to seek and transfer data can still easily exceed the budget for total data fetch time. In this work, we propose an orthogonal approach to aggregate data and store them redundantly in multiple places in the storage device to ensure consistent data fetching performance. We pose this as a linear integer programming problem to minimize the amount of redundancy subject to the fetch time budget constraint. We provide an implementation on datasets with hundreds of millions of triangles to demonstrate how this data clustering can be created in practice and how the optimal solution is found.
C1 [Jiang, Shan; Sajadi, Behzad; Ihler, Alexander; Gopi, M.] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Jiang, S (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.
EM sjiang1714@gmail.com; bsajadi@uci.edu; ihler@ics.uci.edu;
   gopi@ics.uci.edu
OI Ihler, Alexander/0000-0002-4331-1015
CR Aliaga D., 1999, Proceedings 1999 Symposium on Interactive 3D Graphics, P199, DOI 10.1145/300523.300554
   [Anonymous], 2003, Level of detail for 3D graphics
   Bittner J, 2004, COMPUT GRAPH FORUM, V23, P615, DOI 10.1111/j.1467-8659.2004.00793.x
   Cignoni P., 2004, ACM T GRAPHIC, V23, P3
   Corrêa WT, 2003, PVG 2003 PROCEEDINGS, P1
   Diaz-Gutierrez P, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P115
   Diaz-Gutierrez P, 2006, VISUAL COMPUT, V22, P372, DOI 10.1007/s00371-006-0018-2
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   Eilemann S., 2007, IFI200706
   Fernando R., 2004, GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics
   Gobbetti E, 2005, ACM T GRAPHIC, V24, P878, DOI 10.1145/1073204.1073277
   Hoppe H, 1999, COMP GRAPH, P269, DOI 10.1145/311535.311565
   IBM ILOG CPLEX, 2011, ILOG CPLEX OPT STUD
   Isenburg M, 2005, IEEE Visualization 2005, Proceedings, P231
   Isenburg M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P465, DOI 10.1109/VISUAL.2003.1250408
   Jiang S., 2013, SIBGRAPI 13 IEEE
   Lindstrom P, 2001, IEEE VISUAL, P363, DOI 10.1109/VISUAL.2001.964533
   Losasso F., 2004, ACM SIGGRAPH, P269
   Luebke D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P199, DOI 10.1145/258734.258847
   Otaduy M. A., 2003, Symposium on Geometry Processing, P94
   Pascucci Valerio., 2001, Proceedings of the 2001 ACM/IEEE conference on Supercom- puting (CDROM), Supercomputing '01, P2
   Peng C, 2012, COMPUT GRAPH FORUM, V31, P393, DOI 10.1111/j.1467-8659.2012.03018.x
   Sagan H., 1994, SPACE FILLING CURVES
   Sajadi B., 2009, I3D '09: Proceedings of the 2009 symposium on Interactive 3D graphics and games, P23
   Sajadi B., 2011, Symposium on Interactive 3D Graphics and Games, P175, DOI DOI 10.1145/1944745.1944775
   Sander PV, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239540, 10.1145/1276377.1276489]
   Varadhan G, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P69, DOI 10.1109/VISUAL.2002.1183759
   Velho Luiz, 1991, P 18 ANN C COMP GRAP, P81, DOI 10.1145/122718.122727
   Yoon S., 2008, Real-Time Massive Model Rendering
   Yoon SE, 2005, ACM T GRAPHIC, V24, P886, DOI 10.1145/1073204.1073278
NR 30
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 637
EP 647
DI 10.1007/s00371-014-0949-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700008
DA 2024-07-18
ER

PT J
AU Li, XS
   Liu, L
   Wu, W
   Liu, XH
   Wu, EH
AF Li, Xiaosheng
   Liu, Le
   Wu, Wen
   Liu, Xuehui
   Wu, Enhua
TI Dynamic BFECC Characteristic Mapping method for fluid simulations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Fluid simulation; Advection; BFECC; Characteristic Mapping
AB In this paper, we present a new numerical method for advection in fluid simulation. The method is built on the Characteristic Mapping method. Advection is solved via grid mapping function. The mapping function is maintained with higher order accuracy BFECC method and dynamically reset to identity mapping whenever an error criterion is met. Dealing with mapping function in such a way results in a more accurate mapping function and more details can be captured easily with this mapping function. Our error criterion also allows one to control the level of details of fluid simulation by simply adjusting one parameter. Details of implementation of our method are discussed and we present several techniques for improving its efficiency. Both quantitative and visual experiments were performed to test our method. The results show that our method brings significant improvement in accuracy and is efficient in capturing fluid details.
C1 [Li, Xiaosheng; Liu, Le; Liu, Xuehui; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Li, Xiaosheng; Liu, Le] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Wu, Wen; Wu, Enhua] Univ Macau, Macau, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; University
   of Macau
RP Li, XS (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
EM lixs@ios.ac.cn
RI Lin, Fan/JZT-1441-2024
OI Lin, Fan/0000-0002-7330-3833
FU NSFC [61272326]; HK RGC Grant [416212]; University of Macau
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions to improve the quality of the paper.
   This research is supported by NSFC Grant (61272326), HK RGC Grant
   (416212) and the grant of University of Macau.
CR [Anonymous], ARXIV13092731
   Bargteil AW, 2006, ACM T GRAPHIC, V25, P19, DOI 10.1145/1122501.1122503
   Dupont TF, 2003, J COMPUT PHYS, V190, P311, DOI 10.1016/S0021-9991(03)00276-6
   Enright D, 2002, J COMPUT PHYS, V183, P83, DOI 10.1006/jcph.2002.7166
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Hachisuka Toshiya., 2005, ACM SIGGRAPH 2005 SI
   Kim B., 2005, P 1 EUROGRAPHICS C N, DOI DOI 10.2312/NPH/NPH05/051-056
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P467, DOI 10.1111/j.1467-8659.2008.01144.x
   Nave JC, 2010, J COMPUT PHYS, V229, P3802, DOI 10.1016/j.jcp.2010.01.029
   Pharr M., 2010, PHYS BASED RENDERING
   Selle A, 2005, ACM T GRAPHIC, V24, P910, DOI 10.1145/1073204.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tessendorf J., 2011, COMP GRAPH INT WORKS
   ZALESAK ST, 1979, J COMPUT PHYS, V31, P335, DOI 10.1016/0021-9991(79)90051-2
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 17
TC 3
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 787
EP 796
DI 10.1007/s00371-014-0969-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700021
DA 2024-07-18
ER

PT J
AU Zhang, X
   Lin, ZC
   Sun, FC
   Ma, Y
AF Zhang, Xin
   Lin, Zhouchen
   Sun, Fuchun
   Ma, Yi
TI Transform invariant text extraction
SO VISUAL COMPUTER
LA English
DT Article
DE Text extraction; Arbitrary orientation; Stroke width transform; Texture
   invariant low-rank transform
ID IMAGES; VIDEO; ALGORITHM
AB Automatically extracting texts from natural images is very useful for many applications such as augmented reality. Most of the existing text detection systems require that the texts to be detected (and recognized) in an image are taken from a nearly frontal viewpoint. However, texts in most images taken naturally by a camera or a mobile phone can have a significant affine or perspective deformation, making the existing text detection and the subsequent OCR engines prone to failures. In this paper, based on stroke width transform and texture invariant low-rank transform, we propose a framework that can detect and rectify texts in arbitrary orientations in the image against complex backgrounds, so that the texts can be correctly recognized by common OCR engines. Extensive experiments show the advantage of our method when compared to the state of art text detection systems.
C1 [Zhang, Xin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Lin, Zhouchen] Peking Univ, Beijing 100871, Peoples R China.
   [Sun, Fuchun] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Ma, Yi] Microsoft Res Asia, Beijing, Peoples R China.
C3 Tsinghua University; Peking University; Tsinghua University; Microsoft;
   Microsoft Research Asia
RP Zhang, X (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM xinzhang1111@gmail.com
FU National Natural Science Foundation of China (NNSFC) [2013CB329403];
   NNSFC [61272341, 61231002, 61121002]; ONR [N00014-09-1-0230]; NSF [CCF
   09-64215, IIS 11-16012]
FX X. Zhang and F. Sun are supported by the National Natural Science
   Foundation of China (NNSFC) under Grant No. 2013CB329403. Z. Lin is
   supported by the NNSFC under Grant Nos. 61272341, 61231002, and
   61121002. Y. Ma is partially supported by the funding of ONR
   N00014-09-1-0230, NSF CCF 09-64215, NSF IIS 11-16012.
CR ABBYY Corp, ABB FIN
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen XR, 2004, PROC CVPR IEEE, P366
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   Daniilidis K., 2010, LECT NOTES COMPUTER, V6311
   Dinh VC, 2007, LECT NOTES COMPUT SC, V4843, P200
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Everingham M., 2006, PASCAL VISUAL OBJECT
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   He BS, 1998, OPER RES LETT, V23, P151, DOI 10.1016/S0167-6377(98)00044-3
   HP Labs, 1985, TESS OCR
   Jing Zhang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3979, DOI 10.1109/ICPR.2010.968
   Jung C, 2009, PATTERN RECOGN LETT, V30, P114, DOI 10.1016/j.patrec.2008.05.014
   Jung K, 2004, PATTERN RECOGN, V37, P977, DOI 10.1016/j.patcog.2003.10.012
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z
   Liang J, 2008, IEEE T PATTERN ANAL, V30, P591, DOI 10.1109/TPAMI.2007.70724
   Lin Z., 2009, Technical Report (No. UILU-ENG-09-2215
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Navarro G, 2001, ACM COMPUT SURV, V33, P31, DOI 10.1145/375360.375365
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Pastor M, 2004, LECT NOTES COMPUT SC, V3212, P183
   Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138
   Shivakumara P., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3996, DOI 10.1109/ICPR.2010.972
   Shivakumara P., 2008, Pattern recognition, P1
   Shivakumara P, 2009, IEEE INT CON MULTI, P514, DOI 10.1109/ICME.2009.5202546
   Singh C, 2008, PATTERN RECOGN, V41, P3528, DOI 10.1016/j.patcog.2008.06.002
   Subramanian K, 2007, PROC INT CONF DOC, P33
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wu V, 1999, IEEE T PATTERN ANAL, V21, P1224, DOI 10.1109/34.809116
   YAN H, 1993, CVGIP-GRAPH MODEL IM, V55, P538, DOI 10.1006/cgip.1993.1041
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Yi CC, 2011, IEEE T IMAGE PROCESS, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yi J., 2007, MULTIMEDIA 07, P847, DOI DOI 10.1145/1291233.1291426
   Yuan B, 2007, PATTERN RECOGN, V40, P456, DOI 10.1016/j.patcog.2006.02.016
   Zhang Zhengdong., 2010, ACCV
   Zhong Y, 2000, IEEE T PATTERN ANAL, V22, P385, DOI 10.1109/34.845381
NR 39
TC 8
Z9 8
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 401
EP 415
DI 10.1007/s00371-013-0864-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500004
DA 2024-07-18
ER

PT J
AU Fang, XY
   Wei, XP
   Zhang, Q
   Zhou, DS
AF Fang, Xiaoyong
   Wei, Xiaopeng
   Zhang, Qiang
   Zhou, Dongsheng
TI Forward non-rigid motion tracking for facial MoCap
SO VISUAL COMPUTER
LA English
DT Article
DE Facial MoCap; Non-rigid motion; Noise propagation; Motion tracking;
   Missing data; Topological structure; Heuristic checking
ID MISSING MARKERS; CAPTURE
AB For the existing motion capture (MoCap) data processing methods, manual interventions are always inevitable, most of which are derived from the data tracking process. This paper addresses the problem of tracking non-rigid 3D facial motions from sequences of raw MoCap data in the presence of noise, outliers and long time missing. We present a novel dynamic spatiotemporal framework to automatically solve the problem. First, based on a 3D facial topological structure, a sophisticated non-rigid motion interpreter (SNRMI) is put forward; together with a dynamic searching scheme, it cannot only track the non-missing data to the maximum extent but recover missing data (it can accurately recover more than five adjacent markers under long time (about 5 seconds) missing) accurately. To rule out wrong tracks of the markers labeled in open structures (such as mouth, eyes), a semantic-based heuristic checking method was raised. Second, since the existing methods have not taken the noise propagation problem into account, a forward processing framework is presented to solve the problem. Another contribution is the proposed method could track facial non-rigid motions automatically and forward, and is believed to greatly reduce even eliminate the requirements of human interventions during the facial MoCap data processing. Experimental results proved the effectiveness, robustness and accuracy of our system.
C1 [Fang, Xiaoyong; Wei, Xiaopeng; Zhang, Qiang; Zhou, Dongsheng] Dalian Univ, Minist Educ, Key Lab Adv Design & Intelligent Comp, Dalian 116622, Peoples R China.
   [Fang, Xiaoyong] Hunan Inst Technol, Sch Comp & Informat Sci, Hengyang 421002, Peoples R China.
C3 Dalian University; Hunan Institute of Technology
RP Zhang, Q (corresponding author), Dalian Univ, Minist Educ, Key Lab Adv Design & Intelligent Comp, Dalian 116622, Peoples R China.
EM zhangq30@yahoo.com
RI jiang, lei/IWE-1124-2023; Fang, Xiaoyong/AAC-2483-2019; wei,
   xiao/ISB-6027-2023; Zhang, Qiang/IWU-5000-2023
OI Zhang, Qiang/0000-0003-3776-9799
FU Program for Changjiang Scholars and Innovative Research Team in
   University [IRT1109]; Program for Liaoning Science and Technology
   Research in University [LS2010008]; Program for Liaoning Innovative
   Research Team in University [LT2011018]; Natural Science Foundation of
   Liaoning Province [201102008]; Program for Liaoning Key Lab of
   Intelligent Information Processing and Network Technology in University;
   Liaoning BaiQianWan Talents Program [2010921010, 2011921009]; General
   Project of Basic Research Program of Hunan Provincial Science and
   Technology Department [2012FJ3034]
FX This work is supported by the Program for Changjiang Scholars and
   Innovative Research Team in University (No. IRT1109), the Program for
   Liaoning Science and Technology Research in University (No. LS2010008),
   the Program for Liaoning Innovative Research Team in University(No.
   LT2011018), Natural Science Foundation of Liaoning Province (201102008),
   the Program for Liaoning Key Lab of Intelligent Information Processing
   and Network Technology in University, "Liaoning BaiQianWan Talents
   Program (2010921010, 2011921009)" and by the General Project of Basic
   Research Program of Hunan Provincial Science and Technology Department
   (Grant No. 2012FJ3034).
CR [Anonymous], ACM SIGGRAPH P LOS A
   [Anonymous], 2002, C COMP AN GEN SWITZ
   [Anonymous], ACM T GRAPH
   [Anonymous], P 5 C AUD VIS SPEECH
   [Anonymous], INT C MULT EXP
   [Anonymous], COMPUT AIDED D UNPUB
   [Anonymous], 1996, P 7 EUROGRAPHICS INT
   [Anonymous], 4 INT C AUT FAC GEST
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 3DSMAX SOFTW
   [Anonymous], INT C COMP GRAPH INT
   [Anonymous], VIC MOT SYST
   [Anonymous], BRIT MACH VIS C OXF
   [Anonymous], OSUCISRC406TR46 DEP
   [Anonymous], 2003, ACM S VIRT REAL SOFT
   [Anonymous], MAYA SOFTW
   [Anonymous], P SIGGRAPH 2006
   [Anonymous], MOT BUILD
   [Anonymous], SURVEY FACIAL MODELI
   [Anonymous], CS0402 U SHEFF DEP C
   [Anonymous], MIXED SCALE MOTION R
   [Anonymous], ECCV
   Aristidou A, 2008, LECT NOTES COMPUT SC, V5098, P238, DOI 10.1007/978-3-540-70517-8_23
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Ekman P, 1978, FACIAL ACTION CODING
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Grewal Mohinder S., 1993, Kalman filtering: Theory and Practice with MATLAB
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Jiang JT, 2002, EURASIP J APPL SIG P, V2002, P1174, DOI 10.1155/S1110865702206046
   KALMAN RE, 1960, T ASME D, V82, P33
   Li B, 2008, PATTERN RECOGN, V41, P418, DOI 10.1016/j.patcog.2007.06.002
   LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724
   Lin IC, 2005, VISUAL COMPUT, V21, P355, DOI 10.1007/s00371-005-0291-5
   Liu GD, 2006, VISUAL COMPUT, V22, P721, DOI 10.1007/s00371-006-0080-9
   Liu XG, 2004, IEEE DECIS CONTR P, P4180, DOI 10.1109/CDC.2004.1429408
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Sinopoli B, 2004, IEEE T AUTOMAT CONTR, V49, P1453, DOI 10.1109/TAC.2004.834121
   Torresani L., 2008, IEEE T PATTERN ANAL
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Wei XP, 2010, COMPUT SCI INF SYST, V7, P231, DOI 10.2298/CSIS1001231W
NR 43
TC 2
Z9 2
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2014
VL 30
IS 2
BP 139
EP 157
DI 10.1007/s00371-013-0790-8
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA9DL
UT WOS:000331393900002
DA 2024-07-18
ER

PT J
AU Liu, B
   Jia, XY
   Huang, ZH
   Li, HJ
AF Liu, Bin
   Jia, Xianyong
   Huang, Zhihuan
   Li, Haojie
TI An improved system for 3D individualized modeling of the artificial
   femoral head
SO VISUAL COMPUTER
LA English
DT Article
DE Femoral head; Individualized modeling; 3D reconstruction; Reverse
   engineering
AB The matching quality between the femoral head prosthesis and the acetabulum plays an important role in the operative treatment of the artificial femoral head replacement. In order to obtain a more accurate model of the femoral head prosthesis for the target sufferer, an improved individualized modeling system was presented in this paper. It was different from our previous system based on the sphere fitting method. This new system could reconstruct a more accurate ellipsoid model of the femoral head for the target sufferer. It could recover the necrotic femoral heads into the satisfactory models. These models could well match with the acetabulums. This improved system afforded a theoretical model for the accurate surgical position fixing in the orthopaedic clinic. And this system also provided an innovative practical means for the individualized modeling and manufacture of the artificial femoral head prosthesis.
C1 [Liu, Bin; Jia, Xianyong; Huang, Zhihuan; Li, Haojie] Dalian Univ Technol, Dept Digital Media Technol, Dalian 116620, Peoples R China.
C3 Dalian University of Technology
RP Liu, B (corresponding author), Dalian Univ Technol, Dept Digital Media Technol, Dalian 116620, Peoples R China.
EM laohubinbin@yahoo.com.cn
FU National High Technology Research and Development Program of China (863
   Program) [863-306-ZD13-03-6]; Fundamental Research Funds for the Central
   Universities of China [1600-852022, DUT12JR01]; High Technology Research
   and Development Program of Dalian City [2005E21SF134]
FX This work was supported by the National High Technology Research and
   Development Program of China (863 Program) (Grant No.
   863-306-ZD13-03-6), the Fundamental Research Funds for the Central
   Universities of China (Grant No. 1600-852022 and Grant No. DUT12JR01),
   and the High Technology Research and Development Program of Dalian City
   (Grant No. 2005E21SF134).
CR Baruffaldi F, 2006, IEEE T NUCL SCI, V53, P2584, DOI 10.1109/TNS.2006.876047
   Bassounas A, 2001, P ANN INT IEEE EMBS, V23, P1532, DOI 10.1109/IEMBS.2001.1020500
   Choi KH, 1997, P ANN INT IEEE EMBS, V19, P410, DOI 10.1109/IEMBS.1997.754565
   de Luis-Garcia Rodrigo, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P4807
   Fitzgibbon A., 1996, ICPR, P253
   Gu Dongyun, 2003, Sheng Wu Yi Xue Gong Cheng Xue Za Zhi, V20, P618
   Kim JS, 1997, P ANN INT IEEE EMBS, V19, P418, DOI 10.1109/IEMBS.1997.754567
   Li W, 2008, IEEE T BIO-MED ENG, V55, P2731, DOI 10.1109/TBME.2008.925679
   Liu Bin, 2008, Journal of Southeast University (Natural Science Edition), V38, P58
   Mitsuishi M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS, P834
   Nakajima Y, 2007, IEEE T BIO-MED ENG, V54, P1703, DOI 10.1109/TBME.2007.900822
   Sarry L, 2003, IEEE T MED IMAGING, V22, P1172, DOI 10.1109/TMI.2003.817017
   Sharma N, 2005, NORTHEAST BIOENGIN C, P92
   Stelios D.K., 2009, P 9 INT C INF TECHN, P1
   Winzenrieth R, 2006, IEEE T BIO-MED ENG, V53, P1190, DOI 10.1109/TBME.2006.873552
   Zhao De-wei, 2005, Zhonghua Wai Ke Za Zhi, V43, P1054
   Zoroofi RA, 2003, IEEE T INF TECHNOL B, V7, P329, DOI 10.1109/TITB.2003.813791
NR 17
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2013
VL 29
IS 12
BP 1259
EP 1267
DI 10.1007/s00371-012-0767-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 251LB
UT WOS:000326929200003
DA 2024-07-18
ER

PT J
AU von Landesberger, T
   Andrienko, G
   Andrienko, N
   Bremm, S
   Kirschner, M
   Wesarg, S
   Kuijper, A
AF von Landesberger, Tatiana
   Andrienko, Gennady
   Andrienko, Natalia
   Bremm, Sebastian
   Kirschner, Matthias
   Wesarg, Stefan
   Kuijper, Arjan
TI Opening up the "black box" of medical image segmentation with
   statistical shape models
SO VISUAL COMPUTER
LA English
DT Article
DE Medical imaging; Medical modeling; Visual analytics; Image segmentation;
   Statistical shape models; Spatio-temporal data
ID MOVEMENT DATA; VISUALIZATION; MAPS; ANALYTICS
AB The importance of medical image segmentation increases in fields like treatment planning or computer aided diagnosis. For high quality automatic segmentations, algorithms based on statistical shape models (SSMs) are often used. They segment the image in an iterative way. However, segmentation experts and other users can only asses the final segmentation results, as the segmentation is performed in a "black box manner". Users cannot get deeper knowledge on how the (possibly bad) output was produced. Moreover, they do not see whether the final output is the result of a stabilized process.
   We present a novel Visual Analytics method, which offers this desired deeper insight into the image segmentation. Our approach combines interactive visualization and automatic data analysis. It allows the expert to assess the quality development (convergence) of the model both on global (full organ) and local (organ areas, landmarks) level. Thereby, local patterns in time and space, e.g., non-converging parts of the organ during the segmentation, can be identified. The localization and specifications of such problems helps the experts creating segmentation algorithms to identify algorithm drawbacks and thus it may point out possible ways how to improve the algorithms systematically.
   We apply our approach on real-world data showing its usefulness for the analysis of the segmentation process with statistical shape models.
C1 [von Landesberger, Tatiana; Kirschner, Matthias] Tech Univ Darmstadt, Darmstadt, Germany.
   [Andrienko, Gennady; Andrienko, Natalia] Fraunhofer IAIS, Bonn, Germany.
   [Bremm, Sebastian] Tech Univ Darmstadt, Dept Graph Interact Syst, Area Visual Analyt, Darmstadt, Germany.
   [Wesarg, Stefan; Kuijper, Arjan] Fraunhofer IGD, Darmstadt, Germany.
C3 Technical University of Darmstadt; Technical University of Darmstadt
RP von Landesberger, T (corresponding author), Tech Univ Darmstadt, Darmstadt, Germany.
EM Tatiana.von-Landesberger@gris.tu-darmstadt.de;
   Gennady.Andrienko@iais.fraunhofer.de;
   Natalia.Andrienko@iais.fraunhofer.de;
   Sebastian.Bremm@gris.tu-darmstadt.de;
   Matthias.Kirschner@gris.tu-darmstadt.de;
   Stefan.Wesarg@igd.fraunhofer.de; Arjan.Kuijper@igd.fraunhofer.de
RI Wesarg, Stefan/ABF-9706-2021; Andrienko, Natalia/KHV-4755-2024; Kuijper,
   Arjan/A-7814-2012; Andrienko, Gennady L./B-6486-2014
OI Kuijper, Arjan/0000-0002-6413-0061; Andrienko, Gennady
   L./0000-0002-8574-6295; Andrienko, Natalia/0000-0003-3313-1560
FU DFG [SPP 1335]
FX The work has been partially supported by the DFG SPP 1335 project
   "Visual Analytics Methods for Modeling in Medical Imaging". The authors
   would like to thank J. Beutel for his support with the project.
CR Andrienko G, 2010, COMPUT GRAPH FORUM, V29, P913, DOI 10.1111/j.1467-8659.2009.01664.x
   Andrienko N, 2013, INFORM VISUAL, V12, P3, DOI 10.1177/1473871612457601
   Andrienko N, 2011, IEEE T VIS COMPUT GR, V17, P205, DOI 10.1109/TVCG.2010.44
   Angelelli P, 2010, EUR WORKSH VIS COMP, P125
   Bruckner S, 2010, COMPUT GRAPH FORUM, V29, P773, DOI 10.1111/j.1467-8659.2009.01689.x
   Busking S, 2011, VISUAL COMPUT, V27, P347, DOI 10.1007/s00371-010-0541-z
   Busking S, 2010, COMPUT GRAPH FORUM, V29, P973, DOI 10.1111/j.1467-8659.2009.01668.x
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Demsar U, 2008, INFORM VISUAL, V7, P181, DOI 10.1057/palgrave.ivs.9500187
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P2173, DOI 10.1109/TVCG.2011.189
   Heimann T, 2007, LECT NOTES COMPUT SC, V4584, P1
   Heimann T, 2009, IEEE T MED IMAGING, V28, P1251, DOI 10.1109/TMI.2009.2013851
   Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004
   Himberg J, 2000, IEEE IJCNN, P587, DOI 10.1109/IJCNN.2000.861379
   Kohonen T., 2001, INFORM SCIENCES
   Maciejewski R, 2010, IEEE T VIS COMPUT GR, V16, P205, DOI 10.1109/TVCG.2009.100
   Matkovic K, 2010, IEEE T VIS COMPUT GR, V16, P1449, DOI 10.1109/TVCG.2010.171
   Mayer, JAVA SOMTOOLBOX
   Preim Bernhard., 2007, Visualization in Medicine, Theory, Algorithms, and Applications
   Rinzivillo S, 2008, INFORM VISUAL, V7, P225, DOI 10.1057/palgrave.ivs.9500183
   Schreck T, 2009, INFORM VISUAL, V8, P14, DOI 10.1057/ivs.2008.29
   Silva S, 2005, NINTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P842, DOI 10.1109/IV.2005.98
   Tversky B, 2002, INT J HUM-COMPUT ST, V57, P247, DOI 10.1006/ijhc.1017
   Vesanto J., 1999, Intelligent Data Analysis, V3, P111, DOI 10.1016/S1088-467X(99)00013-X
   Willems N, 2009, COMPUT GRAPH FORUM, V28, P959, DOI 10.1111/j.1467-8659.2009.01440.x
   Zhou L, 2001, THESIS U CALIFORNIA
NR 26
TC 12
Z9 12
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2013
VL 29
IS 9
BP 893
EP 905
DI 10.1007/s00371-013-0852-y
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 186CB
UT WOS:000322018100005
DA 2024-07-18
ER

PT J
AU Moore, KD
   Peers, P
AF Moore, Kathleen D.
   Peers, Pieter
TI An empirical study on the effects of translucency on photometric stereo
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International (CGI) Conference
CY 2013
CL Hanover, GERMANY
DE Subsurface scattering; Photometric stereo
AB We present an empirical study on the effects of translucency on photometric stereo. Our study shows that the impact on the accuracy of the photometric normals is related to the relative size of the geometrical features and the mean free path. We show that under simplified conditions, the obtained photometric normals are a blurred version of the true surface normals, where the blur kernel is directly related to the subsurface scattering profile. We furthermore investigate the impact of scattering albedo, index of refraction, and single scattering on the accuracy. We perform our analysis using simulations, and demonstrate the validity on a real world example.
C1 [Moore, Kathleen D.; Peers, Pieter] Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA.
C3 William & Mary
RP Moore, KD (corresponding author), Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA.
EM kdm@cs.wm.edu
FU Google; NSF [IIS-1016703, IIS-1217765]; Virginia Space Grant Consortium;
   Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1016703] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1217765] Funding Source: National Science
   Foundation
FX This work was supported in part by Google, and NSF grants IIS-1016703
   and IIS-1217765. The first author acknowledges additional support from
   the Virginia Space Grant Consortium.
CR Alldrin N., 2007, PROC INT C COMPUTER, P1
   [Anonymous], 1977, TECHNICAL REPORT
   [Anonymous], 2007, RENDERING TECHNIQUES
   [Anonymous], ACM T GRAPH
   Barsky S, 2003, IEEE T PATTERN ANAL, V25, P1239, DOI 10.1109/TPAMI.2003.1233898
   Basri R, 2001, PROC CVPR IEEE, P374
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Godin Guy, 2001, P 5 C OPT 3 D MEAS T
   Goesele M, 2004, ACM T GRAPHIC, V23, P835, DOI 10.1145/1015706.1015807
   Goldman DB, 2005, IEEE I CONF COMP VIS, P341
   Hanrahan P., 1993, Computer Graphics Proceedings, P165, DOI 10.1145/166117.166139
   Holroyd M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2985, DOI 10.1109/CVPR.2011.5995536
   Holroyd M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778836
   Inoshita C, 2012, LECT NOTES COMPUT SC, V7573, P371, DOI 10.1007/978-3-642-33709-3_27
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Nayar S. K., 1989, MIV-89 Proceedings of the International Workshop on Industrial Applications of Machine Intelligence and Vision (Seiken Symposium) (Cat. No.89TH0250-1), P169, DOI 10.1109/MIV.1989.40544
   Nayar SK, 2006, ACM T GRAPHIC, V25, P935, DOI 10.1145/1141911.1141977
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Stam J, 1995, SPRING COMP SCI, P41
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
NR 21
TC 4
Z9 4
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2013
VL 29
IS 6-8
BP 817
EP 824
DI 10.1007/s00371-013-0832-2
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 151RR
UT WOS:000319478400034
DA 2024-07-18
ER

PT J
AU Ahmadian, K
   Gavrilova, M
AF Ahmadian, Kushan
   Gavrilova, Marina
TI A multi-modal approach for high-dimensional feature recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Neural networks; Face recognition; Biometric; Security; Avatars; Virtual
   worlds
ID FACE RECOGNITION
AB Over the past few decades, biometric recognition firmly established itself as one of the areas of tremendous potential to make scientific discovery and to advance state-of-the- art research in security domain. Hardly, there is a single area of IT left untouched by increased vulnerabilities, on-line scams, e-fraud, illegal activities, and event pranks in virtual worlds. In parallel with biometric development, which went from focus on single biometric recognition methods to multi-modal information fusion, another rising area of research is virtual world's security and avatar recognition. This article explores links between multi-biometric system architecture and virtual worlds face recognition, and proposes methodology which can be of benefit for both applications.
C1 [Ahmadian, Kushan; Gavrilova, Marina] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Gavrilova, M (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM kahmadia@cpsc.ucalgary.ca; marina@cpsc.ucalgary.ca
RI Gavrilova, Marina/AHD-3605-2022
OI Gavrilova, Marina/0000-0002-5338-1834
FU NSERC; GEOIDE; Biometric Technologies Laboratory at the University of
   Calgary
FX Authors of the paper would like to acknowledge support of NSERC and
   GEOIDE for partially sponsoring this research, as well as Biometric
   Technologies Laboratory at the University of Calgary. We also would like
   to acknowledge Prof. Roman Yampolskiy for his collaboration on avatar
   recognition methodology and for sharing avatar images.
CR Ahmadian Kushan, 2012, Advances in Artificial Intelligence, DOI 10.1155/2012/124176
   Ahmadian K, 2011, 2011 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P9, DOI 10.1109/CW.2011.48
   [Anonymous], 2003, TR200396 MITS EL RES
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Gao YS, 2002, IEEE T PATTERN ANAL, V24, P764, DOI 10.1109/TPAMI.2002.1008383
   Gavrilova M., 2011, INT J INFORM TECHNOL, V11, P18
   Gavrilova ML, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P179, DOI 10.1109/CW.2010.36
   Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133
   Li SZ, 1999, IEEE T NEURAL NETWOR, V10, P439, DOI 10.1109/72.750575
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P195, DOI 10.1109/TNN.2002.806647
   Monwar MM, 2009, IEEE T SYST MAN CY B, V39, P867, DOI 10.1109/TSMCB.2008.2009071
   Phillips P. J., 1998, ADV NEURAL INF PROCE, V11, P113
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wang LP, 1998, IEEE T NEURAL NETWOR, V9, P716, DOI 10.1109/72.701185
   Wecker L, 2010, COMPUT GRAPH-UK, V34, P468, DOI 10.1016/j.cag.2010.05.012
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Yampolskiy R., 2011, P CYB 2011 OCT BANFF, P9
NR 20
TC 7
Z9 8
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2013
VL 29
IS 2
SI SI
BP 123
EP 130
DI 10.1007/s00371-012-0741-9
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 072FM
UT WOS:000313654700004
DA 2024-07-18
ER

PT J
AU Kunigami, G
   de Rezende, PJ
   de Souza, CC
   Yunes, T
AF Kunigami, Guilherme
   de Rezende, Pedro J.
   de Souza, Cid C.
   Yunes, Tallys
TI Generating optimal drawings of physically realizable symbol maps with
   integer programming
SO VISUAL COMPUTER
LA English
DT Article
DE Visualization; Cartography; Computational geometry; Integer linear
   programming
AB Proportional symbol maps are a tool often used by cartographers and geoscience professionals to visualize geopositioned data associated with events and demographic statistics, such as earthquakes and population counts. Symbols are placed at specific locations on a map, and their areas are scaled to become proportional to the magnitudes of the data points they represent. We focus specifically on creating physically realizable drawings of symbols-opaque disks, in our case-by maximizing two quality metrics: the total and the minimum length of their visible borders. As these two maximization problems have been proven to be NP-hard, we provide integer programming formulations for their solution, along with decomposition techniques designed to decrease the size of input instances. Our computational experiments, which use real-life data sets, demonstrate the effectiveness of our approach and provide, for the first time, a number of optimal solutions to previously studied instances of this problem.
C1 [Kunigami, Guilherme; de Rezende, Pedro J.; de Souza, Cid C.] Univ Estadual Campinas, Inst Comp, Campinas, SP, Brazil.
   [Yunes, Tallys] Univ Miami, Dept Management Sci, Sch Business Adm, Coral Gables, FL 33124 USA.
C3 Universidade Estadual de Campinas; University of Miami
RP Kunigami, G (corresponding author), Univ Estadual Campinas, Inst Comp, Campinas, SP, Brazil.
EM kunigami@gmail.com; rezende@ic.unicamp.br; cid@ic.unicamp.br;
   tallys@miami.edu
RI Yunes, Tallys H/A-9702-2008; de Souza, Cid/A-9289-2008
OI Yunes, Tallys/0000-0002-8308-7812; de Souza, Cid/0000-0002-5945-0845
FU CNPq (Conselho Nacional de Desenvolvimento Cientifico e Tecnologico)
   [830510/1999-0]; CNPq [483177/2009-1, 473867/2010-9, 301732/2007-8,
   472504/2007-0]; FAPESP (Fundacao de Amparo a Pesquisa do Estado de Sao
   Paulo) [07/52015-0]; FAEPEX/UNICAMP; FAPESP [07/52015-0]
FX Guilherme Kunigami is supported by CNPq (Conselho Nacional de
   Desenvolvimento Cientifico e Tecnologico) grant 830510/1999-0. Pedro J.
   de Rezende is partially supported by CNPq grants 483177/2009-1,
   473867/2010-9, FAPESP (Fundacao de Amparo a Pesquisa do Estado de Sao
   Paulo) grant 07/52015-0, and a grant from FAEPEX/UNICAMP. Cid C. de
   Souza is partially supported by CNPq grants 301732/2007-8,
   472504/2007-0, 473867/2010-9, and FAPESP grant 07/52015-0.
CR [Anonymous], 1998, INTEGER PROGRAMMING
   Cabello S, 2010, ALGORITHMICA, V58, P543, DOI 10.1007/s00453-009-9281-8
   Fair Isaac Corp, XPRESS OPT REF MAN
   Griffin T., 1990, Cartography, V19, P21, DOI DOI 10.1080/00690805.1990.10438484
   Kunigami G., 2011, PROPORTIONAL SYMBOL
   Kunigami G., 2011, P 24 C GRAPH PATT IM, P1
   Kunigami G., OPTIMIZING LAYOUT PR
   Kunigami G, 2011, LECT NOTES COMPUT SC, V6784, P1
   Slocum T.A., 2003, Thematic Cartography and Geographic Visualization
NR 9
TC 5
Z9 5
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 1015
EP 1026
DI 10.1007/s00371-012-0727-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900007
DA 2024-07-18
ER

PT J
AU You, LH
   Ugail, H
   Zhang, JJ
AF You, L. H.
   Ugail, H.
   Zhang, Jian J.
TI Controllable C1 continuous blending of time-dependent parametric
   surfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Surface blending; Varying parametric surfaces; C1 continuity; Dynamic
   partial differential equations; Approximate analytical solution; Shape
   control
ID FINITE-ELEMENT METHODS; PARTIAL-DIFFERENTIAL EQUATIONS; ROLLING BALL
   BLENDS; PDE SURFACES; APPROXIMATIONS; GENERATION
AB This paper proposes the concept of blending time-dependent varying surfaces, and develops a new method to create a controllable C1 continuous blending surface between primary parametric surfaces whose position and shape change with time. We treat it as a boundary-valued problem defined by the mathematical model of a vectored dynamic fourth-order partial differential equation subjected to time-dependent C1 continuous blending boundary constraints. High performance blending surface generation is achieved through the development of an approximate analytical solution of the mathematical model. We investigate the accuracy and efficiency of the solution, study the effective shape control of the blending surfaces, and apply the obtained solution to tackle surface blending problems. The applications demonstrate that our proposed approach is very effective and efficient in dealing with controllable C1 continuous surface blending between time-dependent varying parametric surfaces.
C1 [You, L. H.; Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Comp Animat Res Ctr, Bournemouth, Dorset, England.
   [Ugail, H.] Univ Bradford, Ctr Visual Comp, Bradford BD7 1DP, W Yorkshire, England.
C3 Bournemouth University; University of Bradford
RP You, LH (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Comp Animat Res Ctr, Bournemouth, Dorset, England.
EM lyou@bournemouth.ac.uk; h.ugail@bradford.ac.uk; jzhang@bournemouth.ac.uk
OI Zhang, Jian/0000-0002-7069-5771
CR Barnhill R.E., 1993, COMPUTING S, V8, P1
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P324, DOI 10.1016/0010-4485(90)90083-O
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Bloor MIG, 1996, COMPUT AIDED DESIGN, V28, P145, DOI 10.1016/0010-4485(95)00060-7
   Bloor MIG, 2000, MATH COMPUT MODEL, V31, P1, DOI 10.1016/S0895-7177(99)00212-5
   Brown JM, 1998, COMPUT METHOD APPL M, V158, P221, DOI 10.1016/S0045-7825(98)00252-7
   CHENG SY, 1990, ADV DESIGN AUTOMATIO, V1, P257
   CHOI BK, 1989, COMPUT AIDED DESIGN, V21, P213, DOI 10.1016/0010-4485(89)90046-8
   Chuang JH, 1998, J INF SCI ENG, V14, P461
   Chuang JH, 1995, VISUAL COMPUT, V11, P513, DOI 10.1007/BF02434038
   Chuang JH, 1997, VISUAL COMPUT, V13, P316, DOI 10.1007/s003710050106
   Farouki RAMT, 1996, COMPUT AIDED DESIGN, V28, P871, DOI 10.1016/0010-4485(96)00008-5
   Gonzalez C.G., 2010, VISUAL COMPUT, V26, P325
   Kós G, 2000, COMPUT AIDED GEOM D, V17, P127, DOI 10.1016/S0167-8396(99)00043-6
   Li ZC, 1998, J COMPUT MATH, V16, P457
   Li ZC, 1999, J COMPUT APPL MATH, V110, P241, DOI 10.1016/S0377-0427(99)00231-9
   Li ZC, 1999, J COMPUT APPL MATH, V110, P155, DOI 10.1016/S0377-0427(99)00208-3
   Lukacs G, 1998, COMPUT AIDED GEOM D, V15, P585, DOI 10.1016/S0167-8396(98)00006-5
   Rossignac J. R., 1984, Computers in Mechanical Engineering, V3, P65
   Sanglikar M. A., 1990, Computer-Aided Geometric Design, V7, P399, DOI 10.1016/0167-8396(90)90003-A
   Sheng Y, 2010, VISUAL COMPUT, V26, P975, DOI 10.1007/s00371-010-0456-8
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   VIDA J, 1994, COMPUT AIDED DESIGN, V26, P341, DOI 10.1016/0010-4485(94)90023-X
   You LH, 2011, COMPUT AIDED DESIGN, V43, P720, DOI 10.1016/j.cad.2011.01.021
   You LH, 2004, COMPUT GRAPH-UK, V28, P895, DOI 10.1016/j.cag.2004.08.003
   You LH, 2004, VISUAL COMPUT, V20, P199, DOI 10.1007/s00371-003-0241-7
   Zhang JJ, 2004, COMPUT GRAPH FORUM, V23, P311, DOI 10.1111/j.1467-8659.2004.00762.x
NR 28
TC 2
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 573
EP 583
DI 10.1007/s00371-012-0693-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500006
DA 2024-07-18
ER

PT J
AU Yu, HC
   Zhang, JJ
AF Yu, Hongchuan
   Zhang, Jian J.
TI Topology preserved shape deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Image and shape deformation; Topology preservation; Foldover-free
   constraint
AB This paper presents a novel framework for image and shape deformation, and applies the presented method to 2D shape deformation and image/video magnifying. Our proposed framework maintains two distinct advantages. The first is to enforce the topology preservation constraints on a given displacement field. This allows us to develop a foldover-free constrained deformation approach. The second is the ability to easily incorporate the constraint of As-Rigid-As Possible deformation. It guarantees that the resulting distortions are as small as possible. Experiments are carried out on both 2D shape deformations and images/video magnifying.
C1 [Yu, Hongchuan; Zhang, Jian J.] Bournemouth Univ, Media Sch, Poole BH12 5BB, Dorset, England.
C3 Bournemouth University
RP Yu, HC (corresponding author), Bournemouth Univ, Media Sch, Poole BH12 5BB, Dorset, England.
EM hyu@bournemouth.ac.uk; jzhang@bournemouth.ac.uk
RI Yu, Hongnian/AFV-5287-2022
OI Yu, Hongnian/0000-0003-2894-2086; Zhang, Jian/0000-0002-7069-5771; Yu,
   Hongchuan/0000-0002-6024-060X
CR Alexa M, 2000, COMP GRAPH, P157, DOI 10.1145/344779.344859
   [Anonymous], 2007, S GEOMETRY PROCESSIN
   [Anonymous], P VISMATH 02 BERL GE
   Arfken G.B., 1995, MATH METHODS PHYS, P91
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Daniel G, 2003, P 14 IEEE VIS 2003 V
   Dong W., 2009, P SIGGRAPH AS 09, P125
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Fujimura K, 1998, GRAPH MODEL IM PROC, V60, P100, DOI 10.1006/gmip.1998.0454
   IGARASHI T., 2005, ACM T GRAPH, V24, P3
   Karni Z., 2009, P S GEOM PROC
   Kraevoy V, 2003, ACM T GRAPHIC, V22, P326, DOI 10.1145/882262.882271
   Krahenbuhl P., 2009, P SIGGRAPH AS 09
   Lee TY, 2008, IEEE T VIS COMPUT GR, V14, P382, DOI 10.1109/TVCG.2007.70432
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Liu L., 2008, P S GEOM PROC COP DE
   MacCracken R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P181, DOI 10.1145/237170.237247
   Perez P., 2003, ACM T GRAPH, V22
   Rubinstein M., 2008, P SIGGRAPH 08, P16
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Tiddeman B, 2001, COMPUT GRAPH-UK, V25, P59, DOI 10.1016/S0097-8493(00)00107-2
   Tong YY, 2003, ACM T GRAPHIC, V22, P445, DOI 10.1145/882262.882290
   Wang Y., 2008, P SIGGRAPH AS 08, P118
   Wang Y., 2010, P SIGGRAPH 10, P90
   Weber O, 2009, COMPUT GRAPH FORUM, V28, P587, DOI 10.1111/j.1467-8659.2009.01399.x
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhiwei Xiong, 2010, IEEE T IMAGE PROCESS, V19
NR 28
TC 4
Z9 5
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 849
EP 858
DI 10.1007/s00371-012-0708-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500032
DA 2024-07-18
ER

PT J
AU Tang, Z
   Miao, ZJ
   Wan, YL
   Zhang, DY
AF Tang, Zhen
   Miao, Zhenjiang
   Wan, Yanli
   Zhang, Dianyong
TI Video matting via opacity propagation
SO VISUAL COMPUTER
LA English
DT Article
DE Image matting; Opacity propagation; Optical flow; Temporal consistency
   preservation; Video matting
ID INTERACTIVE IMAGE; SEGMENTATION
AB In this paper, a novel video matting method based on opacity propagation is proposed. It adopts a two-step framework by first segmenting each frame into bilayer and generating trimaps, and then applying matting algorithm to extract the final alpha mattes. In the bilayer segmentation step, an opacity propagation algorithm is used to predict the foreground object in the following frame. It propagates the opacity values of the former frame to the current, and generates an Opacity Map (OM), which is in most cases much more accurate than the previously used Probability Map (PM). OM is used to extract the foreground object by combining it with the graph cut algorithm. And then an accurate trimap is generated by a trimap refinement method based on a Local Gaussian Mixture Model (LGMM). At last, the opacity propagation is again applied to the unknown regions of the trimap to generate the accurate alpha mattes in a temporally coherent way. The experiment results demonstrate the effectiveness of our method.
C1 [Tang, Zhen; Miao, Zhenjiang; Wan, Yanli; Zhang, Dianyong] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
C3 Beijing Jiaotong University
RP Tang, Z (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
EM 06120376@bjtu.edu.cn; zjmiao@bjtu.edu.cn; 07112067@bjtu.edu.cn;
   07112064@bjtu.edu.cn
FU National Natural Science Foundation of China [60973061]; Beijing
   Jiaotong University [141047522]
FX This work was supported in part by National Natural Science Foundation
   of China (60973061), and the Technological Innovation Fund of Excellent
   Doctoral Candidate of Beijing Jiaotong University (141047522).
CR Ahn JK, 2008, IEEE IMAGE PROC, P1544, DOI 10.1109/ICIP.2008.4712062
   [Anonymous], ACCV SEP
   [Anonymous], P 5 IASTED INT C VIS
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], P IEEE VECIMS JUL
   [Anonymous], P ACM T GRAPH
   [Anonymous], IEEE WORKSH VAR GEOM
   [Anonymous], P ACM T GRAPHICS
   [Anonymous], P GRAPHICON MOSC RUS
   [Anonymous], 2007, CVPR
   [Anonymous], P ACM T GRAPH
   [Anonymous], P GRAPH JUN
   [Anonymous], 2010, ICME
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IET EUR C VIS MED PR
   [Anonymous], 2008, BMVC
   Bai X, 2009, INT J COMPUT VISION, V82, P113, DOI 10.1007/s11263-008-0191-z
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   Gu YY, 2010, IEEE IMAGE PROC, P4661, DOI 10.1109/ICIP.2010.5651679
   He KM, 2010, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR.2010.5539896
   Lee PG, 2010, IEEE IMAGE PROC, P4665, DOI 10.1109/ICIP.2010.5652939
   Lee SY, 2010, GRAPH MODELS, V72, P25, DOI 10.1016/j.gmod.2010.03.001
   Price BL, 2010, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2010.5539895
   Rhemann C, 2010, PROC CVPR IEEE, P2149, DOI 10.1109/CVPR.2010.5539894
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shum HY, 2004, ACM T GRAPHIC, V23, P143, DOI 10.1145/990002.990005
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun YD, 2006, INT C PATT RECOG, P49
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Wang J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239460
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
NR 31
TC 10
Z9 11
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 47
EP 61
DI 10.1007/s00371-011-0598-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000005
DA 2024-07-18
ER

PT J
AU Das, K
   Majumder, A
   Siegenthaler, M
   Keirstead, H
   Gopi, M
AF Das, Koel
   Majumder, Aditi
   Siegenthaler, Monica
   Keirstead, Hans
   Gopi, M.
TI Automated cell classification and visualization for analyzing
   remyelination therapy
SO VISUAL COMPUTER
LA English
DT Article
DE Cell detection; Geometric processing; Progressive isocontour
ID SPINAL-CORD-INJURY; MULTIPLE-SCLEROSIS; HOUGH TRANSFORM; ACTIVE
   CONTOURS; SEGMENTATION; SNAKES; DEMYELINATION; SHAPES; AXONS
AB Remyelination therapy is a state-of-the-art technique for treating spinal cord injury (SCI). Demyelination-the loss of myelin sheath that insulates axons, is a prominent feature in many neurological disorders resulting in SCI. This lost myelin sheath can be replaced by remyelination. In this paper, we propose an algorithm for efficient automated cell classification and visualization to analyze the progress of remyelination therapy in SCI. Our method takes as input the images of the cells and outputs a density map of the therapeutically important oligodendrocyte-remyelinated axons (OR-axons) which is used for efficacy analysis of the therapy. Our method starts with detecting cell boundaries using a robust, shape-independent algorithm based on iso-contour analysis of the image at progressively increasing intensity levels. The detected boundaries of spatially clustered cells are then separated using the Delaunay triangulation based contour separation method. Finally, the OR-axons are identified and a density map is generated for efficacy analysis of the therapy. Our efficient automated cell classification and visualization of remyelination analysis significantly reduces error due to human subjectivity. We validate the accuracy of our results by extensive cross-verification by the domain experts.
C1 [Das, Koel] S Asian Univ, Delhi, India.
   [Majumder, Aditi; Gopi, M.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA USA.
   [Siegenthaler, Monica] Calif Stem Cell Inc, Irvine, CA USA.
   [Keirstead, Hans] Univ Calif Irvine, Dept Anat & Neurobiol, Irvine, CA 92717 USA.
C3 University of California System; University of California Irvine;
   University of California System; University of California Irvine
RP Das, K (corresponding author), S Asian Univ, Delhi, India.
EM das.koel@gmail.com
CR Aguado AS, 1998, COMPUT VIS IMAGE UND, V69, P202, DOI 10.1006/cviu.1997.0558
   Amenta N, 1998, GRAPH MODEL IM PROC, V60, P125, DOI 10.1006/gmip.1998.0465
   Angulo J, 2003, ANAL CELL PATHOL, V25, P37, DOI 10.1155/2003/642562
   Blakemore WF, 1999, J NEUROIMMUNOL, V98, P69, DOI 10.1016/S0165-5728(99)00083-1
   BLIGHT AR, 1983, NEUROSCIENCE, V10, P521, DOI 10.1016/0306-4522(83)90150-1
   CAHN RL, 1977, J HISTOCHEM CYTOCHEM, V25, P681, DOI 10.1177/25.7.330721
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   Chari DM, 2002, MULT SCLER J, V8, P271, DOI 10.1191/1352458502ms842oa
   DAS K, 2010, P 7 IND C COMP VIS G, P314
   Daul C, 1998, COMPUT VIS IMAGE UND, V72, P215, DOI 10.1006/cviu.1998.0696
   Doraiswamy H, 2009, COMP GEOM-THEOR APPL, V42, P606, DOI 10.1016/j.comgeo.2008.12.003
   Garrido A, 2000, PATTERN RECOGN, V33, P821, DOI 10.1016/S0031-3203(99)00091-6
   Goto T, 2001, J Orthop Sci, V6, P59, DOI 10.1007/s007760170026
   GUY J, 1989, J COMP NEUROL, V287, P446, DOI 10.1002/cne.902870404
   Gyulassy A, 2008, IEEE T VIS COMPUT GR, V14, P1619, DOI 10.1109/TVCG.2008.110
   Hagyard D., 1996, ICIP, V3, P41
   HERZBERG AJ, 1991, J INVEST DERMATOL, V97, P495, DOI 10.1111/1523-1747.ep12481529
   Jones T., 2002, ICPR, V2, P286
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Keirstead HS, 2005, TRENDS NEUROSCI, V28, P677, DOI 10.1016/j.tins.2005.09.008
   Li Y, 1999, J NEUROCYTOL, V28, P417, DOI 10.1023/A:1007026001189
   LIU L, 2001, P ICIP, V3, P1071
   Malpica N, 1997, CYTOMETRY, V28, P289, DOI 10.1002/(SICI)1097-0320(19970801)28:4<289::AID-CYTO3>3.0.CO;2-7
   McTigue DM, 1998, J NEUROSCI, V18, P5354, DOI 10.1523/JNEUROSCI.18-14-05354.1998
   MEYER J, 2008, AICHE
   Milnor J., 1969, Morse Theory
   Najman L, 1996, IEEE T PATTERN ANAL, V18, P1163, DOI 10.1109/34.546254
   Park J, 2001, IEEE T PATTERN ANAL, V23, P1201, DOI 10.1109/34.954609
   PRINEAS J, 1975, HUM PATHOL, V6, P531, DOI 10.1016/S0046-8177(75)80040-2
   Salgado-Ceballos H, 1998, BRAIN RES, V782, P126
   Schnorrenberg F, 1997, IEEE Trans Inf Technol Biomed, V1, P128, DOI 10.1109/4233.640655
   Scolding NJ, 1997, BAILLIERE CLIN NEUR, V6, P525
   STRANGEL M, 2002, PROG NEUROBIOL, V68, P361
   Thiran JP, 1996, IEEE T BIO-MED ENG, V43, P1011, DOI 10.1109/10.536902
   Totoiu MO, 2005, J COMP NEUROL, V486, P373, DOI 10.1002/cne.20517
   Tsai DM, 1997, IMAGE VISION COMPUT, V15, P877, DOI 10.1016/S0262-8856(97)00033-4
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATERSHED BS, 1994, MATH MORPHOLOGY ITS, P69
   WAXMAN SG, 1992, J NEUROTRAUM, V9, pS105
   WAXMAN SG, 1989, J NEUROL SCI, V91, P1, DOI 10.1016/0022-510X(89)90072-5
   WU D, 2007, ICBBE 2007 JUL 2007, P920
   Wu HS, 2000, J MICROSC-OXFORD, V197, P296, DOI 10.1046/j.1365-2818.2000.00653.x
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Zhou XB, 2006, J BIOMED INFORM, V39, P115, DOI 10.1016/j.jbi.2005.05.008
   Zimmer C, 2002, IEEE T MED IMAGING, V21, P1212, DOI 10.1109/TMI.2002.806292
   ZIMMER C, 2005, COMPUTER VISION BIOM, V3765
NR 46
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1055
EP 1069
DI 10.1007/s00371-011-0655-y
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500003
DA 2024-07-18
ER

PT J
AU Faruquie, TA
   Banerjee, S
   Kalra, P
AF Faruquie, T. A.
   Banerjee, S.
   Kalra, P.
TI Latent topic model-based group activity discovery
SO VISUAL COMPUTER
LA English
DT Article
DE Group activity; Surveillance; Topic models; Activity discovery
AB Surveillance videos of public places often consist of group activities composed from multiple co-occurring individual activities. However, latent topic models, such as Latent Dirichlet Allocation (LDA), which have been successfully used to discover individual activities, do not discover group activities. In this paper we propose a method to discover group activities along with individual activities. We use a two layer latent structure where a latent variable is used to discover correlation of individual activities as a group activity using multinomial distribution. Each individual activity is in turn represented as a distribution over local visual features. We use a Gibbs sampling-based algorithm to jointly infer the individual and group activities. Our method can summarize not only the individual activities but also the common group activities in a video. We demonstrate the strength of our method by discovering activities and the salient correlation amongst them in real life videos of crowded public places.
C1 [Faruquie, T. A.; Banerjee, S.; Kalra, P.] Indian Inst Technol Delhi, Dept Comp Sci & Engn, New Delhi 110016, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Delhi
RP Faruquie, TA (corresponding author), Indian Inst Technol Delhi, Dept Comp Sci & Engn, Hauz Khas, New Delhi 110016, India.
EM tanveer@cse.iitd.ac.in; suban@cse.iitd.ac.in; pkalra@cse.iitd.ac.in
CR [Anonymous], ICVGIP
   [Anonymous], P CVPR
   [Anonymous], BMVC
   [Anonymous], 2005, VIS SURV PERF EV TRA
   [Anonymous], 2008, BMVC
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], COMPUT VIS IMAGE UND
   [Anonymous], P 7 IND C COMP VIS G
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BRAND M., 1997, CVPR
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hongeng S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P84, DOI 10.1109/ICCV.2001.937608
   Hospedales T, 2009, IEEE I CONF COMP VIS, P1165, DOI 10.1109/ICCV.2009.5459342
   Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Ronning G., 1989, J STAT COMPUT SIM, V32, P215, DOI DOI 10.1080/00949658908811178
   Savarese S., 2008, IEEEWORKSHOP MOTION, P1
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Vitaladevuni ShivN., 2008, CVPR
   Wang XG, 2006, LECT NOTES COMPUT SC, V3953, P110, DOI 10.1007/11744078_9
   Wang XG, 2010, IEEE T PATTERN ANAL, V32, P56, DOI 10.1109/TPAMI.2008.241
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43
NR 24
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2011
VL 27
IS 12
SI SI
BP 1071
EP 1082
DI 10.1007/s00371-011-0652-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YM
UT WOS:000298994500004
DA 2024-07-18
ER

PT J
AU Sherstyuk, A
   Jay, C
   Treskunov, A
AF Sherstyuk, Andrei
   Jay, Caroline
   Treskunov, Anton
TI Impact of hand-assisted viewing on user performance and learning
   patterns in virtual environments
SO VISUAL COMPUTER
LA English
DT Article
DE View sliding; Non-isomorphic camera control; User study; VR; Learning
   patterns; Variance in learning curves
ID PERCEPTUAL STABILITY; HEAD MOVEMENTS
AB The ability to locate, select and interact with objects is fundamental to most Virtual Reality (VR) applications. Recently, it was demonstrated that the virtual hand metaphor, a technique commonly used for these tasks, can also be employed to control the virtual camera, resulting in improved performance and user evaluation in visual search tasks.
   In this work, we further investigate the effects of hand-assisted viewing on user behavior in immersive virtual environments. We demonstrate that hand-assisted camera control significantly changes the way how people operate their virtual hands, on motor, cognitive, and behavioral levels.
C1 [Sherstyuk, Andrei] Avatar Reality Inc, Honolulu, HI 96813 USA.
   [Jay, Caroline] Univ Manchester, Human Centered Web Lab, Sch Comp Sci, Manchester M13 9PL, Lancs, England.
   [Treskunov, Anton] Samsung Informat Syst Amer, San Jose, CA 95134 USA.
C3 University of Manchester; Samsung
RP Sherstyuk, A (corresponding author), Avatar Reality Inc, 55 Merchant St,Ste 1700, Honolulu, HI 96813 USA.
EM andrei@avatar-reality.com; caroline.jay@manchester.ac.uk;
   anton.t@sisa.samsung.com
OI Jay, Caroline/0000-0002-6080-1382
CR [Anonymous], 2008, P 16 EUR S VIRT ENV
   BIGUER B, 1982, EXP BRAIN RES, V46, P301
   BOGER Y, 2010, ARE EXISTING HEAD MO
   BOLAS M, 2006, DISPLAY RES U SO CAL
   Bowman Doug, 2004, 3D user interfaces: Theory and practice
   Britton EdwardG., 1978, ACM SIGGRAPH Computer Graphics, V12, P222
   Bungert C., HMD HEADSET VR HELME
   Hinckley K., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P1, DOI 10.1145/263407.263408
   JACOBY R, 1994, P SOC PHOTO-OPT INS, V2177, P355, DOI 10.1117/12.173892
   Jaekl P, 2003, J VESTIBUL RES-EQUIL, V13, P265
   Jaekl PM, 2002, P IEEE VIRT REAL ANN, P149, DOI 10.1109/VR.2002.996517
   Jay C, 2003, PRESENCE-VIRTUAL AUG, V12, P268, DOI 10.1162/105474603765879521
   KIYOKAWA K, 2007, P ISMAR 07 NAR JAP
   Land M.F., 2004, VISUAL NEUROSCIENCES, P1357
   LAVIOLA J, 2001, P ACM SIGGRAPH I3D S
   LaViola JJ, 2007, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2007, PROCEEDINGS, P49
   NEALE D, 1998, HCIL9802 DEP IND SYS
   POUPYREV I, 2000, P SIGCHI C HUM FACT, P540
   Poupyrev I., 1996, P 9 ANN ACM S USER I, P79, DOI [DOI 10.1145/237091.237102, 10.1145/237091.237102]
   ROSENBAUM D, 1991, HUMAN MOTOR CONTROL, P253
   SHERSTYUK A, 2007, P IEEE 3DUI
   SHERSTYUK A, 2008, P ICAT YOK DC 1 3
   Vincent DS, 2008, ACAD EMERG MED, V15, P1160, DOI 10.1111/j.1553-2712.2008.00191.x
NR 23
TC 1
Z9 1
U1 1
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2011
VL 27
IS 3
BP 173
EP 185
DI 10.1007/s00371-010-0516-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 722QS
UT WOS:000287450000001
DA 2024-07-18
ER

PT J
AU Song, P
   Wu, XJ
   Wang, MY
AF Song, Peng
   Wu, Xiaojun
   Wang, Michael Yu
TI Volumetric stereo and silhouette fusion for image-based modeling
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-view stereo; Depth map; Oriented point cloud; Visual hull
ID MULTIVIEW STEREO; RECONSTRUCTION; OBJECTS; SHAPE
AB This paper presents a volumetric stereo and silhouette fusion algorithm for acquiring high quality models from multiple calibrated photographs. Our method is based on computing and merging depth maps. Different from previous methods of this category, the silhouette information is also applied in our algorithm to recover the shape information on the textureless and occluded areas. The proposed algorithm starts by computing visual hull using a volumetric method in which a novel projection test method is proposed for visual hull octree construction. Then, the depth map of each image is estimated by an expansion-based approach that returns a 3D point cloud with outliers and redundant information. After generating an oriented point cloud from stereo by rejecting outlier, reducing scale, and estimating surface normal for the depth maps, another oriented point cloud from silhouette is added by carving the visual hull octree structure using the point cloud from stereo to restore the textureless and occluded surfaces. Finally, Poisson Surface Reconstruction approach is applied to convert the oriented point cloud both from stereo and silhouette into a complete and accurate triangulated mesh model. The proposed approach has been implemented and the performance of the approach is demonstrated on several real data sets, along with qualitative comparisons with the state-of-the-art image-based modeling techniques according to the Middlebury benchmark.
C1 [Song, Peng; Wu, Xiaojun] Harbin Inst Technol, Shenzhen Grad Sch, Div Control & Mechatron Engn, Shenzhen 518055, Peoples R China.
   [Wang, Michael Yu] Chinese Univ Hong Kong, Dept Mech & Automat Engn, Shatin, Hong Kong, Peoples R China.
C3 Harbin Institute of Technology; Chinese University of Hong Kong
RP Song, P (corresponding author), Harbin Inst Technol, Shenzhen Grad Sch, Div Control & Mechatron Engn, Shenzhen 518055, Peoples R China.
EM songpenghit@163.com; wuxj@hitsz.edu.cn; yuwang@mae.cuhk.edu.cn
RI Song, Peng/ABH-5214-2020; chen, jia/A-3560-2011; Wang,
   Michael/C-2219-2009; Song, Peng/ABE-7649-2020
OI Wang, Michael/0000-0002-6524-5741; Song, Peng/0000-0003-2734-2783
FU Natural Science Foundation of China [NSFC50805031]; Science & Technology
   Research Projects of Shenzhen [JC200903120184A, ZYC200903230062A];
   Foundation of the State Key Lab of Digital Manufacturing Equipment
   Technology [DMETKF2009013]
FX This project is partially supported by Natural Science Foundation of
   China (NSFC50805031) and Science & Technology Research Projects of
   Shenzhen (Nos. JC200903120184A, ZYC200903230062A), Foundation of the
   State Key Lab of Digital Manufacturing Equipment & Technology (No.
   DMETKF2009013).
CR [Anonymous], 2007, ICCV
   [Anonymous], EUR C COMP VIS
   [Anonymous], POISSON SURFACE RECO
   BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0
   Bradley D., 2008, CVPR
   Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016
   Furukawa Y., 2007, CVPR
   GOESELE M, 2006, CVPR, P2402
   Habbecke M., 2007, CVPR
   HERNANDEZ C, 2002, 3DPVT, P159
   Hoppe Hugues, 1992, P SIGGRAPH 92, P71, DOI DOI 10.1145/133994.134011
   Jarvis R. A., 1973, Information Processing Letters, V2, P18, DOI 10.1016/0020-0190(73)90020-3
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lhuillier M, 2002, IEEE T PATTERN ANAL, V24, P1140, DOI 10.1109/TPAMI.2002.1023810
   Liu YB, 2009, PROC CVPR IEEE, P2121, DOI [10.1109/CVPRW.2009.5206712, 10.1109/CVPR.2009.5206712]
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Matusik W, 2001, SPRING EUROGRAP, P115
   Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694
   Peng Song, 2009, 2009 International Conference on Information and Automation (ICIA), P784, DOI 10.1109/ICINFA.2009.5205027
   Pons JP, 2005, PROC CVPR IEEE, P822
   POTMESIL M, 1987, COMPUT VISION GRAPH, V40, P1, DOI 10.1016/0734-189X(87)90053-3
   Prados E, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P826
   Seitz S.M., 2006, IEEE COMP SOC C COMP, P519
   SINHA SN, 2007, ICCV
   SZELISKI R, 1993, CVGIP-IMAG UNDERSTAN, V58, P23, DOI 10.1006/ciun.1993.1029
   Tarini M, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P283
   Vogiatzis G, 2005, PROC CVPR IEEE, P391
   Vogiatzis G, 2007, IEEE T PATTERN ANAL, V29, P2241, DOI 10.1109/TPAMI.2007.70712
   Vu H., 2009, CVPR
   Yemez Y, 2007, COMPUT VIS IMAGE UND, V105, P30, DOI 10.1016/j.cviu.2006.07.008
   Yemez Y, 2004, IMAGE VISION COMPUT, V22, P1137, DOI 10.1016/j.imavis.2004.06.001
   BIGHEAD SEQUENCE
   3D PHOTOGRAPHY DATA
   DINO TEMPLE DATA SET
NR 36
TC 19
Z9 24
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2010
VL 26
IS 12
BP 1435
EP 1450
DI 10.1007/s00371-010-0429-y
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 678WA
UT WOS:000284112400002
DA 2024-07-18
ER

PT J
AU Mustafa, G
   Hashmi, MS
AF Mustafa, Ghulam
   Hashmi, Muhammad Sadiq
TI Subdivision depth computation for n-ary subdivision curves/surfaces
SO VISUAL COMPUTER
LA English
DT Article
DE Subdivision curve; Subdivision surfaces; Subdivision depth; Error bound;
   Control polygon; Forward differences
ID ESTIMATING ERROR-BOUNDS
AB This paper deals with subdivision depth computation technique for n-ary subdivision curves/surfaces. This technique also includes error bound evaluation technique for n-ary subdivision curves/surfaces with their control polygon. Both techniques provide error control tools in subdivision schemes.
C1 [Mustafa, Ghulam; Hashmi, Muhammad Sadiq] Islamia Univ Bahawalpur, Dept Math, Bahawalpur, Pakistan.
C3 Islamia University of Bahawalpur
RP Mustafa, G (corresponding author), Islamia Univ Bahawalpur, Dept Math, Bahawalpur, Pakistan.
EM mustafa_rakib@yahoo.com; sadiq.hashmi@gmail.com
RI Hashmi, Muhammad Sadiq/AFM-1209-2022; Mustafa, Ghulam/ABH-1050-2020;
   Mustafa, Ghulam/IQV-2174-2023
OI Hashmi, Muhammad Sadiq/0000-0003-1957-5077; Mustafa,
   Ghulam/0000-0002-0441-8485; Mustafa, Ghulam/0000-0003-4467-2987
FU Higher Education Commission (HEC) Pakistan
FX This work is supported by the Indigenous Ph.D. Scholarship Scheme of
   Higher Education Commission (HEC) Pakistan.
CR ASPERT N, 2003, THESIS ECOLE POLYTEC
   Cheng F., 2006, Computer-Aided Design and Applications, V3, P485
   Hashmi S, 2009, J MATH ANAL APPL, V358, P159, DOI 10.1016/j.jmaa.2009.04.050
   Huang ZJ, 2008, COMPUT AIDED GEOM D, V25, P457, DOI 10.1016/j.cagd.2008.05.002
   KO KP, 2007, STUDY SUBDI IN PRESS
   Lian JA, 2008, APPL APPL MATH, V3, P176
   Lian JA, 2008, APPL APPL MATH, V3, P18
   Mustafa G, 2007, J COMPUT MATH, V25, P473
   Mustafa G, 2006, INT J COMPUT MATH, V83, P879, DOI 10.1080/00207160601117263
   Mustafa G, 2006, J COMPUT APPL MATH, V193, P596, DOI 10.1016/j.cam.2005.06.030
   Mustafa G, 2009, ABSTR APPL ANAL, DOI 10.1155/2009/301967
   NAJMA AR, 2009, THESIS ISLAMIA U BAH
   Peters J, 2009, J APPROX THEORY, V161, P491, DOI 10.1016/j.jat.2008.10.012
   Wang HW, 2004, J COMPUT SCI TECH-CH, V19, P657, DOI 10.1007/BF02945592
   Wang Yu, 2004, Cancer Immun, V4, P11
   Zeng XM, 2006, J COMPUT APPL MATH, V195, P252, DOI 10.1016/j.cam.2005.05.035
   王红, 2002, [自然科学进展, Progress in Natural Science], V12, P697
NR 17
TC 12
Z9 12
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 841
EP 851
DI 10.1007/s00371-010-0496-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800044
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, ZF
   Shen, Y
   Ma, LZ
   Chen, ZH
AF Xie, Zhi-Feng
   Shen, Yang
   Ma, Li-Zhuang
   Chen, Zhi-Hua
TI Seamless video composition using optimized mean-value cloning
SO VISUAL COMPUTER
LA English
DT Article
DE Mean-value coordinates; Matting and composition; Video processing
AB As the process of pasting a source video patch into a target video sequence, seamless video composition is an important and useful video editing operation. Recently, a novel composition approach based on Mean-Value Coordinates has been presented. However, its composition result is often degraded by smudging and discoloration artifacts. Thus we propose optimized mean-value cloning to eliminate these artifacts by matting technique and interpolation constraint. On the basis of this optimized approach, we further present a new framework for seamless video composition. In the framework, we first develop a propagation model based on contour flow to yield each trimap that provides each frame with a pre-segmentation: definite foreground, definite background and unknown. This propagation model constructs the contour flow of inter-frame by minimizing a cost function, and employs it to relabel the trimap. Moreover, when the trimap propagation model is inefficient due to abrupt feature change and complex scene pattern, our framework has also implemented a convenient interactive tool to create and modify trimap. Then, we can use the high-quality trimap to achieve the optimized mean-value cloning. Experimental results demonstrate the effectiveness of our seamless video composition framework.
C1 [Xie, Zhi-Feng; Ma, Li-Zhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Digital Media Technol & Data Reconstruct Lab, Shanghai 200030, Peoples R China.
   [Chen, Zhi-Hua] E China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai 200237, Peoples R China.
   [Shen, Yang] Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University; East China University of Science &
   Technology; Shanghai Jiao Tong University
RP Xie, ZF (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Digital Media Technol & Data Reconstruct Lab, Shanghai 200030, Peoples R China.
EM zhifeng_xie@sjtu.edu.cn
FU National Basic Research Project of China [2006CB303105]; National High
   Technology Research and Development Program of China [2009AA01Z334];
   National Natural Science Foundation of China [60873136]
FX We would like to thank the anonymous reviewers for their valuable
   comments. This work was supported by the National Basic Research Project
   of China (Project Number 2006CB303105), the National High Technology
   Research and Development Program of China (Project Number 2009AA01Z334)
   and the National Natural Science Foundation of China (Project Number
   60873136).
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   AGARWALA A, 2004, SIGGRAPH 04 ACM SIGG, P584
   Agarwala A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239545, 10.1145/1276377.1276495]
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   BOLZ J, 2005, SIGGRAPH 2005, P171
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Chuang YY, 2002, ACM T GRAPHIC, V21, P243, DOI 10.1145/566570.566572
   FARBMAN Z, 2009, SIGGRAPH 09, P1
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   GELAUTZ C, 2008, P BRIT MACH VIS C LE, P1155
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Grauman K, 2004, PROC CVPR IEEE, P220
   Guan Y, 2006, COMPUT GRAPH FORUM, V25, P567, DOI 10.1111/j.1467-8659.2006.00976.x
   Jia JY, 2005, IEEE I CONF COMP VIS, P1651
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Kazhdan M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360620
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   LEVIN A, 2006, IEEE COMP SOC C COMP
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Lucas B. D., 1981, P IJCAI, P674
   McCarey BE, 2008, CORNEA, V27, P1, DOI 10.1097/ICO.0b013e31815892da
   MISHIMA Y, 1994, SOFT EDGE CHROMA KEY
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   RUZON MA, 2000, ALPHA ESTIMATION NAT
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Szeliski R, 2006, ACM T GRAPHIC, V25, P1135, DOI 10.1145/1141911.1142005
   WACHPRESS EL, 1975, RATIONAL FINITE ELEM
   Wang HC, 2004, INT C PATT RECOG, P858, DOI 10.1109/ICPR.2004.1334663
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Warren J, 1996, ADV COMPUT MATH, V6, P97, DOI 10.1007/BF02127699
   2002, C CORPORATION KNOCKO
NR 36
TC 17
Z9 20
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 1123
EP 1134
DI 10.1007/s00371-010-0466-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800071
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Nie, YW
   Hua, W
   Zheng, WT
AF Xiao, Chunxia
   Nie, Yongwei
   Hua, Wei
   Zheng, Wenting
TI Fast multi-scale joint bilateral texture upsampling
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Global optimization; Image upsampling; Bilateral
   filter
ID IMAGE; PHOTOGRAPHY; COMPLETION; APPEARANCE; FLASH
AB We present a new approach using a multi-scale joint bilateral filter for upsampling the synthesized texture generated by optimization-based methods. Our method is based on the following motivation: if the available exemplar texture is used as a priority to upsample the synthesized texture, a high resolution result that prevents image blurring can be obtained. Our multi-scale joint bilateral upsampling applies a spatial filter on each multi-scale layer of the synthesized texture, and jointly applies a similar range filter on exemplar texture, which guides the interpolation from low to high resolution, by magnifying and combining the upsampled information; the details of the upsampled texture are progressively enhanced, and the image blurring artifacts can be effectively avoided. We offer an accelerated joint bilateral filter, which enables our upsampling method to interactively generate a large texture. In addition, we propose a detail-aware texture optimization approach that incorporates image detail in texture optimization to improve the quality of the synthesized texture, on which the multi-scale joint bilateral filter works to generate a more convincing result. We show results for upsampling image and video textures and compare them to traditional upsampling methods, by this demonstrating that with low computational and memory costs, our method achieves better results.
C1 [Xiao, Chunxia; Nie, Yongwei] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Hua, Wei; Zheng, Wenting] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Wuhan University; Zhejiang University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
EM cxxiao@whu.edu.cn; nieyongwei@gmail.com; huawei@cad.zju.edu.cn;
   wtzheng@cad.zju.edu.cn
RI zhang, jt/JVE-1333-2024
FU NSFC [60803081]; CADCG [A0808]; State Key Laboratory of Software
   Engineering (SKLSE) [SKLSE2008-07-08]; Ministry of Education of China
   [200804861038]; Natural Science Foundation of Hubei Province
   [2008CDB350]; National High Technology Research and Development Program
   of China (863 Program) [2008AA121603]
FX This work was partly supported by NSFC (No. 60803081), State Key Lab of
   CAD&CG (No. A0808), State Key Laboratory of Software Engineering (SKLSE)
   (No. SKLSE2008-07-08), Ph. D. Programs Foundation of Ministry of
   Education of China (No. 200804861038), Natural Science Foundation of
   Hubei Province (2008CDB350), National High Technology Research and
   Development Program of China (863 Program) (No. 2008AA121603).
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI [10.1109/CVPR.2008.4587843, DOI 10.1109/CVPR.2008.4587843]
   [Anonymous], ACM T GRAPHICS
   [Anonymous], ACM T GRAPH
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   DEBONET JS, 1997, SIGGRAPH 97, P361, DOI DOI 10.1145/258734.258882
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   FARBMAN Z, 2008, SIGGRAPH 2008
   FATTAL R, 2007, ACM T GRAPH TOG, V26
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229, DOI 10.1145/218380.218446
   KOPF J, 2007, ACM SIGGRAPH 2007, P21
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P541, DOI 10.1145/1141911.1141921
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   McLachlan G. J., 1997, The EM Algorithm and Extensions, V473, P486
   Mount D.M., 2006, Ann: A library for approximate nearest neighbor searching
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Su D, 2004, COMPUT GRAPH FORUM, V23, P189, DOI 10.1111/j.1467-8659.2004.00752.x
   Thévenaz P, 2000, BIOMED EN S, P393
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360651
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu Q, 2004, ACM T GRAPHIC, V23, P364, DOI 10.1145/1015706.1015730
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
   Zhang JD, 2003, ACM T GRAPHIC, V22, P295, DOI 10.1145/882262.882266
NR 32
TC 7
Z9 11
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2010
VL 26
IS 4
SI SI
BP 263
EP 275
DI 10.1007/s00371-009-0409-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 584JA
UT WOS:000276746600004
DA 2024-07-18
ER

PT J
AU Chen, LH
   Tsai, TC
   Chen, YS
AF Chen, Lieu-Hen
   Tsai, Tsung-Chih
   Chen, Yu-Sheng
TI Grouped photon mapping
SO VISUAL COMPUTER
LA English
DT Article
DE Photon mapping; Density estimation; Global illumination; Caustics
ID MAPS
AB This paper proposes a novel architecture called Grouped Photon Mapping, which combines standard photon mapping with the light-beam concept to improve the nearest-neighbor density estimation method. Based on spatial coherence, we cluster all of photons, which are deposited in the photon map, into different beam-like groups. Each group of photons is individually stored in an isolated photon map. By the distribution of the photons in each photon map, we construct a polygonal boundary to represent a beam-like illuminated area. These boundaries offer a more accurate and flexible sampling area to filter neighbor photons around the query point. In addition, by a level of detail technique, we can control the photon-count in each group to obtain a balance between biases and noise. The results of our experiments prove that our method can successfully reduce bias errors and light leakage. Especially, for complicated caustic effects through a gemstone-like object, we can render a smoother result than standard photon mapping.
C1 [Chen, Lieu-Hen; Tsai, Tsung-Chih; Chen, Yu-Sheng] Natl Chi Nan Univ, Dept Comp Sci & Informat Engn, Nantou 54561, Hsien, Taiwan.
C3 National Chi Nan University
RP Chen, LH (corresponding author), Natl Chi Nan Univ, Dept Comp Sci & Informat Engn, 1 Univ Rd, Nantou 54561, Hsien, Taiwan.
EM lhchen@csie.ncnu.edu.tw; s321901@ncnu.edu.tw; s96321512@ncnu.edu.tw
CR GARLAND M, P SIGGRAPH 97, P209
   Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2
   HAVRAN V, P EUR S REND 2005, P43
   Herzog R, 2007, COMPUT GRAPH FORUM, V26, P503, DOI 10.1111/j.1467-8659.2007.01073.x
   HEY H, 2001, TR18620104 VIENN U T
   Jensen Henrik Wann., 1996, Rendering Techniques (Proceedings of the Eurographics Workshop on Rendering), P21
   JENSEN HW, 1995, COMPUT GRAPH, V19, P215, DOI 10.1016/0097-8493(94)00145-O
   LASTRA M, 2002, P 13 EUR WORKSH REND, P33
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Schregle R, 2003, COMPUT GRAPH FORUM, V22, P729, DOI 10.1111/j.1467-8659.2003.00720.x
   TOBLER RF, P WSCG 2006, P257
NR 11
TC 4
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2010
VL 26
IS 3
BP 217
EP 226
DI 10.1007/s00371-009-0397-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 558CW
UT WOS:000274719200005
DA 2024-07-18
ER

PT J
AU Oberhofer, K
   Mithraratne, K
   Stott, NS
   Anderson, IA
AF Oberhofer, Katja
   Mithraratne, Kumar
   Stott, Ngaire S.
   Anderson, Iain A.
TI Anatomically-based musculoskeletal modeling: prediction and validation
   of muscle deformation during walking
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE Host mesh fitting; Free form deformation; Gait analysis; Magnetic
   resonance imaging
ID SKELETAL-MUSCLE; RECTUS FEMORIS; TENDON LENGTHS; VELOCITIES; HAMSTRINGS;
   ANATOMY; SYSTEM
AB Accurate modeling of the musculoskeletal system during motion is a challenging task that has not yet been solved. In this paper, we outline and validate a free-form deformation method called the Host Mesh Fitting (HMF) technique for predicting muscle deformation during walking of a subject-specific musculoskeletal model. 20 lower limb muscles were deformed according to the HMF solution of a surrounding host mesh that resembled the skin boundary, resulting in a realistic walking simulation of the anatomically-based model. The shape changes of five muscles were further validated by comparing the predicted deformations with magnetic resonance image data in two lower limb positions.
C1 [Oberhofer, Katja; Mithraratne, Kumar; Anderson, Iain A.] Univ Auckland, Bioengn Inst, Auckland 1, New Zealand.
   [Stott, Ngaire S.] Univ Auckland, Dept Surg, Auckland 1, New Zealand.
C3 University of Auckland; University of Auckland
RP Oberhofer, K (corresponding author), Univ Auckland, Bioengn Inst, 70 Symonds St, Auckland 1, New Zealand.
EM k.oberhofer@auckland.ac.nz
OI Mithraratne, Kumar/0000-0001-8090-8876; Oberhofer,
   Katja/0000-0001-5469-9951
CR Anderson FC, 2001, J BIOMECH ENG-T ASME, V123, P381, DOI 10.1115/1.1392310
   [Anonymous], 2008, ACM SIGGRAPH 2008 PA
   Arnold A S, 2000, Comput Aided Surg, V5, P108, DOI 10.1002/1097-0150(2000)5:2<108::AID-IGS5>3.0.CO;2-2
   Arnold AS, 2006, J BIOMECH, V39, P1498, DOI 10.1016/j.jbiomech.2005.03.026
   Arnold AS, 2006, GAIT POSTURE, V23, P273, DOI 10.1016/j.gaitpost.2005.03.003
   AUBEL A, 2001, C P COMPUTER ANIMATI
   Blemker SS, 2006, J BIOMECH, V39, P1383, DOI 10.1016/j.jbiomech.2005.04.012
   Blemker SS, 2005, ANN BIOMED ENG, V33, P661, DOI 10.1007/s10439-005-1433-7
   Bradley CP, 1997, ANN BIOMED ENG, V25, P96, DOI 10.1007/BF02738542
   Damsgaard M, 2006, SIMUL MODEL PRACT TH, V14, P1100, DOI 10.1016/j.simpat.2006.09.001
   DELP SL, 1995, COMPUT BIOL MED, V25, P21, DOI 10.1016/0010-4825(95)98882-E
   Dong F, 2002, IEEE T VIS COMPUT GR, V8, P154, DOI 10.1109/2945.998668
   Fernandez JW, 2004, BIOMECH MODEL MECHAN, V2, P139, DOI [10.1007/s10237-003-0036-1, 10.1007/S10237-003-0036-1]
   Fernandez JW, 2005, BIOMECH MODEL MECHAN, V4, P39, DOI 10.1007/s10237-005-0071-1
   Horsman MDK, 2007, CLIN BIOMECH, V22, P239, DOI 10.1016/j.clinbiomech.2006.10.003
   Jonkers I, 2006, GAIT POSTURE, V23, P222, DOI 10.1016/j.gaitpost.2005.02.005
   Kalra P, 1998, IEEE COMPUT GRAPH, V18, P42, DOI 10.1109/38.708560
   Lemos RR, 2005, COMPUT ANIMAT VIRT W, V16, P319, DOI 10.1002/cav.83
   Lieber RL, 2000, MUSCLE NERVE, V23, P1647, DOI 10.1002/1097-4598(200011)23:11<1647::AID-MUS1>3.0.CO;2-M
   Magnenat-Thalmann N, 2005, VISUAL COMPUT, V21, P997, DOI 10.1007/s00371-005-0363-6
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   Scheepers F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P163, DOI 10.1145/258734.258827
   Sederberg T. W., 1986, Computer Graphics, V20, P151, DOI 10.1145/15886.15903
   Sutherland DH, 2002, GAIT POSTURE, V16, P159, DOI 10.1016/S0966-6362(02)00004-8
   Teran J, 2005, IEEE T VIS COMPUT GR, V11, P317, DOI 10.1109/TVCG.2005.42
   Vasavada AN, 2008, J BIOMECH, V41, P1450, DOI 10.1016/j.jbiomech.2008.02.027
NR 26
TC 13
Z9 14
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 843
EP 851
DI 10.1007/s00371-009-0314-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100004
DA 2024-07-18
ER

PT J
AU Raunhardt, D
   Boulic, R
AF Raunhardt, Daniel
   Boulic, Ronan
TI Motion constraint
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Inverse kinematics; Motion editing; Posture control
ID POSTURES
AB In this paper, we propose a hybrid postural control approach taking advantage of data-driven and goal-oriented methods while overcoming their limitations. In particular, we take advantage of the latent space characterizing a given motion database. We introduce a motion constraint operating in the latent space to benefit from its much smaller dimension compared to the joint space. This allows its transparent integration into a Prioritized Inverse Kinematics framework. If its priority is high the constraint may restrict the solution to lie within the motion database space. We are more interested in the alternate case of an intermediate priority level that channels the postural control through a spatiotemporal pattern representative of the motion database while achieving a broader range of goals. We illustrate this concept with a sparse database of large range full-body reach motions.
C1 [Raunhardt, Daniel; Boulic, Ronan] Ecole Polytech Fed Lausanne, VRLAB Stn 14, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Raunhardt, D (corresponding author), Ecole Polytech Fed Lausanne, VRLAB Stn 14, CH-1015 Lausanne, Switzerland.
EM daniel.raunhardt@epfl.ch; ronan.boulic@epfl.ch
RI BOULIC, RONAN/A-9108-2008
OI BOULIC, RONAN/0000-0001-9176-6877
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Aydin Y, 1999, COMPUT GRAPH-UK, V23, P145, DOI 10.1016/S0097-8493(98)00122-8
   Baerlocher P, 2004, VISUAL COMPUT, V20, P402, DOI 10.1007/s00371-004-0244-4
   Berthoz Alain., 2002, The Brain's Sense of Movement
   Carvalho SR, 2007, COMPUT ANIMAT VIRT W, V18, P493, DOI 10.1002/cav.210
   Egges A, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P121, DOI 10.1109/PCCGA.2004.1348342
   Glardon P, 2006, VISUAL COMPUT, V22, P194, DOI 10.1007/s00371-006-0376-9
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   GRASSIA FS, 2003, THESIS PITTSBURGH
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Korein JamesU., 1985, GEOMETRIC INVESTIGAT
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kulpa R, 2005, COMPUT GRAPH FORUM, V24, P343, DOI 10.1111/j.1467-8659.2005.00859.x
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Mukai T, 2005, ACM T GRAPHIC, V24, P1062, DOI 10.1145/1073204.1073313
   NAKAMURA Y, 1986, J DYN SYST-T ASME, V108, P163, DOI 10.1115/1.3143764
   Park W, 2004, IEEE T SYST MAN CY A, V34, P376, DOI 10.1109/TSMCA.2003.822965
   Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536
   Rose Charles F., 2001, Computer Graphics Forum, V20, P3
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Shin HJ, 2006, COMPUT ANIMAT VIRT W, V17, P219, DOI 10.1002/cav.125
   Wang X, 1999, J BIOMECH, V32, P453, DOI 10.1016/S0021-9290(99)00023-8
   Wang XG, 1999, BIOL CYBERN, V80, P449, DOI 10.1007/s004220050538
   Weisstein E., 2013, Barycentric Coordinates
   Yamane K, 2004, ACM T GRAPHIC, V23, P532, DOI 10.1145/1015706.1015756
NR 29
TC 13
Z9 18
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 509
EP 518
DI 10.1007/s00371-009-0336-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300015
DA 2024-07-18
ER

PT J
AU Chen, HLJ
   Samavati, FF
   Sousa, MC
AF Chen, Hung-Li Jason
   Samavati, Faramarz F.
   Sousa, Mario Costa
TI GPU-based point radiation for interactive volume sculpting and
   segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE point-based techniques; real-time system; volume segmentation; volume
   cutting
AB Internal structures, features, and properties in volumetric datasets are mostly obscured and hidden. In order to reveal and explore them, appropriate tools are required to remove and carve the occluding materials and isolate and extract different regions of interest. We introduce a framework of interactive tools for real-time volume sculpting and segmentation. We utilize a GPU-based point radiation technique as a fundamental building block to create a collection of high-quality volume manipulation tools for direct drilling, lasering, peeling, and cutting/pasting. In addition, we enable interactive parallel region growing segmentation that allows multiple seed planting by direct sketching on different volumetric regions with segmentation results dynamically modified during the process. We use the same point radiation technique to create high-quality real-time feedback of the segmented regions during the seed growing process. We present results obtained from raw and segmented medical volume datasets.
C1 [Chen, Hung-Li Jason; Samavati, Faramarz F.; Sousa, Mario Costa] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Chen, HLJ (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM hljason.chen@ucalgary.ca
OI Costa Sousa, Mario/0000-0002-4347-0884
CR [Anonymous], 2005, PROC S INTERACTIVE 3
   [Anonymous], 1997, Introduction to Implicit Surfaces
   [Anonymous], 1999, OpenGL programming guide: the official guide to learning OpenGL
   Avila RS, 1996, IEEE VISUAL, P197, DOI 10.1109/VISUAL.1996.568108
   Chen H.-L.Jason., 2006, 3 EUROGRAPHICS WORKS, P123, DOI DOI 10.2312/SBM/SBM06/123-129
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Correa CD, 2006, IEEE T VIS COMPUT GR, V12, P1069, DOI 10.1109/TVCG.2006.144
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   Huff R., 2006, P ACM INT C VIRT REA, P271
   JUSTICE RK, 1996, 18 ANN INT C IEEE P, P1083
   McGuffin MJ, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P401, DOI 10.1109/VISUAL.2003.1250400
   *NVIDIA CORP, 2006, TECHN BRIEF MICR DIR
   PHAM DL, 1999, JHUECE9901
   ROSENFELD A, 1982, DIGITAL PICTURE PROC, V2, P138
   Schenke S., 2005, Image and Vision Computing New Zealand (IVCNZ), P171
   Sherbondy A, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P171, DOI 10.1109/VISUAL.2003.1250369
   Tzeng FY, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P505, DOI 10.1109/VISUAL.2003.1250413
   Wang S. W., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P151, DOI 10.1145/199404.199430
   WATT A, 1989, 3D COMPUTER GRAPHICS
   Weiskopf D, 2003, IEEE T VIS COMPUT GR, V9, P298, DOI 10.1109/TVCG.2003.1207438
   WESTOVER L, 1990, P SIGGRAPH 90, P367
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
NR 23
TC 16
Z9 25
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 689
EP 698
DI 10.1007/s00371-008-0249-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800024
DA 2024-07-18
ER

PT J
AU García-Rojas, A
   Gutiérrez, M
   Thalmann, D
AF Garcia-Rojas, Alejandra
   Gutierrez, Mario
   Thalmann, Daniel
TI Visual creation of inhabited 3D environments
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE inhabited virtual environments; visual programming; authoring tool;
   ontologies
AB The creation of virtual reality applications and 3D environments is a complex task that requires good programming skills and expertise in computer graphics and many other disciplines. The complexity increases when we want to include complex entities such as virtual characters and animate them. In this paper we present a system that assists in the tasks of setting up a 3D scene and configuring several parameters affecting the behavior of virtual entities like objects and autonomous virtual humans.
   Our application is based on a visual programming paradigm, supported by a semantic representation, an ontology for virtual environments. The ontology allows us to store and organize the components of a 3D scene, together with the knowledge associated with them. It is also used to expose functionalities in the given 3D engine. Based on a formal representation of its components, the proposed architecture provides a scalable VR system.
   Using this system, non-experts can set up interactive scenarios with minimum effort; no programming skills or advanced knowledge is required.
C1 [Garcia-Rojas, Alejandra; Thalmann, Daniel] EPFL, Virtual Real Lab, Zurich, Switzerland.
   [Gutierrez, Mario] INAOE, Puebla, Mexico.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Instituto Nacional de Astrofisica, Optica y
   Electronica
RP García-Rojas, A (corresponding author), EPFL, Virtual Real Lab, Zurich, Switzerland.
EM alejandra.garciarojas@epfl.ch; mgutierrez@ccc.inaoep.mx;
   daniel.thalmann@epfl.ch
RI Thalmann, Daniel/A-4347-2008; Thalmann, Daniel/AAL-1097-2020
OI Thalmann, Daniel/0000-0002-0451-7491
CR CELENTANO A, 2001, ICHIM, P315
   Cera CD, 2002, IEEE COMPUT GRAPH, V22, P43, DOI 10.1109/MCG.2002.999787
   COSTAGLIOLA G, 2002, SEKE 02, P601, DOI DOI 10.1145/568760.568865
   Green M., 2003, VRST 03, P117
   Gutiérrez M, 2007, VISUAL COMPUT, V23, P207, DOI 10.1007/s00371-006-0093-4
   GUTIERREZ M, 2005, THESIS EPFL LAUSANNE
   KALOGERAKIS E, 2006, VR 06 P IEEE VIRT RE, P6
   Klesen M., 2003, Virtual Reality, V7, P17, DOI 10.1007/s10055-003-0113-x
   McBride B, 2002, IEEE INTERNET COMPUT, V6, P55, DOI 10.1109/MIC.2002.1067737
   *NETB, VIS LIB
   PERLIN K, 1996, COMPUTER GRAPHICS, V30, P205
   Peternier A, 2006, LECT NOTES COMPUT SC, V3942, P223, DOI 10.1007/11736639_31
   Ponder M, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P96
   *PROT, 2005 STANF MED INF
   *W3C, OWL WEB ONT LANG
NR 15
TC 3
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 719
EP 726
DI 10.1007/s00371-008-0252-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800027
DA 2024-07-18
ER

PT J
AU Winkler, T
   Hormann, K
   Gotsman, C
AF Winkler, Tim
   Hormann, Kai
   Gotsman, Craig
TI Mesh massage - A versatile mesh optimization framework
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 26th International Conference on Computer Graphics
CY JUN 09-11, 2008
CL Istanbul, TURKEY
DE mesh optimization; Laplacian smoothing; Hausdorff distance;
   parameterization; remeshing
AB We present a general framework for post-processing and optimizing surface meshes with respect to various target criteria. On the one hand, the framework allows us to control the shapes of the mesh triangles by applying simple averaging operations; on the other hand we can control the Hausdorff distance to some reference geometry by minimizing a quadratic energy. Due to the simplicity of this setup, the framework is efficient and easy to implement, yet it also constitutes an effective and versatile tool with a variety of possible applications. In particular, we use it to reduce the texture distortion in animated mesh sequences, to improve the results of cross-parameterizations, and to minimize the distance between meshes and their remeshes.
C1 [Winkler, Tim; Hormann, Kai] Tech Univ Clausthal, Clausthal Zellerfeld, Germany.
   [Gotsman, Craig] Technion Israel Inst Technol, Haifa, Israel.
C3 TU Clausthal; Technion Israel Institute of Technology
RP Hormann, K (corresponding author), Tech Univ Clausthal, Clausthal Zellerfeld, Germany.
EM tim.winkler@tu-clausthal.de; kai.hormann@tu-clausthal.de;
   gotsman@cs.technion.ac.il
OI Gotsman, Craig/0000-0001-8579-3588; Hormann, Kai/0000-0001-6455-4246
CR Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   ANUAR N, 2004, P 9 INT FALL WORKSH, P63
   Charpiat G, 2007, INT J COMPUT VISION, V73, P325, DOI 10.1007/s11263-006-9966-2
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   DYN N, 2000, MATH METHODS CURVES, P135
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   GARLAND M, P EUR STAR STAT EUR, P111
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lawson C. L., 1961, Contributions to the theory of linear least maximum approximation
   Liu LG, 2007, COMPUT AIDED DESIGN, V39, P772, DOI 10.1016/j.cad.2007.03.004
   Marez C., 2003, P EUR ACM SIGGRAPH S, V30, P20
   Meyer M., 2002, Journal of Graphics Tools, V7, P13, DOI 10.1080/10867651.2002.10487551
   MOTZKIN TS, 1959, T AM MATH SOC, V91, P231
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Ohtake Y, 2001, COMPUT GRAPH FORUM, V20, pC368, DOI 10.1111/1467-8659.00529
   Owens SR, 2002, EMBO REP, V3, P11, DOI 10.1093/embo-reports/kvf019
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Pottmann H, 2006, INT J COMPUT VISION, V67, P277, DOI 10.1007/s11263-006-5167-2
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   SURAZHSKY, 2003, P 12 INT MESH ROUND, P215
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Toledo S., TAUCS LIB SPARSE LIN
   vanDamme R, 1995, MATHEMATICAL METHODS FOR CURVES AND SURFACES, P517
   Wachspress E.L., 1975, A rational finite element basis
NR 31
TC 6
Z9 6
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2008
VL 24
IS 7-9
BP 775
EP 785
DI 10.1007/s00371-008-0259-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 322OI
UT WOS:000257384800033
DA 2024-07-18
ER

PT J
AU Dey, TK
   Levine, JA
AF Dey, Tamal K.
   Levine, Joshua A.
TI Delaunay meshing of isosurfaces
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 9th International Conference on Shape Modeling and Applications
CY JUN 13-15, 2007
CL Lyon, FRANCE
SP ACM SIGRAPT, CNRS, Groupement Rech Informat Mathemat, Reg Rhone Alpes, Univ Claude Bernard Lyon 1
DE Delaunay refinement; isosurfaces; mesh generation
ID SURFACE
AB We present an isosurface meshing algorithm, DeLIso, based on the Delaunay refinement paradigm. This paradigm has been successfully applied to mesh a variety of domains with guarantees for topology, geometry, mesh gradedness, and triangle shape. A restricted Delaunay triangulation, dual of the intersection between the surface and the three-dimensional Voronoi diagram, is often the main ingredient in Delaunay refinement. Computing and storing three-dimensional Voronoi/Delaunay diagrams become bottlenecks for Delaunay refinement techniques since isosurface computations generally have large input datasets and output meshes. A highlight of our algorithm is that we find a simple way to recover the restricted Delaunay triangulation of the surface without computing the full 3D structure. We employ techniques for efficient ray tracing of isosurfaces to generate surface sample points, and demonstrate the effectiveness of our implementation using a variety of volume datasets.
C1 [Dey, Tamal K.; Levine, Joshua A.] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43201 USA.
C3 University System of Ohio; Ohio State University
RP Dey, TK (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43201 USA.
EM tamaldey@cse.ohio-state.edu; levinej@cse.ohio-state.edu
RI Dey, Tamal Krishna/D-4989-2012
OI Levine, Joshua/0000-0002-4302-1704
CR Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Bhaniramka P, 2004, IEEE T VIS COMPUT GR, V10, P130, DOI 10.1109/TVCG.2004.1260765
   Boissonnat J.-D., 2003, Symposium on Geometry Processing, P9
   BOISSONNAT J.-D., 2004, Symposium on Theory of Computing, P301
   Cheng SW, 2007, SIAM J COMPUT, V37, P1199, DOI 10.1137/060665889
   Cheng SW, 2008, PROCEEDINGS OF THE 16TH INTERNATIONAL MESHING ROUNDTABLE, P477, DOI 10.1007/978-3-540-75103-8_27
   Cheng SW, 2005, INT J COMPUT GEOM AP, V15, P421, DOI 10.1142/S0218195905001774
   CHENG SW, 2007, SODA, P1096
   CHERNYAEV EV, 1995, MARCHING CUBES, V33
   Chew LP., 1993, Proceedings of the Ninth Annual Symposium on Computational Geometry, P274, DOI [10.1145/160985.161150, DOI 10.1145/160985.161150]
   DEY T., 2007, CURVE SURFACE RECONS
   DEY TK, 2005, 141MR, P343
   EDELSBRUNNER H, SCG 94, P285
   Havran V., 2000, THESIS CZECH TECHNIC THESIS CZECH TECHNIC
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Oudot S, 2005, Proceedings of the 14th International Meshing Roundtable, P203, DOI 10.1007/3-540-29090-7_12
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   Schwarze J., 1990, GRAPHICS GEMS, P404
   Shewchuk J. R., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P86, DOI 10.1145/276884.276894
   STANDER BT, 1997, SIGGRAPH 97, P279
   VARADHAN G, 2004, EUROGRAPHICS S GEOME, P241
   Wald I, 2005, IEEE T VIS COMPUT GR, V11, P562, DOI 10.1109/TVCG.2005.79
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
NR 23
TC 9
Z9 11
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2008
VL 24
IS 6
BP 411
EP 422
DI 10.1007/s00371-008-0224-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 299FP
UT WOS:000255741800004
DA 2024-07-18
ER

PT J
AU Batagelo, HC
   Wu, ST
AF Batagelo, Harlen Costa
   Wu, Shin-Ting
TI Estimating curvatures and their derivatives on meshes of arbitrary
   topology from sampling directions
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE curvatures; principal directions; differential geometry properties;
   computational aided geometric design
AB Estimation of local differential geometry properties becomes an important processing step in a variety of applications, ranging from shape analysis and recognition to photorealistic image rendering. This paper presents yet another approach to compute those properties, with comparable numerical and accuracy performances to previous works. The key difference in our approach is simplicity, allowing for direct implementation on the GPU. Experimental results are provided to support our statement.
EM harlen@dca.fee.unicamp.br; ting@dca.fee.unicamp.br
RI Wu, Shin Ting/I-8055-2014; Batagelo, Harlen/G-1318-2014
OI Wu, Shin-Ting/0000-0002-3152-4523; Batagelo, Harlen/0000-0002-2325-2070
CR Agam G, 2005, IEEE T VIS COMPUT GR, V11, P573, DOI 10.1109/TVCG.2005.69
   Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Anderson E., 1999, LAPACK USERSGUIDE, Vthird
   CALVER D, 2004, SHADERX3 ADV RENDERI
   CHEN X, 1992, LECT NOTES COMPUT SC, V588, P739
   Cohen-Steiner David, 2003, SCG 03, P312
   DeCarlo Doug., 2004, P INT S NONPHOTOREAL, P15, DOI DOI 10.1145/987657.987661
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   GOURAUD H, 1971, IEEE T COMPUT, VC 20, P623, DOI 10.1109/T-C.1971.223313
   Gravesen J, 2002, ADV COMPUT MATH, V17, P67, DOI 10.1023/A:1015229622042
   Hamann B., 1993, GEOMETRIC MODELLING, P139, DOI DOI 10.1007/978-3-7091-6916-2_10
   KILGARD MJ, 2000, GAM DEV C 2000 ADV O
   Lange C, 2005, COMPUT AIDED GEOM D, V22, P680, DOI 10.1016/j.cagd.2005.06.010
   Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   *MICR CORP, 2006, MICR DIRECTX SDK PRO
   Praun E, 2001, COMP GRAPH, P581, DOI 10.1145/383259.383328
   Rusinkiewicz S, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P486, DOI 10.1109/TDPVT.2004.1335277
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P902, DOI 10.1109/ICCV.1995.466840
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   YOO KH, 2004, INT C COMP SCI KRAK, P90
NR 21
TC 14
Z9 18
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 803
EP 812
DI 10.1007/s00371-007-0133-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600020
DA 2024-07-18
ER

PT J
AU Biscaro, HH
   Filho, AC
   Nonato, LG
   de Oliveira, MCF
AF Biscaro, Helton H.
   Filho, Antonio C.
   Nonato, Luis G.
   de Oliveira, Maria C. Ferreira
TI A topological approach for surface reconstruction from sample points
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE topological; sample points; reconstruction; morse theory
ID SHAPE RECONSTRUCTION; MORSE-THEORY; ALGORITHM; CRUST
AB Most algorithms for surface reconstruction from sample points rely on computationally demanding operations to derive the reconstruction. In this paper we introduce an innovative approach for generating 3D piecewise linear approximations from sample points that relies strongly on topological information, thus reducing the computational cost and numerical instabilities typically associated with geometric computations. Discrete Morse theory provides the basis for a topological framework that supports a robust reconstruction algorithm capable of handling multiple components and has low computational cost. We describe the proposed approach and introduce the reconstruction algorithm, called TSR-topological surface reconstructor. Some reconstruction results are presented and the performance of TSR is compared with that of other reconstruction approaches for some standard point sets.
C1 EACH USP, BR-03828000 Sao Paulo, Brazil.
   Univ Sao Paulo, Inst Ciencia Matemat & Computacao, BR-13560970 Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo
RP Biscaro, HH (corresponding author), EACH USP, Av Arlindo Bettio 1000, BR-03828000 Sao Paulo, Brazil.
EM heltonhb@usp.br
RI Biscaro, Helton/H-6979-2014; Oliveira, Maria/ISB-2741-2023; Nonato, Luis
   Gustavo/D-5782-2011; Castelo, Antonio/E-6808-2011; Ferreira de Oliveira,
   Maria Cristina/D-9257-2011
OI Biscaro, Helton/0000-0003-1565-1044; Castelo,
   Antonio/0000-0001-8009-4577; Ferreira de Oliveira, Maria
   Cristina/0000-0002-4729-5104
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Amenta N, 2002, INT J COMPUT GEOM AP, V12, P125, DOI 10.1142/S0218195902000773
   Amenta N., 1998, Proceedings of the Fourteenth Annual Symposium on Computational Geometry, P39, DOI 10.1145/276884.276889
   Amenta N, 1998, GRAPH MODEL IM PROC, V60, P125, DOI 10.1006/gmip.1998.0465
   Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   Attali D, 1998, COMP GEOM-THEOR APPL, V10, P239, DOI 10.1016/S0925-7721(98)00013-3
   Bajaj C. L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P109, DOI 10.1145/218380.218424
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Boissonnat JD, 2002, COMP GEOM-THEOR APPL, V22, P5, DOI 10.1016/S0925-7721(01)00054-2
   BOISSONNAT JD, 1988, COMPUT VISION GRAPH, V44, P1, DOI 10.1016/S0734-189X(88)80028-8
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dey T.K., 2003, P 8 ACM S SOL MOD AP, P127, DOI DOI 10.1145/781606.781627
   Du Ding-Zhu, 1992, LECT NOTES SERIES CO, V1
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   EDELSBRUNNER H, 2002, UNPUB SURFACE RECONS
   Forman R, 1998, ADV MATH, V134, P90, DOI 10.1006/aima.1997.1650
   Forman R, 1998, TOPOLOGY, V37, P945, DOI 10.1016/S0040-9383(97)00071-2
   Gois Joao Paulo, 2004, THESIS U SAO PAULO
   Goswami Samrat, 2002, OSUCISRC1202TR31
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Kolluri R., 2004, ser. SGP '04, P11, DOI DOI 10.1145/1057432.1057434
   LEWINER T, 2002, THESIS DEP MATH
   Ni XL, 2004, ACM T GRAPHIC, V23, P613, DOI 10.1145/1015706.1015769
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Schreiner J, 2006, COMPUT GRAPH FORUM, V25, P527, DOI 10.1111/j.1467-8659.2006.00972.x
   Sharf A, 2006, COMPUT GRAPH FORUM, V25, P389, DOI 10.1111/j.1467-8659.2006.00958.x
   Teichmann M, 1998, VISUALIZATION '98, PROCEEDINGS, P67, DOI 10.1109/VISUAL.1998.745286
   VELHO L, 2003, 16 BRAZ S COMP GRAPH
NR 29
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 793
EP 801
DI 10.1007/s00371-007-0134-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600019
DA 2024-07-18
ER

PT J
AU Günther, J
   Friedrich, H
   Seidel, HP
   Slusallek, P
AF Guenther, Johannes
   Friedrich, Heiko
   Seidel, Hans-Peter
   Slusallek, Philipp
TI Interactive ray tracing of skinned animations
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE ray tracing; fuzzy kd-tree; dynamic scenes
AB Recent high-performance ray tracing implementations have already achieved interactive performance on a single PC even for highly complex scenes. However, so far these approaches have been limited to mostly static scenes due to the high cost of updating the necessary spatial index structures after modifying scene geometry. In this paper, we present an approach that avoids these updates almost completely for the case of skinned models as typically used in computer games. We assume that the characters are built from meshes with an underlying skeleton structure, where the set of joint angles defines the character's pose and determines the skinning parameters. Based on a sampling of the possible pose space we build a static fuzzy kd-tree for each skeleton segment in a fast preprocessing step. This fuzzy kd-tree is then organized into a top-level kd-tree. Together with the skeleton's affine transformations this multi-level kd-tree allows fast and efficient scene traversal at runtime, while arbitrary combinations of animation sequences can be applied interactively to the joint angles. We achieve a real-time ray tracing performance of up to 15 frames per second at 1024x1024 resolution even on a single processor core.
C1 MPI Informat, D-66123 Saarbrucken, Germany.
   Univ Saarland, D-66123 Saarbrucken, Germany.
C3 Max Planck Society; Saarland University
RP Günther, J (corresponding author), MPI Informat, Stuhlsatzenhausweg 85, D-66123 Saarbrucken, Germany.
EM guenther@mpi-inf.mpg.de; friedrich@graphics.cs.uni-sb.de;
   hpseidel@mpi-inf.mpg.de; slusallek@graphics.cs.uni-sb.de
OI Slusallek, Philipp/0000-0002-2189-2429
CR [Anonymous], 2001, THESIS CZECH TU PRAG
   [Anonymous], 2004, THESIS SAARLAND U
   [Anonymous], 1987, EUROGRAPHICS
   Appel A., 1968, P AFIPS FALL JOINT C, P37, DOI [DOI 10.1145/1468075.1468082, 10.1145/1468075.1468082]
   Carr NA, 2006, PROC GRAPH INTERF, P203
   Cleary J., 1983, Proceedings of the Association of Simulat Users Conference, P77
   FOLEY T., 2005, HWWS 05, P15
   GLASSNER AS, 1984, IEEE COMPUT GRAPH, V4, P15, DOI 10.1109/MCG.1984.6429331
   GOODNIGHT N, 2003, P 14 EUR WORKSH REND, P26
   GUNTHER J, 2006, IN PRESS COMPUT GRAP, V25
   HAVRAN V, 2000, TR18620014 CZECH TU
   JANSEN FW, 1986, P WORKSH DAT STRUCT, P57
   LAUTERBACH C, 2006, TR06010 U N CAR DEP
   LEXT J, 2000, BENCHMARK ANIMATED R
   LEXT J, 2000, EUR 2001 SHORT PRES, P311
   MacDonald J. D., 1989, Proceedings. Graphics Interface'89, P152
   Magnenat-Thalmann N., 1991, Making them move: mechanics, control, and animation of articulated figures, P243
   Magnenat-Thalmann Nadia, 1988, P GRAPHICS INTERFACE
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Reinhard E, 2000, SPRING COMP SCI, P299
   Reshetov A, 2005, ACM T GRAPHIC, V24, P1176, DOI 10.1145/1073204.1073329
   Rubin S. M., 1980, Computer Graphics, V14, P110, DOI 10.1145/965105.807479
   SCHMITTLER J., 2002, HWWS 02, P27
   WACHTER C, 2006, RENDERING TECHNIQUES
   Wald I, 2003, PVG 2003 PROCEEDINGS, P77, DOI 10.1109/PVGS.2003.1249045
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   WALD I, 2006, UUSCI2006015
   WALD I, 2006, P 2006 IEEE S INT RA
   WALDERHAUG O, 1993, NORWEGIAN PETROLEUM, V3, P485
   Woop S, 2005, ACM T GRAPHIC, V24, P434, DOI 10.1145/1073204.1073211
   WOOP S, 2006, IN PRESS P GRAPH HAR
   YOON SE, 2006, IN PRESS COMPUT GRAP, V25
NR 32
TC 3
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 785
EP 792
DI 10.1007/s00371-006-0063-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000021
DA 2024-07-18
ER

PT J
AU Pavic, D
   Schönefeld, V
   Kobbelt, L
AF Pavic, Darko
   Schoenefeld, Volker
   Kobbelt, Leif
TI Interactive image completion with perspective correction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE image completion; image repair; example-based synthesis; user interface
AB We present an interactive system for fragment-based image completion which exploits information about the approximate 3D structure in a scene in order to estimate and apply perspective corrections when copying a source fragment to a target position. Even though implicit 3D information is used, the interaction is strictly 2D, which makes the user interface very simple and intuitive. We propose different interaction metaphors in our system for providing 3D information interactively. Our search and matching procedure is done in the Fourier domain, and hence it is very fast and it allows us to use large fragments and multiple source images with high resolution while still obtaining interactive response times. Our image completion technique also takes user-specified structure information into account where we generalize the concept of feature curves to arbitrary sets of feature pixels. We demonstrate our technique on a number of difficult completion tasks.
C1 Rhein Westfal TH Aachen, Comp Graph Grp, Aachen, Germany.
C3 RWTH Aachen University
RP Pavic, D (corresponding author), Rhein Westfal TH Aachen, Comp Graph Grp, Aachen, Germany.
EM pavic@cs.rwth-aachen.de; kobbelt@cs.rwth-aachen.de
OI Kobbelt, Leif/0000-0002-7880-9470
CR [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], 2005, P BRIT MACH VIS C BM
   [Anonymous], CVPR
   Arya Sunil., 1993, P 4 ANN ACM SIAM S D, P271
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   BARRETT WA, 2002, SIGGRAPH 02, P77
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   BERTALMIO M, 2003, CVPR, P707
   Boehm W., 1994, Geometric Concepts for Geometric Design
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Datar M., 2004, PROC 20 ANN S COMPUT, P253
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Frigo M, 2005, P IEEE, V93, P216, DOI 10.1109/JPROC.2004.840301
   Harrison P, 2001, W S C G ' 2001, VOLS I & II, CONFERENCE PROCEEDINGS, P190
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HELOR Y, 2003, ICCV, P1486
   HORRY Y, 1997, SIGGRAPH 97, P225
   KASSON JM, 1992, ACM T GRAPHIC, V11, P373, DOI 10.1145/146443.146479
   KOMODAKIS N, 2006, CVPR 06, P442
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Liu YX, 2004, ACM T GRAPHIC, V23, P368, DOI 10.1145/1015706.1015731
   Oh BM, 2001, COMP GRAPH, P433
   Oliveira M., 2001, P INT C VIS IM IM PR, P261
   PAVIC D, ACCOMPANYING VIDEO
   PEREZ P, 2004, MSRTR200404 MICR RES
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
NR 33
TC 44
Z9 57
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 671
EP 681
DI 10.1007/s00371-006-0050-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000010
DA 2024-07-18
ER

PT J
AU Tamura, N
   Johan, H
   Chen, BY
   Nishita, T
AF Tamura, Naoki
   Johan, Henry
   Chen, Bing-Yu
   Nishita, Tomoyuki
TI A practical and fast rendering algorithm for dynamic scenes using
   adaptive shadow fields
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE photo-realistic rendering; real-time rendering; precomputed shadow
   fields
AB Recently, a precomputed shadow fields method was proposed for achieving fast rendering of dynamic scenes under environment illumination and local light sources. This method can render shadows fast by precomputing the occlusion information at many sample points arranged on concentric shells around each object and combining multiple precomputed occlusion information rapidly in the rendering step. However, this method uses the same number of sample points on all shells, and cannot achieve real-time rendering due to the rendering computation rely on CPU rather than graphics hardware. In this paper, we propose an algorithm for decreasing the data size of shadow fields by reducing the amount of sample points without degrading the image quality. We reduce the number of sample points adaptively by considering the differences of the occlusion information between adjacent sample points. Additionally, we also achieve fast rendering under low-frequency illuminations by implementing shadow fields on graphics hardware.
C1 Univ Tokyo, Tokyo, Japan.
   Nanyang Technol Univ, Singapore 639798, Singapore.
   Natl Taiwan Univ, Taipei, Taiwan.
C3 University of Tokyo; Nanyang Technological University; National Taiwan
   University
RP Tamura, N (corresponding author), Univ Tokyo, Tokyo, Japan.
EM naoki@nis-lab.is.s.u-tokyo.ac.jp; henryjohan@ntu.edu.sg;
   robin@ntu.edu.tw; nis@nis-lab.is.s.u-tokyo.ac.jp
RI Johan, Henry/A-3707-2011; Chen, Bing-Yu/E-7498-2016
OI Chen, Bing-Yu/0000-0003-0169-7682
CR Agrawala M, 2000, COMP GRAPH, P375, DOI 10.1145/344779.344954
   Akenine-Moller T., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P297
   [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Assarsson U, 2003, ACM T GRAPHIC, V22, P511, DOI 10.1145/882262.882300
   CROW F, 1977, P SIGGRAPH 77, P242
   DOBASHI Y, 1995, J I IMAGE ELECT ENG, V24, P196
   HECKBERG PS, 1997, CMUCS97104
   Heidrich W, 2000, SPRING COMP SCI, P269
   James DL, 2003, ACM T GRAPHIC, V22, P879, DOI 10.1145/882262.882359
   Kautz J., 2004, Proceedings of the Eurographics Symposium on Rendering, P179
   KAUTZ J, 2002, P 13 EUR WORKSH REND, P291
   KILGARD MJ, 2004, NVIDIA OPENGL EXTENS
   KONTKANEN J, 2005, P ACM SIGGRAPH 2005, P41
   Laine S, 2005, ACM T GRAPHIC, V24, P1156, DOI 10.1145/1073204.1073327
   LEHTINEN J, 2003, P 2003 S INT 3D GRAP, P59
   Mei CH, 2004, COMPUT GRAPH FORUM, V23, P281, DOI 10.1111/j.1467-8659.2004.00759.x
   Ng R, 2004, ACM T GRAPHIC, V23, P477, DOI 10.1145/1015706.1015749
   Ng R, 2003, ACM T GRAPHIC, V22, P376, DOI 10.1145/882262.882280
   NISHITA T, 1985, ACM T GRAPHIC, V4, P124, DOI 10.1145/282918.282938
   NISHITA T, 1986, P SIGGRAPH 86, P125
   NISHITA T, 1985, P SIGGRAPH 85, P23
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Sloan PP, 2003, ACM T GRAPHIC, V22, P370, DOI 10.1145/882262.882279
   SLOAN PP, 2002, P ACM SIGGRAPH, P527
   SOLER C, 1998, P SIGGRAPH 98, P321
   Tamura N, 2005, COMPUT ANIMAT VIRT W, V16, P475, DOI 10.1002/cav.108
   Zhou K, 2005, ACM T GRAPHIC, V24, P1196, DOI 10.1145/1073204.1073332
NR 28
TC 6
Z9 7
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 702
EP 712
DI 10.1007/s00371-006-0056-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000013
DA 2024-07-18
ER

PT J
AU Wang, HW
   Qin, KH
   Tang, K
AF Wang, Huawei
   Qin, Kaihuai
   Tang, Kai
TI Efficient wavelet construction with Catmull-Clark subdivision
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE biorthogonal wavelet; Catmull-Clark subdivision; lifting scheme
ID MULTIRESOLUTION ANALYSIS; SURFACES; SCHEME
AB This paper presents an efficient biorthogonal wavelet construction with the generalized Catmull-Clark subdivision based on the lifting scheme. The subdivision wavelet construction scheme is applicable to all variants of Catmull-Clark subdivision, so it is more universal than the previous wavelet construction for the generalized bicubic B-spline subdivision. Because the analysis and synthesis algorithms of the wavelets are composed of a series of local and in-place lifting operations, they can be performed in linear time. The experiments have demonstrated the stability of the proposed wavelet analysis based on the ordinary Catmull-Clark subdivision. Moreover, the resulting Catmull-Clark subdivision wavelets have better fitting quality than the generalized bicubic B-spline subdivision wavelets at a similar computation cost.
C1 Hong Kong Univ Sci & Technol, Dept Mech Engn, Hong Kong, Hong Kong, Peoples R China.
   Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   Shenyang Inst Aeronaut Engn, Shenyang 110034, Liaoning, Peoples R China.
C3 Hong Kong University of Science & Technology; Tsinghua University;
   Shenyang Aerospace University
RP Wang, HW (corresponding author), Hong Kong Univ Sci & Technol, Dept Mech Engn, Hong Kong, Hong Kong, Peoples R China.
EM wangh@ust.hk; qkh-dcs@tsinghua.edu.cn; mektang@ust.hk
RI Tang, Kai/ABA-9642-2021
OI Tang, Kai/0000-0002-5184-2086
CR Bajaj C, 2002, VISUAL COMPUT, V18, P343, DOI 10.1007/s003710100150
   BALL AA, 1986, COMPUT AIDED DESIGN, V18, P437, DOI 10.1016/0010-4485(86)90067-9
   Bertram M, 2004, COMPUTING, V72, P29, DOI 10.1007/s00607-003-0044-0
   Bertram M, 2004, IEEE T VIS COMPUT GR, V10, P326, DOI 10.1109/TVCG.2004.1272731
   Bertram M, 2000, IEEE VISUAL, P389, DOI 10.1109/VISUAL.2000.885720
   Bonneau GP, 1998, IEEE T VIS COMPUT GR, V4, P365, DOI 10.1109/2945.765329
   CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Eck M, 1995, P 22 ANN C COMP GRAP, P173, DOI DOI 10.1145/218380.218440
   Halstead M., 1993, Computer Graphics Proceedings, P35, DOI 10.1145/166117.166121
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kobbelt L, 1996, COMPUT GRAPH FORUM, V15, pC409, DOI 10.1111/1467-8659.1530409
   Kobbelt LP, 1999, COMPUT GRAPH FORUM, V18, pC119, DOI 10.1111/1467-8659.00333
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Li DG, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P25
   Litke N, 2001, IEEE VISUAL, P319, DOI 10.1109/VISUAL.2001.964527
   Loop C, 1987, THESIS U UTAH
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Qin H, 1998, IEEE T VIS COMPUT GR, V4, P215, DOI 10.1109/2945.722296
   Samavati FF, 1999, COMPUT GRAPH FORUM, V18, P97, DOI 10.1111/1467-8659.00361
   Samavati FF, 2002, COMPUT GRAPH FORUM, V21, P121, DOI 10.1111/1467-8659.00572
   Schroder P., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P161, DOI 10.1145/218380.218439
   Sederberg T. W., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P387, DOI 10.1145/280814.280942
   Sederberg TN, 2003, ACM T GRAPHIC, V22, P477, DOI 10.1145/882262.882295
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P113, DOI 10.1109/TVCG.2004.1260763
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P123, DOI 10.1109/TVCG.2004.1260764
   Wu JS, 2003, INT J GEOGR INF SCI, V17, P273, DOI 10.1080/1365881022000016016
   Zorin D, 2001, COMPUT AIDED GEOM D, V18, P429, DOI 10.1016/S0167-8396(01)00040-1
   ZORIN D., 1996, P SIGGRAPH ANN C COM, P189
   [No title captured]
NR 34
TC 23
Z9 26
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 874
EP 884
DI 10.1007/s00371-006-0074-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000030
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Lee, M
   Kim, YJ
AF Zhang, Xinyu
   Lee, Minkyoung
   Kim, Young J.
TI Interactive continuous collision detection for non-convex polyhedra
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 14th Pacific Conference on Computer Graphics and Applications
CY OCT 11-13, 2005
CL Taipei, TAIWAN
DE continuous collision detection; convex decomposition; conservative
   advancement; dynamics simulation
AB We present a highly interactive, continuous collision detection algorithm for rigid, general polyhedra. Given initial and final configurations of a moving polyhedral model, our algorithm creates a continuous motion with constant translational and angular velocities, thereby interpolating the initial and final configurations of the model. Then, our algorithm reports whether the model under the interpolated motion collides with other rigid polyhedral models in environments, and if it does, the algorithm reports its first time of contact (TOC) with the environment as well as its associated contact features at TOC.
   Our algorithm is a generalization of conservative advancement [19] to general polyhedra. In this approach, we calculate the motion bound of a moving polyhedral model and estimate the TOC based on this bound, and advance the model by the current TOC estimate. We iterate this process until the inter-distance between the moving model and the other objects in the environments is below a user-defined distance threshold.
   We pose the problem of calculating the motion bound as a linear programming problem and provide an efficient, novel solution based on the simplex method. Moreover, we also provide a hierarchical advancement technique based on a bounding volume traversal tree to generalize the conservative advancement for non-convex models.
   Our algorithm is relatively simple to implement and has very small computational overhead of merely performing discrete collision detection multiple times. We extensively benchmarked our algorithm in different scenarios, and in comparison to other known continuous collision detection algorithms, the performance improvement ranges by a factor of 1.4 similar to 45.5 depending on benchmarking scenarios. Moreover, our algorithm can perform CCD at 120 similar to 15460 frames per second on a 3.6 GHz Pentium 4 PC for complex models consisting of 10K similar to 70K triangles.
C1 Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Ewha Womans University
RP Kim, YJ (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM zhangxy@ewha.ac.kr; minkyounglee@ewha.ac.kr; kimy@ewha.ac.kr
RI Zhang, Xinyu/C-9482-2012
OI Kim, Young J./0000-0003-2159-4832
CR ABDELMALEK K, 2002, INT J SHAPE MODEL
   AGARWAL PK, 2001, P 5 WORKSH ALG FDN R, P83
   BARAFF D, 1994, P SIGGRAPH 94, P23, DOI DOI 10.1145/192161.192168
   BARAFF D, 2001, ACM SIGGRAPH COURSE
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P200, DOI 10.1109/TPAMI.1986.4767773
   Choi Y.K., 2006, IEEE T ROBOT
   DOBKIN DP, 1990, LECT NOTES COMPUT SC, V443, P400, DOI 10.1007/BFb0032047
   Ehmann SA, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P2101, DOI 10.1109/IROS.2000.895281
   Ehmann SA, 2001, COMPUT GRAPH FORUM, V20, pC500
   GILBERT EG, 1988, IEEE T ROBOTIC AUTOM, V4, P193, DOI 10.1109/56.2083
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   GUIBAS L, 1999, P ACM S COMP GEOM
   KIM B, 2003, ACM C SOL MOD APPL
   Kim DJ, 1998, IEEE T VIS COMPUT GR, V4, P230, DOI 10.1109/2945.722297
   Kirkpatrick D, 2002, INT J COMPUT GEOM AP, V12, P3, DOI 10.1142/S0218195902000724
   Larsen E., 1999, TR99018 U N CAR DEP
   LIN M, 2003, CRC HDB DISCRETE COM
   Lin M.C., 1993, Ph.D. Thesis
   Mirtich B, 2000, COMP GRAPH, P193, DOI 10.1145/344779.344866
   Mirtich BV., 1996, Impulse-based dynamic simulation of rigid body systems
   ORTEGA M, 2006, IEEE INT C VIRT REAL
   Redon S, 2004, P IEEE VIRT REAL ANN, P117, DOI 10.1109/VR.2004.1310064
   REDON S, 2002, P INT C ROB AUT
   REDON S, 2002, P EUR COMP GRAPH FOR
   REDON S, 2004, P ACM S SOL MOD APPL
   REDON S, 2000, P IEEE C ROB AUT
   SCHWARZER F, 2002, WORKSH ALG FDN ROB W
   SEIDEL R, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P211, DOI 10.1145/98524.98570
   Van den Bergen G., 2001, GAM DEV C
   VANDENBERGEN G, 2004, J GRAPH TOOLS
NR 30
TC 27
Z9 32
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2006
VL 22
IS 9-11
BP 749
EP 760
DI 10.1007/s00371-006-0060-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 082IW
UT WOS:000240381000018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sokolov, D
   Plemenos, D
   Tamine, K
AF Sokolov, Dmitry
   Plemenos, Dimitri
   Tamine, Karim
TI Methods and data structures for virtual world exploration
SO VISUAL COMPUTER
LA English
DT Article
DE scene understanding; automatic virtual camera; good point of view;
   visibility
AB This paper is dedicated to virtual world exploration techniques that help humans to understand a 3D scene. The paper presents a technique to calculate the quality of a viewpoint for a scene, and describes how this information can be used. A two-step method for an automatic real-time scene exploration is introduced. In the first step, a minimal set of "good" points of view is determined; in the second step, these viewpoints are used to compute a camera path around the scene. The proposed method enables one to get a good comprehension of a single virtual artifact or of the scene structure.
C1 Univ Limoges, CNRS, UMR 6172, XLIM Lab, F-87000 Limoges, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite de
   Limoges
RP Sokolov, D (corresponding author), Univ Limoges, CNRS, UMR 6172, XLIM Lab, 83 Rue Isle, F-87000 Limoges, France.
EM s@skisa.org; dimitrios.plemenos@unilim.fr; karim.tamine@unilim.fr
CR [Anonymous], EUROGRAPHICS 2000
   BARRAL P, 1999, GRAPHICON 99
   BARRAL P, 2000, INT C 3IA 00 LIM FRA
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   CHAZOVA LV, 1991, KARDIOLOGIYA, V31, P5
   COLIN C, 1988, EUROGRAPHICS 88
   DORME G, 2001, THESIS U LIMOGES
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Feixas M, 1999, COMPUT GRAPH FORUM, V18, pC95, DOI 10.1111/1467-8659.00331
   FEXIAS M, 2002, THESIS U CATALONIA
   Garey M.R., 1979, Comptuers and Intractability
   JAUBERT B, 2006, INT C 3IA 06 LIM FRA
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Marchand E, 2000, PROC GRAPH INTERF, P69
   NOSER H, 1995, COMPUT GRAPH, V19, P7, DOI 10.1016/0097-8493(94)00117-H
   PLEMENOS D, 2003, INT GRAPH 03 MOSC RU
   PLEMENOS D, 1991, THESIS U NANTES FRAN
   PLEMENOS D, 2005, GRAPHICON 05
   PLEMENOS D, 2004, INT C GRAPH 04 MOSC
   Plemenos D., 1996, Graphicon'96
   SBERT M, 2002, INT C 3IA 02 LIM FRA
   SOKOLOV D, 2005, VAST 2005, P67
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   VAZQUEZ PP, 2003, INT C 3IA 03 LIM FRA
   VAZQUEZ PP, 2002, COMPUTER GRAPHICS IN, P267
   VAZQUEZ PP, 2003, THESIS BARCELONA SPA
NR 27
TC 24
Z9 26
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2006
VL 22
IS 7
BP 506
EP 516
DI 10.1007/s00371-006-0025-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 074VG
UT WOS:000239839300006
DA 2024-07-18
ER

PT J
AU Huh, SB
   Metaxas, DN
AF Huh, Suejung B.
   Metaxas, Dimitris N.
TI A collision resolution algorithm for clump-free fast moving cloth
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference (CGI 2005)
CY JUN 22-24, 2005
CL Stony Brook, NY
SP IEEE Comp Soc, VGTC, ACM SIGGRAPH
DE cloth animation; collision resolution; collision detection
AB Cloth animation is an important area of computer graphics due to its numerous applications. However, so far a fast moving cloth with multiple wrinkles has been difficult to animate because of the cloth clump problem. Cloth clumps are the frozen areas where cloth pieces are clustered unnaturally - an obstacle in making a realistic cloth animation. Hence we present a novel cloth collision resolution algorithm that prevents clump formation during fast cloth motions. The goal of our resolution algorithm is to make cloth move swiftly without having any unnatural frozen cloth clumps, while preventing any cloth-cloth and rigid-cloth penetrations at any moment of a simulation. The non-penetration status of cloth is maintained without the formation of cloth clumps regardless of the speed of cloth motion. Our algorithm is based on a particular order that we found in the resolution of cloth collisions, and can be used with any structural modeling approaches such as spring-masses or finite elements. This paper includes several realistic simulation examples involving fast motions that are clump-free.
C1 Rutgers State Univ, Div Comp & Informat Sci, Piscataway, NJ 08854 USA.
C3 Rutgers University System; Rutgers University New Brunswick
RP Huh, SB (corresponding author), Rutgers State Univ, Div Comp & Informat Sci, 110 Frelinghuysen Rd, Piscataway, NJ 08854 USA.
EM suejung@cs.rutgers.edu; dnm@cs.rutgers.edu
NR 0
TC 5
Z9 6
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2006
VL 22
IS 6
BP 434
EP 444
DI 10.1007/s00371-006-0019-1
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 053BL
UT WOS:000238278500007
DA 2024-07-18
ER

PT J
AU Arvo, J
AF Arvo, J
TI Penumbra masks
SO VISUAL COMPUTER
LA English
DT Article
DE frame buffer algorithms; rendering; shadow algorithms
AB Computation of physically-based shadows can be significantly accelerated by limiting computations into regions where penumbras appear. In this paper, we present a general penumbra detection method that efficiently bounds regions where penumbras occur in a shared projection plane of an area light source. We introduce a novel area filling operator, which allows effective and conservative area masking with respect to all viewpoints, i.e., sampling points within a planar polygonal light source. The area filling operator uses a point sprite rendering technique on a set of silhouette boundaries to create a penumbra mask, which is essentially a modified occlusion map. We show how to efficiently test the geometry and screen-space pixels against the penumbra mask. An important advantage of our method is that we can separate lit and umbra regions, and thus drive various soft shadow algorithms to focus their computational efforts into potential penumbras. Due to the relative simplicity of computations, penumbra masks can be efficiently generated with graphics hardware. As an example, we accelerate shadow map supersampling to demonstrate significant speedups that utilizations of penumbra masks provide.
C1 Univ Turku, Dept Informat Technol, SF-20500 Turku, Finland.
   Bitboys Oy, Noormakku 29600, Finland.
C3 University of Turku
RP Arvo, J (corresponding author), Univ Turku, Dept Informat Technol, SF-20500 Turku, Finland.
EM juarvo@utu.fi
CR AGRAWALA M, 2000, P SIGGRAPH 00, V34, P375
   Akenine-Moller T., 2019, Real-time rendering
   AKENINEMOLLER T, 2002, P REND TECHN 2002 13, P309
   [Anonymous], 1978, P 5 ANN C COMPUTER G, P270
   Arvo J, 2004, COMPUT GRAPH FORUM, V23, P271, DOI 10.1111/j.1467-8659.2004.00758.x
   ARVO J, IN PRESS J GRAPHICS
   ARVO J, 2005, TR709 TUCS
   ARVO J, 2004, J WSCG, V12, P11
   Assarsson U, 2003, ACM T GRAPHIC, V22, P511, DOI 10.1145/882262.882300
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chan Eric., 2004, P 15 EUROGRAPHICS WO, P185
   CHIN N, 1992, ACM S INT 3D GRAPH, P21
   Cook R. L., 1984, Computers & Graphics, V18, P137
   CROW F, 1977, P SIGGRAPH 77, P242
   Durand F, 2000, COMP GRAPH, P239, DOI 10.1145/344779.344891
   Durand F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P89, DOI 10.1145/258734.258785
   Formella A, 1998, WSCG 98, VOL 2, P238
   Greene N., 1993, Proc. SIGGRAPH, P231
   Hart D, 1999, COMP GRAPH, P147, DOI 10.1145/311535.311551
   Hasenfratz JM, 2003, COMPUT GRAPH FORUM, V22, P753, DOI 10.1111/j.1467-8659.2003.00722.x
   Hasselgren J., 2005, CONSERVATIVE RASTERI, P677
   HECKBERT P, 1992, 3 EUR WORKSH REND BR, P203
   LISCHINSKI D, 1992, IEEE COMPUT GRAPH, V12, P25, DOI 10.1109/38.163622
   McCool MD, 2000, ACM T GRAPHIC, V19, P1, DOI 10.1145/343002.343006
   MITCHELL JL, 2004, ADV IMAGE PROCESSING
   NISHITA T, 1985, P SIGGRAPH 85, P23
   SHAPIRO L, 1984, IEEE CG A, V4, P5
   Shirley P, 1996, ACM T GRAPHIC, V15, P1, DOI 10.1145/226150.226151
   SOLER C, 1998, P SIGGRAPH 98, P321
   Tanaka T, 1997, COMPUT GRAPH FORUM, V16, pC231, DOI 10.1111/1467-8659.00160
   WOO A, 1990, IEEE COMPUT GRAPH, V10, P13, DOI 10.1109/38.62693
   ZHANG H, 1997, P ACM SIGGRAPH, P77
NR 32
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2006
VL 22
IS 4
BP 285
EP 297
DI 10.1007/s00371-006-0378-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 032DG
UT WOS:000236753500006
DA 2024-07-18
ER

PT J
AU Kim, L
   Park, SH
AF Kim, L
   Park, SH
TI Haptic interaction and volume modeling techniques for realistic dental
   simulation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 5th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY OCT 11-12, 2004
CL Seoul Natl Univ, Seoul, SOUTH KOREA
HO Seoul Natl Univ
DE dental simulation; haptic interaction; volumetric implicit surface;
   dental workbench
AB We present haptic simulation and volume modeling techniques for a virtual dental training system. The system allows dental students to learn dental procedures and master their skills with realistic tactual feelings. It supports various dental procedures, such as dental probing, to diagnose carious lesions, drilling operation for cavity preparation, and filling the prepared cavities with amalgam. The system requires fast and stable haptic rendering and volume modeling techniques working on the virtual tooth. Collision detection and force computation are implemented on an offset surface in volumetric representation to simulate reasonable physical interactions between dental tools with a certain volume and the teeth model. To avoid discrete haptic feeling due to the gap between the fast haptic process (1 KHz) and much slower visual update frequency (30 Hz) during drilling and filling the cavities, we employed an intermediate implicit surface to be animated between the original and target surfaces. The volumetric teeth model is converted into a geometric model by an adaptive polygonization method to maintain sharp features in every visual frame. Volumetric material properties are represented by stiffness and color values to simulate the resistance and texture information depending on anatomical tissues. Finally, we made a dental workbench to register sensory modalities like visual, auditory and haptic sensation.
C1 Korea Inst Sci & Technol, Seoul, South Korea.
C3 Korea Institute of Science & Technology (KIST)
RP Korea Inst Sci & Technol, Seoul, South Korea.
EM laehyunk@kist.re.kr; sehyung@kist.re.kr
CR [Anonymous], 1997, Introduction to Implicit Surfaces
   [Anonymous], 2001, P 2001 S INT 3D GRAP
   Avila RS, 1996, IEEE VISUAL, P197, DOI 10.1109/VISUAL.1996.568108
   BAERENTZEN J, 1998, IEEE VISUALIZATION 9, P9
   Foskey M, 2002, P IEEE VIRT REAL ANN, P119, DOI 10.1109/VR.2002.996514
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   Hayward V, 2000, LECT NOTES CONTR INF, V250, P403
   Hua J, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P254, DOI 10.1109/PCCGA.2001.962881
   Kim L, 2004, IEEE COMPUT GRAPH, V24, P66, DOI 10.1109/MCG.2004.1274064
   KIM L, 2004, LECT NOTES COMPUTER, V3179
   MAUCH S, FAST ALOGORITHM COMP
   Pandit S.M., 1983, TIME SERIES SYSTEM A
   Perry RN, 2001, COMP GRAPH, P47, DOI 10.1145/383259.383264
   Petersik A, 2003, LECT NOTES COMPUT SC, V2673, P194
   Ranta J. F., 1999, 4 PHANT US GROUPS WO, P73
   Ruspini D, 1998, ADVANCES IN ROBOT KINEMATICS: ANALYSIS AND CONTROL, P523
   RUSPINI DC, 1997, ACM SIGGRAPH P, V1, P295
   SAEKEE B, 2004, ANN MED M VIRT REAL
   *SENS TECHN INC, 1999, FREEF MOD SYST
   Thomas G, 2001, COMPUT METH PROG BIO, V64, P53, DOI 10.1016/S0169-2607(00)00089-4
   Velho L, 1999, ACM T GRAPHIC, V18, P329, DOI 10.1145/337680.337717
   WIEGAND ET, 1999, PRESENCE, V8, P492
   ZILLES C, 1994, HAPTIC INTERFACES VI, P146
NR 23
TC 42
Z9 47
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2006
VL 22
IS 2
BP 90
EP 98
DI 10.1007/s00371-006-0369-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 018GC
UT WOS:000235751200004
DA 2024-07-18
ER

PT J
AU Vivodtzev, F
   Bonneau, GP
   Le Texier, P
AF Vivodtzev, F
   Bonneau, GP
   Le Texier, P
TI Topology-preserving simplification of 2D nonmanifold meshes with
   embedded structures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE computational geometry and its applications; LOD techniques;
   multiresolution curves and surfaces
AB Mesh simplification has received tremendous attention over the years. Most of the previous work in this area deals with a proper choice of error measures to guide the simplification. Preserving the topological characteristics of the mesh and possibly of data attached to the mesh is a more recent topic and the subject of this paper. We introduce a new topology-preserving simplification algorithm for triangular meshes, possibly nonmanifold, with embedded polylines. In this context, embedded means that the edges of the polylines are also edges of the mesh. The paper introduces a robust test to detect if the collapse of an edge in the mesh modifies either the topology of the mesh or the topology of the embedded polylines. This validity test is derived using combinatorial topology results. More precisely, we define a so-called extended complex from the input mesh and the embedded polylines. We show that if an edge collapse of the mesh preserves the topology of this extended complex, then it also preserves both the topology of the mesh and the embedded polylines. Our validity test can be used for any 2-complex mesh, including nonmanifold triangular meshes, and can be combined with any previously introduced error measure. Implementation of this validity test is described. We demonstrate the power and versatility of our method with scientific data sets from neuroscience, geology, and CAD/CAM models from mechanical engineering.
C1 INRIA Rhone Alpes, INP Genoble, CNRS, UJF,Lab GRAVIR, F-38334 Saint Ismier, France.
   CEA, CESTA, F-33114 Le Barp, France.
C3 Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA);
   Centre National de la Recherche Scientifique (CNRS); CEA
RP INRIA Rhone Alpes, INP Genoble, CNRS, UJF,Lab GRAVIR, 655 Ave Europe, F-38334 Saint Ismier, France.
EM fabien.vivodtzev@imag.fr; georges-pierre.bonneau@imag.fr;
   paul.letexier@cea.fr
CR [Anonymous], 2003, Level of detail for 3D graphics
   BAJAJ C, 1996, SPIE, V2656, P34
   Cignoni P, 1998, VISUALIZATION '98, PROCEEDINGS, P59, DOI 10.1109/VISUAL.1998.745285
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   COHEN J, 1998, COMP GRAPH SIGGRAPH, V32, P115
   DEY T, 1998, RGITECH98018
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   ERIKSON C, 1999, S INT 3D GRAPH, P79
   FLORIANI LD, 1996, VISUAL COMPUT, V12, P317
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Garland M., 1997, COMPUTER GRAPHICS, V31, P209, DOI DOI 10.1145/258734.258849
   Gerstner T, 2000, IEEE VISUAL, P259, DOI 10.1109/VISUAL.2000.885703
   Gregorski BF, 2003, MATH VIS, P99
   Gross MH, 1996, IEEE T VIS COMPUT GR, V2, P130, DOI 10.1109/2945.506225
   GUEZIEC A, 1996, 20440 RC IBM RES
   HAEMER MJ, 1991, COMPUT GRAPH-UK, V15, P175
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoppe H, 1998, COMPUT GRAPH-UK, V22, P27, DOI 10.1016/S0097-8493(97)00081-2
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lindstrom P, 1998, VISUALIZATION '98, PROCEEDINGS, P279, DOI 10.1109/VISUAL.1998.745314
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   Popovic J., 1997, SIGGRAPH 97 C P, P217, DOI [10.1145/258734.258852, DOI 10.1145/258734.258852]
   Ronfard R., 1996, Computer Graphics Forum, V15, pC67, DOI 10.1111/1467-8659.1530067
   Rossignac J., 1993, Geometric Modeling in Computer Graphics, P455
   ROSSL C, 2000, P AAAI S SMART GRAPH, P71
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Willmott AJ, 1999, SPRING EUROGRAP, P293
   WU Y, 2004, GRAPHITE 04, P50
NR 32
TC 8
Z9 10
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 679
EP 688
DI 10.1007/s00371-005-0334-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400020
DA 2024-07-18
ER

PT J
AU Drago, F
   Chiba, N
AF Drago, F
   Chiba, N
TI Painting canvas synthesis
SO VISUAL COMPUTER
LA English
DT Article
DE artistic heritage preservation; procedural textures synthesis; texturing
   and shading; displacement mapping; cloth simulation
AB We present the development of a procedural method for texturing and modeling different kinds of woven canvas used to support easel paintings. The detailed macro- and microgeometry of textiles and different weaving patterns found in woven fabrics is conveniently simulated by procedural displacement and surface shading. The common varieties of canvas used in art production since the Italian Renaissance period are presented and recreated. The anatomy of an oil-based painting is briefly introduced and a visual simulation of decay presented. We also apply our texturing and shading techniques to a simple geometric representation of painting to help visualize the changing characteristics developed during the aging process of a canvas support kept in uncontrolled environmental conditions.
C1 Iwate Univ, Comp Graph Lab, Morioka, Iwate 0208551, Japan.
C3 Iwate University
RP Iwate Univ, Comp Graph Lab, Ueda 4-3-5, Morioka, Iwate 0208551, Japan.
EM frederic@cis.iwate-u.ac.jp; nchiba@cis.iwate-u.ac.jp
CR ANDREWS RN, 1924, PENNSYLVANIA MUSEUM
   [Anonymous], 1994, TEXTURING MODELING P
   [Anonymous], 1990, B CLASSE SCI, DOI [10.3406/barb.1990.38523, DOI 10.3406/BARB.1990.38523]
   Apodaca A.A., 1999, ADV RENDERMAN, V1st
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   BLINN JF, 1976, COMMUN ACM, V19, P542, DOI 10.1145/965143.563322
   Breen David E., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques, P365
   CARBONNEL KV, 1980, J AM INT CONSERV, V20, P20
   CARIGNAN M, 1992, COMP GRAPH, V26, P99, DOI 10.1145/142920.134017
   Chen YY, 2003, IEEE T VIS COMPUT GR, V9, P43, DOI 10.1109/TVCG.2003.1175096
   Cook R. L., 1984, Computers & Graphics, V18, P223
   Cook Robert L, 1987, ACM_SIGGRAPH_Computer_Graphics, V21, P95
   *COR CORP, 2001, PAINT 7 0
   Daubert K, 2001, SPRING EUROGRAP, P63
   Daubert K, 2002, COMPUT GRAPH FORUM, V21, P575, DOI 10.1111/1467-8659.t01-1-00708
   DEWILLIGEN P, 1999, WBBM SERIES, V42
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   DORSEY J, 1996, P SIGGRAPH 96, P387
   Drago F, 2002, ADVANCES IN MODELLING, ANIMATION AND RENDERING, P123
   Gobron S, 1999, J VISUAL COMP ANIMAT, V10, P143, DOI 10.1002/(SICI)1099-1778(199907/09)10:3<143::AID-VIS204>3.0.CO;2-W
   Groller E, 1995, IEEE T VIS COMPUT GR, V1, P302, DOI 10.1109/2945.485617
   GROLLER E, 1996, P EUR WORKSH REND JU, P205
   Lokovic T, 2000, COMP GRAPH, P385, DOI 10.1145/344779.344958
   Macklenburg MF, 1999, ASHRAE J, V41, P77
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   Nicolaus K., 1999, RESTORATION PAINTING
   PERCIVALPRESCOT.W, 1974, P C COMP LIN TECHN N
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   *PIX, 2000, REND INT V 3 2 SPEC
   ROSTAIN E, 1981, RENTOILAGE TRANSPORT
   VASARI G, 1550, LIVES ARTISTS PAINTI
   VOLEVICH V, 1997, P 7 C COMP GRAPH SCI
   Volino P, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P257, DOI 10.1109/CGI.2000.852341
   Volino P., 2000, VIRTUAL CLOTHING THE
   WESTIN SH, 1992, COMP GRAPH, V26, P255, DOI 10.1145/142920.134075
   Xu YQ, 2001, COMP GRAPH, P391
   YASUDA T, 1992, IEEE COMPUT GRAPH, V12, P15, DOI 10.1109/38.163621
   Zhong H, 2001, J VISUAL COMP ANIMAT, V12, P13, DOI 10.1002/vis.241
NR 41
TC 11
Z9 16
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2004
VL 20
IS 5
BP 314
EP 328
DI 10.1007/s00371-004-0240-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 842GL
UT WOS:000222991600003
DA 2024-07-18
ER

PT J
AU Kim, JK
   Ra, JB
AF Kim, JK
   Ra, JB
TI A real-time terrain visualization algorithm using wavelet-based
   compression
SO VISUAL COMPUTER
LA English
DT Article
DE terrain visualization; mesh approximation; image compression; wavelet
   transform; restricted quadtree triangulation
AB We propose a real-time terrain visualization algorithm combined with wavelet-based compression. Our approach updates a surface mesh model in real time by using wavelet coefficients and height data decoded from a compressed bitstream. To achieve this, a new mesh approximation method using restricted quadtree triangulation is designed on the basis of wavelet coefficients representing surface complexity. Also, a wavelet-based compression having a multiresolution structure is introduced to handle large terrain data interactively. Simulation results demonstrate that the proposed algorithm is prospective for applications in a network environment where narrow bandwidth and low computational power are usually allowed .
C1 Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Taejon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, 373-1 Guseongdong Yuseonggu, Taejon, South Korea.
EM jbra@ee.kaist.ac.kr
RI Beom, Jong/C-1958-2011
CR Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Duchaineau M, 1997, VISUALIZATION '97 - PROCEEDINGS, P81, DOI 10.1109/VISUAL.1997.663860
   Evans W, 2001, ALGORITHMICA, V30, P264, DOI 10.1007/s00453-001-0006-x
   GERSTNER T, 1999, 29 U BONN I ANG MATH
   Gross MH, 1996, IEEE T VIS COMPUT GR, V2, P130, DOI 10.1109/2945.506225
   Gross MH, 1999, FUTURE GENER COMP SY, V15, P11, DOI 10.1016/S0167-739X(98)00053-3
   HEBERT DJ, 1995, P SOC PHOTO-OPT INS, V2569, P381, DOI 10.1117/12.217594
   Lindstrom P, 2001, IEEE VISUAL, P363, DOI 10.1109/VISUAL.2001.964533
   LINDSTROM P, 1996, P SIGGRAPH 96 NEW OR, P110
   Pajarola R, 1998, VISUALIZATION '98, PROCEEDINGS, P19, DOI 10.1109/VISUAL.1998.745280
   Park KH, 2002, SIGNAL PROCESS-IMAGE, V17, P467, DOI 10.1016/S0923-5965(02)00021-8
   Rabinovich B, 1997, VISUALIZATION '97 - PROCEEDINGS, P95, DOI 10.1109/VISUAL.1997.663863
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Yu HJ, 1999, VISUAL COMPUT, V15, P9, DOI 10.1007/s003710050159
NR 14
TC 11
Z9 19
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 67
EP 85
DI 10.1007/s00371-003-0233-z
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900001
DA 2024-07-18
ER

PT J
AU Ohtake, Y
   Belyaev, A
   Pasko, A
AF Ohtake, Y
   Belyaev, A
   Pasko, A
TI Dynamic mesh optimization for polygonized implicit surfaces with sharp
   features
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on Shape Modeling and Applications (SMI
   2001)
CY MAY 07-11, 2001
CL GENOA, ITALY
SP Consiglio Nazl Ricerche
DE mesh evolution; implicit surfaces
AB The paper presents a novel approach for accurate polygonization of implicit surfaces with sharp features. The approach is based on mesh evolution toward a given implicit surface with simultaneous control of the mesh vertex positions and mesh normals. Given an initial polygonization of an implicit surface, a mesh evolution process initialized by the polygonization is used. The evolving mesh converges to a limit mesh which delivers a high-quality approximation of the implicit surface. For analyzing how close the evolving mesh approaches the implicit surface, we use two error metrics: the metrics measure deviations of the mesh vertices from the implicit surface and deviations of mesh normals from the normals of the implicit surface.
C1 Max Planck Inst Informat, Comp Graph Grp, D-66123 Saarbrucken, Germany.
   Univ Aizu, Shape Modeling Lab, Aizu Wakamatsu 9658580, Japan.
   Hosei Univ, Dept Digital Media, Fac Comp & Informat Sci, Koganei, Tokyo 1848584, Japan.
C3 Max Planck Society; University of Aizu; Hosei University
RP Max Planck Inst Informat, Comp Graph Grp, Stuhlsatzenhausweg 85, D-66123 Saarbrucken, Germany.
EM ohtake@mpi-sb.mpg.de; belyaev@mpi-sb.mpg.de; pasko@k.hosei.ac.jp
RI University, Heriot-Watt/U-9720-2019; Pasko, Alexander/H-9344-2017
OI Belyaev, Alexander/0000-0002-7043-8847; Pasko,
   Alexander/0000-0002-4785-7066
CR Adams R. A., 1975, SOBOLEV SPACES
   ADZHIEV V, 1999, IMPLICIT SURFACES 99, P59
   [Anonymous], 1997, Introduction to Implicit Surfaces
   [Anonymous], 1992, NUMERICAL RECIPES C
   BAJAJ C, 2001, P INF VIS 2001 COMP
   Blake A., 1998, ACTIVE CONTOURS
   Brenner S.C., 1994, MATH THEORY FINITE E
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   DURBIN R, 1987, NATURE, V326, P689, DOI 10.1038/326689a0
   Karkanis T, 2001, IEEE COMPUT GRAPH, V21, P60, DOI 10.1109/38.909016
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   KOBBELT L, 2000, COMPUT GRAPH FORUM, V19, P249
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   McInerney T, 1999, IEEE T MED IMAGING, V18, P840, DOI 10.1109/42.811261
   Meyer M, 2002, INT WORKSH VIS MATH
   MILLER JV, 1991, COMP GRAPH, V25, P217, DOI 10.1145/127719.122742
   MONTAGNAT J, 2000, RR3954 INRIA SOPH AN
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   Ohtake Y, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P74, DOI 10.1109/SMA.2001.923377
   Ohtake Y, 2001, COMPUT GRAPH FORUM, V20, pC368, DOI 10.1111/1467-8659.00529
   Ohtake Y., 2002, SMA'02: Proceedings of the seventh ACM symposium on Solid modeling and applications, (New York, NY, USA), P171
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Rvachev V. L., 1982, THEORY R FUNCTIONS S
   RVACHEV VL, 1973, METHODS ALGEBRA LOGI
   Schroeder W., 1998, The visualization toolkit an object-oriented approach to 3D graphics
   Sethian J., 1999, LEVEL SET METHODS FA
   SHAPIRO V, 1994, COMPUT AIDED GEOM D, V11, P153, DOI 10.1016/0167-8396(94)90030-2
   SHAPIRO V, 1999, P 5 S SOL MOD SOL MO, P852
   Taubin G., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P658, DOI 10.1109/ICCV.1993.378149
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Taubin G, 2000, EUROGRAPHICS 2000 ST, P107
   VELHO L., 1996, J GRAPH TOOLS, V1, P5
   Wood ZJ, 2000, IEEE VISUAL, P275, DOI 10.1109/VISUAL.2000.885705
   WYVILL B, INT J SHAPE MODELLIN, V2, P257
NR 35
TC 15
Z9 16
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 115
EP 126
DI 10.1007/s00371-002-0181-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 691AU
UT WOS:000183583300005
DA 2024-07-18
ER

PT J
AU Chang, KY
   Chang, IN
   Chen, LS
   Lee, C
AF Chang, KY
   Chang, IN
   Chen, LS
   Lee, C
TI Multi-tuple interpolation using Fourier descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE interpolation; object interpolation; Fourier descriptors; 3D
   reconstruction
ID SHAPE-BASED INTERPOLATION; MEDICAL IMAGE-RECONSTRUCTION
AB This paper presents a new object interpolation method called multi-tuple interpolation. Its principal feature is the use of Fourier descriptors to represent a contour and to obtain its four attributes, namely, shape, scale, orientation and position. Each attribute is interpolated in the frequency domain under the control of its individual parameter. A suitable combination of these parameters, which can be regarded as a kind of prior knowledge, can generate a result adaptable to the real 3D object. Several experiments are performed on both synthetic and real medical images to demonstrate this new approach. It is shown that the proposed method can be applied to other similar domains, e.g. shape blending.
C1 Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
   Natl Sun Yat Sen Univ, Dept Comp Sci & Engn, Kaohsiung 80424, Taiwan.
C3 National Cheng Kung University; National Sun Yat Sen University
RP Chen, LS (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, 1 Univ Rd, Tainan 701, Taiwan.
RI Chen, Long-Sheng/GSD-6470-2022; Rohlf, F J/A-8710-2008
OI Chen, Long-Sheng/0000-0002-2967-9956; 
CR [Anonymous], 1992, R. woods digital image processing
   BURR DJ, 1981, IEEE T PATTERN ANAL, V3, P708, DOI 10.1109/TPAMI.1981.4767176
   CHEN GY, 1990, Z OPERATIONS RES, V3, P1
   Foley J.D., 1990, Computer graphics: Principles and practice
   GRANLUND GH, 1972, IEEE T COMPUT, VC 21, P195, DOI 10.1109/TC.1972.5008926
   GUO JF, 1995, COMPUT MED IMAG GRAP, V19, P267, DOI 10.1016/0895-6111(95)00007-D
   HERMAN GT, 1992, IEEE COMPUT GRAPH, V12, P69, DOI 10.1109/38.135915
   HIGGINS WE, 1993, IEEE T MED IMAGING, V12, P439, DOI 10.1109/42.241871
   HUGHES JF, 1992, COMP GRAPH, V26, P43, DOI 10.1145/142920.134004
   LAI KF, 1995, IEEE T PATTERN ANAL, V17, P1084, DOI 10.1109/34.473235
   LIN WC, 1988, IEEE T MED IMAGING, V7, P225, DOI 10.1109/42.7786
   Liu YH, 1997, COMPUT MED IMAG GRAP, V21, P91, DOI 10.1016/S0895-6111(96)00063-8
   Montagnat J, 1997, LECT NOTES COMPUT SC, V1205, P13, DOI 10.1007/BFb0029220
   *NAT LIB MED, 2001, VIS HUM PROJ
   PROMAYON E, 1996, COMPUT GRAPH FORUM, V15, P155
   RAYA SP, 1990, IEEE T MED IMAGING, V9, P32, DOI 10.1109/42.52980
   ROSENFELD A, 1973, IEEE T COMPUT, VC 22, P875, DOI 10.1109/TC.1973.5009188
   SEDERBERG TW, 1992, COMP GRAPH, V26, P25, DOI 10.1145/142920.134001
   Udupa J.K., 2000, 3D Imaging in Medicine, V2nd
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
NR 20
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER-VERLAG
PI NEW YORK
PA 175 FIFTH AVE, NEW YORK, NY 10010 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 1
EP 9
DI 10.1007/s00371-002-0160-4
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 671MB
UT WOS:000182467600001
DA 2024-07-18
ER

PT J
AU Ulucan, O
   Ulucan, D
   Ebner, M
AF Ulucan, Oguzhan
   Ulucan, Diclehan
   Ebner, Marc
TI Multi-scale color constancy based on salient varying local spatial
   statistics
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Computational color constancy; Illumination estimation; White-balancing;
   Scale-space
ID ILLUMINANT ESTIMATION; ALGORITHM; RETINEX
AB The human visual system unconsciously determines the color of the objects by "discounting" the effects of the illumination, whereas machine vision systems have difficulty performing this task. Color constancy algorithms assist computer vision pipelines by removing the effects of the illuminant, which in the end enables these pipelines to perform better on high-level vision tasks based on the color features of the scene. Due to its benefits, numerous color constancy algorithms have been developed, and existing techniques have been improved. Combining different strategies and investigating new methods might help us design simple yet effective algorithms. Thereupon, we present a color constancy algorithm based on the outcomes of our previous works. Our algorithm is built upon the biological findings that the human visual system might be discounting the illuminant based on the highest luminance patches and space-average color. We find the illuminant estimate based on the idea that if the world is gray on average, the deviation of the brightest pixels from the achromatic value should be caused by the illuminant. Our approach utilizes multi-scale operations by only considering the salient pixels. It relies on varying surface orientations by adopting a block-based approach. We show that our strategy outperforms learning-free algorithms and provides competitive results compared to the learning-based methods. Moreover, we demonstrate that using parts of our strategy can significantly improve the performance of several learning-free methods. We also briefly present an approach to transform our global color constancy method into a multi-illuminant color constancy approach.
C1 [Ulucan, Oguzhan; Ulucan, Diclehan; Ebner, Marc] Univ Greifswald, Comp Sci, Walther Rathenau Str 47, D-17489 Greifswald, Germany.
C3 Universitat Greifswald
RP Ulucan, O (corresponding author), Univ Greifswald, Comp Sci, Walther Rathenau Str 47, D-17489 Greifswald, Germany.
EM oguzhan.ulucan@uni-greifswald.de
RI Ebner, Marc/AAH-3839-2019; Ulucan, Diclehan/KHY-4507-2024; Ulucan,
   Oguzhan/AAY-8794-2020
OI Ebner, Marc/0000-0003-2725-2454; Ulucan, Diclehan/0000-0002-7059-302X;
   Ulucan, Oguzhan/0000-0003-2077-9691
FU Universitt Greifswald (1032)
FX No Statement Available
CR Afifi M., 2019, BRIT MACH VIS C
   Afifi M., 2022, P IEEE CVF WINT C AP, P1210
   Afifi M, 2020, Arxiv, DOI arXiv:2009.12632
   Afifi M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1961, DOI 10.1109/ICCV48922.2021.00199
   Afifi M, 2020, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR42600.2020.00147
   Afifi M, 2019, PROC CVPR IEEE, P1535, DOI 10.1109/CVPR.2019.00163
   Akazawa T, 2022, IEEE ACCESS, V10, P89051, DOI 10.1109/ACCESS.2022.3200391
   [Anonymous], 2007, Color Constancy
   [Anonymous], 1993, A vision of the brain
   Barron JT, 2017, PROC CVPR IEEE, P6950, DOI 10.1109/CVPR.2017.735
   Barron JT, 2015, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2015.51
   Beigpour S, 2014, IEEE T IMAGE PROCESS, V23, P83, DOI 10.1109/TIP.2013.2286327
   Bianco S, 2019, PROC CVPR IEEE, P12204, DOI 10.1109/CVPR.2019.01249
   Bianco S, 2017, IEEE T IMAGE PROCESS, V26, P4347, DOI 10.1109/TIP.2017.2713044
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Buzzelli M, 2023, COLOR RES APPL, V48, P40, DOI 10.1002/col.22822
   Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049
   Das P, 2018, Arxiv, DOI arXiv:1812.03085
   Das P, 2021, IEEE INT CONF COMP V, P1194, DOI 10.1109/ICCVW54120.2021.00139
   Domislovic I, 2022, PATTERN RECOGN LETT, V159, P31, DOI 10.1016/j.patrec.2022.04.035
   Ebner M, 2003, LECT NOTES COMPUT SC, V2781, P60
   Ebner M, 2004, J PARALLEL DISTR COM, V64, P79, DOI 10.1016/j.jpdc.2003.06.004
   Ebner M., 2022, J. Artif. Intell. Conscious, V9, P193, DOI [DOI 10.1142/S2705078522500035, 10.1142/S2705078522500035]
   Ebner M, 2007, IEEE T IMAGE PROCESS, V16, P2697, DOI 10.1109/TIP.2007.908086
   Ebner M, 2013, BIO-ALGORITHMS MED-S, V9, P167, DOI 10.1515/bams-2013-0152
   Ebner M, 2009, MACH VISION APPL, V20, P283, DOI 10.1007/s00138-008-0126-2
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   FINLAYSON GD, 1994, J OPT SOC AM A, V11, P3011, DOI 10.1364/JOSAA.11.003011
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, P4387, DOI 10.1109/TIP.2019.2908783
   Gao SB, 2017, J OPT SOC AM A, V34, P1448, DOI 10.1364/JOSAA.34.001448
   Gao SB, 2015, IEEE T PATTERN ANAL, V37, P1973, DOI 10.1109/TPAMI.2015.2396053
   Gao SB, 2014, LECT NOTES COMPUT SC, V8690, P158, DOI 10.1007/978-3-319-10605-2_11
   Gehler PV, 2008, PROC CVPR IEEE, P3291
   Geusebroek JM, 2000, LECT NOTES COMPUT SC, V1842, P331
   Gijsenij A, 2012, IEEE T PATTERN ANAL, V34, P918, DOI 10.1109/TPAMI.2011.197
   Gijsenij A, 2012, IEEE T IMAGE PROCESS, V21, P697, DOI 10.1109/TIP.2011.2165219
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Gijsenij A, 2009, PROC CVPR IEEE, P581, DOI 10.1109/CVPRW.2009.5206497
   Gomez-Villa A, 2019, PROC CVPR IEEE, P12301, DOI 10.1109/CVPR.2019.01259
   Hemrit G., 2018, COLOR IMAGING C 1, V26, P350, DOI [DOI 10.2352/ISSN.2169, 10.2352/issn.2169]
   Hussain MDA, 2018, IEEE ACCESS, V6, P8964, DOI 10.1109/ACCESS.2018.2808502
   Hussain MA, 2019, IEEE ACCESS, V7, P72964, DOI 10.1109/ACCESS.2019.2919997
   Hussain MA, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19102242
   Joze HRV, 2012, COLOR IMAG CONF, P41
   Kinli F., 2023, P IEEE CVF WINT C AP, P4903
   Laakom F, 2021, IEEE ACCESS, V9, P39560, DOI 10.1109/ACCESS.2021.3064382
   Laakom F, 2020, IEEE T IMAGE PROCESS, V29, P7722, DOI 10.1109/TIP.2020.3004921
   Laakom F, 2019, 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2019), P1085, DOI 10.1109/SSCI44817.2019.9002684
   Land E. H., 1974, Proceedings of the Royal Institution of Great Britain, P23
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li B, 2007, IEICE T INF SYST, VE90D, P1121, DOI 10.1093/ietisy/e90-d.7.1121
   Linnell K.J., 1997, John Dalton's Colour Vision Legacy, P501
   Morimoto T, 2021, J VISION, V21, DOI 10.1167/jov.21.3.7
   Ono T., 2022, C COMPUT VISION PATT, P19740
   Qian YL, 2019, Arxiv, DOI arXiv:1803.08326
   Qian YL, 2019, PROC CVPR IEEE, P8054, DOI 10.1109/CVPR.2019.00825
   Shi W, 2016, LECT NOTES COMPUT SC, V9908, P371, DOI 10.1007/978-3-319-46493-0_23
   Uchikawa K, 2012, J OPT SOC AM A, V29, pA133, DOI 10.1364/JOSAA.29.00A133
   Ulucan D., 2023, EUR SIGN PROC C HELS
   Ulucan D., 2023, INT C IM PROC VIS EN, P47
   Ulucan D., 2023, INT C IM PROC VIS EN, P57
   Ulucan O., 2023, EUR SIGN PROC C HEL
   Ulucan O., 2022, BRIT MACH VIS C LOND
   Ulucan O., 2023, INT C AC SPEECH SIGN, P1
   Ulucan O, 2022, IEEE IMAGE PROC, P2826, DOI 10.1109/ICIP46576.2022.9897781
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang F, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.841426
   Lo YC, 2021, PROC CVPR IEEE, P8049, DOI 10.1109/CVPR46437.2021.00796
   Zhang XS, 2016, IEEE T IMAGE PROCESS, V25, P1219, DOI 10.1109/TIP.2016.2516953
   Zini S, 2022, IEEE T COMPUT IMAG, V8, P795, DOI 10.1109/TCI.2022.3203889
NR 70
TC 1
Z9 1
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 2
PY 2023
DI 10.1007/s00371-023-03148-7
EA DEC 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z9QB1
UT WOS:001115341900003
OA hybrid
DA 2024-07-18
ER

PT J
AU Panda, MK
   Thangaraj, V
   Subudhi, BN
   Jakhetiya, V
AF Panda, Manoj Kumar
   Thangaraj, Veerakumar
   Subudhi, Badri Narayan
   Jakhetiya, Vinit
TI Bayesian's probabilistic strategy for feature fusion from visible and
   infrared images
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-scale feature decomposition; Bayesian's probabilistic fusion
   strategy; Infrared image; Visual image
ID FRAMEWORK; TRANSFORM; NETWORK
AB This article introduces a unique and first attempt at the fusion of visible and infrared images depending on multi-scale decomposition and salient feature map detection. The proposed technique integrates the bidimensional empirical mode decomposition (BEMD) strategy with Bayesian's probabilistic strategy for fusion. The proposed mechanism can effectively handle the uncertainty in the challenging source pairs and retain maximum details of the sources at a multi-scale level. The BEMD level features are extracted and integrated with Bayesian's probabilistic fusion strategy to extract several salient feature maps from the infrared and visual sensors images, which are able to preserve the common information and reduce the source images' superfluous information at various scales. The combination of these salient feature maps generates an image that gives the target scene complete information with reduced artifacts. The performance of the proposed algorithm is estimated by testing it on the benchmark "TNO" database. The empirical results of the proposed algorithm are evaluated using both visual analysis and quantitative assessment. In this work, the efficiency of the proposed technique is corroborated against seventeen existing state-of-the-art (SOTA) techniques and found to be effective. For the quantitative assessment, we have used the four most-cited quantitative evaluation measures: mutual information for the discrete cosine features (FMIdct), amount of artifacts added during the fusion process (N-abf), structure similarity index (SSIMa), and edge preservation index (EPIa). It is observed that the proposed algorithm attained the best average values: Avg. FMIdct= 0.39863, Avg.N-abf = 0.00102, Avg.SSIMa = 0.77820, and Avg.EPIa = 0.78404. It is also observed that the proposed scheme outperforms the competitive SOTA techniques in terms of different considered quantitative evaluation measures with at least a gain of 3% and the highest gain of 94%.
C1 [Panda, Manoj Kumar] GIET Univ, Rayagada 765022, Odisha, India.
   [Thangaraj, Veerakumar] Natl Inst Technol Goa, Ponda 403401, Goa, India.
   [Subudhi, Badri Narayan; Jakhetiya, Vinit] Indian Inst Technol Jammu, NH44, Jammu 181221, Jammu & Kashmir, India.
C3 GIET University; National Institute of Technology (NIT System); National
   Institute of Technology Goa; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) Jammu
RP Subudhi, BN (corresponding author), Indian Inst Technol Jammu, NH44, Jammu 181221, Jammu & Kashmir, India.
EM manojkumarpanda@giet.edu; tveerakumar@yahoo.co.in;
   subudhi.badri@gmail.com; vinit.jakhetiya@iitjammu.ac.in
RI Thangaraj, Veerakumar/AAS-8625-2020; Panda, Manoj Kumar/HVU-4540-2023
OI Thangaraj, Veerakumar/0000-0001-9084-1847; Panda, Manoj
   Kumar/0009-0002-5021-2741
CR Aghamaleki JA, 2023, VISUAL COMPUT, V39, P1181, DOI 10.1007/s00371-021-02396-9
   Bavirisetti DP, 2016, INFRARED PHYS TECHN, V76, P52, DOI 10.1016/j.infrared.2016.01.009
   Gao C, 2021, IEEE ACCESS, V9, P91883, DOI 10.1109/ACCESS.2021.3086096
   Gaur M.S., 2020, P 2020 INT C EM TREN, P1
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Jagalingam P., 2014, 3 WORLD C APPL SCI E, P1
   Jian LH, 2021, IEEE T MULTIMEDIA, V24, P3314, DOI 10.1109/TMM.2021.3096088
   Li H., 2018, ARXIV
   Li HF, 2021, IEEE T IMAGE PROCESS, V30, P4070, DOI 10.1109/TIP.2021.3069339
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Liu J., 2022, VISUAL COMPUT, V1, P1
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2019, IEEE SIGNAL PROC LET, V26, P485, DOI 10.1109/LSP.2019.2895749
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Lu R., 2022, VISUAL COMPUT, V1, P1
   Ludusan C, 2012, PATTERN RECOGN LETT, V33, P1388, DOI 10.1016/j.patrec.2012.02.017
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Nunes JC, 2003, IMAGE VISION COMPUT, V21, P1019, DOI 10.1016/S0262-8856(03)00094-5
   Ojagh S, 2021, COMPUT ELECTR ENG, V96, DOI 10.1016/j.compeleceng.2021.107572
   Panda Manoj Kumar, 2020, 2020 IEEE Region 10 Conference (TENCON), P251, DOI 10.1109/TENCON50793.2020.9293815
   Panda MK, 2022, EUR SIGNAL PR CONF, P493
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Singh R, 2008, PATTERN RECOGN, V41, P880, DOI 10.1016/j.patcog.2007.06.022
   Soroush R, 2023, VISUAL COMPUT, V39, P2725, DOI 10.1007/s00371-022-02488-0
   Wang XJ, 2023, VISUAL COMPUT, V39, P4801, DOI 10.1007/s00371-022-02628-6
   Wang ZS, 2015, OPTIK, V126, P4184, DOI 10.1016/j.ijleo.2015.08.118
   Yan YJ, 2018, COGN COMPUT, V10, P94, DOI 10.1007/s12559-017-9529-6
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P4771, DOI 10.1109/TCSVT.2021.3054584
   YIN W, 2023, VISUAL COMPUT, P1
   Zhang Q, 2011, PATTERN RECOGN LETT, V32, P1544, DOI 10.1016/j.patrec.2011.06.002
   Zhang XC, 2019, IEEE ACCESS, V7, P122122, DOI 10.1109/ACCESS.2019.2936914
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
NR 40
TC 0
Z9 0
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4221
EP 4233
DI 10.1007/s00371-023-03078-4
EA SEP 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001094628400001
DA 2024-07-18
ER

PT J
AU Chen, H
   Li, J
   Zhang, JJ
   Fu, Y
   Yan, CG
   Zeng, D
AF Chen, Hu
   Li, Jia
   Zhang, Junjie
   Fu, Yu
   Yan, Chenggang
   Zeng, Dan
TI GLCSA-Net: global-local constraints-based spectral adaptive network for
   hyperspectral image inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Hyperspectral image; Inpainting; Image structure; Local texture; Channel
   attention
ID SUPERRESOLUTION; RECONSTRUCTION
AB Due to the instability of the hyperspectral imaging system and the atmospheric interference, hyperspectral images (HSIs) often suffer from losing the image information of areas with different shapes, which significantly degrades the data quality and further limits the effectiveness of methods for subsequent tasks. Although mainstream deep learning-based methods have achieved promising inpainting performance, the complicated ground object distributions increase the difficulty of HSIs inpainting in practice. In addition, spectral redundancy and complex texture details are two main challenges for deep neural network-based inpainting methods. To address the above issues, we propose a Global-Local Constraints-based Spectral Adaptive Network (GLCSA-Net) for HSI inpainting. To reduce the redundancy of spectral information, a multi-frequency channel attention module is designed to strengthen the essential channels and suppress the less significant ones, which calculates adaptive weight coefficients by converting feature maps to the frequency domain. Furthermore, we propose to constrain the generation of missing areas from both global and local perspectives, by fully leveraging the HSI texture information, so that the overall structure information and regional texture consistency of the original HSI can be maintained. The proposed method has been extensively evaluated on the Indian Pines and FCH datasets. The promising results indicate that GLCSA-Net outperforms the state-of-the-art methods in quantitative and qualitative assessments.
C1 [Chen, Hu; Li, Jia; Zhang, Junjie; Zeng, Dan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shangda Rd 99, Shanghai 200444, Peoples R China.
   [Fu, Yu] Dongguan Univ Technol, Sch Environm & Civil Engn, Xuefu Rd 1, Dongguan 523808, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310018, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Peoples R China.
C3 Shanghai University; Dongguan University of Technology; Hangzhou Dianzi
   University; Hangzhou Dianzi University
RP Zeng, D (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shangda Rd 99, Shanghai 200444, Peoples R China.
EM tigerch@shu.edu.cn; muzijiajia@shu.edu.cn; junjie_zhang@shu.edu.cn;
   fuyu961126@hotmail.com; cgyan@hdu.edu.cn; dzeng@shu.edu.cn
RI Li, Jia/GWC-3327-2022
FU National Natural Science Foundation of China [62202283]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant (No.62202283).
CR Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Cai N, 2017, VISUAL COMPUT, V33, P249, DOI 10.1007/s00371-015-1190-z
   Cheng XY, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3126902
   Dan Y., 2017, IGARSS 2017
   Dua Y, 2022, VISUAL COMPUT, V38, P65, DOI 10.1007/s00371-020-02000-6
   Eismann MT, 2008, IEEE T GEOSCI REMOTE, V46, P237, DOI 10.1109/TGRS.2007.907973
   Esedoglu S, 2002, EUR J APPL MATH, V13, P353, DOI 10.1017/S0956792501004904
   Niresi KF, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3241161
   Fang B, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020159
   Goetz AFH, 2009, REMOTE SENS ENVIRON, V113, pS5, DOI 10.1016/j.rse.2007.12.014
   Hu HJ, 2024, VISUAL COMPUT, V40, P201, DOI 10.1007/s00371-023-02775-4
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Li J, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174418
   Li J, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14143376
   Li J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3098742
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang MY, 2023, VISUAL COMPUT, V39, P4305, DOI 10.1007/s00371-022-02592-1
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Ma AD, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020194
   Melgani F, 2006, IEEE T GEOSCI REMOTE, V44, P442, DOI 10.1109/TGRS.2005.861929
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Phaphuangwittayakul A, 2023, VISUAL COMPUT, V39, P4015, DOI 10.1007/s00371-022-02566-3
   Plaza A, 2009, REMOTE SENS ENVIRON, V113, pS110, DOI 10.1016/j.rse.2007.07.028
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Qing YH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030335
   Rakwatin P, 2009, IEEE T GEOSCI REMOTE, V47, P613, DOI 10.1109/TGRS.2008.2003436
   Salberg AB, 2011, IEEE T GEOSCI REMOTE, V49, P377, DOI 10.1109/TGRS.2010.2052464
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Shi Z, 2019, LECT NOTES COMPUT SC, V11133, P214, DOI 10.1007/978-3-030-11021-5_14
   Sidorov O., 2019, 2019 IEEE CVF INT C
   Sun LW, 2023, OPT QUANT ELECTRON, V55, DOI 10.1007/s11082-022-04399-9
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang ZQ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3112038
   Wong R, 2020, IEEE J-STARS, V13, P4369, DOI 10.1109/JSTARS.2020.3012443
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098
   Yang YZ, 2022, VISUAL COMPUT, V38, P2647, DOI 10.1007/s00371-021-02143-0
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng Y., 2021, Aggregated contextual transformations for high-resolution image inpainting
   Zhang X., 2020, Geosci. J, V13, P335
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao JL, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13234921
   Zhao YS, 2020, INT CONF ACOUST SPEE, P2668, DOI [10.1109/icassp40776.2020.9054658, 10.1109/ICASSP40776.2020.9054658]
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu X., 2022, A transformer-cnn for deep image inpainting forensics
   Zhuang L, 2018, IEEE J-STARS, V11, P730, DOI 10.1109/JSTARS.2018.2796570
NR 50
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3331
EP 3346
DI 10.1007/s00371-023-03036-0
EA SEP 2023
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001058970800002
DA 2024-07-18
ER

PT J
AU Lin, LX
   Zhu, JK
AF Lin, Lixiang
   Zhu, Jianke
TI Topology-preserved human reconstruction with details
SO VISUAL COMPUTER
LA English
DT Article
DE Topology preserved; Human reconstruction
AB Due to the high diversity and complexity of body shapes, it is challenging to directly estimate the human geometry from a single image with the various clothing styles. Most of the model-based approaches are limited to predict the shape and pose of a minimally clothed body with over-smoothing surface. While capturing the fine detailed geometries, the model-free methods are lack of the fixed mesh topology. To address these issues, we propose a novel topology-preserved human reconstruction approach by bridging the gap between model-based and model-free human reconstruction. We present an end-to-end neural network that simultaneously predicts the pixel-aligned implicit surface and an explicit mesh model built by graph convolutional neural network. Experiments on DeepHuman and our collected dataset showed that our approach is effective. The code will be made publicly available at https://github.com/l1346792580123/sdfgcn.
C1 [Lin, Lixiang; Zhu, Jianke] Zhejiang Univ, Coll Comp Sci & technol, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci & technol, Hangzhou, Zhejiang, Peoples R China.
EM jkzhu@zju.edu.cn
OI Zhu, Jianke/0000-0003-1831-0106
FU National Natural Science Foundation of China [61831015]
FX & nbsp;This work is supported by the National Natural Science Foundation
   of China under Grants (61831015).
CR Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127
   Alldieck T, 2018, INT CONF 3D VISION, P98, DOI 10.1109/3DV.2018.00022
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2017, ACM Trans Graph
   Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Chen YC, 2020, COMPUT VIS IMAGE UND, V192, DOI 10.1016/j.cviu.2019.102897
   Choi H., 2020, COMPUTER VISION ECCV, P769
   Corona E, 2021, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR46437.2021.01170
   Desmarais Y, 2021, COMPUT VIS IMAGE UND, V212, DOI 10.1016/j.cviu.2021.103275
   Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Ma QL, 2021, PROC CVPR IEEE, P16077, DOI 10.1109/CVPR46437.2021.01582
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461
   Neophytou Alexandros, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P171, DOI 10.1109/3DV.2014.52
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Renderpeople, About us
   Saito S, 2021, PROC CVPR IEEE, P2885, DOI 10.1109/CVPR46437.2021.00291
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sun WZ, 2022, COMPUT VIS IMAGE UND, V224, DOI 10.1016/j.cviu.2022.103539
   Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Wang JB, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103225
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122
   Yang JL, 2018, LECT NOTES COMPUT SC, V11211, P245, DOI 10.1007/978-3-030-01234-2_15
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462
NR 42
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3609
EP 3619
DI 10.1007/s00371-023-02957-0
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001023823500003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, S
   Dai, J
   Chen, ZM
   Pan, JJ
AF Li, Shuo
   Dai, Ju
   Chen, Zhangmeng
   Pan, Junjun
TI A lightweight pose estimation network with multi-scale receptive field
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose estimation; High-resolution network; Lightweight model;
   Multi-scale receptive field
AB Existing lightweight networks perform inferior to large-scale models in human pose estimation because of shallow model depths and limited receptive fields. Current approaches utilize large convolution kernels or attention mechanisms to encourage long-range receptive field learning at the expense of model redundancy. In this paper, we propose a novel Multi-scale Field Lightweight High-resolution Network (MFite-HRNet) for human pose estimation. Specifically, our model mainly consists of two lightweight blocks, a Multi-scale Receptive Field Block (MRB) and a Large Receptive Field Block (LRB), to learn informative multi-scale and long-range spatial context information. The MRB utilizes group depthwise dilation convolutions with varied dilation rates to extract multi-scale spatial relationships from different feature maps. The LRB leverages large depthwise convolution kernels to model large-range spatial knowledge at the low-level features. We apply MFite-HRNet to single-person and multi-person pose estimation tasks. Experiments on COCO, MPII, and CrowdPose datasets demonstrate that our network outperforms current state-of-the-art lightweight networks in either single-person or multi-person pose estimation tasks. The source code will be publicly available at https://github.com/lskdje/MFite-HRNet.git.
C1 [Li, Shuo; Chen, Zhangmeng; Pan, Junjun] Beihang Univ, Beijing 100191, Peoples R China.
   [Li, Shuo; Dai, Ju; Chen, Zhangmeng; Pan, Junjun] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory
RP Pan, JJ (corresponding author), Beihang Univ, Beijing 100191, Peoples R China.; Dai, J; Pan, JJ (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM leeshuo@buaa.edu.cn; daij@pcl.ac.cn; zhmchen@buaa.edu.cn;
   pan_junjun@buaa.edu.cn
RI Pan, Junjun/A-1316-2013
OI Dai, Ju/0000-0002-9397-8539
FU National Key Ramp;D Program of China [2022ZD0115902]; National Natural
   Science Foundation of China [62102208, 62272017, U20A20195, 62172437]
FX This research is supported by National Key R&D Program of China (No.
   2022ZD0115902) and National Natural Science Foundation of China (Nos.
   62102208, 62272017, U20A20195, 62172437).
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cai H, 2018, AAAI CONF ARTIF INTE, P2787
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Contributors M., 2020, OpenMMLab Pose Estimation Toolbox and Benchmark
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Geng ZG, 2021, PROC CVPR IEEE, P14671, DOI 10.1109/CVPR46437.2021.01444
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jin S., 2020, ECCV, P718
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li Q., 2022, P INT JOINT C ARTIFI, P1095
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Neff C., 2020, ARXIV
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Wang CH, 2024, IEEE CONSUM ELECTR M, V13, P51, DOI 10.1109/MCE.2022.3181759
   Wang Y., 2022, PROC IEEECVF C COMPU, P13126
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 33
TC 1
Z9 1
U1 16
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3429
EP 3440
AR s00371-023-02953-4
DI 10.1007/s00371-023-02953-4
EA JUN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001019713300002
DA 2024-07-18
ER

PT J
AU Jiang, B
   Wang, RJ
   Dai, JW
   Li, Q
   Zeng, WY
AF Jiang, Bin
   Wang, Renjun
   Dai, Jiawu
   Li, Qiao
   Zeng, Weiyuan
TI FBGAN: multi-scale feature aggregation combined with boosting strategy
   for low-light image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Low-light image enhancement; Multi-scale feature aggregation; Generative
   adversarial network; Boosting strategy; Attention mechanism
ID DYNAMIC HISTOGRAM EQUALIZATION
AB Most of the existing low-light image enhancement methods focus only on enhancing the overall image brightness, ignoring the image details during the enhancement process, which leads to problems such as loss of image details and over-smoothing. In addition, the noise presented in the low-light image is still retained or even amplified after enhancement. This paper proposes a single-stage generative adversarial network, dubbed FBGAN, to address the above issues effectively. A multi-scale feature aggregation module based on an error feedback mechanism and a denoising module integrated with boosting strategy guided by attention mechanism are proposed in our model. The former preserves image details entirely during the enhancement, while the latter can simultaneously enhance low-light images and denoise. By these means, our model is competent to restore images with precise details, noise-free, distinct contrast and natural color. Extensive experiments are conducted to show the superiority of our model in terms of both qualitative and quantitative studies.
C1 [Jiang, Bin; Wang, Renjun; Dai, Jiawu; Li, Qiao; Zeng, Weiyuan] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Jiang, Bin; Dai, Jiawu; Li, Qiao; Zeng, Weiyuan] Key Lab Embedded & Network Comp Hunan Prov, Changsha 410082, Peoples R China.
C3 Hunan University
RP Jiang, B (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.; Jiang, B (corresponding author), Key Lab Embedded & Network Comp Hunan Prov, Changsha 410082, Peoples R China.
EM jiangbin@hnu.edu.cn; wrj@hnu.edu.cn
OI Jiang, Bin/0000-0002-5840-9664
FU National Natural Science Foundation of China [62072169]; Natural Science
   Foundation of Hunan Province [2021JJ30138]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072169 and Natural Science Foundation
   of Hunan Province under Grant 2021JJ30138.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2020, IEEE T PATTERN ANAL, V42, P3071, DOI 10.1109/TPAMI.2019.2921548
   Chen C, 2018, LECT NOTES COMPUT SC, V11215, P3, DOI 10.1007/978-3-030-01252-6_1
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Dai SY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1039
   Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Lv F., 2018, P BMVC, V220, P4
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mnih V, 2014, ADV NEUR IN, V27
   Nakai K, 2013, I S INTELL SIG PROC, P445, DOI 10.1109/ISPACS.2013.6704591
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Romano Y, 2015, SIAM J IMAGING SCI, V8, P1187, DOI 10.1137/140990978
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wei C, 2018, ARXIV
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu XM, 2017, IEEE IMAGE PROC, P3190, DOI 10.1109/ICIP.2017.8296871
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xiong W., 2020, ARXIV
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Ying Z, 2017, ARXIV
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 52
TC 0
Z9 0
U1 8
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1745
EP 1756
DI 10.1007/s00371-023-02883-1
EA MAY 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000986398700001
DA 2024-07-18
ER

PT J
AU Zhang, RP
   Chen, CY
   Peng, J
AF Zhang, Ripei
   Chen, Chunyi
   Peng, Jun
TI Multi-scale graph feature extraction network for panoramic image
   saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Panoramic image; Saliency detection; Graph convolution; Super-pixel
   segmentation; Multi-scale fusion
ID VISUAL-ATTENTION; PREDICTION; MODEL
AB The geometric distortion in panoramic images significantly mediates the performance of saliency detection method based on traditional CNN. The strategy of dynamically expanding convolution kernel can achieve good results, but it also produces a lot of computational overhead in the process of reading the adjacency list, which decreases the computational efficiency. The appearance of graph convolution provides a new way to solve such problems. Although using graph convolution can effectively extract the structural features of the graph, it reduces the accuracy of the model resulting from ignoring the spatial features of the image signal. To this end, this paper proposes a construction method of the multi-scale graph structure of the panoramic image and a panoramic image saliency detection model composed of an image saliency feature extraction network and multi-scale saliency feature fusion network combining the image structure information and spatial information in the panoramic image. First, we establish a graph structure consisting of root and leaf nodes obtained by super-pixel segmentation at different scales and spherical Fibonacci sampling, respectively. Then, a feature extraction network composed of two graph convolution layers and two one-dimensional auto-encoders with the same parameterization is used to extract the salient features of the multi-scale graph structure. Finally, the U-Net network fuses the multi-scale saliency features to get the final saliencymap. The results show that the proposed model performs better than the state-of-the-art-models in terms of calculation speed and accuracy.
C1 [Zhang, Ripei; Chen, Chunyi; Peng, Jun] Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
C3 Changchun University of Science & Technology
RP Chen, CY (corresponding author), Changchun Univ Sci & Technol, Sch Comp Sci & Technol, Changchun 130022, Peoples R China.
EM 215312108@qq.com; chenchunyi@hotmail.com
RI Zhang, RIpei/HGC-2847-2022
FU Key-Area Research and Development Program of Guangdong Province
   [2020B1111010002, 2018B010109001]; 2021 Guangdong Provincial Science and
   Technology Special Fund ("Big Project + Task List") [210719145863737];
   Guangdong Marine Economic Development Project [GDNRC[2020]018];
   Laboratory of Autonomous Systems and Network Control of Ministry of
   Education
FX This work was supported by the Key-Area Research and Development Program
   of Guangdong Province under Grant (2020B1111010002, 2018B010109001),
   2021 Guangdong Provincial Science and Technology Special Fund ("Big
   Project + Task List") under Grant 210719145863737, the Guangdong Marine
   Economic Development Project under Grant GDNRC[2020]018, and Laboratory
   of Autonomous Systems and Network Control of Ministry of Education.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Azaza A, 2018, INT MULTICONF SYST, P688, DOI 10.1109/SSD.2018.8570418
   Bogdanova I, 2010, COMPUT VIS IMAGE UND, V114, P100, DOI 10.1016/j.cviu.2009.09.003
   Chang M., 2016, INT C AUGM REAL
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cohen T.S., 2018, arXiv
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Eder Marc, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12423, DOI 10.1109/CVPR42600.2020.01244
   Eder M., 2019, ARXIV
   Filatov A., 2002, INT C DOCUMENT ANAL
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Haoran L., 2020, P 28 ACM INT C MULT
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hu RY, 2021, AAAI CONF ARTIF INTE, V35, P7789
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia L., 2014, OBJECT BASED VISUAL
   Kipf TN, 2016, ARXIV
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Lee Y., 2019, IEEE C COMP VIS PATT
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Marques R, 2013, COMPUT GRAPH FORUM, V32, P134, DOI 10.1111/cgf.12190
   Martin D., 2020, P IEEE C COMP VIS PA, P1
   Martin D., 2020, CVPR WORKSH COMP VIS
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Setlur V., 2004, MUM05 INT C MOB UB M
   Wenguan Wang, 2018, IEEE Transactions on Image Processing, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Xia C, 2016, IEEE T NEUR NET LEAR, V27, P1227, DOI 10.1109/TNNLS.2015.2512898
   Xiaodi H., 2007, 2007 IEEE C COMP VIS
   Xiong B., 2018, EUR C COMP VIS ECCV
   Xu M, 2021, IEEE T IMAGE PROCESS, V30, P2087, DOI 10.1109/TIP.2021.3050861
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yanyu X, 2021, IEEE INT SYMP CIRC S
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang RP, 2023, VISUAL COMPUT, V39, P1163, DOI 10.1007/s00371-021-02395-w
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1198
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 42
TC 0
Z9 0
U1 8
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 953
EP 970
DI 10.1007/s00371-023-02825-x
EA APR 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967899600001
DA 2024-07-18
ER

PT J
AU Esmaeilzehi, A
   Ma, L
   Swamy, MNS
   Ahmad, MO
AF Esmaeilzehi, Alireza
   Ma, Lei
   Swamy, M. N. S.
   Ahmad, M. Omair
TI HighBoostNet: a deep light-weight image super-resolution network using
   high-boost residual blocks
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Image super resolution; High-boost filtering
AB Image distortion is an inevitable part of the image acquisition process, which negatively affects the high-frequency contents of the images. Therefore, it is important to improve the high-frequency contents of the acquired degraded images in the imaging systems and devices. High-boost filtering is an effective method that is used by the scanning and printing devices for enhancing the high-frequency contents of the images and improving their visual quality. In view of this, in this paper, we first develop a residual block for the task of image super-resolution that employs the high-boost filtering operations. It is demonstrated that the super-resolution network, which is formed by a cascade of the proposed residual block, is able to provide a high performance. Further, in this paper, we propose a novel learning method that improves the generalization capability of our deep super-resolution network. Specifically, we generalize the mapping between the spaces of the degraded low-resolution image and the ground-truth image by employing the multiple supervised learning strategy. It is shown that the proposed multiple supervised learning strategy leads to obtaining the weights of our super-resolution network in such a way that its performance is still high when the images are degraded by a set of degradation parameters that is slightly different than that used for the training process.
C1 [Esmaeilzehi, Alireza; Ma, Lei] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada.
   [Swamy, M. N. S.; Ahmad, M. Omair] Concordia Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.
C3 University of Alberta; Concordia University - Canada
RP Esmaeilzehi, A (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada.
EM esmaeilz@ualberta.ca; ma.lei@ualberta.ca; swamy@ece.concordia.ca;
   omair@ece.concordia.ca
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen R, 2020, NEURAL COMPUT APPL, V32, P4885, DOI 10.1007/s00521-018-3886-2
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Esmaeilzehi A., 2020, IEEE INT CON MULTI
   Esmaeilzehi A, 2021, IEEE T COMPUT IMAG, V7, P409, DOI 10.1109/TCI.2021.3070522
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang J., 2021, 2021 IEEECVF INT C C
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mustafa A, 2022, IEEE WINT CONF APPL, P21, DOI 10.1109/WACV51458.2022.00010
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu YR, 2020, NEURAL COMPUT APPL, V32, P14533, DOI 10.1007/s00521-019-04609-8
   Xiaotong Luo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P272, DOI 10.1007/978-3-030-58542-6_17
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Yang Y, 2021, KNOWL-BASED SYST, V233, DOI 10.1016/j.knosys.2021.107520
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H., 2020, ARXIV
NR 41
TC 3
Z9 3
U1 4
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1111
EP 1129
DI 10.1007/s00371-023-02835-9
EA MAR 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967291000003
DA 2024-07-18
ER

PT J
AU Wu, RS
   Bai, J
   Li, WJ
   Jiang, JZ
AF Wu, Rusong
   Bai, Jing
   Li, Wenjing
   Jiang, Jinzhe
TI DCNet: exploring fine-grained vision classification for 3D point clouds
SO VISUAL COMPUTER
LA English
DT Article
DE 3D point clouds; Fine-grained classification; Dynamic adjustment
   learning; Feature extraction
ID NETWORK
AB Fine-grained 3D point cloud classification is vital for shape analysis and understanding. However, due to the subtle inter-class differences and the significant intra-class variations, applying the existing point cloud network directly to fine-grained visual classification tasks may suffer overfitting and cannot achieve good performance. To address this problem, we propose a unified and robust learning framework, named dynamic confusion network (DCNet), which helps the network capture the subtle differences between samples from different sub-categoriesmore robustly. Specifically, in the stage of feature extraction, we design a novel mutual complementary mechanism between an attention block and a dynamic sample confusion block to extract more abundant discriminative features. Furthermore, we construct robust adversarial learning between a dynamic sample confusion loss and a cross-entropy loss based on a siamese network framework to make the network learn more stable feature distributions. We conduct comprehensive experiments and show that DCNet achieves the best performance in three fine-grained categories, with relative accuracy improvements of 1.35%, 1.28%, and 2.30% on Airplane, Car, and Chair, respectively, compared to state-of-the-art point cloud methods. In addition, our approach also achieves comparable performance for the coarse-grained dataset on ModelNet40.
C1 [Wu, Rusong; Bai, Jing; Li, Wenjing; Jiang, Jinzhe] North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Ningxia, Peoples R China.
   [Bai, Jing] North Minzu Univ, IPPRLab, Key Lab Images Proc & Pattern Lab Commiss, Yinchuan 750021, Ningxia, Peoples R China.
C3 North Minzu University; North Minzu University
RP Bai, J (corresponding author), North Minzu Univ, Sch Comp Sci & Engn, Yinchuan 750021, Ningxia, Peoples R China.; Bai, J (corresponding author), North Minzu Univ, IPPRLab, Key Lab Images Proc & Pattern Lab Commiss, Yinchuan 750021, Ningxia, Peoples R China.
EM 1416372803@qq.com; baijing@nun.edu.cn; 20217385@stu.nmu.edu.cn;
   20217432@stu.nmu.edu.cn
RI Bai, Jing/HGD-3571-2022
OI Bai, Jing/0000-0003-4247-6210
FU NationalNatural Science Foundation of China [62162001, 61762003];
   Natural Science Foundation of Ningxia Province of China [2022AAC02041];
   Ningxia Excellent Talent Program, North Minzu University Innovation
   Project [YCX21093]
FX Thank the support of the NationalNatural Science Foundation of China
   under Grant 62162001, 61762003, the Natural Science Foundation of
   Ningxia Province of China under Grant 2022AAC02041, the Ningxia
   Excellent Talent Program, North Minzu University Innovation Project
   (YCX21093).
CR [白静 Bai Jing], 2019, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V31, P1917
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dominguez M, 2018, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV.2018.00218
   Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Feng MT, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107446
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P600, DOI 10.1109/ICCV48922.2021.00066
   Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Lan SY, 2019, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2019.00109
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li M, 2022, VISUAL COMPUT, V38, P811, DOI 10.1007/s00371-020-02052-8
   Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228
   Li YZ, 2018, ADV NEUR IN, V31
   Liu J., 2020, MEMORY BASED JITTER
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P1744, DOI 10.1109/TIP.2020.3048623
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Ma Xiaoteng, 2022, INT C LEARN REPR
   Pan L, 2020, IEEE INT CONF ROBOT, P1113, DOI [10.1109/icra40945.2020.9197499, 10.1109/ICRA40945.2020.9197499]
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu S., 2021, IEEE T PATTERN ANAL
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wen X, 2020, IEEE T IMAGE PROCESS, V29, P8855, DOI 10.1109/TIP.2020.3019925
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang Kevin, 2019, ARXIV
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 45
TC 1
Z9 1
U1 5
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 781
EP 797
DI 10.1007/s00371-023-02816-y
EA MAR 2023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000957508600001
DA 2024-07-18
ER

PT J
AU Zhang, JD
   Xiu, Y
AF Zhang, Jindong
   Xiu, Ying
TI Image stitching based on human visual system and SIFT algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Image stitching; Human visual system; Image fusion; Optimal seamline
   detection
ID FRAMEWORK; FEATURES; NETWORK
AB The image stitching process often produces many undesirable effects. Solving problems such as discontinuity and dislocation of pictures has always been the focus of people's research. From the perspective of human vision, this dislocation situation can be easily perceived and found. In this paper, we propose a stitching strategy based on the human visual system (HVS) and scale-invariant feature transform (SIFT) algorithm. We preprocess the brightness difference and contrast of the stitched images, combining SIFT algorithm and HVS to divide the overlapping areas of the stitched images and establish an attribute relationship model. We use dynamic programming to find the optimal seamline according to the attribute relationship model, and the final result makes the optimal seamline almost invisible under the discriminative vision of human eyes. The experimental results show that our method has more advantages in the HVS.
C1 [Zhang, Jindong; Xiu, Ying] Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.
   [Zhang, Jindong] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Zhang, JD (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.; Zhang, JD (corresponding author), Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China.
EM zhangjindong_100@163.com
FU National Key Research and Development Program of China [2017YFB0102500];
   National Natural Science Foundation of China [61872158, 62172186];
   Science and Technology Development Plan Project of Jilin Province
   [20190701019GH, 20200401132GX]; Korea Foundation for Advanced Studies'
   International Scholar Exchange Fellowship for the academic year of
   2017-2018; Fundamental Research Funds for the Chongqing Research
   Institute Jilin University [2021DQ0009]; Fundamental Research Funds for
   the Central Universities
FX This study was supported by the National Key Research and Development
   Program of China (2017YFB0102500), the National Natural Science
   Foundation of China (61872158, 62172186), the Science and Technology
   Development Plan Project of Jilin Province (20190701019GH,
   20200401132GX), the Korea Foundation for Advanced Studies' International
   Scholar Exchange Fellowship for the academic year of 2017-2018, the
   Fundamental Research Funds for the Chongqing Research Institute Jilin
   University (2021DQ0009), and the Fundamental Research Funds for the
   Central Universities.
CR Agaoglu S, 2015, VISION RES, V115, P23, DOI 10.1016/j.visres.2015.07.003
   Anzid H, 2023, VISUAL COMPUT, V39, P1667, DOI 10.1007/s00371-022-02435-z
   Cao QJ, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041462
   Chen J, 2022, VISUAL COMPUT, V38, P3191, DOI 10.1007/s00371-022-02564-5
   Dehaene S, 2003, TRENDS COGN SCI, V7, P145, DOI 10.1016/S1364-6613(03)00055-X
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Goodhew SC, 2012, COGNITION, V122, P405, DOI 10.1016/j.cognition.2011.11.010
   Hejazifar H, 2018, SIGNAL IMAGE VIDEO P, V12, P885, DOI 10.1007/s11760-017-1231-3
   Hossein-Nejad Z, 2022, VISUAL COMPUT, V38, P1991, DOI 10.1007/s00371-021-02261-9
   Hu FC, 2018, LECT NOTES ARTIF INT, V11013, P140, DOI 10.1007/978-3-319-97310-4_16
   Jia Q, 2021, PROC CVPR IEEE, P12181, DOI 10.1109/CVPR46437.2021.01201
   Kerschner M, 2001, ISPRS J PHOTOGRAMM, V56, P53, DOI 10.1016/S0924-2716(01)00033-8
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Krishnakumar K, 2020, VISUAL COMPUT, V36, P1837, DOI 10.1007/s00371-019-01780-w
   Kyu-Yul Lee, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8195, DOI 10.1109/CVPR42600.2020.00822
   Lee H, 2020, INT J ENG BUS MANAG, V12, DOI 10.1177/1847979020980928
   Li J, 2020, IEEE T IMAGE PROCESS, V29, P2356, DOI 10.1109/TIP.2019.2949424
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li L, 2019, ISPRS J PHOTOGRAMM, V148, P41, DOI 10.1016/j.isprsjprs.2018.12.002
   Li L, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9070701
   Li L, 2016, ISPRS J PHOTOGRAMM, V113, P1, DOI 10.1016/j.isprsjprs.2015.12.007
   Li N, 2018, SIGNAL IMAGE VIDEO P, V12, P967, DOI 10.1007/s11760-018-1241-9
   Liao TL, 2020, IEEE T IMAGE PROCESS, V29, P724, DOI 10.1109/TIP.2019.2934344
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Liu TB, 2022, J SUPERCOMPUT, V78, P12973, DOI 10.1007/s11227-022-04395-6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Pham NT, 2021, IEEE ACCESS, V9, P127852, DOI 10.1109/ACCESS.2021.3111203
   Nie L, 2021, IEEE T IMAGE PROCESS, V30, P6184, DOI 10.1109/TIP.2021.3092828
   Niu C, 2013, VISUAL COMPUT, V29, P253, DOI 10.1007/s00371-012-0763-3
   Sheng MW, 2020, INT J ADV ROBOT SYST, V17, DOI 10.1177/1729881420915062
   Shi ZF, 2017, DIGIT SIGNAL PROCESS, V60, P277, DOI 10.1016/j.dsp.2016.09.013
   Sun JZ, 2012, J MATH PSYCHOL, V56, P495, DOI 10.1016/j.jmp.2012.08.002
   Wang B, 2020, EARTH SCI INFORM, V13, P333, DOI 10.1007/s12145-019-00421-z
   Wang L, 2020, PROCEEDINGS OF 2020 IEEE 4TH INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2020), P694, DOI [10.1109/ITNEC48623.2020.9084886, 10.1109/itnec48623.2020.9084886]
   Wang ZB, 2020, MULTIMEDIA SYST, V26, P413, DOI 10.1007/s00530-020-00651-y
   Zhang J., 2022, WIRES COMPUT MOL SCI
   Zhang JD, 2021, MULTIMED TOOLS APPL, V80, P18181, DOI 10.1007/s11042-020-10370-1
   Zhang L, 2013, IEEE IMAGE PROC, P171, DOI 10.1109/ICIP.2013.6738036
   Zhang WL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041214
   Zhu CH, 2019, PROCEEDINGS OF 2019 IEEE 8TH JOINT INTERNATIONAL INFORMATION TECHNOLOGY AND ARTIFICIAL INTELLIGENCE CONFERENCE (ITAIC 2019), P722, DOI [10.1109/itaic.2019.8785712, 10.1109/ITAIC.2019.8785712]
NR 40
TC 10
Z9 10
U1 9
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 427
EP 439
DI 10.1007/s00371-023-02791-4
EA FEB 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000929399700001
DA 2024-07-18
ER

PT J
AU Eqtedaei, A
   Ahmadyfard, A
AF Eqtedaei, Amir
   Ahmadyfard, Alireza
TI Coarse-to-fine blind image deblurring based on K-means clustering
SO VISUAL COMPUTER
LA English
DT Article
DE Blind image deblurring; ?-means clustering; Kernel estimation
AB Blind image deblurring is a challenging image processing problem, and a proper solution for this problem has many applications in the real world. This is an ill-posed problem, as both the sharp image and blur kernel are unknown. The traditional methods based on maximum a posterior (MAP) apply heavy constraints on the latent image or blur kernel to find the solution. However, these constraints are not always effective; meanwhile, they are very time-consuming. Recently, new approaches based on deep learning have emerged. The methods based on this approach suffer from two problems: the need for a large number of images and kernels for training and also the dependency of the result on the training data. In this paper, we propose a multiscale method based on MAP framework for image motion deblurring. In this method, we represent the blurry image in different scales. We suggest segmenting the image of each scale using kappa-means clustering. Using the image information at dominant edges guided by the segmented images, the blur kernel is estimated at each scale. The blur kernel at the finest level of the pyramid is estimated from the coarser levels in a coarse-to-fine manner. Unlike the existing MAP-based methods, the proposed method does not need mathematically complicated assumptions to estimate the intermediate latent image. So the proposed image deblurring is run fast. We evaluated the proposed method and compared it to the existing methods. The experimental results on real and synthetic blurry images demonstrate that the proposed scheme has promising results. The proposed method competes with the existing MAP-based methods for reconstructing qualitative sharp images, while the execution time for our method is considerably less.
C1 [Eqtedaei, Amir; Ahmadyfard, Alireza] Shahrood Univ Technol, Fac Elect Engn, Shahrud, Iran.
C3 Shahrood University of Technology
RP Eqtedaei, A (corresponding author), Shahrood Univ Technol, Fac Elect Engn, Shahrud, Iran.
EM amireqtedaei1996@gmail.com
CR Aliyan S., 2012, INT J MACH LEARN CYB, DOI [10.14569/IJARAI.2012.010202, DOI 10.14569/IJARAI.2012.010202]
   Bai YC, 2020, IEEE T CIRC SYST VID, V30, P2033, DOI 10.1109/TCSVT.2019.2919159
   Bai YC, 2019, IEEE T IMAGE PROCESS, V28, P1404, DOI 10.1109/TIP.2018.2874290
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Dhanachandra N, 2015, PROCEDIA COMPUT SCI, V54, P764, DOI 10.1016/j.procs.2015.06.090
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Kotera Jan, 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P59, DOI 10.1007/978-3-642-40246-3_8
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2014, IEEE INT CON MULTI
   Pan JS, 2013, SIGNAL PROCESS-IMAGE, V28, P1156, DOI 10.1016/j.image.2013.05.001
   Ren DW, 2020, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR42600.2020.00340
   Ren WQ, 2016, IEEE T IMAGE PROCESS, V25, P3426, DOI 10.1109/TIP.2016.2571062
   Schuler CJ, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481418
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Sun L., 2013, IEEE INT C COMPUTATI, P1, DOI [10.1109/ICCPhot.2013.6528301, DOI 10.1109/ICCPHOT.2013.6528301]
   Xu L., 2010, LECT NOTES COMPUT SC, DOI 10.1007/ 978-3-642-15549-9_12
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   YAN YY, 2017, PROC CVPR IEEE, P6978, DOI DOI 10.1109/CVPR.2017.738
   Yuan Q, 2020, VISUAL COMPUT, V36, P1591, DOI 10.1007/s00371-019-01762-y
NR 37
TC 4
Z9 4
U1 6
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 333
EP 344
DI 10.1007/s00371-023-02785-2
EA FEB 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000934214900002
DA 2024-07-18
ER

PT J
AU Zeng, QT
   Chang, S
   Wang, SS
   Ni, WJ
AF Zeng, Qingtian
   Chang, Sai
   Wang, Shansong
   Ni, Weijian
TI Multi-scale adaptive learning network with double connection mechanism
   for super-resolution on agricultural pest images
SO VISUAL COMPUTER
LA English
DT Article
DE Agricultural pest; Super-resolution; Multi-scale expression; Adaptive
   learning; Residual and dense connection
AB Accurate recognition of pests can effectively reduce the negative impact of pests on the agricultural economy. The unprofessional shooting ways result in low-resolution images, which seriously influences the accuracy of pest recognition. The super-resolution reconstruction method can transform low-resolution images into high-resolution images. The existing super-resolution reconstruction networks have the problems such as feature missing, low feature utilization, and a large amount of computation. The multi-scale adaptive learning network (MALNet) proposed in this paper is based on residual and dense connections and consists of four modules: rear-compensation enlargement module (RCEM), low-frequency feature building module (LFFBM), multi-scale details representation module (MsDRM), and feature channels correction module (FCCM). First, the low-frequency contour feature is built by LFFBM, and MsDRM extracts local detail features of the pest. After that, FCCM enhances important features, being fused with the enlarged feature map generated by RCEM. The experimental results show that, compared with other reconstruction networks, MALNet achieves an average improvement of 0.26 dB and 0.006 in peak signal- to-noise ratio and structural similarity.
C1 [Zeng, Qingtian; Chang, Sai; Wang, Shansong; Ni, Weijian] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Peoples R China.
C3 Shandong University of Science & Technology
RP Wang, SS; Ni, WJ (corresponding author), Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Peoples R China.
EM qtzeng@163.com; changss722@163.com; wangss15689457686@163.com;
   niweijian@gmail.com
FU NSFC [U1931207, 61702306]; Sci. & Tech. Development Fund of Shandong
   Province of China [ZR2022MF288, ZR2017MF027]; Taishan Scholar Program of
   Shandong Province; Shandong Provincial Natural Science Foundation
   [ZR2022MF319]
FX This work was supported in part by NSFC(U1931207 and 61702306), Sci. &
   Tech. Development Fund of Shandong Province of China (ZR2022MF288 and
   ZR2017MF027), and the Taishan Scholar Program of Shandong Province.
   Shandong Provincial Natural Science Foundation (ZR2022MF319).
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Bi ZW, 2022, IEEE T IMAGE PROCESS, V31, P6664, DOI 10.1109/TIP.2022.3214336
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Cao B, 2020, AAAI CONF ARTIF INTE, V34, P10486
   Chen HG, 2022, INFORM FUSION, V79, P124, DOI 10.1016/j.inffus.2021.09.005
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   DOUILLARD C, 1995, EUR T TELECOMMUN, V6, P507, DOI 10.1002/ett.4460060506
   Emad M, 2021, IEEE WINT CONF APPL, P1629, DOI 10.1109/WACV48630.2021.00167
   Esmaeilzehi A, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116228
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Joze HRV, 2020, IEEE COMPUT SOC CONF, P2190, DOI 10.1109/CVPRW50498.2020.00267
   Jun-Jie Huang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition: Workshops (CVPRW), P1067, DOI 10.1109/CVPRW.2017.144
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2020, IEEE SIGNAL PROC LET, V27, P1190, DOI 10.1109/LSP.2020.3005043
   Li BC, 2020, PATTERN RECOGN LETT, V130, P21, DOI 10.1016/j.patrec.2018.07.023
   Li J., 2021, HENAN AGR, P49
   Li YW, 2020, RULE LAW CHINA COMPA, P1
   Li Z, 2020, NEUROCOMPUTING, V398, P377, DOI 10.1016/j.neucom.2019.04.004
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Ma XH, 2021, SCI PROGRESS-UK, V104, DOI 10.1177/00368504211011343
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Niu B., 2020, EUR C COMP VIS, P191, DOI [10.1007/978-3-030-58610-2_47, DOI 10.1007/978-3-030-58610-2_12]
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi XJ, 2015, ADV NEUR IN, V28
   Song DH, 2021, PROC CVPR IEEE, P15643, DOI 10.1109/CVPR46437.2021.01539
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang PJ, 2022, EARTH-SCI REV, V232, DOI 10.1016/j.earscirev.2022.104110
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei P., 2020, P EUR C COMP VIS, P101
   Xiao J., 2020, P AS C COMP VIS ACCV, P80
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang F., 2021, ACTA AUTOM SIN, V47, P10001
   Zhang J, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115925
   Zhang XY, 2020, INT J COMPUT VISION, V128, P1699, DOI 10.1007/s11263-019-01285-y
   Zhang Y., 2021, J PHYS C SER, V1802
   Zhu, 2021, SEED SCI TECH, V39, P2
NR 44
TC 1
Z9 1
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 153
EP 167
DI 10.1007/s00371-023-02772-7
EA JAN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000920846100001
DA 2024-07-18
ER

PT J
AU Ketab, F
   Russel, NS
   Selvaraj, A
   Buhari, SM
AF Ketab, Faris
   Russel, Newlin Shebiah
   Selvaraj, Arivazhagan
   Buhari, Seyed Mohamed
TI Parallel deep learning architecture with customized and learnable
   filters for low-resolution face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Low-resolution face recognition; Gabor filters; Convolutional neural
   network; Surveillance
ID SUPERRESOLUTION
AB Face recognition in visual surveillance systems is important for various applications to identify individuals who are behaving defiantly at the time of an event or for investigation purposes. Despite the dramatic improvements in facial recognition technology in recent years, it is difficult to recognize faces from surveillance feeds due to the presence of multiple people of different scales and orientations. This paper solves the task of low-resolution face recognition by combining exemplary techniques for extracting distinct features. This research utilizes the attributes learned by customized and learnable filters and injected in the training process to better match them with human brain functionality. The Gabor transform aims to convolve a facial image using a range of Gabor filter coefficients at various scales and orientations, resulting in scale and rotation invariant features. The tailored architecture with residual stream aims to enhance functional representation and prevent the gradient of the prediction engine from affecting the backbone network functional map. Experimental analysis is performed on the SCface and TinyFace databases and is reported with an accuracy of 89.21% on the SCface database and 56.68% on the TinyFace database.
C1 [Ketab, Faris; Buhari, Seyed Mohamed] King Abdulaziz Univ, Dept Informat Technol, Jeddah 21589, Saudi Arabia.
   [Russel, Newlin Shebiah; Selvaraj, Arivazhagan] Mepco Schlenk Engn Coll, Ctr Image Proc & Pattern Recognit, Dept Elect & Commun Engn, Sivakasi 626005, Tamilnadu, India.
   [Buhari, Seyed Mohamed] Univ Teknol Brunei, Sch Business, Bandar Seri Begawan, Brunei.
C3 King Abdulaziz University; Mepco Schlenk Engineering College; University
   of Technology Brunei
RP Russel, NS (corresponding author), Mepco Schlenk Engn Coll, Ctr Image Proc & Pattern Recognit, Dept Elect & Commun Engn, Sivakasi 626005, Tamilnadu, India.
EM newlinshebiah@mepcoeng.ac.in
RI kateb, Faris A/C-5042-2019
OI kateb, Faris A/0000-0003-3226-5583; R, Newlin
   Shebiah/0000-0002-0835-5848
FU Deanship of scientific Research (DsR) at King Abdulaziz University,
   Jeddah [J: 1-611-1441]; DSR
FX AcknowledgementsAuthors would like to acknowledge Principal and
   Management of Mepco Schlenk Engineering College, Sivakasi for their
   encouragement and technical support for this research work. This Project
   was funded by the Deanship of scientific Research (DsR) at King
   Abdulaziz University, Jeddah, under grant no. (J: 1-611-1441). The
   authors, therefore, acknowledge with thanks DSR for technical and
   financial support.
CR Cheng ZY, 2019, LECT NOTES COMPUT SC, V11363, P605, DOI 10.1007/978-3-030-20893-6_38
   Fookes C, 2012, J VIS COMMUN IMAGE R, V23, P75, DOI 10.1016/j.jvcir.2011.06.004
   Gao XL, 2022, EXPERT SYST APPL, V209, DOI 10.1016/j.eswa.2022.118275
   Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Gupta S, 2021, VISUAL COMPUT, V37, P447, DOI 10.1007/s00371-020-01814-8
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hennings-Yeomans PH, 2008, PROC CVPR IEEE, P3637
   Hennings-Yeomans PH, 2009, IEEE IMAGE PROC, P33, DOI 10.1109/ICIP.2009.5413920
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Khalid Syed Safwan, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P410, DOI 10.1109/TBIOM.2020.3007356
   Li CR, 2021, PATTERN RECOGN, V119, DOI 10.1016/j.patcog.2021.108085
   Li P, 2019, IEEE T INF FOREN SEC, V14, P2000, DOI 10.1109/TIFS.2018.2890812
   Lu ZJ, 2018, IEEE ACCESS, V6, DOI [10.1109/ACCESS.2018.2864189, 10.1109/LSP.2018.2810121]
   Maity S, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091013
   Meedeniya D. A., 2007, Second International Conference on Industrial and Information Systems - 2007, P347, DOI 10.1109/ICIINFS.2007.4579200
   Mishra NK, 2021, IMAGE VISION COMPUT, V115, DOI 10.1016/j.imavis.2021.104290
   Mudunuri SP, 2018, IEEE COMPUT SOC CONF, P602, DOI 10.1109/CVPRW.2018.00090
   Mudunuri SP, 2016, IEEE T PATTERN ANAL, V38, P1034, DOI 10.1109/TPAMI.2015.2469282
   Obara K, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P478, DOI 10.23919/MVA.2017.7986904
   Peng Y., 2016, LECT NOTES INFORMATI, pP, DOI [10.1109/BIOSIG.2016.7736917, DOI 10.1109/BIOSIG.2016.7736917]
   Rouhsedaghat M, 2021, PATTERN RECOGN LETT, V149, P193, DOI 10.1016/j.patrec.2021.05.009
   Song YZ, 2022, NEUROCOMPUTING, V509, P193, DOI 10.1016/j.neucom.2022.08.058
   Uzun-Per M, 2018, SIGNAL PROCESS-IMAGE, V61, P85, DOI 10.1016/j.image.2017.11.003
   Wang ZH, 2016, PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON ENERGY HARVESTING AND ENERGY-NEUTRAL SENSING SYSTEMS (ENSSYS'16), P1, DOI [10.1109/ICAUMS.2016.8479999, 10.1145/2996884.2996885]
   Wilman W.W.Z., 2010, P 4 IEEE INT C BIOME, P1, DOI 10.1109/BTAS.2010.5634490
   Xie JH, 2022, VISUAL COMPUT, V38, P2515, DOI 10.1007/s00371-021-02127-0
   Yang FW, 2018, IEEE SIGNAL PROC LET, V25, P388, DOI 10.1109/LSP.2017.2746658
   Yin X., P ASIAN C COMPUTER V
   Yuge Huang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P138, DOI 10.1007/978-3-030-58577-8_9
   Zangeneh E, 2020, EXPERT SYST APPL, V139, DOI 10.1016/j.eswa.2019.112854
   Zhang P, 2015, OPTIK, V126, P4352, DOI 10.1016/j.ijleo.2015.08.138
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 35
TC 2
Z9 2
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6699
EP 6710
DI 10.1007/s00371-022-02757-y
EA JAN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000910753800002
DA 2024-07-18
ER

PT J
AU Mahmud, H
   Morshed, MM
   Hasan, MK
AF Mahmud, Hasan
   Morshed, Mashrur M.
   Hasan, Md. Kamrul
TI Quantized depth image and skeleton-based multimodal dynamic hand gesture
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional recurrent neural networks; Dynamic hand gesture
   recognition; Multimodal-fusion networks; Depth image; Hand skeleton
   joint points
ID CONVOLUTIONAL NEURAL-NETWORKS; DATASET
AB An existing approach to dynamic hand gesture recognition is to use multimodal-fusion CRNN (Convolutional Recurrent Neural Networks) on depth images and corresponding 2D hand skeleton coordinates. However, an underlying problem in this method is that raw depth images possess a very low contrast in the hand ROI (region of interest). They do not highlight the details which are important to fine-grained hand gesture recognition details such as finger orientation, the overlap between the fingers and the palm, or overlap between multiple fingers. To address this issue, we propose generating quantized depth images as an alternative input modality to raw depth images. This creates sharp relative contrasts between key parts of the hand, which improves gesture recognition performance. In addition, we explore some ways to tackle the high variance problem in previously researched multimodal-fusion CRNN architectures. We obtained accuracies of 90.82 and 89.21% (14 and 28 gestures, respectively) on the DHG-14/28 dataset and accuracies of 93.81 and 90.24% (14 and 28 gestures, respectively) on the SHREC-2017 dataset, which is a significant improvement over previous multimodal-dusion CRNNs.
C1 [Mahmud, Hasan; Morshed, Mashrur M.; Hasan, Md. Kamrul] Islamic Univ Technol, Dept Comp Sci & Engn, Syst & Software Lab SSL, Dhaka 1704, Gazipur, Bangladesh.
RP Mahmud, H (corresponding author), Islamic Univ Technol, Dept Comp Sci & Engn, Syst & Software Lab SSL, Dhaka 1704, Gazipur, Bangladesh.
EM hasan@iut-dhaka.edu
RI Hasan, Kamrul/AAY-6295-2020
OI Hasan, Kamrul/0000-0001-9099-5424; Mahmud, Hasan/0000-0003-4375-6943
CR Araujo A., 2019, DISTILL, V4, pe21, DOI 10.23915/distill.00021
   Barbhuiya AA, 2021, MULTIMED TOOLS APPL, V80, P3051, DOI 10.1007/s11042-020-09829-y
   Chen XH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020239
   Chen Y., 2019, ARXIV
   De Smedt Q., 2017, 3DOR 10 EUR WORKSH 3, P1, DOI DOI 10.2312/3DOR.20171049
   De Smedt Q, 2016, IEEE COMPUT SOC CONF, P1206, DOI 10.1109/CVPRW.2016.153
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Desai S, 2017, ADV INTELL SYST, V508, P19, DOI 10.1007/978-981-10-2750-5_3
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Facebook, 2019, FVCOR LIB
   Foto B.H., 2021, INTEL REALSENSE DEPT
   Geirhos Robert, 2018, ICLR
   Hou J., 2018, Proceedings of the European Conference on Computer Vision (ECCV), P0
   Iwai Y, 1996, INFORMATION INTELLIGENCE AND SYSTEMS, VOLS 1-4, P76, DOI 10.1109/ICSMC.1996.569743
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Köpüklü O, 2020, IEEE INT CONF AUTOMA, P77, DOI 10.1109/FG47880.2020.00041
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Köpüklü O, 2018, IEEE COMPUT SOC CONF, P2184, DOI 10.1109/CVPRW.2018.00284
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A, 2012, EUR SIGNAL PR CONF, P1975
   Lai K, 2018, INT C PATT RECOG, P3451, DOI 10.1109/ICPR.2018.8545718
   Lai KX, 2020, IEEE IND APPLIC SOC, DOI 10.1109/IAS44978.2020.9334881
   Loshchilov I., 2018, arXiv
   Mahmud H, 2022, VISUAL COMPUT, V38, P1015, DOI 10.1007/s00371-021-02065-x
   Min YC, 2020, PROC CVPR IEEE, P5760, DOI 10.1109/CVPR42600.2020.00580
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Nagi J., 2011, 2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA 2011), P342, DOI 10.1109/ICSIPA.2011.6144164
   Naguri CR, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P1130, DOI 10.1109/ICMLA.2017.00013
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oudah M, 2020, J IMAGING, V6, DOI 10.3390/jimaging6080073
   Pintea SL, 2019, LECT NOTES COMPUT SC, V11134, P213, DOI 10.1007/978-3-030-11024-6_14
   Ramachandran P., 2017, ARXIV
   Rogozhnikov A., 2018, EINOPS FLEXIBLE POWE
   Tao WJ, 2018, ENG APPL ARTIF INTEL, V76, P202, DOI 10.1016/j.engappai.2018.09.006
   Vandersteegen M., 2020, PROC IEEECVF C COMPU, P98
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
NR 37
TC 6
Z9 6
U1 3
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 11
EP 25
DI 10.1007/s00371-022-02762-1
EA JAN 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000907765600002
DA 2024-07-18
ER

PT J
AU Liu, JY
   Li, YZ
   Goel, M
AF Liu, Jingyang
   Li, Yunzhi
   Goel, Mayank
TI A semantic-based approach to digital content placement for immersive
   environments
SO VISUAL COMPUTER
LA English
DT Article
DE Digital content placement; Semantic-based interaction; Scene
   understanding
ID BEZIER CURVE; SEGMENTATION
AB This paper presents a semantic-based interactive system that enables virtual content placement using natural language. We propose a novel computational framework composed of three components including 3D reconstruction, 3D segmentation, and 3D annotation. Based on the framework, the system can automatically construct a semantic representation of the environment from raw point cloud data. Users can then assign virtual content to a specific physical location by referring to its semantic label. Compared with traditional projection mapping which may involve tedious manual adjustments, the proposed system can facilitate intuitive and efficient manipulation of virtual content in immersive environments through speech inputs. The technical evaluation and user study results show that the system can provide users with accurate semantic information for effective virtual content placement at room scale.
C1 [Liu, Jingyang; Li, Yunzhi; Goel, Mayank] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Liu, JY (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM jingyanl@andrew.cmu.edu; yunzhil@andrew.cmu.edu; mayankgoel@cmu.edu
OI Liu, Jingyang/0000-0002-2671-4015
CR Ahuja K, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P189, DOI 10.1145/3332165.3347884
   [Anonymous], 2011, P 24 ANN ACM S US IN, DOI DOI 10.1145/2047196.2047255
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Armeni Iro, 2017, arXiv
   Barreira J, 2018, IEEE T VIS COMPUT GR, V24, P1223, DOI 10.1109/TVCG.2017.2676777
   Bashir U, 2013, APPL MATH COMPUT, V219, P10183, DOI 10.1016/j.amc.2013.03.110
   Bell B., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P101, DOI 10.1145/502348.502363
   BERKMANN J, 1994, IEEE T PATTERN ANAL, V16, P1114, DOI 10.1109/34.334391
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Bibi SMA, 2019, IEEE ACCESS, V7, P165779, DOI 10.1109/ACCESS.2019.2953496
   Billinghurst M., 1998, Computer Graphics, V32, P60, DOI 10.1145/307710.307730
   Bolt R. A., 1980, Computer Graphics, V14, P262, DOI 10.1145/965105.807503
   Chang A, 2017, ARXIV
   Chang A. X., 2015, ARXIV
   Chen L, 2020, COMPUT GRAPH FORUM, V39, P484, DOI 10.1111/cgf.13887
   Chen Q., 2019, arXiv
   Cohen P. R., 1992, UIST. Fifth Annual Symposium on User Interface Software and Technology. Proceedings, P143, DOI 10.1145/142621.142641
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Devlin J., 2018, BERT PRE TRAINING DE
   Du R., 2020, P 33 ANN ACM S USER, P829, DOI 10.1145/3379337. 3415881
   Fayolle PA, 2013, VISUAL COMPUT, V29, P449, DOI 10.1007/s00371-012-0749-1
   Fender A, 2019, PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ISS '19), P303, DOI 10.1145/3343055.3359715
   Fender A, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173843
   Fender A, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P611, DOI 10.1145/3126594.3126621
   Gal R, 2014, INT SYM MIX AUGMENT, P207, DOI 10.1109/ISMAR.2014.6948429
   Grasset R, 2012, INT SYM MIX AUGMENT, P177, DOI 10.1109/ISMAR.2012.6402555
   Izadi Shahram., 2011, Proceedings of the 24th annual ACM symposium on User interface software and technology - UIST'11, page, P559, DOI DOI 10.1145/2047196.2047270
   Jagannathan A, 2007, IEEE T PATTERN ANAL, V29, P2195, DOI 10.1109/TPAMI.2007.1125
   Jones Brett, 2014, P 27 ANN ACM S US IN, P637, DOI [10.1145/2642918.2647383, DOI 10.1145/2642918.2647383]
   Kaiser E., 2003, ICMI 03, P12
   Katz S, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276407, 10.1145/1239451.1239475]
   Li CL, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3824
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   LIAO CW, 1990, PATTERN RECOGN, V23, P475, DOI 10.1016/0031-3203(90)90068-V
   Lindlbauer D, 2019, PROCEEDINGS OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY (UIST 2019), P147, DOI 10.1145/3332165.3347945
   Liu J., 2021, 2021 17 INT C INTELL, P1
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lucente M., 1998, INTELLIGENT ENV S, V98
   Maqsood S, 2020, ADV DIFFER EQU-NY, V2020, DOI 10.1186/s13662-020-03001-4
   Marsh E., 1994, P AAAI SPRING S MULT
   Mattausch O, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12286
   Nuernberger B, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1233, DOI 10.1145/2858036.2858250
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Raskar R, 2001, SPRING EUROGRAP, P89
   Raskar R, 1999, AUGMENTED REALITY, P63
   Raskar R., 2006, ILAMPS GEOMETRICALLY, P7
   Rekimoto J., 1999, P SIGCHI C HUMAN FAC, P378
   Rusu RB, 2008, ROBOT AUTON SYST, V56, P927, DOI 10.1016/j.robot.2008.08.005
   Sappa AD, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P292, DOI 10.1109/IM.2001.924460
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Tatzgern M, 2016, P IEEE VIRT REAL ANN, P83, DOI 10.1109/VR.2016.7504691
   Vaswani A, 2017, ADV NEUR IN, V30
   Weimer D., 1989, SIGCHI Bulletin, P235, DOI 10.1145/67450.67495
   Wilson AD, 2007, SECOND ANNUAL IEEE INTERNATIONAL WORKSHOP ON HORIZONTAL INTERACTIVE HUMAN-COMPUTER SYSTEMS, PROCEEDINGS, P201, DOI 10.1109/TABLETOP.2007.35
   WINOGRAD T, 1972, COGNITIVE PSYCHOL, V3, P1, DOI 10.1016/0010-0285(72)90002-3
   Xiao R., 2013, P SIGCHI C HUM FACT, P879, DOI DOI 10.1145/2470654.2466113
   Zhou J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1127
   Zhou Q-Y, 2018, ARXIV
NR 66
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 5989
EP 6003
DI 10.1007/s00371-022-02707-8
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000880493200002
DA 2024-07-18
ER

PT J
AU Yang, JC
   Ma, MX
   Zhang, J
   Wang, C
AF Yang, Junci
   Ma, Mingxi
   Zhang, Jun
   Wang, Chao
TI Noise removal using an adaptive Euler's elastica-based model
SO VISUAL COMPUTER
LA English
DT Article
DE Noise removal; Euler's elastica regularization; Adaptive weighted
   matrix; Alternating direction method of multipliers
ID AUGMENTED LAGRANGIAN METHOD; FAST ALGORITHM; IMAGE; CURVATURE;
   MINIMIZATION; REGULARIZATION; RESTORATION
AB We consider the Euler's elastica-based model for an image noise removal problem. Several recent works have demonstrated that Euler's elastica-based model performs better than the celebrated total variation model on preserving image features in smooth regions during denoising. On the other hand, an adaptive weighting scheme on total variation is a technique for well-restoring local features of an image. Inspired by these two strategies, we propose an adaptive Euler's elastica-based model to handle both local features of image and image features in smooth regions simultaneously. Numerically, we apply the alternating direction method of multipliers to solve this non-smooth and non-convex model. Experimental results on both natural and synthetic images illustrate the efficiency of the proposed method.
C1 [Ma, Mingxi; Zhang, Jun] Nanchang Inst Technol, Coll Sci, Nanchang 330099, Jiangxi, Peoples R China.
   [Yang, Junci; Zhang, Jun] Nanchang Inst Technol, Jiangxi Prov Key Lab Water Informat Cooperat Sens, Nanchang 330099, Jiangxi, Peoples R China.
   [Wang, Chao] Southern Univ Sci & Technol, Dept Stat & Data Sci, Shenzhen 518055, Guangdong, Peoples R China.
C3 Nanchang Institute Technology; Nanchang Institute Technology; Southern
   University of Science & Technology
RP Wang, C (corresponding author), Southern Univ Sci & Technol, Dept Stat & Data Sci, Shenzhen 518055, Guangdong, Peoples R China.
EM junciyang@126.com; mingxima@126.com; junzhang0805@126.com;
   wangc6@sustech.edu.cn
OI Wang, Chao/0000-0001-6524-504X
FU Natural Science Foundation of Jiangxi Province [20192BAB211005]; Science
   Foundation for Post Doctorate of China [2020M672484]; HKRGC
   [CityU11301120]; NNSF of China [61865012]
FX This work was partly supported by the Natural Science Foundation of
   Jiangxi Province (20192BAB211005), the Science Foundation for Post
   Doctorate of China (2020M672484), HKRGC Grant No.CityU11301120 and the
   NNSF of China (61865012).
CR Aubert G., 2008, MATH PROBLEMIMAGE, V147, DOI DOI 10.1007/978-0-387-44588-5
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Brito-Loeza C, 2016, NUMER METH PART D E, V32, P1066, DOI 10.1002/num.22042
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chan T. F., 2002, SIAM Journal on Applied Mathematics, V63, P564
   Chen DQ, 2014, J SCI COMPUT, V60, P483, DOI 10.1007/s10915-013-9803-z
   Chen HZ, 2009, ADV COMPUT MATH, V31, P115, DOI 10.1007/s10444-008-9097-0
   Duan YP, 2013, NUMER MATH-THEORY ME, V6, P47, DOI 10.4208/nmtma.2013.mssvm03
   Fang YY, 2020, COMPUT VIS IMAGE UND, V200, DOI 10.1016/j.cviu.2020.103044
   Figueiredo MAT, 2010, IEEE T IMAGE PROCESS, V19, P3133, DOI 10.1109/TIP.2010.2053941
   Gao YM, 2018, MULTIDIM SYST SIGN P, V29, P1459, DOI 10.1007/s11045-017-0512-x
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Guo JC, 2021, SIGNAL PROCESS, V186, DOI 10.1016/j.sigpro.2021.108124
   Guo WH, 2014, SIAM J IMAGING SCI, V7, P1309, DOI 10.1137/120904263
   He F, 2021, SIAM J IMAGING SCI, V14, P389, DOI 10.1137/20M1335601
   Jia F, 2021, IEEE INT CONF COMP V, P354, DOI 10.1109/ICCVW54120.2021.00044
   Lefkimmiatis S, 2013, IEEE T IMAGE PROCESS, V22, P1873, DOI 10.1109/TIP.2013.2237919
   Li MM, 2022, SIGNAL IMAGE VIDEO P, V16, P211, DOI 10.1007/s11760-021-01977-4
   Liu XW, 2016, COMPUT MATH APPL, V71, P1694, DOI 10.1016/j.camwa.2016.03.005
   Liu ZF, 2019, NUMER MATH-THEORY ME, V12, P370, DOI 10.4208/nmtma.OA-2017-0149
   Lysaker M, 2003, IEEE T IMAGE PROCESS, V12, P1579, DOI 10.1109/TIP.2003.819229
   Ma M., 2020, MATH PROBL ENG, V2020, P1
   Micchelli CA, 2013, ADV COMPUT MATH, V38, P401, DOI 10.1007/s10444-011-9243-y
   Pang ZF, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107325
   Pang ZF, 2019, SIGNAL PROCESS-IMAGE, V74, P140, DOI 10.1016/j.image.2019.02.003
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Scherzer O., 2010, HDB MATH METHODS IMA
   Tai XC, 2011, SIAM J IMAGING SCI, V4, P313, DOI 10.1137/100803730
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CL, 2010, SIAM J IMAGING SCI, V3, P300, DOI 10.1137/090767558
   Wu TT, 2020, IEEE SIGNAL PROC LET, V27, P1635, DOI 10.1109/LSP.2020.3023299
   You H., 2021, MULTIMED TOOLS APPL, P1
   Zhang J., 2018, INT C GENETIC EVOLUT, P263
   Zhang J, 2022, SIGNAL PROCESS, V193, DOI 10.1016/j.sigpro.2021.108407
   Zhang J, 2017, NUMER MATH-THEORY ME, V10, P98, DOI 10.4208/nmtma.2017.m1611
   Zhang J, 2013, APPL MATH COMPUT, V219, P4964, DOI 10.1016/j.amc.2012.11.060
   Zhang Y, 2022, J SCI COMPUT, V90, DOI 10.1007/s10915-021-01721-7
   Zhong QX, 2021, J MATH IMAGING VIS, V63, P30, DOI 10.1007/s10851-020-00992-3
   Zhu W, 2014, LECT NOTES COMPUT SC, V8293, P104, DOI 10.1007/978-3-642-54774-4_5
   Zhu W, 2013, INVERSE PROBL IMAG, V7, P1409, DOI 10.3934/ipi.2013.7.1409
   Zhu W, 2012, SIAM J IMAGING SCI, V5, P1, DOI 10.1137/110822268
NR 43
TC 3
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5485
EP 5496
DI 10.1007/s00371-022-02674-0
EA OCT 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000864059700001
DA 2024-07-18
ER

PT J
AU Zhang, JC
   Gao, Y
   Xu, Y
   Huang, YB
   Yu, YM
   Shu, XB
AF Zhang, Jiachao
   Gao, Yang
   Xu, Yi
   Huang, Yunbin
   Yu, Yanming
   Shu, Xiangbo
TI A simple yet effective image stitching with computational suture zone
SO VISUAL COMPUTER
LA English
DT Article
DE Image stitching; Optimal seam-line; Blending region
ID NETWORK; MOSAICKING
AB Image stitching is the process of combining two or more photographic images with spatially overlapping areas into a wider-view panorama accommodating the full-scale information. It suffers from ghosting or obvious fracture sometimes after stitching in the overlapped areas, especially when moving targets or foreground targets occurs in blending areas. For the purpose of eliminating the stitching trace and avoiding the ghosting caused by foreground targets in overlapped areas, in this work, a novel image stitching technique by a computational blending zone is proposed. In specific, a dynamic programming of optimal seam-line selection is proposed by exploiting the minimization of a defined energy function based on color, gradient and similarity within the overlapped regions. Based on the optimal seam-line obtained, an optimal region fixed in terms of a proposed gray characteristic function, which expanded from the selected suture line to both sides, is provided for image blending to acquire the final panoramic image. The reference image and the target image are stitched into a panoramic image according to the selected optimal seam-line and suitable blending region. Some experiments are conducted to show the effectiveness of the proposed technique.
C1 [Zhang, Jiachao; Gao, Yang; Xu, Yi] Nanjing Inst Technol, Artificial Intelligence Ind Technol Res Inst, Nanjing, Peoples R China.
   [Huang, Yunbin] Nanjing Univ Posts & Telecommun, Sch Elect & Commun Engn, Nanjing, Peoples R China.
   [Yu, Yanming] Guohua Hami New Energy Co Ltd, Hami, Peoples R China.
   [Shu, Xiangbo] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
C3 Nanjing Institute of Technology; Nanjing University of Posts &
   Telecommunications; Nanjing University of Science & Technology
RP Zhang, JC (corresponding author), Nanjing Inst Technol, Artificial Intelligence Ind Technol Res Inst, Nanjing, Peoples R China.
EM zhangjc07@foxmail.com; ygao@njit.edu.cn; xuyionce@foxmail.com;
   hyb961217@163.com; 277702475@qq.com; shuxb@njust.edu.cn
RI Gao, Yang/JPK-2461-2023
OI Gao, Yang/0000-0003-2320-2839; Shu, Xiangbo/0000-0003-4902-4663
FU National Natural Science Foundation of China [62002160, 62072245,
   62072238, 61703201]; Natural Science Foundation of Jiangsu Province
   [BK20211520, BK20201042]; Science Foundation of Nanjing Institute of
   Technology [ZKJ202003]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant Nos. 62002160, 62072245, 62072238, and
   61703201), the Natural Science Foundation of Jiangsu Province (Grant
   Nos. BK20211520 and BK20201042), and the Science Foundation of Nanjing
   Institute of Technology (Grant No. ZKJ202003).
CR Adel E., 2014, International Journal of Computer Applications, V99, P1, DOI DOI 10.5120/17374-7818
   Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   CHANG CH, 2014, PROC CVPR IEEE, P3254, DOI DOI 10.1109/CVPR.2014.422
   Chen MM, 2015, OCEANS 2015 - GENOVA, DOI 10.1109/OCEANS-Genova.2015.7271744
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao J., 2013, Eurographics (Short Papers), P45, DOI DOI 10.2312/CONF/EG2013/SHORT/045-048
   Haskins G, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01060-x
   Herrmann C, 2018, LECT NOTES COMPUT SC, V11206, P53, DOI 10.1007/978-3-030-01216-8_4
   Kaur H, 2021, ARCH COMPUT METHOD E, V28, P4425, DOI 10.1007/s11831-021-09540-7
   Kerschner M, 2001, ISPRS J PHOTOGRAMM, V56, P53, DOI 10.1016/S0924-2716(01)00033-8
   Lee D, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.063016
   Li HY, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58988
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li L, 2019, ISPRS J PHOTOGRAMM, V148, P41, DOI 10.1016/j.isprsjprs.2018.12.002
   Li L, 2016, ISPRS J PHOTOGRAMM, V113, P1, DOI 10.1016/j.isprsjprs.2015.12.007
   Li XH, 2015, ISPRS J PHOTOGRAMM, V109, P108, DOI 10.1016/j.isprsjprs.2015.09.009
   Liao K, 2020, IEEE T IMAGE PROCESS, V29, P3707, DOI 10.1109/TIP.2020.2964523
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu Risheng, 2020, INT C MACH LEARN, P6305
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Öfverstedt J, 2019, IEEE T IMAGE PROCESS, V28, P3584, DOI 10.1109/TIP.2019.2899947
   Pan J, 2014, IEEE T GEOSCI REMOTE, V52, P1658, DOI 10.1109/TGRS.2013.2253110
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P4857, DOI 10.1109/TNNLS.2019.2958324
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Popovic V, 2015, PROC SPIE, V9400, DOI 10.1117/12.2078889
   Qu Z, 2020, IEEE T IMAGE PROCESS, V29, P6734, DOI 10.1109/TIP.2020.2993134
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Srivastava R, 2016, IET COMPUT VIS, V10, P513, DOI 10.1049/iet-cvi.2015.0251
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tareen Shaharyar Ahmed Khan, 2018, 2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET). Proceedings, DOI 10.1109/ICOMET.2018.8346440
   Tian L, 2020, IEEE T IMAGE PROCESS, V29, P8429, DOI 10.1109/TIP.2020.3013168
   Wang GD, 2017, 2017 16TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS 2017), P769
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang ZB, 2020, MULTIMEDIA SYST, V26, P413, DOI 10.1007/s00530-020-00651-y
   Yang HY, 2017, ACTA GEOPHYS, V65, P1029, DOI 10.1007/s11600-017-0078-x
   Yuan YT, 2021, IEEE T GEOSCI REMOTE, V59, P1565, DOI 10.1109/TGRS.2020.2999404
   Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhao G, 2013, PATTERN RECOGN LETT, V34, P308, DOI 10.1016/j.patrec.2012.10.028
   [周定富 ZHOU Ding-fu], 2009, [测控技术, Measurement & Control Technology], V28, P32
NR 49
TC 3
Z9 3
U1 8
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4915
EP 4928
DI 10.1007/s00371-022-02637-5
EA SEP 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000854700500001
DA 2024-07-18
ER

PT J
AU Chen, RS
   Yin, X
   Yang, YC
   Tong, C
AF Chen, Rongshan
   Yin, Xiang
   Yang, Yuancheng
   Tong, Chao
TI Multi-view Pixel2Mesh++: 3D reconstruction via Pixel2Mesh with more
   images
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; 3D reconstruction; Multiple images; 3D mesh
AB To meet the increasing demand for high-quality 3D models, we propose an end-to-end deep learning network architecture, which can generate 3D mesh models with multiple RGB images and is different from previous methods which generate voxel or point cloud models. Unlike the single-image-based pixel2mesh network, we introduce the ConvLSTM layer to fuse perceptual features, making it possible to process multiple images simultaneously. To constrain the smoothness of 3D shapes, we design a graph pooling layer to reduce mesh structure and define a new loss function-Smooth loss. Collaborating with the graph unpooling layer in Pixel2Mesh (P2M), the graph pooling layer guarantees the mesh topology of the final 3D shapes generated. The application of Smooth loss ensures the visual appeal and structural accuracy of 3D shapes generated. Our experiments on ShapeNet dataset show that our method, compared with previous deep learning networks, can generate higher-precision 3D shapes and achieves the best on F-score and CD. In addition, due to the introduction of fusion features from multiple images, our experimental results are more convincing and credible.
C1 [Chen, Rongshan; Yin, Xiang; Yang, Yuancheng; Tong, Chao] Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
C3 Beihang University
RP Tong, C (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
EM rongshan@buaa.edu.cn; yinxiang@buaa.edu.cn; ycyoung@buaa.cdu.cn;
   tongchao@buaa.edu.cn
FU National Natural Science Foundation of China [62176016]; National Key R
   &D Program of China [2018YFB2101100, 2019YFB2101600]; Guizhou Province
   Science and Technology Project: Research and Demonstration of Sci. &
   Tech Big Data Mining Technology Based on Knowledge Graph [Qiankehe[2021]
   General 382]; Training Program of the Major Research Plan of the
   National Natural Science Foundation of China [92046015]; Beijing Natural
   Science Foundation Program and Scientific Research Key Program of
   Beijing Municipal Commission of Education [KZ202010025047]
FX This study is partially supported by National Natural Science Foundation
   of China (62176016), the National Key R &D Program of China (Nos.
   2018YFB2101100 and 2019YFB2101600), Guizhou Province Science and
   Technology Project: Research and Demonstration of Sci. & Tech Big Data
   Mining Technology Based on Knowledge Graph (supported by Qiankehe[2021]
   General 382), Training Program of the Major Research Plan of the
   National Natural Science Foundation of China (Grant No. 92046015), and
   Beijing Natural Science Foundation Program and Scientific Research Key
   Program of Beijing Municipal Commission of Education (Grant No.
   KZ202010025047).
CR ALOIMONOS J, 1988, BIOL CYBERN, V58, P345, DOI 10.1007/BF00363944
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chang A. X., 2015, ARXIV
   Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Defferrard M, 2016, ADV NEUR IN, V29
   Delanoy J, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203197
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gao Y, 2019, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND SOFTWARE ENGINEERING (CSSE 2019), DOI 10.1145/3339363.3339395
   Haag M, 1999, INT J COMPUT VISION, V35, P295, DOI 10.1023/A:1008112528134
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Kipf TN, 2016, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee YT, 2011, COMPUT AIDED DESIGN, V43, P1025, DOI 10.1016/j.cad.2011.03.008
   Loh A. M., 2005, P BRIT MACH VIS C, P69
   Lyu KJ, 2020, IEEE T IMAGE PROCESS, V29, P1867, DOI 10.1109/TIP.2019.2944522
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Nikoohemat S, 2020, AUTOMAT CONSTR, V113, DOI 10.1016/j.autcon.2020.103109
   Nozawa N., 2019, MOTION INTERACTION G
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Peng Songyou, 2020, ARXIV
   Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855
   Shi XJ, 2015, ADV NEUR IN, V28
   Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91
   Ting Z., 2004, P PAN SYDNEY AREA WO, P83
   Trucco E., 1998, INTRO TECHNIQUES 3D
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Xiang N, 2019, PROCEEDINGS OF THE 32ND INTERNATIONAL CONFERENCE ON COMPUTER ANIMATION AND SOCIAL AGENTS (CASA 2019), P79, DOI 10.1145/3328756.3328766
   Xie H., 2018, P 10 INT C INT MULT
   Xu Q., 2019, Proc. Adv. Neural Inf. Process. Syst., P492, DOI DOI 10.5555/3454287.3454332
   Yang TT, 2022, SCI CHINA TECHNOL SC, V65, P396, DOI 10.1007/s11431-021-1950-9
   Ze-Huan Yuan, 2012, Advanced Research in Applied Artificial Intelligence. Proceedings 25th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2012, P754, DOI 10.1007/978-3-642-31087-4_76
NR 39
TC 2
Z9 2
U1 2
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5153
EP 5166
DI 10.1007/s00371-022-02651-7
EA AUG 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000847661700001
DA 2024-07-18
ER

PT J
AU Jin, Y
   Hu, YB
   Jiang, ZW
   Zheng, QF
AF Jin, Yan
   Hu, Yibiao
   Jiang, Zhiwei
   Zheng, Qiufu
TI Polyp segmentation with convolutional MLP
SO VISUAL COMPUTER
LA English
DT Article
DE Polyp segmentation; MLP; Attention mechanism; Neural network; Medical
   image segmentation
ID NETWORK
AB Accurate polyp segmentation can help doctors find and resect abnormal tissue and decrease the chances of polyps changing into colorectal cancer. The current polyp segmentation neural networks are still challenged by complicated scenarios where polyps have large variations of shapes, size, color, and appearance. In this paper, we propose convolutional multilayer perceptron polyp segmentation network to achieve more accurate polyp segmentation in colonoscopy images. The proposed network adopts a convolutional MLP encoder and enhances the low-level feature using the parallel self-attention module. Furthermore, instead of directly adding encoder features to the decoder, we introduce a cascaded context aggregation module to aggregate the high-level semantic feature and low-level local feature. Finally, channel guide group reverse attention is used to enhance structural and textural details by mining the relationship between areas and boundary cues. The proposed approach is evaluated on six widely adopted datasets and demonstrates superior performance compared to other state-of-the-art models.
C1 [Jin, Yan; Hu, Yibiao; Jiang, Zhiwei; Zheng, Qiufu] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Jin, Y (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
EM jy@zjut.edu.cn
OI zhiwei, jiang/0000-0001-7314-2083; Jin, Yan/0000-0001-8956-7684
CR Ahmed, 2020, MEDIAEVAL20 MULTIMED, DOI [10.1109/EMBC.2019.8857958, DOI 10.1109/EMBC.2019.8857958]
   Akbari M, 2018, IEEE ENG MED BIO, P69, DOI 10.1109/EMBC.2018.8512197
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Brandao P., 2018, J MED ROBOT RES, V3, DOI 10.1142/s2424905x18400020
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Ding X., ARXIV
   Fan D.-P., 2021, Scientia Sinica Informationis, V6, P6
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guo J., ARXIV
   Guo JD, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102906
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Q., 2021, ARXIV
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Jiafu Zhong, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P285, DOI 10.1007/978-3-030-59725-2_28
   Jiang M, 2022, VISUAL COMPUT, V38, P2473, DOI 10.1007/s00371-021-02124-3
   Lai HL, 2023, VISUAL COMPUT, V39, P1453, DOI 10.1007/s00371-022-02422-4
   Li HX, 2021, NEURAL COMPUT APPL, V33, P11589, DOI 10.1007/s00521-021-05856-4
   Li J., ARXIV
   Liu H., arXiv
   Loshchilov I., 2018, ARXIV
   Loukas A., 2020, IBER CONF INF SYST, DOI DOI 10.23919/cisti49556.2020.9141108
   Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Murugesan B, 2019, IEEE ENG MED BIO, P7223, DOI [10.1109/EMBC.2019.8857339, 10.1109/embc.2019.8857339]
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nogueira-Rodríguez A, 2022, NEURAL COMPUT APPL, V34, P10375, DOI 10.1007/s00521-021-06496-4
   Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/CRV52889.2021.00032, 10.1109/crv52889.2021.00032]
   Qian, ARXIV
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25
   Sabour S, 2017, ADV NEUR IN, V30
   Sánchez-Peralta LF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10238501
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Sundaram P, 2008, MED IMAGE ANAL, V12, P99, DOI 10.1016/j.media.2007.08.001
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vaswani A, 2017, ADV NEUR IN, V30
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wei J, 2021, LECT NOTES COMPUT SC, V12901, P699, DOI 10.1007/978-3-030-87193-2_66
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wickstrom K, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101619
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HS, 2021, AAAI CONF ARTIF INTE, V35, P2916
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yunchao Wei, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6488, DOI 10.1109/CVPR.2017.687
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhou SK, 2021, P IEEE, V109, P820, DOI 10.1109/JPROC.2021.3054390
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
NR 59
TC 4
Z9 4
U1 14
U2 45
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4819
EP 4837
DI 10.1007/s00371-022-02630-y
EA AUG 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000843733800001
DA 2024-07-18
ER

PT J
AU Zhu, XS
   Lu, JY
   Ren, HH
   Wang, HQ
   Sun, B
AF Zhu, Xinshan
   Lu, Junyan
   Ren, Honghao
   Wang, Hongquan
   Sun, Biao
TI A transformer-CNN for deep image inpainting forensics
SO VISUAL COMPUTER
LA English
DT Article
DE Forensics; Inpainting; Transformer; Convolutional neural networks
ID FORGERY DETECTION ALGORITHM; OBJECT REMOVAL; MODELS
AB As an advanced image editing technology, image inpainting leaves very weak traces in the tampered image, causing serious security issues, particularly those based on deep learning. In this paper, we propose the global-local feature fusion network (GLFFNet) to locate the image regions tampered by inpainting based on deep learning. GLFFNet consists of a two-stream encoder and a decoder. In the two-stream encoder, a spatial self-attention stream (SSAS) and a noise feature extraction stream (NFES) are designed. By a transformer network, the SSAS extracts global features regarding deep inpainting manipulations. The NFES is constructed by the residual blocks, which are used to learn manipulation features from noise maps produced by filtering the input image. Through a feature fusion layer, the features output by the encoder is fused and then fed into the decoder, where the up-sampling and convolutional operations are employed to derive the confidential map for inpainting manipulation. The proposed network is trained by the designed two-stage loss function. Experimental results show that GLFFNet achieves a high location accuracy for deep inpainting manipulations and effectively resists JPEG compression and additive noise attacks.
C1 [Zhu, Xinshan; Lu, Junyan; Ren, Honghao; Wang, Hongquan; Sun, Biao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhu, Xinshan] State Key Lab Digital Publishing Technol, Beijing 100871, Peoples R China.
C3 Tianjin University
RP Sun, B (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM xszhu@tju.edu.cn; ljy9826@tju.edu.cn; rhh@tju.edu.cn;
   1015203049@tju.edu.cn; sunbiao@tju.edu.cn
FU National Natural Science Foundation of China [61972282, 61971303];
   Opening Project of StateKey Laboratory ofDigital Publishing Technology
   [Cndplab-2019-Z001]
FX This study was supported by the National Natural Science Foundation of
   China under Grants 61972282 and 61971303, and by the Opening Project of
   StateKey Laboratory ofDigital Publishing Technology under Grant
   Cndplab-2019-Z001. The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.
CR Amerini I, 2017, IEEE COMPUT SOC CONF, P1865, DOI 10.1109/CVPRW.2017.233
   Bacchuwar KS, 2013, 2013 IEEE INTERNATIONAL MULTI CONFERENCE ON AUTOMATION, COMPUTING, COMMUNICATION, CONTROL AND COMPRESSED SENSING (IMAC4S), P723, DOI 10.1109/iMac4s.2013.6526502
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Chang IC, 2013, IMAGE VISION COMPUT, V31, P57, DOI 10.1016/j.imavis.2012.09.002
   Chen BJ, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102967
   Chen H., 2021, VISUAL COMPUT, V1
   Chen H., 2021, VISUAL COMPUT, P1
   Chen YT, 2021, APPL INTELL, V51, P4367, DOI 10.1007/s10489-020-02116-1
   Chen YT, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02778-2
   Chen YT, 2021, MULTIMED TOOLS APPL, V80, P4237, DOI 10.1007/s11042-020-09887-2
   Chen YT, 2021, VISUAL COMPUT, V37, P1691, DOI 10.1007/s00371-020-01932-3
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Esedoglu S, 2002, EUR J APPL MATH, V13, P353, DOI 10.1017/S0956792501004904
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gao H, 2020, J INF SECUR APPL, V53, DOI 10.1016/j.jisa.2020.102506
   Grossauer H, 2004, LECT NOTES COMPUT SC, V3022, P214
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Khan MJ, 2022, VISUAL COMPUT, V38, P509, DOI 10.1007/s00371-020-02031-z
   Li HD, 2019, IEEE I CONF COMP VIS, P8300, DOI 10.1109/ICCV.2019.00839
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P3050, DOI 10.1109/TIFS.2017.2730822
   Li XH, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-190
   Liu QZ, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P164, DOI [10.1109/ICMLA.2016.93, 10.1109/ICMLA.2016.0035]
   Liu X., ARXIV
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Lu M, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9050858
   Nair G., 2021, 2021 12 INT C COMPUT, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Stamm MC, 2010, IEEE T INF FOREN SEC, V5, P492, DOI 10.1109/TIFS.2010.2053202
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinolin V, 2021, VISUAL COMPUT, V37, P2369, DOI 10.1007/s00371-020-01992-5
   Wan Z., 2021, ARXIV
   Wang XY, 2021, IETE TECH REV, V38, P149, DOI 10.1080/02564602.2020.1782274
   Wu HW, 2022, IEEE T CIRC SYST VID, V32, P1172, DOI 10.1109/TCSVT.2021.3075039
   Wu Q, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1222, DOI 10.1109/ICMLC.2008.4620591
   Xinyi Wang, 2019, Artificial Intelligence and Security. 5th International Conference, ICAIS 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11634), P476, DOI 10.1007/978-3-030-24271-8_43
   Yang JX, 2021, DIGIT SIGNAL PROCESS, V113, DOI 10.1016/j.dsp.2021.103032
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang DY, 2018, MULTIMED TOOLS APPL, V77, P11823, DOI 10.1007/s11042-017-4829-0
   Zhang J, 2020, IEEE SIGNAL PROC LET, V27, P276, DOI 10.1109/LSP.2020.2966888
   Zhao Shengyu, 2021, International Conference on Learning Representations (ICLR)
   Zhao YQ, 2013, OPTIK, V124, P2487, DOI 10.1016/j.ijleo.2012.08.018
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhu XS, 2018, SIGNAL PROCESS-IMAGE, V67, P90, DOI 10.1016/j.image.2018.05.015
NR 56
TC 6
Z9 6
U1 4
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4721
EP 4735
DI 10.1007/s00371-022-02620-0
EA AUG 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000836085100001
DA 2024-07-18
ER

PT J
AU Mo, HB
   Li, BQ
   Shi, WX
   Zhang, XB
AF Mo, Hongbao
   Li, Baoquan
   Shi, Wuxi
   Zhang, Xuebo
TI Cross-based dense depth estimation by fusing stereo vision with measured
   sparse depth
SO VISUAL COMPUTER
LA English
DT Article
DE Dense depth estimation; Stereo matching; Sensor fusion; Adaptive
   cross-arm
ID LIDAR; RANGE
AB Dense depth estimation is significant in robotic systems, such as for mapping, localization, and object recognition. For multiple sensors, an active depth sensor can provide accurate but sparse measurements for environments, and a camera pair can provide dense but imprecise stereo reconstruction results. In this paper, a tightly coupled fusion method is proposed for depth sensor and stereo camera to complete dense depth estimation, and advantages of the two type sensors are combined so as to achieve better depth estimation. An adaptive dynamic cross-arm algorithm are developed to integrate sparse depth measurements into camera-dominated semiglobal stereo matching. To obtain the optimal arm length for a measured pixel point, each cross-arm shape is variational and calculated automatically. Public datasets of KITTI, Middlebury, and Scene Flow datasets are used with comparison experiments to test performance of the proposed method, and real-world experiments are further conducted for verification.
C1 [Mo, Hongbao; Li, Baoquan; Shi, Wuxi] Tiangong Univ, Tianjin Key Lab Adv Technol Elect Engn & Energy, Sch Control Sci & Engn, Tianjin 300387, Peoples R China.
   [Zhang, Xuebo] Nankai Univ, Tianjin Key Lab Intelligent Robot, Inst Robot & Automat Informat Syst, Tianjin 300071, Peoples R China.
C3 Tiangong University; Nankai University
RP Li, BQ (corresponding author), Tiangong Univ, Tianjin Key Lab Adv Technol Elect Engn & Energy, Sch Control Sci & Engn, Tianjin 300387, Peoples R China.
EM mohongbao1996@163.com; libq@tiangong.edu.cn; shiwuxi@163.com;
   zhangxuebo@nankai.edu.cn
OI , Baoquan Li/0000-0001-8390-3907
FU National Natural Science Foundation of China [61973234]; Tianjin Natural
   Science Foundation [18JCZDJC96700, 20JCYBJC00180]; Tianjin Science Fund
   for Distinguished Young Scholars [19JCJQJC62100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61973234, in part by the Tianjin Natural
   Science Foundation under Grant 18JCZDJC96700 and Grant 20JCYBJC00180,
   and in part by Tianjin Science Fund for Distinguished Young Scholars
   under Grant 19JCJQJC62100.
CR Agresti Gianluca, 2018, P EUR C COMP VIS ECC, P0
   Beder C., 2007, COMP VIS PATT REC 20, P1, DOI 10.1109/CVPR.2007.383348
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen ST, 2021, VISUAL COMPUT, V37, P411, DOI 10.1007/s00371-020-01811-x
   Cheng Xuelian, 2020, Advances in Neural Information Processing Systems, V33
   Choe J, 2021, IEEE ROBOT AUTOM LET, V6, P4672, DOI 10.1109/LRA.2021.3068712
   Courtois H, 2017, 2017 WORKSHOP ON RESEARCH, EDUCATION AND DEVELOPMENT OF UNMANNED AERIAL SYSTEMS (RED-UAS), P186, DOI 10.1109/RED-UAS.2017.8101664
   Evangelidis GD, 2015, IEEE T PATTERN ANAL, V37, P2178, DOI 10.1109/TPAMI.2015.2400465
   Gandhi V, 2012, IEEE INT CONF ROBOT, P4742, DOI 10.1109/ICRA.2012.6224771
   Hambarde P, 2020, IEEE T COMPUT IMAG, V6, P806, DOI 10.1109/TCI.2020.2981761
   Hernandez-Juarez D, 2016, PROCEDIA COMPUT SCI, V80, P143, DOI 10.1016/j.procs.2016.05.305
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Huang YK, 2021, PROC CVPR IEEE, P16701, DOI 10.1109/CVPR46437.2021.01643
   Huber D., 2011, 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), 2011 International Conference on, P405, DOI [10.1109/3DIMPVT.2011.58, DOI 10.1109/3DIMPVT.2011.58]
   Jian MW, 2020, IEEE T MULTIMEDIA, V22, P970, DOI 10.1109/TMM.2019.2937187
   Kraft H., 2004, OPTO
   Kuhnert KD, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4780, DOI 10.1109/IROS.2006.282349
   Ma FC, 2018, IEEE INT CONF ROBOT, P4796
   Maddern W, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2181, DOI 10.1109/IROS.2016.7759342
   Marin G, 2016, LECT NOTES COMPUT SC, V9911, P386, DOI 10.1007/978-3-319-46478-7_24
   Oberle W.F., 2005, ARMY RESE LABOR
   Park K, 2020, IEEE T INTELL TRANSP, V21, P321, DOI 10.1109/TITS.2019.2891788
   Poggi M, 2020, IEEE SENS J, V20, P1411, DOI 10.1109/JSEN.2019.2946591
   Premebida C, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2469
   Qingxiong Yang, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P69, DOI 10.1109/MMSP.2010.5661996
   Rother C, 2002, INT J COMPUT VISION, V49, P117, DOI 10.1023/A:1020189404787
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Shim H, 2012, VISUAL COMPUT, V28, P1139, DOI 10.1007/s00371-011-0664-x
   Shivakumar SS, 2019, IEEE INT CONF ROBOT, P6482, DOI [10.1109/icra.2019.8794023, 10.1109/ICRA.2019.8794023]
   Veitch-Michaelis J, 2015, INT ARCH PHOTOGRAMM, V44, P107, DOI 10.5194/isprsarchives-XL-4-W5-107-2015
   Wang TH, 2019, IEEE INT C INT ROBOT, P5895, DOI 10.1109/iros40897.2019.8968170
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang JM, 2020, IEEE INT CONF ROBOT, P7829, DOI [10.1109/ICRA40945.2020.9196628, 10.1109/icra40945.2020.9196628]
   Zhao T, 2021, IEEE T CONSUM ELECTR, V67, P195, DOI 10.1109/TCE.2021.3095378
   Zhao T, 2022, VISUAL COMPUT, V38, P1619, DOI 10.1007/s00371-021-02092-8
   Zhu JJ, 2011, IEEE T PATTERN ANAL, V33, P1400, DOI 10.1109/TPAMI.2010.172
NR 39
TC 1
Z9 1
U1 6
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4339
EP 4350
DI 10.1007/s00371-022-02594-z
EA AUG 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000835604500001
DA 2024-07-18
ER

PT J
AU Zhang, XW
   Ma, WF
   Varinlioglu, G
   Rauh, N
   He, L
   Aliaga, D
AF Zhang, Xiaowei
   Ma, Wufei
   Varinlioglu, Gunder
   Rauh, Nick
   He, Liu
   Aliaga, Daniel
TI Guided pluralistic building contour completion
SO VISUAL COMPUTER
LA English
DT Article
DE Digital cultural heritage; Image processing and analysis; Machine
   learning for graphics
AB Image/sketch completion is a core task that addresses the problem of completing the missing regions of an image/sketch with realistic and semantically consistent content. We address one type of completion which is producing a tentative completion of an aerial view of the remnants of a building structure. The inference process may start with as little as 10% of the structure and thus is fundamentally pluralistic (e.g., multiple completions are possible). We present a novel pluralistic building contour completion framework. A feature suggestion component uses an entropy-based model to request information from the user for the next most informative location in the image. Then, an image completion component trained using self-supervision and procedurally generated content produces a partial or full completion. In our synthetic and real-world experiments for archaeological sites in Turkey, with up to only 4 iterations, we complete building footprints having only 10-15% of the ancient structure initially visible. We also compare to various state-of-the-art methods and show our superior quantitative/qualitative performance. While we show results for archaeology, we anticipate our method can be used for restoring highly incomplete historical sketches and for modern day urban reconstruction despite occlusions.
C1 [Zhang, Xiaowei; Ma, Wufei; Varinlioglu, Gunder; Rauh, Nick; He, Liu; Aliaga, Daniel] Purdue Univ, W Lafayette, IN 47907 USA.
   [Varinlioglu, Gunder] Mimar Sinan Fine Arts Univ, Istanbul, Turkey.
C3 Purdue University System; Purdue University; Mimar Sinan Guzel Sanatlar
   University
RP Zhang, XW (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM zhan2597@purdue.edu
RI He, Liu/JDV-7351-2023
OI He, Liu/0000-0001-9715-2606; Zhang, Xiaowei/0000-0001-7008-6848
FU National Science Foundation [1816514, 1835739]; Direct For Computer &
   Info Scie & Enginr; Office of Advanced Cyberinfrastructure (OAC)
   [1835739] Funding Source: National Science Foundation; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1816514] Funding Source: National Science Foundation
FX This research was funded in part by National Science Foundation grants
   #1816514 CHS: Small: Functional Proceduralization of 3D Geometric
   Models, #1835739 U-Cube: A Cyberinfrastructure for Unified and
   Ubiquitous Urban Canopy Parameterization, and #2107096 Deep Generative
   Modeling for Urban and Archaeological Recovery
CR [Anonymous], 2015, P 2015 IEEE C COMPUT
   [Anonymous], 2017, NIPS W
   [Anonymous], 2015, P INT C LEARN REPR
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Bao F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461977
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Demir I, 2016, INT CONF 3D VISION, P194, DOI 10.1109/3DV.2016.28
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   He K., 2012, ECCV, P1629
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kingma DP, 2013, ARXIV
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Köhler R, 2014, LECT NOTES COMPUT SC, V8753, P523, DOI 10.1007/978-3-319-11752-2_43
   Köppel M, 2015, IEEE IMAGE PROC, P1795, DOI 10.1109/ICIP.2015.7351110
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Levin A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P305
   Lin HY, 2020, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR42600.2020.00679
   Liu F, 2019, PROC CVPR IEEE, P5823, DOI 10.1109/CVPR.2019.00598
   Ming YS, 2012, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2012.6247755
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Ren J.S., 2015, NIPS
   Ren XF, 2005, IEEE I CONF COMP VIS, P1214
   Ritchie D, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766895
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sasaki K, 2017, PROC CVPR IEEE, P5768, DOI 10.1109/CVPR.2017.611
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Su G., 2020, 31 BRIT MACHINE VISI 31 BRIT MACHINE VISI
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Vanegas CA, 2010, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.2010.5540190
   Wan Ziyu, 2021, ICCV
   Xu J, 2013, PROC CVPR IEEE, P1886, DOI [10.1109/cvpr.2013.246, 10.1109/CVPR.2013.246]
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang X., 2020, ECCV
   Zhao Z., 2021, CVPR
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
NR 45
TC 2
Z9 2
U1 2
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3205
EP 3216
DI 10.1007/s00371-022-02532-z
EA JUN 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000807970000002
DA 2024-07-18
ER

PT J
AU Amiri, Z
   Hassanpour, H
   Beghdadi, A
AF Amiri, Zahra
   Hassanpour, Hamid
   Beghdadi, Azeddine
TI Abnormalities detection in wireless capsule endoscopy images using EM
   algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Abnormalities detection; Angiodysplasia; Capsule endoscopy; Ulcer;
   Lymphoid hyperplasia
ID COLON
AB In this paper, a novel method is proposed to detect common abnormalities in Wireless Capsule Endoscopy (WCE) video frames including Lymphoid Hyperplasia, ulcer, and angiodysplasia lesions. Inspecting WCE video frames to detect abnormality is a tedious task for physicians. One important step in the proposed approach is to extract the region of interest (ROI), i.e., suspicious region, using the expectation-maximization (EM) algorithm. Suspicious regions in WCE frames are segmented using the EM algorithm considering the color and texture information of the image. Then, suitable descriptors associated with the shape, texture, and color of ROIs are examined for further analysis. These descriptors include histogram of gradients for shape, local binary patterns for texture and different statistical characteristics from pixel values for color information. These features are then fed to a support-vector machine for classification. The results show that the proposed approach can detect abnormalities in WCE frames with the accuracy of 91.3%.
C1 [Amiri, Zahra; Hassanpour, Hamid] Shahrood Univ Technol, Fac Comp Engn, Shahrood, Iran.
   [Beghdadi, Azeddine] Univ Paris 13, Dept Comp Sci & Engn, Paris, France.
C3 Shahrood University of Technology; Universite Paris 13
RP Hassanpour, H (corresponding author), Shahrood Univ Technol, Fac Comp Engn, Shahrood, Iran.
EM zahra.amiri@shahroodut.ac.ir; h.hassanpour@shahroodut.ac.ir;
   beghdadi@univ-paris13.fr
RI Amiri, Zahra/KHU-7955-2024; Amiri, Zahra/HHC-9302-2022; Beghdadi,
   Azeddine/ABF-9801-2022; Hassanpour, Hamid/AAL-7271-2020
OI Amiri, Zahra/0000-0001-8714-723X; Beghdadi,
   Azeddine/0000-0002-5595-0615; Hassanpour, Hamid/0000-0002-5513-9822
CR Ajam A, 2019, 2019 5TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS 2019), DOI 10.1109/icspis48872.2019.9066062
   AlyanNezhadi M. M., 2020, International journal of Engineering, V33, P949, DOI 10.5829/ije.2020.33.05b.28
   Amiri Z, 2019, 2019 5TH IRANIAN CONFERENCE ON SIGNAL PROCESSING AND INTELLIGENT SYSTEMS (ICSPIS 2019), DOI 10.1109/icspis48872.2019.9066008
   Amiri Z, 2019, EUR W VIS INF PROCES, P217, DOI 10.1109/EUVIP47703.2019.8946168
   [Anonymous], 2018, GASTROINTESTINAL IMA
   Ayadi W, 2022, VISUAL COMPUT, V38, P107, DOI 10.1007/s00371-020-02005-1
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Charfi S, 2019, IET IMAGE PROCESS, V13, P1023, DOI 10.1049/iet-ipr.2018.6232
   Constantinescu A. F., 2015, Applied Medical Informatics, V36, P31
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deeba F, 2018, J MED BIOL ENG, V38, P325, DOI 10.1007/s40846-017-0299-0
   Fan SH, 2018, PHYS MED BIOL, V63, DOI 10.1088/1361-6560/aad51c
   Gobpradit S, 2020, LECT NOTES ARTIF INT, V12033, P283, DOI 10.1007/978-3-030-41964-6_25
   Hazgui M, 2022, VISUAL COMPUT, V38, P457, DOI 10.1007/s00371-020-02028-8
   Iwamuro M, 2016, INT J COLORECTAL DIS, V31, P313, DOI 10.1007/s00384-015-2392-6
   Jia X, 2017, I S BIOMED IMAGING, P179, DOI 10.1109/ISBI.2017.7950496
   Kanafani Q., 2000, 10 EUROPEAN SIGNAL P, P14
   Khuroo MS, 2011, BMC GASTROENTEROL, V11, DOI 10.1186/1471-230X-11-36
   Koulaouzidis A, 2017, ENDOSC INT OPEN, V5, pE477, DOI 10.1055/s-0043-105488
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Ming Y, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124640
   Mohammed A, 2018, J IMAGING, V4, DOI 10.3390/jimaging4060075
   Nawarathna R, 2014, NEUROCOMPUTING, V144, P70, DOI 10.1016/j.neucom.2014.02.064
   Naz J, 2023, NEURAL PROCESS LETT, V55, P115, DOI 10.1007/s11063-021-10481-2
   Noya F, 2017, IEEE ENG MED BIO, P3158, DOI 10.1109/EMBC.2017.8037527
   Regula J, 2008, BEST PRACT RES CL GA, V22, P313, DOI 10.1016/j.bpg.2007.10.026
   Shvets AA, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P612, DOI 10.1109/ICMLA.2018.00098
   Smedsrud PH, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00920-z
   Souaidi M, 2019, MULTIMED TOOLS APPL, V78, P13091, DOI 10.1007/s11042-018-6086-2
   Usman MA, 2016, COMPUT MED IMAG GRAP, V54, P16, DOI 10.1016/j.compmedimag.2016.09.005
   Vieira P.M., 2016, IEEE 38 ANN INT C EN
   Vieira PM, 2019, ANN BIOMED ENG, V47, P1446, DOI 10.1007/s10439-019-02248-7
   Voderholzer WA, 2005, GUT, V54, P369, DOI 10.1136/gut.2004.040055
   Walker HF, 2011, SIAM J NUMER ANAL, V49, P1715, DOI 10.1137/10078356X
   Wu X, 2016, IEEE T MED IMAGING, V35, P1741, DOI 10.1109/TMI.2016.2527736
   Yuan YX, 2016, IEEE J BIOMED HEALTH, V20, P624, DOI 10.1109/JBHI.2015.2399502
   Yuan YX, 2015, IEEE T MED IMAGING, V34, P2046, DOI 10.1109/TMI.2015.2418534
   Zhao JW, 2018, VISUAL COMPUT, V34, P1677, DOI 10.1007/s00371-017-1441-2
NR 38
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2999
EP 3010
DI 10.1007/s00371-022-02507-0
EA MAY 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000801139000001
DA 2024-07-18
ER

PT J
AU Pei, HN
   Guo, RZ
   Tan, ZY
   Huang, XQ
   Bai, ZH
AF Pei, Huining
   Guo, Renzhe
   Tan, Zhaoyun
   Huang, Xueqin
   Bai, Zhonghang
TI Fine-grained classification of automobile front face modeling based on
   Gestalt psychology*
SO VISUAL COMPUTER
LA English
DT Article
DE Automobile design; Fine-grained classification; U-shaped prior;
   Attention-based classification
AB In this paper, we propose a fine-grained classification method for automobile front face modeling images based on Gestalt psychology. This method divides pixels into features of visual regions through convolutional neural network, divides automobile front face images into parts, and conducts fine-grained classification based on the overall modeling of parts. A more objective method of fine granularity classification of automobile front face image is explored. A fine-grained classification and recognition model of automobile front face modeling based on Gestalt psychology is proposed in this work. Firstly, unclassified input car front face images are filtered through part detection, part segmentation, and regularization processing by combining the image classification training sets of car front face shapes. Secondly, to facilitate weakly supervised learning for each part, we establish recognition models using the simple a priori of U-shaped distribution for individual parts of car images and train the net using image-level object labels on the ResNet-101 network framework. Attention mechanism is then reused for aggregate features to output classification vectors. Finally, recognition accuracy of 89.9% is reached on the Comprehensive Cars (CompCars) dataset. Compared with other CNN methods, the results confirm that U-shaped distribution combined with parts in the exploration image has a higher recognition rate. Moreover, model interpretability can be achieved by dividing images and recognizing the contribution of each part in the classification.
C1 [Pei, Huining; Guo, Renzhe; Tan, Zhaoyun; Huang, Xueqin; Bai, Zhonghang] Hebei Univ Technol, Sch Architecture & Art Design, Tianjin 300130, Peoples R China.
   [Bai, Zhonghang] Hebei Univ Technol, Natl Technol Innovat Method & Tool Engn Res Ctr, Tianjin 300401, Peoples R China.
C3 Hebei University of Technology; Hebei University of Technology
RP Guo, RZ (corresponding author), Hebei Univ Technol, Sch Architecture & Art Design, Tianjin 300130, Peoples R China.
EM peihuining@hebut.edu.cn; 940101970@qq.com; Tanzhaoyun1217@163.com;
   huangxueqin0606@163.com; baizhonghang@hebut.edu.cn
OI Huang, Xueqin/0009-0009-0724-8125; Bai, Zhonghang/0000-0001-6655-3730;
   Guo, Renzhe/0000-0002-9217-2613
FU Natural Science Foundation of Hebei Province [G2021202008]; Social
   Science Foundation of Hebei Province [HB20YS046]
FX This work was supported by the Natural Science Foundation of Hebei
   Province (Grant Number: G2021202008) and Social Science Foundation of
   Hebei Province (Grant Number: HB20YS046).
CR Abbasov, 2015, PSYCHOL VISUAL PERCE, P111
   Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Ali H, 2021, VISUAL COMPUT, V37, P939, DOI 10.1007/s00371-020-01845-1
   [Anonymous], 2022, LEARNING ANNOTATE PA, P120
   Averbuch-Elor H, 2018, VISUAL COMPUT, V34, P1761, DOI 10.1007/s00371-017-1467-5
   Brendel Wieland, 2019, INT C LEARN REPR
   Chassy P, 2015, PERCEPTION, V44, P1085, DOI 10.1177/0301006615596882
   Chen CF, 2019, ADV NEUR IN, V32
   Ding WW, 2020, SIGNAL PROCESS-IMAGE, V83, DOI 10.1016/j.image.2019.115776
   Fang J, 2017, IEEE T INTELL TRANSP, V18, P1782, DOI 10.1109/TITS.2016.2620495
   Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He LM, 2005, J AOAC INT, V88, P1104
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Jampani V., 2018, ARXIV
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Joseph S, 2016, JOURNAL PRACT, V10, P730, DOI 10.1080/17512786.2015.1058180
   Li BQ, 2018, ADV MECH ENG, V10, DOI 10.1177/1687814017752480
   Li M., 2021, Vis. Comput
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li Y, 2018, ADV NEUR IN, V31
   Lu W., 2014, BMVC 2014 PROC BR MA, DOI 10.5244/c.28.118
   Ludlow Morwenna., 2007, Gregory of Nyssa, Ancient and (Post)modern
   Luo LK, 2019, VISUAL COMPUT, V35, P1869, DOI 10.1007/s00371-018-1580-0
   Pan YT, 2020, WORLD WIDE WEB, V23, P2259, DOI 10.1007/s11280-020-00793-z
   Si TZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108462
   Sun X, 2021, IEEE T IND ELECTRON, V68, P3588, DOI 10.1109/TIE.2020.2977553
   Pham TA, 2023, VISUAL COMPUT, V39, P927, DOI 10.1007/s00371-021-02375-0
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HR, 2022, INTEGR COMPUT-AID E, V29, P141, DOI 10.3233/ICA-210672
   Yan JJ, 2015, PROC CVPR IEEE, P5107, DOI 10.1109/CVPR.2015.7299146
   Yan YJ, 2018, PATTERN RECOGN, V79, P65, DOI 10.1016/j.patcog.2018.02.004
   Yang JW, 2020, INTERDISCIP SCI, V12, P323, DOI 10.1007/s12539-020-00385-5
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yu SY, 2017, NEUROCOMPUTING, V257, P97, DOI 10.1016/j.neucom.2016.09.116
   Yu YH, 2018, ADV SOC SCI EDUC HUM, V182, P1
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 43
TC 2
Z9 2
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2981
EP 2998
DI 10.1007/s00371-022-02506-1
EA MAY 2022
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000800982800001
DA 2024-07-18
ER

PT J
AU Zhou, PC
   Xue, Y
   Xue, MG
AF Zhou, Pu-Cheng
   Xue, Ying
   Xue, Mo-Gen
TI Adaptive side window joint bilateral filter
SO VISUAL COMPUTER
LA English
DT Article
DE Image smoothing; Edge-preserving; Side window filtering; Joint bilateral
   filter
ID IMAGE; DECOMPOSITION; PHOTOGRAPHY; RETINEX; FLASH
AB Edge-preserving image smoothing is a fundamental step for many computer vision problems, and so far, countless algorithms have been proposed. Among these algorithms, bilateral filtering and its extensions are widely used in image preprocessing. However, several difficulties are hindering its further development. First, the phenomenon of "halo artifact" occurs along the edges. Second, most of the existing algorithms work only with a fixed filtering kernel and cannot accurately distinguish the edges and textures which leads to inappropriate filtering. To address these issues, we present a novel edge-preserving image smoothing via adaptive side window joint bilateral filtering. As a local optimized-based algorithm, different from the traditional filtering, the position of the target pixel in the filtering kernel is changed from the center to the optimal edge and the filtering kernel size of each pixel is effectively estimated. Combined side window filtering with the joint bilateral filter, the capability of texture removal and edge preservation is improved and the halo artifacts are alleviated. Experimental results show that the proposed method outperforms existing state-of-the-arts in removing the texture information while preserving the main image content.
C1 [Zhou, Pu-Cheng; Xue, Ying; Xue, Mo-Gen] PLA Army Acad Artillery & Air Def, Dept Informat Engn, Hefei 230031, Anhui, Peoples R China.
   [Xue, Mo-Gen] Anhui Prov Key Lab Polarized Imaging Detect Techn, Hefei 230031, Anhui, Peoples R China.
RP Zhou, PC (corresponding author), PLA Army Acad Artillery & Air Def, Dept Informat Engn, Hefei 230031, Anhui, Peoples R China.
EM zhoupc@hit.edu.cn; 156609414@qq.com
OI Zhou, Pu-cheng/0000-0001-9969-3086
FU Natural Science Foundation of Anhui Province of China [1908085MF208];
   Natural Science Foundation of China [61379105]
FX The work was supported by the Natural Science Foundation of Anhui
   Province of China (No. 1908085MF208) and the Natural Science Foundation
   of China (No. 61379105). We sincerely thank Professor Gong Yuanhao for
   the side window filtering codes that he provided.
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Aurich V., 1995, Informatik Aktuell, P538, DOI DOI 10.1007/978-3-642-79980-8_63
   Bao LC, 2014, IEEE T IMAGE PROCESS, V23, P555, DOI 10.1109/TIP.2013.2291328
   Cai BL, 2017, IEEE IMAGE PROC, P250, DOI 10.1109/ICIP.2017.8296281
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Chaudhury KN, 2011, IEEE T IMAGE PROCESS, V20, P3376, DOI 10.1109/TIP.2011.2159234
   Chen BH., 2021, IEEE ACCESS, V99, P1
   Chen BH, 2020, IEEE SIGNAL PROC LET, V27, P1670, DOI 10.1109/LSP.2020.3024990
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239546, 10.1145/1276377.1276496]
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gavaskar RG, 2019, IEEE T IMAGE PROCESS, V28, P779, DOI 10.1109/TIP.2018.2871597
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Gu B, 2013, IEEE T IMAGE PROCESS, V22, P70, DOI 10.1109/TIP.2012.2214047
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Jevnisek RJ, 2017, PROC CVPR IEEE, P3816, DOI 10.1109/CVPR.2017.406
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Kou F, 2015, IEEE SIGNAL PROC LET, V22, P211, DOI 10.1109/LSP.2014.2353774
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lebrun M, 2012, IMAGE PROCESS ON LIN, V2, P175, DOI 10.5201/ipol.2012.l-bm3d
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Porikli F, 2008, PROC CVPR IEEE, P3895
   Prasath VBS, 2019, IEEE T IMAGE PROCESS, V28, P6198, DOI 10.1109/TIP.2019.2924799
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Veerakumar T, 2019, CIRC SYST SIGNAL PR, V38, P2630, DOI 10.1007/s00034-018-0984-4
   Veerakumar T, 2019, EXPERT SYST APPL, V121, P18, DOI 10.1016/j.eswa.2018.12.009
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Z, 2018, IEEE T CIRC SYST VID, V28, P550, DOI 10.1109/TCSVT.2016.2611944
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Xu PP, 2019, IEEE T IMAGE PROCESS, V28, P4354, DOI 10.1109/TIP.2019.2904847
   Xue Y., 2020, P SOC PHOTO-OPT INS, V1720, P1
   Yang QX, 2013, IEEE T IMAGE PROCESS, V22, P4841, DOI 10.1109/TIP.2013.2278917
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhou PC, 2019, IET IMAGE PROCESS, V13, P1835, DOI 10.1049/iet-ipr.2018.5133
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 50
TC 5
Z9 5
U1 7
U2 26
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1533
EP 1555
DI 10.1007/s00371-022-02427-z
EA MAR 2022
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000763898100001
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Zhou, WB
   Xu, JL
AF Zhang, Yufan
   Zhou, Wenbiao
   Xu, Jinling
TI A dynamic range adjustable inverse tone mapping operator based on human
   visual system
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range (HDR); Human visual system (HVS); Inverse tone
   mapping; Adjustable dynamic range
ID ADAPTATION
AB Conventional inverse tone mapping operator (iTMO) reconstructs high dynamic range (HDR) images with fixed dynamic range and tend to produce extended distortion in both high and low exposure regions of HDR images. This paper proposes a dynamic range adjustable inverse tone mapping algorithm based on single LDR image, which combined photoreceptor response and adaptation. Firstly, the linearized image is converted to retinal response in the LDR environment. Then, it is extended to obtain the retinal response corresponding to HDR scenes. This extension can also be adjusted according to the target dynamic range. Since the different states of light adaptation, expanded retinal response is converted into a set of HDR images with different exposure background intensity. The corresponding processing for different exposure regions can effectively reduce the distortion of high exposure and low exposure regions. Finally, this group of HDR images is synthesized base on the corresponding weighted graph. The efficiency and high visual quality of the proposed algorithm are validated in open-source datasets, and the superior performance of proposed algorithm is also proved by objective evaluations of different types of LDR images.
C1 [Zhang, Yufan; Zhou, Wenbiao; Xu, Jinling] Beijing Inst Technol, Beijing, Peoples R China.
C3 Beijing Institute of Technology
RP Zhang, YF (corresponding author), Beijing Inst Technol, Beijing, Peoples R China.
EM zhangyufan@bit.edu.cn; zhouwenbiao@bit.edu.cn; 3120190713@bit.edu.cn
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Bist C, 2017, COMPUT GRAPH-UK, V62, P77, DOI 10.1016/j.cag.2016.12.006
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Dowling, 1988, RETINA APPROACHABLE, P175
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   GEISLER WS, 1978, VISION RES, V18, P279, DOI 10.1016/0042-6989(78)90162-1
   Hall R., 1989, ILLUMINATION COLOR C, P45, DOI [10.1007/978-1-4612-3526-2, DOI 10.1007/978-1-4612-3526-2]
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4
   Kim DE, 2020, IEEE T CIRC SYST VID, V30, P400, DOI 10.1109/TCSVT.2019.2892438
   KORTUM PT, 1995, VISION RES, V35, P1595, DOI 10.1016/0042-6989(94)00206-2
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Kunkel S., 2016, HIGH DYNAMIC RANGE V, P391
   Kuo PH, 2014, IEEE INT WORKSH MULT
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   MANTIUK R, 2011, ACM T GRAPHIC, V30, P1, DOI DOI 10.1145/2010324.1964935
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mohammadi P, 2021, IEEE T CIRC SYST VID, V31, P1711, DOI 10.1109/TCSVT.2020.3014679
   Nemoto H., 2015, 9 INT WORKSHOP VIDEO
   Rana A, 2020, IEEE T IMAGE PROCESS, V29, P1285, DOI 10.1109/TIP.2019.2936649
   Reinhard E., 2010, HIGH DYNAMIC RANGE I, P405
   Reinhard E, 2007, J SOC INF DISPLAY, V15, P997, DOI 10.1889/1.2825110
   Robertson M. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P159, DOI 10.1109/ICIP.1999.817091
   Walraven J., 1990, VISUAL PERCEPT, DOI [10.1016/B978-0-12-657675-7.50011-9, DOI 10.1016/B978-0-12-657675-7.50011-9]
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Yan QS, 2021, NEUROCOMPUTING, V428, P79, DOI 10.1016/j.neucom.2020.11.056
   Yan QS, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103079
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yan QS, 2019, PATTERN RECOGN LETT, V127, P66, DOI 10.1016/j.patrec.2018.10.008
   Yan QS, 2019, MULTIMED TOOLS APPL, V78, P11487, DOI 10.1007/s11042-018-6625-x
   Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012
   Yan QS, 2017, NEUROCOMPUTING, V269, P160, DOI 10.1016/j.neucom.2017.03.083
NR 39
TC 1
Z9 1
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 413
EP 427
DI 10.1007/s00371-021-02338-5
EA FEB 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000759383200003
DA 2024-07-18
ER

PT J
AU Liang, S
   Chu, G
   Xie, C
   Wang, JW
AF Liang, Shuang
   Chu, Gang
   Xie, Chi
   Wang, Jiewen
TI Joint relation based human pose estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Joint relation; Feature moving; Body structure; Human pose estimation
AB With the increasing application of computer vision technology in real life, human pose estimation task becomes more and more important. However, inferencing accurate coordinates of limb joints or invisible joints is still difficult for even state-of-the-art approaches. The positions of limb joints are diversified, and a percentage of the joints are occluded. In this paper, we aim to solve such problem by proposing joint relation based human pose estimation framework. Joint relation is the spatial relation between selected neighbor joints which can imitate human body structure and localize a complex joint with the help of its neighbor joint. We evaluate the joint relation module on challenging dataset and demonstrate its effectiveness by accuracy and visualization results. The proposed joint relation based human pose estimation method achieves state-of-the-art performance.
C1 [Liang, Shuang; Chu, Gang; Xie, Chi; Wang, Jiewen] Tongji Univ, Sch Software Engn, Shanghai, Peoples R China.
C3 Tongji University
RP Liang, S (corresponding author), Tongji Univ, Sch Software Engn, Shanghai, Peoples R China.
EM shuangliang@tongji.edu.cn
RI Xie, Chi/B-5612-2014
OI Xie, Chi/0000-0002-5808-1742; Liang, Shuang/0000-0003-0457-6093
FU National Natural Science Foundation of China [62076183, 61936014,
   61976159]; Natural Science Foundation of Shanghai [20ZR1473500,
   19ZR1461200]; Shanghai Innovation Action Project of Science and
   Technology [20511100700]; National Key Research and Development Project
   [2019YFB2102300, 2019YFB2102301]; Fundamental Research Funds for the
   Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076183, 61936014 and 61976159, in part
   by the Natural Science Foundation of Shanghai under Grant 20ZR1473500,
   19ZR1461200, in part by the Shanghai Innovation Action Project of
   Science and Technology under Grant 20511100700, in part by the National
   Key Research and Development Project under Grant 2019YFB2102300,
   2019YFB2102301, and in part by the Fundamental Research Funds for the
   Central Universities.
CR Abdelbaky A, 2021, VISUAL COMPUT, V37, P1821, DOI 10.1007/s00371-020-01940-3
   Agahian S., 2018, VISUAL COMPUT, V35, P1
   Cao X, 2019, ANTICONFUSING REGION
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hong S, 2020, HINTPOSE
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Rechy-Ramirez EJ, 2019, VISUAL COMPUT, V35, P41, DOI 10.1007/s00371-017-1446-x
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kocabas M, 2018, LECT NOTES COMPUT SC, V11215, P437, DOI 10.1007/978-3-030-01252-6_26
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P11354
   Liu T, 2020, VIEWINVARIANT OCCLUS
   Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Qammaz A, 2021, INT C PATT RECOG, P6904, DOI 10.1109/ICPR48806.2021.9411956
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi MY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3407659
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang J., 2020, P 16 EUROPEAN C COMP, P492
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang QN, 2022, VISUAL COMPUT, V38, P2447, DOI 10.1007/s00371-021-02122-5
   Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 46
TC 3
Z9 4
U1 3
U2 30
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1369
EP 1381
DI 10.1007/s00371-021-02282-4
EA SEP 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000692956800004
DA 2024-07-18
ER

PT J
AU Li, M
   Zhao, L
   Zhou, DM
   Nie, RC
   Liu, YY
   Wei, YX
AF Li, Miao
   Zhao, Li
   Zhou, Dongming
   Nie, Rencan
   Liu, Yanyu
   Wei, Yixue
TI AEMS: an attention enhancement network of modules stacking for lowlight
   image enhancement
SO VISUAL COMPUTER
LA English
DT Article
DE Module stacking; Lowlight image; Video enhancement; Attention modules;
   Feature loss
ID DYNAMIC HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT; INFORMATION;
   ALGORITHM; RETINEX
AB Due to the images obtained in lowlight environments often showing low contrast, low brightness and artifacts, it is difficult to distinguish the details of these images for people. In the field of images fusion and target tacking, lowlight images cannot be processed better. In this paper, we proposed an end-to-end lowlight image enhancement network, which uses modules stacking methods and attention modules. Firstly, the method of module stacking was applied to extract the different features of images, and then the features are fused on the channel dimension. Finally, the final image was reconstructed with a series of convolutions. In particular, our loss function consists of two parts: the first part of the loss function was calculated using L-1 loss, L-2 loss and the gradient loss, and VGG network was utilized to calculate the second part. Furthermore, we verified the effectiveness of the model via a large number of comparative experiments, and illustrated the comparison results through quantitative and qualitative methods. We additionally show the performance of our network on lowlight video enhancement, which also has better results than the other methods.
C1 [Li, Miao; Zhou, Dongming; Nie, Rencan; Liu, Yanyu; Wei, Yixue] Yunnan Univ, Sch Informat Sci & Engn, Kunming, Yunnan, Peoples R China.
   [Zhao, Li] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England.
C3 Yunnan University; University of Sheffield
RP Zhou, DM (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming, Yunnan, Peoples R China.
EM zhoudm@ynu.edu.cn
RI li, miao/JED-3109-2023
OI li, miao/0000-0001-6196-2041; Zhou, Dongming/0000-0003-0139-9415
FU National Natural Science Foundation of China [62066047, 61966037,
   61463052, 61365001]
FX We sincerely thank the editors and the anonymous reviewers for their
   valuable comments. Besides, this work was supported by the National
   Natural Science Foundation of China under Grants 62066047, 61966037,
   61463052 and 61365001.
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen GC, 2022, VISUAL COMPUT, V38, P1051, DOI 10.1007/s00371-021-02067-9
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Chen YL, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106335
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo TD, 2021, VISUAL COMPUT, V37, P2069, DOI 10.1007/s00371-020-01964-9
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Janner M, 2017, ADV NEUR IN, V30
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin XJ, 2016, AAAI CONF ARTIF INTE, P1737
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li M, 2021, IET IMAGE PROCESS, V15, P2020, DOI 10.1049/ipr2.12173
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Liu YY, 2020, IET IMAGE PROCESS, V14, P1327, DOI 10.1049/iet-ipr.2019.0118
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2018, P BMVC, V220, P4
   Pathak SS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON ENGINEERING AND TECHNOLOGY (ICETECH), P163
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   PIZER SM, 1984, J COMPUT ASSIST TOMO, V8, P300
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheet D, 2010, IEEE T CONSUM ELECTR, V56, P2475, DOI 10.1109/TCE.2010.5681130
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shen L., 2017, ARXIV171102488
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tao L, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang C, 2005, IEEE T CONSUM ELECTR, V51, P1326, DOI 10.1109/TCE.2005.1561863
   Wang CM, 2021, VISUAL COMPUT, V37, P1233, DOI 10.1007/s00371-021-02079-5
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Ying Z., 2017, ARXIV PREPRINT ARXIV
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang XS, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3031398
   Zhang Y., 2020, arXiv
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhong JH, 2014, IEEE SENS J, V14, P2955, DOI 10.1109/JSEN.2014.2319891
NR 61
TC 4
Z9 4
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4203
EP 4219
DI 10.1007/s00371-021-02289-x
EA AUG 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000691202600001
DA 2024-07-18
ER

PT J
AU Mukherjee, S
AF Mukherjee, Soham
TI Denoising with discrete Morse theory
SO VISUAL COMPUTER
LA English
DT Article
DE Persistent homology; Discrete Morse theory; Topological data analysis;
   Noise removal
ID PERSISTENCE
AB Denoising noisy datasets is a crucial task in this data-driven world. In this paper, we develop a persistence-guided discrete Morse theoretic denoising framework. We use our method to denoise point-clouds and to extract surfaces from noisy volumes. In addition, we show that our method generally outperforms standard methods. Our paper is a synergy of classical noise removal techniques and topological data analysis.
C1 [Mukherjee, Soham] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
C3 Purdue University System; Purdue University
RP Mukherjee, S (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
EM mukher26@purdue.edu
RI Mukherjee, Soham/HTM-9584-2023
OI Mukherjee, Soham/0000-0003-0486-7763
CR Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   Attali D., 2009, TOPOINVIS 9, V9, P23
   Bauer U, 2012, DISCRETE COMPUT GEOM, V47, P347, DOI 10.1007/s00454-011-9350-z
   Beksi W., 2018, TOPOLOGICAL METHODS
   Berger Matthew, 2017, Computer Graphics Forum, V36, P301, DOI 10.1111/cgf.12802
   Bock A, 2018, IEEE T VIS COMPUT GR, V24, P812, DOI 10.1109/TVCG.2017.2743980
   Bremer PT, 2004, IEEE T VIS COMPUT GR, V10, P385, DOI 10.1109/TVCG.2004.3
   Buchet M, 2017, 33 INT S COMP GEOM, P231
   Carr H, 2003, COMP GEOM-THEOR APPL, V24, P75, DOI 10.1016/S0925-7721(02)00093-7
   Carr H., 2004, 14th Annual Fall Workshop on Computational Geometry, P51
   Chazal F, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1021
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   De Floriani L, 2015, COMPUT GRAPH FORUM, V34, P761, DOI 10.1111/cgf.12596
   Delfinado CecilJose A., 1993, SCG 93, P232, DOI DOI 10.1145/160985.161140
   Delgado-Friedrichs O, 2015, IEEE T PATTERN ANAL, V37, P654, DOI 10.1109/TPAMI.2014.2346172
   Dey T.K., 2018, CoRR
   Dey T. K., 2021, COMPUTATIONAL TOPOLO
   Dey T.K., 2010, COMPUT GRAPH FORUM
   Dey TK, 2018, COMPUT GRAPH-UK, V74, P33, DOI 10.1016/j.cag.2018.05.002
   Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454
   Edelsbrunner Herbert, 2010, Computational topology: an introduction
   Falcidieno B, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P329, DOI 10.1109/SMI.2004.1314520
   Forman, 1995, GEOMETRY TOPOLOGY 6
   Forman R, 1998, ADV MATH, V134, P90, DOI 10.1006/aima.1997.1650
   Forman R., 2002, SEM LOTHAR COMBIN, V48, pB48c
   Giraudot S., 2020, CGAL USER REFERENCE
   Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Levin D., 1998, Mathematics of Computation, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Munkres J.R., 1984, Elements of Algebraic Topology, DOI DOI 10.1201/9780429493911
   Pascucci V., 2004, PROC IASTED C VISUAL, P452
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   Rineau Laurent, 2020, CGAL User and Reference Manual
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P44, DOI 10.1109/38.103393
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Sousbie T, 2011, MON NOT R ASTRON SOC, V414, P350, DOI 10.1111/j.1365-2966.2011.18394.x
   The CGAL Project, 2020, CGAL User and Reference Manual
   Toth C.D., 2018, 34 INT S COMP GEOM, V99
   Xiong SY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661263
   Zhang J, 2013, EAI ENDORSED TRANS S, V13, DOI 10.4108/trans.sis.2013.01-03.e2
   Zhou JL, 2013, J VISUAL-JAPAN, V16, P341, DOI 10.1007/s12650-013-0180-3
NR 42
TC 1
Z9 2
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2883
EP 2894
DI 10.1007/s00371-021-02255-7
EA JUL 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000674208900002
DA 2024-07-18
ER

PT J
AU Zhao, SH
   Wang, L
   Qian, XM
   Chen, JP
AF Zhao, Shenghuan
   Wang, Luo
   Qian, Xueming
   Chen, Jianping
TI Enhancing performance-based generative architectural design with
   sketch-based image retrieval: a pilot study on designing building facade
   fenestrations
SO VISUAL COMPUTER
LA English
DT Article
DE Computer-aided design; Geometric computation; Design method; Artificial
   intelligence; Sketch-based image retrieval (SBIR)
ID MULTIOBJECTIVE OPTIMIZATION
AB By coupling parametric modelling, building performance (like energy efficiency) simulation, and algorithmic optimization, performance-based generative architectural design (PGAD) can automatically generate lots of high-performance architectural design solutions. Although it is 'performance-based', the final selection of a real design project still needs to consider the aesthetics of design choices. However, due to the overwhelming number of design choices generated by PGAD, it is difficult for designers to choose the most favourable one from them. Therefore, the current study tries to integrate the technology of sketch-based image retrieval (SBIR) into the selecting stage of PGAD. Rather than navigating alternatives one from another and getting lost, designers can directly find the most aesthetically preferred one by inputting his/her hand-drawn design. A design project of fenestrating a multiple-floor office building is used to demonstrate this method and test three SBIR algorithms: Angular radial partitioning (ARP), Angular radial orientation partitioning (AROP), and Sketch-A-Net model (SAN). Test results show that AROP performs the best among these three algorithms. Its retrievals are most similar to inquiry images drawn by architects. Meanwhile, performances of AROP with different template combinations are also rated. After that, AROP with the best template is also tested with incompletely drawn inquiry images. In the end, investigation results are validated by another building facade design case. The current study automates the PGAD process stepwise, making it more applicable to real design projects.
C1 [Zhao, Shenghuan; Chen, Jianping] Suzhou Univ Sci & Technol, Sch Architecture & Urban Planning, Suzhou, Peoples R China.
   [Zhao, Shenghuan; Chen, Jianping] Suzhou Univ Sci & Technol, Jiangsu Key Lab Intelligent Bldg Energy Efficienc, Suzhou, Peoples R China.
   [Wang, Luo; Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian, Peoples R China.
   [Chen, Jianping] Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou, Peoples R China.
C3 Suzhou University of Science & Technology; Suzhou University of Science
   & Technology; Xi'an Jiaotong University; Suzhou University of Science &
   Technology
RP Chen, JP (corresponding author), Suzhou Univ Sci & Technol, Sch Architecture & Urban Planning, Suzhou, Peoples R China.; Chen, JP (corresponding author), Suzhou Univ Sci & Technol, Jiangsu Key Lab Intelligent Bldg Energy Efficienc, Suzhou, Peoples R China.; Chen, JP (corresponding author), Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou, Peoples R China.
EM alanjpchen@yahoo.com
RI Chen, Jianping/HTO-0989-2023
OI Chen, Jianping/0000-0002-2109-5761; ZHAO, Shenghuan/0000-0003-3109-6087
FU Key Technologies Research and Development Program [2020YFC2006602];
   National Natural Science Foundation of China [62072324]; Jiangsu
   Provincial Key Research and Development Program [BE2020026]
FX Funding was provided by Key Technologies Research and Development
   Program (Grant No. 2020YFC2006602), National Natural Science Foundation
   of China (Grant No. 62072324) and Jiangsu Provincial Key Research and
   Development Program (Grant No. BE2020026).
CR Ahmed S, 2014, PATTERN RECOGN LETT, V35, P91, DOI 10.1016/j.patrec.2013.04.005
   [Anonymous], 2019, WALLACEI WALLACEI EV
   [Anonymous], 2019, AUTODESK PROJECT REF
   [Anonymous], 2015, SKETCH A NET BEATS H
   Brintrup AM, 2008, IEEE T EVOLUT COMPUT, V12, P343, DOI 10.1109/TEVC.2007.904343
   Bui T, 2017, COMPUT VIS IMAGE UND, V164, P27, DOI 10.1016/j.cviu.2017.06.007
   Bui T, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1012, DOI 10.1109/ICCVW.2015.133
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chalechale A, 2004, IEE P-VIS IMAGE SIGN, V151, P93, DOI 10.1049/ip-vis:20040332
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Gerber DJ, 2017, AUTOMAT CONSTR, V76, P45, DOI 10.1016/j.autcon.2017.01.001
   Guan ZY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3776
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Jakubiec J.A., 2018, BSO2018, P11
   Jonatan: TT TOOLBOX, 2017, FOOD4RHINO
   Kai Q, 2019, 2019 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P255, DOI 10.1109/ICSGEA.2019.00066
   Kazi RH, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P401, DOI 10.1145/3126594.3126662
   Klein R., 2011, INT C INF KNOWL MAN, P2097
   Kowaliw T, 2012, IEEE T EVOLUT COMPUT, V16, P523, DOI 10.1109/TEVC.2011.2166764
   Langenhan C., 2011, Proceedings of the CAADfutures 2011, Liege, P85
   Langenhan C, 2013, ADV ENG INFORM, V27, P413, DOI 10.1016/j.aei.2013.04.005
   Li Y, 2018, MACH VISION APPL, V29, P1083, DOI 10.1007/s00138-018-0953-8
   Liu PZ, 2017, IEEE T IMAGE PROCESS, V26, P5706, DOI 10.1109/TIP.2017.2736343
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Muehlbauer Manuel, 2020, Artificial Intelligence in Music, Sound, Art and Design. 9th International Conference, EvoMUSART 2020. Held as Part of EvoStar 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12103), P134, DOI 10.1007/978-3-030-43859-3_10
   Nagy D., 2017, P S SIM ARCH URB DES, P1, DOI [10.22360/simaud.2017.simaud.007, DOI 10.22360/SIMAUD.2017.SIMAUD.007]
   Parui S, 2014, LECT NOTES COMPUT SC, V8694, P398, DOI 10.1007/978-3-319-10599-4_26
   Petzold, 2017, VIS ENG, V5
   Qi YG, 2016, IEEE IMAGE PROC, P2460, DOI 10.1109/ICIP.2016.7532801
   Qian XM, 2017, IEEE T IMAGE PROCESS, V26, P3734, DOI 10.1109/TIP.2017.2699623
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712
   Rodrigues E, 2017, AUTOMAT CONSTR, V80, P48, DOI 10.1016/j.autcon.2017.03.017
   Roudsari M. S, 2019, HONEYBEE
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Seddati O, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P189, DOI 10.1145/3078971.3078985
   Sileryte R, 2016, 2016 PROCEEDINGS OF THE SYMPOSIUM ON SIMULATION FOR ARCHITECTURE AND URBAN DESIGN (SIMAUD 2016), P215
   Suga K, 2010, BUILD ENVIRON, V45, P1144, DOI 10.1016/j.buildenv.2009.10.021
   Sun Xinghai., 2013, Proceedings of the International Conference On Multimedia (MM), P233
   Takagi H, 2001, P IEEE, V89, P1275, DOI 10.1109/5.949485
   Thornton, 2017, DESIGN EXPLORER 2
   Wang, 2019, IJCAI 2019
   Wang L, 2019, IEEE SENSOR LETT, V3, DOI [10.1109/LSENS.2019.2896072, 10.1109/ICALT.2019.00007]
   Wang YX, 2018, IEEE T IMAGE PROCESS, V27, P4437, DOI 10.1109/TIP.2018.2837219
   Weber M, 2010, LECT NOTES ARTIF INT, V6176, P510
   Wessel, 2013, SHAPE RETRIEVAL METH
   Xia ZH, 2016, IEEE T INF FOREN SEC, V11, P2594, DOI 10.1109/TIFS.2016.2590944
   Xiao R, 2021, NEXUS NETW J, V23, P165, DOI 10.1007/s00004-020-00530-z
   Xu JY, 2021, VISUAL COMPUT, V37, P765, DOI 10.1007/s00371-020-01976-5
   Yang D, 2018, AUTOMAT CONSTR, V92, P242, DOI 10.1016/j.autcon.2018.03.023
   Yang JJ, 2017, ENERG BUILDINGS, V146, P27, DOI 10.1016/j.enbuild.2017.03.071
   Yasseen Z, 2017, VISUAL COMPUT, V33, P565, DOI 10.1007/s00371-016-1328-7
   Yousif S, 2019, COMM COM INF SC, V1028, P459, DOI 10.1007/978-981-13-8410-3_32
   [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao SH, 2020, SCI TECHNOL BUILT EN, V26, P1337, DOI 10.1080/23744731.2020.1777048
   Zhao SH, 2018, J INTEGR DES PROCESS, V22, P55, DOI 10.3233/JID190001
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
NR 59
TC 3
Z9 3
U1 5
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2981
EP 2997
DI 10.1007/s00371-021-02170-x
EA JUN 2021
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000660360700003
DA 2024-07-18
ER

PT J
AU Lee, SB
   Lee, KW
   Ko, HS
AF Lee, Sang-Bin
   Lee, Kyoung-Won
   Ko, Hyeong-Seok
TI Regularly striped preconditioner for implicit clothing simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Computer animation; Physically based simulation; Clothing simulation;
   Numerical integration
ID LINEAR-SYSTEMS
AB This paper investigates the question of whether the Jacobi preconditioner can be made to run faster by including a few additional outlying diagonals, for the case of mesh-based implicit clothing simulation. For the given N x N block system matrix A, we investigate two regularly striped (RS) preconditioners: (1) 3-RS which has two symmetric outlying diagonals with offset N/2 from the main diagonal and (2) 5-RS which has four symmetric outlying diagonals with offsets N/3 and 2N/3 from the main diagonal. This paper finds that both 3- and 5-RS preconditioners are consistently superior to the Jacobi preconditioner in the performance. Based on the loop iteration count and time measurement, we finally recommend 5-RS rather than 3-RS.
C1 [Lee, Sang-Bin; Lee, Kyoung-Won; Ko, Hyeong-Seok] Seoul Natl Univ, 1 Gwanak Ro, Seoul, South Korea.
C3 Seoul National University (SNU)
RP Ko, HS (corresponding author), Seoul Natl Univ, 1 Gwanak Ro, Seoul, South Korea.
EM sblee@graphics.snu.ac.kr; luis@graphics.snu.ac.kr;
   hsko@graphics.snu.ac.kr
OI Lee, Sang-Bin/0000-0001-8839-2472
FU R&D program for Advanced Integrated-intelligence for IDentification
   (AIID) through the National Research Foundation of Korea (NRF) -
   Ministry of Science and ICT [NRF-2018M3E3A1057288]; ASRI (Automation and
   Systems Research Institute at Seoul National University)
FX This research was supported by R&D program for Advanced
   Integrated-intelligence for IDentification (AIID) through the National
   Research Foundation of Korea (NRF) funded by Ministry of Science and ICT
   (NRF-2018M3E3A1057288), and ASRI (Automation and Systems Research
   Institute at Seoul National University).
CR Ainsley S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366170
   Ament M, 2010, EUROMICRO WORKSHOP P, P583, DOI 10.1109/PDP.2010.51
   Ascher UM, 2003, VISUAL COMPUT, V19, P526, DOI 10.1007/s00371-003-0220-4
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Benzi M, 1999, APPL NUMER MATH, V30, P305, DOI 10.1016/S0168-9274(98)00118-4
   Benzi M, 2002, J COMPUT PHYS, V182, P418, DOI 10.1006/jcph.2002.7176
   Bernstein DS., 2009, MATRIX MATH THEORY F, DOI DOI 10.1515/9781400833344
   Boxerman Eddy., 2004, SCA 04 P 2004 ACM SI, P153
   Chentanez N, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531394
   Choi Kwang-Jin., 2005, ACM SIGGRAPH 2005 CO, P1, DOI DOI 10.1145/1198555.1198571
   Cuthill E., 1969, P 1969 24 NAT C, P157, DOI [10.1145/800195.805928, DOI 10.1145/800195.805928]
   Grinspun E., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P62
   Hauth M, 2003, VISUAL COMPUT, V19, P581, DOI 10.1007/s00371-003-0206-2
   Saad Y., 2003, ITERATIVE METHODS SP, V82
   Shewchuk JR., 1994, INTRO CONJUGATE GRAD
   Sideris Costas, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P389, DOI 10.1007/978-3-642-25090-3_33
   Smolarski DC, 2006, J COMPUT APPL MATH, V186, P416, DOI 10.1016/j.cam.2005.02.012
   Swesty FD, 2004, ASTROPHYS J SUPPL S, V153, P369, DOI 10.1086/420785
   Ye JT, 2009, VISUAL COMPUT, V25, P959, DOI 10.1007/s00371-008-0307-z
NR 19
TC 1
Z9 1
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2022
VL 38
IS 8
BP 2827
EP 2838
DI 10.1007/s00371-021-02158-7
EA JUN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3B2OE
UT WOS:000658662600001
DA 2024-07-18
ER

PT J
AU Huang, DJ
   Wang, XL
   Liu, JH
   Li, JY
   Tang, W
AF Huang, Dongjin
   Wang, Xianglong
   Liu, Jinhua
   Li, Jinyao
   Tang, Wen
TI Virtual reality safety training using deep EEG-net and physiology data
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Brain&#8211; computer interface; EEG neural network;
   Construction safety; Health assessment
ID CLASSIFICATION; FEATURES; SIGNAL; VIDEO
AB Virtual reality (VR) safety training systems can enhance safety awareness while supporting health assessment in various work conditions. This paper proposes a novel VR system for construction safety training, which augments an individual's functioning in VR via a brain-computer interface of electroencephalography (EEG) and physiology data such as blood pressure and heart rate. The use of VR aims to support high levels of interactions and immersion. Crucially, we apply novel clipping training algorithms to improve the performance of a deep EEG neural network, including batch normalization and ELU activation functions for real-time assessment. It significantly improves the system performance in time efficiency while maintaining high accuracy of over 80% on the testing datasets. For assessing workers' competence under various construction environments, the risk assessment metrics are developed based on a statistical model and workers' EEG data. One hundred and seventeen construction workers in Shanghai took part in the study. Nine of the participants' EEG is identified with highly abnormal levels by the proposed evaluation metric. They have undergone further medical examinations, and among them, six are diagnosed with high-risk health conditions. It proves that our system plays a significant role in understanding workers' physical condition, enhancing safety awareness, and reducing accidents.
C1 [Huang, Dongjin; Wang, Xianglong; Liu, Jinhua; Li, Jinyao] Shanghai Univ, Shanghai Film Acad, Shanghai, Peoples R China.
   [Tang, Wen] Univ Bournemouth, Fac Sci Design & Technol, Poole, Dorset, England.
C3 Shanghai University; Bournemouth University
RP Huang, DJ (corresponding author), Shanghai Univ, Shanghai Film Acad, Shanghai, Peoples R China.
EM djhuang@shu.edu.cn
RI huang, shan/JVN-1240-2024; feng, chen/JLM-8296-2023
FU Shanghai Natural Science Foundation of China [19ZR1419100]; National
   Natural Science Foundation of China [61402278]
FX This work was supported by the Shanghai Natural Science Foundation of
   China under Grant No. 19ZR1419100 and the National Natural Science
   Foundation of China under Grant No. 61402278.
CR Ang KK, 2008, IEEE IJCNN, P2390, DOI 10.1109/IJCNN.2008.4634130
   Badcock NA, 2013, PEERJ, V1, DOI 10.7717/peerj.38
   Cai HS, 2016, IEEE INT C BIOINFORM, P1239, DOI 10.1109/BIBM.2016.7822696
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Clifford RMS, 2021, VISUAL COMPUT, V37, P63, DOI 10.1007/s00371-020-01816-6
   Corelli F, 2020, HUCAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 2: HUCAPP, P146, DOI 10.5220/0008962401460153
   Duncan CC, 2009, CLIN NEUROPHYSIOL, V120, P1883, DOI 10.1016/j.clinph.2009.07.045
   Gong H, 2013, J CHENGDU AERONAUT P, V29, P34
   Hajinoroozi M, 2016, SIGNAL PROCESS-IMAGE, V47, P549, DOI 10.1016/j.image.2016.05.018
   Harvey C., 2019, VISUAL COMPUT
   Hosseinifard B, 2013, COMPUT METH PROG BIO, V109, P339, DOI 10.1016/j.cmpb.2012.10.008
   Huang DJ, 2020, 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020), P172, DOI 10.1109/CW49994.2020.00036
   Janjarasjitt S, 2017, MED BIOL ENG COMPUT, V55, P1743, DOI 10.1007/s11517-017-1613-2
   KAHNEMAN D, 1972, COGNITIVE PSYCHOL, V3, P430, DOI 10.1016/0010-0285(72)90016-3
   Kang JN, 2020, COMPUT BIOL MED, V120, DOI 10.1016/j.compbiomed.2020.103722
   Kweon SH, 2018, ADV INTELL SYST, V592, P194, DOI 10.1007/978-3-319-60366-7_19
   Lan ZR, 2016, VISUAL COMPUT, V32, P347, DOI 10.1007/s00371-015-1183-y
   Lawhern VJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aace8c
   Le QT, 2015, J INTELL ROBOT SYST, V79, P487, DOI 10.1007/s10846-014-0112-z
   Liang S, 2019, CHIN HLTH SERV MANAG, V36, P183
   Lin CT, 2008, IEEE INT SYMP CIRC S, P1088, DOI 10.1109/ISCAS.2008.4541611
   Liu YS, 2017, 2017 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P64, DOI 10.1109/CW.2017.37
   Lu YA, 2018, TECHNOL HEALTH CARE, V26, pS337, DOI 10.3233/THC-174679
   Marathe AR, 2016, IEEE T NEUR SYS REH, V24, P333, DOI 10.1109/TNSRE.2015.2502323
   Margaux P, 2012, ADV HUM-COMPUT INTER, V2012, DOI 10.1155/2012/578295
   Moldovan AN, 2013, IEEE INT SYM BROADB
   Nicolaou N, 2012, EXPERT SYST APPL, V39, P202, DOI 10.1016/j.eswa.2011.07.008
   Ozkan DG, 2018, J NEUROPHYSIOL, V119, P1254, DOI 10.1152/jn.00825.2017
   PETTITT AN, 1977, TECHNOMETRICS, V19, P205, DOI 10.2307/1268631
   Polich J, 2007, CLIN NEUROPHYSIOL, V118, P2128, DOI 10.1016/j.clinph.2007.04.019
   Raake A, 2013, APPL METHODS, P55
   Saunders J, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1908, DOI [10.1109/vr.2019.8798371, 10.1109/VR.2019.8798371]
   Shamsudin NM, 2018, ADV SCI LETT, V24, P2444, DOI 10.1166/asl.2018.10976
   Singh N, 2020, INTELL DECIS TECHNOL, V14, P239, DOI 10.3233/IDT-190043
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   SUN FT, 2012, MOBILE COMPUTING APP, P211, DOI DOI 10.1007/978-3-642-29336-8_12
   Tabar YR, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2560/14/1/016003
   THEILER J, 1995, PHYS LETT A, V196, P335, DOI 10.1016/0375-9601(94)00856-K
   Vourvopoulos A, 2017, VISUAL COMPUT, V33, P533, DOI 10.1007/s00371-016-1304-2
   Wölfel M, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P168, DOI 10.1109/CW.2018.00040
   Yoon S, 2020, J MECH MED BIOL, V20, DOI 10.1142/S0219519420400072
NR 41
TC 16
Z9 18
U1 8
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2022
VL 38
IS 4
SI SI
BP 1195
EP 1207
DI 10.1007/s00371-021-02140-3
EA MAY 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0D9DX
UT WOS:000648249600002
DA 2024-07-18
ER

PT J
AU Ye, X
   Wang, H
   Li, Y
AF Ye, Xiang
   Wang, Heng
   Li, Yong
TI Image content-dependent steerable kernels
SO VISUAL COMPUTER
LA English
DT Article
AB Attention mechanism plays an essential role in many tasks such as image classification, object detection, and instance segmentation. However, existing methods typically assigned attention weights to feature maps of the previous layer. The kernels in current layer remained static during the inference stage. To explicitly model the dependency of individual kernel weights on image content at the inference stage, this work proposed attention weight block (AWB) that converts kernels to be steerable to the content in a test image. Specifically, AWB computes a set of on-the-fly coefficients according to the feature maps of the previous layer and applies the coefficients to the kernels in current layers, which makes them steerable. AWB kernels emphasize or suppress the weights of certain kernels depending on the content of input samples and hence significantly improve the feature representation ability of deep neural networks. The proposed AWB is evaluated on various datasets, and experimental results show that steerable kernels in AWB outperformed the state-of-the-art attention approaches when embedded in the architecture for classification, object detection, and semantic segmentation tasks. It outperforms ECA by 1.1% and 1.0% on CIFAR-100 and Tiny ImageNet datasets, respectively, for image classification task; outperforms CornerNet-Lite by 1.5% on COCO2017 dataset for object detection task; and outperforms FCN8s by 1.2% on SBUshadow dataset for semantic segmentation task.
C1 [Ye, Xiang; Wang, Heng; Li, Yong] Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Li, Y (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
EM xye@bupt.edu.cn; yli@bupt.edu.cn
FU National Natural Science Foundation of China [62071060]; Beijing Key
   Laboratory of Work Safety and Intelligent Monitoring Foundation
FX This work was supported by National Natural Science Foundation of China
   (No. 62071060) and the Beijing Key Laboratory of Work Safety and
   Intelligent Monitoring Foundation. We would like to thank Huachun Tan
   for helpful discussion and Zihang He for proof-reading the manuscript.
CR Bochkovskiy A., 2020, PREPRINT
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H., 2019, Cornernet-lite: Efficient keypoint based object detection
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li YZ, 2017, AIP ADV, V7, DOI 10.1063/1.5010804
   Li Z., 2017, CORR
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZY, 2021, VISUAL COMPUT, V37, P529, DOI 10.1007/s00371-020-01821-9
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma, 2018, ARXIV181207166
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012
   Sharma PK, 2021, VISUAL COMPUT, V37, P2083, DOI 10.1007/s00371-020-01971-w
   Srivastava RK, 2015, ARXIV150500387
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Wang, 2018, ARXIV181107484
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao L., 2015, 231N CS, V2, P8
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao Q, 2018, ARXIV181104533
NR 49
TC 1
Z9 1
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2527
EP 2538
DI 10.1007/s00371-021-02128-z
EA APR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000644371000001
DA 2024-07-18
ER

PT J
AU Sykes, ER
AF Sykes, Edward R.
TI A deep learning computer vision iPad application for Sales Rep
   optimization in the field
SO VISUAL COMPUTER
LA English
DT Article
DE Applications of computer vision; Deep learning; Mobile computing; Object
   detection in real-world environments
AB Computer vision is becoming an increasingly critical area of research, and its applications to real-world problems are gaining significance. In this paper, we describe the design, development and evaluation of our computer vision Faster R-CNN iPad App for Sales Representatives in grocery store environments. Our system aims to assist Sales Reps to be more productive, reduce errors, and provide increased efficiencies. We report on the creation of the iPad app, the data capturing guidelines we created for the creation of good classifiers and the results of professional Sales Reps evaluating our system. Our system was tested in a variety of conditions in grocery store environments and has an accuracy of 99%, a System Usability Score usability score of 85 (high). It supports up to 40 classifiers running concurrently to perform product identification in less than 3.8 s. We also created a set of data capturing guidelines that will enable other researchers to create their own classifiers for these types of products in complex environments (e.g., products with very similar packaging located on shelves).
C1 [Sykes, Edward R.] Sheridan Coll, Ctr Mobile Innovat, 1430 Trafalgar Rd, Oakville, ON L6H 2L1, Canada.
RP Sykes, ER (corresponding author), Sheridan Coll, Ctr Mobile Innovat, 1430 Trafalgar Rd, Oakville, ON L6H 2L1, Canada.
EM ed.sykes@sheridancollege.ca
RI Sykes, Edward/AAB-3544-2020
OI Sykes, Dr. Edward R/0000-0001-7339-7481
FU Natural Science and Engineering Research Council (NSERC) [NSERC CARD2
   469614]
FX We gratefully acknowledge the Natural Science and Engineering Research
   Council (NSERC: nserc-crsng.gc.ca) for the funding for this research
   (Grant Number NSERC CARD2 469614) and Tuan Mai's contributions.
CR ADVANI G, 2015, IEEE SYM EMBED SYST, P1
   Akkas, 2018, IMPACT SHELF SPACE P
   Andersen Kirsti., 2007, The Geometry of an Art: The History of the Mathematical Theory of Perspective from Alberti to Monge
   [Anonymous], 2019, IMAGENET IMAGENET
   [Anonymous], 2020, MATLAB MATLAB MATHWO
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Brooke J, 2013, J USABILITY STUD, V8, P29
   Brownlee J., 2018, Imbalanced classification with Python: better metrics, balance skewed classes, cost-sensitive learning
   Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2
   Castelo-Branco F, 2020, SMART INNOV SYST TEC, V167, P406, DOI 10.1007/978-981-15-1564-4_38
   Cetin, 2017, 25 SIGN PROC COMM AP
   Damisch Hubert., 1994, The Origin of Perspective
   DREZE X, 1994, J RETAILING, V70, P301, DOI 10.1016/0022-4359(94)90002-7
   Elkan C., 2011, EVALUATING CLASSIFIE
   Faruqui N., 2017, Open Source Computer Vision for Beginners: Learn OpenCV Using C++ in Fastest Possible Way
   Forman George, 2010, ACM SIGKDD Explorations Newsletter, DOI DOI 10.1145/1882471.1882479
   Franco A, 2017, EXPERT SYST APPL, V81, P163, DOI 10.1016/j.eswa.2017.02.050
   HAFIZ R, 2016, COMP INT IWCI INT WO, P192
   Howard, 2011, CONFUSION MATRIX
   Kaehler A., 2017, LEARNING OPENCV COMP
   Kohavi R., 1998, Machine Learning, V30, P271
   Lu Z, 2004, BIOINFORMATICS, V20, P547, DOI 10.1093/bioinformatics/btg447
   MANKODIYA K., 2012, Sensor Systems and Software, P121
   Melek CG, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P145, DOI 10.1109/UBMK.2017.8093584
   Mentzer JT, 2000, J RETAILING, V76, P549, DOI 10.1016/S0022-4359(00)00040-3
   MOREAU P, 2020, J RETAILING, DOI DOI 10.1016/j.jretai.2019.12.003
   Porter E., 2017, GARTNER RES
   Pouyanfar S, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3234150
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roelofs R., 2019, P 33 INT C NEURAL IN, P9179
   Snoek J, 2015, PR MACH LEARN RES, V37, P2171
   SOLTIA A, 2018, IEEE T PATTERN ANAL, V112, P76, DOI DOI 10.1016/j.dss.2018.06.006
   Song YL, 2017, IEEE WINT CONF APPL, P9, DOI 10.1109/WACVW.2017.9
   Tian, 2016, COMPUT VIS PATTERN R
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 35
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 729
EP 748
DI 10.1007/s00371-020-02047-5
EA FEB 2021
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ZE7IA
UT WOS:000614677500003
OA hybrid
DA 2024-07-18
ER

PT J
AU Xu, RJ
   Ren, B
AF Xu, Rong-Jie
   Ren, Bo
TI Solving the fluid pressure with an iterative multi-resolution guided
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Eulerian fluid simulation; Acceleration; Convolutional neural network
   (CNN); Pressure solution
ID WATER
AB In Eulerian methods, the simulation of an incompressible fluid field requires a pressure field solution, which takes a large amount of time and computation resources to solve a large coarse linear system. The pressure solver has two mathematical features. The first is that it obtains the pressure solution from a velocity divergence distribution in high-dimensional space. The second is that the pressure is iteratively solved in the projection step. Based on these two features, we investigate a convolutional-based neural network, which learns to map the fluid quantities to pressure solution iteratively by inferring from multiple grid scales. Our proposed network extracts features from multiple scales and then aligns them to obtain a pressure field in the original resolution. We trim our network structure to be compact and fast and design it to be iterative like to improve performance. Our approach requires less computation cost, while it achieves comparable performance with recently proposed data-driven methods. Our method can easily be parallelized in GPU devices, and we demonstrate its speed-up ability with the fluid field in larger input scenes.
C1 [Xu, Rong-Jie; Ren, Bo] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
C3 Nankai University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
EM xrj@mail.nankai.edu.cn; rb@nankai.edu.cn
RI ren, bo/IST-0814-2023
FU National Key R&D Program of China [2017YFB1002701]; Natural Science
   Foundation of China [61602265]
FX This study was funded by the National Key R&D Program of China
   (2017YFB1002701) and Natural Science Foundation of China (61602265).
CR [Anonymous], 2010, P 2010 ACM SIGGRAPHE
   Bridson R., 2015, FLUID SIMULATION COM, DOI [10.1201/9781315266008, DOI 10.1201/9781315266008]
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cornelis J, 2019, VISUAL COMPUT, V35, P579, DOI 10.1007/s00371-018-1488-8
   Dupont TF, 2003, J COMPUT PHYS, V190, P311, DOI 10.1016/S0021-9991(03)00276-6
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kim B, 2019, COMPUT GRAPH FORUM, V38, P59, DOI 10.1111/cgf.13619
   Kim T, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360649
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Lagaris IE, 1998, IEEE T NEURAL NETWOR, V9, P987, DOI 10.1109/72.712178
   Lagaris IE, 2000, IEEE T NEURAL NETWOR, V11, P1041, DOI 10.1109/72.870037
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lim JG, 2016, VISUAL COMPUT, V32, P641, DOI 10.1007/s00371-015-1080-4
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   Ma PX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201334
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Raissi Maziar, 2017, Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations
   Rockafellar R.T, 1979, NAVIER STOKES EQUATI, P530
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Sirignano J, 2018, J COMPUT PHYS, V375, P1339, DOI 10.1016/j.jcp.2018.08.029
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Thuerey N, 2020, AIAA J, V58, P15, DOI 10.2514/1.J058291
   Tompson J, 2017, PR MACH LEARN RES, V70
   Wiewel S, 2019, COMPUT GRAPH FORUM, V38, P71, DOI 10.1111/cgf.13620
   Xie Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201304
   Yang C, 2016, COMPUT ANIMAT VIRT W, V27, P415, DOI 10.1002/cav.1695
NR 30
TC 0
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 433
EP 442
DI 10.1007/s00371-020-02025-x
EA JAN 2021
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000605853500001
DA 2024-07-18
ER

PT J
AU An, FP
   Liu, JE
   Bai, L
AF An, Feng-Ping
   Liu, Jun-e
   Bai, Lei
TI Object recognition algorithm based on optimized nonlinear activation
   function-global convolutional neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Deep learning; Global convolutional neural network; Nonlinear activation
   function; Object detection; Object recognition
ID CLASSIFICATION; ACCURATE; CNN
AB Traditional object recognition algorithms cannot meet the requirements of object recognition accuracy in the actual warehousing and logistics field. In recent years, the rapid development of the deep learning theory has provided a technical approach for solving the above problems, and a number of object recognition algorithms has been proposed based on deep learning, which have been promoted and applied. However, deep learning has the following problems in the application process of object recognition: First, the nonlinear modeling ability of the activation function in the deep learning model is poor; second, the deep learning model has a large number of repeated pooling operations during which information is lost. In view of these shortcomings, this paper proposes multiple-parameter exponential linear units with uniform and learnable parameter forms and introduces two learned parameters in the exponential linear unit (ELU), enabling it to represent piecewise linear and exponential nonlinear functions. Therefore, the ELU has good nonlinear modeling capabilities. At the same time, to improve the problem of losing information in the large number of repeated pooling operations, this paper proposes a new global convolutional neural network structure. This network structure makes full use of the local and global information of different layer feature maps in the network. It can reduce the problem of losing feature information in the large number of pooling operations. Based on the above ideas, this paper suggests an object recognition algorithm based on the optimized nonlinear activation function-global convolutional neural network. Experiments were carried out on the CIFAR100 dataset and the ImageNet dataset using the object recognition algorithm proposed in this paper. The results show that the object recognition method suggested in this paper not only has a better recognition accuracy than traditional machine learning and other deep learning models but also has a good stability and robustness.
C1 [An, Feng-Ping] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [An, Feng-Ping] Huaiyin Normal Univ, Sch Phys & Elect Elect Engn, Huaian 223300, JS, Peoples R China.
   [Liu, Jun-e] Beijing Wuzi Univ, Sch Informat, Beijing 100081, Peoples R China.
   [Bai, Lei] North China Inst Sci & Technol, Hebei IoT Monitoring Engn Technol Res Ctr, Beijing 101049, Peoples R China.
C3 Beijing Institute of Technology; Huaiyin Normal University; Beijing Wuzi
   University; North China Institute Science & Technology
RP An, FP (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.; An, FP (corresponding author), Huaiyin Normal Univ, Sch Phys & Elect Elect Engn, Huaian 223300, JS, Peoples R China.; Bai, L (corresponding author), North China Inst Sci & Technol, Hebei IoT Monitoring Engn Technol Res Ctr, Beijing 101049, Peoples R China.
EM anfengping@163.com; 2924175349@qq.com; 271007963@qq.com
FU National Natural Science Foundation of China [61701188]; China
   Postdoctoral ScienceFoundation [2019M650512]; Beijing Intelligent
   Logistics System Collaborative Innovation Center [BILSCIC-2019KF-22];
   Hebei IoT Monitoring Engineering Technology Research Center [IOT202004]
FX This paper is supported by National Natural Science Foundation of China
   (No. 61701188), China Postdoctoral ScienceFoundation (No. 2019M650512),
   Beijing Intelligent Logistics System Collaborative Innovation Center
   (No. BILSCIC-2019KF-22), and Hebei IoT Monitoring Engineering Technology
   Research Center funded project (No. IOT202004).
CR Al-Halah Z, 2015, IEEE WINT CONF APPL, P837, DOI 10.1109/WACV.2015.116
   Asaoka Tadashi, 2018, ROBOMECH Journal, V5, DOI 10.1186/s40648-018-0118-6
   Bao YQ, 2019, STRUCT HEALTH MONIT, V18, P401, DOI 10.1177/1475921718757405
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen L., 2019, CHIN C IM GRAPH TECH, P602
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Eudes A, 2018, AUTONOMOUS SAFE INSP, P221
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao HB, 2018, IEEE T IND INFORM, V14, P4224, DOI 10.1109/TII.2018.2822828
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Gualdi G, 2012, IEEE T PATTERN ANAL, V34, P1589, DOI 10.1109/TPAMI.2011.247
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoang D.C., 2019, OBJECT RPE DENSE 3D
   Hoffman J, 2014, INT J COMPUT VISION, V109, P28, DOI 10.1007/s11263-014-0719-3
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Huang Y, 2016, IEEE INT CONF ROBOT, P1672, DOI 10.1109/ICRA.2016.7487308
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Khagi B, 2019, INT J IMAG SYST TECH, V29, P297, DOI 10.1002/ima.22316
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mitra V, 2017, SPEECH COMMUN, V89, P103, DOI 10.1016/j.specom.2017.03.003
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tian LL, 2019, APPL OPTICS, V58, P7523, DOI 10.1364/AO.58.007523
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang J., 2018, PROC BRIT MACH VIS C
   Wang JD, 2019, PATTERN RECOGN LETT, V119, P3, DOI 10.1016/j.patrec.2018.02.010
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xie GT, 2018, PROC CVPR IEEE, P8847, DOI 10.1109/CVPR.2018.00922
   Xiong W, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5934, DOI 10.1109/ICASSP.2018.8461870
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhan Y., 2015, AAAI FALL S SER
   Zhao Liming, 2016, ARXIV161107718
NR 55
TC 12
Z9 13
U1 8
U2 59
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 541
EP 553
DI 10.1007/s00371-020-02033-x
EA JAN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604490300005
DA 2024-07-18
ER

PT J
AU Lyu, CZ
   Hu, GQ
   Wang, D
AF Lyu, Chengzhi
   Hu, Guoqing
   Wang, Dan
TI Attention to fine-grained information: hierarchical multi-scale network
   for retinal vessel segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Convolutional neural network; Fine-grained; Retinal vessel segmentation;
   Sensitivity
AB Medical segmentation is a task that pays attention to details. The rapid development of deep learning in image processing technology makes it possible to segment objects accurately on small datasets. In this paper, we propose a hierarchical multi-scale attention network that focuses on the fine-grained parts of the target. Our attention network consists of a hierarchical encoder module with dense connections, a multi-scale module attention to fine-grained parts, and a decoder module. We also combine the weighted cross-entropy loss function based on details and the Dice coefficient loss to increase the sensitivity of fine grains. To verify our module's performance, we carried out a series of comparative experiments on the multi-scale attention module on the DRIVE dataset. We determine the best structure through experiments and compare it with several classical deep learning models. Our experiments show that extracting semantic information of images at an appropriate resolution can also improve the accuracy of detail segmentation. To show the generalization ability of the work, we conducted experiments on different DRIVE, STARE, and CHASE_DB 1 datasets, and our method achieved 0.8802/0.8464/0.8216 in sensitivity performance metric, 0.9756/0.9869/0.9784 in specificity, and 0.9675/0.9657/0.9637 in accuracy.
C1 [Lyu, Chengzhi; Hu, Guoqing; Wang, Dan] South China Univ Technol, Sch Mech & Automot Engn, Guangzhou, Peoples R China.
C3 South China University of Technology
RP Lyu, CZ (corresponding author), South China Univ Technol, Sch Mech & Automot Engn, Guangzhou, Peoples R China.
EM sanchilongquan@outlook.com; gqhu@scut.edu.cn
OI Lyu, Chengzhi/0000-0001-9160-0324
FU Nature Science Foundation of Guangdong province [2016A030313520]
FX This work is supported by the Nature Science Foundation of Guangdong
   province, No. 2016A030313520.
CR Alom M. Z., 2018, ARXIV180206955, V6, P014006, DOI 10.1109/NAECON.2018.8556686
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Buyssens P, 2013, MULTISCALE CONVOLUTI, P342
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cholakkal H, 2016, PROC CVPR IEEE, P5278, DOI 10.1109/CVPR.2016.570
   Diaz-Pinto A, 2019, IEEE T MED IMAGING, V38, P2211, DOI 10.1109/TMI.2019.2903434
   Fang LY, 2019, IEEE T MED IMAGING, V38, P1959, DOI 10.1109/TMI.2019.2898414
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Gibson E, 2018, IEEE T MED IMAGING, V37, P1822, DOI 10.1109/TMI.2018.2806309
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Hafiane A, 2008, INT C PATT RECOG, P2116, DOI 10.1109/icpr.2008.4761744
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huazhu Fu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P132, DOI 10.1007/978-3-319-46723-8_16
   Orlando JI, 2017, IEEE T BIO-MED ENG, V64, P16, DOI 10.1109/TBME.2016.2535311
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Neverova N, 2015, LECT NOTES COMPUT SC, V8925, P474, DOI 10.1007/978-3-319-16178-5_33
   Oliveira A, 2018, EXPERT SYST APPL, V112, P229, DOI 10.1016/j.eswa.2018.06.034
   Owen CG, 2009, INVEST OPHTH VIS SCI, V50, P2004, DOI 10.1167/iovs.08-3018
   Paulano F, 2014, VISUAL COMPUT, V30, P939, DOI 10.1007/s00371-014-0963-0
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song TH, 2017, IEEE T BIO-MED ENG, V64, P2913, DOI 10.1109/TBME.2017.2690863
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang DW, 2020, SHOCK VIB, V2020, DOI 10.1155/2020/6190215
   Wu YC, 2020, NEURAL NETWORKS, V126, P153, DOI 10.1016/j.neunet.2020.02.018
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan ZQ, 2019, IEEE J BIOMED HEALTH, V23, P1427, DOI 10.1109/JBHI.2018.2872813
   Zhang J, 2017, PATTERN RECOGN, V69, P107, DOI 10.1016/j.patcog.2017.04.008
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao X, 2019, FINE GRAINED LUNG NO
   Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937
   Zhuang J., 2018, PREPRINTS
NR 43
TC 8
Z9 9
U1 4
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2022
VL 38
IS 1
BP 345
EP 355
DI 10.1007/s00371-020-02018-w
EA DEC 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YW6DE
UT WOS:000600824200001
DA 2024-07-18
ER

PT J
AU Song, HT
   Wang, MH
   Zhang, LG
   Li, Y
   Jiang, ZY
   Yin, GS
AF Song, Hongtao
   Wang, Minghao
   Zhang, Liguo
   Li, Yang
   Jiang, Zhengyi
   Yin, Guisheng
TI S<SUP>2</SUP>RGAN: sonar-image super-resolution based on generative
   adversarial network
SO VISUAL COMPUTER
LA English
DT Article
DE Sonar-image; Super-resolution; Generative adversarial network; Transfer
   learning
ID RESOLUTION
AB As an important display mode of underwater environments, the sonar image has limitations on the resolution, which often leads to problems with low resolution of underwater objects. Therefore, the image super-resolution algorithm is needed to transform the images from low-resolution to high-resolution. It can improve the visual effect and contribute to subsequent processing such as 3D reconstruction and object recognition. This paper proposes a method for sonar image super-resolution based on generative adversarial networks (GAN). By comparing the super-resolution effects of various interpolation and convolutional neural network algorithms on sonar images, a Residual-in-Residual Dense Block network is employed as the generator of GAN since it has the low distortion and high perceptual quality. Because the sonar image training set does not have enough data, the generator utilizes the transfer learning on the sonar images to produce an optimized network model which is more suitable for super-resolution of sonar image. The VGG19 network is employed as the discriminator. In addition, the perceptual loss is introduced into the loss function of S(2)RGAN to further improve the perceptual quality of super-resolution images. The experimental results indicate that the proposed S(2)RGAN shows excellent performance. The generated super-resolution images of S(2)RGAN have the remarkable advantages of both lower distortion and higher perceptual quality comparing with other methods. Because S(2)RGAN focuses more on the reality and overall visual effect of super-resolution sonar images, it is suitable for various underwater situations.
C1 [Song, Hongtao; Wang, Minghao; Zhang, Liguo; Li, Yang; Jiang, Zhengyi; Yin, Guisheng] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Harbin Engineering University
RP Zhang, LG (corresponding author), Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
EM songhongtao@hrbeu.edu.cn; wangrninghao@hrbeu.edu.cn;
   zhangliguo@hrbeu.edu.cn; liyang@hrbeu.edu.cn; jiangzhengyi@hrbeu.edu.cn;
   yinguisheng@hrbeu.edu.cn
RI LIGUO, ZHANG/AAC-8765-2021
OI Song, Hongtao/0000-0001-6132-5843
FU National Natural Science Foundation of China [61501132]; China
   Postdoctoral Science Foundation [2019M661319]; Heilong-jiang
   Postdoctoral Scientific Research Developmental Founda-tion [LBH-Q17042];
   Fundamental Research Funds for the Central Universities [3072019CFT0603]
FX This work is supported by National Natural Science Foundation of China
   (No. 61501132), China Postdoctoral Science Foundation (No. 2019M661319),
   Heilong-jiang Postdoctoral Scientific Research Developmental Founda-tion
   (No. LBH-Q17042), and Fundamental Research Funds for the Central
   Universities (No. 3072019CFT0603).
CR Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Dai SY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1039
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Hurtós N, 2012, IEEE INT C INT ROBOT, P5298, DOI 10.1109/IROS.2012.6385813
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Mikaeli E, 2020, VISUAL COMPUT, V36, P1573, DOI 10.1007/s00371-019-01756-w
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Pandey G, 2020, VISUAL COMPUT, V36, P1291, DOI 10.1007/s00371-019-01729-z
   Park J, 2019, IEICE T INF SYST, VE102D, P210, DOI 10.1587/transinf.2018EDL8170
   Rasti P, 2014, SIG PROCESS COMMUN, P552, DOI 10.1109/SIU.2014.6830288
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stark H., 2010, DIGITAL IMAGE SYNTHE, P80
   Sung M., 2018, OCEANS MTS IEEE KOBE, P1
   Sung M, 2018, TENCON IEEE REGION, P0457, DOI 10.1109/TENCON.2018.8650176
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Yu J, 2006, INT CONF SIGN PROCES, P1019
   Zhang XH, 2012, VISUAL COMPUT, V28, P1167, DOI 10.1007/s00371-011-0666-8
NR 34
TC 15
Z9 14
U1 2
U2 38
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2285
EP 2299
DI 10.1007/s00371-020-01986-3
EA OCT 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000578108800001
DA 2024-07-18
ER

PT J
AU Khan, SD
   Basalamah, S
AF Khan, Sultan Daud
   Basalamah, Saleh
TI Scale and density invariant head detection deep model for crowd counting
   in pedestrian crowds
SO VISUAL COMPUTER
LA English
DT Article
DE Dense scales; Crowd counting; Head detection; High density crowds
AB Crowd counting in high density crowds has significant importance in crowd safety and crowd management. Existing state-of-the-art methods employ regression models to count the number of people in an image. However, regression models are blind and cannot localize the individuals in the scene. On the other hand, detection-based crowd counting in high density crowds is a challenging problem due to significant variations in scales, poses and appearances. The variations in poses and appearances can be handled through large capacity convolutional neural networks. However, the problem of scale lies in the heart of every detector and needs to be addressed for effective crowd counting. In this paper, we propose a end-to-end scale invariant head detection framework that can handle broad range of scales. We demonstrate that scale variations can be handled by modeling a set of specialized scale-specific convolutional neural networks with different receptive fields. These scale-specific detectors are combined into a single backbone network, where parameters of the network is optimized in end-to-end fashion. We evaluated our framework on challenging benchmark datasets, i.e., UCF-QNRF, UCSD. From experiment results, we demonstrate that proposed framework beats existing methods by a great margin.
C1 [Khan, Sultan Daud; Basalamah, Saleh] Natl Univ Technol, Islamabad, Pakistan.
RP Khan, SD (corresponding author), Natl Univ Technol, Islamabad, Pakistan.
EM sultandaud@nutech.edu.pk
RI Khan, Sultan Daud/J-7563-2019
OI Khan, Sultan Daud/0000-0002-7406-8441
FU NVIDIA Corporation
FX We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the Titan Xp GPU for this research.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai YL, 2018, LECT NOTES COMPUT SC, V11216, P21, DOI 10.1007/978-3-030-01258-8_2
   Basalamah S, 2019, IEEE ACCESS, V7, P71576, DOI 10.1109/ACCESS.2019.2918650
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jin MX, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.1.013006
   Kang D., 2018, T CIRCUITS SYSTEMS V
   Khan SD, 2019, IEEE IMAGE PROC, P4474, DOI [10.1109/icip.2019.8803409, 10.1109/ICIP.2019.8803409]
   Li YZ, 2016, LECT NOTES COMPUT SC, V9907, P420, DOI 10.1007/978-3-319-46487-9_26
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mliki H, 2020, SIGNAL IMAGE VIDEO P, V14, P1345, DOI 10.1007/s11760-020-01680-w
   QIN HW, 2016, PROC CVPR IEEE, P3456, DOI DOI 10.1109/CVPR.2016.376
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Saqib M.F., 2018, 2018 3rd International Conference for Convergence in Technology (I2CT), P1
   Saqib M, 2019, IEEE ACCESS, V7, P35317, DOI 10.1109/ACCESS.2019.2904712
   Shami MB, 2019, IEEE T CIRC SYST VID, V29, P2627, DOI 10.1109/TCSVT.2018.2803115
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Tong K, 2020, IMAGE VISION COMPUT, V97, DOI 10.1016/j.imavis.2020.103910
   Vora Aditya, 2018, ARXIV180908766
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhu LP, 2020, NEURAL COMPUT APPL, V32, P5105, DOI 10.1007/s00521-018-3954-7
NR 30
TC 40
Z9 40
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2127
EP 2137
DI 10.1007/s00371-020-01974-7
EA SEP 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000570021200001
DA 2024-07-18
ER

PT J
AU Guo, TD
   Xu, X
AF Guo, Tengda
   Xu, Xin
TI Salient object detection from low contrast images based on local
   contrast enhancing and non-local feature learning
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; Low contrast; Non-local feature;
   Image-enhanced network
ID REGION; CNN; GRAPHICS
AB Salient object detection can facilitate numerous applications. Traditional salient object detection models mainly utilize low-level hand-crafted features or high-level deep features. However, they may face great challenges in the nighttime scene, due to the difficulties in extracting well-defined features to represent saliency information from low contrast images. In this paper, we present a salient object detection model based on local contrast enhancing and non-local feature learning. This model extracts non-local feature combines with local features under a unified deep learning framework. Besides, a deeply enhanced network is employed as a preprocessing of the low contrast images to assist our saliency detection model. The key idea of this paper is firstly hierarchically introducing a non-local module with local contrast-processing blocks, to provide a detailed and robust representation of saliency information. Then, an encoder-decoder image-enhanced network with full convolution layers is introduced to process the low contrast images for higher contrast and completer structure. As a minor contribution, this paper contributes a new dataset, including 676 low contrast images for testing our model. Extensive experiments have been conducted in the proposed low contrast image dataset to evaluate the performance of our method. Experimental results indicate that the proposed method yields competitive performance compared to existing state-of-the-art models.
C1 [Guo, Tengda; Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.
   [Xu, Xin] Wuhan Univ Sci & Technol, Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan 430065, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University of Science &
   Technology
RP Xu, X (corresponding author), Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.; Xu, X (corresponding author), Wuhan Univ Sci & Technol, Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan 430065, Peoples R China.
EM xuxin0336@gmail.com
RI Xu, Xin/JRW-5800-2023
FU Natural Science Foundation of China [U1803262, 61602349, 61440016]
FX This work was supported by the Natural Science Foundation of China
   (U1803262, 61602349, 61440016).
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen ZH, 2016, MULTIMED TOOLS APPL, V75, P16943, DOI 10.1007/s11042-015-2965-y
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   FU HZ, 2015, PROC CVPR IEEE, P4428, DOI DOI 10.1109/CVPR.2015
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mehrani Paria., 2010, BMVC, P1
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mu N., 2019, IEEE C COMPUT VIS PA, P1
   Mu N., 2017, P 9 INT C MACHINE LE, P314
   Mu N, 2019, PATTERN RECOGN LETT, V125, P124, DOI 10.1016/j.patrec.2019.04.011
   Mu N, 2018, LECT NOTES COMPUT SC, V11166, P35, DOI 10.1007/978-3-030-00764-5_4
   Mu N, 2018, NEURAL COMPUT APPL, V29, P181, DOI 10.1007/s00521-017-2870-6
   Nan Mu, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9226, P295, DOI 10.1007/978-3-319-22186-1_29
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Qi W, 2017, VISUAL COMPUT, V33, P209, DOI 10.1007/s00371-015-1176-x
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Taha AA, 2015, BMC MED IMAGING, V15, DOI 10.1186/s12880-015-0068-x
   Tao L, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao De-Gui, 2014, Journal of Software, V25, P675
   Xu X, 2018, IEEE DECIS CONTR P, P1580, DOI 10.1109/CDC.2018.8619417
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Zeng Y, 2018, PROC CVPR IEEE, P1644, DOI 10.1109/CVPR.2018.00177
   Zhang J, 2016, IEEE T NEUR NET LEAR, V27, P1177, DOI 10.1109/TNNLS.2015.2464316
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Q, 2018, VISUAL COMPUT, V34, P473, DOI 10.1007/s00371-017-1354-0
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhong GY, 2016, VISUAL COMPUT, V32, P611, DOI 10.1007/s00371-015-1077-z
   Zou KH, 2004, ACAD RADIOL, V11, P178, DOI 10.1016/S1076-6332(03)00671-8
NR 63
TC 10
Z9 10
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2069
EP 2081
DI 10.1007/s00371-020-01964-9
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000563736000001
DA 2024-07-18
ER

PT J
AU Lee, J
   Son, H
   Lee, G
   Lee, J
   Cho, S
   Lee, S
AF Lee, Junyong
   Son, Hyeongseok
   Lee, Gunhee
   Lee, Jonghyeop
   Cho, Sunghyun
   Lee, Seungyong
TI Deep color transfer using histogram analogy
SO VISUAL COMPUTER
LA English
DT Article
DE Color transfer; Histogram analogy; Photo-realistic style transfer;
   Recolorization
AB We propose a novel approach to transferring the color of a reference image to a given source image. Although there can be diverse pairs of source and reference images in terms of content and composition similarity, previous methods are not capable of covering the whole diversity. To resolve this limitation, we propose a deep neural network that leveragescolor histogram analogyfor color transfer. A histogram contains essential color information of an image, and our network utilizes the analogy between the source and reference histograms to modulate the color of the source image with abstract color features of the reference image. In our approach, histogram analogy is exploited basically among the whole images, but it can also be applied to semantically corresponding regions in the case that the source and reference images have similar contents with different compositions. Experimental results show that our approach effectively transfers the reference colors to the source images in a variety of settings. We also demonstrate a few applications of our approach, such as palette-based recolorization, color enhancement, and color editing.
C1 [Lee, Junyong; Son, Hyeongseok; Lee, Jonghyeop; Cho, Sunghyun; Lee, Seungyong] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
   [Lee, Gunhee] NCSOFT, Seongnam, South Korea.
C3 Pohang University of Science & Technology (POSTECH)
RP Lee, S (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
EM junyonglee@postech.ac.kr; sonhs@postech.ac.kr; victorleee@ncsoft.com;
   ljh5644@postech.ac.kr; s.cho@postech.ac.kr; leesy@postech.ac.kr
RI Son, Hyeongseok/ACF-8681-2022; Lee, Junyong/AAH-9531-2021
OI Son, Hyeongseok/0000-0002-9525-4040; Lee, Junyong/0000-0001-6472-0582
FU Ministry of Science and ICT, Korea, through IITP Grants
   [IITP-2015-0-00174, IITP-2019- 0-01906]; NRF [NRF-2017M3C4A7066317]
FX This work was supported by the Ministry of Science and ICT, Korea,
   through IITP Grants (SW Star Lab, IITP-2015-0-00174; Artificial
   Intelligence Graduate School Program (POSTECH), IITP-2019- 0-01906) and
   NRF Grant (NRF-2017M3C4A7066317).
CR [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], 2011, 24 IEEE C COMP VIS P
   [Anonymous], 2017, TPAMI
   Arbelot B., 2017, COMPUT GRAPH-UK, V62
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Cho J., 2017, CVPR WORKSH
   Cho Wonwoong, 2019, P CVPR
   Freedman D., 2010, P CVPR
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   He MM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3292482
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hristova H., 2015, P WORKSH COMP AESTH
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Kingma D. P., 2014, arXiv
   Kingma D.P., 2015, INT C MED IM COMP CO
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee JY, 2016, PROC CVPR IEEE, P2470, DOI 10.1109/CVPR.2016.271
   Li C., 2008, P CVPR
   Li Xueting, 2019, P CVPR
   Li Y., 2017, Universal style transfer via feature transforms, P386
   Li Y., 2017, UNIVERSAL STYLE TRAN
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Liao J., 2017, P SIGGRAPH
   Lin S., 2013, ACM T GRAPHIC
   Liu S., 2017, P ADV NEUR INF PROC, V30, P1519
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   O'Donovan P., 2011, P SIGGRAPH
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Tai YW, 2005, PROC CVPR IEEE, P747
   Tsai YH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925942
   Wang Z., 2016, LEARNABLE HISTOGRAM
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
NR 36
TC 15
Z9 16
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2129
EP 2143
DI 10.1007/s00371-020-01921-6
EA AUG 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000559296000001
DA 2024-07-18
ER

PT J
AU Zikas, P
   Papagiannakis, G
   Lydatakis, N
   Kateros, S
   Ntoa, S
   Adami, I
   Stephanidis, C
AF Zikas, Paul
   Papagiannakis, George
   Lydatakis, Nick
   Kateros, Steve
   Ntoa, Stavroula
   Adami, Ilia
   Stephanidis, Constantine
TI Immersive visual scripting based on VR software design patterns for
   experiential training
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Authoring tool; VR training; Visual scripting
ID VIRTUAL-REALITY
AB Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer product, and training on simulators is rapidly becoming standard in many industrial sectors. However, the available systems are either focusing on gaming context, featuring limited capabilities or they support only content creation of virtual environments without any rapid prototyping and modification. In this project, we propose a code-free, visual scripting platform to replicate gamified training scenarios through rapid prototyping and VR software design patterns. We implemented and compared two authoring tools: a) visual scripting and b) VR editor for the rapid reconstruction of VR training scenarios. Our visual scripting module is capable of generating training applications utilizing a node-based scripting system, whereas the VR editor gives user/developer the ability to customize and populate new VR training scenarios directly from the virtual environment. We also introduce action prototypes, a new software design pattern suitable to replicate behavioral tasks for VR experiences. In addition, we present the training scenegraph architecture as the main model to represent training scenarios on a modular, dynamic and highly adaptive acyclic graph based on a structured educational curriculum. Finally, a user-based evaluation of the proposed solution indicated that users-regardless of their programming expertise-can effectively use the tools to create and modify training scenarios in VR.
C1 [Zikas, Paul; Lydatakis, Nick; Kateros, Steve] ORamaVR, Iraklion, Greece.
   [Papagiannakis, George] Univ Crete, Fdn Res & Technol Hellas, Inst Comp Sci, ORamaVR,Dept Comp Sci, Iraklion, Greece.
   [Ntoa, Stavroula; Adami, Ilia] Fdn Res & Technol Hellas, Inst Comp Sci, Iraklion, Greece.
   [Stephanidis, Constantine] Univ Crete, Fdn Res & Technol Hellas, Inst Comp Sci, Dept Comp Sci, Iraklion, Greece.
C3 Foundation for Research & Technology - Hellas (FORTH); University of
   Crete; Foundation for Research & Technology - Hellas (FORTH); Foundation
   for Research & Technology - Hellas (FORTH); University of Crete
RP Zikas, P (corresponding author), ORamaVR, Iraklion, Greece.
EM paul@oramavr.com; george.papagiannakis@oramavr.com; nick@oramavr.com;
   steve@oramavr.com; stant@ics.forth.gr; iadami@ics.forth.gr;
   cs@ics.forth.gr
RI papagiannakis, george/AAI-7973-2020
OI papagiannakis, george/0000-0002-2977-9850; Ntoa,
   Stavroula/0000-0002-6270-8333; Stephanidis,
   Constantine/0000-0003-3687-4220; Zikas, Paul/0000-0003-2422-1169
FU European Union's Horizon 2020 research and innovation programme [871793,
   727585]; Greek national fund (project VRADA); Greek national fund
   (project vipGPU); H2020 Societal Challenges Programme [727585] Funding
   Source: H2020 Societal Challenges Programme; H2020 - Industrial
   Leadership [871793] Funding Source: H2020 - Industrial Leadership
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No 871793
   (ACCORDION) and No727585 (STARS-PCP) and supported by Greek national
   funds (projects VRADA and vipGPU).
CR Alsumait A, 2013, INT J PERVASIVE COMP, V9, P209, DOI 10.1108/IJPCC-07-2013-0016
   Andersen SAW, 2016, JAMA OTOLARYNGOL, V142, P635, DOI 10.1001/jamaoto.2016.0454
   Bouchard S, 2017, BRIT J PSYCHIAT, V210, P276, DOI 10.1192/bjp.bp.116.184234
   Ebrahimi A., 2017, ACM SIGGRAPH 2017 RE, P27, DOI [10.1145/3098333.3098918, DOI 10.1145/3098333.3098918]
   Ellis T. O., 1969, RM5999ARPA RAND CORP
   Ganier F., 2014, ERGONOMICS, V10
   Göbel S, 2008, FOURTH INTERNATIONAL CONFERENCE ON AUTOMATED SOLUTIONS FOR CROSS MEDIA CONTENT AND MULTI-CHANNEL DISTRIBUTION, PROCEEDINGS, P103, DOI 10.1109/AXMEDIS.2008.45
   Greenleaf W, 2016, ACM SIGGRAPH 2016 VR VILLAGE (SIGGRAPH '16), DOI 10.1145/2929490.2956569
   Greenwald ScottW., 2017, Technology and applications for collaborative learning in virtual reality
   Kotis KI, 2019, RES IDEAS OUTCOMES, V5, DOI 10.3897/rio.5.e36464
   Monahan T, 2008, COMPUT EDUC, V50, P1339, DOI 10.1016/j.compedu.2006.12.008
   Murcia-López M, 2018, IEEE T VIS COMPUT GR, V24, P1574, DOI 10.1109/TVCG.2018.2793638
   Nielsen J., 1993, USABILITY ENG, P165, DOI [10.1016/b978-0-08-052029-2.50009-7, DOI 10.1016/B978-0-08-052029-2.50009-7]
   Nystrom R., 2014, Game Programming Patterns, V3rd
   Pan XN, 2018, BRIT J PSYCHOL, V109, P395, DOI 10.1111/bjop.12290
   Papagiannakis P, 2017, MASTER CASE SERIES T, P827
   Pasternak E, 2017, 2017 IEEE BLOCKS AND BEYOND WORKSHOP (B&B), P21, DOI 10.1109/BLOCKS.2017.8120404
   Pfeiffer-Lessmann N, 2018, COMM COM INF SC, V851, P311, DOI 10.1007/978-3-319-92279-9_42
   Roque R., 2008, OPENBLOCKS EXTENDABL
   Slater M, 2017, SMART COMPUT INTELL, P19, DOI 10.1007/978-981-10-5490-7_2
   Stefanidi E, 2019, LECT NOTES COMPUT SC, V11542, P417, DOI 10.1007/978-3-030-22514-8_39
   Tullis T, 2013, MEASURING USER EXPER, P2
   de Faria JWV, 2016, J NEUROSURG, V125, P1105, DOI 10.3171/2015.8.JNS141563
   Villela R, 2019, WORKING CODEDOM, P155
NR 24
TC 11
Z9 12
U1 4
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 1965
EP 1977
DI 10.1007/s00371-020-01919-0
EA AUG 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000557344700002
DA 2024-07-18
ER

PT J
AU Adli, SE
   Shoaran, M
   Noorani, SMS
AF Eivazi Adli, Sahand
   Shoaran, Maryam
   Sayyed Noorani, S. Mohammadreza
TI GSP<i>n</i>P: simple and geometric solution for P<i>n</i>P problem
SO VISUAL COMPUTER
LA English
DT Article
DE Perspective-n-Point; Perspective projection; Pose estimation; Coplanar
   points
ID POSE ESTIMATION; CAMERA POSE; ACCURATE; REALITY
AB Camera pose estimation known as Perspective-n-Point (PnP) problem has essential applications in different fields such as robotics and augmented reality. In this paper, we propose a novel method for PnP problem calledGeometric and Simple PnP(GSPnP) using coplanar feature points. Some characteristics of our proposed algorithm are non-iterativity, simplicity, ease-of-implementation on planer markers, and better accuracy. Our method reaches a very fast solution, beyond any complicated calculations just by relying on the projection geometry. We compare our proposed method with the available methods in solvePnP function of OpenCV library using AR.Drone 2.0 quadcopter simulation in Gazebo world and ArUco markers with the help of ROS. Moreover, we practically make some experiments using a real AR.Drone 2.0 quadcopter fitted in the table of a milling machine. The results show that GSPnP method is the fastest method (specifically, 13 times in simulation case and five times in experimental case faster than the current fastest method) with almost the same accuracy and even better results in some cases (e.g., a higher accuracy in the camera's height estimation) compared to the other methods.
C1 [Eivazi Adli, Sahand; Shoaran, Maryam; Sayyed Noorani, S. Mohammadreza] Univ Tabriz, Sch Engn Emerging Technol, Dept Mechatron Engn, Tabriz, Iran.
C3 University of Tabriz
RP Shoaran, M (corresponding author), Univ Tabriz, Sch Engn Emerging Technol, Dept Mechatron Engn, Tabriz, Iran.
EM sahandlprs@gmail.com; mshoaran@tabrizu.ac.ir; smrs.noorani@tabrizu.ac.ir
OI Eivazi Adli, Sahand/0009-0004-8489-6947; Shoaran,
   Maryam/0000-0002-4105-0835
CR [Anonymous], P BRIT MACH VIS C BM
   Ansar A, 2003, IEEE T PATTERN ANAL, V25, P578, DOI 10.1109/TPAMI.2003.1195992
   Bacik J, 2017, INTEL SERV ROBOT, V10, P185, DOI 10.1007/s11370-017-0219-8
   Carreira TiagoGomes., 2013, Quadcopter Automatic Landing on a Docking Station
   Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71
   Fiore PD, 2001, IEEE T PATTERN ANAL, V23, P140, DOI 10.1109/34.908965
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Garro V, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P262, DOI 10.1109/3DIMPVT.2012.40
   Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266
   Kneip L, 2011, PROC CVPR IEEE
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41
   Liu YL, 2018, VISUAL COMPUT, V34, P899, DOI 10.1007/s00371-018-1523-9
   Lu CP, 2000, IEEE T PATTERN ANAL, V22, P610, DOI 10.1109/34.862199
   Penate-Sanchez A, 2013, IEEE T PATTERN ANAL, V35, P2387, DOI 10.1109/TPAMI.2013.36
   Pi C.-H., 2016, P 4 IIAE INT C INT S
   Reif R, 2008, VISUAL COMPUT, V24, P987, DOI 10.1007/s00371-008-0271-7
   Roweis S.T., LEVENBERG MARQUARDT
   Siltanen S, 2017, VISUAL COMPUT, V33, P193, DOI 10.1007/s00371-015-1174-z
   Wang GH, 2007, LECT NOTES COMPUT SC, V4844, P363
   Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291
   Zheng YQ, 2013, IEICE T INF SYST, VE96D, P1525, DOI 10.1587/transinf.E96.D.1525
NR 22
TC 3
Z9 3
U1 3
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1549
EP 1557
DI 10.1007/s00371-019-01747-x
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000549620700003
DA 2024-07-18
ER

PT J
AU Yuan, ZH
   Su, QT
   Liu, DC
   Zhang, XT
AF Yuan, Zihan
   Su, Qingtang
   Liu, Decheng
   Zhang, Xueting
TI A blind image watermarking scheme combining spatial domain and frequency
   domain
SO VISUAL COMPUTER
LA English
DT Article
DE DC component; Variable steps; Blind watermarking; High performance;
   Spatial domain
ID DCT; DECOMPOSITION; TRANSFORM
AB In order to realize the copyright protection of color image effectively, combining the advantages of spatial-domain watermarking scheme and frequency-domain one, a blind color image watermarking scheme with high performance in the spatial domain is proposed in the paper. The presented scheme does not require real discrete cosine transform (DCT) and discrete Hartley transform (DHT), but only uses the different quantization steps to complete the embedding and blind extracting of color watermark in the spatial domain according to the unique characteristic of direct current (DC) components of DCT and DHT. The contributions of this paper include the following: (1) This scheme combined the strengths of watermarking scheme in the spatial domain and frequency domain, which has fast speed and strong robustness; (2) the scheme makes full use of the energy aggregation characteristics of image block, and the invisibility of the watermarking scheme has greatly improved; and (3) different quantization steps are chosen to embed and extract watermark in different layers, which reduce the modification range of pixel value effectively. The experimental results show that compared with the existing schemes, the proposed watermarking scheme has higher performance, such as better invisibility, stronger robustness and shorter execution time.
C1 [Yuan, Zihan; Su, Qingtang; Liu, Decheng; Zhang, Xueting] Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
C3 Ludong University
RP Su, QT (corresponding author), Ludong Univ, Sch Informat & Elect Engn, Yantai 264025, Peoples R China.
EM sdytsqt@163.com
FU National Natural Science Foundations of China [61771231, 61772253,
   61873117, 61872170]; Key Research and Development Program of Shandong
   Province [2019GGX101025, 2019GGX101032]
FX The work was supported by the National Natural Science Foundations of
   China (No. 61771231, 61772253, 61873117 and 61872170) and the Key
   Research and Development Program of Shandong Province (No.
   2019GGX101025, 2019GGX101032).
CR Abdulrahman AK, 2019, MULTIMED TOOLS APPL, V78, P17027, DOI 10.1007/s11042-018-7085-z
   Abraham J, 2019, J KING SAUD UNIV-COM, V31, P125, DOI 10.1016/j.jksuci.2016.12.004
   Araghi TK, 2018, EXPERT SYST APPL, V112, P208, DOI 10.1016/j.eswa.2018.06.024
   Ahmadi SBB, 2021, VISUAL COMPUT, V37, P385, DOI 10.1007/s00371-020-01808-6
   Cao LJ, 2013, VISUAL COMPUT, V29, P231, DOI 10.1007/s00371-012-0732-x
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Ernawan F, 2018, IEEE ACCESS, V6, P20464, DOI 10.1109/ACCESS.2018.2819424
   Golea N., 2010, IEEE INT C COMPUTER, P1
   González ER, 2002, ORG DIVERS EVOL, V2
   Hamidi M, 2018, MULTIMED TOOLS APPL, V77, P27181, DOI 10.1007/s11042-018-5913-9
   Hsu LY, 2017, J VIS COMMUN IMAGE R, V46, P33, DOI 10.1016/j.jvcir.2017.03.009
   Jagadeesh B, 2015, PROCEDIA COMPUT SCI, V46, P1618, DOI 10.1016/j.procs.2015.02.095
   Li JZ, 2018, MULTIMED TOOLS APPL, V77, P4545, DOI 10.1007/s11042-017-4452-0
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Luo AW, 2020, MULTIMED TOOLS APPL, V79, P243, DOI 10.1007/s11042-019-08074-2
   Moosazadeh M, 2019, J INF SECUR APPL, V47, P28, DOI 10.1016/j.jisa.2019.04.001
   Moosazadeh M, 2017, OPTIK, V140, P975, DOI 10.1016/j.ijleo.2017.05.011
   Rasti P, 2016, J VIS COMMUN IMAGE R, V38, P838, DOI 10.1016/j.jvcir.2016.05.001
   Sangwine S.J., 1998, The Colour Image Processing Handbook, DOI [10.1007/978-1-4615-5779-1, DOI 10.1007/978-1-4615-5779-1]
   Su QT, 2019, IEEE ACCESS, V7, P30398, DOI 10.1109/ACCESS.2019.2895062
   Su QT, 2019, IEEE ACCESS, V7, P4358, DOI 10.1109/ACCESS.2018.2888857
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Su QT, 2016, IET IMAGE PROCESS, V10, P817, DOI 10.1049/iet-ipr.2016.0048
   Su QT, 2015, SIGNAL IMAGE VIDEO P, V9, P991, DOI 10.1007/s11760-013-0534-2
   Su QT, 2013, OPTIK, V124, P6255, DOI 10.1016/j.ijleo.2013.05.013
   Su QT, 2014, SIGNAL PROCESS, V94, P219, DOI 10.1016/j.sigpro.2013.06.025
   University of Granada Computer vision group, CVG UGR IM DAT
   University of Southern California Signal and Inage Processing Institute n, USC SIPI IM DAT
   Ying QC, 2019, MATH BIOSCI ENG, V16, P4788, DOI 10.3934/mbe.2019241
   Zhang FY, 2019, MULTIMED TOOLS APPL, V78, P20133, DOI 10.1007/s11042-019-7326-9
NR 30
TC 31
Z9 31
U1 1
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1867
EP 1881
DI 10.1007/s00371-020-01945-y
EA JUL 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000557083300001
DA 2024-07-18
ER

PT J
AU Yatagawa, T
   Yamaguchi, Y
   Morishima, S
AF Yatagawa, Tatsuya
   Yamaguchi, Yasushi
   Morishima, Shigeo
TI LinSSS: linear decomposition of heterogeneous subsurface scattering for
   real-time screen-space rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Subsurface scattering; Real-time rendering; Reflectance modeling
ID DIFFUSION; REPRESENTATION; MODEL
AB Screen-space subsurface scattering is currently the most common approach to represent translucent materials in real-time rendering. However, most of the current approaches approximate the diffuse reflectance profile of translucent materials as a symmetric function, whereas the profile has an asymmetric shape in nature. To address this problem, we propose LinSSS, a numerical representation of heterogeneous subsurface scattering for real-time screen-space rendering. Although our representation is built upon a previous method, it makes two contributions. First, LinSSS formulates the diffuse reflectance profile as a linear combination of radially symmetric Gaussian functions. Nevertheless, it can also represent the spatial variation and the radial asymmetry of the profile. Second, since LinSSS is formulated using only the Gaussian functions, the convolution of the diffuse reflectance profile can be efficiently calculated in screen space. To further improve the efficiency, we deform the rendering equation obtained using LinSSS by factoring common convolution terms and approximate the convolution processes using a MIP map. Consequently, our method works as fast as the state-of-the-art method, while our method successfully represents the heterogeneity of scattering.
C1 [Yatagawa, Tatsuya] Univ Tokyo, Sch Engn, Bunkyo Ku, Tokyo, Japan.
   [Yamaguchi, Yasushi] Univ Tokyo, Grad Sch Arts & Sci, Meguro Ku, Tokyo, Japan.
   [Morishima, Shigeo] Waseda Univ, Grad Sch Adv Sci & Engn, Shinjuku Ku, Tokyo, Japan.
C3 University of Tokyo; University of Tokyo; Waseda University
RP Yatagawa, T (corresponding author), Univ Tokyo, Sch Engn, Bunkyo Ku, Tokyo, Japan.
EM tatsy@acm.org; yama@graw.c.u-tokyo.ac.jp; shigeo@waseda.jp
RI Yatagawa, Tatsuya/HKV-3976-2023; Yamaguchi, Yasushi/S-5779-2019
OI Yatagawa, Tatsuya/0000-0003-4653-2435; Morishima,
   Shigeo/0000-0001-8859-6539; YAMAGUCHI, Yasushi/0000-0003-0790-4144
FU JSPS KAKENHI [JP18K18075, JP20H04203, JP17H06101, JP19H01129]; JST ACCEL
   [JPMJAC1602]; JST Mirai Project [JPMJMI19B2]; Waseda Institute of
   Advanced Science and Engineering; Grants-in-Aid for Scientific Research
   [20H04203, 20K21784] Funding Source: KAKEN
FX This project was jointly supported by JSPS KAKENHI (JP18K18075,
   JP20H04203, JP17H06101, and JP19H01129), JST ACCEL (JPMJAC1602), JST
   Mirai Project (JPMJMI19B2), and a Grant-in-Aid from theWaseda Institute
   of Advanced Science and Engineering.
CR [Anonymous], 1993, ACM C COMP GRAPH INT
   Arbree A, 2011, IEEE T VIS COMPUT GR, V17, P956, DOI 10.1109/TVCG.2010.117
   Chandrasekhar S., 1950, RAD TRANSFER
   Chen GJ, 2012, VISUAL COMPUT, V28, P701, DOI 10.1007/s00371-012-0704-1
   d'Eon E, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964951
   Dachsbacher C., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P197
   Dachsbacher C., 2005, Proc. Symp. Interactive Graph. and Games, P203, DOI DOI 10.1145/1053427.1053460
   dEon E., 2007, P EUR S REND TECHN, P147
   Donner C, 2005, ACM T GRAPHIC, V24, P1032, DOI 10.1145/1073204.1073308
   Donner C., 2007, P 18 EUR C REND TECH, P243
   Donner Craig, 2008, ACM T GRAPHIC, V27
   Elek O, 2018, P EUR WORKSH MAT APP, DOI [10.2312/mam.20181200, DOI 10.2312/MAM.20181200]
   Elek O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130890
   Elek O, 2013, IEEE COMPUT GRAPH, V33, P53, DOI 10.1109/MCG.2013.17
   Frederickx R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073681
   Frisvad JR, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682629
   Goesele M, 2004, ACM T GRAPHIC, V23, P835, DOI 10.1145/1015706.1015807
   Habel R, 2013, COMPUT GRAPH FORUM, V32, P27, DOI 10.1111/cgf.12148
   Hable J., 2009, SHADER X7, V7, P161
   Jakob Wenzel, 2010, Mitsuba renderer
   Jensen HW, 2001, COMP GRAPH, P511, DOI 10.1145/383259.383319
   Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619
   Jimenez J., 2010, GPU PRO ADV RENDERIN, P335
   Jimenez J, 2008, P SPAN COMP GRAPH C
   Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P453, DOI 10.1109/TVCG.2008.106
   Lensch HPA, 2003, COMPUT GRAPH FORUM, V22, P195, DOI 10.1111/1467-8659.00660
   LU F, 2009, ACM T GRAPHIC, V26
   Mertens T., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P130
   Mertens T, 2005, COMPUT GRAPH FORUM, V24, P41, DOI 10.1111/j.1467-8659.2005.00827.x
   Munoz A, 2011, COMPUT GRAPH FORUM, V30, P455, DOI 10.1111/j.1467-8659.2011.01873.x
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Peers P, 2006, ACM T GRAPHIC, V25, P746, DOI 10.1145/1141911.1141950
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Shah MA, 2009, IEEE COMPUT GRAPH, V29, P66, DOI 10.1109/MCG.2009.11
   Sone H, 2017, EUROGRAPH SHORT PAP, DOI [10.2312/egsh.20171018, DOI 10.2312/EGSH.20171018]
   Song Y., 2013, Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, P1, DOI DOI 10.1109/APSIPA.2013.6694333
   Stam J, 1995, SPRING COMP SCI, P41
   Wang J, 2009, L N INST COMP SCI SO, V10, P1, DOI 10.1080/07357900903179617
NR 39
TC 1
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 1979
EP 1992
DI 10.1007/s00371-020-01915-4
EA JUL 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000554044000003
DA 2024-07-18
ER

PT J
AU Sun, YJ
   Xiong, H
   Yiu, SM
AF Sun, Yujing
   Xiong, Hao
   Yiu, Siu Ming
TI Understanding deep face anti-spoofing: from the perspective of data
SO VISUAL COMPUTER
LA English
DT Article
DE Face anti-spoofing; Biometrics; Image adjustment; Image processing
ID LIVENESS DETECTION; IMAGE; RECOGNITION
AB Face biometrics systems are increasingly used by many business applications, which can be vulnerable to malicious attacks, leading to serious consequences. How to effectively detect spoofing faces is a critical problem. Traditional methods rely on handcraft features to distinguish real faces from fraud ones, but it is difficult for feature descriptors to handle all attack variations. More recently, in order to overcome the limitation of traditional methods, newly emerging CNN-based approaches were proposed, most of which, if not all, carefully design different network architectures. To make CNN-related approaches effective, data and learning strategies are both indispensable. In this paper, instead of focusing on network design, we explore more from the perspective of data. We present that appropriate nonlinear adjustment and hair geometry can amplify the contrast between real faces and attacks. Given our exploration, a simple convolutional neural network can solve the face anti-spoofing problem under different attack scenarios and achieve state-of-the-art performance on well-known face anti-spoofing benchmarks.
C1 [Sun, Yujing; Xiong, Hao; Yiu, Siu Ming] Univ Hong Kong, Dept Comp Sci, Pok Fu Lam, Hong Kong, Peoples R China.
C3 University of Hong Kong
RP Sun, YJ (corresponding author), Univ Hong Kong, Dept Comp Sci, Pok Fu Lam, Hong Kong, Peoples R China.
EM yjsun@cs.hku.hk; hxiong@hku.hk; smyiu@cs.hku.hk
RI Yiu, Siu Ming/C-1843-2009
OI Yiu, Siu Ming/0000-0002-3975-8500; Sun, Yujing/0000-0003-0819-296X
FU Hong Kong Government
FX Prof. Xiu Ming Yiu has received research Grants from the Hong Kong
   Government.
CR [Anonymous], 2014, ABS14085601 CORR
   [Anonymous], 2010, IEEE INT C FUZZY SYS
   [Anonymous], 2013, P INT C BIOM ICB JUN
   [Anonymous], 2013, I W BIOMETRIC FORENS
   [Anonymous], 2013, BIOMETRICS
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Bharadwaj S, 2013, IEEE COMPUT SOC CONF, P105, DOI 10.1109/CVPRW.2013.23
   BOULKENAFET Z, 2017, OULU NPU MOBILE FACE
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112
   Chan PPK, 2018, IEEE T INF FOREN SEC, V13, P521, DOI 10.1109/TIFS.2017.2758748
   Chetty G, 2006, 2006 BIOMETRICS SYMPOSIUM: SPECIAL SESSION ON RESEARCH AT THE BIOMETRIC CONSORTIUM CONFERENCE, P13
   Chetty G, 2009, INT J BIOMETRICS, V1, P463, DOI 10.1504/IJBM.2009.027306
   Chetty G, 2009, FUSION: 2009 12TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P2255
   Chingovska Ivana, 2012, BIOSIG
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Gan J., 2017, INT C TRANSPARENT OP, P1, DOI [DOI 10.1109/ICTON.2017.8024849, 10.1109/ICTON.2017.8024849]
   Garcia DC, 2015, IEEE T INF FOREN SEC, V10, P778, DOI 10.1109/TIFS.2015.2411394
   Jourabloo A., 2018, P EUR C COMP VIS ECC, P290
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Kim S, 2014, SENSORS-BASEL, V14, P22471, DOI 10.3390/s141222471
   Kingma D. P., 2014, arXiv
   Komulainen Jukka, 2013, 2013 IEEE 6 INT C BI
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Li LF, 2016, CRYSTALS, V6, DOI 10.3390/cryst6040045
   Li Y., 2014, P 9 ACM S INFORM COM, P413
   Liu P, 2014, J DIGIT IMAGING, V27, P12, DOI 10.1007/s10278-013-9636-1
   Liu Y., 2018, ARXIV180311097
   Maatta J., 2011, 2011 INT JOINT C BIO, P1
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Peixoto Bruno, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3557, DOI 10.1109/ICIP.2011.6116484
   Pereira TD, 2013, INT CONF BIOMETR
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Pinto A, 2015, IEEE T INF FOREN SEC, V10, P1025, DOI 10.1109/TIFS.2015.2395139
   Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16
   Schlick C., 1994, GRAPHICS GEMS, P401
   Siddiqui TA, 2016, INT C PATT RECOG, P1035, DOI 10.1109/ICPR.2016.7899772
   Socolinsky DA, 2003, COMPUT VIS IMAGE UND, V91, P72, DOI 10.1016/S1077-3142(03)00075-4
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Tao HY, 2019, INT GEOSCI REMOTE SE, P3468, DOI 10.1109/igarss.2019.8899067
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Wang T. M., 2013, ADV DIFFER EQU-NY, V2013, P1, DOI DOI 10.1371/J0URNAL.P0NE.0058952
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang J, 2013, SCI WORLD J, DOI 10.1155/2013/812469
   Zhang Z, 2012, J NANOMATER, V2012, DOI 10.1155/2012/238605
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
NR 53
TC 10
Z9 10
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1015
EP 1028
DI 10.1007/s00371-020-01849-x
EA MAY 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000533063400001
DA 2024-07-18
ER

PT J
AU Ayadi, M
   Scuturici, M
   Ben Amar, C
   Miguet, S
AF Ayadi, Mehdi
   Scuturici, Mihaela
   Ben Amar, Chokri
   Miguet, Serge
TI A skyline-based approach for mobile augmented reality
SO VISUAL COMPUTER
LA English
DT Article
DE Mobile augmented reality; Image to geometry registration; 3D city
   models; Skyline matching; Urban landscape
ID REGISTRATION; LOCALIZATION
AB This paper presents a skyline-based approach to enhance the visualization of a new construction project in augmented reality. We propose to process the video stream acquired with a mobile phone to register the real buildings with a 3D city model. We first combine the data acquired with the device's instruments to estimate a rough user's pose in the world coordinates system. Then, we use this estimated pose to generate a synthetic image of the user's view from which we calculate a virtual skyline. In parallel, we extract a real skyline from the real-time video stream. Finally, we match these real and virtual skylines to correct the user's pose (six degrees of freedom) and thus generate a more realistic augmented reality view. We evaluate the precision and the processing time of our approach using 2D and 3D registration algorithms, as well as with a novel double 2D strategy.
C1 [Ayadi, Mehdi; Scuturici, Mihaela; Miguet, Serge] Univ Lumiere Lyon 2, Univ Lyon, CNRS, LIRIS,UMR 5205, F-69676 Bron, France.
   [Ayadi, Mehdi; Ben Amar, Chokri] Univ Sfax, ENIS, REGIM Lab REsearch Grp Intelligent Machines, BP 1173, Sfax 3038, Tunisia.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon;
   Universite Lyon 2; Centre National de la Recherche Scientifique (CNRS);
   Universite de Sfax; Ecole Nationale dIngenieurs de Sfax (ENIS)
RP Ayadi, M (corresponding author), Univ Lumiere Lyon 2, Univ Lyon, CNRS, LIRIS,UMR 5205, F-69676 Bron, France.; Ayadi, M (corresponding author), Univ Sfax, ENIS, REGIM Lab REsearch Grp Intelligent Machines, BP 1173, Sfax 3038, Tunisia.
EM mehdi.ayadi@univ-lyon2.fr; mihaela.scuturici@univ-lyon2.fr;
   chokri.benamar@ieee.org; serge.miguet@univ-lyon2.fr
RI Chokri, BEN AMAR/K-5237-2012; Cherifi, Hocine/X-9376-2019; Ayadi,
   Mehdi/O-3313-2014
OI Cherifi, Hocine/0000-0001-9124-4921; 
CR [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2012, Mastering OpenCV With Practical Computer VisionProjects
   Ayadi M, 2018, ADV VISUAL COMPUTING
   Ayadi M, 2016, LECT NOTES COMPUT SC, V10016, P604, DOI 10.1007/978-3-319-48680-2_53
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Baatz G, 2012, LECT NOTES COMPUT SC, V7573, P517, DOI 10.1007/978-3-642-33709-3_37
   Behzadan AH, 2008, ADV ENG INFORM, V22, P90, DOI 10.1016/j.aei.2007.08.005
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carozza L, 2014, COMPUT-AIDED CIV INF, V29, P2, DOI 10.1111/j.1467-8667.2012.00798.x
   Cirulis A, 2013, PROCEDIA COMPUT SCI, V25, P71, DOI 10.1016/j.procs.2013.11.009
   Fukuda T, 2014, FRONT ARCHIT RES, V3, P386, DOI 10.1016/j.foar.2014.08.003
   Gaillard J, 2015, WEB3D 2015, P81, DOI 10.1145/2775292.2775302
   Ghadirian P, 2008, LANDSCAPE URBAN PLAN, V86, P226, DOI 10.1016/j.landurbplan.2008.03.004
   Guislain M, 2017, COMPUT VIS IMAGE UND, V157, P90, DOI 10.1016/j.cviu.2016.12.004
   Hart PE, 2009, IEEE SIGNAL PROC MAG, V26, P18, DOI 10.1109/MSP.2009.934181
   Johns D., 2006, 3 CANADIAN C COMPUTE, P22, DOI DOI 10.1109/CRV.2006.81
   Kirchbach Kim, 2013, 2013 17th International Conference on Information Visualisation, P398, DOI 10.1109/IV.2013.52
   LANGE E, 1994, LANDSCAPE URBAN PLAN, V30, P99, DOI 10.1016/0169-2046(94)90070-1
   Luo JB, 2002, IEEE T IMAGE PROCESS, V11, P201, DOI 10.1109/83.988954
   Meguro J., 2007, IEEE ASME INT C ADV, P1
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Nielsen Rune, 2005, COMP URB PLANN URB M
   Ramalingam Srikumar, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P23, DOI 10.1109/ICCVW.2009.5457723
   Ramalingam S, 2010, IEEE INT C INT ROBOT, P3816, DOI 10.1109/IROS.2010.5649105
   Reitmayr G., 2007, Mixed and Augmented Reality, P161
   Sassi A., 2019, INT J INTELL SYST AP, V4, P14, DOI DOI 10.5815/IJISA.2019.04.02
   Saurer O, 2016, INT J COMPUT VISION, V116, P213, DOI 10.1007/s11263-015-0830-0
   Schall G, 2009, INT SYM MIX AUGMENT, P153, DOI 10.1109/ISMAR.2009.5336489
   Ventura J, 2012, INT SYM MIX AUGMENT, P3, DOI 10.1109/ISMAR.2012.6402531
   Yabuki N, 2012, LECT NOTES COMPUT SC, V7467, P227, DOI 10.1007/978-3-642-32609-7_32
   Yabuki N, 2011, AUTOMAT CONSTR, V20, P228, DOI 10.1016/j.autcon.2010.08.003
   Zhu S, 2013, IEEE IMAGE PROC, P3632, DOI 10.1109/ICIP.2013.6738749
NR 32
TC 3
Z9 3
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 789
EP 804
DI 10.1007/s00371-020-01830-8
EA MAR 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000518343100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Buonamici, F
   Furferi, R
   Governi, L
   Lazzeri, S
   McGreevy, KS
   Servi, M
   Talanti, E
   Uccheddu, F
   Volpe, Y
AF Buonamici, Francesco
   Furferi, Rocco
   Governi, Lapo
   Lazzeri, Simone
   McGreevy, Kathleen S.
   Servi, Michaela
   Talanti, Emiliano
   Uccheddu, Francesca
   Volpe, Yary
TI A practical methodology for computer-aided design of custom 3D printable
   casts for wrist fractures
SO VISUAL COMPUTER
LA English
DT Article
DE CAD; Reverse engineering; Orthosis modelling; Cast modelling;
   Personalized medicine
AB In recent years, breakthroughs in the fields of reverse engineering and additive manufacturing techniques have led to the development of innovative solutions for personalized medicine. 3D technologies are quickly becoming a new treatment concept that hinges on the ability to shape patient-specific devices. Among the wide spectrum of medical applications, the orthopaedic sector is experiencing the most benefits. Several studies proposed modelling procedures for patient-specific 3D-printed casts for wrist orthoses, for example. Unfortunately, the proposed approaches are not ready to be used directly in clinical practice since the design of these devices requires significant interaction among medical staff, reverse engineering experts, additive manufacturing specialists and CAD designers. This paper proposes a new practical methodology to produce 3D printable casts for wrist immobilization with the aim of overcoming these drawbacks. In particular, the idea is to realize an exhaustive system that can be used within a paediatric environment. It should provide both a fast and accurate dedicated scanning of the hand-wrist-arm district, along with a series of easy-to-use semi-automatic tools for the modelling of the medical device. The system was designed to be used directly by the clinical staff after a brief training. It was tested on a set of five case studies with the aim of proving its general reliability and identifying possible major flaws. Casts obtained using the proposed system were manufactured using a commercial 3D printer, and the device's compliance with medical requirements was tested. Results showed that the designed casts were correctly generated by the medical staff without the need of involving engineers. Moreover, positive feedback was provided by the users involved in the experiment.
C1 [Buonamici, Francesco; Furferi, Rocco; Governi, Lapo; Servi, Michaela; Uccheddu, Francesca; Volpe, Yary] Univ Florence, DIEF, Via Santa Marta 3, I-50139 Florence, Italy.
   [Lazzeri, Simone; McGreevy, Kathleen S.; Talanti, Emiliano] Childrens Hosp A Meyer Florence, Florence, Italy.
C3 University of Florence
RP Volpe, Y (corresponding author), Univ Florence, DIEF, Via Santa Marta 3, I-50139 Florence, Italy.
EM francesco.buonamici@unifi.it; rocco.furferi@unifi.it;
   lapo.governi@unifi.it; simone.lazzeri@meyer.it;
   kathleen.mcgreevy@meyer.it; michaela.servi@unifi.it;
   emiliano.talanti@meyer.it; francesca.uccheddu@unifi.it;
   yary.volpe@unifi.it
RI Buonamici, Francesco/AAC-1863-2020; Lazzeri, Simone/GQI-3662-2022;
   Governi, Lapo/D-7150-2012; Furferi, Rocco/C-6660-2008
OI Buonamici, Francesco/0000-0001-5186-9724; Volpe,
   Yary/0000-0002-5668-1912; Lazzeri, Simone/0000-0002-5473-6510
FU Fondazione Ospedale Pediatrico Meyer Onlus
FX The authors wish to acknowledge the valuable contribution of Gianmaria
   Viciconte in providing useful hints for processing 3D data. The authors
   also wish to thank the Fondazione Ospedale Pediatrico Meyer Onlus
   (http://www.fondazionemeyer.it/) for funding the T3DDY lab (Personalized
   paediatrics by inTegrating 3D aDvanced technologY), which originated and
   made possible this research.
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Ata STR, ABS M30
   Boyd AS, 2009, AM FAM PHYSICIAN, V79, P16
   Brackett D., 2011, P INT SOL FREEF FABR, P348, DOI DOI 10.1017/CBO9781107415324.004
   Carfagni M, 2017, PROCEDIA MANUF, V11, P1600, DOI 10.1016/j.promfg.2017.07.306
   Carfagni M, 2017, IEEE SENS J, V17, P4508, DOI 10.1109/JSEN.2017.2703829
   Chen Yan-Jun, 2017, 3D Print Med, V3, P11, DOI 10.1186/s41205-017-0019-y
   Chudnofsky C.R., 2004, Clinical Procedures in Emergency Medicine
   Crivellaro A, 2015, IEEE I CONF COMP VIS, P4391, DOI 10.1109/ICCV.2015.499
   Davidson S., 2015, GRASSHOPPER ALGORITH
   Dhall A., 2017, COMMUNICATION
   Douglas L, 2011, PUBLIC SPACES, PRIVATE GARDENS: A HISTORY OF DESIGNED LANDSCAPES IN NEW ORLEANS, P15
   Eltorai AEM, 2015, ORTHOPEDICS, V38, P684, DOI 10.3928/01477447-20151016-05
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Gordon C, ANTHROPOMETRIC DATA
   Guo H, 2016, VISUAL COMPUT, V32, P1511, DOI 10.1007/s00371-015-1136-5
   Joneja A, 2003, P ASME ENG TECH C, V3, P363
   Kim H, 2015, J MECH SCI TECHNOL, V29, P5151, DOI 10.1007/s12206-015-1115-9
   Lin Hui, 2015, 3D Print Med, V2, P4, DOI 10.1186/s41205-016-0007-7
   López-Fernández D, 2016, J VIS COMMUN IMAGE R, V38, P396, DOI 10.1016/j.jvcir.2016.03.020
   McNeel Robert., GRASSHOPPER ALGORITH
   Mesuda Y, 2018, COMPUT IND, V95, P93, DOI 10.1016/j.compind.2017.11.004
   Morhart M., ETIOLOGY
   Mulford JS, 2016, ANZ J SURG, V86, P648, DOI 10.1111/ans.13533
   Munoz-Salinas Rafael, 2012, Aruco: a minimal library for augmented reality applications based on opencv
   Palousek D, 2014, RAPID PROTOTYPING J, V20, P27, DOI 10.1108/RPJ-03-2012-0027
   Park S, 2006, VISUAL COMPUT, V22, P168, DOI 10.1007/s00371-006-0374-y
   Qt Company, 2016, QT DEV DEV CROSS PLA
   Schmidt B, 2014, ROBOT CIM-INT MANUF, V30, P678, DOI 10.1016/j.rcim.2013.11.004
   Schroeder W., 2006, KITWARE
   Siemens, 2018, SIEMENS DOCUMENTATIO
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Zanuttigh P., 2016, Time-of-Flight and Structured Light Depth Cameras, P43
   Zhang XT, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P243, DOI 10.1145/3126594.3126600
   Zhong Y, 2011, WOODHEAD PUBL SER TE, P69
NR 35
TC 30
Z9 31
U1 0
U2 29
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 375
EP 390
DI 10.1007/s00371-018-01624-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300011
DA 2024-07-18
ER

PT J
AU Krishnakumar, K
   Gandhi, SI
AF Krishnakumar, K.
   Gandhi, S. Indira
TI Video stitching based on multi-view spatiotemporal feature points and
   grid-based matching
SO VISUAL COMPUTER
LA English
DT Article
DE Video stitching; Video registration; Spatiotemporal; Image stitching
ID IMAGE; COLOR
AB Video stitching enables creation of videos with a wide field of view from videos captured from ordinary, probably moderately displaced, cameras. In this paper, a novel video stitching algorithm based on multi-view feature detection and registration using region-based statistics has been proposed. While stitching the videos captured using moving cameras, the motion in those cameras will affect the alignment of video frames. Movements in the cameras and in the objects will affect the temporal consistency while stitching the videos. Temporally reliable feature points are considered to maintain both spatial and temporal consistency using multi-view spatiotemporal feature points in this paper. Experiments have been carried out on different data sets, and the results showed the better and effective performance of the proposed approach.
C1 [Krishnakumar, K.; Gandhi, S. Indira] Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai 600044, Tamil Nadu, India.
C3 Anna University; Madras Institute of Technology; Anna University Chennai
RP Krishnakumar, K (corresponding author), Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai 600044, Tamil Nadu, India.
EM kunakrishnakumar@gmail.com
RI S, Indira Gandhi/AAC-4330-2022
OI , Krishnakumar K/0000-0003-2443-1553
CR Andersen D, 2016, VISUAL COMPUT, V32, P1481, DOI 10.1007/s00371-015-1135-6
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chakraborty B, 2012, COMPUT VIS IMAGE UND, V116, P396, DOI 10.1016/j.cviu.2011.09.010
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Eden A., 2006, P 2006 IEEE COMP SOC, VVolume 2, P2498
   Guo H, 2016, IEEE T IMAGE PROCESS, V25, P5491, DOI 10.1109/TIP.2016.2607419
   Harris C., 1988, ALVEY VISION C, P147151
   Jia JY, 2008, IEEE T PATTERN ANAL, V30, P617, DOI 10.1109/TPAMI.2007.70729
   Jia JY, 2005, IEEE I CONF COMP VIS, P1651
   Jiang W, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301374
   Kim BS, 2017, IEEE T CONSUM ELECTR, V63, P109, DOI 10.1109/TCE.2017.014841
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lei YQ, 2012, IEEE T CIRC SYST VID, V22, P1332, DOI 10.1109/TCSVT.2012.2201670
   Levieuge G, 2017, APPL ECON, V49, P823, DOI 10.1080/00036846.2016.1208350
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li J, 2015, IEEE T CYBERNETICS, V45, P2707, DOI 10.1109/TCYB.2014.2381774
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Nie YW, 2018, IEEE T IMAGE PROCESS, V27, P164, DOI 10.1109/TIP.2017.2736603
   Oikonomopoulos A, 2011, IEEE T IMAGE PROCESS, V20, P1126, DOI 10.1109/TIP.2010.2076821
   Perazzi F, 2015, COMPUT GRAPH FORUM, V34, P57, DOI 10.1111/cgf.12541
   Ruby R, 2008, WMUNEP'08 : PROCEEDINGS OF THE FOURTH ACM INTERNATIONAL WORKSHOP ON WIRELESS MULTIMEDIA NETWORKING AND PERFORMANCE MODELING, P1
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Tomasi C, 1991, DETECTION TRACKING P
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wong S, 2007, 2007 INTERNATIONAL SYMPOSIUM ON VLSI TECHNOLOGY, SYSTEMS AND APPLICATIONS (VLSI-TSA), PROCEEDINGS OF TECHNICAL PAPERS, P66
   Xu W, 2010, PROC CVPR IEEE, P263, DOI 10.1109/CVPR.2010.5540202
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
NR 32
TC 8
Z9 8
U1 2
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1837
EP 1846
DI 10.1007/s00371-019-01780-w
EA DEC 2019
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000541644200001
DA 2024-07-18
ER

PT J
AU Yang, X
   Zhu, SY
   Xia, SJ
   Zhou, DK
AF Yang, Xin
   Zhu, Songyan
   Xia, Sijun
   Zhou, Dake
TI A new TLD target tracking method based on improved correlation filter
   and adaptive scale
SO VISUAL COMPUTER
LA English
DT Article
DE Target tracking; TLD; SRDCF; Correlation filter
ID OBJECT TRACKING
AB Target tracking is a popular but challenging problem in computer vision field. Due to many disturbing factors such as position transformation, illumination, and occlusion, it is difficult to achieve continuous target tracking. On the basis of the above analyses, a novel target tracking method based on correlation filters is proposed in this paper. This method uses the improved Tracking-Learning-Detection (TLD) tracking framework which combines the tracker with the detector through the learning mechanism. In the TLD tracking framework, the Spatially Regularized Discriminatively Correlation Filters tracker is used and improved. In addition, the adaptive tracking scale is realized according to the confidence of the searching area. The experimental results show that the proposed algorithm can effectively deal with the attitude change and the illumination problem so that it has better robustness and stability for target continuous tracking.
C1 [Yang, Xin; Xia, Sijun; Zhou, Dake] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Jiangsu, Peoples R China.
   [Zhu, Songyan] Jiangsu Coll Engn & Technol, Nantong 226000, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Yang, X (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Jiangsu, Peoples R China.
EM yangxin@nuaa.edu.cn
RI fang, li/JNS-8415-2023
FU National Natural Science Foundation of China [61573182]
FX This research was supported by the National Natural Science Foundation
   of China (61573182).
CR Allenmark F, 2019, VIS COGN, V27, P576, DOI 10.1080/13506285.2019.1666953
   [Anonymous], 2015, PAMI
   Babenko B., P IEEE C COMP
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Denman S, 2007, PATTERN RECOGN LETT, V28, P1232, DOI 10.1016/j.patrec.2007.02.008
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong SH, 2011, PROG ELECTROMAGN RES, V120, P481, DOI 10.2528/PIER11081901
   Hu WM, 2014, IEEE T CYBERNETICS, V44, P66, DOI 10.1109/TCYB.2013.2247592
   Jia X., P IEEE C COMP
   Kalal Zdenek, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1417, DOI 10.1109/ICCVW.2009.5457446
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kalal Z, 2010, IEEE IMAGE PROC, P3789, DOI 10.1109/ICIP.2010.5653525
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Mahmoudi SA, 2014, B POL ACAD SCI-TECH, V62, P139, DOI 10.2478/bpasts-2014-0016
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang HY, 2018, VISUAL COMPUT, V34, P41, DOI 10.1007/s00371-016-1310-4
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhao YJ, 2013, CHIN CONTR CONF, P4486
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 30
TC 7
Z9 7
U1 0
U2 31
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1783
EP 1795
DI 10.1007/s00371-019-01772-w
EA NOV 2019
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000493680200001
DA 2024-07-18
ER

PT J
AU Vishwakarma, DK
   Dhiman, C
AF Vishwakarma, Dinesh Kumar
   Dhiman, Chhavi
TI A unified model for human activity recognition using spatial
   distribution of gradients and difference of Gaussian kernel
SO VISUAL COMPUTER
LA English
DT Article
DE Human activity recognition; Average energy image (AEI); Spatial
   Distribution of Gradients (SDGs); Spatio-temporal interest points
   (STIP); Difference of Gaussian (DoG)
ID RECOGNIZING HUMAN ACTIONS; SILHOUETTE EXTRACTION; MOTION;
   CLASSIFICATION; DESCRIPTOR; FEATURES
AB Understanding of human action and activity from video data is growing field and received rapid importance due to surveillance, security, entertainment and personal logging. In this work, a new hybrid technique is proposed for the description of human action and activity in video sequences. The unified framework endows a robust feature vector wrapping both global and local information strengthening discriminative depiction of action recognition. Initially, entropy-based texture segmentation is used for human silhouette extraction followed by construction of average energy silhouette images (AEIs). AEIs are the 2D binary projection of human silhouette frames of the video sequences, which reduces the feature vector generation time complexity. Spatial Distribution Gradients are computed at different levels of resolution of sub-images of AEI consisting overall shape variations of human silhouette during the activity. Due to scale, rotation and translation invariant properties of STIPs, the vocabulary of DoG-based STIPs are created using vector quantization which is unique for each class of the activity. Extensive experiments are conducted to validate the performance of the proposed approach on four standard benchmarks, i.e., Weizmann, KTH, Ballet Movements, Multi-view IXMAS. Promising results are obtained when compared with the similar state of the arts, demonstrating the robustness of the proposed hybrid feature vector for different types of challenges-illumination, view variations posed by the datasets.
C1 [Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Dept Informat Technol, New Delhi 110042, India.
   [Dhiman, Chhavi] Delhi Technol Univ, Dept Elect & Commun Engn, New Delhi 110042, India.
C3 Delhi Technological University; Delhi Technological University
RP Vishwakarma, DK (corresponding author), Delhi Technol Univ, Dept Informat Technol, New Delhi 110042, India.
EM dvishwakarma@gmail.com; chhavi1990delhi@gmail.com
RI VISHWAKARMA, DINESH/ABK-7887-2022; VISHWAKARMA, DINESH KUMAR/L-3815-2018
OI VISHWAKARMA, DINESH KUMAR/0000-0002-1026-0047; Dhiman,
   Chhavi/0000-0002-3401-596X
CR Al-Ali S., 2014, Computer Vision in Control Systems-2 Innovations in Practice, P11, DOI DOI 10.1007/978-3-319-11430-9_2
   Al-Maadeed S, 2014, PATTERN RECOGN LETT, V49, P69, DOI 10.1016/j.patrec.2014.06.001
   Chaaraoui AA, 2013, PATTERN RECOGN LETT, V34, P1799, DOI 10.1016/j.patrec.2013.01.021
   [Anonymous], 2011, P CVPR 2011 PROV RI
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 2008, P 2008 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2008.4587735
   [Anonymous], IEEE T AUTON MENT DE
   Asadi-Aghbolaghi M, 2018, MULTIMED TOOLS APPL, V77, P14115, DOI 10.1007/s11042-017-5017-y
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bregonzio M., 2009, CVPR
   Brutzer S, 2011, PROC CVPR IEEE
   Clausi DA, 2002, CAN J REMOTE SENS, V28, P45, DOI 10.5589/m02-004
   Coniglio C., 2015, INT C ADV CONC INT V
   Coniglio C, 2017, PATTERN RECOGN LETT, V93, P182, DOI 10.1016/j.patrec.2016.12.014
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Dollar P., 2005, INT C COMP COMM NETW
   Fu YH, 2017, MULTIMED TOOLS APPL, V76, P12645, DOI 10.1007/s11042-016-3630-9
   Gómez-Conde I, 2015, EXPERT SYST APPL, V42, P5472, DOI 10.1016/j.eswa.2015.03.010
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Goudelis G, 2013, PATTERN RECOGN, V46, P3238, DOI 10.1016/j.patcog.2013.06.006
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Guo H, 2017, PATTERN RECOGN LETT, V94, P38, DOI 10.1016/j.patrec.2017.05.012
   Han H, 2015, IMAGING SCI J, V63, P45, DOI 10.1179/1743131X14Y.0000000091
   Han J, 2018, OPT REV, V25, P301, DOI 10.1007/s10043-018-0420-9
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Ijjina EP, 2017, PATTERN RECOGN, V72, P504, DOI 10.1016/j.patcog.2017.07.013
   Iosifidis A, 2014, PATTERN RECOGN LETT, V49, P185, DOI 10.1016/j.patrec.2014.07.011
   Jalal A, 2017, PATTERN RECOGN, V61, P295, DOI 10.1016/j.patcog.2016.08.003
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Komorkiewicz M., 2013, IEEE CONFERENCE ON D
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lei J, 2016, IET COMPUT VIS, V10, P537, DOI 10.1049/iet-cvi.2015.0408
   Li BL, 2012, PROC CVPR IEEE, P1362, DOI 10.1109/CVPR.2012.6247822
   Liu HH, 2018, IEEE T NEUR NET LEAR, V29, P1427, DOI 10.1109/TNNLS.2017.2669522
   Liu J., 2008, CVPR
   Liu L, 2016, IEEE T CYBERNETICS, V46, P158, DOI 10.1109/TCYB.2015.2399172
   Ming X.L., 2014, J S CENT U, V20, P3171
   Mosabbeb EA, 2013, SENSORS-BASEL, V13, P8750, DOI 10.3390/s130708750
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ojala T, 1999, PATTERN RECOGN, V32, P477, DOI 10.1016/S0031-3203(98)00038-7
   Patrona F, 2018, PATTERN RECOGN, V76, P612, DOI 10.1016/j.patcog.2017.12.007
   Pei LS, 2016, VISUAL COMPUT, V32, P1395, DOI 10.1007/s00371-015-1090-2
   Permuter H, 2006, PATTERN RECOGN, V39, P695, DOI 10.1016/j.patcog.2005.10.028
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Rahman SA, 2014, EXPERT SYST APPL, V41, P574, DOI 10.1016/j.eswa.2013.07.082
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Rampun A., 2013, INT C COMP VIS
   Raptis M, 2010, LECT NOTES COMPUTER
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Sadek S., 2012, INTERNATIONAL CONFER
   Saghafi B, 2012, SIGNAL PROCESS-IMAGE, V27, P96, DOI 10.1016/j.image.2011.05.002
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Singh S., 2010, INT C ADV VID SIGN B
   Soh LK, 1999, IEEE T GEOSCI REMOTE, V37, P780, DOI 10.1109/36.752194
   Takano W, 2017, ROBOT AUTON SYST, V91, P247, DOI 10.1016/j.robot.2017.02.003
   Nguyen TN, 2015, VISUAL COMPUT, V31, P391, DOI 10.1007/s00371-014-0934-5
   Touati R., 2014, CAN C COMP ROB VIS M
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Vishwakarma DK, 2016, AEU-INT J ELECTRON C, V70, P341, DOI 10.1016/j.aeue.2015.12.016
   Vishwakarma DK, 2015, EXPERT SYST APPL, V42, P6957, DOI 10.1016/j.eswa.2015.04.039
   Vishwakarma DK, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT (INDIACOM), P336
   Vishwakarma DK, 2017, IEEE T COGN DEV SYST, V9, P316, DOI 10.1109/TCDS.2016.2577044
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang J, 2016, IEEE T CIRC SYST VID, V26, P1461, DOI 10.1109/TCSVT.2014.2382984
   Wang XF, 2017, SIGNAL PROCESS-IMAGE, V57, P91, DOI 10.1016/j.image.2017.05.007
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Weng ZK, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0250-5
   Wu D, 2013, IEEE T CIRC SYST VID, V23, P236, DOI 10.1109/TCSVT.2012.2203731
   Wu X., 2012, P 12 EUR C COMP VIS
   Yan S., 2018, AAAI, P1
   Zeng S, 2014, NEUROCOMPUTING, V144, P346, DOI 10.1016/j.neucom.2014.04.037
NR 76
TC 35
Z9 35
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1595
EP 1613
DI 10.1007/s00371-018-1560-4
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000008
DA 2024-07-18
ER

PT J
AU Chen, L
   Wang, RG
   Yang, J
   Xue, LX
   Hu, M
AF Chen, Long
   Wang, Ronggui
   Yang, Juan
   Xue, Lixia
   Hu, Min
TI Multi-label image classification with recurrently learning semantic
   dependencies
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-label; CNN-RNN; Attention; LSTM; Dependencies
AB Recognizing multi-label images is a significant but challenging task toward high-level visual understanding. Remarkable success has been achieved by applying CNN-RNN design-based models to capture the underlying semantic dependencies of labels and predict the label distributions over the global-level features output by CNNs. However, such global-level features often fuse the information of multiple objects, leading to the difficulty in recognizing small object and capturing the label co-relation. To better solve this problem, in this paper, we propose a novel multi-label image classification framework which is an improvement to the CNN-RNN design pattern. By introducing the attention network module in the CNN-RNN architecture, the objects features of the attention map are separated by the channels which are further send to the LSTM network to capture dependencies and predict labels sequentially. A category-wise max-pooling operation is then performed to integrate these labels into the final prediction. Experimental results on PASCAL2007 and MS-COCO datasets demonstrate that our model can effectively exploit the correlation between tags to improve the classification performance as well as better recognize the small targets.
C1 [Chen, Long; Wang, Ronggui; Yang, Juan; Xue, Lixia; Hu, Min] Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.
C3 Hefei University of Technology
RP Yang, J (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.
EM 787630720@qq.com; wangrgui@foxmail.com; yangjuan6985@163.com;
   xlxzzm@163.com; jsjxhumin@hfut.edu.cn
RI Lin, Kuan-Yu/JXM-6653-2024
FU National Natural Science Foundation of China [61672202]
FX Funding was provided by the National Natural Science Foundation of China
   (Grant No. 61672202).
CR [Anonymous], LECT NOTES ARTIFICIA
   [Anonymous], 2015, PROCIEEE CONFCOMPUT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.37
   [Anonymous], 2014, ICLR
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu RS, 2018, INT J DIGIT MULTIMED, V2018, DOI [10.1155/2018/7543875, 10.1109/TMM.2018.2812605]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nguyen TV, 2015, AAAI CONF ARTIF INTE, P4286
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 36
TC 15
Z9 15
U1 0
U2 24
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2019
VL 35
IS 10
BP 1361
EP 1371
DI 10.1007/s00371-018-01615-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IU7PN
UT WOS:000483775900003
DA 2024-07-18
ER

PT J
AU Ma, M
   Wang, X
   Duan, Y
   Frey, SH
   Gu, XF
AF Ma, Ming
   Wang, Xu
   Duan, Ye
   Frey, Scott H.
   Gu, Xianfeng
TI Optimal mass transport based brain morphometry for patients with
   congenital hand deformities
SO VISUAL COMPUTER
LA English
DT Article
DE Congenital hand deformities; Brain morphometry; Optimal mass transport;
   Wasserstein distance
ID SHAPE; CLASSIFICATION; REGISTRATION; SEGMENTATION; ANOMALIES
AB Congenital hand deformities (CHD) have attracted increasing research attention in the past few decades. The impacts of CHD on the brain structure, however, are not fully studied to date. In this work, we propose a novel framework to study brain morphometry in CHD patients using Wasserstein distance based on optimal mass transport (OMT) theory. We first employ conformal mapping to map the left and right surface-based functional brain areas to planar rectangles, which pushes the area element on the brain surface to the planar rectangle and incurs the area distortion. A measure is then determined by this area distortion. We further propose a new rectangle domain-based OMT map algorithm. Given two measures on two surfaces, we employ the proposed algorithm to compute a unique OMT map between the two measures encoding the geometric information of left and right surface-based functional brain areas. The transportation cost of this OMT map gives the Wasserstein distance between two surfaces, which intrinsically measures the dissimilarities between two surface-based shapes. Our method is theoretically rigorous and computationally efficient and stable. We finally evaluate the proposed Wasserstein distance-based method on the left and right post-central gyri from the CHD patients and healthy control subjects for analyzing brain cortical morphometry. Experimental results demonstrate the efficiency and efficacy of our method, and shed insightful lights on the study of the brain morphometry for those subjects with CHD.
C1 [Ma, Ming; Gu, Xianfeng] SUNY Stony Brook, Comp Sci Dept, Stony Brook, NY 11794 USA.
   [Wang, Xu; Duan, Ye] Univ Missouri, Comp Sci Dept, Columbia, MO 65211 USA.
   [Frey, Scott H.] Univ Missouri, Psychol Sci Dept, Columbia, MO 65211 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; University of Missouri System; University of
   Missouri Columbia; University of Missouri System; University of Missouri
   Columbia
RP Ma, M (corresponding author), SUNY Stony Brook, Comp Sci Dept, Stony Brook, NY 11794 USA.
EM minma@cs.stonybrook.edu
OI Gu, Xianfeng David/0000-0001-8226-5851
FU NSF [DMS-1418255]; AFOSR [FA9550-14-1-0193]
FX This paper has been partially supported by NSF DMS-1418255, AFOSR
   FA9550-14-1-0193.
CR Andersson GB, 2011, J HAND SURG-EUR VOL, V36E, P795, DOI 10.1177/1753193411412869
   [Anonymous], 2003, PROC 19 ANN S COMPUT, DOI DOI 10.1145/777792.777839
   [Anonymous], 1948, CR (Doklady) Acad. Sci. URSS (NS), DOI [DOI 10.1007/S10958-006-0050-9, 10.1007/s10958-006-0050-9]
   [Anonymous], 2012, AM J CLIN NUTR
   Ashburner J, 1998, HUM BRAIN MAPP, V6, P348, DOI 10.1002/(SICI)1097-0193(1998)6:5/6<348::AID-HBM4>3.3.CO;2-G
   Ben-Chen M, 2008, COMPUT GRAPH FORUM, V27, P449, DOI 10.1111/j.1467-8659.2008.01142.x
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BRENIER Y, 1991, COMMUN PUR APPL MATH, V44, P375, DOI 10.1002/cpa.3160440402
   Chung MK, 2008, IEEE T MED IMAGING, V27, P1143, DOI 10.1109/TMI.2008.918338
   Corsello G, 2012, J MATERN-FETAL NEO M, V25, P25, DOI 10.3109/14767058.2012.664943
   Davies RH, 2003, LECT NOTES COMPUT SC, V2732, P38
   de Goes F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366190
   de Goes F, 2011, COMPUT GRAPH FORUM, V30, P1593, DOI 10.1111/j.1467-8659.2011.02033.x
   Dominitz A, 2010, IEEE T VIS COMPUT GR, V16, P419, DOI 10.1109/TVCG.2009.64
   Gardner A, 2014, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2014.25
   Gold NB, 2011, AM J MED GENET A, V155A, P1225, DOI 10.1002/ajmg.a.33999
   Goldfarb CA, 2009, J HAND SURG-AM, V34A, P1351, DOI 10.1016/j.jhsa.2009.06.014
   Gu X., 2013, ARXIV13025472
   Gu X.D., 2008, Computational Conformal Geometry
   Haker S, 2004, INT J COMPUT VISION, V60, P225, DOI 10.1023/B:VISI.0000036836.66311.97
   He Q, 2011, INT J DATA MIN BIOIN, V5, P158, DOI 10.1504/IJDMB.2011.039175
   Im K, 2008, CEREB CORTEX, V18, P2181, DOI 10.1093/cercor/bhm244
   Jin M, 2008, IEEE T VIS COMPUT GR, V14, P1030, DOI 10.1109/TVCG.2008.57
   Landin-Romero R, 2017, AUST NZ J PSYCHIAT, V51, P42, DOI 10.1177/0004867416631827
   Liu XW, 2010, INT J COMPUT VISION, V89, P69, DOI 10.1007/s11263-010-0323-0
   Lo Iacono N, 2008, DEVELOPMENT, V135, P1377, DOI 10.1242/dev.011759
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Manske PR, 2009, HAND CLIN, V25, P157, DOI 10.1016/j.hcl.2008.10.005
   Mérigot Q, 2011, COMPUT GRAPH FORUM, V30, P1583, DOI 10.1111/j.1467-8659.2011.02032.x
   Meyer M., 2002, VISUALIZATION MATH, V6, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Oberg KC, 2010, J HAND SURG-AM, V35A, P2066, DOI 10.1016/j.jhsa.2010.09.031
   Pizer SM, 1999, IEEE T MED IMAGING, V18, P851, DOI 10.1109/42.811263
   Rehman TU, 2009, MED IMAGE ANAL, V13, P931, DOI 10.1016/j.media.2008.10.008
   Rockafellar R., 2010, VARIATIONAL ANAL
   Specht M, 2007, VISUAL COMPUT, V23, P743, DOI 10.1007/s00371-007-0156-1
   Su ZY, 2013, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2013.290
   Taimouri V, 2014, GRAPH MODELS, V76, P57, DOI 10.1016/j.gmod.2013.12.001
   Tonkin MA, 2013, J HAND SURG-AM, V38A, P1845, DOI 10.1016/j.jhsa.2013.03.019
   Tremblay P, 2016, BRAIN STRUCT FUNCT, V221, P3275, DOI 10.1007/s00429-015-1100-1
   Wade BSC, 2015, NEUROIMAGE-CLIN, V9, P564, DOI 10.1016/j.nicl.2015.10.006
   Wang YL, 2009, PROC CVPR IEEE, P202, DOI 10.1109/CVPRW.2009.5206578
   Zeng W, 2013, INT J COMPUT VISION, V105, P155, DOI 10.1007/s11263-012-0586-8
   Zhao X, 2013, IEEE T VIS COMPUT GR, V19, P2838, DOI 10.1109/TVCG.2013.135
   Zhu L., 2003, AREA PRESERVING MAPP
NR 44
TC 4
Z9 4
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2019
VL 35
IS 9
BP 1311
EP 1325
DI 10.1007/s00371-018-1543-5
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IQ2JJ
UT WOS:000480574500010
DA 2024-07-18
ER

PT J
AU Ali, M
   Jones, MW
   Xie, XH
   Williams, M
AF Ali, Mohammed
   Jones, Mark W.
   Xie, Xianghua
   Williams, Mark
TI TimeCluster: dimension reduction applied to temporal data for visual
   analytics
SO VISUAL COMPUTER
LA English
DT Article
DE Time-series data; Visual analytics; Sliding window; Dimension reduction;
   Time-series graph; 2D projection; Repeated patterns; Outliers; Labeling
ID TIME-SERIES; VISUALIZATION
AB There is a need for solutions which assist users to understand long time-series data by observing its changes over time, finding repeated patterns, detecting outliers, and effectively labeling data instances. Although these tasks are quite distinct and are usually tackled separately, we present an interactive visual analytics system and approach that can address these issues in a single system. It enables users to visualize, understand and explore univariate or multivariate long time-series data in one image using a connected scatter plot. It supports interactive analysis and exploration for pattern discovery and outlier detection. Different dimensionality reduction techniques are used and compared in our system. Because of its power of extracting features, deep learning is used for multivariate time-series along with 2D reduction techniques for rapid and easy interpretation and interaction with large amount of time-series data. We deploy our system with different time-series datasets and report two real-world case studies that are used to evaluate our system.
C1 [Ali, Mohammed] Swansea Univ, Visual Comp Res Grp, Swansea, W Glam, Wales.
   [Ali, Mohammed] King Khalid Univ, Dept Comp Sci, Abha, Saudi Arabia.
   [Jones, Mark W.] Swansea Univ, Dept Comp Sci, Visual Comp Res Grp, Swansea, W Glam, Wales.
   [Xie, Xianghua] Swansea Univ, Dept Comp Sci, Comp Vis & Machine Leaning Lab, Swansea, W Glam, Wales.
   [Williams, Mark] Univ South Wales, Fac Life Sci & Educ, Pontypridd, M Glam, Wales.
C3 Swansea University; King Khalid University; Swansea University; Swansea
   University; University of South Wales
RP Ali, M (corresponding author), Swansea Univ, Visual Comp Res Grp, Swansea, W Glam, Wales.; Ali, M (corresponding author), King Khalid Univ, Dept Comp Sci, Abha, Saudi Arabia.
EM 884715@swansea.ac.uk; m.w.jones@swansea.ac.uk; x.xie@swansea.ac.uk;
   mark.williams@southwales.ac.uk
RI Jones, Mark W./F-1114-2015
OI Jones, Mark W./0000-0001-8991-1190; Ali, Mohammed/0000-0002-5908-4013;
   Xie, Xianghua/0000-0002-2701-8660
FU EPSRC [EP/N028139/1]
FX We would like to acknowledge that this work was supported by EPSRC
   (Grant Number EP/N028139/1).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Abdelhameed AM, 2018, IEEE WRK SIG PRO SYS, P223, DOI 10.1109/SiPS.2018.8598447
   Albers D, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P551, DOI 10.1145/2556288.2557200
   Alsallakh B, 2014, EUROVA EUROVIS
   [Anonymous], VISUALIZING MULTIVAR
   [Anonymous], P SPIE
   [Anonymous], 2018, UMAP UNIFORM MANIFOL, DOI DOI 10.21105/JOSS.00861
   [Anonymous], 2018, ABNORMAL RESP
   [Anonymous], P 7 INT S VIS INF CO
   [Anonymous], 2013, P SIGCHI C HUMAN FAC, DOI [DOI 10.1145/2470654.2466441, DOI 10.1145/2470654.24664412,3,4, 10.1145/2470654.24664412,3,4]
   [Anonymous], 2016, PROC INT S ELECT IMA
   Becht E, 2019, NAT BIOTECHNOL, V37, P38, DOI 10.1038/nbt.4314
   Bernard J, 2018, VISUAL COMPUT, V34, P1189, DOI 10.1007/s00371-018-1500-3
   Bernard J, 2018, IEEE T VIS COMPUT GR, V24, P298, DOI 10.1109/TVCG.2017.2744818
   Bidder OR, 2015, MOV ECOL, V3, DOI 10.1186/s40462-015-0055-4
   Campello Ricardo J. G. B., 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference (PAKDD 2013). Proceedings, P160, DOI 10.1007/978-3-642-37456-2_14
   Cavallo M, 2019, IEEE T VIS COMPUT GR, V25, P267, DOI 10.1109/TVCG.2018.2864477
   Cheung CM, 2017, IEEE INT CONF BIG DA, P1277, DOI 10.1109/BigData.2017.8258055
   Correll M., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P1095, DOI [DOI 10.1145/2207676.2208556, 10.1145/2207676.22085562, DOI 10.1145/2207676.22085562]
   Francois Chollet., 2015, KERASIO
   Gogolouis A, 2019, IEEE T VIS COMPUT GR, V25, P523, DOI 10.1109/TVCG.2018.2865077
   Grundy E, 2009, COMPUT GRAPH FORUM, V28, P815, DOI 10.1111/j.1467-8659.2009.01469.x
   Guan QJ, 2017, INT C INTEL HUM MACH, P373, DOI 10.1109/IHMSC.2017.91
   Hensman J, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-252
   Huang H, 2018, IEEE T MED IMAGING, V37, P1551, DOI 10.1109/TMI.2017.2715285
   Javed W, 2010, IEEE T VIS COMPUT GR, V16, P927, DOI 10.1109/TVCG.2010.162
   Martinez-Murcia FJ, 2019, ADV INTELL SYST, V771, P47, DOI 10.1007/978-3-319-94120-2_5
   Keim E.D., 2010, Mastering the information age: Solving problems with visual analytics, eurographics association
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Legg PA, 2013, IEEE T VIS COMPUT GR, V19, P2109, DOI 10.1109/TVCG.2013.207
   Lesch RH, 1999, PROCEEDINGS OF THE IEEE/IAFE 1999 CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR FINANCIAL ENGINEERING, P183, DOI 10.1109/CIFER.1999.771118
   Li J, 2019, IEEE T VIS COMPUT GR, V25, P2554, DOI 10.1109/TVCG.2018.2851227
   Li Yuan., 2012, SDM, P895
   Lin J., 2005, Information Visualization, V4, P61, DOI 10.1057/palgrave.ivs.9500089
   Lin J., 2002, Finding motifs in time series, P53
   Mohseni-Kabir A, 2016, IEEE ROMAN, P267, DOI 10.1109/ROMAN.2016.7745141
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Rahman A, 2018, 2018 2ND INTERNATIONAL CONFERENCE ON ENERGY CONSERVATION AND EFFICIENCY (ICECE), P6, DOI 10.1109/ECE.2018.8554977
   Röhlig M, 2014, IEEE CONF VIS ANAL, P269, DOI 10.1109/VAST.2014.7042524
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Senin Pavel, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P468, DOI 10.1007/978-3-662-44845-8_37
   Shepard Emily L. C., 2010, Endangered Species Research, V10, P47, DOI 10.3354/esr00084
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Singhal A, 2005, J CHEMOMETR, V19, P427, DOI 10.1002/cem.945
   Swihart BJ, 2010, EPIDEMIOLOGY, V21, P621, DOI 10.1097/EDE.0b013e3181e5b06a
   Dang TN, 2013, IEEE T VIS COMPUT GR, V19, P470, DOI 10.1109/TVCG.2012.128
   van den Elzen S, 2016, IEEE T VIS COMPUT GR, V22, P1, DOI 10.1109/TVCG.2015.2468078
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Der Maaten Laurens., J Mach Learn Res, V10, P66
   van Unen V, 2016, IMMUNITY, V44, P1227, DOI 10.1016/j.immuni.2016.04.014
   Walker J, 2016, IEEE T VIS COMPUT GR, V22, P549, DOI 10.1109/TVCG.2015.2467751
   Walker JS, 2015, MOV ECOL, V3, DOI 10.1186/s40462-015-0056-3
   Walker JS, 2015, VISUAL COMPUT, V31, P1067, DOI 10.1007/s00371-015-1112-0
   Wilson W, 2007, LECT NOTES COMPUT SC, V4628, P276
   Wilson W, 2008, INT J AUTOM COMPUT, V5, P32, DOI 10.1007/s11633-008-0032-0
   Xie C, 2019, IEEE T VIS COMPUT GR, V25, P215, DOI 10.1109/TVCG.2018.2865026
   Yang K., 2004, ACM INT WORKSHOP MUL, P65, DOI DOI 10.1145/1032604.1032616
   Yang KY, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P805
   Yuan G., 2013, McMaster Univ. Med. J, V10, P23, DOI DOI 10.1111/head.12649
NR 61
TC 61
Z9 69
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 1013
EP 1026
DI 10.1007/s00371-019-01673-y
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200020
OA hybrid
DA 2024-07-18
ER

PT J
AU Wei, MQ
   Yan, QA
   Luo, F
   Song, CF
   Xiao, CX
AF Wei, Mengqiang
   Yan, Qingan
   Luo, Fei
   Song, Chengfang
   Xiao, Chunxia
TI Joint bilateral propagation upsampling for unstructured multi-view
   stereo
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-view stereo; Unstructured images; 3D reconstruction; Joint
   bilateral propagation upsampling
ID GEOMETRY COMPLETION
AB In this paper, we explore a new way to accelerate and densify unstructured multi-view stereo (MVS). While many unstructured MVS algorithms have been proposed, we discover that the image-guided resizing can easily and significantly benefit their 3D reconstruction results in both efficiency and completeness. Therefore, we build our framework upon a novel selective joint bilateral upsampling and depth propagation strategy. First, we downsample the input unstructured images into lower resolution ones and perform the MVS calculation to efficiently obtain depth and normal maps from these resized pictures. Then, the proposed algorithm upsamples the normal maps with the guidance of input images, and jointly take them into consideration to recover the low-resolution depth maps into high resolution with geometry details simultaneously enriched. Finally by adaptively fusing the reconstructed depth and normal maps, we construct the final dense 3D scene. Quantitative results validate the efficiency and effectiveness of the proposed method.
C1 [Wei, Mengqiang; Luo, Fei; Song, Chengfang; Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Yan, Qingan] JD Com Amer Technol Corp, Mountain View, CA 94043 USA.
C3 Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM 2292507220@qq.com; qingan.yan@jd.com; luofei@whu.edu.cn;
   Songchf@whu.edu.cn; cxxiao@whu.edu.cn
RI Luo, Fei/IZQ-5485-2023
OI Luo, Fei/0000-0002-8481-8357
FU National Key Research and Development Program of China
   [2017YF-B1002600]; NSFC [61672390, 41201404]; Wuhan Science and
   Technology Plan Project [2017010201010109]; Key Technological Innovation
   Projects of Hubei Province [2018AAA062]
FX This work was partly supported by The National Key Research and
   Development Program of China (2017YF-B1002600), the NSFC (Nos. 61672390,
   41201404), Wuhan Science and Technology Plan Project (No.
   2017010201010109) and Key Technological Innovation Projects of Hubei
   Province (2018AAA062).
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445
   Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Chen JW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982423
   Fu YP, 2018, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2018.00488
   Fuhrmann S, 2015, COMPUT GRAPH-UK, V53, P44, DOI 10.1016/j.cag.2015.09.003
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2010, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2010.5539802
   Furukawa Yasutaka, 2015, FDN TRENDS COMPUTER, V9, P1
   Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949
   Heinly J, 2014, LECT NOTES COMPUT SC, V8692, P780, DOI 10.1007/978-3-319-10593-2_51
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XQ, 2017, COMPUT AIDED GEOM D, V54, P49, DOI 10.1016/j.cagd.2017.02.011
   Moulon P, 2017, LECT NOTES COMPUT SC, V10214, P60, DOI 10.1007/978-3-319-56414-2_5
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Schönberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148
   Schops T., 2017, CVPR, V3
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Strecha C., 2008, 2008 IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2008.4587706
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Xiao CX, 2007, VISUAL COMPUT, V23, P433, DOI 10.1007/s00371-007-0115-x
   Yagou H, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/GMAP.2002.1027503
   Yan Q., 2017, CVPR
   Yang L, 2017, VISUAL COMPUT, V33, P385, DOI 10.1007/s00371-016-1208-1
   Yao Yao, 2019, CVPR
   Yao Yingjie, 2018, ECCV
   Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
NR 38
TC 10
Z9 10
U1 1
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 797
EP 809
DI 10.1007/s00371-019-01688-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200003
DA 2024-07-18
ER

PT J
AU Jachym, M
   Lavernhe, S
   Euzenat, C
   Tournier, C
AF Jachym, Marc
   Lavernhe, Sylvain
   Euzenat, Charly
   Tournier, Christophe
TI Effective NC machining simulation with OptiX ray tracing engine
SO VISUAL COMPUTER
LA English
DT Article
DE Machining simulation; Ray tracing; GPU computing; CUDA architecture;
   OptiX
AB The manufacturing of high-added-value products in multi-axis machining requires advanced simulation in order to validate the process. Whereas CAM software editors provide simulation software that allows the detection of global interferences or local gouging, research works have shown that it is possible to consider multi-scale simulations of the surface, with a realistic description of both the tools and the machining path. However, computing capacity remains a problem for interactive and realistic simulations in 5-axis continuous machining. In this context, using general-purpose computing on graphics processing units as well as NVIDIA OptiX ray tracing engine makes it possible to develop a robust simulation application. Thus, the aim of this paper is to evaluate the use of NVIDIA OptiX ray tracing engine compared to a fully integrated CUDA software, in terms of computing time and development effort. Experimental investigations are carried out on different hardware such as Xeon CPU, Quadro4000, Tesla K40 and Titan Z GPUs. Results show that the development of such an application with the OptiX development kit is very simple and that the performances in roughing simulations are very promising. Developed software as well as dataset can be downloaded from http://webserv.lurpa.ens-cachan.fr/simsurf.
C1 [Jachym, Marc; Lavernhe, Sylvain; Euzenat, Charly; Tournier, Christophe] Univ Paris Saclay, Univ Paris Sud, LURPA, ENS Paris Saclay, F-94235 Cachan, France.
C3 Universite Paris Saclay; Universite Paris Cite
RP Tournier, C (corresponding author), Univ Paris Saclay, Univ Paris Sud, LURPA, ENS Paris Saclay, F-94235 Cachan, France.
EM christophe.tournier@ens-paris-saclay.fr
RI LAVERNHE, Sylvain/HPC-0753-2023; Tournier, Christophe/J-7403-2013
OI LAVERNHE, Sylvain/0000-0002-6701-4648; Tournier,
   Christophe/0000-0003-4153-0836; Euzenat, Charly/0000-0002-7902-9492
FU NVIDIA Corporation; Farman Institute [CNRS FR3311]
FX We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the Tesla K40 GPU used for this research as well as the
   support of the Farman Institute (CNRS FR3311).
CR Abecassis F, 2015, COMPUT IND, V71, P1, DOI 10.1016/j.compind.2015.02.007
   CUDA C, 2012, C PROGR GUID
   He W, 2007, INT J ADV MANUF TECH, V33, P1173, DOI 10.1007/s00170-006-0543-1
   Inui M., 2013, T I SYSTEMS CONTROL, V6, P95
   Jang D, 2000, INT J ADV MANUF TECH, V16, P709, DOI 10.1007/s001700070022
   Jerard R. B., 1989, Visual Computer, V5, P329, DOI 10.1007/BF01999101
   Lavernhe S, 2014, INT J ADV MANUF TECH, V74, P393, DOI 10.1007/s00170-014-5689-7
   Moller T., 1997, J GRAPHICS TOOLS, V2, P21, DOI [DOI 10.1080/10867651.1997.10487468, 10.1080/10867651.1997.10487468]
   Morell-Giménez V, 2013, COMPUT IND, V64, P50, DOI 10.1016/j.compind.2012.09.009
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Quinsat Y, 2008, J MATER PROCESS TECH, V195, P135, DOI 10.1016/j.jmatprotec.2007.04.129
   Zhang W., 2010, FAST TRIANGLE RASTER, P78
NR 12
TC 4
Z9 4
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 281
EP 288
DI 10.1007/s00371-018-1497-7
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600010
DA 2024-07-18
ER

PT J
AU Yu, ZB
   Liu, QS
   Liu, GC
AF Yu, Zhenbo
   Liu, Qinshan
   Liu, Guangcan
TI Deeper cascaded peak-piloted network for weak expression recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Peak-piloted; Deep network; Cascaded
   fine-tune
ID FACIAL EXPRESSION
AB Facial expression recognition is in general a challenging problem, especially in the presence of weak expression. Most recently, deep neural networks have been emerging as a powerful tool for expression recognition. However, due to the lack of training samples, existing deep network-based methods cannot fully capture the critical and subtle details of weak expression, resulting in unsatisfactory results. In this paper, we propose Deeper Cascaded Peak-piloted Network (DCPN) for weak expression recognition. The technique of DCPN has three main aspects: (1) Peak-piloted feature transformation, which utilizes the peak expression (easy samples) to supervise the non-peak expression (hard samples) of the same type and subject; (2) the back-propagation algorithm is specially designed such that the intermediate-layer feature maps of non-peak expression are close to those of the corresponding peak expression; and (3) an novel integration training method, cascaded fine-tune, is proposed to prevent the network from overfitting. Experimental results on two popular facial expression databases, CK+ and Oulu-CASIA, show the superiority of the proposed DCPN over state-of-the-art methods.
C1 [Yu, Zhenbo; Liu, Qinshan; Liu, Guangcan] Nanjing Univ Informat Sci & Technol, Sch Informat & Control, B DAT, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Yu, ZB (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Informat & Control, B DAT, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
EM zbyu@nuist.edu.cn; qsliu@nuist.edu.cn; gcliu@nuist.edu.cn
RI Liu, Guangcan/J-1391-2014
OI Liu, Guangcan/0000-0002-9428-4387
FU National Natural Science Foundation of China (NSFC) [61532009]; NSFC
   [61622305, 61502238]; Natural Science Foundation of Jiangsu Province of
   China (NSFJPC) [BK20160040]
FX The work of Qingshan Liu is supported by National Natural Science
   Foundation of China (NSFC) under Grant 61532009. The work of Guangcan
   Liu is supported in part by NSFC under Grant 61622305 and Grant
   61502238, and in part by the Natural Science Foundation of Jiangsu
   Province of China (NSFJPC) under Grant BK20160040.
CR Agarwal S, 2018, VISUAL COMPUT, V34, P177, DOI 10.1007/s00371-016-1323-z
   [Anonymous], 2006, COMP ROB VIS 2006 3
   [Anonymous], 2014, DEEPLY LEARNING DEFO
   [Anonymous], 2016, INCEPTION V4 INCEPTI
   [Anonymous], INT C PATT REC
   [Anonymous], 2016, APPL COMP VIS WACV 2
   Bargal SA, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433, DOI 10.1145/2993148.2997627
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Chi J, 2014, VISUAL COMPUT, V30, P649, DOI 10.1007/s00371-014-0960-3
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Danelakis A, 2016, VISUAL COMPUT, V32, P1001, DOI 10.1007/s00371-016-1243-y
   Dhall A, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2993148.2997638
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Guo Y, 2012, DYNAMIC FACIAL EXPRE
   Han S., 2016, ADV NEURAL INF PROCE, P109
   He JC, 2017, PATTERN RECOGN, V66, P44, DOI 10.1016/j.patcog.2016.11.029
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hung APL, 2015, VISUAL COMPUT, V31, P527, DOI 10.1007/s00371-014-0945-2
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Klaser A., 2008, BRIT MACH VIS C 2008
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Valstar M.F., 2015, P 2015 IEEE INT C WO, P1
   Yao AB, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/2993148.2997639
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zhang K., 2016, IEEE J SOLID-ST CIRC, V23, P1161
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhao R, 2016, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2016.377
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
NR 36
TC 45
Z9 47
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1691
EP 1699
DI 10.1007/s00371-017-1443-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400006
DA 2024-07-18
ER

PT J
AU Zhao, JW
   Wang, SH
   Liu, X
   Liu, Y
   Chen, YQ
AF Zhao, Jingwen
   Wang, Shuo Hong
   Liu, Xiang
   Liu, Ye
   Chen, Yan Qiu
TI Early diagnosis of cirrhosis via automatic location and geometric
   description of liver capsule
SO VISUAL COMPUTER
LA English
DT Article
DE Automatic diagnosis; Liver capsule; Spatial context constrained; Scale
   space; Cirrhosis
ID CLASSIFICATION; ULTRASONOGRAPHY; HEPATITIS; SNAKES
AB We propose in this paper an automatic method for early diagnosis of cirrhosis using high-frequency ultrasound images. Instead of analyzing image texture, our method exploits image characteristics of liver capsule. To this end, we first propose a novel spatial context-constrained multi-scale method to accurately extract the boundaries of the liver capsule. Our approach detects all the possible edges in scale space, and the irrelevant edges are then filtered out with a spatial context-based energy function. Secondly on this basis, two novel descriptors are proposed to characterize the geometric properties of liver capsule and the changes of liver capsule with the aggravation of liver cirrhosis. These two descriptors are used as features fed into a support vector machine classifier for quantitative analysis in automatic diagnosis. Experiment results show that the proposed method can reliably localize liver capsule and accurately classify ultrasound liver images into normal and cirrhosis classes automatically.
C1 [Zhao, Jingwen; Wang, Shuo Hong; Liu, Xiang; Chen, Yan Qiu] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China.
   [Liu, Xiang] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai, Peoples R China.
   [Liu, Ye] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing, Jiangsu, Peoples R China.
C3 Fudan University; Shanghai University of Engineering Science; Nanjing
   University of Posts & Telecommunications
RP Chen, YQ (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China.
EM jingwenzhao13@fudan.edu.cn; sh_wang@fudan.edu.cn;
   xiangliu09@fudan.edu.cn; yeliu@njupt.edu.cn; chenyq@fudan.edu.cn
OI Zhao, Jingwen/0000-0001-9704-4066
FU Science and Technology Commission of Shanghai Municipality
   [17ZR1402300]; National Natural Science Foundation of China [61602255];
   Natural Science Foundation of the Jiangsu Higher Education Institutions
   of China [16KJB520032]
FX This work is supported by Science and Technology Commission of Shanghai
   Municipality, Grant No. 17ZR1402300. National Natural Science Foundation
   of China (Grant No. 61602255). Natural Science Foundation of the Jiangsu
   Higher Education Institutions of China (Grant No.16KJB520032).
CR [Anonymous], 2016, IEEE INT CON MULTI
   Dalwadi MN, 2013, IJARCET, V2, P2369
   DILELIO A, 1989, RADIOLOGY, V172, P389, DOI 10.1148/radiology.172.2.2526349
   FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W
   Gaiani S, 1997, J HEPATOL, V27, P979, DOI 10.1016/S0168-8278(97)80140-7
   Iloeje UH, 2006, GASTROENTEROLOGY, V130, P678, DOI 10.1053/j.gastro.2005.11.016
   Kadah YM, 1996, IEEE T MED IMAGING, V15, P466, DOI 10.1109/42.511750
   Kalyan Karthik, 2014, Advances in Bioinformatics, V2014, P708279, DOI 10.1155/2014/708279
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Lee WL, 2013, APPL SOFT COMPUT, V13, P3683, DOI 10.1016/j.asoc.2013.03.009
   Lee WL, 2003, IEEE T MED IMAGING, V22, P382, DOI 10.1109/TMI.2003.809593
   McPhee SJ., 2010, CURRENT MED DIAGNOSI, V49th
   Naghavi M, 2015, LANCET, V385, P117, DOI 10.1016/S0140-6736(14)61682-2
   RAETH U, 1985, J CLIN ULTRASOUND, V13, P87, DOI 10.1002/jcu.1870130203
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Steger C., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P630, DOI 10.1007/BFb0015573
   Steger C, 1998, IEEE T PATTERN ANAL, V20, P113, DOI 10.1109/34.659930
   TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769
   Vicas C, 2009, INT C INTELL COMP CO, P133, DOI 10.1109/ICCP.2009.5284775
   Virmani Jitendra, 2013, International Journal of Convergence Computing, V1, P19, DOI 10.1504/IJCONVC.2013.054658
   Virmani J, 2013, J DIGIT IMAGING, V26, P530, DOI 10.1007/s10278-012-9537-8
   Wang SH, 2016, IEEE INT C BIOINFORM, P799, DOI 10.1109/BIBM.2016.7822627
   Wu CC, 2012, EXPERT SYST APPL, V39, P9389, DOI 10.1016/j.eswa.2012.02.128
   WU CM, 1992, IEEE T MED IMAGING, V11, P141, DOI 10.1109/42.141636
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Yamaguchi T, 2002, JPN J APPL PHYS 1, V41, P3585, DOI 10.1143/JJAP.41.3585
NR 26
TC 9
Z9 12
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1677
EP 1689
DI 10.1007/s00371-017-1441-2
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400005
DA 2024-07-18
ER

PT J
AU Jentner, W
   Sacha, D
   Stoffel, F
   Ellis, G
   Zhang, LS
   Keim, DA
AF Jentner, Wolfgang
   Sacha, Dominik
   Stoffel, Florian
   Ellis, Geoffrey
   Zhang, Leishi
   Keim, Daniel A.
TI Making machine intelligence less scary for criminal analysts:
   reflections on designing a visual comparative case analysis tool
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Euro VA Conference
CY JUN 12-13, 2017
CL Barcelona, SPAIN
DE Crime intelligence analysis; Visual analytics; Clustering; System
   design; Human-computer interaction; Sequential pattern mining; Text
   analysis; Dimensionality reduction
ID SEMANTIC INTERACTION; MODEL
AB A fundamental task in criminal intelligence analysis is to analyze the similarity of crime cases, called comparative case analysis (CCA), to identify common crime patterns and to reason about unsolved crimes. Typically, the data are complex and high dimensional and the use of complex analytical processes would be appropriate. State-of-the-art CCA tools lack flexibility in interactive data exploration and fall short of computational transparency in terms of revealing alternative methods and results. In this paper, we report on the design of the Concept Explorer, a flexible, transparent and interactive CCA system. During this design process, we observed that most criminal analysts are not able to understand the underlying complex technical processes, which decrease the users' trust in the results and hence a reluctance to use the tool. Our CCA solution implements a computational pipeline together with a visual platform that allows the analysts to interact with each stage of the analysis process and to validate the result. The proposed visual analytics workflow iteratively supports the interpretation of the results of clustering with the respective feature relations, the development of alternative models, as well as cluster verification. The visualizations offer an understandable and usable way for the analyst to provide feedback to the system and to observe the impact of their interactions. Expert feedback confirmed that our user-centered design decisions made this computational complexity less scary to criminal analysts.
C1 [Zhang, Leishi] Middlesex Univ, Data Intens Syst, London, England.
   [Jentner, Wolfgang; Ellis, Geoffrey] Univ Konstanz, Univ Str 10, D-78464 Constance, Germany.
   [Sacha, Dominik; Stoffel, Florian; Keim, Daniel A.] Univ Konstanz, Data Anal & Visualizat Grp, Univ Str 10, D-78464 Constance, Germany.
C3 Middlesex University; University of Konstanz; University of Konstanz
RP Jentner, W (corresponding author), Univ Konstanz, Univ Str 10, D-78464 Constance, Germany.
EM wolfgang.jentner@uni.kn; dominik.sacha@uni.kn; florian.stoffel@uni.kn;
   geoffrey.ellis@uni-konstanz.de; l.x.zhang@mdx.ac.uk; daniel.keim@uni.kn
RI Jentner, Wolfgang/HJP-1206-2023; Keim, Daniel/X-7749-2019; Jentner,
   Wolfgang/AAA-4001-2019
OI Jentner, Wolfgang/0000-0003-1045-6020; Keim, Daniel/0000-0001-7966-9740;
   Jentner, Wolfgang/0000-0003-1045-6020; Zhang, Leishi/0000-0002-3158-2328
FU EU project VALCRI [FP7-SEC-2013-608142]
FX This work was supported by the EU project VALCRI under grant number
   FP7-SEC-2013-608142.
CR AGRAWAL R, 1995, PROC INT CONF DATA, P3, DOI 10.1109/ICDE.1995.380415
   [Anonymous], 1993, Political Science: The State of Discipline II
   Baker C.F., 1998, P 36 ANN M ASS COMP, P86, DOI [DOI 10.3115/980845.980860, DOI 10.3115/980451.980860]
   Bennell C, 2002, SCI JUSTICE, V42, P153, DOI 10.1016/S1355-0306(02)71820-0
   Bradel L, 2014, IEEE CONF VIS ANAL, P163, DOI 10.1109/VAST.2014.7042492
   Brandes U, 2007, LECT NOTES COMPUT SC, V4372, P42
   Canter DV, 2004, PSYCHOL PUBLIC POL L, V10, P293, DOI 10.1037/1076-8971.10.3.293
   Cope N, 2004, BRIT J CRIMINOL, V44, P188, DOI 10.1093/bjc/44.2.188
   Demiralp C, 2017, CORR
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Endert A., 2016, SYNTH LECT VIS, DOI [10.2200/S00730ED1V01Y201608VIS007, DOI 10.2200/S00730ED1V01Y201608VIS007]
   Endert A, 2012, IEEE T VIS COMPUT GR, V18, P2879, DOI 10.1109/TVCG.2012.260
   Endert Alex., 2012, P SIGCHI C HUM FACT, P473, DOI DOI 10.1145/2207676.2207741
   Gleicher M, 2013, IEEE T VIS COMPUT GR, V19, P2042, DOI 10.1109/TVCG.2013.157
   Gomariz Antonio, 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference, PAKDD 2013. Proceedings, P50, DOI 10.1007/978-3-642-37453-1_5
   Jäckle D, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 3, P164, DOI 10.5220/0006265101640175
   Jentner W., 2016, EVENT EVENT TEMPORAL
   Johansson J, 2017, IEEE COMPUT GRAPH, V37, P54, DOI 10.1109/MCG.2016.49
   Jurafsky D., 2009, SPEECH LANGUAGE PROC, V2nd
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   NPIA, 2008, NAT POL IMPR AG PROF
   Prowse J., 2000, WORKING MANUAL CRIMI
   Ruppert T., 2017, VISUALIZATION DATA A, P46
   Sacha D., 2017, EUROVIS WORKSH VIS A
   Sacha D, 2017, NEUROCOMPUTING, V268, P164, DOI 10.1016/j.neucom.2017.01.105
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   Saneifar H., 2008, AusDM, V87, P95
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   Stasko J, 2008, INFORM VISUAL, V7, P118, DOI 10.1057/palgrave.ivs.9500180
   Turkay C, 2011, IEEE T VIS COMPUT GR, V17, P2591, DOI 10.1109/TVCG.2011.178
   van der Corput P, 2016, COMPUT GRAPH FORUM, V35, P31, DOI 10.1111/cgf.12879
   Wenskovitch J, 2018, IEEE T VIS COMPUT GR, V24, P131, DOI 10.1109/TVCG.2017.2745258
   Wise JA, 1999, J AM SOC INFORM SCI, V50, P1224, DOI 10.1002/(SICI)1097-4571(1999)50:13<1224::AID-ASI8>3.0.CO;2-4
   Witten IH, 2011, MOR KAUF D, P1
   Wong B. L. W., 2017, TECHNICAL REPORT
   Xu K, 2015, IEEE COMPUT GRAPH, V35, P54, DOI 10.1109/MCG.2015.50
   Yan XF, 2003, SIAM PROC S, P166
   Yuan XR, 2013, IEEE T VIS COMPUT GR, V19, P2625, DOI 10.1109/TVCG.2013.150
   Zhang L., 2016, Mathematical Problems in Engineering, V2016, P1, DOI DOI 10.1089/NAT.2016.0626
NR 40
TC 10
Z9 11
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2018
VL 34
IS 9
BP 1225
EP 1241
DI 10.1007/s00371-018-1483-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GQ5MB
UT WOS:000441727000008
DA 2024-07-18
ER

PT J
AU Xie, NW
   Wang, LL
   Dutré, P
AF Xie, Naiwen
   Wang, Lili
   Dutre, Philip
TI Reflection reprojection using temporal coherence
SO VISUAL COMPUTER
LA English
DT Article
DE Specular reflections; View-dependent rendering; Temporal coherence
   rendering
ID GPU
AB A powerful approach for rendering high-quality images at low cost is to exploit temporal coherence by projecting already computed images into a novel view. However, conventional temporal coherence projection methods assume pixel values remain almost unchanged from frame to frame, which does not extend well to reflection rendering. We present a novel projection method to reuse reflections from adjacent frames. A novel reflection reprojection method is introduced to establish the mapping of reflections between individual frames. By reusing the information from the reference frame, our method can reduce the overall workloads of reflection computation, which makes rendering efficiently.
C1 [Xie, Naiwen] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Wang, Lili] Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
   [Dutre, Philip] Katholieke Univ Leuven, Leuven, Belgium.
C3 Beihang University; Beihang University; KU Leuven
RP Xie, NW (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM sienaiwun@gmail.com
RI wang, lili/HJP-8047-2023; Dutré, Philip LMJ/A-8716-2014
OI Dutre, Philip/0000-0002-9344-2523
FU National High Technology Research and Development Program of China
   through 863 Program [2013AA 01A604]; China Scholarship Council (CSC)
   [201506020037]; National Natural Science Foundation of China [61272349,
   61190121, 61190125]
FX This work was supported in part by the National Natural Science
   Foundation of China through Projects 61272349, 61190121 and 61190125, by
   the National High Technology Research and Development Program of China
   through 863 Program No. 2013AA 01A604. Naiwen Xie gratefully
   acknowledges financial support from China Scholarship Council (CSC)
   through No. 201506020037.
CR [Anonymous], EUR S REND ACM NEW Y
   [Anonymous], P 23 ANN C COMP GRAP
   [Anonymous], 2014, Journal of Computer Graphics Techniques (JCGT)
   [Anonymous], IMAGE
   Carr NA, 2006, PROC GRAPH INTERF, P203
   de Macedo DV, 2018, VISUAL COMPUT, V34, P337, DOI 10.1007/s00371-016-1335-8
   FOLEY T., 2005, HWWS 05, P15
   Ganestam P, 2015, VISUAL COMPUT, V31, P1395, DOI 10.1007/s00371-014-1021-7
   Glassner A. S., 1989, INTRO RAY TRACING
   Green P, 2007, COMPUT GRAPH FORUM, V26, P495, DOI 10.1111/j.1467-8659.2007.01072.x
   Heidrich W, 1999, SPRING EUROGRAP, P187
   Kautz J, 2000, PROC GRAPH INTERF, P119
   Laine S., 2007, P EUR S REND, P277, DOI DOI 10.2312/EGWR/EGSR07/277-286
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li SY, 2006, EUROGR TECH REP SER, P29
   Lochmann Gerrit, 2014, Real-time Reflective and Refractive Novel-view Synthesis, P9, DOI [DOI 10.2312/VMV, 10.2312/vmv.20141270, DOI 10.2312/VMV.20141270]
   Mattausch O, 2010, COMPUT GRAPH FORUM, V29, P2492, DOI 10.1111/j.1467-8659.2010.01784.x
   NEHAB D., 2007, GRAPHICS HARDWARE, V41, P61
   Ohtake Y, 2005, ACM SIGGRAPH 2005 CO, P173
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Popescu V, 2006, IEEE T VIS COMPUT GR, V12, P1590, DOI 10.1109/TVCG.2006.103
   Popescu V, 2006, COMPUT GRAPH FORUM, V25, P313, DOI 10.1111/j.1467-8659.2006.00950.x
   Purcell TJ, 2002, ACM T GRAPHIC, V21, P703, DOI 10.1145/566570.566640
   Scherzer D, 2009, LECT NOTES COMPUT SC, V5876, P13, DOI 10.1007/978-3-642-10520-3_2
   Scherzer Daniel, 2007, P 18 EUR C REND TECH, P45
   Schwarzler M., 2013, P ACM SIGGRAPH S INT, P79
   Sitthi-amorn P., 2008, ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware, P95
   Sitthi-Amorn P, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409080
   Szirmay-Kalos L, 2005, COMPUT GRAPH FORUM, V24, P695, DOI 10.1111/j.1467-8659.2005.0m894.x
   Taguchi Y, 2010, PROC CVPR IEEE, P499, DOI 10.1109/CVPR.2010.5540172
   Vardis K, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P171, DOI 10.1145/2856400.2856401
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   Wang LL, 2014, IEEE T VIS COMPUT GR, V20, P1316, DOI 10.1109/TVCG.2014.2314666
   Whitted T., 1979, P 6 ANN C COMPUTER G, VVolume 13, P14
   Yoon SE, 2006, VISUAL COMPUT, V22, P772, DOI 10.1007/s00371-006-0062-y
   Yu Jingyi., 2005, Proceedings of Symposium on Interactive 3D Graphics and Games (I3D), P133
   Yu X, 2008, COMPUT GRAPH FORUM, V27, P1987, DOI 10.1111/j.1467-8659.2008.01348.x
NR 37
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2018
VL 34
IS 4
BP 517
EP 529
DI 10.1007/s00371-017-1358-9
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FY6BJ
UT WOS:000426924400006
DA 2024-07-18
ER

PT J
AU Kikuuwe, R
AF Kikuuwe, Ryo
TI A time-integration method for stable simulation of extremely deformable
   hyperelastic objects
SO VISUAL COMPUTER
LA English
DT Article
DE QMR; Finite elements; Interactive simulation; Hyperelasticity
ID QMR
AB This paper presents a time integration method for realtime simulation of extremely deformable objects subject to geometrically nonlinear hyperelasticity. In the presented method, the equation of motion of the system is discretized by the backward Euler method, and linearly approximated through the first-order Taylor expansion. The approximate linear equation is solved with the quasi-minimal residual method (QMR), which is an iterative linear equation solver for non-symmetric or indefinite matrices. The solution is then corrected considering the nonlinear term that is omitted at the Taylor expansion. The method does not demand the constitutive law to guarantee the positive definiteness of the stiffness matrix. Experimental results show that the presented method realizes stable behavior of the simulated model under such deformation that the tetrahedral elements are almost flattened. It is also shown that QMR outperforms the biconjugate gradient stabilized method (BiCGStab) in this application.
C1 [Kikuuwe, Ryo] Kyushu Univ, Dept Mech Engn, Fukuoka 8190395, Japan.
C3 Kyushu University
RP Kikuuwe, R (corresponding author), Kyushu Univ, Dept Mech Engn, Fukuoka 8190395, Japan.
EM kikuuwe@ieee.org
RI Kikuuwe, Ryo/A-8129-2012
OI Kikuuwe, Ryo/0000-0002-1500-6777
FU Grants-in-Aid for Scientific Research [25540044] Funding Source: KAKEN
CR [Anonymous], EUROGRAPHICS
   [Anonymous], 2005, ACMEUROGRAPHICS S CO
   [Anonymous], 2012, Computer Animation 2012-ACM SIGGRAPH / Eurographics Symposium Proceedings, SCA
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   BARBIC J., 2012, TECH REP
   Barbic J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964986
   Bickel Bernd, 2006, P VISION MODELING VI, P209
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Chao I, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778775
   Civit-Flores O, 2014, COMPUT GRAPH FORUM, V33, P298, DOI 10.1111/cgf.12351
   Delingette H, 2008, LECT NOTES COMPUT SC, V5104, P40, DOI 10.1007/978-3-540-70521-5_5
   Desbrun M, 1999, PROC GRAPH INTERF, P1
   Fletcher R., 1976, Lecture Notes in Mathematics, P73, DOI DOI 10.1007/BFB0080116
   FREUND RW, 1994, SIAM J SCI COMPUT, V15, P313, DOI 10.1137/0915022
   FREUND RW, 1991, NUMER MATH, V60, P315, DOI 10.1007/BF01385726
   Hirota K, 2001, PRESENCE-TELEOP VIRT, V10, P525, DOI 10.1162/105474601753132696
   Irving G, 2006, GRAPH MODELS, V68, P66, DOI 10.1016/j.gmod.2005.03.007
   Kapre N, 2007, P S COMP ARITHM, P205, DOI 10.1109/ARITH.2007.25
   Kikuuwe R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477934
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Lloyd BA, 2007, IEEE T VIS COMPUT GR, V13, P1081, DOI 10.1109/TVCG.2007.1055
   McAdams A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964932
   Müller M, 2004, PROC GRAPH INTERF, P239
   Myronenko A., 2009, ARXIV09041613
   Nakao Megumi, 2006, J Med Syst, V30, P371, DOI 10.1007/s10916-006-9021-4
   Natsupakpong S, 2010, GRAPH MODELS, V72, P61, DOI 10.1016/j.gmod.2010.10.001
   Nienhuys Han-Wen., 2000, EUROGRAPH SHORT PRES, P43
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Parker E.G., 2009, PROC S COMP ANIM, P165, DOI DOI 10.1145/1599470.1599492.
   Patterson T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366216
   Peterlík I, 2010, COMPUT GRAPH-UK, V34, P43, DOI 10.1016/j.cag.2009.10.005
   Picinbono G, 2003, GRAPH MODELS, V65, P305, DOI 10.1016/S1524-0703(03)00045-6
   SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058
   Saad Y, 2003, ITERATIVE METHODS SP, DOI DOI 10.1137/1.9780898718003
   Saupin G., 2008, Computer Graphics International
   Schmedding R, 2008, VISUAL COMPUT, V24, P625, DOI 10.1007/s00371-008-0243-y
   Sin FS, 2013, COMPUT GRAPH FORUM, V32, P36, DOI 10.1111/j.1467-8659.2012.03230.x
   Song C, 2014, COMPUT GRAPH-UK, V40, P49, DOI 10.1016/j.cag.2014.01.003
   SONNEVELD P, 1989, SIAM J SCI STAT COMP, V10, P36, DOI 10.1137/0910004
   Stomakhin Alexey., 2012, Proc. Symp. Comp. Anim, P25
   VANDERVORST HA, 1992, SIAM J SCI STAT COMP, V13, P631, DOI 10.1137/0913035
   Wang H., 2015, ACM Trans. Graph, V34, p246:1
   Zhong Hualiang, 2005, Computer Methods in Biomechanics and Biomedical Engineering, V8, P177, DOI 10.1080/10255840500295852
NR 45
TC 0
Z9 0
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2017
VL 33
IS 10
BP 1335
EP 1346
DI 10.1007/s00371-016-1225-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FF8VN
UT WOS:000409296000010
DA 2024-07-18
ER

PT J
AU Zhao, LJ
   Zhao, QJ
   Liu, H
   Lv, P
   Gu, DB
AF Zhao, Liujun
   Zhao, Qingjie
   Liu, Hao
   Lv, Peng
   Gu, Dongbing
TI Structural sparse representation-based semi-supervised learning and edge
   detection proposal for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Structural sparse representation; Semi-supervised learning; Edge
   detection proposal; Object tracking
ID ROBUST OBJECT TRACKING; APPEARANCE MODEL; FRAMEWORK; COLOR
AB In discriminative tracking, lots of tracking methods easily suffer from changes of pose, illumination and occlusion. To deal with this problem, we propose a novel object tracking method using structural sparse representation-based semi-supervised learning and edge detection. First, the object appearance model is constructed by extracting sparse code features on different layers to exploit local information and holistic information. To utilize unlabelled samples information, the semi-supervised learning is introduced and a classifier is trained which is used to measure candidates. In addition, an auxiliary positive sample set is maintained to improve the performance of the classifier. We subsequently adopt an edge detection to alleviate the error accumulation based on the ranking results from the learned classifier. Finally, the proposed method is implemented under theBayesian inference framework. Both the proposed tracker and several current trackers are tested on some challenging videos, where the target objects undergo pose change, illumination and occlusion. The experimental results demonstrate that the proposed tracker outperforms the other state-of-the art methods in terms of effectiveness and robustness.
C1 [Zhao, Liujun; Zhao, Qingjie; Liu, Hao; Lv, Peng] Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Gu, Dongbing] Univ Essex, Sch Comp Sci & Elect Engn, Wivenhoe Pk, Colchester CO4 3SQ, Essex, England.
C3 Beijing Institute of Technology; University of Essex
RP Zhao, LJ (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM zhaolj110@126.com
FU National Natural Science Foundation of China [61175096, 61300082];
   Specialized Fund for Joint Building Program of Beijing Municipal
   Education Commission; Liaoning Natural Science Foundation [2015020015]
FX This research is supported by the National Natural Science Foundation of
   China (No. 61175096, No. 61300082), Specialized Fund for Joint Building
   Program of Beijing Municipal Education Commission, and Liaoning Natural
   Science Foundation (2015020015). The authors would like to thank the
   anonymous editor and reviewers who gave valuable suggestions that have
   helped to improve the quality of the manuscript.
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2015, PROC CVPR IEEE
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bai YC, 2012, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2012.6247884
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Gao J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P145, DOI 10.1109/ICCVW.2013.25
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Hong ZB, 2013, IEEE I CONF COMP VIS, P649, DOI 10.1109/ICCV.2013.86
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jiang N, 2011, IEEE T IMAGE PROCESS, V20, P2288, DOI 10.1109/TIP.2011.2114895
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Khanloo BYS, 2012, COMPUT VIS IMAGE UND, V116, P676, DOI 10.1016/j.cviu.2012.01.004
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Leichter I, 2009, IEEE T PATTERN ANAL, V31, P164, DOI 10.1109/TPAMI.2008.194
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li ZY, 2015, VISUAL COMPUT, V31, P1319, DOI 10.1007/s00371-014-1014-6
   Lin L, 2012, IEEE T IMAGE PROCESS, V21, P4844, DOI 10.1109/TIP.2012.2211373
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu XB, 2011, IEEE T CIRC SYST VID, V21, P1588, DOI 10.1109/TCSVT.2011.2129410
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Ning JF, 2009, INT J PATTERN RECOGN, V23, P1245, DOI 10.1142/S0218001409007624
   Paragios N, 2000, IEEE T PATTERN ANAL, V22, P266, DOI 10.1109/34.841758
   Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Tsagkatakis G, 2011, IEEE T CIRC SYST VID, V21, P1810, DOI 10.1109/TCSVT.2011.2133970
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani N, 2010, IEEE T IMAGE PROCESS, V19, P841, DOI 10.1109/TIP.2009.2037465
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YX, 2015, VISUAL COMPUT, V31, P471, DOI 10.1007/s00371-014-0942-5
   Wu YW, 2014, IEEE T CIRC SYST VID, V24, P865, DOI 10.1109/TCSVT.2013.2291283
   Zeisl B, 2010, PROC CVPR IEEE, P1879, DOI 10.1109/CVPR.2010.5539860
   Zha YF, 2010, PATTERN RECOGN, V43, P187, DOI 10.1016/j.patcog.2009.06.011
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang Z, 2014, PROC CVPR IEEE, P1226, DOI 10.1109/CVPR.2014.160
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 45
TC 9
Z9 9
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2017
VL 33
IS 9
BP 1169
EP 1184
DI 10.1007/s00371-016-1279-z
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD1CS
UT WOS:000407275600007
DA 2024-07-18
ER

PT J
AU Kruijff, E
   Marquardt, A
   Trepkowski, C
   Schild, J
   Hinkenjann, A
AF Kruijff, Ernst
   Marquardt, Alexander
   Trepkowski, Christina
   Schild, Jonas
   Hinkenjann, Andre
TI Designed emotions: challenges and potential methodologies for improving
   multisensory cues to enhance user engagement in immersive systems
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Immersion; Games; Presence; Multisensory cues; 3D user
   interfaces; User engagement; Challenges; Methodologies
ID VIRTUAL-REALITY; ENVIRONMENTS; PERCEPTION; VISION; SENSE; AWE
AB In this article, we report on challenges and potential methodologies to support the design and validation of multisensory techniques. Such techniques can be used for enhancing engagement in immersive systems. Yet, designing effective techniques requires careful analysis of the effect of different cues on user engagement. The level of engagement spans the general level of presence in an environment, as well as the specific emotional response to a set trigger. Yet, measuring and analyzing the actual effect of cues is hard as it spans numerous interconnected issues. In this article, we identify the different challenges and potential validation methodologies that affect the analysis of multisensory cues on user engagement. In doing so, we provide an overview of issues and potential validation directions as an entry point for further research. The various challenges are supported by lessons learned from a pilot study, which focused on reflecting the initial validation methodology by analyzing the effect of different stimuli on user engagement.
C1 [Kruijff, Ernst; Marquardt, Alexander; Trepkowski, Christina; Schild, Jonas; Hinkenjann, Andre] Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, St Augustin, Germany.
C3 Hochschule Bonn Rhein Sieg
RP Kruijff, E (corresponding author), Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, St Augustin, Germany.
EM ernst.kruijff@h-brs.de
OI Schild, Jonas/0000-0002-1931-9359
CR Afonso C., 2011, AM 11 P 6 AUDIO MOST, P101
   [Anonymous], 2005, CHI '05: Proceedings of the SIGCHI conference on Human factors in computing systems, DOI [10.1145/1054972.1055040, DOI 10.1145/1054972.1055040]
   Ark W., 1999, Human-Computer Interaction: Ergonomics and User Interfaces. Proceedings of HCI International '99 (8th International Conference on Human-Computer Interaction), P818
   Asteriadis S, 2009, MULTIMED TOOLS APPL, V41, P469, DOI 10.1007/s11042-008-0240-1
   Attfield S., 2011, P WSDM WORK US MOD W
   Barfield W., 1995, VIRTUAL ENV ADV INTE, P473, DOI [DOI 10.1093/OSO/9780195075557.001.0001, 10.1093/oso/9780195075557.001.0001]
   Bateman C., 2011, DIGRA 11 P 2011 DIGR, V6
   Beckhaus S., 2004, ACM SIGGRAPH 2004 Course Notes (SIGGRAPH '04)
   Bell J., 2012, P 2 INT WORKSH GAM S
   Bernhaupt R, 2010, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-1-84882-963-3
   Bersak D., 2001, P UBICOMP
   Biocca F., 1995, COMMUNICATION AGE VI
   Blake R, 2004, PSYCHOL SCI, V15, P397, DOI 10.1111/j.0956-7976.2004.00691.x
   Bowman D.A., 2005, 3D User Interfaces: Theory and Practice
   Bradley MM, 2000, PSYCHOPHYSIOLOGY, V37, P204, DOI 10.1017/S0048577200990012
   Bradley MM, 1996, PSYCHOPHYSIOLOGY, V33, P662, DOI 10.1111/j.1469-8986.1996.tb02362.x
   Bresciani JP, 2005, EXP BRAIN RES, V162, P172, DOI 10.1007/s00221-004-2128-2
   Brogni A., 2006, Proceedings of the ACM symposium on Virtual reality software and technology - VRST 06, DOI [DOI 10.1145/1180495.1180572, 10.1145/1180495. 1180572]
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Brown E., 2004, CHI 04 HUM FACT COMP, P1297, DOI DOI 10.1145/985921.986048
   Buchenau M., 2000, DIS2000. Designing Interactive Systems Processes, Practices, Methods, and Techniques. Conference Proceedings, P424, DOI 10.1145/347642.347802
   Calvert GA, 2004, J PHYSIOL-PARIS, V98, P191, DOI 10.1016/j.jphysparis.2004.03.018
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Csikszentmihalyi M., 1990, Flow: The psychology of optimal experience
   D'Mello SK, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682899
   Dillon C, 2001, 4 INT WKSHP PRES
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Dow StevenP., 2008, Understanding user engagement in immersive and interactive stories
   Eidenberger H., 2015, P 21 ACM S VIRT REAL, P9, DOI [10.1145/2821592.2821612, DOI 10.1145/2821592.2821612]
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Ellis SR, 1996, PRESENCE-TELEOP VIRT, V5, P247, DOI 10.1162/pres.1996.5.2.247
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Feng M, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P95, DOI 10.1109/3DUI.2016.7460037
   FORLIZZI J, 2004, P 5 C DES INT SYST P, DOI [10.1145/1013115.1013152, DOI 10.1145/1013115.1013152]
   Fu YJ, 2014, 2014 38TH ANNUAL IEEE INTERNATIONAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE WORKSHOPS (COMPSACW 2014), P258, DOI 10.1109/COMPSACW.2014.46
   Goldstein E.B., 2002, Sensation and perception, V6ieme
   Greenwald M. K., 1989, Journal of Psychophysiology, V3, P51
   Harris L, 1999, P IEEE VIRT REAL ANN, P229, DOI 10.1109/VR.1999.756956
   Held R. M., 1992, Presence: Teleoperators and Virtual Environments, V1, P109, DOI [https://doi.org/10.1162/pres.1992.1.1.109, 10.1162/pres.1992.1.1.109, DOI 10.1162/PRES.1992.1.1.109]
   Hoffman HG, 1998, P IEEE VIRT REAL ANN, P59, DOI 10.1109/VRAIS.1998.658423
   Hoggan E., 2008, PROC INT C MULTIMODA, P157
   Hulsmann F., 2014, P VIRT REAL INT C, P1
   IJsselsteijn W.A., 2013, The game experience questionnaire
   IJsselsteijn WA., P SPIE, V3959, P520
   Israr A., 2011, ACM SIGGRAPH 2011 EM, P1
   Izard C.E., 1972, PATTERNS OF EMOTIONS
   Jacques R., 1995, Canadian Journal of Educational Communication, V24, P49
   Jang DP, 2002, CYBERPSYCHOL BEHAV, V5, P11, DOI 10.1089/109493102753685845
   Jang EH, 2015, J PHYSIOL ANTHROPOL, V34, DOI 10.1186/s40101-015-0063-5
   Kaye J., 1999, THESIS
   Kruijff E., 2005, P IEEE VR 2005 WORKS, P37
   Kruijff E, 2007, THESIS
   Kruijff E, 2013, PROCEEDINGS OF 2013 23RD INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE (ICAT 2013), P129
   Kuppens P, 2013, PSYCHOL BULL, V139, P917, DOI 10.1037/a0030811
   Lalmas M., 2014, Synthesis Lectures on Information Concepts, Retrieval, and Services, V6, P1, DOI [DOI 10.2200/S00605ED1V01Y201410ICR038, 10.2200/s00605ed1v01y201410icr038]
   LEDERMAN SJ, 1986, J EXP PSYCHOL HUMAN, V12, P169, DOI 10.1037/0096-1523.12.2.169
   Lehmann J., 2012, USER MODELING ADAPTA, V7379
   Lenay C., 2003, TOUCHING KNOWING, P275, DOI DOI 10.1075/AICR.53.22LEN
   Lindeman RW, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P175
   Lombard M., 2006, J. Comput. Mediat. Commun, V3, P72, DOI [DOI 10.1111/J.1083-6101.1997.TB00072.X, https://doi.org/10.1111/j.1083-6101.1997.tb00072.x]
   Loomis J, 2002, CONVERGING TECHNOLOG, P220
   McMahan RP, 2012, IEEE T VIS COMPUT GR, V18, P626, DOI 10.1109/TVCG.2012.43
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Mi Feng, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P149, DOI 10.1109/3DUI.2015.7131744
   Mine Mark., 2003, EGVE '03, P11
   MORGAN RL, 1988, SOC PSYCHOL QUART, V51, P19, DOI 10.2307/2786981
   Mueller F.F., 2012, Proceedings of CHI'12, P1853, DOI [10.1145/2207676, DOI 10.1145/2207676, 10.1145/2207676.2208322, DOI 10.1145/2207676.2208322]
   Nacke L., 2008, P 2008 C FUT PLAY, P81
   Naef M., 2002, VIRTUAL REALITY SOFT, P65
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Narumi Takuji, 2010, SIGGRAPH 10 ACM SIGG
   Nielsen J., 1993, USABILITY ENG
   O'Brien HL, 2013, INFORM PROCESS MANAG, V49, P1092, DOI 10.1016/j.ipm.2012.08.005
   PADDAN GS, 1988, J BIOMECH, V21, P191, DOI 10.1016/0021-9290(88)90169-8
   Pai D., 2003, P INT S ROB RES, V15, P489
   Park CH, 2002, P IEEE VIRT REAL ANN, P269, DOI 10.1109/VR.2002.996532
   Partala T., 2009, AFFECTIVE INFORM HUM
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Picard R.W., 2001, INT C HUMAN COMPUTER, P1538
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Pugnetti L, 2001, PRESENCE-VIRTUAL AUG, V10, P384, DOI 10.1162/1054746011470244
   Pulkki V., 2001, THESIS
   Ramic-Brkic B, 2014, ACM T APPL PERCEPT, V11, DOI 10.1145/2617917
   Ranasinghe N., 2013, Proceedings of the 2013 ACM International Workshop on Immersive Media Experiences, P29, DOI DOI 10.1145/2512142.2512148
   Regenbrecht HT, 1998, INT J HUM-COMPUT INT, V10, P233, DOI 10.1207/s15327590ijhc1003_2
   Rizzo A, 2005, PRESENCE-TELEOP VIRT, V14, P119, DOI 10.1162/1054746053967094
   Rudd M, 2012, PSYCHOL SCI, V23, P1130, DOI 10.1177/0956797612438731
   Ryan RM, 2006, MOTIV EMOTION, V30, P347, DOI 10.1007/s11031-006-9051-8
   Scheirer J, 2002, INTERACT COMPUT, V14, P93, DOI 10.1016/S0953-5438(01)00059-5
   Schell J, 2001, IEEE COMPUT GRAPH, V21, P11, DOI 10.1109/38.933519
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Schild J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P207, DOI 10.1145/2556288.2557283
   Schild J, 2015, PROC SPIE, V9391, DOI 10.1117/12.2083190
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Schubert T., 1999, VISUAL REPRESENTATIO, P269, DOI [DOI 10.1007/978-1-4471-0563-3_30, 10.1007/978-1-4471-0563-3_30]
   Sekuler R, 1997, NATURE, V385, P308, DOI 10.1038/385308a0
   Shimojo S, 2001, CURR OPIN NEUROBIOL, V11, P505, DOI 10.1016/S0959-4388(00)00241-5
   Shiota MN, 2007, COGNITION EMOTION, V21, P944, DOI 10.1080/02699930600923668
   Slater M., 1996, VRST'96. Proceedings of the ACM Symposium on Virtual Reality and Technology, P163
   Slater M., 1994, PRESENCE-TELEOP VIRT, V3, P130, DOI DOI 10.1162/PRES.1994.3.2.130
   Slater Mel, 2003, PRES 2003 6 ANN INT, V157
   Spence C, 2003, CURR BIOL, V13, pR519, DOI 10.1016/S0960-9822(03)00445-7
   Spence C., 2015, Flavour, V4, P30, DOI DOI 10.1186/S13411-015-0040-2
   Steffin M., 2005, EMED J
   STEUER J, 1992, J COMMUN, V42, P73, DOI 10.1111/j.1460-2466.1992.tb00812.x
   Sutcliffe Alistair., 2003, Multimedia and virtual reality: designing multisensory user interfaces
   Sweetser P, 2005, COMPUTERS ENTERTAINM, V3, P3, DOI [10.1145/1077246.1077253, DOI 10.1145/1077246.1077253]
   Sykes J., 2003, Conference on Human Factors in Computing Systems, P732, DOI DOI 10.1145/765891.765957
   Tsingos N, 2004, ACM T GRAPHIC, V23, P249, DOI 10.1145/1015706.1015710
   Usoh M, 2000, PRESENCE-TELEOP VIRT, V9, P497, DOI 10.1162/105474600566989
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Weisenberger JM, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P161, DOI 10.1109/HAPTIC.2004.1287192
   Wiederhold BK, 2005, APPL PSYCHOPHYS BIOF, V30, P183, DOI 10.1007/s10484-005-6375-1
   Wiederhold BK, 2002, CYBERPSYCHOL BEHAV, V5, P77, DOI 10.1089/109493102753685908
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yanagida Y., 2003, P ACM SIGGRAPH SKETC
NR 116
TC 11
Z9 13
U1 0
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2017
VL 33
IS 4
BP 471
EP 488
DI 10.1007/s00371-016-1294-0
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ER4JZ
UT WOS:000398767100007
DA 2024-07-18
ER

PT J
AU Kot, M
   Nagahashi, H
AF Kot, Maciej
   Nagahashi, Hiroshi
TI Mass spring models with adjustable Poisson's ratio
SO VISUAL COMPUTER
LA English
DT Article
DE Mass spring model; Soft body deformation; Physically based modeling
AB In this paper we show how to construct mass spring models for the representation of homogeneous isotropic elastic materials with adjustable Poisson's ratio. Classical formulation of elasticity on mass spring models leads to the result, that while materials with any value of Young's modulus can be modeled reliably, only fixed value of Poisson's ratio is possible. We show how to extend the conventional model to overcome this limitation. The technique is demonstrated on cubic lattice as well as disordered networks.
C1 [Kot, Maciej; Nagahashi, Hiroshi] Tokyo Inst Technol, Imaging Sci & Engn Lab, Tokyo, Japan.
C3 Tokyo Institute of Technology
RP Kot, M (corresponding author), Tokyo Inst Technol, Imaging Sci & Engn Lab, Tokyo, Japan.
EM eustachy@gmail.com
FU JSPS KAKENHI [24300035]
FX Authors acknowledge the support of JSPS KAKENHI (Grant Number 24300035).
CR [Anonymous], 1906, TREATISE MATH THEORY, DOI DOI 10.1038/074074A0
   Baudet V., 2007, RRLIRIS2007004 CNRSI
   HARDY RJ, 1982, J CHEM PHYS, V76, P622, DOI 10.1063/1.442714
   Kot M, 2015, VISUAL COMPUT, V31, P1339, DOI 10.1007/s00371-014-1015-5
   Ladd AJC, 1997, PHYSICA A, V240, P349, DOI 10.1016/S0378-4371(97)00158-1
   LAKES R, 1991, J MATER SCI, V26, P2287, DOI 10.1007/BF01130170
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Nealen A, 2006, COMPUT GRAPH FORUM, V25, P809, DOI 10.1111/j.1467-8659.2006.01000.x
   OSTOJA-STARZEWSKI M., 2002, Appl. Mech. Rev., V55, P35, DOI [10.1115/1.1432990, DOI 10.1115/1.1432990]
   Press W. H., 2007, NUM REC ART SCI COMP
   Sahimi M., 2003, INTERDISCIPLINARY AP, V1
   San-Vicente G, 2012, IEEE T VIS COMPUT GR, V18, P228, DOI 10.1109/TVCG.2011.32
   Van Gelder A., 1998, Journal of Graphics Tools, V3, P21, DOI 10.1080/10867651.1998.10487490
   Zhao GF, 2011, INT J NUMER ANAL MET, V35, P859, DOI 10.1002/nag.930
   Zimmerman JA, 2004, MODEL SIMUL MATER SC, V12, pS319, DOI 10.1088/0965-0393/12/4/S03
NR 15
TC 17
Z9 18
U1 1
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 283
EP 291
DI 10.1007/s00371-015-1194-8
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300003
DA 2024-07-18
ER

PT J
AU Kumar, D
   Bezdek, JC
   Rajasegarar, S
   Leckie, C
   Palaniswami, M
AF Kumar, Dheeraj
   Bezdek, James C.
   Rajasegarar, Sutharshan
   Leckie, Christopher
   Palaniswami, Marimuthu
TI A visual-numeric approach to clustering and anomaly detection for
   trajectory data
SO VISUAL COMPUTER
LA English
DT Article
DE Trajectory clustering; Anomaly detection; ClusiVAT hierarchical
   clustering; MIT trajectory dataset
ID TRACKING
AB This paper proposes a novel application of Visual Assessment of Tendency (VAT)-based hierarchical clustering algorithms (VAT, iVAT, and clusiVAT) for trajectory analysis. We introduce a new clustering based anomaly detection framework named iVAT+ and clusiVAT+ and use it for trajectory anomaly detection. This approach is based on partitioning the VAT-generated Minimum Spanning Tree based on an efficient thresholding scheme. The trajectories are classified as normal or anomalous based on the number of paths in the clusters. On synthetic datasets with fixed and variable numbers of clusters and anomalies, we achieve 98 % classification accuracy. Our two-stage clusiVAT method is applied to 26,039 trajectories of vehicles and pedestrians from a parking lot scene from the real life MIT trajectories dataset. The first stage clusters the trajectories ignoring directionality. The second stage divides the clusters obtained from the first stage by considering trajectory direction. We show that our novel two-stage clusiVAT approach can produce natural and informative trajectory clusters on this real life dataset while finding representative anomalies.
C1 [Kumar, Dheeraj; Palaniswami, Marimuthu] Univ Melbourne, Dept Elect & Elect Engn, Melbourne, Vic, Australia.
   [Bezdek, James C.; Rajasegarar, Sutharshan; Leckie, Christopher] Univ Melbourne, Dept Comp & Informat Syst, Melbourne, Vic, Australia.
   [Rajasegarar, Sutharshan; Leckie, Christopher] Natl ICT Australia, Melbourne, Vic, Australia.
C3 University of Melbourne; University of Melbourne; NICTA
RP Kumar, D (corresponding author), Univ Melbourne, Dept Elect & Elect Engn, Melbourne, Vic, Australia.
EM dheerajk@student.unimelb.edu.au; jbezdek@unimelb.edu.au;
   sraja@unimelb.edu.au; caleckie@unimelb.edu.au; palani@unimelb.edu.au
RI Kumar, Dheeraj/JNI-6342-2023; Palaniswami, Marimuthu/AAE-2179-2022;
   Rajasegarar, Sutharshan/AAW-4727-2021
OI Rajasegarar, Sutharshan/0000-0002-6559-6736; D, SATHEESH
   KUMAR/0000-0002-3808-9595; Kumar, Dheeraj/0000-0002-5006-8471;
   Palaniswami, Marimuthu Swami/0000-0002-3635-4252; LECKIE,
   CHRISTOPHER/0000-0002-4388-0517
FU Australian Research Council (ARC) Linkage Project grant [LP120100529];
   ARC Linkage Infrastructure, Equipment and Facilities scheme (LIEF) grant
   [LF120100129]; EU-FP7 SOCIOTAL grant; National ICT Australia (NICTA)
FX We acknowledge the support from the Australian Research Council (ARC)
   Linkage Project grant (LP120100529), the ARC Linkage Infrastructure,
   Equipment and Facilities scheme (LIEF) grant (LF120100129), the EU-FP7
   SOCIOTAL grant and National ICT Australia (NICTA).
CR Ali I, 2012, IMAGE VISION COMPUT, V30, P966, DOI 10.1016/j.imavis.2012.08.013
   [Anonymous], P 2008 IEEE COMP SOC, DOI DOI 10.1109/CVPR.2008.4587718
   [Anonymous], 2015, 2015 IEEE INT C EL C
   Arandjelovic O, 2012, COMPUTER AND INFORMATION SCIENCES II, P403, DOI 10.1007/978-1-4471-2155-8_51
   Benezeth Y, 2009, PROC CVPR IEEE, P2450
   Bezdek JC, 2002, IEEE IJCNN, P2225, DOI 10.1109/IJCNN.2002.1007487
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Bousetouane F, 2013, VISUAL COMPUT, V29, P155, DOI 10.1007/s00371-012-0677-0
   Brun Luc, 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P709
   Chan AB, 2011, MACH VISION APPL, V22, P751, DOI 10.1007/s00138-010-0262-3
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Pham DS, 2015, IEEE T IMAGE PROCESS, V24, P332, DOI 10.1109/TIP.2014.2378034
   Fu ZY, 2005, IEEE IMAGE PROC, P2029
   Hathaway RJ, 2006, PATTERN RECOGN, V39, P1315, DOI 10.1016/j.patcog.2006.02.011
   Havens TC, 2012, IEEE T KNOWL DATA EN, V24, P813, DOI 10.1109/TKDE.2011.33
   Hoare C.A.R., 1961, Communications of the ACM, V4, P321, DOI [DOI 10.1145/366622.366647, DOI 10.1145/366622.366644]
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Jiang F, 2009, IEEE T IMAGE PROCESS, V18, P907, DOI 10.1109/TIP.2008.2012070
   Keogh E, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P226, DOI 10.1109/ICDM.2005.79
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kumar Dheeraj, 2013, 2013 IEEE International Conference on Big Data, P112, DOI 10.1109/BigData.2013.6691561
   Laxhammar R., 2011, INF FUS FUSION 2011, P1
   Laxhammar R, 2014, IEEE T PATTERN ANAL, V36, P1158, DOI 10.1109/TPAMI.2013.172
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Li X., 2007, SIAM INT C DAT MIN
   Martin R, 2010, LECT NOTES COMPUT SC, V6455, P89
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Morris B, 2009, PROC CVPR IEEE, P312, DOI 10.1109/CVPRW.2009.5206559
   Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109
   Naftel A, 2006, MULTIMEDIA SYST, V12, P227, DOI [10.1007/s00530-006-0058-5, 10.1007/S00530-006-0058-5]
   Piciarelli C, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P153, DOI 10.1109/AVSS.2007.4425302
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rao YB, 2015, VISUAL COMPUT, V31, P271, DOI 10.1007/s00371-013-0917-y
   Rodriguez M., 2011, INT C COMP VIS ICCV
   Roshtkhari MJ, 2013, PROC CVPR IEEE, P2611, DOI 10.1109/CVPR.2013.337
   Saleemi I, 2009, IEEE T PATTERN ANAL, V31, P1472, DOI 10.1109/TPAMI.2008.175
   Saligrama V, 2010, IEEE SIGNAL PROC MAG, V27, P18, DOI 10.1109/MSP.2010.937393
   Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang XG, 2011, INT J COMPUT VISION, V95, P287, DOI 10.1007/s11263-011-0459-6
   Xinyi Cui, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3161, DOI 10.1109/CVPR.2011.5995558
   Zhou Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1087
NR 44
TC 39
Z9 44
U1 1
U2 31
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 265
EP 281
DI 10.1007/s00371-015-1192-x
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300002
DA 2024-07-18
ER

PT J
AU Javaran, TA
   Hassanpour, H
   Abolghasemi, V
AF Javaran, Taiebeh Askari
   Hassanpour, Hamid
   Abolghasemi, Vahid
TI Automatic estimation and segmentation of partial blur in natural images
SO VISUAL COMPUTER
LA English
DT Article
DE Partial blur; Blur metric; Blur map; Blur/nonblur region segmentation;
   Pixon
ID LOW-DEPTH; CAMERA
AB Digital images may contain undesired blurred regions. Automatic detection of such regions and estimation of the amount of blurriness in a given image are important issues in many computer vision applications. This paper presents a simple and effective method to automatically detect blurred regions. The proposed method consists of two main parts. First, a novel blur metric, which can significantly distinguish blur and non-blur regions, is proposed. This metric is then used to generate a blur map to encode the amount of blurriness for individual pixels in a given image. Finally, the estimated blur map is used to segment the input image into blurred/non-blurred regions by applying a pixon-based technique. The proposed approach is evaluated for out-of-focus and motion-blurred natural images. By conducting experiments on a large dataset containing real images with defocus blur and partial motion blur regions, qualitative and quantitative measures are performed. The obtained results in this paper show that the proposed approach outperforms the state-of-the-art methods for blur estimation in digital images.
C1 [Javaran, Taiebeh Askari; Hassanpour, Hamid] Univ Shahrood, Fac Comp Engn & Informat Technol, IPDM Res Lab, Shahrood, Iran.
   [Abolghasemi, Vahid] Univ Shahrood, Fac Elect Engn & Robot, Shahrood, Iran.
RP Javaran, TA (corresponding author), Univ Shahrood, Fac Comp Engn & Informat Technol, IPDM Res Lab, Shahrood, Iran.
EM t.askari@shahroodut.ac.ir
RI Abolghasemi, Vahid/AAC-8242-2020; Hassanpour, Hamid/AAL-7271-2020
OI Hassanpour, Hamid/0000-0002-5513-9822; Abolghasemi,
   Vahid/0000-0002-2151-5180
CR [Anonymous], 2014, DISCRIMINATIVE BLUR
   [Anonymous], 2008, PROC CVPR IEEE
   Bahrami K, 2013, IEEE INT WORKS INFOR, P144, DOI 10.1109/WIFS.2013.6707809
   Chakrabarti A, 2010, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2010.5539954
   Chen XG, 2010, IEEE IMAGE PROC, P2533, DOI 10.1109/ICIP.2010.5652935
   Chetouani A, 2009, LECT NOTES COMPUT SC, V5879, P1185, DOI 10.1007/978-3-642-10467-1_120
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Da Rugna J., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5304, P285, DOI 10.1117/12.526949
   Dai SY, 2008, IEEE IMAGE PROC, P661, DOI 10.1109/ICIP.2008.4711841
   Fauzi M.F., 2003, P BRIT MACHINE C, P519
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Graf F., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2861, DOI 10.1109/ICIP.2011.6116145
   Kim CG, 2005, IEEE T IMAGE PROCESS, V14, P1503, DOI 10.1109/TIP.2005.846030
   Kovács L, 2007, IEEE T PATTERN ANAL, V29, P1080, DOI 10.1109/TPAMI.2007.1079
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Levin A., 2006, P NEURAL INFORM PROC, V19, P841
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Li HL, 2007, IEEE T CIRC SYST VID, V17, P1742, DOI 10.1109/TCSVT.2007.903326
   Liu SG, 2015, VISUAL COMPUT, V31, P733, DOI 10.1007/s00371-014-0998-2
   Mavridaki E, 2014, IEEE IMAGE PROC, P566, DOI 10.1109/ICIP.2014.7025113
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Su B, 2011, Proceedings of the 19th ACM International Conference on Multimedia. MM'11, DOI [DOI 10.1145/2072298.2072024, DOI 10.5555/1785794.1785825]
   Tai YW, 2009, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2009.5414620
   Tai YW, 2010, IEEE T PATTERN ANAL, V32, P1012, DOI 10.1109/TPAMI.2009.97
   Wang W, 2014, SIGNAL IMAGE VIDEO P, V8, P647, DOI 10.1007/s11760-013-0573-8
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yang FG, 2003, IEEE T IMAGE PROCESS, V12, P1552, DOI 10.1109/TIP.2003.817242
   Zhu X, 2013, IEEE T IMAGE PROCESS, V22, P4879, DOI 10.1109/TIP.2013.2279316
   Zhuo SJ, 2011, PATTERN RECOGN, V44, P1852, DOI 10.1016/j.patcog.2011.03.009
NR 31
TC 17
Z9 19
U1 0
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2017
VL 33
IS 2
BP 151
EP 161
DI 10.1007/s00371-015-1166-z
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI2TF
UT WOS:000392340400004
DA 2024-07-18
ER

PT J
AU Baldacci, A
   Bernabei, D
   Corsini, M
   Ganovelli, F
   Scopigno, R
AF Baldacci, Andrea
   Bernabei, Daniele
   Corsini, Massimiliano
   Ganovelli, Fabio
   Scopigno, Roberto
TI 3D reconstruction for featureless scenes with curvature hints
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based reconstruction; Image-based modeling,surface reconstruction;
   Depth maps fusion; Energy minimization on the GPU
AB We present a novel interactive framework for improving 3D reconstruction starting from incomplete or noisy results obtained through image-based reconstruction algorithms. The core idea is to enable the user to provide localized hints on the curvature of the surface, which are turned into constraints during an energy minimization reconstruction. To make this task simple, we propose two algorithms. The first is a multi-view segmentation algorithm that allows the user to propagate the foreground selection of one or more images both to all the images of the input set and to the 3D points, to accurately select the part of the scene to be reconstructed. The second is a fast GPU-based algorithm for the reconstruction of smooth surfaces from multiple views, which incorporates the hints provided by the user. We show that our framework can turn a poor-quality reconstruction produced with state of the art image-based reconstruction methods into a high- quality one.
C1 [Baldacci, Andrea; Bernabei, Daniele; Corsini, Massimiliano; Ganovelli, Fabio] ISTI CNR, Visual Comp Lab, Pisa, Italy.
   [Scopigno, Roberto] ISTI CNR, Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); Consiglio
   Nazionale delle Ricerche (CNR); Istituto di Scienza e Tecnologie
   dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP Corsini, M (corresponding author), ISTI CNR, Visual Comp Lab, Pisa, Italy.
EM andrea.baldacci@isti.cnr.it; daniele.bernabei@isti.cnr.it;
   massimiliano.corsini@isti.cnr.it; fabio.ganovelli@isti.cnr.it;
   roberto.scopigno@isti.cnr.it
RI Corsini, Massimiliano/B-6375-2015; scopigno, roberto/AAH-7645-2020
OI Corsini, Massimiliano/0000-0003-0543-1638; 
FU EU [323567]
FX The research leading to these results was funded by EU FP7 project ICT
   FET Harvest4D (http://www.harvest4d.org/, G.A. no. 323567). The Museum
   Dataset is courtesy of Chaurasia et al. [45].
CR Adarsh Kowdle S. N. S., 2012, EUR C COMP VIS ECCV
   Alexe B., 2010, ECCV2010, P8
   [Anonymous], 2000, SOC IND APPL MATH
   Bao SY, 2013, PROC CVPR IEEE, P1264, DOI 10.1109/CVPR.2013.167
   BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141
   Bleyer M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3081, DOI 10.1109/CVPR.2011.5995581
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bradley D., 2008, CVPR IEEE COMPUTER S, DOI [10.1109/CVPR.2008.4587792, DOI 10.1109/CVPR.2008.4587792]
   Campbell N. D. F., 2011, 2011 Conference for Visual Media Production, P126, DOI 10.1109/CVMP.2011.21
   Campbell Neill DF, 2008, EUR C COMP VIS, P766
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Djelouah A, 2013, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2013.328
   Djelouah A, 2012, LECT NOTES COMPUT SC, V7576, P818, DOI 10.1007/978-3-642-33715-4_59
   Freedman D, 2005, PROC CVPR IEEE, P755
   Furukawa M, 2010, IEEE SYS MAN CYBERN
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Furukawa Y, 2009, PROC CVPR IEEE, P1422, DOI 10.1109/CVPRW.2009.5206867
   Gallup D, 2010, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2010.5539804
   Goesele M., 2006, COMP VIS PATT REC 20, P2402, DOI DOI 10.1109/CVPR.2006.199
   Gopi M, 2000, COMPUT GRAPH FORUM, V19, pC467, DOI 10.1111/1467-8659.00439
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   JANCOSEK M, 2009, COMP VIS WINT WORKSH
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kolev K., 2007, WORKSH PHOT AN COMP
   Kolev K., 2010, ECCV 10
   Kolev K, 2006, LECT NOTES COMPUT SC, V4174, P688
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Mortensen E., 1995, P 22 ANN C, V84602
   Nan LL, 2014, COMPUT GRAPH FORUM, V33, P249, DOI 10.1111/cgf.12493
   Nguyen H., 2015, VISUAL COMPUT, P1, DOI 10.1007/s00371-015-1078-y
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Rother Carsten, 2004, ACM T GRAPH
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462
   Sinha SN, 2009, IEEE I CONF COMP VIS, P1881, DOI 10.1109/ICCV.2009.5459417
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sormann M, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P1085
   Vicente S., 2008, PROC IEEE C COMPUT V, P1, DOI DOI 10.1109/CVPR.2008.4587440
   Wahba G., 1990, SPLINE MODELS FOROBS
   Wu CC, 2012, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2012.6247839
   Yezzi A, 2003, INT J COMPUT VISION, V53, P31, DOI 10.1023/A:1023079624234
   Zhu C, 2013, VISUAL COMPUT, V29, P609, DOI 10.1007/s00371-013-0827-z
NR 44
TC 3
Z9 3
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2016
VL 32
IS 12
BP 1605
EP 1620
DI 10.1007/s00371-015-1144-5
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB3NH
UT WOS:000387271200009
DA 2024-07-18
ER

PT J
AU Kolár, M
   Chalmers, A
   Debattista, K
AF Kolar, Martin
   Chalmers, Alan
   Debattista, Kurt
TI Repeatable texture sampling with interchangeable patches
SO VISUAL COMPUTER
LA English
DT Article
DE Texture synthesis; Texture mapping; Parallel rendering; Ray tracing
ID WANG TILES; IMAGE; EXAMPLE
AB Rendering textures in real-time environments is a key task in computer graphics. This paper presents a new parallel patch-based method which allows repeatable sampling without cache, and does not create visual repetitions. Interchangeable patches of arbitrary shape are prepared in a preprocessing step, such that patches may lie over the boundary of other patches in a repeating tile. This compresses the example texture into an infinite texture map with small memory requirements, suitable for GPU and ray-tracing applications. The quality of textures rendered with this method can be tuned in the offline preprocessing step, and they can then be rendered in times comparable to Wang tiles. Experimental results demonstrate combined benefits in speed, memory requirements, and quality of randomisation when compared to previous methods.
C1 [Kolar, Martin; Debattista, Kurt] Univ Warwick, WMG, Coventry, W Midlands, England.
   [Chalmers, Alan] Univ Warwick, WMG, Int Digital Lab, Visualisat, Coventry, W Midlands, England.
C3 University of Warwick; University of Warwick
RP Kolár, M (corresponding author), Univ Warwick, WMG, Coventry, W Midlands, England.
EM m.kolar@warwick.ac.uk; alan.chalmers@warwick.ac.uk;
   k.debattista@warwick.ac.uk
FU EPSRC [EP/I006192/1] Funding Source: UKRI
CR [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 1986, TILINGS PATTERNS
   Cohen MF, 2003, ACM T GRAPHIC, V22, P287, DOI 10.1145/882262.882265
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Ephanov A., 2006, INTERSERVICE IND TRA, V2006
   Gilet G, 2012, COMPUT GRAPH FORUM, V31, P2117, DOI 10.1111/j.1467-8659.2012.03204.x
   Jamriska O, 2012, PROC CVPR IEEE, P3673, DOI 10.1109/CVPR.2012.6248113
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lagae A, 2006, ACM T GRAPHIC, V25, P1442, DOI 10.1145/1183287.1183296
   Lasram A., 2012, P 4 ACM SIGGRAPH EUR
   Lefebvre S, 2005, ACM T GRAPHIC, V24, P777, DOI 10.1145/1073204.1073261
   Neyret F, 1999, COMP GRAPH, P235, DOI 10.1145/311535.311561
   Obert Juraj, 2012, ACM SIGGRAPH 2012 CO
   Praun E, 2000, COMP GRAPH, P465, DOI 10.1145/344779.344987
   Taibo Javier, 2009, Journal of WSCG, V17, P25
   Vanhoey K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508383
   Wei L.Y., 2002, TECHNICAL REPORT
   Wei Li-Yi., 2004, SIGGRAPHEUROGRAPHICS, P55
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Xue F, 2007, J COMPUT SCI TECH-CH, V22, P590, DOI 10.1007/s11390-007-9072-0
   Zhang X.M., 1996, Society of Information Display Symposium Technical Digest, V27, P731
NR 23
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2016
VL 32
IS 10
BP 1263
EP 1272
DI 10.1007/s00371-015-1161-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EA2BQ
UT WOS:000386396800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, S
   Chen, Y
   Lai, YK
   Martin, RR
   Cheng, ZQ
AF Lin, Shuai
   Chen, Yin
   Lai, Yu-Kun
   Martin, Ralph R.
   Cheng, Zhi-Quan
TI Fast capture of textured full-body avatar with RGB-D cameras
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Full-body avatar; High-quality texture; RGB-depth camera; Global
   registration
ID OBJECTS
AB We present a practical system which can provide a textured full-body avatar within 3 s. It uses sixteen RGB-depth (RGB-D) cameras, ten of which are arranged to capture the body, while six target the important head region. The configuration of the multiple cameras is formulated as a constraint-based minimum set space-covering problem, which is approximately solved by a heuristic algorithm. The camera layout determined can cover the full-body surface of an adult, with geometric errors of less than 5 mm. After arranging the cameras, they are calibrated using a mannequin before scanning real humans. The 16 RGB-D images are all captured within 1 s, which both avoids the need for the subject to attempt to remain still for an uncomfortable period, and helps to keep pose changes between different cameras small. All scans are combined and processed to reconstruct the photorealistic textured mesh in 2 s. During both system calibration and working capture of a real subject, the high-quality RGB information is exploited ton.o assist geometric reconstruction and texture stitching optimizati
C1 [Lin, Shuai] Natl Univ Def Technol, Sch Comp, PDL, Changsha, Hunan, Peoples R China.
   [Chen, Yin] PLA Univ Sci & Technol, Def Engn Sch, Nanjing, Jiangsu, Peoples R China.
   [Lai, Yu-Kun; Martin, Ralph R.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
   [Cheng, Zhi-Quan] Avatar Sci Co, Changsha, Hunan, Peoples R China.
C3 National University of Defense Technology - China; Army Engineering
   University of PLA; Cardiff University
RP Cheng, ZQ (corresponding author), Avatar Sci Co, Changsha, Hunan, Peoples R China.
EM cheng.zhiquan@gmail.com
RI Lai, Yu-Kun/D-2343-2010; Martin, Ralph R/D-2366-2010
OI Martin, Ralph/0000-0002-8495-8536
CR Afzal H, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P7, DOI 10.1109/3DV.2014.114
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Amplianitis K, 2014, INT ARCH PHOTOGRAMM, P7, DOI 10.5194/isprsarchives-XL-3-W1-7-2014
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI DOI 10.1109/ICCV.2007.4408907
   [Anonymous], KIN
   [Anonymous], 2014, ACM T GRAPHIC, DOI DOI 10.1145/2601097.2601165
   ASUS, 2013, XTION PROL
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940
   Chen Y, 2014, J MANUF SYST, V33, P233, DOI 10.1016/j.jmsy.2013.11.005
   Cheng ZQ, 2013, IEEE T VIS COMPUT GR, V19, P1885, DOI 10.1109/TVCG.2013.15
   Cui Y., 2012, ACCV Workshops, P133
   Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647
   Dou MS, 2013, INT SYM MIX AUGMENT, P99, DOI 10.1109/ISMAR.2013.6671769
   Gal R, 2010, COMPUT GRAPH FORUM, V29, P479, DOI 10.1111/j.1467-8659.2009.01617.x
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x
   Karp R, 1972, COMPLEXITY COMPUTER, V40, P85, DOI 10.1007/978-3-540-68279-08
   Kerl C, 2015, IEEE I CONF COMP VIS, P2264, DOI 10.1109/ICCV.2015.261
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374
   Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Primesense, 2013, CARM SENS
   Richardt C, 2012, COMPUT GRAPH FORUM, V31, P247, DOI 10.1111/j.1467-8659.2012.03003.x
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Steinbrücker F, 2014, IEEE INT CONF ROBOT, P2021, DOI 10.1109/ICRA.2014.6907127
   Steinbrücker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405
   Sturm J, 2013, LECT NOTES COMPUT SC, V8142, P405, DOI 10.1007/978-3-642-40602-7_43
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Whelan T., 2012, RSS WORKSH RGB D ADV, P127
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Zhou QY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461919
   Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134
NR 38
TC 15
Z9 16
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 681
EP 691
DI 10.1007/s00371-016-1245-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Meister, D
   Bittner, J
AF Meister, Daniel
   Bittner, Jiri
TI Parallel BVH construction using <i>k</i>-means clustering
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Ray tracing; Object hierarchies; Three-dimensional graphics; Realism
ID HIERARCHIES
AB We propose a novel method for fast parallel construction of bounding volume hierarchies (BVH) on the GPU. Our method is based on a combination of divisible and agglomerative clustering. We use the k-means algorithm to subdivide scene primitives into clusters. From these clusters, we construct treelets using the agglomerative clustering algorithm. Applying this procedure recursively, we construct the entire bounding volume hierarchy. We implemented the method using parallel programming concepts on the GPU. The results show the versatility of the method: it can be used to construct medium-quality hierarchies very quickly, but also it can be used to construct high-quality hierarchies given a slightly longer computational time. We evaluate the method in the context of GPU ray tracing and show that it provides results comparable with other state-of-the-art GPU techniques for BVH construction. We also believe that our approach based on the k-means algorithm gives a new insight into how bounding volume hierarchies can be constructed.
C1 [Meister, Daniel; Bittner, Jiri] Czech Tech Univ, Fac Elect Engn, Prague, Czech Republic.
C3 Czech Technical University Prague
RP Meister, D (corresponding author), Czech Tech Univ, Fac Elect Engn, Prague, Czech Republic.
EM meistdan@fel.cvut.cz
RI Bittner, Jiri/B-1677-2010
OI Bittner, Jiri/0000-0002-5818-934X
CR Aila T., 2009, P C HIGH PERFORMANCE, P145, DOI [DOI 10.1145/1572769.1572792, 10.1145/1572769.1572792]
   Aila T., 2013, P 5 HIGH PERFORMANCE, P101
   [Anonymous], 2008, 2008 IEEE Hot Chips 20 Symposium (HCS), DOI 10.1109/HOTCHIPS.2008.7476516
   Apetrei Ciprian., 2014, Computer Graphics and Visual Computing (CGVC)
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bittner J., 2009, P 25 SPRING C COMPUT, P61
   Bittner J, 2013, COMPUT GRAPH FORUM, V32, P85, DOI 10.1111/cgf.12000
   Dammertz H, 2008, COMPUT GRAPH FORUM, V27, P1225, DOI 10.1111/j.1467-8659.2008.01261.x
   Dasgupta S., 2008, The hardness of k-means clustering
   Domingues Leonardo., 2015, Proceedings of High-Performance Graphics, P13
   Fabianowski B., 2009, EUROGRAPHICS SHORT P, P49
   Fatahalian G., 2013, P 5 HIGH PERF GRAPH, P81, DOI 10.1145/2492045.2492054
   Feltman Nicolas., 2012, P 2012 C HIGH PERFOR, P49
   Ganestam P., 2015, J COMPUTER GRAPHICS, V4, P23
   Garanzha K., 2011, P ACM SIGGRAPH S HIG, P59, DOI DOI 10.1145/2018323.2018333
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Havran V, 2006, IEEE S INT RAY TRAC, V2006, P71
   Hunt W, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P47, DOI 10.1109/RT.2007.4342590
   Hunt WA, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P77, DOI 10.1109/RT.2008.4634625
   Ize T., 2007, P EUR S PAR GRAPH VI, P101
   Jain A K, 1988, ALGORITHMS CLUSTERIN
   Karras T., 2012, P 4 ACM SIGGRAPH EUR, P33, DOI [10.2312/EGGH/HPG12/033-037, DOI 10.2312/EGGH/HPG12/033-037]
   Karras Tero., 2013, Proceedings of the 5th High-Performance Graphics Conference, P89, DOI DOI 10.1145/2492045.2492055
   Kay T. L., 1986, Computer Graphics, V20, P269, DOI 10.1145/15886.15916
   Kensler A, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P73, DOI 10.1109/RT.2008.4634624
   KRIVANEK M, 1986, ACTA INFORM, V23, P311, DOI 10.1007/BF00289116
   Lauterbach C, 2009, COMPUT GRAPH FORUM, V28, P375, DOI 10.1111/j.1467-8659.2009.01377.x
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Pantaleoni J., 2010, P C HIGH PERFORMANCE, P87
   Rubin S. M., 1980, Computer Graphics, V14, P110, DOI 10.1145/965105.807479
   Vinkler M, 2013, COMPUT GRAPH FORUM, V32, P13, DOI 10.1111/cgf.12140
   Wald I, 2007, RT07: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2007, P33, DOI 10.1109/RT.2007.4342588
   Wald I, 2012, IEEE T VIS COMPUT GR, V18, P47, DOI 10.1109/TVCG.2010.251
   Walter B, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P81, DOI 10.1109/RT.2008.4634626
   WEGHORST H, 1984, ACM T GRAPHIC, V3, P52, DOI 10.1145/357332.357335
NR 36
TC 6
Z9 8
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 977
EP 987
DI 10.1007/s00371-016-1241-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600029
DA 2024-07-18
ER

PT J
AU Liu, W
   Zhang, DH
   Cui, MY
   Ding, JW
AF Liu, Wei
   Zhang, Dehua
   Cui, Mingyue
   Ding, Jianwei
TI An enhanced depth map based rendering method with directional depth
   filter and image inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Depth image-based rendering; Directional depth filter; Directional hole
   inpainting; Stereoscopic image generation
ID STEREOSCOPIC VIDEO; GENERATION
AB Depth image-based rendering (DIBR), which is used to render virtual views with a color image and the corresponding depth map, is one of the key techniques in the 2D to 3D video conversion process. In this paper, a novel method is proposed to partially solve two puzzles of DIBR, i.e. visual image generation and hole filling. The method combines two different approaches for synthesizing new views from an existing view and a corresponding depth map. Disoccluded parts of the synthesized image are first classified as either smooth or highly structured. At structured regions, inpainting is used to preserve the background structure. In other regions, an improved directional depth smoothing is used to avoid disocclusion. Thus, more details and straight line structures in the generated virtual image are preserved. The key contributions include an enhanced adaptive directional filter and a directional hole inpainting algorithm. Experiments show that the disocclusion is removed and the geometric distortion is reduced efficiently. The proposed method can generate more visually satisfactory results.
C1 [Liu, Wei; Cui, Mingyue] Nanyang Normal Univ, Nanyang 473061, Henan, Peoples R China.
   [Liu, Wei] Chinese Acad Sci, Ctr Internet Things, Inst Microelect, Beijing 100029, Peoples R China.
   [Zhang, Dehua] Tsinghua Univ, Beijing 100190, Peoples R China.
   [Ding, Jianwei] Peoples Publ Secur Univ China, Beijing 100190, Peoples R China.
C3 Nanyang Normal College; Chinese Academy of Sciences; Institute of
   Microelectronics, CAS; Tsinghua University; People's Public Security
   University of China
RP Liu, W (corresponding author), Nanyang Normal Univ, Nanyang 473061, Henan, Peoples R China.
EM lw3171796@163.com; dhuazhang@163.com; cuiminyue@sina.com;
   flydjw@gmail.com
RI Ding, Jianwei/GWC-4386-2022; ZHANG, DEHUA/ADX-1625-2022; CUI,
   Mingyue/JJD-9863-2023
OI Ding, Jianwei/0000-0001-8178-2822; ZHANG, DEHUA/0000-0001-8623-8439; 
FU Ministry of Science and Technology of China under National 973 Basic
   Research Program [2013CB228206, 2011CB302505]; National Natural Science
   Foundation of China [U1404614]
FX This work was supported in part by Ministry of Science and Technology of
   China under National 973 Basic Research Program (Grants No. 2013CB228206
   and No. 2011CB302505) and National Natural Science Foundation of China
   (Grant No. U1404614).
CR [Anonymous], SIGN INF PROC ASS AN
   [Anonymous], 2003, P IIASTED VIIP
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   Choi J., 2012, European Wireless, P1
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Daribo I, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P312, DOI 10.1109/MMSP.2007.4412880
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hwang J, 2013, IEEE ICCE, P470, DOI 10.1109/ICCE.2013.6486980
   Knorr S., 2007, 3DTV C, P1
   Lee Soo Chan, 2009, Commun Integr Biol, V2, P414
   Liu WW, 2013, MED GAS RES, V3, DOI [10.1186/2045-9912-3-3, 10.1063/2.1306201]
   Peyré G, 2010, IEEE T PATTERN ANAL, V32, P733, DOI 10.1109/TPAMI.2009.54
   Rotem E, 2005, PROC SPIE, V5664, P198, DOI 10.1117/12.586599
   Shen JB, 2007, COMPUT GRAPH-UK, V31, P119, DOI 10.1016/j.cag.2006.10.004
   Tam WJ, 2004, PROC SPIE, V5599, P162, DOI 10.1117/12.583105
   Wang LH, 2010, IEEE T BROADCAST, V56, P425, DOI 10.1109/TBC.2010.2053971
   Zhang GF, 2007, IEEE T VIS COMPUT GR, V13, P686, DOI 10.1109/TVCG.2007.1032
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhang L, 2011, IEEE T BROADCAST, V57, P372, DOI 10.1109/TBC.2011.2122930
NR 20
TC 8
Z9 8
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2016
VL 32
IS 5
BP 579
EP 589
DI 10.1007/s00371-015-1074-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK5UI
UT WOS:000374985800004
DA 2024-07-18
ER

PT J
AU Camozzato, D
   Dihl, L
   Silveira, I
   Marson, F
   Musse, SR
AF Camozzato, Daniel
   Dihl, Leandro
   Silveira, Ivan
   Marson, Fernando
   Musse, Soraia R.
TI Procedural floor plan generation from building sketches
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Sketch-based modeling; Image processing; Procedural generation
AB We present a method for automated reconstruction of building interiors from hand-drawn building sketches. Image processing is used to extract the building's outline and openings. Then, a procedural generation algorithm creates a floor plan according to user requisites. The proposed method handles a wide variety of input image styles and building shapes, including non-convex polygons. Possible applications include architectural tools and digital content generation.
C1 [Camozzato, Daniel; Dihl, Leandro; Musse, Soraia R.] Pontificia Univ Catolica Rio Grande do Sul, Soraia Raupp Musse, Porto Alegre, RS, Brazil.
   [Silveira, Ivan; Marson, Fernando] Univ Vale Rio dos Sinos, Sao Leopoldo, Brazil.
C3 Pontificia Universidade Catolica Do Rio Grande Do Sul; Universidade do
   Vale do Rio dos Sinos (Unisinos)
RP Camozzato, D (corresponding author), Pontificia Univ Catolica Rio Grande do Sul, Soraia Raupp Musse, Porto Alegre, RS, Brazil.
EM daniel.camozzato@gmail.com; soraia.musse@pucrs.br
RI Musse, Soraia Raupp/AAS-3787-2021; Musse, Soraia Raupp R/G-4801-2012;
   Marson, Fernando/E-1883-2018; Dihl, Leandro Lorenzett/J-2033-2017
OI Marson, Fernando/0000-0001-6700-6165; Dihl, Leandro
   Lorenzett/0000-0002-9486-8534; Musse, Soraia Raupp/0000-0002-3278-217X
FU CAPES; CNPq; FAPERGS
FX This work was conducted with financial support from the Brazilian
   research agencies CAPES, CNPq and FAPERGS.
CR Ahmed S., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P339, DOI 10.1109/DAS.2012.22
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Dahl A., 2008, THESIS CHALMERS U TE
   de las Heras LP, 2013, PROC INT CONF DOC, P1245, DOI 10.1109/ICDAR.2013.252
   Duda R. O., 1973, PATTERN CLASSIFICATI, V576
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hendrikx M, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422957
   Liu H, 2013, VISUAL COMPUT, V29, P663, DOI 10.1007/s00371-013-0825-1
   Lopes R, 2010, 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT GAMES AND SIMULATION, GAME-ON 2010, P13
   Mace S., 2010, P 9 IAPR INT WORKSH, P167, DOI 10.1145/1815330.1815352
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Marson F, 2010, INT J COMPUT GAMES T, V2010, DOI 10.1155/2010/624817
   Merrell P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866203
   Peng CH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601164
   Peponis J, 1997, ENVIRON PLANN B, V24, P761, DOI 10.1068/b240761
   Rodrigues E, 2014, ENERG BUILDINGS, V81, P170, DOI 10.1016/j.enbuild.2014.06.016
   Serra J., 1983, IMAGE ANAL MATH MORP
   Smelik RM, 2014, COMPUT GRAPH FORUM, V33, P31, DOI 10.1111/cgf.12276
   So C., 1998, P ACM S VIRT REAL SO, P17
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Weber B, 2009, COMPUT GRAPH FORUM, V28, P481, DOI 10.1111/j.1467-8659.2009.01387.x
   Yeung W. Y., 2008, CREATION 3D MODEL 2D
   Yin XT, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.9
   Zmugg R, 2014, VISUAL COMPUT, V30, P1009, DOI 10.1007/s00371-013-0912-3
NR 25
TC 17
Z9 19
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 753
EP 763
DI 10.1007/s00371-015-1102-2
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500002
DA 2024-07-18
ER

PT J
AU Alderson, T
   Samavati, F
AF Alderson, Troy
   Samavati, Faramarz
TI Optimizing line-of-sight using simplified regular terrains
SO VISUAL COMPUTER
LA English
DT Article
DE Line-of-sight; Terrain simplification; Multiresolution; Subdivision;
   Reverse subdivision
ID REVERSING SUBDIVISION RULES; MULTIRESOLUTION
AB In this work, we explore a set of techniques for speeding up line-of-sight queries whilst attempting to maintain accuracy. Line-of-sight queries, which test if two entities can see each other over a 3D terrain model, are an important operation in several applications. Given enough entities and a large enough terrain, computing these queries can be expensive. We apply reverse subdivision methods to simplify the terrain model and speed up the queries, including a novel feature-aware reverse subdivision scheme. To counteract the loss of accuracy due to simplification, we also examine the problem of where entities should be placed after terrain simplification to increase accuracy. Using iterative methods that attempt to maximize accuracy, we show that room for improvement exists over the standard projection method. Then, using residual multiresolution vectors, we develop a relocation method designed to maximize accuracy over simplified terrain models. Finally, we present a fast line-of-sight algorithm that combines these techniques with pre-existing algorithms.
C1 [Alderson, Troy] Univ Calgary, Calgary, AB, Canada.
   [Samavati, Faramarz] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary; University of Calgary
RP Alderson, T (corresponding author), Univ Calgary, Calgary, AB, Canada.
EM tfalders@ucalgary.ca
OI Alderson, Troy/0000-0001-5528-8652
FU Mitacs Accelerate program
FX Our thanks go out to C4i Consultants, Inc. for motivating the problem,
   assisting in research, and providing SRTM terrain data. Our thanks go
   also to the Mitacs Accelerate program for sponsoring the collaboration.
   Terrain map images were generated with Global Mapper software.
   Conceptual figures were created using Inkscape.
CR Alderson Troy F., 2012, Proceedings of the International Conference on Computer Graphics Theory and Applications (GRAPP 2012) and International Conference on Information Visualization Theory and Applications (IVAPP 2012), P143
   Andrade MVA, 2011, GEOINFORMATICA, V15, P381, DOI 10.1007/s10707-009-0100-9
   Bartels RH, 2000, J COMPUT APPL MATH, V119, P29, DOI 10.1016/S0377-0427(00)00370-8
   Ben-Moshe B., 2002, P 18 ANN S COMP GEOM
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Brosz J, 2007, COMM COM INF SC, V4, P58
   Chaikin GM., 1974, COMPUT GRAPHICS IMAG, V3, P346, DOI DOI 10.1016/0146-664X(74)90028-8
   De Floriani L., 1993, P 1993 ACM SIGAPP S
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   Duvenhage B., 2009, P 6 INT C COMP GRAPH
   Dyn N., 1987, Computer-Aided Geometric Design, V4, P257, DOI 10.1016/0167-8396(87)90001-X
   Franklin W.R., 1994, P 6 INT S SPAT DAT H
   Garland M., 1995, CMUCS95181, P1
   GARLAND M, 1997, P SIGGRAPH 1997
   Garland M., 1999, THESIS CARNEGIE MELL
   Guttman Antonin., 1984, P 1984 ACM SIGMOD C, P47
   Langetepe E., 2006, GEOMETRIC DATA STRUC
   Losasso F., 2004, ACM SIGGRAPH 2004 SI
   Ramer U., 1972, Comput. Graph. Image Process., V1, P244, DOI DOI 10.1016/S0146-664X(72)80017-0
   Sadeghi J, 2009, COMPUT GRAPH-UK, V33, P217, DOI 10.1016/j.cag.2009.03.012
   Samavati FF, 2007, SER MACH PERCEPT ART, V67, P65
   Samavati FF, 1999, COMPUT GRAPH FORUM, V18, P97, DOI 10.1111/1467-8659.00361
   Seixas R.d.B., 1999, P S PESQ OP LOG MAR
   Silva C.T., 1998, P GIS 1998
   Silva CT, 1995, VISUALIZATION '95 - PROCEEDINGS, P201, DOI 10.1109/VISUAL.1995.480813
   Wecker L, 2010, COMPUT GRAPH-UK, V34, P468, DOI 10.1016/j.cag.2010.05.012
   Wecker L, 2007, VISUAL COMPUT, V23, P881, DOI 10.1007/s00371-007-0148-1
NR 27
TC 7
Z9 8
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2015
VL 31
IS 4
BP 407
EP 421
DI 10.1007/s00371-014-0936-3
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD2IW
UT WOS:000350901600004
DA 2024-07-18
ER

PT J
AU El Ouafdi, AF
   Ziou, D
AF El Ouafdi, Ahmed Fouad
   Ziou, Djemel
TI Global diffusion method for smoothing triangular mesh
SO VISUAL COMPUTER
LA English
DT Article
DE Anisotropic diffusion; Enhancing; Physics-based modeling; Mesh smoothing
ID COMPUTATION; SURFACES
AB In this paper, we propose a new smoothing method based on physical principles. The smoothing process is modeled using the heat transfer process. We start from the global equation of heat conservation and we decompose it into basic laws. The numerical scheme is derived directly from the discretization of the basic heat transfer laws using computation algebraic topological tools, thus providing a physical and topological explanation for each step of the discretization process. In such a way, the geometry, topology and physics are concurring together in a unified framework to define and simulate the diffusion process to reduce random noise on the surface of the object. Experimental results show a good performance in improvement of the proposed approach compared to existing smoothing methods.
C1 [El Ouafdi, Ahmed Fouad] Fac Polydisciplinaire Ouarzazate, Ouarzazate, Morocco.
   [Ziou, Djemel] Univ Sherbrooke, Fac Sci, Sherbrooke, PQ J1K 2R1, Canada.
C3 Ibn Zohr University of Agadir; University of Sherbrooke
RP El Ouafdi, AF (corresponding author), Fac Polydisciplinaire Ouarzazate, Ouarzazate, Morocco.
EM elouafdi@gmail.com; Djemel.Ziou@USherbrooke.ca
CR [Anonymous], S 3D DAT PROC VIS TR
   Auclair-Fortier MF, 2006, IEEE T IMAGE PROCESS, V15, P2558, DOI 10.1109/TIP.2006.877410
   Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   Belyaev Alexander., 2003, P ISRAEL KOREA BINAT, P83
   Bernardini F, 2002, COMPUT GRAPH FORUM, V21, P149, DOI 10.1111/1467-8659.00574
   Branin F.H., 1966, S GEN NETWORKS, P453
   Burke W.L., 1985, APPL DIFFERENTIAL GE, DOI 10.1017/CBO9781139171786
   Chard JA, 2000, MATH COMPUT SIMULAT, V54, P33, DOI 10.1016/S0378-4754(00)00198-1
   Clarenz U, 2004, IEEE T IMAGE PROCESS, V13, P248, DOI 10.1109/TIP.2003.819863
   Croom FH., 1978, BASIC CONCEPTS ALGEB
   Delfinado CecilJose A., 1993, SCG 93, P232, DOI DOI 10.1145/160985.161140
   Desbrun Mathieu., 2005, SIGGRAPH 05, P7, DOI DOI 10.1145/1198555.1198666
   Elcott S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1189762.1189766
   GROSS PW, 2001, PROGR ELECTROMAGNETI, V32, P151
   Gu X., 2008, ADV LECT MATH
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Huang CH, 1996, INT J NUMER METH ENG, V39, P605, DOI 10.1002/(SICI)1097-0207(19960229)39:4<605::AID-NME872>3.0.CO;2-H
   Incropera F.P., 2006, FUNDAMENTALS HEAT MA
   Kaczynski T, 1998, COMPUT MATH APPL, V35, P59, DOI 10.1016/S0898-1221(97)00289-7
   Mattiussi C, 2002, ADV IMAG ELECT PHYS, V121, P143
   Meyer M., 2002, VIS MATH, V3, P35
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   Repetto M, 2003, IEEE T MAGN, V39, P1135, DOI 10.1109/TMAG.2003.810161
   Sun XF, 2009, GRAPH MODELS, V71, P34, DOI 10.1016/j.gmod.2008.12.002
   Tarmissi K, 2011, SIGNAL IMAGE VIDEO P, V5, P191, DOI 10.1007/s11760-010-0154-z
   Tonti E, 2002, IEEE T MAGN, V38, P333, DOI 10.1109/20.996090
   van de Weygaert Rien, 2010, Proceedings of the Seventh International Symposium on Voronoi Diagrams in Science and Engineering (ISVD 2010), P224, DOI 10.1109/ISVD.2010.24
   Voitovich T.V., 2006, NUMER METH PART D E, P1
   Weickert J., 1994, P 7 THEOR FDN COMP V, P221
   Yoshizawa S, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P38
   Zhang Y, 2007, IEEE T IMAGE PROCESS, V16, P1036, DOI 10.1109/TIP.2007.891787
   Ziou D, 2002, PATTERN RECOGN, V35, P2833, DOI 10.1016/S0031-3203(01)00238-2
NR 32
TC 5
Z9 5
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 295
EP 310
DI 10.1007/s00371-014-0922-9
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900005
DA 2024-07-18
ER

PT J
AU Jafri, R
   Ali, SA
   Arabnia, HR
   Fatima, S
AF Jafri, Rabia
   Ali, Syed Abid
   Arabnia, Hamid R.
   Fatima, Shameem
TI Computer vision-based object recognition for the visually impaired in an
   indoors environment: a survey
SO VISUAL COMPUTER
LA English
DT Article
DE Survey; Visually impaired; Assistive technologies; Object recognition;
   Computer vision; Computer vision; Blind; Review
ID SYSTEM; BLIND; PROTOTYPE; PEOPLE
AB Though several electronic assistive devices have been developed for the visually impaired in the past few decades, however, relatively few solutions have been devised to aid them in recognizing generic objects in their environment, particularly indoors. Nevertheless, research in this area is gaining momentum. Among the various technologies being utilized for this purpose, computer vision based solutions are emerging as one of the most promising options mainly due to their affordability and accessibility. This paper provides an overview of the various technologies that have been developed in recent years to assist the visually impaired in recognizing generic objects in an indoors environment with a focus on approaches based on computer vision. It aims to introduce researchers to the latest trends in this area as well as to serve as a resource for developers who wish to incorporate such solutions into their own work.
C1 [Jafri, Rabia] King Saud Univ, Dept Informat Technol, Riyadh, Saudi Arabia.
   [Fatima, Shameem] King Saud Univ, Dept Informat Technol, Coll Comp & Informat Sci, Riyadh, Saudi Arabia.
   [Ali, Syed Abid] ISM TEC LLC, Wilmington, DE USA.
   [Arabnia, Hamid R.] Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
   [Ali, Syed Abid] Univ Georgia, Dept Biochem & Mol Biol, Athens, GA 30602 USA.
   [Ali, Syed Abid] Sesneber Int, Riyadh, Saudi Arabia.
   [Arabnia, Hamid R.] Univ Georgia, Athens, GA 30602 USA.
C3 King Saud University; King Saud University; University System of
   Georgia; University of Georgia; University System of Georgia; University
   of Georgia; University System of Georgia; University of Georgia
RP Jafri, R (corresponding author), 2885 Sanford Ave SW 17557, Grandville, MI 49418 USA.
EM rabia.ksu@gmail.com; syedabidali@gmail.com; hra@cs.uga.edu;
   sfatima@ksu.edu.sa
RI Jafri, Rabia/ABV-0411-2022
OI Jafri, Rabia/0000-0003-2844-0879
FU "Research Center of the Female Scientific and Medical Colleges",
   Deanship of Scientific Research, King Saud University
FX This research project was supported by a grant from the "Research Center
   of the Female Scientific and Medical Colleges", Deanship of Scientific
   Research, King Saud University.
CR Akhter S, 2011, NORTHEAST BIOENGIN C
   Al-Khalifa HS, 2008, LECT NOTES COMPUT SC, V5105, P1065, DOI 10.1007/978-3-540-70540-6_159
   Ancuti C, 2007, PROCEEDINGS OF THE 5TH INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P130
   [Anonymous], Indoor positioning system
   [Anonymous], P 2013 INT C IM PROC
   [Anonymous], Sonar
   [Anonymous], 2008, The Engineering Handbook of Smart Technology for Aging, Disability, and Independence, DOI DOI 10.1002/9780470379424.CH25
   [Anonymous], HCI INT 2011
   [Anonymous], 2006, BRAILLE MONITOR, V49, P101
   [Anonymous], 2008, P 2 INT CONV REH ASS
   [Anonymous], 2010, 2010 IEEE COMP SOC C
   [Anonymous], Radio frequency identification
   Bach-y-Rita P, 1998, J REHABIL RES DEV, V35, P427
   Bach-y-Rita P, 2003, INT J HUM-COMPUT INT, V15, P285, DOI 10.1207/S15327590IJHC1502_6
   Bach-y-Rita P., 1972, BRAIN MECHANISMS IN
   Bakken T., 2007, AN EVALUATION OF THE
   Bauer J., 2007, PROC OF THE INTERNAT
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bigham J., 2010, 3RD WORKSHOP ON COMP
   Blauert J., 1997, SPATIAL HEARING PSYC
   Borenstein J, 1997, IEEE INT CONF ROBOT, P1283, DOI 10.1109/ROBOT.1997.614314
   Calderon D.E., 2010, P AUSTR C ROB AUT DE, P1
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Martínez BDC, 2011, LECT NOTES COMPUT SC, V6783, P522
   Capelle C, 1998, IEEE T BIO-MED ENG, V45, P1279, DOI 10.1109/10.720206
   Cardin S, 2007, VISUAL COMPUT, V23, P109, DOI 10.1007/s00371-006-0032-4
   Chincha R, 2011, IEEE INT C BIO BIO W, P526, DOI 10.1109/BIBMW.2011.6112423
   Chong NY, 2004, IEEE INT CONF ROBOT, P3494
   Chumkamon S, 2008, ECTI-CON 2008: PROCEEDINGS OF THE 2008 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING/ELECTRONICS, COMPUTER, TELECOMMUNICATIONS AND INFORMATION TECHNOLOGY, VOLS 1 AND 2, P765, DOI 10.1109/ECTICON.2008.4600543
   Collignon O, 2007, CEREB CORTEX, V17, P457, DOI 10.1093/cercor/bhj162
   Crandall W, 1999, J REHABIL RES DEV, V36, P341
   Dakopoulos D, 2007, PROCEEDINGS OF THE 7TH IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING, VOLS I AND II, P930
   Dakopoulos D, 2009, THESIS WRIGHT STATE
   Dakopoulos D, 2010, IEEE T SYST MAN CY C, V40, P25, DOI 10.1109/TSMCC.2009.2021255
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Duen Y., 2007, Currently Available Electronic Travel Aids For The Blind
   Dumitras T, 2006, TENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P145
   Durette Barthelemy, 2008, WORKSHOP COMPUTER VI, P1
   Earl J., 2011, NOTTINGHAM ECON REV
   Ecemis MI, 2004, INT J ROBOT AUTOM, V19, P178, DOI 10.2316/Journal.206.2004.4.206-2712
   Fan P., 2009, PRESENTED AT THE IEE
   Fink W., 2004, DORA DIGITAL OBJECT
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Gude R., BLIND NAVIGATION AND
   Hafez M, 2007, VISUAL COMPUT, V23, P267, DOI 10.1007/s00371-007-0102-2
   Hakkinen J., 2002, IEEE International Conference on Systems, Man and Cybernetics, V4, P147, DOI [DOI 10.1109/ICSMC.2002.1167964, 10.1109/ICSMC.2002.1167964]
   Hasanuzzaman Faiz M, 2011, WOCC, V2011, P1
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Hub A., 2006, NORTHRIDGE CENTER ON
   Hub Andreas., 2006, Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility, P111
   Iannizzotto G., 2005, PRESENTED AT THE 200
   Ivanov Rosen., 2010, P 11 INT C COMPUTER, P143
   Jafri Rabia., 2013, The 2013 International Conference on Information and Knowledge (IKE13), P153
   Jinying Chen, 2010, Proceedings of the 2010 International Conference on Electrical and Control Engineering (ICECE 2010), P548, DOI 10.1109/iCECE.2010.141
   Johnson Lise A, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P6289
   Kawai Y, 2002, INT C PATT RECOG, P974, DOI 10.1109/ICPR.2002.1048200
   Kawai Y., 1998, PRESENTED AT THE PRO
   Kim J.-H., 2003, SID SYMP DIGEST TECH, V34, P1156, DOI [10.1889/1.1832493, DOI 10.1889/1.1832493]
   Koley C., 2005, Proceedings. Third International Conference on Intelligent Sensing and Information Processing (IEEE Cat. No. 05EX1239), P99
   Kulyukin V., 2010, THE 33 RD ANNUAL CON
   Kulyukin V., 2010, THE 2010 INTERNATION
   Kutiyanawala A., 2010, ENVISION 2010
   Lanigan P., 2007, Proceedings of the International IEEE-BAIS Symposium on Research on Assistive Technologies, P29
   Lanigan PE, 2006, TENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P147
   Legood R, 2002, INJ PREV, V8, P155, DOI 10.1136/ip.8.2.155
   Leibs A., 2012, TOP 10 IPHONE APPS F
   Leporini B., 2004, P 2004 INT CROSS DIS, P57, DOI DOI 10.1145/990657.990668
   Liu S., 2004, IT Professional, V6, P28, DOI 10.1109/MITP.2004.36
   Loomis J., 2003, CONVERGING TECHNOLOG, P189
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maddox B.G., 2004, U S GEOLOGICAL SURVE
   Mae Y, 2000, INT C PATT RECOG, P845, DOI 10.1109/ICPR.2000.903049
   Manduchi R., 2011, INSIGHT RESEARCH AND, V4
   Manduchi R, 2012, COMMUN ACM, V55, P96, DOI 10.1145/2063176.2063200
   Marston JamesR., 2006, ACM T APPL PERCEPT, V3, P110, DOI [DOI 10.1145/1141897.1141900, 10.1145/1141897.1141900]
   MARSTON R.G., 2004, Journal of Visual Impairment and Blindness, V98, P135, DOI DOI 10.1177/0145482X0409800304
   McDaniel T., 2008, Preparing Democratic Leaders for Quality Teaching and Student Success: A Time for Action. UCEA Conference Proceedings for Convention 2008, P1
   MEIJER PBL, 1992, IEEE T BIO-MED ENG, V39, P112, DOI 10.1109/10.121642
   Merabet LB, 2005, NAT REV NEUROSCI, V6, P71, DOI 10.1038/nrn1586
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Murad M., 2011, 2011 7th International Conference on Emerging Technologies (ICET 2011), DOI 10.1109/ICET.2011.6048486
   Nagarajan R, 2003, TENCON IEEE REGION, P735, DOI 10.1109/TENCON.2003.1273276
   Nicholson J., 2009, OPEN REHABIL J, V2, P11, DOI [10.2174/1874943700902010011, DOI 10.2174/1874943700902010011]
   Ohkugo H., 2005, 20TH ANNUAL CSUN INT
   Parlouar R, 2009, ASSETS'09: PROCEEDINGS OF THE 11TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P227
   Parry D., 2008, HEALTH INFORMATICS N
   Pascolini D, 2012, BRIT J OPHTHALMOL, V96, P614, DOI 10.1136/bjophthalmol-2011-300539
   Pavlidis T, 2008, AN EVALUATION OF THE
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Pullin G, 2009, DESIGN MEETS DISABILITY, P1
   Ramadevi Y., 2010, INT J COMPUT SCI INF, V2
   Ramisa A, 2009, FRONT ARTIF INTEL AP, V202, P9, DOI 10.3233/978-1-60750-061-2-9
   Ross D. A., 2000, ACM C ASS TECHN ARL, P193, DOI 10.1145/354324.354380
   Roth P., 2008, TECHNICAL REPORT ICG
   Saaid MF, 2009, CSPA: 2009 5TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, PROCEEDINGS, P250, DOI 10.1109/CSPA.2009.5069227
   Sarfraz M, 2007, GMAI 2007: GEOMETRIC MODELING AND IMAGING, PROCEEDINGS, P127, DOI 10.1109/GMAI.2007.23
   Schauerte B, 2012, LECT NOTES COMPUT SC, V7383, P566, DOI 10.1007/978-3-642-31534-3_83
   Shim SO, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P493
   Sudol Jeremi, 2010, Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2010.5543725
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Takemura K., 2004, PROCEEDINGS OF THE 1
   Tanaka M, 2008, INT C PATT RECOG, P1962
   Technovelgy.com-where science meets fiction, PASS RFID TAG PASS T
   Tekin E, 2010, LECT NOTES COMPUT SC, V6180, P290, DOI 10.1007/978-3-642-14100-3_43
   Thompson RW, 2003, INVEST OPHTH VIS SCI, V44, P5035, DOI 10.1167/iovs.03-0341
   Tian X., 2006, 8TH INTERNATIONAL CO
   Tian Y., 2012, MACH VISION APPL, DOI [10.5121/ijcsit.2010.2614, DOI 10.5121/IJCSIT.2010.2614]
   Velázquez R, 2005, P ANN INT IEEE EMBS, P6821, DOI 10.1109/IEMBS.2005.1616071
   Wikipedia, IMAGE PROCESSING
   Willis S, 2005, NINTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P34, DOI 10.1109/ISWC.2005.46
   Woods RL, 2011, OPHTHAL PHYSL OPT, V31, P258, DOI 10.1111/j.1475-1313.2011.00833.x
   World Health Organisation, FIFTY SIXTH WORLD HE
   World Health Organization, 10TH REVISION OF THE
   Yang M., 2009, ENCY DATABASE SYSTEM, P1936, DOI [10.1007/978-0-387-39940-9_1042, DOI 10.1007/978-0-387-39940-9_1042]
   Zuckerman D.M., 2004, BLIND ADULTS IN AMER
NR 117
TC 74
Z9 82
U1 1
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2014
VL 30
IS 11
BP 1197
EP 1222
DI 10.1007/s00371-013-0886-1
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS3HX
UT WOS:000344169300002
DA 2024-07-18
ER

PT J
AU Guo, J
   Pan, JG
AF Guo, Jie
   Pan, Jin-Gui
TI Real-time simulating and rendering of layered dust
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Layered dust; Scattering; Surface reflection; Real-time rendering; The
   Kubelka-Munk model
AB Detailed modeling of appearance affected by dust coverage is challenging because of the numerous physical and material factors on which it depends. The two principal roadblocks are the complexity of simulating dust particle deposition and the difficulty of accurately depicting the reflectance. In this paper, we present a practical framework for simulating and rendering the appearance of objects covered by the layered dust of spatially varying thickness. First, a fast evaluation approach is proposed to compute the dust's thickness distribution based on a surface's inclination and stickiness properties, as well as surface exposure and local stability. Then, to ensure high fidelity of light scattering and real-time performance, we render the scene based on a revised Kubelka-Munk model implemented on the GPU, including the effects of external and internal surface reflection. Finally, experimental results reveal that our framework can produce visually convincing dusty objects of arbitrary complexity in real time.
C1 [Guo, Jie; Pan, Jin-Gui] Nanjing Univ, Nanjing 210008, Jiangsu, Peoples R China.
C3 Nanjing University
RP Guo, J (corresponding author), Nanjing Univ, Nanjing 210008, Jiangsu, Peoples R China.
EM guojie_022@163.com
CR Abdul-Rahman A, 2005, COMPUT GRAPH FORUM, V24, P413, DOI 10.1111/j.1467-8659.2005.00866.x
   Bandeira D, 2010, VISUAL COMPUT, V26, P965, DOI 10.1007/s00371-010-0495-1
   BARRON V, 1986, J SOIL SCI, V37, P499, DOI 10.1111/j.1365-2389.1986.tb00382.x
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Bosch C, 2004, COMPUT GRAPH FORUM, V23, P361, DOI 10.1111/j.1467-8659.2004.00767.x
   Bosch C, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966399
   Chen YY, 2005, ACM T GRAPHIC, V24, P1127, DOI 10.1145/1073204.1073321
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   Dorsey J, 2008, MKS COMP GRAPH GEOME, P1
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Ershov S, 2001, COMPUT GRAPH FORUM, V20, pC227, DOI 10.1111/1467-8659.00515
   Fearing P, 2000, COMP GRAPH, P37, DOI 10.1145/344779.344809
   FERNANDO R., 2005, ACM SIGGRAPH 2005 SK
   GERSHUN A, 1945, J OPT SOC AM, V35, P162, DOI 10.1364/JOSA.35.000162
   Gu J., 2007, EUR S REND
   HAASE CS, 1992, ACM T GRAPHIC, V11, P305, DOI 10.1145/146443.146452
   Hanrahan P., 1993, Computer Graphics Proceedings, P165, DOI 10.1145/166117.166139
   Hasan M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778798
   HSU SC, 1995, IEEE COMPUT GRAPH, V15, P18, DOI 10.1109/38.364957
   Kimmel BW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421646
   KUBELKA P, 1954, J OPT SOC AM, V44, P330, DOI 10.1364/JOSA.44.000330
   Kubelka P., 1931, Z TECH PHYS, V12, P259, DOI DOI 10.4236/MSCE.2014.28004
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Onoue K, 2005, COMPUT GRAPH FORUM, V24, P51, DOI 10.1111/j.1467-8659.2005.00828.x
   Pharr M, 2000, COMP GRAPH, P75, DOI 10.1145/344779.344824
   Rudolf D, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P163, DOI 10.1109/PCCGA.2003.1238258
   Saunderson JL, 1942, J OPT SOC AM, V32, P727, DOI 10.1364/JOSA.32.000727
   von Festenberg N, 2011, COMPUT GRAPH FORUM, V30, P1837, DOI 10.1111/j.1467-8659.2011.01904.x
NR 28
TC 7
Z9 7
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 797
EP 807
DI 10.1007/s00371-014-0967-9
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700022
DA 2024-07-18
ER

PT J
AU Huang, SS
   Zhang, GX
   Lai, YK
   Kopf, J
   Cohen-Or, D
   Hu, SM
AF Huang, Shi-Sheng
   Zhang, Guo-Xin
   Lai, Yu-Kun
   Kopf, Johannes
   Cohen-Or, Daniel
   Hu, Shi-Min
TI Parametric meta-filter modeling from a single example pair
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 31st CGI conference
CY JUN 10-13, 2014
CL Sydney, AUSTRALIA
DE Image filters; Filter space; Sparsity; Learning and Transfer
AB We present a method for learning a meta-filter from an example pair comprising an original image and its filtered version using an unknown image filter. A meta-filter is a parametric model, consisting of a spatially varying linear combination of simple basis filters. We introduce a technique for learning the parameters of the meta-filter such that it approximates the effects of the unknown filter, i.e., approximates . The meta-filter can be transferred to novel input images, and its parametric representation enables intuitive tuning of its parameters to achieve controlled variations. We show that our technique successfully learns and models meta-filters that approximate a large variety of common image filters with high accuracy both visually and quantitatively.
C1 [Huang, Shi-Sheng; Zhang, Guo-Xin; Hu, Shi-Min] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Cardiff CF10 3AX, S Glam, Wales.
   [Kopf, Johannes] Microsoft Res, Redmond, WA USA.
   [Cohen-Or, Daniel] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
C3 Tsinghua University; Cardiff University; Microsoft; Tel Aviv University
RP Hu, SM (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM YuKun.Lai@cs.cardiff.ac.uk; kopf@microsoft.com; dcor@tau.ac.il;
   shimin@tsinghua.edu.cn
RI Lai, Yu-Kun/D-2343-2010; Hu, Shi-Min/AAW-1952-2020
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61120106007, 61133008]; National High Technology
   Research and Development Program of China [2012AA011802]; Tsinghua
   University Initiative Scientific Research Program
FX This work was supported by the National Basic Research Project of China
   (Project Number 2011CB302205), the Natural Science Foundation of China
   (Project Number 61120106007, 61133008), the National High Technology
   Research and Development Program of China (Project Number 2012AA011802),
   and Tsinghua University Initiative Scientific Research Program.
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Berthouzoz F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019639
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Davis T.A., 2011, CHOLMOD SPARSE CHOLE
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Ji H, 2012, IEEE T IMAGE PROCESS, V21, P1624, DOI 10.1109/TIP.2011.2171699
   Joshi N, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587834
   Kang SB, 2010, PROC CVPR IEEE, P1799, DOI 10.1109/CVPR.2010.5539850
   Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112
   Li XY, 2013, IEEE T IMAGE PROCESS, V22, P1915, DOI 10.1109/TIP.2013.2237922
   Ling Y, 2012, VISUAL COMPUT, V28, P733, DOI 10.1007/s00371-012-0691-2
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Menon D, 2009, IEEE T IMAGE PROCESS, V18, P2209, DOI 10.1109/TIP.2009.2025092
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russ J.C., 2006, The Image Processing Handbook
   Sahba F, 2003, CCECE 2003: CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3, PROCEEDINGS, P847
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Tang Y, 2012, VISUAL COMPUT, V28, P743, DOI 10.1007/s00371-012-0701-4
   Wang XH, 2012, J COMPUT SCI TECH-CH, V27, P1119, DOI 10.1007/s11390-012-1290-4
   Wang XH, 2013, VISUAL COMPUT, V29, P1121, DOI 10.1007/s00371-012-0755-3
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zang Y, 2013, J COMPUT SCI TECH-CH, V28, P762, DOI 10.1007/s11390-013-1375-8
   Ziang Ding, 2012, Tsinghua Science and Technology, V17, P463
NR 30
TC 7
Z9 7
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2014
VL 30
IS 6-8
BP 673
EP 684
DI 10.1007/s00371-014-0973-y
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AI7GX
UT WOS:000337054700011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, GY
   Ren, GH
   Jiang, LH
   Quan, TF
AF Wang, Gangyi
   Ren, Guanghui
   Jiang, Lihui
   Quan, Taifan
TI Hole-based traffic sign detection method for traffic signs with red rim
SO VISUAL COMPUTER
LA English
DT Article
DE Traffic sign recognition; Object detection; Image segmentation; Driver
   assistant system
ID RECOGNITION; ALGORITHMS; CLASSIFICATION
AB A traffic sign detection method is proposed in this paper based on detecting the inner nonred region of prohibitory and danger signs. It consists of five steps: red pixel extraction, red hole extraction, hole region filtering, semicircle combination, and final decision. In the step of red pixel extraction, a new red bitmap extraction method considering relative color of neighboring pixels is proposed to improve the accuracy of the extracted red bitmap. A series of weak classifiers are cascaded in the step of hole region filtering to quickly filter out the false alarms. A support vector machine is adopted in the final decision step to further reduce the false alarm ratio. Experimental results indicate that the proposed method is robust to many kinds of adverse situations including bad lighting condition, small rotation, out-of-plane rotation, similar background color, multiple signs clustered, and partial occlusion. Experiments on the GTSDB traffic sign dataset show that the proposed method achieves the recall of 99 % and 97 % for prohibitory and danger signs, respectively, while keeps the precision above 99 %. In addition, the mean processing time of the proposed method is about 100 ms for each 1366x768 image from GTSDB on a Core I3 CPU, which confirms its suitability for real-time applications such as driver assistance systems.
C1 [Wang, Gangyi; Ren, Guanghui; Jiang, Lihui; Quan, Taifan] Harbin Inst Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Ren, GH (corresponding author), Harbin Inst Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM ghui.ren@gmail.com
CR Akinlar C, 2013, PATTERN RECOGN, V46, P725, DOI 10.1016/j.patcog.2012.09.020
   Aoyagi Y, 1996, IEEE IND ELEC, P1838, DOI 10.1109/IECON.1996.570749
   Barnes N, 2005, IEEE I CONF COMP VIS, P778
   Baró X, 2009, IEEE T INTELL TRANSP, V10, P113, DOI 10.1109/TITS.2008.2011702
   Creusen IM, 2010, IEEE IMAGE PROC, P2669, DOI 10.1109/ICIP.2010.5651637
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   de la Escalera A, 2003, IMAGE VISION COMPUT, V21, P247, DOI 10.1016/S0262-8856(02)00156-7
   delaEscalera A, 1997, IEEE T IND ELECTRON, V44, P848, DOI 10.1109/41.649946
   Everingham M., 2009, The PASCAL Visual Object Classes Challenge 2009 (VOC) Results
   Fang CY, 2003, IEEE T VEH TECHNOL, V52, P1329, DOI 10.1109/TVT.2003.810999
   Fleyeh H, 2011, IET INTELL TRANSP SY, V5, P190, DOI 10.1049/iet-its.2010.0159
   García-Garrido MA, 2005, LECT NOTES COMPUT SC, V3643, P543, DOI 10.1007/11556985_71
   García-Garrido MA, 2012, SENSORS-BASEL, V12, P1148, DOI 10.3390/s120201148
   Gómez-Moreno H, 2010, IEEE T INTELL TRANSP, V11, P917, DOI 10.1109/TITS.2010.2054084
   Herbschleb E., 2009, P SPIE, V7257
   Houben S, 2013, IEEE IJCNN
   Houben S, 2011, IEEE INT VEH SYM, P124, DOI 10.1109/IVS.2011.5940429
   Janssen R., 1993, Intelligent Vehicles '93 Symposium, P390, DOI 10.1109/IVS.1993.697358
   Jiménez PG, 2008, SIGNAL PROCESS, V88, P2943, DOI 10.1016/j.sigpro.2008.06.019
   Kang D. S., 1994, Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation, P88, DOI 10.1109/IAI.1994.336679
   Keller CG, 2008, IEEE INT VEH SYM, P946
   Khan J.F., 2009, P 16 IEEE INT C IM P
   Lafuente-Arroyo S, 2011, ICAART 2011: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE, VOL 1, P269
   Lin CC, 2012, SENSORS-BASEL, V12, P6415, DOI 10.3390/s120506415
   Liu X, 2009, INT C INTEL HUM MACH, P193, DOI 10.1109/IHMSC.2009.172
   Maji S., 2008, CVPR
   Maldonado-Bascón S, 2007, IEEE T INTELL TRANSP, V8, P264, DOI 10.1109/TITS.2007.895311
   Nguwi YY, 2010, NEURAL COMPUT APPL, V19, P601, DOI 10.1007/s00521-009-0315-6
   Overett G, 2011, IEEE INT VEH SYM, P326, DOI 10.1109/IVS.2011.5940549
   Paclík P, 2000, PATTERN RECOGN LETT, V21, P1165, DOI 10.1016/S0167-8655(00)00078-7
   Prasad DK, 2013, PATTERN RECOGN, V46, P1449, DOI 10.1016/j.patcog.2012.11.007
   Prieto MS, 2009, MACH VISION APPL, V20, P379, DOI 10.1007/s00138-008-0133-3
   Ren FX, 2009, 2009 24TH INTERNATIONAL CONFERENCE IMAGE AND VISION COMPUTING NEW ZEALAND (IVCNZ 2009), P409, DOI 10.1109/IVCNZ.2009.5378370
   Ruta A, 2011, MACH VISION APPL, V22, P359, DOI 10.1007/s00138-009-0231-x
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Varun S, 2007, ICCIMA 2007: INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, VOL III, PROCEEDINGS, P360, DOI 10.1109/ICCIMA.2007.190
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang G., 2013, INT JOINT C NEUR NET
   Wang G., 2013, INT JOINT C IN PRESS
   Xie Y, 2009, IEEE INT VEH SYM, P24, DOI 10.1109/IVS.2009.5164247
NR 40
TC 15
Z9 17
U1 0
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2014
VL 30
IS 5
BP 539
EP 551
DI 10.1007/s00371-013-0879-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2AN
UT WOS:000334515100007
DA 2024-07-18
ER

PT J
AU Liu, YG
   Cao, LP
   Liu, CL
   Pu, YF
   Cheng, H
AF Liu, Yiguang
   Cao, Liping
   Liu, Chunling
   Pu, Yifei
   Cheng, Hong
TI Recovering shape and motion by a dynamic system for low-rank matrix
   approximation in <i>L</i> <sub>1</sub> norm
SO VISUAL COMPUTER
LA English
DT Article
DE Structure from motion; Low-rank matrix approximation; Dynamic system;
   L-1 norm; Convergence
ID 3D RECONSTRUCTION; MISSING DATA; FACTORIZATION; EIGENVECTORS;
   EIGENVALUES; ALGORITHMS
AB To recover motion and shape matrices from a matrix of tracking feature points on a rigid object under orthography, we can do low-rank matrix approximation of the tracking matrix with its each column minus the row mean vector of the matrix. To obtain the row mean vector, usually 4-rank matrix approximation is used to recover the missing entries. Then, 3-rank matrix approximation is used to recover the shape and motion. Obviously, the procedure is not convenient. In this paper, we build a cost function which calculates the shape matrix, motion matrix as well as the row mean vector at the same time. The function is in L (1) norm, and is not smooth everywhere. To optimize the function, a continuous-time dynamic system is newly proposed. With time going on, the product of the shape and rotation matrices becomes closer and closer, in L (1)-norm sense, to the tracking matrix with each its column minus the mean vector. A parameter is implanted into the system for improving the calculating efficiency, and the influence of the parameter on approximation accuracy and computational efficiency are theoretically studied and experimentally confirmed. The experimental results on a large number of synthetic data and a real application of structure from motion demonstrate the effectiveness and efficiency of the proposed method. The proposed system is also applicable to general low-rank matrix approximation in L (1) norm, and this is also experimentally demonstrated.
C1 [Liu, Yiguang; Pu, Yifei] Sichuan Univ, Coll Comp Sci, Vis & Image Proc Lab, Chengdu 610065, Sichuan, Peoples R China.
   [Cao, Liping] Sichuan Univ, Chengdu 610065, Sichuan, Peoples R China.
   [Liu, Chunling] Sichuan Univ, West China Hosp, Dept Ophthalmol, Chengdu 610041, Sichuan, Peoples R China.
   [Cheng, Hong] Univ Elect Sci & Technol China, Pattern Recognit & Machine Intelligence Lab, Chengdu 610054, Sichuan, Peoples R China.
C3 Sichuan University; Sichuan University; Sichuan University; University
   of Electronic Science & Technology of China
RP Liu, YG (corresponding author), Sichuan Univ, Coll Comp Sci, Vis & Image Proc Lab, Chengdu 610065, Sichuan, Peoples R China.
EM lygpapers@yahoo.com.cn; Caolp@scu.edu.cn; chunlingsc@163.com;
   hchenguestc@gmail.com
RI Liu, Yiguang/C-6404-2011
FU NSFC [61173182, 61179071]; Applied Basic Research Project of Sichuan
   Province [2011JY0124]; International Cooperation and Exchange Project of
   Sichuan Province [2012HH0004]
FX We thank the Editor and Reviewers for time and effort going in reviewing
   this paper. This work was supported by NSFC under Grants 61173182 and
   61179071, and the Applied Basic Research Project (2011JY0124) and the
   International Cooperation and Exchange Project (2012HH0004) of Sichuan
   Province.
CR [Anonymous], AUST ADV WORK COMPUT
   [Anonymous], 2009, MATH PROGRAM
   Buchanan AM, 2005, PROC CVPR IEEE, P316
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen P, 2008, INT J COMPUT VISION, V80, P125, DOI 10.1007/s11263-008-0135-7
   Chen P, 2008, IEEE T SIGNAL PROCES, V56, P1429, DOI 10.1109/TSP.2007.909353
   De la Torre F, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P362, DOI 10.1109/ICCV.2001.937541
   El-Melegy MT, 2007, 2007 INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING & SYSTEMS: ICCES '07, P38, DOI 10.1109/ICCES.2007.4447023
   Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139
   Ke QF, 2005, PROC CVPR IEEE, P739
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   KHALIL H., 2014, Nonlinear Systems, V3rd
   Li S. Z, 2001, MARKOV RANDOM FIELD
   Liu J, 2010, IEEE T NEURAL NETWOR, V21, P621, DOI 10.1109/TNN.2010.2040290
   Liu YG, 2005, NEURAL NETWORKS, V18, P1293, DOI 10.1016/j.neunet.2005.04.008
   Liu YG, 2006, THEOR COMPUT SCI, V367, P273, DOI 10.1016/j.tcs.2006.05.026
   Liu YG, 2006, PATTERN RECOGN, V39, P2258, DOI 10.1016/j.patcog.2006.05.034
   Liu YG, 2011, IEEE T NEURAL NETWOR, V22, P1256, DOI 10.1109/TNN.2011.2153210
   Liu YH, 2010, IEEE T PATTERN ANAL, V32, P12, DOI 10.1109/TPAMI.2008.280
   Martinec D, 2005, PROC CVPR IEEE, P198
   Morgan A. B., 2004, TECHNICAL REPORT
   Morita T, 1997, IEEE T PATTERN ANAL, V19, P858, DOI 10.1109/34.608289
   Okatani T, 2007, INT J COMPUT VISION, V72, P329, DOI 10.1007/s11263-006-9785-5
   Orban GA, 2006, TRENDS NEUROSCI, V29, P466, DOI 10.1016/j.tins.2006.06.012
   Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138
   Stoykova E, 2007, IEEE T CIRC SYST VID, V17, P1568, DOI 10.1109/TCSVT.2007.909975
   Toh KC, 2010, PAC J OPTIM, V6, P615
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Wang GH, 2008, IEEE T SYST MAN CY B, V38, P90, DOI 10.1109/TSMCB.2007.910534
   Weickert J, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P3
   Wiberg T., 1976, Second Symposium on Computational Statistics, P229
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Ye JP, 2005, MACH LEARN, V61, P167, DOI 10.1007/s10994-005-3561-6
   Zhong H, 2007, IMAGE VISION COMPUT, V25, P1814, DOI 10.1016/j.imavis.2007.04.005
   Zucker S, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P359
NR 36
TC 3
Z9 4
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2013
VL 29
IS 5
BP 421
EP 431
DI 10.1007/s00371-012-0745-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 127RD
UT WOS:000317715200009
DA 2024-07-18
ER

PT J
AU Saboia, P
   Goldenstein, S
AF Saboia, Priscila
   Goldenstein, Siome
TI Crowd simulation: applying mobile grids to the social force model
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Social force model; Mobile grid
ID BEHAVIOR; FLOW
AB The social force model (SF) is able to reproduce many emergent phenomena observed in real crowds. Unfortunately, in some situations, such as low density environments, SF may produce counterintuitive results where the trajectories simulated look more like particles than to real people. We modify the SF model through the use of a mobile grid to allow the simulated pedestrians to change the direction of their desired velocity at reasonable times, thus avoiding nearby blocked or crowded areas smoothly. Our experiments focus on qualitative behavior, and verify that our model produces the desired trajectories of the pedestrians, achieving softer and more coherent trajectories when compared to the pure SF model solution. Like SF, our model reproduces the "faster-is-slower" and the arching underlying the clogging effects. Finally, we examine the occupation rates of the space when pedestrians were submitted to narrowed corridors and observe the "edge effect.".
C1 [Saboia, Priscila; Goldenstein, Siome] Univ Estadual Campinas, Inst Comp, UNICAMP, Campinas, SP, Brazil.
C3 Universidade Estadual de Campinas
RP Saboia, P (corresponding author), Univ Estadual Campinas, Inst Comp, UNICAMP, Campinas, SP, Brazil.
EM psaboia@ic.unicamp.br
RI Goldenstein, Siome K/A-4468-2013
FU CNPq; CAPES; FAPESP
FX We would like to thank CNPq, CAPES, and FAPESP for the financial
   support. We also express our gratitude to Dirk Helbing, Illes Farkas,
   and Tamas Vicsek for kindly providing us with their source code.
CR Goldenstein S, 2001, COMPUT GRAPH-UK, V25, P983, DOI 10.1016/S0097-8493(01)00153-4
   Goldenstein S, 1999, VISUAL COMPUT, V15, P349, DOI 10.1007/s003710050184
   Guo RY, 2008, PHYSICA A, V387, P580, DOI 10.1016/j.physa.2007.10.001
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Helbing D, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.046109
   HENDERSON LF, 1971, NATURE, V229, P381, DOI 10.1038/229381a0
   Huang L, 2009, TRANSPORT RES B-METH, V43, P127, DOI 10.1016/j.trb.2008.06.003
   Hughes RL, 2002, TRANSPORT RES B-METH, V36, P507, DOI 10.1016/S0191-2615(01)00015-7
   Kapadia M., 2009, Proceedings of the 2009 symposium on Interactive 3D graphics and games, I3D '09, P215
   Lerner A., 2009, SCA, ACM, P199, DOI DOI 10.1145/1599470.1599496
   Muramatsu M, 1999, PHYSICA A, V267, P487, DOI 10.1016/S0378-4371(99)00018-7
   Musse SR, 2007, COMPUT ANIMAT VIRT W, V18, P83, DOI 10.1002/cav.163
   Musse SR, 2001, IEEE T VIS COMPUT GR, V7, P152, DOI 10.1109/2945.928167
   Perez GJ, 2002, PHYSICA A, V312, P609, DOI 10.1016/S0378-4371(02)00987-1
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Shao W., 2005, SCA 05 P 2005 ACM SI, P19, DOI DOI 10.1145/1073368.1073371
   Singh S, 2009, COMPUT ANIMAT VIRT W, V20, P533, DOI 10.1002/cav.277
   Still G., 2000, THESIS WARWICK U
   Tajima Y, 2001, PHYSICA A, V292, P545, DOI 10.1016/S0378-4371(00)00630-0
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Varas A, 2007, PHYSICA A, V382, P631, DOI 10.1016/j.physa.2007.04.006
   Xiaoyuan Tu, 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P43
NR 23
TC 30
Z9 34
U1 0
U2 44
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2012
VL 28
IS 10
SI SI
BP 1039
EP 1048
DI 10.1007/s00371-012-0731-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 003XE
UT WOS:000308643900009
DA 2024-07-18
ER

PT J
AU Liao, B
   Xiao, CX
   Liu, M
   Dong, Z
   Peng, QS
AF Liao, Bin
   Xiao, Chunxia
   Liu, Meng
   Dong, Zhao
   Peng, Qunsheng
TI Fast hierarchical animated object decomposition using approximately
   invariant signature
SO VISUAL COMPUTER
LA English
DT Article
DE Animated object; Geometry decomposition; Multi-scale features; Gaussian
   mixture models
ID MESH; POINT; SEGMENTATION
AB In this paper, we introduce a novel method to hierarchically decompose the animated 3d object efficiently by utilizing high-dimensional and multi-scale geometric information. The key idea is to treat the animated surface sequences as a whole and extract the near-rigid components from it. Our approach firstly detects a set of the multi-scale feature points on the animated object and computes approximately invariant signature vectors for these points. Then, exploiting both the geometric attributes and the local signature vector of each point (vertex) of the animated object, all the points (vertices) of the animated object can be clustered efficiently using a GPU-accelerated mean shift clustering algorithm. To refine the decomposition boundaries, the initially-generated boundaries of the animated object can be further improved by applying a boundary refinement technique based on Gaussian Mixture Models (GMMs). Furthermore, we propose a hierarchical decomposition technique using a topology merging strategy without introducing additional computations.
   Our animated object decomposition approach does not require the topological connectivity of the animated object, thus it can be applied for both triangle mesh and point-sampled geometry sequences. The experimental results demonstrate that our method achieves both good quality results and high performance for the decomposition of animated object.
C1 [Liao, Bin; Xiao, Chunxia; Liu, Meng] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
   [Xiao, Chunxia; Liu, Meng] Wuhan Univ, Sch Comp Sci & Technol, Wuhan 430072, Peoples R China.
   [Liao, Bin] Hubei Univ, Fac Math & Comp Sci, Wuhan 430062, Peoples R China.
   [Dong, Zhao] MPI Informat, Comp Graph Grp, Saarbrucken, Germany.
   [Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Wuhan University; Wuhan University; Hubei University; Max Planck
   Society; Zhejiang University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
EM bliao@hubu.edu.cn; cxxiao@whu.edu.cn; mengliu.whu@gmail.com;
   dong@mpi-sb.mpg.de; peng@cad.zju.edu.cn
RI dong, zhao/JHS-9392-2023
FU NSFC [60803081, 61070081]; Fundamental Research Funds for the Central
   Universities [6081005]; Wuhan University; Ph.D. Programs Foundation of
   Ministry of Education of China [200804861038]
FX We would like to thank the anonymous reviewers for their valuable
   comments and insightful suggestions. This work was partly supported by
   NSFC (Nos. 60803081, 61070081), the Fundamental Research Funds for the
   Central Universities (6081005), Luojia Young Scholar Research Funds of
   Wuhan University and the Ph.D. Programs Foundation of Ministry of
   Education of China (No. 200804861038).
CR Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327
   [Anonymous], 2007, CUD TECHN
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Attene M, 2010, COMPUT GRAPH FORUM, V29, P1905, DOI 10.1111/j.1467-8659.2010.01658.x
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Chu HK, 2009, IEEE T VIS COMPUT GR, V15, P853, DOI [10.1109/TVCG.2009.40, 10.1109/TVCG.2008-06-0082]
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Edelsbrunner H., 2001, PROC 17 ANN ACM SYMP, P70, DOI DOI 10.1145/378583.378626
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Golovinskiy A, 2009, COMPUT GRAPH-UK, V33, P262, DOI 10.1016/j.cag.2009.03.010
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Lee TY, 2006, VISUAL COMPUT, V22, P729, DOI 10.1007/s00371-006-0059-6
   Lee TY, 2005, COMPUT ANIMAT VIRT W, V16, P519, DOI 10.1002/cav.79
   Lewis JP, 2000, COMP GRAPH, P165, DOI 10.1145/344779.344862
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Li X., 2005, S GEOMETRY PROCESSIN, V255, P217
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Popa T, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P141
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Tierny J, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P105
   Wuhrer S, 2010, VISUAL COMPUT, V26, P147, DOI 10.1007/s00371-009-0394-5
   Xiao CX, 2010, COMPUT GRAPH FORUM, V29, P2065, DOI 10.1111/j.1467-8659.2010.01793.x
   Xiao CX, 2006, VISUAL COMPUT, V22, P210, DOI 10.1007/s00371-006-0377-8
   Xiao CX, 2004, COMPUT ANIMAT VIRT W, V15, P201, DOI 10.1002/cav.22
   Yamauchi H, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P236, DOI 10.1109/SMI.2005.21
   Zhou K, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409079
NR 31
TC 10
Z9 11
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 387
EP 399
DI 10.1007/s00371-011-0625-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300005
DA 2024-07-18
ER

PT J
AU Laga, H
AF Laga, Hamid
TI Data-driven approach for automatic orientation of 3D shapes
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE Best view selection; Boosting; Shape symmetries
ID VIEW; ALGORITHM; IMAGE
AB Visualization and visual browsing of 3D model collections require rendering the 3D models from viewpoints that allow the viewer to distinguish between them. In this paper, we introduce a new framework for the automatic selection of the best views of 3D models. We build on the assumption that models belonging to the same class of shapes share the same salient features that discriminate them from the models of other classes. This allows us to formulate the best-view selection problem as a feature selection and classification task. First a 3D model is described with a set of view-based descriptors characterizing the appearance of the model when it is seen from different viewpoints. In a second step we train a classifier that learns for each shape class the set of 2D views that maximize the intra-class similarity and the inter-class dissimilarities. Finally, we post-process the selected 2D views to estimate their upright orientation. We exploit the fact that most of natural and man-made shapes are symmetric and their upright orientation is aligned with their major axis of symmetry. Experiments on the best-view selection benchmark demonstrate that the estimated best views with our data-driven approach are robust to intra-class variations and are consistent within the models of the same class of shapes. This makes the approach suitable for online visual browsing of large 3D data collections.
C1 Telecom Lille1 LIFL, Inst Telecom, UMR8022, Lille, France.
C3 IMT - Institut Mines-Telecom; IMT Atlantique; Universite de Lille
RP Laga, H (corresponding author), Telecom Lille1 LIFL, Inst Telecom, UMR8022, Lille, France.
EM hamid.laga@etelecom-lille1.eu
RI Laga, Hamid/B-5116-2012
CR [Anonymous], 2010, P 3 EUR WORKSH 3D OB
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Chaouch M, 2009, GRAPH MODELS, V71, P63, DOI 10.1016/j.gmod.2008.12.006
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Denton T, 2004, INT C PATT RECOG, P273, DOI 10.1109/ICPR.2004.1334159
   Dutagaci H., 2010, P ACM WORKSH 3D OBJ, P45, DOI DOI 10.1145/1877808.1877819
   FU H, 2008, ACM T GRAPHIC, P1, DOI DOI 10.1145/1399504.1360641
   Karypis, 1997, FAMILY MULTILEVEL PA
   Laga Hamid., 2008, J SOC ART SCI, V7, P124
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Mokhtarian F., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P272
   Mortara M, 2009, COMPUT GRAPH-UK, V33, P280, DOI 10.1016/j.cag.2009.03.003
   Page DL, 2003, IEEE IMAGE PROC, P229
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   Takahashi S., 2005, IEEE VISUALIZATION, V0, P63
   Vazquez P.-P., 2001, Proceedings of Vision Modeling and Visualization Conference, P273
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   VEZHNEVET A, 2010, GML ADABOOST MATLAB
   Wang LW, 2009, NEURAL COMPUT, V21, P1459, DOI 10.1162/neco.2008.08-06-805
   Yamauchi H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P265
NR 24
TC 7
Z9 9
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 977
EP 989
DI 10.1007/s00371-011-0628-1
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400004
DA 2024-07-18
ER

PT J
AU Liziér, MAS
   Siqueira, MF
   Daniels, J
   Silva, CT
   Nonato, LG
AF Lizier, Mario A. S.
   Siqueira, Marcelo F.
   Daniels, Joel, II
   Silva, Claudio T.
   Gustavo Nonato, L.
TI Template-based quadrilateral mesh generation from imaging data
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based mesh generation; Template-based mesh; Quadrilateral mesh;
   Mesh smoothing; Mesh segmentation; Bezier patches
ID QUALITY MESHES
AB This paper describes a novel template-based meshing approach for generating good quality quadrilateral meshes from 2D digital images. This approach builds upon an existing image-based mesh generation technique called Imeshp, which enables us to create a segmented triangle mesh from an image without the need for an image segmentation step. Our approach generates a quadrilateral mesh using an indirect scheme, which converts the segmented triangle mesh created by the initial steps of the Imesh technique into a quadrilateral one. The triangle-to-quadrilateral conversion makes use of template meshes of triangles. To ensure good element quality, the conversion step is followed by a smoothing step, which is based on a new optimization-based procedure. We show several examples of meshes generated by our approach, and present a thorough experimental evaluation of the quality of the meshes given as examples.
C1 [Lizier, Mario A. S.] Univ Fed Sao Carlos, Dept Comp, BR-13560 Sao Carlos, SP, Brazil.
   [Siqueira, Marcelo F.] Univ Fed Rio Grande do Norte, Dept Informat & Matemat Aplicada, BR-59072970 Natal, RN, Brazil.
   [Silva, Claudio T.] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT USA.
   [Silva, Claudio T.] NYU, Comp Sci & Engn Polytech Inst, New York, NY USA.
   [Gustavo Nonato, L.] Univ Sao Paulo, Inst Ciencias Matemat & Comp, Sao Paulo, Brazil.
C3 Universidade Federal de Sao Carlos; Universidade Federal do Rio Grande
   do Norte; Utah System of Higher Education; University of Utah; New York
   University; New York University Tandon School of Engineering;
   Universidade de Sao Paulo
RP Liziér, MAS (corresponding author), Univ Fed Sao Carlos, Dept Comp, BR-13560 Sao Carlos, SP, Brazil.
EM lizier@dc.ufscar.br; mfsiqueira@dimap.ufrn.br; jdaniels@cs.utah.edu;
   csilva@poly.edu; gnonato@icmc.usp.br
RI Nonato, Luis Gustavo/D-5782-2011; Lizier, Mario/G-1572-2012; Macc,
   Inct/K-3440-2013; Siqueira, Marcelo/D-5969-2014
OI Lizier, Mario/0000-0001-9123-5822; Siqueira, Marcelo/0000-0003-3619-0531
FU Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1229185] Funding Source: National Science Foundation
CR Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   ALLMAN DJ, 1988, INT J NUMER METH ENG, V26, P717, DOI 10.1002/nme.1620260314
   Atalay FB, 2008, PROCEEDINGS OF THE 17TH INTERNATIONAL MESHING ROUNDTABLE, P73, DOI 10.1007/978-3-540-87921-3_5
   BERN M, 2000, HDB COMPUT
   Berti G, 2004, ECCOMAS EUR C COMP M
   Boissonnat JD, 2009, LECT NOTES COMPUT SC, V5416, P13
   CEBRAL J.R., 1999, PROC 8 IMR, P321
   COLEMAN S, 2005, IEEE ICIP, P1342
   Cuadros-Vargas AJ, 2009, J MATH IMAGING VIS, V33, P11, DOI 10.1007/s10851-008-0105-2
   DANIELS J, 2011, SMI 11 IN PRESS
   Everett H., 1992, P 4 CANADIAN C COMPU, P77
   GARCIA MA, 1999, IEEE INT C IM PROC K, P168
   Gevers T, 1997, PROC CVPR IEEE, P1021, DOI 10.1109/CVPR.1997.609455
   GREEN PJ, 1978, COMPUT J, V21, P168, DOI 10.1093/comjnl/21.2.168
   Herman GT., 1998, Geometry of digital spaces
   JOHNSTON BP, 1991, INT J NUMER METH ENG, V31, P67, DOI 10.1002/nme.1620310105
   Knupp PM, 2001, SIAM J SCI COMPUT, V23, P193, DOI 10.1137/S1064827500371499
   Kocharoen P, 2005, IEEE ICC, P2052
   Lai MJ, 1996, COMPUT AIDED GEOM D, V13, P81, DOI 10.1016/0167-8396(95)00007-0
   Lawson CL, 1995, CLASSICS APPL MATH, V15
   Lizier MAS, 2009, J VIS COMMUN IMAGE R, V20, P190, DOI 10.1016/j.jvcir.2009.01.002
   Malanthara A., 1997, P 6 INT MESH ROUNDT, P437
   MILLER GL, 2003, P 12 INT MESH ROUNDT, P91
   Owen SJ, 1999, INT J NUMER METH ENG, V44, P1317
   POWELL MJD, 1964, COMPUT J, V7, P155, DOI 10.1093/comjnl/7.2.155
   Ramaswami S, 2005, INT J COMPUT GEOM AP, V15, P55, DOI 10.1142/S0218195905001609
   Ramaswami S, 1998, COMP GEOM-THEOR APPL, V9, P257, DOI 10.1016/S0925-7721(97)00019-9
   RUPPERT J, 1995, J ALGORITHM, V18, P548, DOI 10.1006/jagm.1995.1021
   Schneiders Robert, 1996, TRANSITION, V2, P1
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Shewchuk J.R., 2002, Proceedings of International Meshing Roundtable, P115
   SHIMADA K, 1998, P 7 INT MESH ROUNDT, P61
   Skrinjar O, 2009, INT J BIOMED IMAGING, V2009, DOI 10.1155/2009/313517
   STAPLES J, 1995, LECT NOTES COMPUTER, V1004
   Teng SH, 2000, INT J COMPUT GEOM AP, V10, P227, DOI 10.1142/S0218195900000152
   Velho L, 2001, COMPUT AIDED GEOM D, V18, P397, DOI 10.1016/S0167-8396(01)00039-5
   Viswanath N., 2000, Proceedings of the 9th International Meshing Roundtable, P217
   Yang YY, 2003, IEEE T IMAGE PROCESS, V12, P866, DOI 10.1109/TIP.2003.812757
   Zhang Y., 2003, Proceedings of the eighth ACM symposium on Solid modeling and applications, SM '03, P286
   [No title captured]
NR 40
TC 3
Z9 3
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2011
VL 27
IS 10
SI SI
BP 887
EP 903
DI 10.1007/s00371-011-0603-x
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 831MU
UT WOS:000295736200003
DA 2024-07-18
ER

PT J
AU Seiler, M
   Steinemann, D
   Spillmann, J
   Harders, M
AF Seiler, Martin
   Steinemann, Denis
   Spillmann, Jonas
   Harders, Matthias
TI Robust interactive cutting based on an adaptive octree simulation mesh
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY JUN 12-15, 2011
CL Ottawa, CANADA
DE Physically-based modeling; Cutting; Adaptive simulation; Octree
AB We present an adaptive octree based approach for interactive cutting of deformable objects. Our technique relies on efficient refine- and node split-operations. These are sufficient to robustly represent cuts in the mechanical simulation mesh. A high-resolution surface embedded into the octree is employed to represent a cut visually. Model modification is performed in the rest state of the object, which is accomplished by back-transformation of the blade geometry. This results in an improved robustness of our approach. Further, an efficient update of the correspondences between simulation elements and surface vertices is proposed. The robustness and efficiency of our approach is underlined in test examples as well as by integrating it into a prototype surgical simulator.
C1 [Seiler, Martin; Spillmann, Jonas; Harders, Matthias] ETH, CH-8092 Zurich, Switzerland.
   [Steinemann, Denis] VirtaMed AG, CH-8005 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Seiler, M (corresponding author), ETH, Sternwartstr 7, CH-8092 Zurich, Switzerland.
EM seiler@vision.ee.ethz.ch; steinemann@virtamed.com;
   jonas.spillmann@vision.ee.ethz.ch; mharders@vision.ee.ethz.ch
CR [Anonymous], P 2007 ACM SIGGRAPH
   [Anonymous], 2004, P 2004 ACM SIGGRAPH, DOI DOI 10.1145/1028523.1028541
   Bielser D, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P116, DOI 10.1109/PCCGA.2000.883933
   Capell S., 2002, P 2002 ACM SIGGRAPHE, P41
   Delingette H, 1999, COMP ANIM CONF PROC, P70, DOI 10.1109/CA.1999.781200
   Dequidt B, 2005, COMPUT ANIMAT VIRT W, V16, P177, DOI 10.1002/cav.110
   DICK C, 2011, IEEE T VIS COMPUT GR, V99, P1077
   FIERZ B, 2010, J WSCG, V18, P81
   Forest C, 2002, COMP ANIM CONF PROC, P225, DOI 10.1109/CA.2002.1017541
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   GISSLER MTM, 2011, INT C COMP GRAPH THE
   Grinspun E, 2002, ACM T GRAPHIC, V21, P281, DOI 10.1145/566570.566578
   Ihmsen M, 2011, COMPUT GRAPH FORUM, V30, P99, DOI [10.1111/j.1467-8659.2010.01832.x, 10.1111/j.1467-8659.2010.01834.x]
   Jerabkova L, 2009, IEEE COMPUT GRAPH, V29, P61, DOI 10.1109/MCG.2009.32
   KAUFMANN P, 2009, ACM SIGGRAPH 2009 SI, V50
   Kaufmann P, 2009, GRAPH MODELS, V71, P153, DOI 10.1016/j.gmod.2009.02.002
   Labelle F, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239508
   Martin S, 2008, COMPUT GRAPH FORUM, V27, P1521, DOI 10.1111/j.1467-8659.2008.01293.x
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   Mor AB, 2000, LECT NOTES COMPUT SC, V1935, P598
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Nesme M, 2009, SIGGRAPH 09 ACM SIGG, P1
   NESME M, 2006, 2 WORKSH COMP ASS DI
   Nienhuys H.-W., 2001, LNCS, V2208, P145
   Pascucci Valerio., 2001, Proceedings of the 2001 ACM/IEEE conference on Supercom- puting (CDROM), Supercomputing '01, P2
   PAULY M, 2005, ACM SIGGRAPH 2005 PA, P957
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Seiler M., 2010, J WINTER SCH COMPUT, V18, P89
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   STEINEMANN D, 2006, VR 06, P35
   STEINEMANN D, 2008, P ACM SIGGRAPH EUR S
   Steinemann D., 2006, PROC ACM SIGGRAPHEUR, P63
   TERZOPOULOS D, 1988, SIGGRAPH 88, P269
   Wicke M, 2007, COMPUT GRAPH FORUM, V26, P355, DOI 10.1111/j.1467-8659.2007.01058.x
   WOJTAN C, 2008, ACM T GRAPH P SIGGRA
   Wojtan C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531382
NR 36
TC 28
Z9 34
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2011
VL 27
IS 6-8
BP 519
EP 529
DI 10.1007/s00371-011-0561-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 766FY
UT WOS:000290767600011
DA 2024-07-18
ER

PT J
AU Weissenfeld, A
   Liu, K
   Ostermann, J
AF Weissenfeld, Axel
   Liu, Kang
   Ostermann, Joern
TI Video-realistic image-based eye animation via statistically driven state
   machines
SO VISUAL COMPUTER
LA English
DT Article
DE Eye animation; Talking-heads; Sample-based image synthesis; Computer
   vision
ID HEAD; MOVEMENTS
AB In this work we elaborate on a novel image-based system for creating video-realistic eye animations to arbitrary spoken output. These animations are useful to give a face to multimedia applications such as virtual operators in dialog systems. Our eye animation system consists of two parts: eye control unit and rendering engine, which synthesizes eye animations by combining 3D and image-based models. The designed eye control unit is based on eye movement physiology and the statistical analysis of recorded human subjects. As already analyzed in previous publications, eye movements vary while listening and talking. We focus on the latter and are the first to design a new model which fully automatically couples eye blinks and movements with phonetic and prosodic information extracted from spoken language. We extended the already known simple gaze model by refining mutual gaze to better model human eye movements. Furthermore, we improved the eye movement models by considering head tilts, torsion, and eyelid movements. Mainly due to our integrated blink and gaze model and to the control of eye movements based on spoken language, subjective tests indicate that participants are not able to distinguish between real eye motions and our animations, which has not been achieved before.
C1 [Weissenfeld, Axel; Liu, Kang; Ostermann, Joern] Leibniz Univ Hannover, Inst Informat Verarbeitung, D-30167 Hannover, Germany.
C3 Leibniz University Hannover
RP Weissenfeld, A (corresponding author), Leibniz Univ Hannover, Inst Informat Verarbeitung, Appelstr 9A, D-30167 Hannover, Germany.
EM aweissen@tnt.uni-hannover.de; kang@tnt.uni-hannover.de;
   ostermann@tnt.uni-hannover.de
OI Weissenfeld, Axel/0000-0002-7246-2744
CR Aitchison J., 1973, The Lognormal Distribution
   [Anonymous], 1998, QUATERNION ROTATION
   Argyle M., 1976, Gaze and Mutual Gaze
   ARONS B, 1994, P ICSLP 94, P1931
   BARNES GR, 1979, J PHYSIOL-LONDON, V287, P127, DOI 10.1113/jphysiol.1979.sp012650
   BREGLER C, 1997, ANN C SERIES
   Cassell J, 1998, MACHINE CONVERSATION
   Cassell J., 1994, P 21 ANN C COMP GRAP, P413, DOI DOI 10.1145/192161.192272
   COLBURN A, 2000, MSRTR200081
   CONDON WS, 1967, J PSYCHIAT RES, V5, P221, DOI 10.1016/0022-3956(67)90004-0
   COSATTO E, 2002, THESIS SWISS FEDERAL
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   Deng ZG, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.35
   Donders F.C., 1848, Hollndische Beitrge zu den anatomischen und physiologischen Wissenschaften, V1, P104
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Freedman EG, 2000, EXP BRAIN RES, V131, P22, DOI 10.1007/s002219900296
   Fukayama A., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P41, DOI 10.1145/503376.503385
   Garau M., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P309, DOI 10.1145/365024.365121
   HASLWANTER T, 1995, VISION RES, V35, P1727, DOI 10.1016/0042-6989(94)00257-M
   HELMHOLTZ H, 1863, ARCH OPHTHALMOLOGIE, V9, P153
   HEYLEN D, 2005, EXPT GAZE CONVERSATI
   *ITU INT TEL, 1999, ITUTP910 ITU INT TEL
   *ITU TEL, 2002, BT50011 ITUR
   KENDON A, 1967, ACTA PSYCHOL, V26, P22, DOI 10.1016/0001-6918(67)90005-4
   KENNEDY L, 2003, IEEE AUT SPEECH REC, P243
   Kolmogoroff A, 1941, ANN MATH STAT, V12, P461, DOI 10.1214/aoms/1177731684
   *LCTECHNOLOGIES, EYEG SYST
   LEE SP, 2002, SIGGRAPH 02, P637
   Limpert E, 2001, BIOSCIENCE, V51, P341, DOI 10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2
   MAAND X, 2009, VR 09, P143
   Masuko S., 2006, Systems and Computers in Japan, V37, P33, DOI 10.1002/scj.20513
   Ostermann J, 2004, INT C PATT RECOG, P826, DOI 10.1109/ICPR.2004.1334656
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   Pighin F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P75, DOI 10.1145/280814.280825
   Poggi I, 2000, AI COMMUN, V13, P169
   Schworm HD, 2002, INVEST OPHTH VIS SCI, V43, P662
   Stahl JS, 1999, EXP BRAIN RES, V126, P41, DOI 10.1007/s002210050715
   TERKEN J, 1994, J ACOUST SOC AM, V95, P3662, DOI 10.1121/1.409936
   VONCRANACH M, 1969, PSYCHOL FORSCH, V33, P68, DOI 10.1007/BF00424617
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Young S.J., 2005, HTK BOOK
   Zheng J, 2000, INT CONF ACOUST SPEE, P1775, DOI 10.1109/ICASSP.2000.862097
NR 42
TC 5
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2010
VL 26
IS 9
BP 1201
EP 1216
DI 10.1007/s00371-009-0401-x
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 635JD
UT WOS:000280650500006
DA 2024-07-18
ER

PT J
AU Groenegress, C
   Spanlang, B
   Slater, M
AF Groenegress, Christoph
   Spanlang, Bernhard
   Slater, Mel
TI The physiological mirror-a system for unconscious control of a virtual
   environment through physiological activity
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Human-computer interaction; Physiological processing;
   Whole-body interaction
ID RESPONSES
AB This paper introduces a system for real-time physiological measurement, analysis, and metaphorical visualization within a virtual environment (VE). Our goal is to develop a method that allows humans to unconsciously relate to parts of an environment more strongly than to others, purely induced by their own physiological responses to the virtual reality (VR) displays. In particular, we exploit heart rate, respiration, and galvanic skin response in order to control the behavior of virtual characters in the VE. Such unconscious processes may become a useful tool for storytelling or assist guiding participants through a sequence of tasks in order to make the application more interesting, e.g., in rehabilitation. We claim that anchoring of subjective bodily states to a virtual reality (VR) can enhance a person's sense of realism of the VR and ultimately create a stronger relationship between humans and the VR.
C1 [Groenegress, Christoph; Spanlang, Bernhard; Slater, Mel] Univ Barcelona, Dept Personalitat Avaluacio & Tractaments Psicol, EVENT Lab, Barcelona, Spain.
   [Slater, Mel] ICREA, Barcelona, Spain.
C3 University of Barcelona; ICREA
RP Slater, M (corresponding author), Univ Barcelona, Dept Personalitat Avaluacio & Tractaments Psicol, EVENT Lab, Barcelona, Spain.
EM melslater@ub.edu
RI Slater, Mel/M-5210-2014
OI Slater, Mel/0000-0002-6223-0050; Spanlang, Bernhard/0000-0002-9659-635X
FU European Union [27731]; European Project [MIMICS 215 756]; ICREA Funding
   Source: Custom
FX This work was started under the European Union FET project PRESENCCIA
   Contract Number 27731, and completed under the European Project MIMICS
   215 756.
CR Akizuki H, 2005, NEUROSCI LETT, V379, P23, DOI 10.1016/j.neulet.2004.12.041
   Andreassi J. L., 2000, Psychophysiology human behavior and physiological response, V4th
   [Anonymous], 2007, INT J BIOELECTROMAGN
   ANTLEY A, 2010, IEEE T VIS COMPUT GR
   BROGNI A, 2007, INT J VIRTUAL REALIT, V6, P1, DOI DOI 10.HTTP://WWW.IJVR.0RG/ISSUES/ISSUE2-2007/1.PDF
   CARDILLO C, 2007, LECT NOTES COMPUTER, P137
   Dawson ME, 2007, HANDBOOK OF PSYCHOPHYSIOLOGY, 3RD EDITION, P159, DOI 10.1017/cbo9780511546396.007
   Garau M, 2005, PRESENCE-TELEOP VIRT, V14, P104, DOI 10.1162/1054746053890242
   GILLIES M, 2005, 8 ANN INT WORKSH PRE, V8, P103
   GROENEGRESS C, 2010, PRESENCE TE IN PRESS
   Groenegress C, 2009, CYBERPSYCHOL BEHAV, V12, P429, DOI 10.1089/cpb.2007.0256
   Hall Edward T., 1966, The Hidden Dimension
   Heater C., 1992, Presence: Teleoperators and Virtual Environments, V1, P262, DOI DOI 10.1162/PRES.1992.1.2.262
   Huang RS, 2007, LECT NOTES ARTIF INT, V4565, P65
   Jang DP, 2002, CYBERPSYCHOL BEHAV, V5, P11, DOI 10.1089/109493102753685845
   Meehan M, 2003, P IEEE VIRT REAL ANN, P141, DOI 10.1109/VR.2003.1191132
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   NAGAI Y, 2007, NEW RES BIOFEEDBACK, P1
   Pan XN, 2008, LECT NOTES COMPUT SC, V5208, P89
   Pertaub DP, 2002, PRESENCE-TELEOP VIRT, V11, P68, DOI 10.1162/105474602317343668
   Sandro B., 2005, Proceedings of the 2005 ACM SIGCHI International Con- ference on Advances in Computer Entertainment Technology, P270, DOI DOI 10.1145/1178477.1178524
   SLATER M, 2010, PLOS ONE IN PRESS
   Slater M, 2006, PLOS ONE, V1, DOI 10.1371/journal.pone.0000039
   Slater M, 2006, PRESENCE-VIRTUAL AUG, V15, P553, DOI 10.1162/pres.15.5.553
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Slater M, 2008, FRONT HUM NEUROSCI, V2, DOI 10.3389/neuro.09.006.2008
   Slater Mel, 1995, ACM Transactions on Computer-Human Interaction, V2, P201, DOI DOI 10.1145/210079.210084
   SPANLANG B, 2009, 20091 EV LAB
   Tarvainen MP, 2001, IEEE T BIO-MED ENG, V48, P1071, DOI 10.1109/10.951509
   Venables P. H., 1980, TECHNIQUES PSYCHOPHY, P2
NR 30
TC 7
Z9 8
U1 1
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 649
EP 657
DI 10.1007/s00371-010-0471-9
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800025
DA 2024-07-18
ER

PT J
AU Pajak, D
   Cadík, M
   Aydin, TO
   Okabe, M
   Myszkowski, K
   Seidel, HP
AF Pajak, Dawid
   Cadik, Martin
   Aydin, Tunc Ozan
   Okabe, Makoto
   Myszkowski, Karol
   Seidel, Hans-Peter
TI Contrast prescription for multiscale image editing
SO VISUAL COMPUTER
LA English
DT Article
DE Multiscale image editing; Contrast enhancement; Interactive image
   processing; HDR; Computational photography; Image decomposition
ID RESTORATION
AB Recently proposed edge-preserving multi-scale image decompositions enable artifact-free and visually appealing image editing. As the human eye is sensitive to contrast, per-band contrast manipulation is a natural way of image editing. However, contrast modification in one band usually affects contrasts in other bands, which is not intuitive for the user. In practice, the desired image appearance is achieved through an iterative editing process, which often requires fine tuning of contrast in one band several times. In this article we show an analysis of properties of multiscale contrast editing frameworks and we introduce the concept of contrast prescription, which enables the user to lock the contrast in selected areas and bands and make it immune to contrast manipulations in other bands.
C1 [Pajak, Dawid] W Pomeranian Univ Technol, Dept Comp Sci, PL-71210 Szczecin, Poland.
   [Cadik, Martin; Aydin, Tunc Ozan; Okabe, Makoto; Myszkowski, Karol; Seidel, Hans-Peter] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
C3 West Pomeranian University of Technology; Max Planck Society
RP Pajak, D (corresponding author), W Pomeranian Univ Technol, Dept Comp Sci, Zolnierska 49, PL-71210 Szczecin, Poland.
EM dpajak@mpi-inf.mpg.de; mcadik@mpi-inf.mpg.de; tunc@mpi-inf.mpg.de;
   mokabe@mpi-inf.mpg.de; karol@mpi-inf.mpg.de; hpseidel@mpi-inf.mpg.de
RI Cadik, Martin/O-4824-2014
OI Cadik, Martin/0000-0001-7058-9912; Myszkowski, Karol/0000-0002-8505-4141
CR [Anonymous], ACM T GRAPH
   [Anonymous], 2000, HDB IMAGE VIDEO PROC
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Jansen M., 2005, Second generation wavelets and applications
   Krawczyk G, 2007, COMPUT GRAPH FORUM, V26, P581, DOI 10.1111/j.1467-8659.2007.01081.x
   LAGENDIJK RL, 1988, IEEE T ACOUST SPEECH, V36, P1874, DOI 10.1109/29.9032
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li Y, 2008, COMPUT GRAPH FORUM, V27, P1255, DOI 10.1111/j.1467-8659.2008.01264.x
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Livingstone M., 2002, Vision and art: The biology of seeing
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   SUBR K, 2009, ACM T GRAPHICS
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Uytterhoeven G., 1997, WAVELET TRANSFORMS U
NR 29
TC 4
Z9 4
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 739
EP 748
DI 10.1007/s00371-010-0485-3
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sheng, Y
   Sourin, A
   Castro, GG
   Ugail, H
AF Sheng, Yun
   Sourin, Alexei
   Castro, Gabriela Gonzalez
   Ugail, Hassan
TI A PDE method for patchwise approximation of large polygon meshes
SO VISUAL COMPUTER
LA English
DT Article
DE Partial differential equations; Surface modeling; Surface approximation;
   3D reconstruction
ID DESIGN; SURFACES
AB Three-dimensional (3D) representations of complex geometric shapes, especially when they are reconstructed from magnetic resonance imaging (MRI) and computed tomography (CT) data, often result in large polygon meshes which require substantial storage for their handling, and normally have only one fixed level of detail (LOD). This can often be an obstacle for efficient data exchange and interactive work with such objects. We propose to replace such large polygon meshes with a relatively small set of coefficients of the patchwise partial differential equation (PDE) function representation. With this model, the approximations of the original shapes can be rendered with any desired resolution at interactive rates. Our approach can directly work with any common 3D reconstruction pipeline, which we demonstrate by applying it to a large reconstructed medical data set with irregular geometry.
C1 [Sheng, Yun; Sourin, Alexei] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Castro, Gabriela Gonzalez; Ugail, Hassan] Univ Bradford, Ctr Visual Comp, Bradford BD7 1DP, W Yorkshire, England.
C3 Nanyang Technological University; University of Bradford
RP Sourin, A (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM assourin@ntu.edu.sg
RI Sourin, Alexei/A-3701-2011
OI Sourin, Alexei/0000-0003-4051-2927
FU Singapore National Research Foundation [NRF2008IDM-IDM004-002]; UK
   Engineering and Physical Sciences Research Council [EP/G067732/1]; EPSRC
   [EP/G067732/1] Funding Source: UKRI
FX This work was supported by the Singapore National Research Foundation
   Interactive Digital Media R&D Program, under research Grant
   NRF2008IDM-IDM004-002" Visual and Haptic Rendering in Co-Space", and by
   the UK Engineering and Physical Sciences Research Council Grant
   EP/G067732/1" Function based Geometry Modeling within Visual
   Cyberworlds". The authors sincerely thank Michael Athanasopoulos, a
   research student at the University of Bradford, for his assistance in
   extracting boundary curves and programming.
CR Bernardini F, 2002, COMPUT GRAPH FORUM, V21, P149, DOI 10.1111/1467-8659.00574
   Bloor M.I.G., 1993, COMPUTING S, V8, P21
   BLOOR MIG, 1990, COMPUT AIDED DESIGN, V22, P202, DOI 10.1016/0010-4485(90)90049-I
   Bloor MIG, 1995, J AIRCRAFT, V32, P1269, DOI 10.2514/3.46874
   BLOOR MIG, 1989, COMPUT AIDED DESIGN, V21, P165, DOI 10.1016/0010-4485(89)90071-7
   Castro Gabriela Gonzalez, 2007, Journal of Multimedia, V2, P15, DOI 10.4304/jmm.2.6.15-25
   Dekanski CW, 1997, J PROPUL POWER, V13, P398, DOI 10.2514/2.5177
   Dekanski CW, 1996, J SHIP RES, V40, P117
   Du H, 2000, COMPUT GRAPH FORUM, V19, pC261, DOI 10.1111/1467-8659.00418
   Du HX, 2007, IEEE T VIS COMPUT GR, V13, P549, DOI 10.1109/TVCG.2007.1004
   Kubiesa S, 2004, VISUAL COMPUT, V20, P682, DOI 10.1007/s00371-004-0261-3
   LOWE TW, 1994, J SHIP RES, V38, P319
   Ugail H, 1999, ACM T GRAPHIC, V18, P195, DOI 10.1145/318009.318078
   Ugail H, 2005, THEOR BIOL MED MODEL, V2, DOI 10.1186/1742-4682-2-28
   Ugail H, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P224, DOI 10.1109/CW.2008.42
   You LH, 2003, COMPUTING, V71, P353, DOI 10.1007/s00607-003-0028-0
   Zhang YC, 2004, ORG LETT, V6, P23, DOI 10.1021/ol036020y
NR 17
TC 11
Z9 12
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 975
EP 984
DI 10.1007/s00371-010-0456-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800057
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Eagleson, R
   Peters, TM
AF Zhang, Qi
   Eagleson, Roy
   Peters, Terry M.
TI High-quality cardiac image dynamic visualization with feature
   enhancement and virtual surgical tool inclusion
SO VISUAL COMPUTER
LA English
DT Article
DE Color encoding; Cardiac anatomical feature enhancement; Boundary color
   adjustment; Volume shading; Virtual surgical tools
ID REAL-TIME; VOLUME; GUIDANCE; MODELS; MDCT; MRI; CT
AB Traditional approaches for rendering segmented volumetric data sets usually deliver unsatisfactory results, such as insufficient frame rate, low image quality, and intermixing artifacts. In this paper, we introduce a novel "color encoding" technique, based on graphics processing unit (GPU) accelerated raycasting and post-color attenuated classification, to address this problem. The result is an algorithm that can generate artifact-free dynamic volumetric images in real time. Next, we present a pre-integrated volume shading algorithm to reduce graphics memory requirements and computational cost when compared to traditional shading methods. We also present a normal-adjustment technique to improve image quality at clipped planes. Furthermore, we propose a new algorithm for color and depth texture indexing that permits virtual solid objects, such as surgical tools, to be manipulated within the dynamically rendered volumetric cardiac images in real time. Finally, all these techniques are combined within an environment that permits real-time visualization, enhancement, and manipulation of dynamic cardiac data sets.
C1 [Zhang, Qi; Peters, Terry M.] Univ Western Ontario, Imaging Res Labs, Robarts Res Inst, London, ON, Canada.
C3 Western University (University of Western Ontario)
RP Zhang, Q (corresponding author), Univ Western Ontario, Imaging Res Labs, Robarts Res Inst, London, ON, Canada.
EM qzhang@imaging.robarts.ca; eagleson@imaging.robarts.ca;
   tpeters@robarts.ca
RI Peters, Terry Malcolm/AAD-7797-2022; Peters, Terry M/K-6853-2013;
   Eagleson, Roy/B-7702-2015
OI Peters, Terry Malcolm/0000-0003-1440-7488; Eagleson,
   Roy/0000-0001-9264-8135
FU Canadian Institutes for Health Research [MOP 74626]; National Science
   and Engineering Research Council of Canada [R314GA01]; Ontario Research
   and Development Challenge Fund; Canadian Foundation for Innovation;
   Ontario Innovation Trust; Ontario Ministry of Education; University of
   Western Ontario
FX The authors would like to thank Drs. G. Guiraudon and J. White for the
   clinical based performance evaluation and analysis, Dr. A. Islam for
   clinical data acquirement, J. Moore, C. Wedlake, and Drs. U. Aladl and
   M. Wierzbicki for valuable discussions and technical support, and Ms. J.
   Williams for editorial assistance. This project was supported by the
   Canadian Institutes for Health Research (Grant MOP 74626), the National
   Science and Engineering Research Council of Canada (Grant R314GA01), the
   Ontario Research and Development Challenge Fund, the Canadian Foundation
   for Innovation and Ontario Innovation Trust. Graduate student funding
   for Qi Zhang was provided by scholarships from the Ontario Ministry of
   Education and by the University of Western Ontario.
CR [Anonymous], 2005, GPU GEMS
   Boll DT, 2006, AM J ROENTGENOL, V186, pS379, DOI 10.2214/AJR.04.1781
   Buhler K., 2005, DIGITAL REVOLUTION R, Vsecond
   Bullitt E, 2002, IEEE T MED IMAGING, V21, P998, DOI 10.1109/TMI.2002.803088
   Cabral B., 1994, P 1994 S VOLUME VISU, P91, DOI DOI 10.1145/197938.197972
   Dong F, 2005, VISUAL COMPUT, V21, P463, DOI 10.1007/s00371-005-0294-2
   Engel K., 2001, HWWS 01, P9
   HADWIGER M, 2003, VIS 03, P40, DOI DOI 10.1109/VISUAL.2003.1250386
   HADWIGER M, 2006, REALTIME VOLUME GRAP
   Hauser H, 2001, IEEE T VIS COMPUT GR, V7, P242, DOI 10.1109/2945.942692
   Higuera FV, 2005, PROC SPIE, V5744, P13, DOI 10.1117/12.593314
   Hohne K. H., 1990, Visual Computer, V6, P28, DOI 10.1007/BF01902627
   HOHNE KH, 1992, J COMPUT ASSIST TOMO, V16, P285, DOI 10.1097/00004728-199203000-00019
   Kniss J, 2003, IEEE T VIS COMPUT GR, V9, P150, DOI 10.1109/TVCG.2003.1196003
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   KRATZ A, 2006, INT WORKSH AUGM ENV
   Krüger J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P287, DOI 10.1109/VISUAL.2003.1250384
   Lai HY, 2008, AM J ROENTGENOL, V191, P1711, DOI 10.2214/AJR.07.3813
   Lawler LP, 2005, AM J ROENTGENOL, V184, P774, DOI 10.2214/ajr.184.3.01840774
   Lehmann H, 2006, PROC SPIE, V6141, DOI [10.1117/12.652087, 10.1117/12.652109]
   Levin D, 2005, COMPUT MED IMAG GRAP, V29, P463, DOI 10.1016/j.compmedimag.2005.02.007
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Linte CA, 2008, COMPUT AIDED SURG, V13, P82, DOI [10.1080/10929080801951160, 10.3109/10929080801951160]
   Linte CA, 2007, PROC SPIE, V6509, DOI 10.1117/12.711041
   LUM EB, 2004, JOINT EUROGRAPHICS I
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mochizuki N, 2008, 7TH INTERNATIONAL CONFERENCE ON SYSTEM SIMULATION AND SCIENTIFIC COMPUTING ASIA SIMULATION CONFERENCE 2008, VOLS 1-3, P555, DOI 10.1109/ASC-ICSC.2008.4675422
   Peters TM, 2006, PHYS MED BIOL, V51, pR505, DOI 10.1088/0031-9155/51/14/R01
   REITINGER B, 2006, 3DUI 06, P37
   REZKSALAMA C, 2001, THESIS U SIEGEN GERM
   Rheingans P, 2001, IEEE T VIS COMPUT GR, V7, P253, DOI 10.1109/2945.942693
   Spevak PJ, 2008, AM J ROENTGENOL, V191, P854, DOI 10.2214/AJR.07.2889
   Subramanian N, 2006, PROC SPIE, V6141, DOI 10.1117/12.653240
   Termeer M, 2007, IEEE T VIS COMPUT GR, V13, P1632, DOI 10.1109/TVCG.2007.70550
   Tiede U, 1998, VISUALIZATION '98, PROCEEDINGS, P255, DOI 10.1109/VISUAL.1998.745311
   van Dam A, 2000, IEEE COMPUT GRAPH, V20, P26, DOI 10.1109/38.888006
   VanGelder A, 1996, 1996 SYMPOSIUM ON VOLUME VISUALIZATION, PROCEEDINGS, P23, DOI 10.1109/SVV.1996.558039
   VANOOIJEN PMA, 2003, RADIOGRAPHICS, V28, pE16
   Vernhet-Kovacsik H, 2006, AM J ROENTGENOL, V186, pS395, DOI 10.2214/AJR.04.1773
   Weiskopf D, 2003, IEEE T VIS COMPUT GR, V9, P298, DOI 10.1109/TVCG.2003.1207438
   Wierzbicki M, 2004, MED IMAGE ANAL, V8, P387, DOI 10.1016/j.media.2004.06.014
   Wu YC, 2007, IEEE T VIS COMPUT GR, V13, P1027, DOI 10.1109/TVCG.2007.1051
   Zajtchuk R, 1997, COMMUN ACM, V40, P63, DOI 10.1145/260750.260768
   Zhang Q, 2007, PROC SPIE, V6509, DOI 10.1117/12.710682
   Zhang Q, 2006, I S BIOMED IMAGING, P343
   Zhang Q, 2008, PROC SPIE, V6918, DOI 10.1117/12.773225
   Zhang Q, 2007, I S BIOMED IMAGING, P1168
   Zhang Q, 2007, LECT NOTES COMPUT SC, V4792, P86
NR 48
TC 4
Z9 4
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 1019
EP 1035
DI 10.1007/s00371-009-0364-y
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700005
DA 2024-07-18
ER

PT J
AU Dos Passos, VA
   Walter, M
AF Dos Passos, V. A.
   Walter, M.
TI 3D virtual mosaics: Opus Palladium and mixed styles
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 21st Brazilian Symposium on Computer Graphics and Image Processing
CY OCT 12-15, 2008
CL Campo Grande, BRAZIL
SP Brazilian Comput Soc
DE 3D mosaics; Surface mosaics
AB One of the oldest art forms, mosaics are built by careful selection and placement of small pieces called tiles. Although 2D mosaics have attracted attention in computer graphics research, 3D virtual mosaic sculptures are less common. In this work, we present a method to simulate mosaic sculptures using tiles with irregular shapes, a method known by mosaicists as Opus Palladium, or simply "crazy paving," due to the inherent freedom of mixing the tiles. In order to add expressiveness and emphasize some features, artists distribute the tiles following a high-level design over the shape. We use Voronoi polygons to represent the tiles computed from a distribution of points on the surface of the 3D object. We also address the simulation of mixed mosaics, where both irregular and squared-shape tiles are used on the same object. Previous works on such surface mosaics have used only square-shaped tiles, with fixed or variable size. Special mosaic-like effects are obtained with the help from texture maps, which control the high-level design of the tile distribution.
C1 [Dos Passos, V. A.; Walter, M.] Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
C3 Universidade Federal de Pernambuco
RP Dos Passos, VA (corresponding author), Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
EM vap2@cin.ufpe.br; marcelow@cin.ufpe.br
RI Walter, Marcelo/O-7526-2019; Walter, Marcelo/A-1964-2013
OI Walter, Marcelo/0000-0002-5634-8765; 
CR [Anonymous], ACM SIGGRAPH 1994 C
   Battiato S, 2007, COMPUT GRAPH FORUM, V26, P794, DOI 10.1111/j.1467-8659.2007.01021.x
   BATTIATO S, 2008, P EUR 2008 SHORT PAP
   BATTIATO S, 2006, P EUR IT CHAPT
   BLESER TW, 1988, ACM T GRAPHIC, V7, P76, DOI 10.1145/42188.42230
   CURTIS CJ, 1997, ANN C SERIES, P421
   Di Blasi G, 2005, VISUAL COMPUT, V21, P373, DOI 10.1007/s00371-005-0292-4
   Dos Passos VA, 2008, VISUAL COMPUT, V24, P617, DOI 10.1007/s00371-008-0242-z
   Drago F, 2004, VISUAL COMPUT, V20, P314, DOI 10.1007/s00371-004-0240-8
   Elber G, 2003, VISUAL COMPUT, V19, P67, DOI 10.1007/s00371-002-0175-x
   Faustino GM, 2005, SIBGRAPI 2005: XVIII BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, CONFERENCE PROCEEDINGS, P315
   Foley J.D., 1995, COMPUTER GRAPHICS PR, V2nd
   Haeberli P., 1990, P 17 ANN C COMP GRAP, P207, DOI [10.1145/97879.97902, DOI 10.1145/97879.97902]
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   King Sonia., 2003, MOSAIC TECHNIQUES TR
   Lai YK, 2006, VISUAL COMPUT, V22, P604, DOI 10.1007/s00371-006-0047-x
   Lee H., 2006, NPAR 2006, P37, DOI [10. 1145/1124728. 1124735, DOI 10.1145/1124728.1124735]
   Lee KJ, 2007, VISUAL COMPUT, V23, P873, DOI 10.1007/s00371-007-0142-7
   LUFT T., 2006, NPAR 2006, P11
   Schlechtweg S, 2005, COMPUT GRAPH FORUM, V24, P137, DOI 10.1111/j.1467-8659.2005.00838.x
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Turk G., 1990, GRAPHICS GEMS GENERA, P24
   Walter M, 2001, COMP GRAPH, P317, DOI 10.1145/383259.383294
NR 24
TC 2
Z9 2
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 939
EP 946
DI 10.1007/s00371-009-0370-0
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 500TP
UT WOS:000270328200005
DA 2024-07-18
ER

PT J
AU Wallner, G
AF Wallner, Guenter
TI An extended GPU radiosity solver
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Extended radiosity; Global illumination; GPU programming;
   Interreflections
AB In this paper we present an extended GPU progressive radiosity solver which integrates ideal diffuse as well as specular transmittance and reflection. The solver is capable to handle multiple specular reflections with correct mirror-object-mirror occlusions. The use of graphics hardware allows to consider attenuation of radiation due to reflections and/or transmissions on a per-pixel basis, enabling us to handle multiple specular triangles with different reflection coefficients at once. Alpha masks are used to replace complex geometry in certain cases to reduce computation times. Furthermore, the inclusion of ambient overshooting into the radiosity solver is discussed.
C1 Univ Appl Arts Vienna, Inst Art & Technol, A-1010 Vienna, Austria.
RP Wallner, G (corresponding author), Univ Appl Arts Vienna, Inst Art & Technol, Oskar Kakoschka Pl 2, A-1010 Vienna, Austria.
EM wallner.guenter@uni-ak.ac.at
OI Wallner, Guenter/0000-0002-0815-5985
CR BARSI A, 2004, P 8 CENTR EUR SEM CO
   CARR NA, 2003, HWWS 03, P51
   COHEN MF, 1985, SIGGRAPH 85, P31
   COHEN MF, 1988, SIGGRAPH 88, P75
   Coombe G, 2004, PROC GRAPH INTERF, P161
   COOMBE G, 2005, GPU GEMS 2, P635
   FEDA M, 1992, 3 EUR WORKSH REND BR, P21
   Glaeser G., 2000, J. Geometry Graph., V4, P1
   GORAL CM, 1984, SIGGRAPH 84, P213
   GORTLER S, 1994, IEEE COMPUT GRAPH, V14, P48, DOI 10.1109/38.329094
   HANRAHAN P, 1991, COMP GRAPH, V25, P197
   Immel D. S., 1986, Computer Graphics, V20, P133, DOI 10.1145/15886.15901
   Kautz J., 2004, Proceedings of the Eurographics Symposium on Rendering, P179
   LEISS T, 1998, P 14 SPRING C COMP G, P103
   LENGYEL E, 2005, GAME PROGRAMMING GEM, V5
   Li SY, 2000, VISUAL COMPUT, V16, P481, DOI 10.1007/s003710000084
   MCREYNOLDS T, 1999, SIGGRAPH 1999 COURSE
   Modest M. F., 2003, RAD HEAT TRANSFER
   Nielsen KasperHoy., 2002, J. Graph. Tools, V6, P1
   Nielsen KH, 2002, WSCG'2002 SHORT COMMUNICATION PAPERS, CONFERENCE PROCEEDINGS, P91
   Reinhard E., 2002, Journal of Graphics Tools, V7, P45, DOI 10.1080/10867651.2002.10487554
   RUSHMEIER HE, 1990, ACM T GRAPHIC, V9, P1, DOI 10.1145/77635.77636
   Shao MZ, 1993, GATHERING SHOOTING P
   SILLION FX, 1991, COMP GRAPH, V25, P187, DOI 10.1145/127719.122739
   WALLACE JR, 1989, SIGGRAPH 89, P315
   Wallner G., 2008, J WSCG, V16
NR 26
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 529
EP 537
DI 10.1007/s00371-009-0347-z
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300017
DA 2024-07-18
ER

PT J
AU Knoll, AM
   Wald, I
   Hansen, CD
AF Knoll, Aaron M.
   Wald, Ingo
   Hansen, Charles D.
TI Coherent multiresolution isosurface ray tracing
SO VISUAL COMPUTER
LA English
DT Article
DE Ray tracing; Isosurfaces; Volume data; Compression; Level of detail
AB We implement and evaluate a fast ray tracing method for rendering large structured volumes. Input data is losslessly compressed into an octree, enabling residency in CPU main memory. We cast packets of coherent rays through a min/max acceleration structure within the octree, employing a slice-based technique to amortize the higher cost of compressed data access. By employing a multiresolution level of detail (LOD) scheme in conjunction with packets, coherent ray tracing can efficiently render inherently incoherent scenes of complex data. We achieve higher performance with lesser footprint than previous isosurface ray tracers, and deliver large frame buffers, smooth gradient normals and shadows at relatively lesser cost. In this context, we weigh the strengths of coherent ray tracing against those of the conventional single-ray approach, and present a system that visualizes large volumes at full data resolution on commodity computers.
C1 [Knoll, Aaron M.; Wald, Ingo; Hansen, Charles D.] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Knoll, AM (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM knolla@sci.utah.edu; wald@sci.utah.edu; hansen@sci.utah.edu
FU U. S. Department of Energy [W-7405-ENG-48, DE-FC02-06ER25781]; National
   Science Foundation [CRI-0513212, CCF-0541113, SEII-0513212]; Intel Corp
FX This work was supported by the U. S. Department of Energy through CSAFE
   grant W-7405-ENG-48, and by the National Science Foundation under CISE
   grants CRI-0513212, CCF-0541113, and SEII-0513212. It was also supported
   by the US Department of Energy SciDAC VACET, Contract No.
   DE-FC02-06ER25781 (SciDAC VACET), and a visiting professorship sponsored
   by Intel Corp. Thanks to Mark Duchaineau at Lawrence Livermore National
   Laboratory for use of the Richtmyer-Meshkov dataset, and to Steve Parker
   and Heiko Friedrich for their support and insights.
CR Amanatides J., 1987, EUROGRAPHICS, V87, P3
   Boada I, 2001, VISUAL COMPUT, V17, P185, DOI 10.1007/PL00013406
   Cabral B, 1994, VVS 94, P91, DOI DOI 10.1145/197938.197972
   Castanié L, 2006, IEEE T VIS COMPUT GR, V12, P1299, DOI 10.1109/TVCG.2006.135
   DeMarle DE, 2003, PVG 2003 PROCEEDINGS, P87, DOI 10.1109/PVGS.2003.1249046
   DJEU P, 2007, ARCHITECTURE DYNAMIC
   FRIEDRICH H, 2007, P 2007 EUR S PAR GRA
   GRIBBLE C, 2006, UUSCI2006024
   Guthe S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P53, DOI 10.1109/VISUAL.2002.1183757
   Igehy H, 1999, COMP GRAPH, P179, DOI 10.1145/311535.311555
   Knittel G., 2000, S VOLUME VISUALIZATI, P71, DOI DOI 10.1145/353888.353901
   KNOLL A, 2007, P 2007 EUR IEEE S IN
   KNOLL A, 2006, P IEEE S INT RAY TRA
   KRAUS M, 2002, P ACM SIGGRAPH EUR W
   LAMAR E, 1999, P IEEE VIS 1999
   LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965
   Liu ZY, 2002, COMPUT GRAPH-UK, V26, P209, DOI 10.1016/S0097-8493(02)00052-3
   Livnat Y, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P457, DOI 10.1109/VISUAL.2004.52
   Livnat Y, 1998, VISUALIZATION '98, PROCEEDINGS, P175, DOI 10.1109/VISUAL.1998.745300
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MARMITT G, 2004, P VIS MOD VIS VMV, P429
   Parker S, 1998, VISUALIZATION '98, PROCEEDINGS, P233, DOI 10.1109/VISUAL.1998.745713
   Parker S, 1999, IEEE T VIS COMPUT GR, V5, P238, DOI 10.1109/2945.795215
   Reshetov A, 2005, ACM T GRAPHIC, V24, P1176, DOI 10.1145/1073204.1073329
   RUIJTERS D, 2006, OPTIMIZING GPU VOLUM
   Wald I, 2005, IEEE T VIS COMPUT GR, V11, P562, DOI 10.1109/TVCG.2005.79
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   WALD I, 2006, P ACM SIGGRAPH 2006
   WALD I, 2006, UUSCI2006023
   Westermann R, 1999, VISUAL COMPUT, V15, P100, DOI 10.1007/s003710050165
   WILHELMS J, 1992, ACM T GRAPHIC, V11, P201, DOI 10.1145/130881.130882
   Yoon SE, 2006, VISUAL COMPUT, V22, P772, DOI 10.1007/s00371-006-0062-y
NR 32
TC 19
Z9 24
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 209
EP 225
DI 10.1007/s00371-008-0215-2
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Méndez-Feliu, A
   Sbert, M
AF Mendez-Feliu, Alex
   Sbert, Mateu
TI From obscurances to ambient occlusion: A survey
SO VISUAL COMPUTER
LA English
DT Article
DE Obscurances; Ambient occlusion; Global illumination; Rendering
   techniques
AB This survey deals with obscurances and ambient occlusion. These are relatively cheap techniques that simulate diffuse indirect illumination in a way that looks realistic, though they are not global illumination techniques. The concept of obscurances first appeared in the late 1990s in the context of videogames and a few years later the idea was simplified and used in production rendering of movies under the name of ambient occlusion. In recent years many articles with ideas to improve or accelerate these techniques have appeared, while ambient occlusion has been included in commercial renderers and popularized in videogames and the movie industry.
   This survey reviews the birth and evolution of obscurances and ambient occlusion techniques in recent years.
C1 [Mendez-Feliu, Alex] ART VPS Ltd, Cambridge, England.
   [Sbert, Mateu] Univ Girona, Girona, Spain.
C3 Universitat de Girona
RP Méndez-Feliu, A (corresponding author), ART VPS Ltd, Cambridge, England.
EM alex@txapulin.net; mateu@ima.udg.edu
RI Sbert, Mateu/G-6711-2011
OI Sbert, Mateu/0000-0003-2164-6858
FU Gametools project from the VIth European Framework Spanish Government
   [TIN2007-68066-C04-01]
FX This work has been funded by the Gametools project from the VIth
   European Framework and project TIN2007-68066-C04-01 from the Spanish
   Government.
CR BREDOW R, 2002, ACM SIGGRAPH 2002
   BUNNEL M, 2005, DYNAMIC AMBIENT OCCL, P233
   Castro F., 2000, Journal of Graphics Tools, V5, P1, DOI 10.1080/10867651.2000.10487527
   CHRISTENSEN P, S INT RAY TRAC 2006
   CHRISTENSEN P, 2003, ACM SIGGRAPH 203 ACM
   Cochran W.G., 1977, Sampling techniques, V3rd ed.
   FRANKLIN D, 2006, SHADERX4 CHAPTER HAR, P91
   Garcia I., 2005, Objavljeno v Proceedings of Third Hungarian Conference on Computer Graphics and Geometry, strani, P9
   Hasenfratz JM, 2003, COMPUT GRAPH FORUM, V22, P753, DOI 10.1111/j.1467-8659.2003.00722.x
   HAVRAN V, 2003, P 19 SPRING C COMP G, P149
   Hegeman K., 2006, Proceedings of the 2006 symposium on Interactive 3D graphics and games, P87, DOI [10.1145/1111411.1111427.4, DOI 10.1145/1111411.1111427.4]
   Iones A, 2003, IEEE COMPUT GRAPH, V23, P54, DOI 10.1109/MCG.2003.1198263
   Kirk AG, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P47
   KONTKANEN J, 2006, SHADERX 4, P101
   KONTKANEN J, 2005, P ACM SIGGRAPH 2005, P41
   Kontkanen Janne, 2006, RENDERING TECHNIQUES, P343, DOI DOI 10.2312/EGWR/EGSR06/343-348
   KRISHNAIAH PR, 1988, HDB STAT, V6
   LANDIS H, 2002, ACM SIGGRAPH 2002
   Lu JY, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1189762.1189765, 10.1145/1186644.1186647]
   Malmer M., 2007, Journal of Graphics Tools, V12, P59
   MENDEZ A, 2003, P 19 SPRING C COMP G, P171
   Méndez-Feliu A, 2004, COMPUT ANIMAT VIRT W, V15, P463, DOI 10.1002/cav.50
   MENDEZFELIU A, 2006, P GRAPH 2006 NOV RUS
   MENDEZFELIU A, 2006, P COMP AN SOC AG CAS
   MENDEZFELIU A, 2004, P INT C COMP GRAPH A
   MENDEZFELIU A, 2006, SHADERX 4, P121
   MENTAL RAY AMBIENT O
   AMBIENT OCCLUSION SH
   3DS MAX 7 MENTAL RAY
   VRAY AMBIENT OCCLUSI
   AMBIENT OCCLUSION PL
NR 31
TC 31
Z9 38
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2009
VL 25
IS 2
BP 181
EP 196
DI 10.1007/s00371-008-0213-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 394XE
UT WOS:000262485700007
DA 2024-07-18
ER

PT J
AU Salzbrunn, T
   Garth, C
   Scheuermann, G
   Meyer, J
AF Salzbrunn, Tobias
   Garth, Christoph
   Scheuermann, Gerik
   Meyer, Joerg
TI Pathline predicates and unsteady flow structures
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on SIBGRAPI 2007
CY SEP, 2007
CL Belo Horizonte, BRAZIL
DE Unsteady flow visualization; Feature detection; Computational fluid
   dynamics (CFD)
ID LINE INTEGRAL CONVOLUTION; VISUALIZATION; ADVECTION; TRACKING; 3D
AB In most fluid dynamics applications, unsteady flow is a natural phenomenon and steady models are just simplifications of the real situation. Since computing power increases, the number and complexity of unsteady flow simulations grows, too. Besides time-dependent features, scientists and engineers are essentially looking for a description of the overall flow behavior, usually with respect to the requirements of their application domain. We call such a description a flow structure, requiring a framework of definitions for an unsteady flow structure. In this article, we present such a framework based on pathline predicates. Using the common computer science definition, a predicate is a Boolean function, and a pathline predicate is a Boolean function on pathlines that decides if a pathline has a property of interest to the user. We will show that any suitable set of pathline predicates can be interpreted as an unsteady flow structure definition. The visualization of the resulting unsteady flow structure provides a visual description of overall flow behavior with respect to the user's interest. Furthermore, this flow structure serves as a basis for pathline placements tailored to the requirements of the application.
C1 [Salzbrunn, Tobias; Scheuermann, Gerik] Univ Leipzig, Inst Informat, D-04009 Leipzig, Germany.
   [Garth, Christoph] Univ Calif Davis, Inst Data Anal & Visualizat, Davis, CA 95616 USA.
   [Meyer, Joerg] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 Leipzig University; University of California System; University of
   California Davis; University of California System; University of
   California Irvine
RP Salzbrunn, T (corresponding author), Univ Leipzig, Inst Informat, Postfach 100920, D-04009 Leipzig, Germany.
EM salzbrunn@informatik.uni-leipzig.de;
   scheuermann@informatik.uni-leipzig.de; cgarth@ucdavis.edu;
   jmeyer@uci.edu
RI Garth, Christoph/Q-5901-2018
OI Garth, Christoph/0000-0003-1669-8549
CR Bauer D, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P525, DOI 10.1109/VISUAL.2002.1183821
   BAUER D, 2002, DATA VISUALIZATION 2, P233
   Becker BG, 1995, VISUALIZATION '95 - PROCEEDINGS, P329, DOI 10.1109/VISUAL.1995.485146
   Cabral B., 1993, Computer Graphics Proceedings, P263, DOI 10.1145/166117.166151
   FORSSELL LK, 1995, IEEE T VIS COMPUT GR, V1, P133, DOI 10.1109/2945.468406
   Garth C, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P329, DOI 10.1109/VISUAL.2004.107
   Griebel M, 1998, Numerical simulation in fluid dynamics: A practical introduction
   Ji GF, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P209, DOI 10.1109/VISUAL.2003.1250374
   Jobard B, 2002, IEEE T VIS COMPUT GR, V8, P211, DOI 10.1109/TVCG.2002.1021575
   Jobard B, 2000, IEEE VISUAL, P155, DOI 10.1109/VISUAL.2000.885689
   Jonker PP, 2000, LECT NOTES COMPUT SC, V1953, P371
   Jonker PP, 2002, PATTERN RECOGN LETT, V23, P677, DOI 10.1016/S0167-8655(01)00144-1
   Lane D.A., 1997, Scientific Visualiza-tion, Overviews, Methodologies, and Techniques, P125
   Liu ZP, 2005, IEEE T VIS COMPUT GR, V11, P113, DOI 10.1109/TVCG.2005.21
   Palagyi K, 1998, PATTERN RECOGN LETT, V19, P613, DOI 10.1016/S0167-8655(98)00031-2
   Park SungW., 2005, P JOINT EUROGRAPHICS, P21
   Peikert R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P263, DOI 10.1109/VISUAL.1999.809896
   Post FH, 2003, COMPUT GRAPH FORUM, V22, P775, DOI 10.1111/j.1467-8659.2003.00723.x
   Reinders F., 1999, Data Visualization '99. Proceedings of the Joint EUROGRAPHICS and IEEE TCVG Symposium on Visualization, P63
   Salzbrunn T, 2006, IEEE T VIS COMPUT GR, V12, P1601, DOI 10.1109/TVCG.2006.104
   SAMTANEY R, 1994, COMPUTER, V27, P20, DOI 10.1109/2.299407
   Shen HW, 1998, IEEE T VIS COMPUT GR, V4, P98, DOI 10.1109/2945.694952
   SHI K, 2007, TOPOINVIS IN PRESS
   Silver D, 1997, IEEE T VIS COMPUT GR, V3, P129, DOI 10.1109/2945.597796
   Sujudi D, 1995, 12 COMP FLUID DYN C
   Theisel H, 2005, IEEE T VIS COMPUT GR, V11, P383, DOI 10.1109/TVCG.2005.68
   Theisel H., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P141
   Tricoche X, 2002, COMPUT GRAPH-UK, V26, P249, DOI 10.1016/S0097-8493(02)00056-0
   Tricoche X, 2001, SPRING EUROGRAP, P117
   van Wijk JJ, 2002, ACM T GRAPHIC, V21, P745, DOI 10.1145/566570.566646
   vanWalsum T, 1996, IEEE T VIS COMPUT GR, V2, P111, DOI 10.1109/2945.506223
   VERMA V, 1999, IEEE VIS 99, P341
   WEIGLE C, 1998, P S VOL VIS, P143
   Weiskopf D, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P107, DOI 10.1109/VISUAL.2003.1250361
   Westermann R, 2001, IEEE T VIS COMPUT GR, V7, P222, DOI 10.1109/2945.942690
   Wischgoll T., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P447
NR 36
TC 42
Z9 51
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2008
VL 24
IS 12
BP 1039
EP 1051
DI 10.1007/s00371-007-0204-x
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 363VH
UT WOS:000260294500005
DA 2024-07-18
ER

PT J
AU Holdstein, Y
   Fischer, A
AF Holdstein, Y.
   Fischer, A.
TI Three-dimensional surface reconstruction using meshing growing neural
   gas (MGNG)
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE 3D reconstruction; neural networks; mesh approximation; bone micro
   structure
ID FREEFORM OBJECTS; NETWORK
AB The neural network method, a relatively new method in reverse engineering (RE), has the potential to reconstruct 3D models accurately and fast. A neural network (NN) is a set of interconnected neurons, in which each neuron is capable of making autonomous arithmetic and geometric calculations. Moreover, each neuron is affected by its surrounding neurons through the structure of the network.
   This work proposes a new approach that utilizes growing neural gas neural network (GNG NN) techniques to reconstruct a triangular manifold mesh. This method has the advantage of reconstructing the surface of an n-genus freeform object without a priori knowledge regarding the original object, its topology or its shape. The resulting mesh can be improved by extending the MGNG into an adaptive algorithm. The proposed method was also extended for micro-structure modeling. The feasibility of the proposed method is demonstrated on several examples of freeform objects with complex topologies.
C1 [Holdstein, Y.; Fischer, A.] Technion Israel Inst Technol, Fac Mech Engn, Lab CAD & LCE, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Holdstein, Y (corresponding author), Technion Israel Inst Technol, Fac Mech Engn, Lab CAD & LCE, IL-32000 Haifa, Israel.
EM meyaron@technion.ac.il; meranath@technion.ac.il
CR [Anonymous], SELF ORGANIZING MAPS
   Azernikov S, 2006, J COMPUT INF SCI ENG, V6, P355, DOI 10.1115/1.2356500
   Bajaj C. L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P109, DOI 10.1145/218380.218424
   Bar-Yoseph PZ, 2001, COMPUT MECH, V27, P378, DOI 10.1007/s004660100250
   Barhak J, 2002, COMPUT GRAPH-UK, V26, P745, DOI 10.1016/S0097-8493(02)00129-2
   Barhak J, 2001, IEEE T VIS COMPUT GR, V7, P1, DOI 10.1109/2945.910817
   Barhak J, 2001, VISUAL COMPUT, V17, P353, DOI 10.1007/s003710100112
   CULESS B, 1996, COMPUTER GRAPHICS SI, P303
   DIEKMANN R, 1996, LECT NOTES COMPUT SC, V1067, P580
   Eck M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P325, DOI 10.1145/237170.237271
   FRITZKE B, 1994, NEURAL NETWORKS, V7, P1441, DOI 10.1016/0893-6080(94)90091-4
   Fritzke Bernd., 1995, Advances in neural information processing systems, V7, P625
   GOPI M, 2002, 15 BRAZ S COMP GRAPH, P179
   Haykin S., 1998, A Comprehensive Foundation, Vsecond
   HOLDSTEIN Y, 2006, THESIS FACULTY MECH
   HOLE K, 1988, COMPUT AIDED DESIGN, V20, P27, DOI 10.1016/0010-4485(88)90138-8
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Krause FL, 2003, CIRP ANN-MANUF TECHN, V52, P125, DOI 10.1016/S0007-8506(07)60547-2
   Steiner D, 2004, VISUAL COMPUT, V20, P266, DOI 10.1007/s00371-003-0232-0
NR 19
TC 44
Z9 45
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 295
EP 302
DI 10.1007/s00371-007-0202-z
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100008
DA 2024-07-18
ER

PT J
AU Shapira, L
   Shamir, A
   Cohen-Or, D
AF Shapira, Lior
   Shamir, Ariel
   Cohen-Or, Daniel
TI Consistent mesh partitioning and skeletonisation using the shape
   diameter function
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 7th Korea-Israel Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY JAN 29-30, 2007
CL Seoul, SOUTH KOREA
DE mesh decomposition; skeleton extraction; geometry processing
ID 3D; DECOMPOSITION; SEGMENTATION; POWER
AB Mesh partitioning and skeletonisation are fundamental for many computer graphics and animation techniques. Because of the close link between an object's skeleton and its boundary, these two problems are in many cases complementary. Any partitioning of the object can assist in the creation of a skeleton and any segmentation of the skeleton can infer a partitioning of the object. In this paper, we consider these two problems on a wide variety of meshes, and strive to construct partitioning and skeletons which remain consistent across a family of objects, not a single one. Such families can consist of either a single object in multiple poses and resolutions, or multiple objects which have a general common shape. To achieve consistency, we base our algorithms on a volume-based shape-function called the shape-diameter-function (SDF), which remains largely oblivious to pose changes of the same object and maintains similar values in analogue parts of different objects. The SDF is a scalar function defined on the mesh surface; however, it expresses a measure of the diameter of the object's volume in the neighborhood of each point on the surface. Using the SDF we are able to process and manipulate families of objects which contain similarities using a simple and consistent algorithm: consistently partitioning and creating skeletons among multiple meshes.
C1 [Shapira, Lior; Cohen-Or, Daniel] Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
   [Shamir, Ariel] Interdisciplinary Ctr, Herzliyya, Israel.
C3 Tel Aviv University; Reichman University
RP Shapira, L (corresponding author), Tel Aviv Univ, IL-69978 Tel Aviv, Israel.
EM liors@post.tau.ac.il; arik@idc.ac.il; dcor@tau.ac.il
CR Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   [Anonymous], COMP VIS PATT REC 20
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Attene M, 2003, VISUAL COMPUT, V19, P127, DOI 10.1007/s00371-002-0182-y
   Choi HI, 1997, PAC J MATH, V181, P57, DOI 10.2140/pjm.1997.181.57
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cox M., 1994, MULTIDIMENSIONAL SCA
   DASGUPTA S, 1999, UCBCSD991047 ECCS DE
   Dey TK, 2003, LECT NOTES COMPUT SC, V2748, P25
   DEY TK, 2002, 10 ANN EUROPEAN S, P387
   Gelfand Natasha, 2004, Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, P214
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KRAEVOY V, 2007, IN PRESS VISUAL COMP
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Lee Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P279
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Lien J.-M., 2004, P 28 ANN S COMP GEOM, P17, DOI 10.1145/997817.997823
   LIEN JM, 2005, SIMULTANEOUS SHAPE D
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   Mortara M, 2004, ALGORITHMICA, V38, P227, DOI 10.1007/s00453-003-1051-4
   Mortara M., 2002, INT J SHAPE MODELING, V8, P139
   Ni XL, 2004, ACM T GRAPHIC, V23, P613, DOI 10.1145/1015706.1015769
   Page D., 2003, P 1 IEEE LAT AM C RO, P91
   Page DL, 2003, PROC CVPR IEEE, P27
   SHAMIR A, 2006, P EUR 2006
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Svensson S, 2002, IMAGE VISION COMPUT, V20, P529, DOI 10.1016/S0262-8856(02)00042-2
   Tierny J., 2006, PACIFIC GRAPHICS, V2006, P85
   Tierny J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P215, DOI 10.1109/SMI.2007.38
   Verroust A, 2000, VISUAL COMPUT, V16, P15, DOI 10.1007/PL00007210
   Vlassis N, 2002, NEURAL PROCESS LETT, V15, P77, DOI 10.1023/A:1013844811137
   Wu FC, 2006, VISUAL COMPUT, V22, P117, DOI 10.1007/s00371-005-0357-4
   Zhu SC, 1996, INT J COMPUT VISION, V20, P187
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 37
TC 389
Z9 476
U1 1
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2008
VL 24
IS 4
BP 249
EP 259
DI 10.1007/s00371-007-0197-5
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 277UL
UT WOS:000254240100004
DA 2024-07-18
ER

PT J
AU Runions, A
   Samavati, F
   Prusinkiewicz, P
AF Runions, Adam
   Samavati, Faramarz
   Prusinkiewicz, Przemyslaw
TI Ribbons - A representation for point clouds
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE point cloud; point based rendering; non-photorealistic rendering;
   geometric modeling
AB Point clouds are usually represented either globally, as surfaces, or locally, as sets of points with small neighbourhoods. We propose an intermediate representation, called ribbons, which is obtained by partitioning a point cloud into one-dimensional strips. This representation is well suited to the placement of strokes in nonphotorealistic rendering, and can be visualized efficiently using quad strips. Methods for performing hatching, cross-hatching, and silhouette renderings are presented. Ribbons also allow for the application of curve-based operations to the point cloud.
C1 Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Runions, A (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM runionsa@cpsc.ucalgary.ca; samavati@cpsc.ucalgary.ca;
   pwp@cpsc.ucalgary.ca
RI Runions, Adam/Z-6049-2019
OI Runions, Adam/0000-0002-7758-7423
CR Adamson A., 2003, Symposium on Geometry Processing, P230
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   Bartels RH, 2000, J COMPUT APPL MATH, V119, P29, DOI 10.1016/S0377-0427(00)00370-8
   BOTSCH M, 2004, S POINT BAS GRAPH, P25
   Boubekeur T, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P23, DOI 10.1109/SMI.2005.49
   BROSZ J, 2004, SCCG 04 P SPRING C C, P157
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Dey TK, 2001, IEEE 2001 SYMPOSIUM ON PARALLEL AND LARGE-DATA VISUALIZATION AND GRAPHICS, PROCEEDINGS, P19, DOI 10.1109/PVGS.2001.964399
   Hertzmann A, 2000, COMP GRAPH, P517, DOI 10.1145/344779.345074
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Kawata H, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P293, DOI 10.1109/CW.2004.42
   Levin D, 2004, MATH VISUAL, P37
   MELLO V, 2003, S SOL MOD APPL 2003, P108
   NICE C, 1997, DRAWING PEN INK N LI
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   PAULY M, 2003, P EUR ACM SIGGRAPH S, P281
   Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451
   Praun E, 2001, COMP GRAPH, P581, DOI 10.1145/383259.383328
   XU H., 2004, Proceedings of the 3rd International Symposium on Non-Photorealistic Animation and Rendering (NPAR 2006), P25, DOI DOI 10.22633/RPGE.V25I3.15840
   Zakaria N., 2004, COMPUTER GRAPHICS IN, P242
   Zwicker M, 2001, COMP GRAPH, P371, DOI 10.1145/383259.383300
NR 21
TC 5
Z9 6
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 945
EP 954
DI 10.1007/s00371-007-0153-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600035
DA 2024-07-18
ER

PT J
AU Gamito, MN
   Maddock, SC
AF Gamito, Manuel N.
   Maddock, Steve C.
TI Ray casting implicit fractal surfaces with reduced affine arithmetic
SO VISUAL COMPUTER
LA English
DT Article
DE affine arithmetic; implicit surfaces; procedural noise functions; ray
   casting
AB A method is presented for ray casting implicit surfaces defined by fractal combinations of procedural noise functions. The method is robust and uses affine arithmetic to bound the variation of the implicit function along a ray. The method is also efficient due to a modification in the affine arithmetic representation that introduces a condensation step at the end of every non-affine operation. We show that our method is able to retain the tight estimation capabilities of affine arithmetic for ray casting implicit surfaces made from procedural noise functions while being faster to compute and more efficient to store.
C1 Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.
C3 University of Sheffield
RP Gamito, MN (corresponding author), Univ Sheffield, Dept Comp Sci, 211 Portobello St, Sheffield S1 4DP, S Yorkshire, England.
EM M.Gamito@dcs.shef.ac.uk; S.Maddock@dcs.shef.ac.uk
RI Maddock, Steve/J-1849-2016
OI Maddock, Steve/0000-0003-3179-0263
CR [Anonymous], P 10 ANN C COMP GRAP
   [Anonymous], 1966, Interval Arithmetic
   BARR AH, 1986, COMPUTER GRAPHICS, P287
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Bloomenthal J., 1988, Computer-Aided Geometric Design, V5, P341, DOI 10.1016/0167-8396(88)90013-1
   De Cusatis A.  Jr., 1999, XII Brazilian Symposium on Computer Graphics and Image Processing (Cat. No.PR00481), P65, DOI 10.1109/SIBGRA.1999.805711
   de Figueiredo LH, 2004, NUMER ALGORITHMS, V37, P147, DOI 10.1023/B:NUMA.0000049462.70970.b6
   Ebert D.S., 2003, TEXTURING MODELING, V3rd
   HART J, 1993, SIGGRAPH 93 COURSE N, V25
   Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084
   Hart JC, 1997, COMPUT GRAPH FORUM, V16, P91, DOI 10.1111/1467-8659.00125
   Hedrich W, 1998, ACM T GRAPHIC, V17, P158, DOI 10.1145/285857.285859
   Kalra D., 1989, Computer Graphics, V23, P297, DOI 10.1145/74334.74364
   Lewis J. P., 1989, Computer Graphics, V23, P263, DOI 10.1145/74334.74360
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mitchell D. P., 1990, Proceedings. Graphics Interface '90, P68
   Morse BS, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P89, DOI 10.1109/SMA.2001.923379
   MUSGRAVE FK, 2003, TEXTURING MODELING P, P565
   Nishimura H., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P718
   PEACHEY DR, 2003, TEXTURING MODELING P, P7
   Perlin K, 2002, ACM T GRAPHIC, V21, P681, DOI 10.1145/566570.566636
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Saupe D., 1989, Visualisierung in Mathematik und Naturwissenschaften: Bremer Computergraphik-Tage, V1988, P114
   Sherstyuk A, 1999, COMPUT GRAPH FORUM, V18, P139, DOI 10.1111/1467-8659.00364
   Stolfi J., 1997, Self-Validated Numerical Methods and Applications; Monograph for the 21st Brazilian Mathematics Colloquium (CBM'97)
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   VANWIJK JJ, 1985, COMPUT GRAPH, V9, P283, DOI 10.1016/0097-8493(85)90055-X
   Voss R., 1988, SCI FRACTAL IMAGES, P21, DOI DOI 10.1007/978-1-4612-3784-6_1
   WORLEY S, 1996, COMPUTER GRAPHICS, V30, P291
   WORLEY SP, 1996, P IMPL SURF 96 OCT, P99
   WYVILL G, 1990, CG INTERNATIONAL 90, P469
NR 32
TC 11
Z9 13
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2007
VL 23
IS 3
BP 155
EP 165
DI 10.1007/s00371-006-0090-7
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 134PM
UT WOS:000244095300001
DA 2024-07-18
ER

PT J
AU Sappa, AD
   Garcia, MA
AF Domingo Sappa, Angel
   Angel Garcia, Miguel
TI Generating compact representations of static scenes by means of 3D
   object hierarchies
SO VISUAL COMPUTER
LA English
DT Article
DE world modeling; object clustering; hierarchical representation; minimum
   spanning tree; minimum distance computation
AB This paper presents a new heuristic algorithm for computing a compact hierarchical representation of the objects contained in a 3D static scene. The algorithm first builds a fully-connected adjacency graph that keeps the costs of grouping the different pairs of objects of the scene. Afterward, the graph's minimum spanning tree is computed and its edges sorted in ascending order according to their cost. Next, from that sorted list, a cost-based clustering technique is applied, thus generating new objects at a higher level in the hierarchy. A new object can be defined after merging two or more objects according to their corresponding linking costs. The algorithm starts over by generating a new adjacency graph from those new objects, along with the objects that could not be merged before. The iterative process is applied until an adjacency graph with a single object is obtained. The latter is the root of the hierarchical representation. Balance and coherence of the hierarchy, in which spatially close objects are also structurally close, is achieved by defining an appropriate cost function. The proposed technique is evaluated upon several 3D scenes and compared to a previous technique. In addition, the benefits of the proposed technique with respect to techniques based on octrees and kd-trees are analyzed in terms of a practical application.
C1 UAB, Comp Vis Ctr, Barcelona 08193, Spain.
   Univ Autonoma Madrid, Dept Informat Engn, E-28049 Madrid, Spain.
C3 Centre de Visio per Computador (CVC); Autonomous University of
   Barcelona; Autonomous University of Madrid
RP Sappa, AD (corresponding author), UAB, Comp Vis Ctr, Edifici O Campus, Barcelona 08193, Spain.
EM angel.sappa@cvc.uab.es; miguelangel.garcia@uam.es
RI Sappa, Angel D./A-2072-2009; Garcia, Miguel Angel/C-4304-2014
OI Sappa, Angel/0000-0003-2468-0031; Garcia, Miguel
   Angel/0000-0003-2611-6821
CR [Anonymous], 2000, Computational Geometry Algorithms and Applications
   Breen DE, 1998, IEEE SYMPOSIUM ON VOLUME VISUALIZATION, P7, DOI 10.1109/SVV.1998.729579
   BRUNET P, 1990, ACM T GRAPHIC, V9, P170, DOI 10.1145/78956.78959
   de Miras JR, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P352, DOI 10.1109/TDPVT.2002.1024082
   DELPOBIL AP, 1992, 1992 IEEE INTERNATIONAL CONF ON ROBOTICS AND AUTOMATION : PROCEEDINGS, VOLS 1-3, P246, DOI 10.1109/ROBOT.1992.220255
   Fernandez JA, 1998, IEEE INT CONF ROBOT, P656, DOI 10.1109/ROBOT.1998.677047
   Fernández-Madrigal JA, 2002, IEEE T PATTERN ANAL, V24, P103, DOI 10.1109/34.982887
   Fischer K, 2003, LECT NOTES COMPUT SC, V2832, P630, DOI 10.1007/978-3-540-39658-1_57
   García MA, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1359, DOI 10.1109/ROBOT.1999.772550
   GOLDSMITH J, 1987, IEEE COMPUT GRAPH, V7, P14, DOI 10.1109/MCG.1987.276983
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Havran V, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P209
   Huerta J, 1997, IEEE INFOR VIS, P148, DOI 10.1109/IV.1997.626502
   Klosowski JT, 1998, IEEE T VIS COMPUT GR, V4, P21, DOI 10.1109/2945.675649
   Martinez-Salvador B, 1998, IEEE INT CONF ROBOT, P624
   O'Sullivan C., 1999, PROC SPRING C COMPUT, P83
   Payeur P, 1998, IEEE INT CONF ROBOT, P3071, DOI 10.1109/ROBOT.1998.680897
   Rimon E, 1997, J INTELL ROBOT SYST, V18, P105, DOI 10.1023/A:1007960531949
   ROSEN K, 1990, DISCRETE MATH ITS AP
   Subramanian K. R., 1991, Proceedings. Graphics Interface '91, P93
   Xavier PG, 1996, IEEE INT CONF ROBOT, P3644, DOI 10.1109/ROBOT.1996.509268
NR 21
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2007
VL 23
IS 2
BP 143
EP 154
DI 10.1007/s00371-006-0089-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 125NZ
UT WOS:000243450200006
DA 2024-07-18
ER

PT J
AU Schein, S
   Karpen, E
   Elber, G
AF Schein, S
   Karpen, E
   Elber, G
TI Real-time geometric deformation displacement maps using programmable
   hardware
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE deformation displacement mapping; graphics hardware; geometric texture
AB Striving for photorealism, texture mapping, and its more advanced variations, bump and displacement mapping, have all become fundamental tools in computer graphics. Recently, the introduction of programmable graphics hardware has enabled the employment of displacement mapping in real-time applications. While displacement mapping facilitates the actual modification of the underlying geometry, it is constrained by being an injective mapping. Further, it is also limited because it usually maps the geometry of the (low-resolution) smooth base surfaces, typically by displacing their vertices.
   Drawing from recent work on deformation displacement mapping (DDM) [4], in this paper we offer real-time solutions to both these limitations. Our solutions make it possible to employ the DDM paradigm on programmable graphics hardware. By reversing the roles of the base surfaces and their geometric details, both the one-to-one constraint and the base surface resolution limitation are resolved. Furthermore, this role reversal also paves the way for other benefits such as a tremendous decrease in the memory consumption of geometric detail information in the DDM and the ability to animate the details over the base surface. We show that the presented scheme can be used effectively to generate highly complex renderings and animations, in real time, on modern graphics hardware. The capabilities of the proposed method are demonstrated for both rational parametric base surfaces and polygonal base surfaces.
C1 Technion Israel Inst Technol, Dept Comp Sci, CGGC, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Schein, S (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, CGGC, IL-32000 Haifa, Israel.
EM schein@cs.technion.ac.il; gershon@cs.technion.ac.il
CR Blinn J., 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   CATMULL E, 1974, THESIS U UTAH SALT L
   Cook R. L., 1984, Computers & Graphics, V18, P223
   Elber G, 2005, IEEE COMPUT GRAPH, V25, P66, DOI 10.1109/MCG.2005.79
   Elber G, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P156, DOI 10.1109/PCCGA.2002.1167850
   FLOATER M, 2004, MATH VISUALIZAITON, P157
   GERASIMOV P, 2004, SHADER MODEL 3 0 USI
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Hirche J, 2004, PROC GRAPH INTERF, P153
   Kaplan CS, 2000, COMP GRAPH, P499, DOI 10.1145/344779.345022
   Mark WR, 2003, ACM T GRAPHIC, V22, P896, DOI 10.1145/882262.882362
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   Oliveira MM, 2000, COMP GRAPH, P359, DOI 10.1145/344779.344947
   Peng J, 2004, ACM T GRAPHIC, V23, P635, DOI 10.1145/1015706.1015773
   SEDERBERG TW, 1986, T GRAPH, V20, P151
   Sheffer A, 2001, ENG COMPUT-GERMANY, V17, P326, DOI 10.1007/PL00013391
   SLOAN PP, 2000, 11 EUR WORKSH REND
   Wang LF, 2003, ACM T GRAPHIC, V22, P334, DOI 10.1145/882262.882272
   WANG X, 2004, EUR S REND
NR 19
TC 9
Z9 9
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 791
EP 800
DI 10.1007/s00371-005-0338-7
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400032
DA 2024-07-18
ER

PT J
AU Yamauchi, H
   Gumhold, S
   Zayer, R
   Seidel, HP
AF Yamauchi, H
   Gumhold, S
   Zayer, R
   Seidel, HP
TI Mesh segmentation driven by Gaussian curvature
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 13th Pacific Conference on Computer Graphics and Applications
CY OCT 12-14, 2005
CL Macao, PEOPLES R CHINA
DE mesh segmentation; Gauss map; Gaussian curvature; parameterization;
   developability
ID SURFACES
AB Mesh parameterization is a fundamental problem in computer graphics as it allows for texture mapping and facilitates many mesh processing tasks. Although there exists a variety of good parameterization methods for meshes that are topologically equivalent to a disk, the segmentation into nicely parameterizable charts of higher genus meshes has been studied less. In this paper we propose a new segmentation method for the generation of charts that can be flattened efficiently. The integrated Gaussian curvature is used to measure the developability of a chart, and a robust and simple scheme is proposed to integrate the Gaussian curvature. The segmentation approach evenly distributes Gaussian curvature over the charts and automatically ensures a disklike topology of each chart. For numerical stability, we use an area on the Gauss map to represent Gaussian curvature. The resulting parameterization shows that charts generated in this way have less distortion compared to charts generated by other methods.
C1 MPI Informat, Saarbrucken, Germany.
C3 Max Planck Society
RP MPI Informat, Saarbrucken, Germany.
EM hitoshi@mpi-inf.mpg.de; sgumhold@mpi-inf.mpg.de; zayer@mpi-inf.mpg.de;
   hpseidel@mpi-inf.mpg.de
CR Benkö P, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P169, DOI 10.1109/GMAP.2002.1027508
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cohen-Steiner D., 2003, SCG'03: Proceedings ofthe nineteenth annual symposium on Computational geometry, P312
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Dey TK, 2003, LECT NOTES COMPUT SC, V2748, P25
   ELBER G, 1995, COMPUT AIDED DESIGN, V27, P283, DOI 10.1016/0010-4485(95)91138-B
   ERICKSON J, 2002, WORKSH 18 ACM S COMP, P244
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   GARLAND M, 2001, WORKSH ACM S INT 3D, P49
   GELFAND N, 2004, WORKSH EUR S GEOM PR, P219
   Hoschek J, 1998, COMPUT AIDED DESIGN, V30, P757, DOI 10.1016/S0010-4485(98)00030-X
   INOUE K, 1999, P 8 INT MESH ROUNDT, P281
   JULIUS D, 2005, IN PRESS P EUR
   Kalvin AD, 1996, IEEE COMPUT GRAPH, V16, P64, DOI 10.1109/38.491187
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   MAILLOT J, 1993, P SIGGRAPH 93, P27, DOI DOI 10.1145/166117.166120
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   Max N., 1999, Journal of Graphics Tools, V4, P1, DOI 10.1080/10867651.1999.10487501
   Mitani J, 2004, ACM T GRAPHIC, V23, P259, DOI 10.1145/1015706.1015711
   Page DL, 2003, PROC CVPR IEEE, P27
   POTTMANN H, 1995, COMPUT AIDED GEOM D, V12, P513, DOI 10.1016/0167-8396(94)00031-M
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   SANDER PV, 2003, P EUR ACM SIGGRAPH S, P146
   SHAMIR A, 2004, 3DPVT 04, P82, DOI DOI 10.1109/3DPVT.2004.13
   Shlafman S, 2002, COMPUT GRAPH FORUM, V21, P219, DOI 10.1111/1467-8659.00581
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   WELCH W, 1994, P SIGGRAPH 94, P247
   ZHOU K, 2004, P EUR ACM SIGGRAPH S, P47
NR 31
TC 70
Z9 83
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2005
VL 21
IS 8-10
SI SI
BP 659
EP 668
DI 10.1007/s00371-005-0319-x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 964CX
UT WOS:000231857400018
DA 2024-07-18
ER

PT J
AU Coors, V
   Rossignac, J
AF Coors, V
   Rossignac, J
TI Delphi: geometry-based connectivity prediction in triangle mesh
   compression
SO VISUAL COMPUTER
LA English
DT Article
DE triangle meshes; connectivity coding; geometric prediction; geometry
   compression
ID REGULAR MESHES; EDGEBREAKER
AB Delphi is a new geometry-guided predictive scheme for compressing the connectivity of triangle meshes. Both compression and decompression algorithms traverse the mesh using the EdgeBreaker state machine. However, instead of encoding the EdgeBreaker clers symbols that capture connectivity explicitly, they estimate the location of the unknown vertex, v, of the next triangle. If the predicted location lies sufficiently close to the nearest vertex, w, on the boundary of the previously traversed portion of the mesh, then Delphi estimates that v coincides with w. When the guess is correct, a single confirmation bit is encoded. Otherwise, additional bits are used to encode the rectification of that prediction. When v coincides with a previously visited vertex that is not adjacent to the parent triangle (EdgeBreaker S case), the offset, which identifies the vertex v, must be encoded, mimicking the cut-border machine compression proposed by Gumhold and Strasser. On models where 97% of Delphi predictions are correct, the connectivity is compressed down to 0.19 bits per triangle. Compression rates decrease with the frequency of wrong predictors, but remains below 1.50 bits per triangle for all models tested.
C1 Stuttgart Univ Appl Sci Geomat Comp Sci & Math, D-70174 Stuttgart, Germany.
   Georgia Inst Technol, Coll Comp, IRIS, Atlanta, GA 30332 USA.
   Georgia Inst Technol, Coll Comp, GVU Ctr, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Stuttgart Univ Appl Sci Geomat Comp Sci & Math, Schellingstr 24, D-70174 Stuttgart, Germany.
EM Volker.Coors@hft-stuttgart.de; jarek@cc.gatech.edu
CR ALLIEZ P, 2001, EUR 2001 C P
   ALLIEZ P, 2001, SIGGRAPH 2001 C P
   Attene M, 2003, ACM T GRAPHIC, V22, P982, DOI 10.1145/944020.944022
   COHENOR D, 1999, VIS 99 C P, P67
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Evans F, 1996, IEEE VISUAL, P319, DOI 10.1109/VISUAL.1996.568125
   GUMHOLD S, 1998, SIGGRAPH 98 C P, P133, DOI DOI 10.1145/280814.280836
   GUMHOLD S, 2000, SIGGRAPH 2000
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   HOPPE H, 1996, SIGGRAPH 96, P99, DOI DOI 10.1145/237170.237216
   Isenburg M, 2001, COMP GEOM-THEOR APPL, V20, P39, DOI 10.1016/S0925-7721(01)00034-7
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KING D, 1999, 11 CAN C COMP GEOM, P146
   KING D, 1999, GITGVU9929
   LEE H, 2002, ANGLE ANAL TRIANGLE
   LOPES H, 2002, ACM S SOL MOD SAARBR
   PAJAROLA R, 1999, IEEE T VIS COMPUT GR, P47
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Rossignac J, 1999, COMP GEOM-THEOR APPL, V14, P119, DOI 10.1016/S0925-7721(99)00028-0
   ROSSIGNAC J, 2001, SHAP MOD INT C GEN I
   Rossignac J., 1999, P 5 ACM S SOL MOD AP, P31
   Szymczak A, 2002, GRAPH MODELS, V64, P183, DOI 10.1006/gmod.2002.0577
   Szymczak A, 2001, COMP GEOM-THEOR APPL, V20, P53, DOI 10.1016/S0925-7721(01)00035-9
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   TAUBIN G, 1998, SIGGRAPH 98 C P, P123
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   TUTTE WT, 1962, CANADIAN J MATH, V14, P21, DOI 10.4153/CJM-1962-002-9
   TZSCHACH H, 1993, CODES STORUNGSFREIEN
   VALETTE S, 2003, IEEE INT C IM PROC I
NR 29
TC 18
Z9 20
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 507
EP 520
DI 10.1007/s00371-004-0255-1
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600001
DA 2024-07-18
ER

PT J
AU Ershov, S
   Durikovic, R
   Kolchin, K
   Myszkowski, K
AF Ershov, S
   Durikovic, R
   Kolchin, K
   Myszkowski, K
TI Reverse engineering approach to appearance-based design of metallic and
   pearlescent paints
SO VISUAL COMPUTER
LA English
DT Article
DE appearance design; paint models; reflection models
AB We propose a new approach to interactive design of metallic and pearlescent coatings, such as automotive paints and plastic finishes of electronic appliances. This approach includes solving the inverse problem, that is, finding pigment composition of a paint from its bidirectional reflectance distribution function (BRDF) based on a simple paint model. The inverse problem is solved by two consecutive optimizations calculated in real-time on a contemporary PC. Such reverse engineering can serve as a starting point for subsequent design of new paints in terms of appearance attributes that are directly connected to the physical parameters of our model. This allows the user to have a paint composition in parallel with the appearance being designed.
C1 MV Keldysh Appl Math Inst, Moscow 125047, Russia.
   Univ Aizu, Fukushima 9658580, Japan.
   Digital Media Profess Inc, Musashino, Tokyo 1800006, Japan.
   Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
C3 Russian Academy of Sciences; Keldysh Institute of Applied Mathematics;
   University of Aizu; Max Planck Society
RP MV Keldysh Appl Math Inst, Moscow 125047, Russia.
EM measure@spp.keldysh.ru; roman@u-aizu.ac.jp; karol@mpi-sb.mpg.de
RI Kolchin, Konstantin Viktorovich/HDN-9157-2022; Ďurikovič,
   Roman/T-4421-2018; Ďurikovič, Roman/J-8811-2014
OI Kolchin, Konstantin Viktorovich/0000-0001-6324-0560; Ďurikovič,
   Roman/0000-0002-4246-663X; Myszkowski, Karol/0000-0002-8505-4141
CR Callet P, 1996, COMPUT GRAPH FORUM, V15, P119, DOI 10.1111/1467-8659.1520119
   CALLET P, 1997, VISUALIZATION MODELL, P287
   COSTA AC, 1999, EUR REND WORKSH GRAN
   Dorsey J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P387, DOI 10.1145/237170.237278
   Dorsey J, 1999, COMP GRAPH, P225, DOI 10.1145/311535.311560
   Ershov S, 2001, COMPUT GRAPH FORUM, V20, pC227, DOI 10.1111/1467-8659.00515
   Hanrahan P., 1993, Computer Graphics Proceedings, P165, DOI 10.1145/166117.166139
   HOFMEISTER F, 1985, MONDIAL COULEUR 85
   Icart I, 1999, COMPUT GRAPH-UK, V23, P405, DOI 10.1016/S0097-8493(99)00048-5
   Kawai J. K., 1993, Computer Graphics Proceedings, P147, DOI 10.1145/166117.166136
   LETUNOV AA, 1999, P GRAPH 99 AUG 26 SE, P129
   McCamy CS, 1996, COLOR RES APPL, V21, P292, DOI 10.1002/(SICI)1520-6378(199608)21:4<292::AID-COL4>3.3.CO;2-7
   Pellacini F, 2000, COMP GRAPH, P55, DOI 10.1145/344779.344812
   Pharr M, 2000, COMP GRAPH, P75, DOI 10.1145/344779.344824
   Poulin P, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P58
   RANZ KD, 1985, MONDIAL COULEUR 85
   Rusinkiewicz Szymon., 1998, Eurographics rendering workshop 98, P11
   Schoeneman C., 1993, Computer Graphics Proceedings, P143, DOI 10.1145/166117.166135
   Schramm M, 1997, PROC GRAPH INTERF, P56
   SEQUIN CH, 1989, COMPUTER GRAPHICS, P307
   Stam J, 2001, SPRING EUROGRAP, P39
   Westlund HB, 2001, COMP GRAPH, P501, DOI 10.1145/383259.383318
NR 22
TC 26
Z9 30
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2004
VL 20
IS 8-9
BP 586
EP 600
DI 10.1007/s00371-004-0248-0
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 866DC
UT WOS:000224752600006
DA 2024-07-18
ER

PT J
AU Wilhelms, J
   Van Gelder, A
AF Wilhelms, J
   Van Gelder, A
TI Combining vision and computer graphics for video motion capture
SO VISUAL COMPUTER
LA English
DT Article
DE computer graphics; animation; vision; characters
ID NONRIGID MOTION
AB We describe innovative methods for extracting three-dimensional motion of humans and animals from unrestricted monocular video, using a combination of new and established computer vision and computer graphics techniques. We identity features using image processing and active contours. Active contours become anchored to model segments in specified image frames and automatically "pull" the segments to feature positions in other frames. Adjustments are subject to joint limits and may use inverse kinematics. Occluded contour points are detected using object geometry and do not participate in feature tracking. Interactive adjustments are possible at any time in the process, allowing extraction of complicated movements regardless of background, camera movement, or feature clarity.
C1 Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Santa Cruz
RP Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
EM wilhelms@cse.ucsc.edu; avg@cse.ucsc.edu
CR Aggarwal JK, 1998, COMPUT VIS IMAGE UND, V70, P142, DOI 10.1006/cviu.1997.0620
   AKITA K, 1984, PATTERN RECOGN, V17, P73, DOI 10.1016/0031-3203(84)90036-0
   [Anonymous], P C COMP VIS PATT RE
   ATKINSONDERMAN L, 2000, THESIS U CALIFORNIA
   Blake A., 1998, ACTIVE CONTOURS
   Bregler C., 1998, COMPUTER VISION PATT
   CHEN Z, 1992, IEEE T SYST MAN CYB, V22, P336, DOI 10.1109/21.148408
   GONCALVES L, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P764, DOI 10.1109/ICCV.1995.466861
   Goody P.C., 1983, Horse anatomy. A pictorial approach to equine structure
   HelOr Y, 1996, INT J COMPUT VISION, V19, P5, DOI 10.1007/BF00131146
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   KAKADIARIS IA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P618, DOI 10.1109/ICCV.1995.466881
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   LAPIERRE J, 1999, P 1999 IASTED COMP G
   Morris DD, 1998, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.1998.698622
   PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661
   Perales F. J., 1994, Proceedings of the 1994 IEEE Workshop on Motion of Non-Rigid and Articulated Objects (Cat. No.94TH0671-8), P83, DOI 10.1109/MNRAO.1994.346263
   REGH JM, 1994, P 1994 IEEE WORKSH M, P16
   Schneider PJ, 1998, COMP ANIM CONF PROC, P161
   SIMMONS M, 2002, ACM S COMP AN SAN AN, P139
   Sobel I., 1990, An Isotropic 3x3 Image Gradient Operator, P376, DOI [10.13140/RG.2.1.1912.4965, DOI 10.13140/RG.2.1.1912.4965]
   STUBBS G, 1976, ANATOMY HORSE
   Wilhelms J., 2001, Journal of Graphics Tools, V6, P27, DOI 10.1080/10867651.2001.10487539
   Wilhelms J, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P155, DOI 10.1109/HUMO.2000.897386
   Wilhelms J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P173, DOI 10.1145/258734.258833
NR 26
TC 3
Z9 4
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2003
VL 19
IS 6
BP 360
EP 376
DI 10.1007/s00371-003-0201-7
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 745PC
UT WOS:000186696200002
DA 2024-07-18
ER

PT J
AU Li, XZ
   Li, XF
   Deng, J
AF Li, Xiangzheng
   Li, Xiaofei
   Deng, Jia
TI Disentangled representation transformer network for 3D face
   reconstruction and robust dense alignment
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Face alignment and reconstruction; Convolutional neural networks;
   Disentangled representation learning; Biometrics
ID SINGLE IMAGE; REGRESSION
AB This paper proposes a disentangled representation transformer network (DRTN) for 3D dense face alignment and reconstruction. Unlike traditional 3DMM-based approaches, the target parameters, such as shape, expression, and pose, are individually estimated without considering their direct influences on one another and then jointly optimized. Hence, DRTN aims to enhance the representation of facial attributes in a semantic sense by learning the correlation of different 3D facial attribute parameters. To achieve this, we present a novel strategy to design disentangled 3D face attribute representation, which decomposes the given facial attributes into identity, expression, and poses. Specifically, the 3D face parameter estimation in the regression network depends on the correlation of other face attribute parameters rather than being independent. The branching of the identity component aims to reinforce learning the expression and pose attributes by preserving the overall face geometry structure and identity. Accordingly, the expression and pose parts of the branch preserve the consistency of expression and pose attributes, respectively. Moreover, DRTN helps refine the reconstruction and alignment of facial details in large poses, mainly by coupling other facial attribute parameters. Extensive qualitative and quantitative experimental results on widely evaluated benchmarking datasets demonstrate that our approach achieves competitive performance compared to state-of-the-art methods.
C1 [Li, Xiangzheng; Li, Xiaofei; Deng, Jia] Ningxia Normal Univ, Sch Math & Comp Sci, Xueyuan Rd, Guyuan 756000, Ningxia, Peoples R China.
C3 Ningxia Normal University
RP Li, XZ (corresponding author), Ningxia Normal Univ, Sch Math & Comp Sci, Xueyuan Rd, Guyuan 756000, Ningxia, Peoples R China.
EM 82021011@nxnu.edu.cn; 2080755443@qq.com; d792254122@163.com
RI Li, Xiangzheng/JWO-3584-2024
FU Ningxia Natural Science Foundation of China under Grant [2022AAC03327];
   Ningxia Natural Science Foundation of China [NJYKCSZ2308]; Ningxia
   Normal University Undergraduate Teaching Project
FX This work was supported in part by the Ningxia Natural Science
   Foundation of China under Grant 2022AAC03327 and in part by the Ningxia
   Normal University Undergraduate Teaching Project NJYKCSZ2308.
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], Faces in-the-wild challenge
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   BETTADAPURA V, 2012, COMPUTER SCI
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Browatzki B, 2020, PROC CVPR IEEE, P6109, DOI 10.1109/CVPR42600.2020.00615
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Bulat A, 2016, LECT NOTES COMPUT SC, V9914, P616, DOI 10.1007/978-3-319-48881-3_43
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925873
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao X, 2018, PROC CVPR IEEE, P4635, DOI 10.1109/CVPR.2018.00487
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cootes TF, 2012, LECT NOTES COMPUT SC, V7578, P278, DOI 10.1007/978-3-642-33786-4_21
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jain V., 2010, UMass Amherst technical report, V2
   Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142
   Jiang L., 2019, P IEEECVF INT C COMP, P0
   Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Lee GH, 2020, PROC CVPR IEEE, P6099, DOI 10.1109/CVPR42600.2020.00614
   Li C, 2014, LECT NOTES COMPUT SC, V8693, P218, DOI 10.1007/978-3-319-10602-1_15
   Li L, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4265, DOI 10.1109/ICASSP39728.2021.9413649
   Li XZ, 2021, INT C PATT RECOG, P7226, DOI 10.1109/ICPR48806.2021.9412668
   Liu F, 2018, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2018.00547
   Liu F, 2016, LECT NOTES COMPUT SC, V9909, P545, DOI 10.1007/978-3-319-46454-1_33
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Ning X, 2020, IEEE SIGNAL PROC LET, V27, P1944, DOI 10.1109/LSP.2020.3032277
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Wansa Wickramaratne V., 2009, Pattern Recognition and Image Analysis, V19, P78, DOI 10.1134/S1054661809010143
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Xia X, 2022, Arxiv, DOI arXiv:2205.09579
   Xing Wang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P493, DOI 10.1007/978-3-030-67832-6_40
   Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Ye HQ, 2016, INT J PROSTHODONT, V29, P213, DOI 10.11607/ijp.4397
   Yu R, 2017, IEEE I CONF COMP VIS, P4733, DOI 10.1109/ICCV.2017.506
   Yu X, 2016, IEEE T PATTERN ANAL, V38, P2212, DOI 10.1109/TPAMI.2015.2509999
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 57
TC 1
Z9 1
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 DEC 27
PY 2023
DI 10.1007/s00371-023-03202-4
EA DEC 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DK1M4
UT WOS:001131839600002
OA hybrid
DA 2024-07-18
ER

PT J
AU Endo, Y
AF Endo, Yuki
TI Masked-attention diffusion guidance for spatially controlling
   text-to-image generation
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Diffusion model; Text-to-image synthesis; Multimodal; Classifier
   guidance
AB Text-to-image synthesis has achieved high-quality results with recent advances in diffusion models. However, text input alone has high spatial ambiguity and limited user controllability. Most existing methods allow spatial control through additional visual guidance (e.g., sketches and semantic masks) but require additional training with annotated images. In this paper, we propose a method for spatially controlling text-to-image generation without further training of diffusion models. Our method is based on the insight that the cross-attention maps reflect the positional relationship between words and pixels. Our aim is to control the attention maps according to given semantic masks and text prompts. To this end, we first explore a simple approach of directly swapping the cross-attention maps with constant maps computed from the semantic regions. Some prior works also allow training-free spatial control of text-to-image diffusion models by directly manipulating cross-attention maps. However, these approaches still suffer from misalignment to given masks because manipulated attention maps are far from actual ones learned by diffusion models. To address this issue, we propose masked-attention guidance, which can generate images more faithful to semantic masks via indirect control of attention to each word and pixel by manipulating noise images fed to diffusion models. Masked-attention guidance can be easily integrated into pre-trained off-the-shelf diffusion models (e.g., Stable Diffusion) and applied to the tasks of text-guided image editing. Experiments show that our method enables more accurate spatial control than baselines qualitatively and quantitatively.
C1 [Endo, Yuki] Univ Tsukuba, Tsukuba, Ibaraki, Japan.
C3 University of Tsukuba
RP Endo, Y (corresponding author), Univ Tsukuba, Tsukuba, Ibaraki, Japan.
EM endo@cs.tsukuba.ac.jp
FU Japan Society for the Promotion of Science [23K11143]; JSPS KAKENHI
FX This work was supported by JSPS KAKENHI Grant Number 23K11143.
CR Abdal Rameen, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530747
   Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Antoniadis I, 2020, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2020/04/033
   Avrahami O, 2023, PROC CVPR IEEE, P18370, DOI 10.1109/CVPR52729.2023.01762
   Avrahami O, 2023, Arxiv, DOI arXiv:2206.02779
   Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Balaji Y, 2022, arXiv
   Bar-Tal O, 2023, Arxiv, DOI arXiv:2302.08113
   Brock A., 2019, INT C LEARN REPR
   Brooks T, 2023, PROC CVPR IEEE, P18392, DOI 10.1109/CVPR52729.2023.01764
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Couairon G., 2022, arXiv
   Dhariwal P, 2021, ADV NEUR IN, V34
   Endo Y, 2022, COMPUT GRAPH FORUM, V41, P395, DOI 10.1111/cgf.14686
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Ham C, 2023, Arxiv, DOI arXiv:2302.12764
   Harkonen Erik., 2020, NEURIPS 2019
   He Zhenliang., 2021, ICCV 2021
   Hertz Amir, 2022, arXiv
   Ho JAT, 2022, Arxiv, DOI arXiv:2207.12598
   Jahanian Ali., 2020, ICLR 2020
   Kang M, 2023, PROC CVPR IEEE, P10124, DOI 10.1109/CVPR52729.2023.00976
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2021, NeurIPS
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Kumari N, 2023, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR52729.2023.00192
   Kynkaanniemi Tuomas, 2023, P ICLR
   Li YH, 2023, PROC CVPR IEEE, P22511, DOI 10.1109/CVPR52729.2023.02156
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Ma WDK, 2023, Arxiv, DOI arXiv:2302.13153
   Meng C., 2022, 10 INT C LEARNING RE
   Nichol A, 2021, PR MACH LEARN RES, V139
   Pan X., 2023, ACM SIGGRAPH 2023 C
   Parmar G., 2023, arXiv
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, arXiv, DOI 10.48550/arXiv.2204.06125
   Ramesh P, 2022, 38 INT C MACHINE LEA
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Saharia C., 2022, NEURIPS 2022
   Sauer Axel, 2022, SIGGRAPH22 Conference Proceeding: Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, DOI 10.1145/3528233.3530738
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Song Jiaming, 2021, 9 INT C LEARN REPR I
   Spingarn Nurit., 2021, ICLR 2021
   Voynov A., 2020, ICML 2020
   Wang H, 2023, Arxiv, DOI arXiv:2210.00445
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926
   Zeng Y, 2023, PROC CVPR IEEE, P22468, DOI 10.1109/CVPR52729.2023.02152
   Zhan FN, 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence, V45, DOI arXiv:2112.13592
   Zhang L, 2023, Arxiv, DOI arXiv:2302.05543
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhu J., 2021, NEURIPS
NR 61
TC 0
Z9 0
U1 10
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 30
PY 2023
DI 10.1007/s00371-023-03151-y
EA NOV 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ4K2
UT WOS:001122252600004
DA 2024-07-18
ER

PT J
AU Ullah, F
   Wei, W
   Fan, Z
   Yu, QD
AF Ullah, Faheem
   Wei, Wu
   Fan, Zhun
   Yu, Qiuda
TI 6D object pose estimation based on dense convolutional object center
   voting with improved accuracy and efficiency
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE 6D object pose estimation; Pixel-wise labeling; Pixel-wise center
   localization; Pixel-wise quaternion rotation
AB 6D object pose estimation is an important application of computer vision and a basic module in robotic manipulation, but dealing with occlusion in a cluttered environment, handling symmetries, and textureless surfaces, are real issues. Other issues with such systems are accuracy and efficiency. The recent two-stage methods perform well in terms of accuracy; however, a linear increase in their runtimes occurs due to the increase in the number of objects in a scene. This paper proposes a fully convolutional and parallel architecture that obtains the 3D translation and orientation for object poses from the same pixel-wise dense estimation. It exploits the same voting block for inliers for multiple instances, final 3D translation estimation, and quaternions aggregation. Only the center point estimation of the objects decreases the model's running time, while still useful for occlusions and texturelessness. Symmetries and varieties are handled with a loss function based on shape matching and the pose of the object. Our proposed approach has fewer parameters and takes less time to train and evaluate, achieving great accuracy. Experiments on LINEMOD and occlusion LINEMOD datasets using ADD (-S) and 2D projection evaluation metrics show that the proposed method outperforms state-of-the-art approaches for 6D pose estimation.
C1 [Ullah, Faheem; Wei, Wu; Yu, Qiuda] China Univ Technol, Sch Automat Sci & Engn South, Guangzhou, Peoples R China.
   [Fan, Zhun] Shantou Univ, Coll Engn, Shantou, Guangdong, Peoples R China.
C3 Shantou University
RP Wei, W (corresponding author), China Univ Technol, Sch Automat Sci & Engn South, Guangzhou, Peoples R China.
EM ds.faheem@hotmail.com; weiwu@scut.edu.cn; zfan@stu.edu.cn;
   yuqiuda@163.com
FU Science and Technology Planning Project of Guangdong Province, China; 
   [2019A050520001]
FX This work was supported by the Science and Technology Planning Project
   of Guangdong Province, China, under the grant (2019A050520001).
CR Billings G., SilhoNet: an RGB method for 3D object pose estimation and grasp planning
   Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Bukschat Y, 2020, Arxiv, DOI arXiv:2011.04307
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Cao Z, 2016, IEEE INT CONF ROBOT, P2441, DOI 10.1109/ICRA.2016.7487396
   Capellen C, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 5: VISAPP, P162, DOI 10.5220/0008990901620172
   Chen T, 2022, COGN COMPUT, V14, P702, DOI 10.1007/s12559-021-09966-y
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Feng HT, 2021, INT C PATT RECOG, P685, DOI 10.1109/ICPR48806.2021.9412494
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FORL Arge, 2015, ICLR, P1
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He Yang, 2020, CVPR, P2009
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hua WT, 2021, IEEE ROBOT AUTOM LET, V6, P2886, DOI 10.1109/LRA.2021.3062304
   Iwase S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3283, DOI 10.1109/ICCV48922.2021.00329
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Krull A, 2015, IEEE I CONF COMP VIS, P954, DOI 10.1109/ICCV.2015.115
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XL, 2022, VISUAL COMPUT, V38, P2603, DOI 10.1007/s00371-021-02135-0
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Pavlakos G., 2017, P IEEE C COMPUTER VI, P2011, DOI DOI 10.1109/CVPR.2017.139
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Periyasamy A. S., 2020, INT JOINT C COMP VIS, P353, DOI [10.1007/978-3-030-94893-1, DOI 10.1007/978-3-030-94893-1]
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Ramashala P A., 2018, International Association for Management of Technology, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Su YZ, 2022, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR52688.2022.00662
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758
   Ullah F, 2022, SCI PROGRAMMING-NETH, V2022, DOI 10.1155/2022/2037141
   Wu J, 2022, Arxiv, DOI arXiv:2207.00260
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xu Z., 2022, BiCo-net: regress globally, match locally for robust 6D pose estimation
   Yu X, 2020, Arxiv, DOI arXiv:2002.03923
   Yu Y, 2022, VISUAL COMPUT, V38, P1483, DOI 10.1007/s00371-021-02082-w
   Zabulis X, 2018, VISUAL COMPUT, V34, P193, DOI 10.1007/s00371-016-1326-9
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
NR 49
TC 0
Z9 0
U1 4
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 NOV 25
PY 2023
DI 10.1007/s00371-023-03113-4
EA NOV 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Z7AD3
UT WOS:001113553900001
DA 2024-07-18
ER

PT J
AU Chen, LQ
   Chen, JW
   Xu, ZM
   Liao, YP
   Chen, ZZ
AF Chen, Liangqin
   Chen, Jiwang
   Xu, Zhimeng
   Liao, Yipeng
   Chen, Zhizhang
TI Two-stage dual-resolution face network for cross-resolution face
   recognition in surveillance systems
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Cross-resolution face recognition; Convolutional neural network;
   Multi-resolution feature fusion; Curriculum learning; Surveillance
   systems
AB Face recognition for surveillance remains a complex challenge due to the disparity between low-resolution (LR) face images captured by surveillance cameras and the typically high-resolution (HR) face images in databases. To address this cross-resolution face recognition problem, we propose a two-stage dual-resolution face network to learn more robust resolution-invariant representations. In the first stage, we pre-train the proposed dual-resolution face network using solely HR images. Our network utilizes a two-branch structure and introduces bilateral connections to fuse the high- and low-resolution features extracted by two branches, respectively. In the second stage, we introduce the triplet loss as the fine-tuning loss function and design a training strategy that combines the triplet loss with competence-based curriculum learning. According to the competence function, the pre-trained model can train first from easy sample sets and gradually progress to more challenging ones. Our method achieves a remarkable face verification accuracy of 99.25% on the native cross-quality dataset SCFace and 99.71% on the high-quality dataset LFW. Moreover, our method also enhances the face verification accuracy on the native low-quality dataset.
C1 [Chen, Liangqin; Chen, Jiwang; Xu, Zhimeng; Liao, Yipeng; Chen, Zhizhang] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou, Peoples R China.
   [Chen, Zhizhang] Dalhousie Univ, Dept Elect & Comp Engn, Halifax, NS, Canada.
C3 Fuzhou University; Dalhousie University
RP Xu, ZM (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou, Peoples R China.
EM chenlq2020@fzu.edu.cn; 211127153@fzu.edu.cn; zhmxu@fzu.edu.cn;
   fzu_lyp@163.com; z.chen@dal.ca
RI XU, ZHIMENG/KIH-8372-2024
OI XU, ZHIMENG/0000-0003-2705-1812
FU This study was supported by the National Natural Science Foundation of
   China under Grant 62071125 and the Natural Science Foundation of Fujian
   Province under Grants 2021J01581 and 2018J01805. [62071125]; National
   Natural Science Foundation of China [2021J01581, 2018J01805]; Natural
   Science Foundation of Fujian Province
FX This study was supported by the National Natural Science Foundation of
   China under Grant 62071125 and the Natural Science Foundation of Fujian
   Province under Grants 2021J01581 and 2018J01805.
CR [Anonymous], 2015, BRIT MACH VIS C
   Platanios EA, 2019, Arxiv, DOI arXiv:1903.09848
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Cheng Z., 2018, ASIAN C COMPUTER VIS, P605
   Cheng ZY, 2018, Arxiv, DOI arXiv:1804.09691
   Chu J, 2018, IEEE ACCESS, V6, P19959, DOI 10.1109/ACCESS.2018.2815149
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Ge SM, 2020, AAAI CONF ARTIF INTE, V34, P10845
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049
   Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han Fang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P741, DOI 10.1007/978-3-030-58555-6_44
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Jiang JJ, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3485132
   Khalid Syed Safwan, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P410, DOI 10.1109/TBIOM.2020.3007356
   Knoche M., 2021, arXiv
   Knoche M, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9666960
   Konche M., 2023, INT C AUTOMATIC FACE, P1, DOI [10.1109/fg57933.2023.10042669, DOI 10.1109/FG57933.2023.10042669]
   Lai SC, 2021, ASIAPAC SIGN INFO PR, P1444
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Lu ZJ, 2018, IEEE ACCESS, V6, DOI [10.1109/ACCESS.2018.2864189, 10.1109/LSP.2018.2810121]
   Massoli FV, 2020, IMAGE VISION COMPUT, V99, DOI 10.1016/j.imavis.2020.103927
   Mudunuri SP, 2018, IEEE COMPUT SOC CONF, P602, DOI 10.1109/CVPRW.2018.00090
   Pan HH, 2023, IEEE T INTELL TRANSP, V24, P3448, DOI 10.1109/TITS.2022.3228042
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun JN, 2020, SIGNAL PROCESS-IMAGE, V82, DOI 10.1016/j.image.2019.115766
   Sun Y, 2014, ADV NEUR IN, V27
   Wang HH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4053, DOI 10.1145/3503161.3548196
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Yin X., 2020, P AS C COMP VIS, DOI [10.1007/978-3-030-69532-3_19, DOI 10.1007/978-3-030-69532-3_19]
   Zangeneh E, 2020, EXPERT SYST APPL, V139, DOI 10.1016/j.eswa.2019.112854
   Zha J, 2019, INT CONF ACOUST SPEE, P3302, DOI [10.1109/icassp.2019.8682384, 10.1109/ICASSP.2019.8682384]
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang YQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041010
   Zhong YY, 2021, Arxiv, DOI arXiv:2103.14803
NR 38
TC 0
Z9 0
U1 5
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 20
PY 2023
DI 10.1007/s00371-023-03121-4
EA OCT 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U9HU2
UT WOS:001087848800002
DA 2024-07-18
ER

PT J
AU El Salam, NHA
   Xiong, SW
   Liu, X
AF El Salam, Nada Hussien Abd
   Xiong, Shengwu
   Liu, Xuan
TI Reversible data-hiding exploiting huffman encoding in dual image using
   weighted matrix and generalized exploiting modification direction (GEMD)
SO VISUAL COMPUTER
LA English
DT Article
DE Reversibility; Weighted matrix; Security; Generalized exploiting
   modification direction (GEMD); Multilayer center folding technique
   (MCFT)
ID PREDICTION-ERROR EXPANSION; EFFICIENT; SCHEME; INTERPOLATION; STRATEGY;
   PAYLOAD; PIXELS
AB In reversible data-hiding techniques, the quality of the steganographic image and its embedding capacity are the most crucial characteristics. The main objective of this study is to enhance the Biswapati et al. approach, which embeds secret data directly in interpolated pixels without taking context pixel properties into account. Additionally, they recorded an extremely large position value of the weighted matrix, which led to a significant amount of visual distortion. To deal with this issue, a novel reversible data-hiding method based on a multilayer center folding technique (MCFT) is developed. The suggested approach divides the interpolated cover image into 5 x 5 non-overlapping blocks and then sorts them in descending order of standard deviations. As a result of this sorting, edges and textures are more effectively preserved while also reducing the appearance of frequent interpolation defects. Unlike previous weighted matrix approaches, the position value of the weighted matrix is not embedded directly to generate the stego-image. MCFT is used before embedding to reduce the difference between an image pixel and a stego pixel so that image quality is not destroyed. The Huffman encoding technique is used to preprocess the acquired secret data to increase the embedding capacity as much as feasible. Additionally, to increase embedding rates, dual images are used for exploiting the characteristics of exploiting modification direction. The findings achieved significantly outperform state-of-the-art algorithms, which provide visual quality of more than 50.8 dB peak signal-to-noise ratio PSNR. When compared to modern techniques, the approach also offers the highest level of security and can retrieve the original image without losing any data.
C1 [El Salam, Nada Hussien Abd] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
   [El Salam, Nada Hussien Abd] Minia Univ, Fac Sci, Comp Sci Dept, El Minia 61111, Egypt.
   [Xiong, Shengwu] Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Hainan 572000, Peoples R China.
   [Xiong, Shengwu] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
   [Xiong, Shengwu] Qiongtai Normal Univ, Sch Informat Sci & Technol, Hainan 571127, Peoples R China.
   [Liu, Xuan] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Kowloon, Hong Kong, Peoples R China.
C3 Wuhan University of Technology; Egyptian Knowledge Bank (EKB); Minia
   University; Wuhan University of Technology; Qiongtai Normal University;
   Hong Kong Polytechnic University
RP Xiong, SW (corresponding author), Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Hainan 572000, Peoples R China.; Xiong, SW (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.; Xiong, SW (corresponding author), Qiongtai Normal Univ, Sch Informat Sci & Technol, Hainan 571127, Peoples R China.
EM xiongsw@whut.edu.cn
OI Liu, Xuan/0000-0002-6194-4588
FU This work was in part supported by the National Key Research and
   Development Program of China (Grant No. 2022ZD0160604) and NSFC (Grant
   No. 62176194), and the Key Research and Development Program of Hubei
   Province (Grant No. 2023BAB083), the Project of San [2022ZD0160604];
   National Key Research and Development Program of China [62176194]; NSFC
   [2023BAB083]; Key Research and Development Program of Hubei Province
   [SCKJ-JYRC-2022-76, SKJC-2022-PTDX-031]; Project of Sanya Yazhou Bay
   Science and Technology City [2021KF0031]; Project of Sanya Science and
   Education Innovation Park of Wuhan University of Technology
FX This work was in part supported by the National Key Research and
   Development Program of China (Grant No. 2022ZD0160604) and NSFC (Grant
   No. 62176194), and the Key Research and Development Program of Hubei
   Province (Grant No. 2023BAB083), the Project of Sanya Yazhou Bay Science
   and Technology City (Grant No. SCKJ-JYRC-2022-76, SKJC-2022-PTDX-031),
   and the Project of Sanya Science and Education Innovation Park of Wuhan
   University of Technology (Grant No. 2021KF0031).
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], 2009, International Journal of Signal processing, Image processing and pattern
   Bai XM, 2022, J INF SECUR APPL, V65, DOI 10.1016/j.jisa.2021.103068
   Biswapati J, 2016, ADV INTELL SYST, V411, P239, DOI 10.1007/978-81-322-2731-1_22
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang C.-C., 2007, TENCON 2007 2007 IEE
   Chang C.-C., 2013, 2013 9 INT C INF COM
   Chang C.-C., 2007, 3 INT C INT INF HID
   Chang CC, 2009, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA AND UBIQUITOUS ENGINEERING (MUE 2009), P145, DOI 10.1109/MUE.2009.35
   Chang YT, 2013, J SUPERCOMPUT, V66, P1093, DOI 10.1007/s11227-013-1016-6
   Chen XF, 2021, J INF SECUR APPL, V58, DOI 10.1016/j.jisa.2020.102702
   Chen YQ, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102584
   Chowdhuri P, 2020, J INF SECUR APPL, V50, DOI 10.1016/j.jisa.2019.102420
   Fan L, 2013, COMPUT ELECTR ENG, V39, P873, DOI 10.1016/j.compeleceng.2012.06.014
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gao GY, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107817
   Gao XY, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107579
   Garcia-Nonoal Z., 2023, VISUAL COMPUT, V6, P66
   Gull S, 2020, COMPUT COMMUN, V163, P134, DOI 10.1016/j.comcom.2020.08.023
   Hassan FS, 2020, MULTIMED TOOLS APPL, V79, P30087, DOI 10.1007/s11042-020-09513-1
   Jafar IF, 2016, SIGNAL PROCESS, V128, P98, DOI 10.1016/j.sigpro.2016.03.023
   Jana B, 2016, OPTIK, V127, P3347, DOI 10.1016/j.ijleo.2015.12.055
   Jaya Prakash S., 2022, VISUAL COMPUT, V66, P1
   Jia YJ, 2019, SIGNAL PROCESS, V163, P238, DOI 10.1016/j.sigpro.2019.05.020
   Jung KH, 2009, COMPUT STAND INTER, V31, P465, DOI 10.1016/j.csi.2008.06.001
   Kuo WC, 2013, IMAGING SCI J, V61, P484, DOI 10.1179/1743131X12Y.0000000011
   Kuo WC, 2016, OPTIK, V127, P1762, DOI 10.1016/j.ijleo.2015.08.056
   Lee CF, 2017, MULTIMED TOOLS APPL, V76, P9993, DOI 10.1007/s11042-016-3591-z
   Lee CF, 2012, EXPERT SYST APPL, V39, P6712, DOI 10.1016/j.eswa.2011.12.019
   Li J, 2013, SIGNAL PROCESS, V93, P2748, DOI 10.1016/j.sigpro.2013.01.020
   Lu TC, 2021, SOFT COMPUT, V25, P161, DOI 10.1007/s00500-020-05129-7
   Lu TC, 2018, SIGNAL PROCESS, V142, P244, DOI 10.1016/j.sigpro.2017.07.025
   Lu TC, 2017, MULTIMED TOOLS APPL, V76, P1827, DOI 10.1007/s11042-015-3168-2
   Lu TC, 2015, SIGNAL PROCESS, V115, P195, DOI 10.1016/j.sigpro.2015.03.017
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Mandal PC, 2021, MULTIMED TOOLS APPL, V80, P3623, DOI 10.1007/s11042-020-09341-3
   Mata-Mendoza D, 2022, VISUAL COMPUT, V38, P2073, DOI 10.1007/s00371-021-02267-3
   Mohammad AA, 2019, MULTIMED TOOLS APPL, V78, P7181, DOI 10.1007/s11042-018-6465-8
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Parah SA, 2020, FUTURE GENER COMP SY, V108, P935, DOI 10.1016/j.future.2018.02.023
   Peng F, 2012, SIGNAL PROCESS, V92, P54, DOI 10.1016/j.sigpro.2011.06.006
   Qin C, 2015, MULTIMED TOOLS APPL, V74, P5861, DOI 10.1007/s11042-014-1894-5
   Qiu YQ, 2016, IEEE SIGNAL PROC LET, V23, P130, DOI 10.1109/LSP.2015.2504464
   Shaji C, 2021, MULTIMED TOOLS APPL, V80, P13595, DOI 10.1007/s11042-020-10240-w
   Shastri S, 2019, J VIS COMMUN IMAGE R, V61, P130, DOI 10.1016/j.jvcir.2019.03.022
   Su G-D., 2022, VISUAL COMPUT, V66, P1
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tseng YC, 2002, IEEE T COMMUN, V50, P1227, DOI 10.1109/TCOMM.2002.801488
   Wahed MA, 2019, MULTIMED TOOLS APPL, V78, P10795, DOI 10.1007/s11042-018-6616-y
   Wang X, 2010, IEEE SIGNAL PROC LET, V17, P567, DOI 10.1109/LSP.2010.2046930
   Weber Allan G, 2006, The USC-SIPI Image Database
   Xiong XG, 2022, SIGNAL PROCESS, V194, DOI 10.1016/j.sigpro.2022.108458
   Yang H.W., 2011, 2011 4 INT C UB MED
   Yao H, 2017, SIGNAL PROCESS, V135, P26, DOI 10.1016/j.sigpro.2016.12.029
   Zhang XP, 2006, IEEE COMMUN LETT, V10, P781, DOI 10.1109/LCOMM.2006.060863
NR 56
TC 0
Z9 0
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3663
EP 3691
DI 10.1007/s00371-023-03058-8
EA OCT 2023
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001079888400003
DA 2024-07-18
ER

PT J
AU Ding, B
   Zhang, LB
   He, YJ
   Qin, J
AF Ding, Bo
   Zhang, Libao
   He, Yongjun
   Qin, Jian
TI 3D shape classification based on global and local features extraction
   with collaborative learning
SO VISUAL COMPUTER
LA English
DT Article
DE 3D shape classification; Transformer encoder; Collaborative learning;
   Local features; Global features
ID NETWORK
AB It is important to extract both global and local features for view-based 3D shape classification. Therefore, we propose a 3D shape classification method based on global and local features extraction with collaborative learning. This method consists of a patch-level transformer sub-network (PTS) and a view-level transformer sub-network (VTS). In the PTS, a single view is divided into multiple patches. And a multi-layer transformer encoder is employed to accurately highlight discriminative patches and capture correlations among patches in a view, which can efficiently filter out the meaningless information and enhance meaningful information. The PTS can aggregate patch features into a 3D shape representation with rich local details. In the VTS, a multi-layer transformer encoder is employed to assign different attention to each view and obtain the contextual relationship among views, which can highlight the discriminative views among all the views of the same 3D shape and efficiently aggregate view features into a 3D shape representation. A collaborative loss is applied to encourage the two branches to learn collaboratively and teach each other in training. Experiments on two 3D benchmark datasets show that our proposed method outperforms current methods.
C1 [Ding, Bo; Zhang, Libao; Qin, Jian] Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xuefu Rd, Harbin 150080, Peoples R China.
   [He, Yongjun] Harbin Inst Technol, Sch Comp Sci & Technol, 92 XiDaZhi St, Harbin 150006, Peoples R China.
C3 Harbin University of Science & Technology; Harbin Institute of
   Technology
RP He, YJ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, 92 XiDaZhi St, Harbin 150006, Peoples R China.
EM heyongjun@hit.edu.cn
OI He, Yongjun/0000-0002-5156-651X
FU This paper is supported by the National Natural Science Foundation of
   China (No.61673142) and the Natural Science Foundation of Hei LongJiang
   Province of China (No.LH2022F029). [61673142]; National Natural Science
   Foundation of China; Natural Science Foundation of Hei LongJiang
   Province of China
FX This paper is supported by the National Natural Science Foundation of
   China (No.61673142) and the Natural Science Foundation of Hei LongJiang
   Province of China (No.LH2022F029).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [白静 Bai Jing], 2019, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V31, P303
   Chen S., 2022, IEEE T PATTERN ANAL, V01, P1
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DING B, 2020, COMPLEXITY, P1
   Ding B, 2020, IEEE ACCESS, V8, P200812, DOI 10.1109/ACCESS.2020.3035583
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao YB, 2023, IEEE T INTELL TRANSP, V24, P2158, DOI 10.1109/TITS.2022.3140355
   Gao Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2699, DOI 10.1145/3474085.3475450
   Gao Z, 2020, NEURAL NETWORKS, V125, P290, DOI 10.1016/j.neunet.2020.02.017
   Ge Y., 2019, INT C LEARN REPR
   Han ZZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P766
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   Hassani A., 2021, ARXIV
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He XW, 2019, IEEE I CONF COMP VIS, P7514, DOI 10.1109/ICCV.2019.00761
   Hegde V., 2016, ARXIV
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Liang Q, 2021, PATTERN RECOGN LETT, V150, P214, DOI 10.1016/j.patrec.2021.07.010
   Liang Q, 2020, IEEE ACCESS, V8, P139792, DOI 10.1109/ACCESS.2020.3012692
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Liu AA, 2020, IEEE ACCESS, V8, P155939, DOI 10.1109/ACCESS.2020.3018875
   Liu AA, 2020, MULTIMED TOOLS APPL, V79, P4699, DOI 10.1007/s11042-019-7521-8
   Liu SK, 2018, INT CONF 3D VISION, P542, DOI 10.1109/3DV.2018.00068
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P1744, DOI 10.1109/TIP.2020.3048623
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZH, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108774
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun K, 2021, IEEE T IMAGE PROCESS, V30, P868, DOI 10.1109/TIP.2020.3039378
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Yu RX, 2020, IEEE T IMAGE PROCESS, V29, P4530, DOI 10.1109/TIP.2020.2967579
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 43
TC 0
Z9 0
U1 5
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4539
EP 4551
DI 10.1007/s00371-023-03098-0
EA SEP 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001075960500001
DA 2024-07-18
ER

PT J
AU Chen, ZJ
   Li, Q
   Feng, HJ
   Xu, ZH
   Chen, YT
   Jiang, TT
AF Chen, Zhaojie
   Li, Qi
   Feng, Huajun
   Xu, Zhihai
   Chen, Yueting
   Jiang, Tingting
TI Dehaze on small-scale datasets via self-supervised learning
SO VISUAL COMPUTER
LA English
DT Article
DE Pretext task; Single image dehazing; Self-supervised learning
ID IMAGE; NETWORK
AB Real-world dehazing datasets usually suffer from small scales because of high collection costs. If networks are trained with such insufficient data, it leads to not only low performance on objective metrics, but also visually insufficient contrast enhancement. Self-supervised learning helps networks learn useful knowledge from unlabeled data and further achieve better performance on small-scale data, which has achieved great success on high-level vision tasks. However, there are rare works to develop self-supervised learning on low-level vision tasks, such as dehazing. In this paper, we propose a simple but effective self-supervised learning method for dehazing, to improve networks' performance on small-scale real-world datasets. Our useful observations are twofold. First, generating visually pleasing haze-free images from real-world hazy images is very difficult, but generating visually pleasing denser hazy images is much easier. Second, forcing networks to reduce dense haze will enhance the contrast enhancement capability of networks, and it is beneficial for further dehazing. Therefore, we generate numerous denser hazy images rehazy from a real-world hazy image. With pretraining on image pairs [rehazy, hazy], networks learn key capabilities of enhancing contrast. Experiments show that it stably outperforms directly supervised learning by a considerable margin, but only spends a cheap extra pretraining time cost.
C1 [Chen, Zhaojie; Jiang, Tingting] Zhejiang Lab, Res Ctr Intelligent Sensing Syst, Hangzhou 311100, Peoples R China.
   [Chen, Zhaojie; Li, Qi; Feng, Huajun; Xu, Zhihai; Chen, Yueting] Zhejiang Univ, State Key Lab Modern Opt Instrumentat, Hangzhou 310027, Peoples R China.
C3 Zhejiang Laboratory; Zhejiang University
RP Jiang, TT (corresponding author), Zhejiang Lab, Res Ctr Intelligent Sensing Syst, Hangzhou 311100, Peoples R China.
EM 22030045@zju.edu.cn; liqi@zju.edu.cn; fenghj@zju.edu.cn;
   xuzh@zju.edu.cn; chenyt@zju.edu.cn; eagerjtt@zhejianglab.com
FU This work was supported by the Key Research Project of Zhejiang Lab
   (No.2021MH0AC01), Civil Aerospace Pre-Research Project (No. D040104) and
   National Natural Science Foundation of China (No. 61975175). We also
   thank Meijuan Bian from the facility platform [2021MH0AC01]; Key
   Research Project of Zhejiang Lab [D040104]; Civil Aerospace Pre-Research
   Project [61975175]; National Natural Science Foundation of China
FX This work was supported by the Key Research Project of Zhejiang Lab
   (No.2021MH0AC01), Civil Aerospace Pre-Research Project (No. D040104) and
   National Natural Science Foundation of China (No. 61975175). We also
   thank Meijuan Bian from the facility platform of optical engineering of
   Zhejiang University for instrument support.
CR An Y., 2021, IJCAI, P2140
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/icip.2019.8803046, 10.1109/ICIP.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fathony R., 2018, DISCRETE WASSERSTEIN
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Guo TT, 2019, IEEE COMPUT SOC CONF, P2122, DOI 10.1109/CVPRW.2019.00265
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Liu J, 2020, IEEE COMPUT SOC CONF, P1732, DOI 10.1109/CVPRW50498.2020.00223
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Manu CM, 2023, VISUAL COMPUT, V39, P3923, DOI 10.1007/s00371-022-02536-9
   Morales P, 2019, IEEE COMPUT SOC CONF, P2078, DOI 10.1109/CVPRW.2019.00260
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Singh Ayush, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P166, DOI 10.1007/978-3-030-66823-5_10
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Yu H, 2022, LECT NOTES COMPUT SC, V13679, P181, DOI 10.1007/978-3-031-19800-7_11
   Zhang JW, 2011, VISUAL COMPUT, V27, P749, DOI 10.1007/s00371-011-0569-8
   Zhang JA, 2022, IEEE T CYBERNETICS, V52, P11187, DOI 10.1109/TCYB.2021.3070310
   Zhang M., 2020, P INT C LEARN REPR, P1
   Zhang S., 2021, IEEE T CYBERNETICS, V51, P457
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
NR 46
TC 0
Z9 0
U1 5
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4235
EP 4249
DI 10.1007/s00371-023-03079-3
EA SEP 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001069085600001
DA 2024-07-18
ER

PT J
AU Yu, XS
   Pang, Y
   Chi, JN
   Qi, Q
AF Yu, Xiaosheng
   Pang, Yu
   Chi, Jianning
   Qi, Qi
TI Cross-modal collaborative propagation for RGB-T saliency detection
SO VISUAL COMPUTER
LA English
DT Article
DE Saliency detection; Collaborative learning; Propagation mechanism; Deep
   features optimization; Multi-modal integration
ID IMAGE
AB Recently, RGB-T saliency detection becomes gradually a hot topic due to the fact that RGB-T multi-modal data could overcome the limitation of conventional RGB data in some cases. However, existing RGB-T saliency detection methods usually fail to take both advantages of two modalities and cannot boost performance effectively. Therefore, we achieve RGB-T saliency detection via a novel method, namely cross-modal collaborative propagation (CMCP), which contains a novel saliency propagation mechanism and a novel cross-modal collaborative learning framework relied on the proposed propagation mechanism. More specifically, we firstly propose a novel saliency propagation method and then, respectively, regard two modalities as inputs to generate RGB-induced and thermal-induced propagation mechanisms. To bridge RGB-T modalities, a novel cross-modal collaborative learning framework between RGB-induced and thermal-induced propagation mechanisms is devised to optimize, respectively, two propagation results. In other words, two modalities constantly extract supervision information to help the opposite side to refine propagation result until attaining a stable state. Finally, we integrate two modalities-induced propagation results into a refined saliency map. We compare our model with the state-of-the-art RGB-T and RGB saliency detection algorithms on three benchmark datasets, and experimental results show that the proposed CMCP achieves the significant improvement.
C1 [Yu, Xiaosheng; Chi, Jianning] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
   [Pang, Yu] Shenyang Univ Technol, Sch Artificial Intelligence, Shenyang 110870, Peoples R China.
   [Qi, Qi] Liaoning Prov Party Comm, Party Sch, Dept Decis Consulting, Shenyang 110004, Peoples R China.
C3 Northeastern University - China; Shenyang University of Technology
RP Pang, Y (corresponding author), Shenyang Univ Technol, Sch Artificial Intelligence, Shenyang 110870, Peoples R China.
EM pangyu@sut.edu.cn
FU Natural Science Foundation of Liaoning Province; Foundation of National
   Key Laboratory [2021-KF-12-01];  [OEIP-O-202005]
FX This work was supported in part by Natural Science Foundation of
   Liaoning Province 2021-KF-12-01 and the Foundation of National Key
   Laboratory OEIP-O-202005.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Cao YZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108447
   Chen SH, 2016, PATTERN RECOGN, V60, P2, DOI 10.1016/j.patcog.2016.05.016
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Huang LM, 2022, IEEE T CIRC SYST VID, V32, P1366, DOI 10.1109/TCSVT.2021.3069812
   Huang LM, 2020, IEEE SIGNAL PROC LET, V27, P1585, DOI 10.1109/LSP.2020.3020735
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Li CL, 2019, IEEE T CIRC SYST VID, V29, P2913, DOI 10.1109/TCSVT.2018.2874312
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Liao X, 2022, IEEE T DEPEND SECURE, V19, P897, DOI 10.1109/TDSC.2020.3004708
   Liao X, 2020, IEEE T CIRC SYST VID, V30, P685, DOI 10.1109/TCSVT.2019.2896270
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu YH, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108430
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Ma JL, 2020, IEEE T INSTRUM MEAS, V69, P9720, DOI 10.1109/TIM.2020.3001796
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Pang Y, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109138
   Pang Y, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115928
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan JX, 2022, IEEE T NETW SCI ENG, V9, P888, DOI 10.1109/TNSE.2021.3139671
   Tang J, 2020, IEEE T CIRC SYST VID, V30, P4421, DOI 10.1109/TCSVT.2019.2951621
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Tu Z., 2020, ARXIV
   Tu Z., 2020, IEEE C MULT INF PROC, V87
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Wang G., 2018, CHINESE C IMAGE GRAP, P359
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zeng HT, 2022, IEEE T MULTIMEDIA, V24, P141, DOI 10.1109/TMM.2020.3046877
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733
   Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang M, 2018, J VIS COMMUN IMAGE R, V52, P131, DOI 10.1016/j.jvcir.2018.01.004
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhou H, 2023, IEEE T IMAGE PROCESS, V32, P2593, DOI 10.1109/TIP.2023.3270801
NR 48
TC 0
Z9 0
U1 4
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4337
EP 4354
DI 10.1007/s00371-023-03085-5
EA SEP 2023
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001094628400002
DA 2024-07-18
ER

PT J
AU Zhang, XN
   Tian, CN
   Gao, XB
AF Zhang, Xiangnan
   Tian, Chunna
   Gao, Xinbo
TI An efficient and universal polygon prediction method based on derivable
   analytic geometry for arbitrary-shaped text detection
SO VISUAL COMPUTER
LA English
DT Article
DE Text detection; Intersection over union; Curved text; Polygon prediction
AB A polygon can represent the boundary of curved text more compactly than a rectangle. However, predicting reasonable polygon lacks of solutions due to the complex spatial relationships caused by having more vertices. The two main challenges are how to satisfy the constraints between vertices and how to cope with data conflicts caused by inconsistent annotation standards. To address these problems, we propose a divide and conquer methodology, in which a polygon is considered as a set of convex quadrangles. By predicting quadrangles in sequence, the vertices of the polygon are obtained consecutively and constrained by the previous ones. Then, we propose a measure for the overlap between convex quadrangles, with which the IoU between two polygons is calculated densely. Our method is derivable and can be trained end-to-end. Also, the polygon prediction branch that we proposed is universal and transplantable. We select basic architecture as the backbone, and the text/non-text classification branch adopts an online hard example mining strategy. Experiments on curved benchmark datasets, namely Total Text and CTW1500, demonstrate that our approach achieves state-of-the-art accuracy. It also maintains a high level of inferring efficiency.
C1 [Zhang, Xiangnan; Tian, Chunna] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Chongqing University of Posts & Telecommunications
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM zxnn81@outlook.com; chnatian@xidian.edu.cn; gaoxb@cqupt.edu.cn
FU National Natural Science Foundation of China [62036007, 62176195,
   62221005, U22A2096, U21A20514]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62036007, 62176195, 62221005, U22A2096
   and U21A20514.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Cao M, 2022, IEEE T CIRC SYST VID, V32, P758, DOI 10.1109/TCSVT.2021.3068133
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H, 2022, VISUAL COMPUT, V38, P3231, DOI 10.1007/s00371-022-02570-7
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Yun, 2017, arXiv
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Nayef N., 2019 INT C DOC AN RE
   Nayef N., 14 IAPR INT C DOC AN
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Qiao L, 2020, AAAI CONF ARTIF INTE, V34, P11899
   Raisi Z, 2021, IEEE COMPUT SOC CONF, P3156, DOI 10.1109/CVPRW53098.2021.00353
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ronen R., 2022, COMPUTER VISION ECCV
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Song SB, 2022, PROC CVPR IEEE, P15660, DOI 10.1109/CVPR52688.2022.01523
   Tang JQ, 2022, PROC CVPR IEEE, P4553, DOI 10.1109/CVPR52688.2022.00452
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2023, IEEE T IMAGE PROCESS, V32, P1, DOI 10.1109/TIP.2022.3201467
   Wang P., 2021, 5 AAAI C ART INT AAA, P2782
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wu H, 2017, VISUAL COMPUT, V33, P113, DOI 10.1007/s00371-015-1156-1
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Yu J., 2016, P 2016 ACM C MULT C
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 49
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2024
VL 40
IS 6
BP 4273
EP 4285
DI 10.1007/s00371-023-03081-9
EA SEP 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV2X4
UT WOS:001068126100001
DA 2024-07-18
ER

PT J
AU Meikap, S
   Jana, B
   Lu, TC
AF Meikap, Sudipta
   Jana, Biswapati
   Lu, Tzu-Chuen
TI Context pixel-based reversible data hiding scheme using pixel value
   ordering
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Reversible data hiding; Context pixel-based PVO; Embedding capacity;
   Steganalysis; Steganographic attacks
ID PREDICTION-ERROR EXPANSION; DIFFERENCE EXPANSION; IMAGE WATERMARKING;
   TRANSFORM; INTERPOLATION; PVO
AB Pixel value ordering (PVO) and prediction error expansion (PEE) techniques are now being combined for improved performance in the domain of reversible data hiding (RDH). Conventional PVO can only embed two data bits in a smooth picture block in a row or column, which is insufficient to meet the contemporary demands of much payload in RDH. There is a lot of room to enhance the bit stuffing by inserting more than two data bits in a smooth block while maintaining acceptable visual image quality and data security. In this study, we suggest a context pixel-based data concealing strategy based on PVO combined with PEE, in which each block can insert two or more data bits. This proposed approach has three distinct features: smooth block selection, minimum and maximum position selection, and pixel difference with median value for data embedding. Furthermore, more than two pixels can be used to embed the secret data bit. The block is recommended by the selection of the neighbouring and present block's pixel. The use of context pixels in context pixel-based PVO (CPPVO) improves both embedding efficiency and picture quality. When the block size is Z = (3x3), the average embedding capacity (EC) of our novel approach is over 70,000 bits, and the average PSNR is over 40 dB. The results of the experiments reveal that our technique is more dependable than other state-of-the-art systems in terms of security, picture quality, and secret volume.
C1 [Meikap, Sudipta; Jana, Biswapati] Vidyasagar Univ, Dept Comp Sci, Midnapore 721102, W Bengal, India.
   [Meikap, Sudipta] Hijli Coll, Dept Comp Sci, Rangamatia 721306, W Bengal, India.
   [Lu, Tzu-Chuen] Chaoyang Univ Technol, Dept Informat Management, Taichung 41349, Taiwan.
C3 Vidyasagar University; Chaoyang University of Technology
RP Meikap, S (corresponding author), Vidyasagar Univ, Dept Comp Sci, Midnapore 721102, W Bengal, India.; Meikap, S (corresponding author), Hijli Coll, Dept Comp Sci, Rangamatia 721306, W Bengal, India.
EM sudiptameikap@gmail.com; biswapatijana@gmail.com; tclu@cyut.edu.tw
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], The USC-SIPI Image Database
   Benseddik ML, 2022, MULTIMED TOOLS APPL, V81, P20329, DOI 10.1007/s11042-022-12288-2
   Caciula I, 2019, SIGNAL PROCESS-IMAGE, V71, P120, DOI 10.1016/j.image.2018.11.005
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang CC, 2015, INFORM SCIENCES, V300, P85, DOI 10.1016/j.ins.2014.12.028
   Chang J, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103097
   Di FQ, 2019, MULTIMED TOOLS APPL, V78, P7125, DOI 10.1007/s11042-018-6469-4
   Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097
   Fridrich J, 2002, PROC SPIE, V4675, P1, DOI 10.1117/12.465263
   Hong WE, 2011, J VIS COMMUN IMAGE R, V22, P131, DOI 10.1016/j.jvcir.2010.11.004
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Kaur G, 2022, VISUAL COMPUT, V38, P1027, DOI 10.1007/s00371-021-02066-w
   Kaur G, 2020, J INFORM OPTIM SCI, V41, P205, DOI 10.1080/02522667.2020.1714185
   Kim HJ, 2008, IEEE T INF FOREN SEC, V3, P456, DOI 10.1109/TIFS.2008.924600
   Li S, 2020, IEEE ACCESS, V8, P214732, DOI 10.1109/ACCESS.2020.3040048
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Li XL, 2013, SIGNAL PROCESS, V93, P198, DOI 10.1016/j.sigpro.2012.07.025
   Li XL, 2011, IEEE T IMAGE PROCESS, V20, P3524, DOI 10.1109/TIP.2011.2150233
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Meikap Sudipta, 2017, Communication, Devices, and Computing. ICCDC 2017. Proceedings: LNEE 470, P47, DOI 10.1007/978-981-10-8585-7_5
   Meikap S, 2021, MULTIMED TOOLS APPL, V80, P5617, DOI 10.1007/s11042-020-09823-4
   Meikap S, 2019, SN APPL SCI, V1, DOI 10.1007/s42452-019-0659-1
   Meikap S, 2018, MULTIMED TOOLS APPL, V77, P31281, DOI 10.1007/s11042-018-6203-2
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Nikolaidis N, 1998, SIGNAL PROCESS, V66, P385, DOI 10.1016/S0165-1684(98)00017-6
   Nottingham Trent University, UCID IM DAT
   /openi.nlm.nih.gov, NAT LIB MED PRES MED
   Ou B, 2014, SIGNAL PROCESS-IMAGE, V29, P760, DOI 10.1016/j.image.2014.05.003
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Pan ZB, 2015, J VIS COMMUN IMAGE R, V31, P64, DOI 10.1016/j.jvcir.2015.05.005
   Peng F, 2014, DIGIT SIGNAL PROCESS, V25, P255, DOI 10.1016/j.dsp.2013.11.002
   Qu X, 2015, SIGNAL PROCESS, V111, P249, DOI 10.1016/j.sigpro.2015.01.002
   r0k.us, Kodak Lossless True Color Image Suite
   RoselinKiruba R, 2023, VISUAL COMPUT, V39, P59, DOI 10.1007/s00371-021-02312-1
   Singh L, 2020, MULTIMED TOOLS APPL, V79, P15901, DOI 10.1007/s11042-018-6407-5
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Sudipta Meikap, 2021, Proceedings of International Conference on Frontiers in Computing and Systems. COMSYS 2020. Advances in Intelligent Systems and Computing (AISC 1255), P659, DOI 10.1007/978-981-15-7834-2_61
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tsai P, 2009, SIGNAL PROCESS, V89, P1129, DOI 10.1016/j.sigpro.2008.12.017
   Tu SC, 2010, VISUAL COMPUT, V26, P1177, DOI 10.1007/s00371-009-0398-1
   University of California Berkeley, BERK SEGM DAT BENCHM
   Verma VS, 2015, EXPERT SYST APPL, V42, P8184, DOI 10.1016/j.eswa.2015.06.041
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Wang XY, 2019, INFORM SCIENCES, V503, P274, DOI 10.1016/j.ins.2019.06.059
   Wang XT, 2013, DIGIT SIGNAL PROCESS, V23, P569, DOI 10.1016/j.dsp.2012.06.015
   Wu HR, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107264
   Yao H, 2021, INFORM SCIENCES, V563, P130, DOI 10.1016/j.ins.2021.02.015
   Yu CY, 2021, ACTA HORTIC, V1313, P1, DOI [10.17660/ActaHortic.2021.1313.1, 10.1109/TCSVT.2021.3062947]
   Yu CQ, 2022, SIGNAL PROCESS, V196, DOI 10.1016/j.sigpro.2022.108527
   Yu CQ, 2022, INFORM SCIENCES, V584, P89, DOI 10.1016/j.ins.2021.10.050
NR 52
TC 1
Z9 1
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 AUG 31
PY 2023
AR s00371-023-03050-2
DI 10.1007/s00371-023-03050-2
EA AUG 2023
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R2FT1
UT WOS:001062559600002
DA 2024-07-18
ER

PT J
AU Hu, K
   Huang, ZYF
   Wang, XC
   Wang, XJ
AF Hu, Kun
   Huang, Zhaoyangfan
   Wang, Xiaochao
   Wang, Xingjun
TI StegaEdge: learning edge-guidance steganography
SO VISUAL COMPUTER
LA English
DT Article
DE Image Steganography; Deep Neural Network; High Imperceptibility
ID IMAGE STEGANOGRAPHY; WATERMARKING; CAPACITY
AB Steganography is critical in traceability, authentication, and secret delivery for multimedia. In this paper, we propose a novel image steganography framework, named StegaEdge, via learning edge-guidance network to simultaneously address three challenges, capacity, multi-task, and invisibility. First, we use an upsampling strategy to expand the embedding space and thus increase the capacity of the embedded message. Second, our algorithm improves the embedding way of messages so that it can handle different messages embedded in the same image and achieve split-task recovery completely. Different information can be embedded in one cover image without affecting each other. Third, we innovatively propose an edge-guidance strategy to solve the problem of poor invisibility in smooth regions. The human eye is significantly less perceptive of intensity changes in edges than in smooth areas. Unlike traditional steganography methods, our edge-guidance steganography can appropriately embed part of the information into non-edge regions when the amount of embedded information is too large. Experimental results on three datasets show that the newly proposed StegaEdge algorithm achieves satisfactory results in terms of capacity, multi-task, imperceptibility, and security compared to the state-of-the-art algorithms.
C1 [Hu, Kun; Huang, Zhaoyangfan; Wang, Xingjun] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Wang, Xiaochao] Tiangong Univ, Sch Math Sci, Tianjin 300387, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Tiangong University
RP Wang, XJ (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM thu_hukun@163.com; hcyf20@mails.tsinghua.edu.cn; wangxiaochao18@163.com;
   xingjunwang_drm@163.com
FU Science, Technology and Innovation Commission of Shenzhen Municipality
   [WDZC20200818121348001, KCXFZ202002011010487]
FX AcknowledgementsThis work was supported in part by the Science,
   Technology and Innovation Commission of Shenzhen Municipality under
   Grant WDZC20200818121348001 and Grant KCXFZ202002011010487.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Al-Afandy KA, 2016, COLLOQ INF SCI TECH, P400, DOI 10.1109/CIST.2016.7805079
   Al-Dmour H, 2016, EXPERT SYST APPL, V46, P293, DOI 10.1016/j.eswa.2015.10.024
   Baluja S., 2017, ADV NEURAL INF PROCE, V30, P589
   Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877
   Barni M, 1999, PROC SPIE, V3657, P437, DOI 10.1117/12.344694
   Boehm B., 2014, ARXIV
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Bose R. C., 1960, Inf. Control, V3, P79, DOI DOI 10.1016/S0019-9958(60)90287-4
   Chan CK, 2004, PATTERN RECOGN, V37, P469, DOI 10.1016/j.patcog.2003.08.007
   Cox IJ, 1997, P SOC PHOTO-OPT INS, V3016, P92, DOI 10.1117/12.274502
   Cui JB, 2021, IEEE T IMAGE PROCESS, V30, P8567, DOI 10.1109/TIP.2021.3107999
   Fang H., 2022, IEEE T MULTIMEDIA, V5, P214
   Gaurav K, 2018, J INF SECUR APPL, V41, P41, DOI 10.1016/j.jisa.2018.05.001
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hu K, 2022, VISUAL COMPUT, V38, P2153, DOI 10.1007/s00371-021-02275-3
   Hu K, 2021, VISUAL COMPUT, V37, P2841, DOI 10.1007/s00371-021-02168-5
   Huang C.H., 2022, ARXIV
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Islam S, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-8
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Kumar V, 2018, MULTIMED TOOLS APPL, V77, P13279, DOI 10.1007/s11042-017-4947-8
   Li XL, 2013, IEEE T INF FOREN SEC, V8, P1091, DOI 10.1109/TIFS.2013.2261062
   Li YF, 2021, IET IMAGE PROCESS, V15, P3332, DOI 10.1049/ipr2.12329
   Lizhi Xiong, 2018, Multidimensional Systems and Signal Processing, V29, P1191, DOI 10.1007/s11045-017-0497-5
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P201, DOI 10.1109/TIFS.2010.2041812
   Mstafa RJ, 2017, IEEE ACCESS, V5, P5354, DOI 10.1109/ACCESS.2017.2691581
   Patel H., 2012, International Journal of Engineering Research and Applications, V2, P713
   Rabie T, 2016, MULTIMED TOOLS APPL, V75, P5939, DOI 10.1007/s11042-015-2557-x
   Ray B, 2021, MULTIMED TOOLS APPL, V80, P33475, DOI 10.1007/s11042-021-11177-4
   Roy R, 2013, PROC TECH, V10, P138, DOI 10.1016/j.protcy.2013.12.346
   Ruder S., 2016, ARXIV
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sivaraman R, 2020, IET IMAGE PROCESS, V14, P2987, DOI 10.1049/iet-ipr.2019.0168
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Voloshynovskiy S, 2000, LECT NOTES COMPUT SC, V1768, P211
   Voloshynovskiy S, 2001, SIGNAL PROCESS, V81, P1177, DOI 10.1016/S0165-1684(01)00039-1
   Wang XC, 2020, VISUAL COMPUT, V36, P2201, DOI 10.1007/s00371-020-01909-2
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Wu DC, 2003, PATTERN RECOGN LETT, V24, P1613, DOI 10.1016/S0167-8655(02)00402-6
   Xu H., 2021, ARXIV
   Yaghmaee F, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/851920
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhang F, 2007, PATTERN RECOGN LETT, V28, P1, DOI 10.1016/j.patrec.2006.04.020
   Zhang J, 2020, AAAI CONF ARTIF INTE, V34, P12805
   Zhang Kevin, 2019, ARXIV
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 52
TC 1
Z9 1
U1 9
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3319
EP 3331
DI 10.1007/s00371-023-02974-z
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001035766800006
OA Bronze
DA 2024-07-18
ER

PT J
AU Hu, XR
   Zheng, C
   Huang, JJ
   Luo, RQ
   Liu, JP
   Peng, T
AF Hu, Xinrong
   Zheng, Cheng
   Huang, Junjie
   Luo, Ruiqi
   Liu, Junping
   Peng, Tao
TI Cloth texture preserving image-based 3D virtual try-on
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual try-on; 3D reconstruction; Multi-resolution parallelism; Depth
   estimation; Generative adversarial network
ID SHAPE
AB 3D virtual try-on based on a single image can provide an excellent shopping experience for Internet users and has enormous business potential. The existing methods of processing the clothed 3D human body generated from the virtual try-on images are reconstructed in 3D models by extracting the depth information from the input images. However, the generated results are unstable and often fail to capture the high-frequency information loss detail features in the larger spatial background during the process of downsampling for depth prediction, and the loss of the generator gradient when predicting the occluded areas in the high-resolution images. To address this problem, we propose a multi-resolution parallel approach to obtain low-frequency information and retain as much of the high-frequency depth features in the images during depth prediction; at the same time, we use a multi-scale generator and discriminator to more accurately infer the feature images of the occluded regions to generate a fine-grained dressed 3D human body. Our method not only provides better details and effects to the final 3D mannequin generation for 3D virtual fitting, but also significantly improves the user's try-on experience than previous studies, as evidenced by our higher quantitative and qualitative evaluations.
C1 [Hu, Xinrong; Zheng, Cheng; Huang, Junjie; Luo, Ruiqi; Liu, Junping; Peng, Tao] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
   [Hu, Xinrong] State Key Lab New Text Mat & Adv Proc Technol, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University
RP Huang, JJ (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
EM hxr@wtu.edu.cn; 13138952456@163.com; jjhuang@wtu.edu.cn;
   rqluo@wtu.edu.cn; jpliu@wtu.edu.cn; pt@wtu.edu.cn
RI Li, Yan/KFQ-9244-2024; Chen, Bowen/KFB-3986-2024; zhou,
   yuwei/KHD-4127-2024; liu, zhao/KGM-5884-2024; li, yan/KFQ-3850-2024;
   Huang, Junjie/U-1939-2018; Liu, Yan/KFQ-1417-2024
CR Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Boyi Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P18, DOI 10.1007/978-3-030-58565-5_2
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Corona E, 2021, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR46437.2021.01170
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He Tong, 2021, COMPUTER VISION, P11046
   Heming Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P512, DOI 10.1007/978-3-030-58452-8_30
   Hensel M, 2017, ADV NEUR IN, V30
   Hong F., 2021, Adv. Neural Inf. Process. Syst., V33, P27940
   Jafarian Y, 2021, PROC CVPR IEEE, P12748, DOI 10.1109/CVPR46437.2021.01256
   Jiahui Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P121, DOI 10.1007/978-3-030-58523-5_8
   Jiang Yuheng, 2022, P IEEE CVF C COMP VI, P6155
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Lizhen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P430, DOI 10.1007/978-3-030-58565-5_26
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma QL, 2022, Arxiv, DOI arXiv:2209.06814
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Peng SD, 2021, Arxiv, DOI arXiv:2105.02872
   PETROV AP, 1993, COLOR RES APPL, V18, P375, DOI 10.1002/col.5080180605
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Shao RZ, 2022, LECT NOTES COMPUT SC, V13692, P702, DOI 10.1007/978-3-031-19824-3_41
   Sterzentsenko V, 2019, IEEE I CONF COMP VIS, P1242, DOI 10.1109/ICCV.2019.00133
   Te GS, 2022, LECT NOTES COMPUT SC, V13666, P275, DOI 10.1007/978-3-031-20068-7_16
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang SF, 2022, LECT NOTES COMPUT SC, V13692, P1, DOI 10.1007/978-3-031-19824-3_1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng Chung-Yi, 2022, P IEEE CVF C COMP VI, P16210
   Xiu YL, 2022, PROC CVPR IEEE, P13286, DOI 10.1109/CVPR52688.2022.01294
   Yan S, 2018, LECT NOTES COMPUT SC, V11214, P155, DOI 10.1007/978-3-030-01249-6_10
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhao F., 2021, P IEEE CVF INT C COM, P13239
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhi YH, 2022, Arxiv, DOI arXiv:2208.14851
NR 36
TC 5
Z9 5
U1 2
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3347
EP 3357
DI 10.1007/s00371-023-02999-4
EA JUL 2023
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001026397800002
OA Bronze
DA 2024-07-18
ER

PT J
AU Wu, CM
   Peng, SY
   Zhang, XL
AF Wu, Chengmao
   Peng, Siyun
   Zhang, Xialu
TI Robust interval type-2 kernel-based possibilistic fuzzy deep local
   information clustering driven by Lambert-W function
SO VISUAL COMPUTER
LA English
DT Article
DE Image segmentation; Fuzzy clustering; Possibilistic clustering; Type-2
   fuzzy set; Local information; Non-local information; Kernel metric; Deep
   neighborhood window
ID IMAGE SEGMENTATION; ALGORITHMS
AB Interval type-2 fuzzy sets not only have stronger ability to deal with uncertainty, but also have low computational complexity than general type-2 fuzzy set, so they are widely used in fuzzy clustering methods. However, most existing interval type-2 fuzzy clustering methods are still sensitive to noise and lack a certain degree of robustness in segmenting images with noise. Therefore, this paper proposes a novel interval type-2 enhanced kernel possibilistic fuzzy local and non-local information c-means clustering method for segmenting images with high noise. Interval type-2 possibilistic fuzzy clustering with Lambert-W function is first extended to obtain a novel interval type-2 enhanced possibilistic fuzzy clustering with product partition. Then deep local neighborhood information including local and non-local information is used to constrain interval type-2 enhanced possibilistic fuzzy product partition c-means clustering, and a robust interval type-2 enhanced possibilistic fuzzy deep local information clustering with kernel metric is proposed. Experimental results demonstrate that the proposed algorithm significantly outperforms the latest fuzzy clustering-related algorithms in the presence of high noise.
C1 [Wu, Chengmao; Peng, Siyun; Zhang, Xialu] Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Peng, SY (corresponding author), Xian Univ Posts & Telecommun, Sch Elect Engn, Xian 710121, Peoples R China.
EM wuchengmao123@sohu.com; pengsiyun97@163.com; zhangxialu0708@163.com
OI peng, siyun/0000-0002-6562-8662; Wu, Chengmao/0000-0002-5881-4723
FU National Natural Science Foundation of China [62071378]; Shaanxi Natural
   Science Foundation of China [2022JM-370]
FX & nbsp;This work was supported by the National Natural Science
   Foundation of China (62071378), and the Shaanxi Natural Science
   Foundation of China (2022JM-370). The authors would like to thank the
   anonymous reviewers for their constructive suggestions to improve the
   overall quality of the paper. Besides, the authors would like to thank
   the School of Electronic Engineering, Xi'an University of Posts &
   Telecommunications, Xi'an, China for financially support.
CR Alruwaili M, 2020, EGYPT INFORM J, V21, P51, DOI 10.1016/j.eij.2019.10.005
   Barni M, 2012, SIGNAL PROCESS-IMAGE, V27, P998, DOI 10.1016/j.image.2012.07.006
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Cai WL, 2007, PATTERN RECOGN, V40, P825, DOI 10.1016/j.patcog.2006.07.011
   Cai YM, 2021, IEEE T GEOSCI REMOTE, V59, P4191, DOI 10.1109/TGRS.2020.3018135
   Cheng HH, 2019, INT J APPROX REASON, V106, P89, DOI 10.1016/j.ijar.2018.12.010
   Chui KT, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10134571
   cocodataset, About us
   Darányi A, 2022, J MANUF SYST, V63, P15, DOI 10.1016/j.jmsy.2022.02.010
   eecs.berkeley, About us
   escience, US
   Fan T, 2022, J APPL GEOPHYS, V197, DOI 10.1016/j.jappgeo.2022.104537
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Gong MG, 2012, IEEE T IMAGE PROCESS, V21, P2141, DOI 10.1109/TIP.2011.2170702
   Hu FK, 2020, IEEE ACCESS, V8, P4500, DOI 10.1109/ACCESS.2019.2963444
   Hwang C, 2007, IEEE T FUZZY SYST, V15, P107, DOI 10.1109/TFUZZ.2006.889763
   Ji ZX, 2014, FUZZY SET SYST, V253, P138, DOI 10.1016/j.fss.2013.12.011
   kaggle, US
   Karnik NN, 2001, INFORM SCIENCES, V132, P195, DOI 10.1016/S0020-0255(01)00069-X
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Krishnapuram R., 1993, IEEE Transactions on Fuzzy Systems, V1, P98, DOI 10.1109/91.227387
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Li HF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3147513
   Li YM, 2023, IEEE T MULTIMEDIA, V25, P6852, DOI 10.1109/TMM.2022.3215000
   Liao X, 2021, INFORM SCIENCES, V575, P231, DOI 10.1016/j.ins.2021.06.045
   Lv GH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3093502
   Maji P, 2021, IEEE T CYBERNETICS, V51, P3641, DOI 10.1109/TCYB.2019.2925130
   medpix.nlm.nih, US
   Memon KH, 2019, INT J FUZZY SYST, V21, P321, DOI 10.1007/s40815-018-0537-9
   Mendel JM, 2004, IEEE T FUZZY SYST, V12, P84, DOI 10.1109/TFUZZ.2003.822681
   Mittal H, 2021, IEEE T FUZZY SYST, V29, P3249, DOI 10.1109/TFUZZ.2020.3016339
   Nawaz M, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113654
   Nguyen R, 2023, NEUROCOMPUTING, V525, P88, DOI 10.1016/j.neucom.2023.01.015
   Pal NR, 2005, IEEE T FUZZY SYST, V13, P517, DOI 10.1109/TFUZZ.2004.840099
   Ruiz-García G, 2019, IEEE T FUZZY SYST, V27, P2381, DOI 10.1109/TFUZZ.2019.2898582
   Shen LY, 2021, IEEE T MED IMAGING, V40, P1113, DOI 10.1109/TMI.2020.3046444
   Shen YH, 2020, IEEE T CYBERNETICS, V50, P4722, DOI 10.1109/TCYB.2018.2886725
   Shirkhorshidi AS, 2021, IEEE T FUZZY SYST, V29, P560, DOI 10.1109/TFUZZ.2019.2956900
   Sotudian S., ARXIV
   Sushir RD, 2022, MULTIMED TOOLS APPL, V81, P29177, DOI 10.1007/s11042-022-12923-y
   Szilágyi L, 2011, LECT NOTES ARTIF INT, V6820, P150, DOI 10.1007/978-3-642-22589-5_15
   Tan JX, 2022, IEEE T NETW SCI ENG, V9, P888, DOI 10.1109/TNSE.2021.3139671
   Tang YM, 2020, APPL SOFT COMPUT, V87, DOI 10.1016/j.asoc.2019.105928
   Dang TH, 2019, ENG APPL ARTIF INTEL, V85, P85, DOI 10.1016/j.engappai.2019.05.004
   Wang QS, 2021, APPL SOFT COMPUT, V105, DOI 10.1016/j.asoc.2021.107245
   Wang QS, 2020, APPL SOFT COMPUT, V92, DOI 10.1016/j.asoc.2020.106318
   Wu CM, 2022, INT J APPROX REASON, V148, P80, DOI 10.1016/j.ijar.2022.05.007
   Wu CM, 2021, INT J APPROX REASON, V136, P281, DOI 10.1016/j.ijar.2021.06.004
   Yongchen Zhou, 2020, 2020 IEEE 3rd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE), P299, DOI 10.1109/AUTEEE50969.2020.9315614
   Yuan YD, 2019, IEEE J BIOMED HEALTH, V23, P519, DOI 10.1109/JBHI.2017.2787487
   ZADEH LA, 1975, INFORM SCIENCES, V8, P199, DOI 10.1016/0020-0255(75)90046-8
   Zare Alina., 2017, IEEE Transactions on Pattern Analysis Machine Intelligence, V1, P1, DOI 10.1109/SSCI.2017.8285358
   [张阿龙 Zhang Along], 2018, [测绘科学技术学报, Journal of Geomatics Science and Technology], V35, P376
   Zhang JW, 2022, PATTERN RECOGN LETT, V153, P159, DOI 10.1016/j.patrec.2021.12.001
   Zhang XF, 2021, INFORM SCIENCES, V550, P129, DOI 10.1016/j.ins.2020.10.039
NR 55
TC 0
Z9 0
U1 2
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2161
EP 2201
DI 10.1007/s00371-023-02910-1
EA JUN 2023
PG 41
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001014470900002
DA 2024-07-18
ER

PT J
AU Wang, J
   Li, SX
   Li, KL
   Zhu, QZ
AF Wang, Jun
   Li, Sixuan
   Li, Kunlun
   Zhu, Qizhen
TI Adaptive cascaded and parallel feature fusion for visual object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual object tracking; Correlation filter; Feature fusion; ALW;
   Adaptive update
AB Due to its quick tracking, simple deployment, and straightforward principle, correlation filter-based tracking methods continue to have significant research implications. In order to make full use of different features while balancing the tracking speed and performance, the adaptive cascaded and parallel feature fusion-based tracker (ACPF), which could estimate the position, rotation and scale, respectively, is proposed. Comparing with other correlation filter-based trackers, the ACPF could fuse deep and handcrafted features in both Log-Polar and Cartesian branch and update templates according to the weights of response maps adaptively. Adaptive linear weights (ALW) are proposed to fuse feature response maps adaptively in the Log-Polar coordinates branch to improve the estimation of scale and rotation of object by solving constrained optimization problems. Response maps of shallow and deep features are merged adaptively by cascading numerous ALW modules in the Cartesian branch to make better utilize shallow and deep feature, and increase tracking accuracy. The final results are computed simultaneously by Cartesian and Log-Polar branch in parallel. Additionally, the learning rates are automatically changed in accordance with the weights of the ALW module to execute the adaptive template update. Extensive experiments on benchmarks show that the proposed tracker achieves the comparable results, especially in dealing with the challenges of deformation, rotation and scale variation.
C1 [Wang, Jun; Li, Sixuan; Li, Kunlun; Zhu, Qizhen] Hebei Univ, 180 Wusi Rd, Baoding 071000, Hebei, Peoples R China.
C3 Hebei University
RP Li, KL (corresponding author), Hebei Univ, 180 Wusi Rd, Baoding 071000, Hebei, Peoples R China.
EM junwanghbu@hbu.edu.cn; lsx990102@163.com; likunlun@hbu.edu.cn;
   zqzhbu@163.com
RI Wang, Jun/HGA-9618-2022; li, sixuan/KGR-3943-2024
OI Wang, Jun/0000-0002-5901-9019; 
FU Natural Science Foundation of Hebei Province [F2022201013]; Startup
   Foundation for Advanced Talents of Hebei University [521100221003]
FX AcknowledgementsThis research is partially supported by Natural Science
   Foundation of Hebei Province (F2022201013) and Startup Foundation for
   Advanced Talents of Hebei University (No. 521100221003).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P993, DOI 10.1007/s00371-020-01848-y
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Bouraffa T, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104468
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Dunnhofer M, 2022, IMAGE VISION COMPUT, V122, DOI 10.1016/j.imavis.2022.104448
   Fan CX, 2022, VISUAL COMPUT, V38, P4291, DOI 10.1007/s00371-021-02296-y
   Gundogdu E, 2018, IEEE T IMAGE PROCESS, V27, P2526, DOI 10.1109/TIP.2018.2806280
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang YP, 2022, VISUAL COMPUT, V38, P1495, DOI 10.1007/s00371-021-02083-9
   Kristan M., 2022, EUR C COMP VIS, P431
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Qian Q, 2021, VISUAL COMPUT, V37, P1029, DOI 10.1007/s00371-020-01850-4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang Q., 2017, arXiv
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yan B, 2021, PROC CVPR IEEE, P15175, DOI 10.1109/CVPR46437.2021.01493
   Yang SD, 2022, VISUAL COMPUT, V38, P2107, DOI 10.1007/s00371-021-02271-7
   Zhang JM, 2021, J AMB INTEL HUM COMP, V12, P8427, DOI 10.1007/s12652-020-02572-0
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang JQ, 2021, VISUAL COMPUT, V37, P2671, DOI 10.1007/s00371-021-02237-9
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
NR 45
TC 0
Z9 0
U1 5
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 2119
EP 2138
DI 10.1007/s00371-023-02908-9
EA JUN 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:001003038200001
DA 2024-07-18
ER

PT J
AU Barbato, F
   Michieli, U
   Toldo, M
   Zanuttigh, P
AF Barbato, Francesco
   Michieli, Umberto
   Toldo, Marco
   Zanuttigh, Pietro
TI Road scenes segmentation across different domains by disentangling
   latent representations
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Domain adaptation; Latent space shaping;
   Representation learning
AB Deep learning models obtain impressive accuracy in road scene understanding; however, they need a large number of labeled samples for their training. Additionally, such models do not generalize well to environments where the statistical properties of data do not perfectly match those of training scenes, and this can be a significant problem for intelligent vehicles. Hence, domain adaptation approaches have been introduced to transfer knowledge acquired on a label-abundant source domain to a related label-scarce target domain. In this work, we design and carefully analyze multiple latent space-shaping regularization strategies that work together to reduce the domain shift. More in detail, we devise a feature clustering strategy to increase domain alignment, a feature perpendicularity constraint to space apart features belonging to different semantic classes, including those not present in the current batch, and a feature norm alignment strategy to separate active and inactive channels. In addition, we propose a novel evaluation metric to capture the relative performance of an adapted model with respect to supervised training. We validate our framework in driving scenarios, considering both synthetic-to-real and realto-real adaptation, outperforming previous feature-level state-of-the-art methods on multiple road scenes benchmarks.
C1 [Barbato, Francesco; Michieli, Umberto; Toldo, Marco; Zanuttigh, Pietro] Univ Padua, Dept Informat Engn, Via Gradenigo 6B, I-35131 Padua, Italy.
C3 University of Padua
RP Barbato, F (corresponding author), Univ Padua, Dept Informat Engn, Via Gradenigo 6B, I-35131 Padua, Italy.
EM francesco.barbato@dei.unipd.it; umberto.michieli@dei.unipd.it;
   marco.toldo.3@phd.unipd.it; zanuttigh@dei.unipd.it
RI Barbato, Francesco/AAD-1361-2022; Michieli, Umberto/KLC-7487-2024
OI Barbato, Francesco/0000-0001-9893-5813; Michieli,
   Umberto/0000-0003-2666-4342
FU Universita degli Studi di Padova within the CRUI-CARE Agreement;
   University of Padova SID2020 project "Semantic Segmentation in the Wild"
   and by the Italian Ministry for Education (MIUR) under the "Departments
   of Excellence" initialive [Law 232/2016]
FX Open access funding provided by Universita degli Studi di Padova within
   the CRUI-CARE Agreement. This work has also been partially funded by the
   University of Padova SID2020 project "Semantic Segmentation in the Wild"
   and by the Italian Ministry for Education (MIUR) under the "Departments
   of Excellence" initialive (Law 232/2016).
CR Barbato, 2022, ARXIV
   Barbato F, 2021, IEEE COMPUT SOC CONF, P2829, DOI 10.1109/CVPRW53098.2021.00318
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Biasetton M, 2019, IEEE COMPUT SOC CONF, P1211, DOI 10.1109/CVPRW.2019.00160
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Choi H., 2020, ARXIV
   Congcong Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P481, DOI 10.1007/978-3-030-58601-0_29
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding Y, 2022, VISUAL COMPUT, V38, P1871, DOI 10.1007/s00371-021-02246-8
   Dong N., 2018, P BRIT MACH VIS C, V3
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du L, 2019, IEEE I CONF COMP VIS, P982, DOI 10.1109/ICCV.2019.00107
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks Dan, 2019, ARXIV190312261
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffman Judy, 2016, arXiv
   Jiqing C., 2022, VISUAL COMPUT, V38, P1
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Khindkar Vaishnavi, 2022, P IEEE CVF WINT C AP, P3632
   Kunert C., 2022, 2021 INT C CYBERWORL, V23, P1
   Lee S, 2019, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2019.00018
   Liang J, 2019, PROC CVPR IEEE, P2970, DOI 10.1109/CVPR.2019.00309
   Lin ZK, 2023, VISUAL COMPUT, V39, P597, DOI 10.1007/s00371-021-02360-7
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Michieli U, 2023, IEEE INTERNET THINGS, V10, P1517, DOI 10.1109/JIOT.2022.3209865
   Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117
   Michieli U, 2020, IEEE T INTELL VEHICL, V5, P508, DOI 10.1109/TIV.2020.2980671
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Park S, 2018, AAAI CONF ARTIF INTE, P3917
   Pinheiro PO, 2018, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2018.00835
   Pizzati F, 2020, IEEE WINT CONF APPL, P2979, DOI 10.1109/WACV45572.2020.9093540
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Rizzoli G, 2022, TECHNOLOGIES, V10, DOI 10.3390/technologies10040090
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ros G., 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, P3234, DOI DOI 10.1109/CVPR.2016.352
   Saito K., 2018, P INT C LEARNING REP
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Spadotto T., 2020, INT C PATT RECOG
   Sun T., 2022, P IEEE CVF C COMP VI, P21371, DOI [10.48550/arXiv.2206.08367, DOI 10.48550/ARXIV.2206.08367]
   Testolina P., 2022, ARXIV
   Tian L., 2020, ARXIV
   Toldo, 2022, ARXIV
   Toldo M, 2021, IEEE WINT CONF APPL, P1357, DOI 10.1109/WACV48630.2021.00140
   Toldo M, 2020, TECHNOLOGIES, V8, DOI 10.3390/technologies8020035
   Toldo M, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103889
   Tranheden W, 2021, IEEE WINT CONF APPL, P1378, DOI 10.1109/WACV48630.2021.00142
   Truong T.-D., 2021, IEEE INT C COMPUTER, P8548
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varma G, 2019, IEEE WINT CONF APPL, P1743, DOI 10.1109/WACV.2019.00190
   Wang H, 2022, IEEE T INTELL TRANSP, V23, P21405, DOI 10.1109/TITS.2022.3177615
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang K, 2022, VISUAL COMPUT, V38, P2329, DOI 10.1007/s00371-021-02115-4
   Wang Q, 2020, AAAI CONF ARTIF INTE, V34, P6243
   Wu S, 2019, AAAI CONF ARTIF INTE, P5450
   Xie EZ, 2021, ADV NEUR IN, V34
   Yayun Lei, 2020, 2020 IEEE International Conference on Mechatronics and Automation (ICMA), P1094, DOI 10.1109/ICMA49215.2020.9233538
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou Q., 2021, ARXIV
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 76
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 811
EP 830
DI 10.1007/s00371-023-02818-w
EA APR 2023
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000969747600002
OA hybrid, Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Chiang, TH
   Lin, YT
   Lin, JCH
   Tseng, YC
AF Chiang, Ting-Hui
   Lin, Yun-Tang
   Lin, Jaden Chao-Ho
   Tseng, Yu-Chee
TI Trapezoid-structured LSTM with segregated gates and bridge joints for
   video frame inpainting
SO VISUAL COMPUTER
LA English
DT Article
DE Bridge joints; Multi-kernel LSTM; Segregated spatial-temporal gates;
   Trapezoid-structured LSTM; Video frame inpainting
AB This work considers the video frame inpainting problem, where several former and latter frames are given, and the goal is to predict the middle frames. The state-of-the-art solution has applied bidirectional long short-term memory (LSTM) networks, which has a spatial-temporal mismatch problem. In this paper, we propose a trapezoid-structured LSTM architecture called T-LSTM-sbm for video frame inpainting with three designs: (i) segregated spatial-temporal gates, (ii) bridge joints, and (iii) multi-kernel LSTM. To prevent the spatial-temporal mismatch problem, while features are being passed through multilayered LSTM nodes, the trapezoid structure reduces its number of LSTM nodes by two after each layer. This makes the model converge to the inpainted results more effectively. The separated temporal and spatial gates design can learn better spatial and temporal features by using individual gates. To relieve the information loss problem during the convergence of the trapezoidal layers, we use bridge joints among layers to better preserve useful information. The multiple kernels in LSTM are to enable extracting multi-scale information flows. T-LSTM-sbm is proved to outperform the state-of-the-art solutions in peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) on three common datasets, KTH Action, HMDB-51, and UCF-101.
C1 [Chiang, Ting-Hui] Chunghwa Telecom Labs, Adv Technol Lab, Taoyuan, Taiwan.
   [Lin, Yun-Tang; Lin, Jaden Chao-Ho] Natl Yang Ming Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Tseng, Yu-Chee] Natl Yang Ming Chiao Tung Univ, Coll Artificial Intelligence, Tainan, Taiwan.
C3 Chunghwa Telecom; National Yang Ming Chiao Tung University; National
   Yang Ming Chiao Tung University
RP Chiang, TH (corresponding author), Chunghwa Telecom Labs, Adv Technol Lab, Taoyuan, Taiwan.
EM thchiang@cht.com.tw
OI Chiang, Ting-Hui/0000-0002-0886-2479
FU Industrial Technology Research Institute (ITRI); National Science and
   Technology Council; Ministry of Science and Technology (MOST); "Center
   for Open Intelligent Connectivity" of "Higher Education Sprout Project"
   of National Yang Ming Chiao Tung University (NYCU) andMinistry of
   Education (MOE), Taiwan
FX Y.-C. Tseng's research is co-sponsored by Industrial Technology Research
   Institute (ITRI), National Science and Technology Council, and Ministry
   of Science and Technology (MOST). This work is also financially
   supported by "Center for Open Intelligent Connectivity" of "Higher
   Education Sprout Project" of National Yang Ming Chiao Tung University
   (NYCU) andMinistry of Education (MOE), Taiwan.
CR Agethen S, 2020, IEEE T MULTIMEDIA, V22, P819, DOI 10.1109/TMM.2019.2932564
   Ahn HE, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050619
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bengio S, 2015, ADV NEUR IN, V28
   Borzì A, 2002, SIAM J SCI COMPUT, V24, P818, DOI 10.1137/S1064827501386481
   Chen KL, 2012, IEEE T IMAGE PROCESS, V21, P1020, DOI 10.1109/TIP.2011.2179305
   Cheung SCS, 2006, IEEE IMAGE PROC, P705, DOI 10.1109/ICIP.2006.312432
   Chiang TH, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108807
   Chiu S.-Y., 2023, P IEEECVF WINTER C A
   Cox D., 2017, INT C LEARNING REPRE
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kingma D. P., 2014, arXiv
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kumar M., 2020, ICLR
   Li JY, 2023, IEEE INTERNET THINGS, V10, P4698, DOI 10.1109/JIOT.2022.3219163
   Li Z, 2022, PROC CVPR IEEE, P17541, DOI 10.1109/CVPR52688.2022.01704
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu BW, 2021, PROC CVPR IEEE, P701, DOI 10.1109/CVPR46437.2021.00076
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Mathieu M., 2015, PROC INT C LEARN REP
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Ouyang H., 2021, P IEEE CVF INT C COM, P14579
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi ZH, 2022, PROC CVPR IEEE, P17461, DOI 10.1109/CVPR52688.2022.01696
   Sim Hyeonjun, 2021, P IEEECVF INT C COMP, P14489
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K., 2012, ARXIV12120402CS
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Szeto R, 2020, IEEE T PATTERN ANAL, V42, P1053, DOI 10.1109/TPAMI.2019.2951667
   Van L., 2021, DISCOV INTERNET THIN, V1
   Villegas Ruben, 2017, ICLR
   Wang CQ, 2021, IEEE T CIRC SYST VID, V31, P2953, DOI 10.1109/TCSVT.2020.3034422
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang Y., 2018, INT C MACH LEARN, P5123, DOI 10.48550/arXiv.1804.06300
   Wang YB, 2017, ADV NEUR IN, V30
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Werlberger M., 2011, INT C EN MIN METH CO, P273
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037
   Wu Y., 2022, CVPR, P17814
   Wu Y, 2020, PROC CVPR IEEE, P5538, DOI 10.1109/CVPR42600.2020.00558
   Xiang XY, 2020, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR42600.2020.00343
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
   Zou XY, 2021, PROC CVPR IEEE, P16443, DOI 10.1109/CVPR46437.2021.01618
NR 51
TC 2
Z9 2
U1 3
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 1069
EP 1082
DI 10.1007/s00371-023-02832-y
EA APR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000967899600004
DA 2024-07-18
ER

PT J
AU Wang, L
   Sun, D
   Yuan, Z
   Gao, QW
   Lu, YX
AF Wang, Lin
   Sun, Dong
   Yuan, Zhu
   Gao, Qingwei
   Lu, Yixiang
TI Multi-view clustering based on graph learning and view diversity
   learning
SO VISUAL COMPUTER
LA English
DT Article
DE Graph learning; Multi-view clustering; View diversity learning; Graph
   fusion
ID LOW-RANK
AB Multi-view clustering is to make full use of different views of the data for clustering. In recent years, many multi-view clustering methods have been proposed. Previous methods usually do not consider the smooth representation of clusters and the diversity of different views simultaneously. Or they are limited to using the average of graphs as the input to the clustering algorithm, ignoring the possible impact of noisy views. Therefore, we propose a multi-view clustering method based on graph learning and view diversity learning. Specifically, in view self-expression learning, manifold learning is added to mine the structural graph information of data and control the geometry of data distribution. After that, view diversity is added, which is used to explore the complementary information of multi-view data and reduce the redundant information of the data. In addition, we also apply an automatic weighting strategy to distinguish different contributions from different views, and generate a consensus graph from multiple similarity graphs. Our work combines graph learning, view diversity learning, and graph fusion in a unified framework for the first time. An alternating iterative method is used to optimize the solution. Experimental results on six data sets show that our model has good clustering performance on different evaluation indicators.
C1 [Wang, Lin; Sun, Dong; Yuan, Zhu; Gao, Qingwei; Lu, Yixiang] Anhui Univ, Sch Elect Engn & Automat, Hefei, Peoples R China.
C3 Anhui University
RP Sun, D (corresponding author), Anhui Univ, Sch Elect Engn & Automat, Hefei, Peoples R China.
EM ahuwang1003@163.com; sundong@ahu.edu.cn
RI wang, lin/HTP-3337-2023
FU National Natural Science Foundation of China [62071001]; Anhui Natural
   Science Foundation of China [2008085MF192, 2008085MF183]; Key Science
   Project of Anhui Education Department of China [KJ2018A0012,
   KJ2019A0023, KJ2019A0022]; CERNET Innovation Project of China
   [NGII20180612, NGII20180312, NGII20180624]
FX This work is supported by the National Natural Science Foundation of
   China (No. 62071001), the Anhui Natural Science Foundation of China
   (Nos. 2008085MF192 and 2008085MF183), the Key Science Project of Anhui
   Education Department of China (Nos. KJ2018A0012, KJ2019A0023, and
   KJ2019A0022), and the CERNET Innovation Project of China (Nos.
   NGII20180612, NGII20180312, and NGII20180624).
CR Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024
   CAO XC, 2015, PROC CVPR IEEE, P586, DOI DOI 10.1109/CVPR.2015.7298657
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Dai WT, 2022, VISUAL COMPUT, V38, P1181, DOI 10.1007/s00371-021-02137-y
   Du GW, 2021, DATA SCI ENG, V6, P323, DOI 10.1007/s41019-021-00159-z
   Gao HH, 2024, IEEE T NEUR NET LEAR, V35, P4826, DOI 10.1109/TNNLS.2022.3155486
   Guo L, 2021, INFORM SCIENCES, V573, P453, DOI 10.1016/j.ins.2021.05.070
   Houthuys L, 2018, INFORM FUSION, V44, P46, DOI 10.1016/j.inffus.2017.12.002
   Huang SD, 2020, INFORM SCIENCES, V512, P18, DOI 10.1016/j.ins.2019.09.079
   Huang SD, 2018, NEUROCOMPUTING, V311, P197, DOI 10.1016/j.neucom.2018.05.072
   Huang Y, 2022, NAT PROD RES, V36, P1485, DOI 10.1080/14786419.2021.1893318
   Huang ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2563
   Kang Z, 2022, IEEE T CYBERNETICS, V52, P8976, DOI 10.1109/TCYB.2021.3061660
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kang Z, 2018, AAAI CONF ARTIF INTE, P3366
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Li J, 2021, VISUAL COMPUT, V37, P619, DOI 10.1007/s00371-020-01828-2
   Li RH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2916
   Li RH, 2019, IEEE I CONF COMP VIS, P8171, DOI 10.1109/ICCV.2019.00826
   Li XF, 2021, NEUROCOMPUTING, V424, P215, DOI 10.1016/j.neucom.2020.10.052
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Li YZ, 2021, APPL INTELL, V51, P1201, DOI 10.1007/s10489-020-01864-4
   Li Z., 2021, IEEE T MULTIMED, DOI [10.1016/j.knosys.2021.107632, DOI 10.1016/J.KNOSYS.2021.107632]
   Liang YW, 2019, IEEE DATA MINING, P1204, DOI 10.1109/ICDM.2019.00148
   Lin Z., 2021, IEEE T KNOWL DATA EN, DOI [10.24963/ijcai.2021/3750, DOI 10.24963/IJCAI.2021/3750]
   Liu H., 2018, ACM T KNOWL DISCOV D, V20, P17, DOI [10.7287/peerj-cs.922v0.2/reviews/2, DOI 10.7287/PEERJ-CS.922V0.2/REVIEWS/2]
   Liu XW, 2016, AAAI CONF ARTIF INTE, P1888
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Peng X, 2019, PR MACH LEARN RES, V97
   Qiu YH, 2021, VISUAL COMPUT, V37, P2253, DOI 10.1007/s00371-020-01984-5
   Tang C, 2022, IEEE T KNOWL DATA EN, V34, P4705, DOI 10.1109/TKDE.2020.3048678
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang H, 2019, KNOWL-BASED SYST, V163, P1009, DOI 10.1016/j.knosys.2018.10.022
   Wang H, 2016, IEEE DATA MINING, P1245, DOI [10.1109/ICDM.2016.34, 10.1109/ICDM.2016.0167]
   Wang S, 2022, VISUAL COMPUT, V38, P2539, DOI 10.1007/s00371-021-02129-y
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xia W, 2023, IEEE T PATTERN ANAL, V45, P5187, DOI 10.1109/TPAMI.2022.3187976
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yang ZY, 2019, IEEE T IMAGE PROCESS, V28, P5147, DOI 10.1109/TIP.2019.2913096
   Yang ZY, 2023, VISUAL COMPUT, V39, P1409, DOI 10.1007/s00371-022-02419-z
   Yao SX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4121
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang XT, 2016, IEEE T KNOWL DATA EN, V28, P3324, DOI 10.1109/TKDE.2016.2603983
   Zhang XY, 2018, INFORM SCIENCES, V432, P463, DOI 10.1016/j.ins.2017.11.038
   Zhang YL, 2019, KNOWL-BASED SYST, V163, P776, DOI 10.1016/j.knosys.2018.10.001
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhou SH, 2020, IEEE T NEUR NET LEAR, V31, P1351, DOI 10.1109/TNNLS.2019.2919900
   Zhu PF, 2018, PATTERN RECOGN, V84, P126, DOI 10.1016/j.patcog.2018.07.009
NR 57
TC 0
Z9 0
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6133
EP 6149
DI 10.1007/s00371-022-02717-6
EA NOV 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000883235000003
DA 2024-07-18
ER

PT J
AU Yuan, WQ
   Fu, CP
   Liu, RS
   Fan, X
AF Yuan, Wanqi
   Fu, Chenping
   Liu, Risheng
   Fan, Xin
TI SSoB: searching a scene-oriented architecture for underwater object
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Underwater scenes; Neural architecture search; Deep
   learning
AB Underwater object detection (UOD) suffers from low detection accuracy because of environmental degradations, such as haze-like effects, color distortions, and imaging noises. Therefore, we commit to resolving the issue of object detection with compounded environmental degradations that greatly challenges existing deep learning-based detectors. We propose a neural architecture search -based deep learning network to realize the UOD task, which can automatically discover the scene-oriented feature representation. Our network is accomplished through a unified macro-detector and a novel mixed anti-aliasing block (MAaB)-based search space. The macro-detector targets to learn intrinsic feature representations automatically from underwater images containing various environmental degradations and complete the subsequent detection tasks. The novel MAaB-based search space is proposed toward complex underwater scenes. The candidate operator MAaB has multiple kernels and anti-aliased convolutions in a single block for boosting the contextual representation capacity and the robustness of degraded factors. Finally, we use the differential search strategy guides the whole learning process to obtain the scene-friendly results. Extensive experiments demonstrate that our method outperforms the state-of-the-art detectors by a large margin. More importantly, in cases where environmental degradation is severely disturbed, our method is also superior to other popular detectors.
C1 [Yuan, Wanqi; Liu, Risheng; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Fu, Chenping] Dalian Univ Technol, Sch Software Technol, Dalian, Peoples R China.
   [Liu, Risheng; Fan, Xin] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
   [Liu, Risheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Peng
   Cheng Laboratory
RP Fan, X (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Fan, X (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Peoples R China.
EM xin.fan@dlut.edu.cn
CR [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2009, Construction and Analysis of a Large Scale Image Ontology
   Baojie Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P275, DOI 10.1007/978-3-030-58565-5_17
   Cai H, 2018, AAAI CONF ARTIF INTE, P2787
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YK, 2019, ADV NEUR IN, V32
   Du Xianzhi, 2020, IEEE CVF C COMP VIS
   Ge Z., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2107.08430
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang LH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4259, DOI 10.1145/3474085.3475563
   Jianyuan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11402, DOI 10.1109/CVPR42600.2020.01142
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li H., 2018, P BRIT MACH VIS C
   Liang PW, 2023, VISUAL COMPUT, V39, P1829, DOI 10.1007/s00371-022-02448-8
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin WH, 2020, INT CONF ACOUST SPEE, P2588, DOI [10.1109/icassp40776.2020.9053829, 10.1109/ICASSP40776.2020.9053829]
   Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2
   Liu CW, 2022, IEEE T CIRC SYST VID, V32, P2831, DOI 10.1109/TCSVT.2021.3100059
   Liu H, 2019, INT C LEARNING REPRE, DOI DOI 10.1109/CVPR42600.2020.00243
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YD, 2020, AAAI CONF ARTIF INTE, V34, P11653
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Ma L, 2020, IEEE SIGNAL PROC LET, V27, P1210, DOI 10.1109/LSP.2020.3008347
   Mhala NC, 2021, VISUAL COMPUT, V37, P2097, DOI 10.1007/s00371-020-01972-9
   Ning Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11940, DOI 10.1109/CVPR42600.2020.01196
   Pang Y, 2023, VISUAL COMPUT, V39, P1959, DOI 10.1007/s00371-022-02458-6
   Real E, 2019, AAAI CONF ARTIF INTE, P4780
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan M., 2019, ARXIV190709595
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675
   Xu Y, 2020, PLANT SOIL, V449, P133, DOI 10.1007/s11104-020-04435-1
   Xue C, 2019, PROC CVPR IEEE, P8994, DOI 10.1109/CVPR.2019.00921
   Yang ZH, 2020, PROC CVPR IEEE, P1826, DOI 10.1109/CVPR42600.2020.00190
   Zhang R, 2019, PR MACH LEARN RES, V97
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zichao Guo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P544, DOI 10.1007/978-3-030-58517-4_32
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 56
TC 4
Z9 4
U1 4
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5199
EP 5208
DI 10.1007/s00371-022-02654-4
EA SEP 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000852356100001
DA 2024-07-18
ER

PT J
AU Xi, Y
   Mao, QR
   Zhou, L
AF Xi, Yan
   Mao, Qirong
   Zhou, Ling
TI Weighted contrastive learning using pseudo labels for facial expression
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Facial expression recognition; Representation learning; Contrastive
   learning; Self-supervised learning; Pseudo labels
AB Supervised learning style requires a large amount of manually annotated data which are not readily available for facial attributes. Particularly annotations for emotion images are subjective and inconsistent, thus labeled facial expression data are insufficient, which cannot satisfy the demand for large-scale Facial Expression Recognition in deep learning era. In this paper, we propose a new self-supervised pretext task, called Weighted Contrastive Learning to make full use of extensive non-curated facial data, and learn the discriminative representation in a self-supervised way. Specifically, WeiCL learns the discriminative representation by two steps: (1) a pre-classification module is designed to build a specific batch for weighted contrastive learning by obtaining pseudo labels of the pre-training unlabeled facial data, and (2) a novel weighted contrastive objective function is proposed to reduce the intra-class variation and enlarge the distance among different instances by weighting the intermediate samples. Experiments on benchmark FER datasets validate the effect of our method. Results demonstrate that our approach outperforms current leading methods with 86.96% on RAF-DB and 71.42% on FER2013, under a rigorous evaluation protocol.
C1 [Xi, Yan; Mao, Qirong; Zhou, Ling] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Mao, Qirong] Jiangsu Engn Res Ctr Big Data Ubiquitous Percept, Zhenjiang 212013, Jiangsu, Peoples R China.
C3 Jiangsu University
RP Mao, QR (corresponding author), Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.; Mao, QR (corresponding author), Jiangsu Engn Res Ctr Big Data Ubiquitous Percept, Zhenjiang 212013, Jiangsu, Peoples R China.
EM xiy@stmail.ujs.edu.cn; mao_qr@ujs.edu.cn; 2111808003@stmail.ujs.edu.cn
FU Key Projects of the National Natural Science Foundation of China
   [U1836220]; National Nature Science Foundation of China [61672267,
   62176106]; Jiangsu Province key research and development plan
   [BE2020036]; Postgraduate Research & Practice Innovation Program of
   Jiangsu Province [KYCX19_1616]
FX This work is supported in part by the Key Projects of the National
   Natural Science Foundation of China under Grant U1836220, the National
   Nature Science Foundation of China under Grant 61672267, 62176106,
   Jiangsu Province key research and development plan under Grant
   BE2020036, in part by the Postgraduate Research & Practice Innovation
   Program of Jiangsu Province under Grant KYCX19_1616.If anyone needs
   code, you can contact the corresponding author by email and we will give
   it after evaluation.
CR Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Brock A., 2019, 7 INT C LEARNING REP
   Chen M.-H., 2020, P IEEE CVF C COMP VI, P9454
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen T, 2019, PROC CVPR IEEE, P12146, DOI 10.1109/CVPR.2019.01243
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Sang DV, 2017, INT CONF KNOWL SYS, P130, DOI 10.1109/KSE.2017.8119447
   Donahue J, 2019, Advances in Neural Information Processing Systems
   Dumoulin V., 2016, INT C LEARNING REPRE
   Fan YR, 2018, LECT NOTES COMPUT SC, V11139, P84, DOI 10.1007/978-3-030-01418-6_9
   Florea Corneliu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P1, DOI 10.1007/978-3-030-58592-1_1
   Florea C., 2019, BMVC, P104, DOI DOI 10.1109/ECAI50035.2020.9223242
   Gidaris S., 2018, P 6 INT C LEARNING R
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Gou JP, 2020, COMPUT ELECTR ENG, V84, DOI 10.1016/j.compeleceng.2020.106632
   Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256
   Gupta S. K., 2011, Proceedings of the Seventh International Conference on Signal-Image Technology & Internet-Based Systems (SITIS 2011), P422, DOI 10.1109/SITIS.2011.64
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Jiang J, 2020, NEURAL NETWORKS, V123, P305, DOI 10.1016/j.neunet.2019.12.005
   Jiang Z., ANN C NEURAL INFORM
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Kim M., 2020, ANN C NEURAL INFORM
   Kumar RJR, 2021, VISUAL COMPUT, V37, P2315, DOI 10.1007/s00371-020-01988-1
   Kumar S, 2021, VISUAL COMPUT, V37, P143, DOI 10.1007/s00371-019-01788-2
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Lee W, 2019, PROC CVPR IEEE, P4979, DOI 10.1109/CVPR.2019.00512
   Levin E., 1988, Complex Systems, V2, P3
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509
   Poggi M, 2020, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR42600.2020.00329
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Shao J, 2021, APPL INTELL, V51, P549, DOI 10.1007/s10489-020-01855-5
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Sonawane B, 2021, VISUAL COMPUT, V37, P1151, DOI 10.1007/s00371-020-01859-9
   Tan FT, 2020, PROC CVPR IEEE, P647, DOI 10.1109/CVPR42600.2020.00073
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   Tian YL, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P229, DOI 10.1109/AFGR.2002.1004159
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Vielzeuf V, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P589, DOI 10.1145/3242969.3264980
   Wang XH, 2013, 2013 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII), P227, DOI 10.1109/SII.2013.6776664
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Yang CY, 2021, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR46437.2021.00398
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zeng GH, 2018, IEEE INT CONF AUTOMA, P423, DOI 10.1109/FG.2018.00068
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhou MH, 2020, PROC CVPR IEEE, P11771, DOI 10.1109/CVPR42600.2020.01179
NR 64
TC 4
Z9 4
U1 1
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 5001
EP 5012
DI 10.1007/s00371-022-02642-8
EA AUG 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000844899900001
DA 2024-07-18
ER

PT J
AU Ma, JW
   Lv, Q
   Yan, HT
   Ye, T
   Shen, YB
   Sun, HC
AF Ma, Jianwei
   Lv, Qi
   Yan, Huiteng
   Ye, Tao
   Shen, Yabin
   Sun, Hechen
TI Color-saliency-aware correlation filters with approximate affine
   transform for visual tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual tracking; Correlation filter; Saliency proposal; Complementary
   learning; Aspect ratio adaptability
ID OBJECT TRACKING; SCALE; SIMILARITY
AB Aspects like deformation and occlusion are still the challenge cases which will result in failures of visual tracking. Many existing correlation filters (CFs) try to fuse the color information to improve the performance but ignore the sensitivity of color information to the background interference. For this case, we propose a color-saliency-aware correlation filter which exploits the color statistics as the model of image boundary connectivity cues. The proposed method limits the drift of correlation filter because of the saliency proposal. In addition, the bounding boxes of CFs absorb too much background information which can easily lead to the tracking failure. To solve this problem, we also present a decoupled-Fourier-Mellin (DFM) transform which is related to the independence of scale variations in log-polar coordinates. In addition to the rotation angle, the proposed DFM can also gain the scale factors of both horizontal and vertical directions, and the larger search space (5-DoF) is closer to the upper bound of object masks. Ultimately, multiple popular benchmarks demonstrate the superiority of our tracker. Compared with the current advanced CFs, our method achieves better performance, which is of great significance for continuous tasks requiring high DoF information, such as manipulator visual servo.
C1 [Ma, Jianwei; Lv, Qi; Yan, Huiteng; Ye, Tao; Shen, Yabin; Sun, Hechen] Dalian Univ Technol, Sch Mech Engn, Minist Educ, Key Lab Precis & Nontradit Machining Technol, Dalian, Peoples R China.
C3 Dalian University of Technology
RP Ma, JW (corresponding author), Dalian Univ Technol, Sch Mech Engn, Minist Educ, Key Lab Precis & Nontradit Machining Technol, Dalian, Peoples R China.
EM mjw2011@dlut.edu.cn
RI zhang, zhang/KGK-5266-2024; Ye, Tao/HPC-6168-2023
FU NKRDPC [2018YFA0704603]; National Natural Science Foundation of China
   [51975098, U1937602]; LiaoNing Revitalization Talents Program
   [XLYC1907006, XLYCYSZX1901]; Science and Technology Innovation Fund of
   Dalian [2019CT01]; Fundamental Research Funds for the Central
   Universities
FX The project is supported by NKRDPC (No. 2018YFA0704603), National
   Natural Science Foundation of China (Nos. 51975098 and U1937602),
   LiaoNing Revitalization Talents Program (Nos. XLYC1907006,
   XLYCYSZX1901), Science and Technology Innovation Fund of Dalian (No.
   2019CT01) and the Fundamental Research Funds for the Central
   Universities. The authors wish to thank the anonymous reviewers for
   their comments which led to improvements of this paper.
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen Z., 2015, Comput. Sci., V53, P68
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Elayaperumal D, 2021, INFORM SCIENCES, V577, P467, DOI 10.1016/j.ins.2021.06.084
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Fu CH, 2020, IEEE T GEOSCI REMOTE, V58, P8940, DOI 10.1109/TGRS.2020.2992301
   Fu CH, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11050549
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Gao LN, 2022, APPL INTELL, V52, P5897, DOI 10.1007/s10489-021-02260-2
   Gupta DK, 2021, PROC CVPR IEEE, P12357, DOI 10.1109/CVPR46437.2021.01218
   Han ZJ, 2020, IEEE T CIRC SYST VID, V30, P155, DOI 10.1109/TCSVT.2018.2888492
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang DF, 2017, INT J COMPUT VISION, V122, P524, DOI 10.1007/s11263-016-0974-6
   Huang Dafei., 2015, P BRIT MACHINE VISIO, p185.1, DOI DOI 10.5244/C.29.185
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Islam MM, 2018, IEEE ACCESS, V6, P75244, DOI 10.1109/ACCESS.2018.2883650
   Kristan, 2016, EUROPEAN C COMPUTER
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li Y, 2016, IEEE IMAGE PROC, P454, DOI 10.1109/ICIP.2016.7532398
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2018, IEEE INT CONF ROBOT, P651
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin FL, 2021, IEEE T CIRC SYST VID, V31, P2160, DOI 10.1109/TCSVT.2020.3023440
   Lu XK, 2021, IEEE T CIRC SYST VID, V31, P1268, DOI 10.1109/TCSVT.2019.2944654
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Meng Y, 2021, APPL INTELL, V51, P3202, DOI 10.1007/s10489-020-01992-x
   Miao Q, 2023, VISUAL COMPUT, V39, P1237, DOI 10.1007/s00371-022-02401-9
   Montero AS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P587, DOI 10.1109/ICCVW.2015.80
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Touil DE, 2018, APPL INTELL, V48, P2837, DOI 10.1007/s10489-017-1120-z
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu LB, 2022, APPL INTELL, V52, P7566, DOI 10.1007/s10489-021-02825-1
   Xue XZ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101644
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Zhang JM, 2022, APPL INTELL, V52, P6129, DOI 10.1007/s10489-021-02694-8
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang MD, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P595, DOI 10.1109/ICCVW.2015.81
   Zhao DW, 2019, INFORM SCIENCES, V470, P78, DOI 10.1016/j.ins.2018.08.053
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zheng G, 2021, ARXIV
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   Zokai S, 2005, IEEE T IMAGE PROCESS, V14, P1422, DOI 10.1109/TIP.2005.854501
NR 70
TC 4
Z9 5
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2023
VL 39
IS 9
BP 4065
EP 4086
DI 10.1007/s00371-022-02573-4
EA AUG 2022
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8PV6
UT WOS:000838568700002
DA 2024-07-18
ER

PT J
AU Tsopouridis, G
   Fudos, I
   Vasilakis, AA
AF Tsopouridis, Grigoris
   Fudos, Ioannis
   Vasilakis, Andreas-Alexandros
TI Deep hybrid order-independent transparency
SO VISUAL COMPUTER
LA English
DT Article
DE Visibility determination; Multifragment rendering; k-buffer; Order
   independent transparency; Deep learning
AB Correctly compositing transparent fragments is an important and long-standing open problem in real-time computer graphics. Multifragment rendering is considered a key solution to providing high-quality order-independent transparency at interactive frame rates. To achieve that, practical implementations severely constrain the overall memory budget by adopting bounded fragment configurations such as the k-buffer. Relying on an iterative trial-and-error procedure, however, where the value of k is manually configured per case scenario, can inevitably result in bad memory utilization and view-dependent artifacts. To this end, we introduce a novel intelligent k-buffer approach that performs a non-uniform per pixel fragment allocation guided by a deep learning prediction mechanism. A hybrid scheme is further employed to facilitate the approximate blending of non-significant (remaining) fragments and thus contribute to a better overall final color estimation. An experimental evaluation substantiates that our method outperforms previous approaches when evaluating transparency in various high-depth-complexity scenes.
C1 [Tsopouridis, Grigoris; Fudos, Ioannis; Vasilakis, Andreas-Alexandros] Univ Ioannina, Dept Comp Sci & Engn, Ioannina, Greece.
   [Vasilakis, Andreas-Alexandros] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
C3 University of Ioannina; Athens University of Economics & Business
RP Fudos, I (corresponding author), Univ Ioannina, Dept Comp Sci & Engn, Ioannina, Greece.
EM fudos@uoi.gr
OI Tsopouridis, Grigorios/0000-0001-8033-5481; Vasilakis, Andreas
   A./0000-0001-6895-3324
FU project "Dioni: Computing Infrastructure for Big-Data Processing and
   Analysis" [5047222]; European Union (ERDF); Operational Program
   "Competitiveness, Entrepreneurship and Innovation," NSRF 2014-2020;
   European Union (European Regional Development Fund-ERDF); Greek national
   funds through the Interreg Greece Albania 2014-2020 Program (project
   VirtuaLand)
FX This research was supported by project "Dioni: Computing Infrastructure
   for Big-Data Processing and Analysis" (MIS No. 5047222) co-funded by
   European Union (ERDF) and Greece through Operational Program
   "Competitiveness, Entrepreneurship and Innovation," NSRF 2014-2020. This
   work has been co-financed by the European Union (European Regional
   Development Fund-ERDF) and Greek national funds through the Interreg
   Greece Albania 2014-2020 Program (project VirtuaLand).
CR Andersson P, 2020, P ACM COMPUT GRAPH, V3, DOI 10.1145/3406183
   Bavoil L., 2008, ORDER INDEPENDENT TR
   Bavoil L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P97
   Carpenter L., 1984, Computers & Graphics, V18, P103
   Catmull Edwin Earl, 1974, A Subdivision Algorithm for Computer Display of Curved Surfaces
   Lavoué G, 2016, IEEE T VIS COMPUT GR, V22, P1987, DOI 10.1109/TVCG.2015.2480079
   Liu F., 2010, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P75, DOI [DOI 10.1145/1730804.1730817, DOI 10.1145/1730804]
   Maule M, 2011, COMPUT GRAPH-UK, V35, P1023, DOI 10.1016/j.cag.2011.07.006
   Maule Marilena., 2013, I3D, P103, DOI DOI 10.1145/2448196.2448212
   McGuire M., 2017, Computer Graphics Archive
   Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P65, DOI 10.1111/cgf.13225
   Porter T., 1984, Computers & Graphics, V18, P253
   Rossignac J, 2013, COMPUT AIDED DESIGN, V45, P288, DOI 10.1016/j.cad.2012.10.012
   Salvi M., 2014, Proceedings of the 18th Meeting of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P151, DOI [10.1145/2556700.2556705, DOI 10.1145/2556700.2556705]
   Salvi Marco., 2011, PROC ACM SIGGRAPH S, P119, DOI [10.1145/2018323.2018342, DOI 10.1145/2018323.2018342]
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Thomas M.M., 2017, DEEP ILLUMINATION AP
   Vardis K, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P171, DOI 10.1145/2856400.2856401
   Vasilakis AA, 2020, COMPUT GRAPH FORUM, V39, P623, DOI 10.1111/cgf.14019
   Vasilakis A. A., 2014, Proceedings of the 18th Meeting of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, P143, DOI [10.1145/2556700.2556702, DOI 10.1145/2556700.2556702]
   Vasilakis A. A., 2017, Proceedings of the European Association for Computer Graphics: Short Papers, P21, DOI [10.2312/egsh.20171005, DOI 10.2312/EGSH.20171005]
   Vasilakis Andreas., 2012, Eurographics, P101, DOI DOI 10.2312/CONF/EG2012/SHORT/101-104
   Vasilakis AA, 2015, IEEE T VIS COMPUT GR, V21, P688, DOI 10.1109/TVCG.2015.2417581
   Wyman C., 2016, Proceedings of High Performance Graphics, P1, DOI [10.2312/hpg.20161187, DOI 10.2312/HPG.20161187]
   Yang JC, 2010, COMPUT GRAPH FORUM, V29, P1297, DOI 10.1111/j.1467-8659.2010.01725.x
   Zhang DJ, 2020, IEEE ACCESS, V8, P64434, DOI 10.1109/ACCESS.2020.2984771
NR 26
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3289
EP 3300
DI 10.1007/s00371-022-02562-7
EA JUL 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000819705400006
OA Bronze
DA 2024-07-18
ER

PT J
AU Ströter, D
   Mueller-Roemer, JS
   Weber, D
   Fellner, DW
AF Stroeter, D.
   Mueller-Roemer, J. S.
   Weber, D.
   Fellner, D. W.
TI Fast harmonic tetrahedral mesh optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Numerical optimization; GPGPU; Simplicial meshes; Simulation
ID QUALITY; MATRIX; FRAMEWORK
AB Mesh optimization is essential to enable sufficient element quality for numerical methods such as the finite element method (FEM). Depending on the required accuracy and geometric detail, a mesh with many elements is necessary to resolve small-scale details. Sequential optimization of large meshes often imposes long run times. This is especially an issue for Delaunay-based methods. Recently, the notion of harmonic triangulations [1] was evaluated for tetrahedral meshes, revealing significantly faster run times than competing Delaunay-based methods. A crucial aspect for efficiency and high element quality is boundary treatment. We investigate directional derivatives for boundary treatment and massively parallel GPUs for mesh optimization. Parallel flipping achieves compelling speedups by up to 318x. We accelerate harmonic mesh optimization by 119x for boundary preservation and 78x for moving every boundary vertex, while producing superior mesh quality.
C1 [Stroeter, D.; Mueller-Roemer, J. S.; Weber, D.; Fellner, D. W.] Tech Univ Darmstadt, D-64277 Darmstadt, Germany.
   [Mueller-Roemer, J. S.; Weber, D.; Fellner, D. W.] Fraunhofer IGD, Darmstadt, Germany.
   [Fellner, D. W.] Graz Univ Technol, Darmstadt, Germany.
C3 Technical University of Darmstadt; Graz University of Technology
RP Ströter, D (corresponding author), Tech Univ Darmstadt, D-64277 Darmstadt, Germany.
EM daniel.stroeter@gris.tu-darmstadt.de
OI Stroter, Daniel/0000-0002-2672-7377; Fellner, Dieter
   W./0000-0001-7756-0901; Mueller-Roemer, Johannes
   Sebastian/0000-0002-0712-0457; Weber, Daniel/0000-0001-6184-0570
FU EC project DIGITbrain [952071]; Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL. The second
   and the third author were supported by the EC project DIGITbrain, No.
   952071, H2020. We thank Marc Alexa for providing sources of the original
   harmonic mesh optimization implementation.
CR Alexa M, 2020, COMPUT GRAPH FORUM, V39, P55, DOI 10.1111/cgf.14068
   Alexa M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322986
   [Anonymous], 2014, Finite Element Mesh Generation
   Benitez D., 2018, P 6 EUROPEAN C COMPU, P4403
   Cao Thanh-Tung., 2014, P 18 M ACM SIGGRAPH, P47, DOI DOI 10.1145/2556700.2556710
   D'Amato JP, 2013, J PARALLEL DISTR COM, V73, P1127, DOI 10.1016/j.jpdc.2013.03.007
   Dassi F, 2018, COMPUT AIDED DESIGN, V103, P2, DOI 10.1016/j.cad.2017.11.010
   Drakopoulos F, 2019, AIAA J, V57, P4007, DOI 10.2514/1.J057657
   Freitag L, 1999, SIAM J SCI COMPUT, V20, P2023, DOI 10.1137/S1064827597323208
   Freitag LA, 1997, INT J NUMER METH ENG, V40, P3979, DOI 10.1002/(SICI)1097-0207(19971115)40:21<3979::AID-NME251>3.0.CO;2-9
   Fu XM, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766938
   Hager W. W., 2006, Pacific J. Optimization, V2, P35
   Hormann K., 2000, MIPS EFFICIENT GLOBA
   Hu YX, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392385
   Ibanez D., 2016, 201624 RENSS POL I
   Klingner BM, 2008, PROCEEDINGS OF THE 16TH INTERNATIONAL MESHING ROUNDTABLE, P3, DOI 10.1007/978-3-540-75103-8_1
   Knupp PM, 2000, INT J NUMER METH ENG, V48, P1165, DOI 10.1002/(SICI)1097-0207(20000720)48:8<1165::AID-NME940>3.0.CO;2-Y
   Liu H.T.D., 2019, SPECTRAL COARSENING
   Manteaux PL, 2017, COMPUT GRAPH FORUM, V36, P312, DOI 10.1111/cgf.12941
   Mueller-Roemer JS, 2018, COMPUT GRAPH FORUM, V37, P443, DOI 10.1111/cgf.13581
   Mueller-Roemer JS, 2017, COMPUT GRAPH FORUM, V36, P59, DOI 10.1111/cgf.13245
   Nvidia, 2022, CUD 11 2
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rabinovich M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983621
   Ruder S., 2016, ARXIV
   Shang MM, 2016, PROCEDIA ENGINEER, V163, P289, DOI 10.1016/j.proeng.2016.11.062
   Shewchuk J., 2002, PREPRINT
   Shontz S. M., 2003, P 12 INT MESH ROUNDT, P147
   Shontz S.M., 2020, P 28 INT MESHING ROU
   Si H, 2015, ACM T MATH SOFTWARE, V41, DOI 10.1145/2629697
   Smith J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766947
   Stein O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3186564
   Stroter D., 2021, P 11 INT C SIMULATIO
   Weber D, 2015, COMPUT GRAPH-UK, V53, P185, DOI 10.1016/j.cag.2015.06.010
   Wicke M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778786
   Xi N, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11125543
   Xu K, 2009, COMPUT GRAPH-UK, V33, P250, DOI 10.1016/j.cag.2009.03.020
   Yin J, 2008, ENG COMPUT-GERMANY, V24, P231, DOI 10.1007/s00366-008-0090-5
   ZHANG H., 2007, P EUROGRAPHICS STATE, P1, DOI DOI 10.1109/IPDPS.2007.370248
   Zint Daniel, 2019, 27th International Meshing Roundtable. Lecture Notes in Computational Science and Engineering (LNCSE 127), P445, DOI 10.1007/978-3-030-13992-6_24
NR 40
TC 4
Z9 4
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3419
EP 3433
DI 10.1007/s00371-022-02547-6
EA JUN 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000813592000001
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, XR
   Shi, XY
   Iwamoto, Y
   Cheng, JL
   Bai, J
   Zhao, GH
   Han, XH
   Chen, YW
AF Zhang, Xinran
   Shi, Xiaoyu
   Iwamoto, Yutaro
   Cheng, Jingliang
   Bai, Jie
   Zhao, Guohua
   Han, Xian-hua
   Chen, Yen-Wei
TI IDH mutation status prediction by a radiomics associated modality
   attention network
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Isocitrate dehydrogenase; Glioma; Modality attention; Radiomics;
   Self-attention
ID SYSTEM
AB Isocitrate dehydrogenase (IDH) status is an important factor for the diagnosis of gliomas reported in the 2016 World Health Organization classification scheme for gliomas. There is a strong relationship between IDH mutation status and prognosis. The preoperative prediction of IDH status is necessary for appropriate treatment planning. However, existing methods cannot predict IDH status accurately before the operation. In this paper, we propose a radiomics associated modality attention network to predict IDH mutation status on multi-modality MRI images. Our method first predicts the importance of each modality for the classification task and calculates weights, then uses weighted images for prediction. We also present a light-weight and high-performance self-attention network for gliomas tumor classification to solve the overfitting problem. Additionally, we associate radiomics features for computation of modality attention and classification to enhance the classification accuracy. Our method achieves a 0.7246 F1 Score on our private dataset provided by the First Affiliated Hospital of Zhengzhou University (FHZU), which is better than state-of-the-art methods.
C1 [Zhang, Xinran; Shi, Xiaoyu; Iwamoto, Yutaro; Chen, Yen-Wei] Ritsumeikan Univ, 1-1-1 Noji Higashi, Shiga 5258577, Japan.
   [Cheng, Jingliang; Bai, Jie; Zhao, Guohua] Zhengzhou Univ, Affiliated Hosp 1, Dept Magnet Resonance Imaging, Zhengzhou 450004, Peoples R China.
   [Han, Xian-hua] Yamaguchi Univ, 1677-1 Yoshida, Yamaguchi, Yamaguchi 7538511, Japan.
C3 Ritsumeikan University; Zhengzhou University; Yamaguchi University
RP Chen, YW (corresponding author), Ritsumeikan Univ, 1-1-1 Noji Higashi, Shiga 5258577, Japan.
EM chen@is.ritsumei.ac.jp
RI Li, Binxu/KDO-3273-2024; Han, Xian-Hua/A-5563-2017; Shi,
   Xiaoyu/D-4682-2017
OI Chen, Yenwei/0000-0002-5952-0188
FU Japanese Ministry for Education, Science, Culture and Sports (MEXT)
   [20KK0234, 20K21821]; Zhejiang Lab Program [2020ND8AD01]; Grants-in-Aid
   for Scientific Research [20K21821, 20KK0234] Funding Source: KAKEN
FX This work is supported in part by the Grant-in-Aid for Scientific
   Research from the Japanese Ministry for Education, Science, Culture and
   Sports (MEXT) under the Grant No. 20KK0234, No. 20K21821. Authors would
   like to thank Ms. Swathi ANANDA and Rahul JAIN of Ritsumeikan University
   for their English proof and corrections. Funding was provided by the
   Zhejiang Lab Program (No. 2020ND8AD01).
CR Broen MPG, 2018, NEURO-ONCOLOGY, V20, P1393, DOI 10.1093/neuonc/noy048
   Choi KS, 2019, NEURO-ONCOLOGY, V21, P1197, DOI 10.1093/neuonc/noz095
   Choi YS, 2021, NEURO-ONCOLOGY, V23, P304, DOI 10.1093/neuonc/noaa177
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   De Vleeschouwer S, 2017, GLIOBLASTOMA
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jang K, 2020, NEURORADIOLOGY, V62, P771, DOI 10.1007/s00234-020-02403-1
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li C, 2019, LECT NOTES COMPUT SC, V11765, P57, DOI 10.1007/978-3-030-32245-8_7
   Liu SD, 2020, SCI REP-UK, V10, DOI [10.1038/s41598-020-64588-y, 10.1177/2158244020924052]
   Louis DN, 2016, ACTA NEUROPATHOL, V131, P803, DOI 10.1007/s00401-016-1545-1
   Lowekamp BC, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00045
   Ostrom QT, 2015, NEURO-ONCOLOGY, V17, P1, DOI [10.1093/neuonc/now207, 10.1093/neuonc/nov189]
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   van Griethuysen JJM, 2017, CANCER RES, V77, pE104, DOI 10.1158/0008-5472.CAN-17-0339
   Xinran Zhang, 2021, Innovation in Medicine and Healthcare. Proceedings of 9th KES-InMed 2021. Smart Innovation, Systems and Technologies (SIST 242), P51, DOI 10.1007/978-981-16-3013-2_5
   Yan H, 2009, NEW ENGL J MED, V360, P765, DOI 10.1056/NEJMoa0808710
   Yaniv Z, 2018, J DIGIT IMAGING, V31, P290, DOI 10.1007/s10278-017-0037-8
   Zhang G., 2020, CROSS MODAL SELF ATT
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
NR 21
TC 0
Z9 0
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2367
EP 2379
DI 10.1007/s00371-022-02452-y
EA MAR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000777227800002
DA 2024-07-18
ER

PT J
AU Rao, RV
   Prasad, TJC
AF Rao, R. Varaprasada
   Prasad, T. Jaya Chandra
TI An efficient content-based medical image retrieval based on a new Canny
   steerable texture filter and Brownian motion weighted deep learning
   neural network
SO VISUAL COMPUTER
LA English
DT Article
DE Modified Kuan Filter (MKF); Gaussian Linear Contrast Stretching Model
   (GLCSM); Canny steerable texture filter (CSTF); Mean Correlation
   Coefficient Component Analysis (MCCCA); Brownian motion weighting deep
   learning neural network (BMWDLNN) classifier; Harmonic Mean-based Fisher
   Score (HMFS)
ID FEATURE DESCRIPTOR; PATTERNS; EXTRACTION
AB The increasing size of medical image repositories is due to the increasing number of digital imaging data sources. Most of the image content descriptors proposed in the literature are not suitable for the retrieval of large medical image datasets. The ability to extract features from an image is a vital criterion that should be considered to evaluate retrieval efficacy. This paper proposes an efficient image retrieval system for medical applications based on the new Canny steerable texture filter (CSTF) feature descriptor and Brownian motion weighting deep learning neural network (BMWDLNN) classifier. Initially, Modified Kuan Filter (MKF) is used to condense the noise in images. Then, the image contrast is enhanced using the Gaussian Linear Contrast Stretching Model (GLCSM) method. Then, the image features are extracted using the CSTF method. Later, the dimensionality of the extracted features is reduced by means of the Mean Correlation Coefficient Component Analysis (MCCCA) method and then the BMWDLNN classifier is applied. For the classified images, the score values are calculated using the Harmonic Mean-based Fisher Score (HMFS) method. Thereafter, various distance values are calculated for the score value of the image and are summed up to find the average. The retrieval outcome is determined by the minimum distance between database images and the query image. The proposed method obtained an average precision rate of 0.9981, 0.9992, 0.9951, and 0.9940 for EXACT-09, TCIA, NEMA-CT, and OASIS databases, respectively. The experimental results revealed that the proposed methodology outperforms the existing methods.
C1 [Rao, R. Varaprasada] Jawaharlal Nehru Technol Univ Anantapur JNTUA, Dept Elect & Commun Engn, Ananthapuramu, Andhra Pradesh, India.
   [Prasad, T. Jaya Chandra] JNTUA, Rajeev Gandhi Mem Coll Engn & Technol, Dept Elect & Commun Engn, Nandyal, Andhra Pradesh, India.
C3 Jawaharlal Nehru Technological University - Anantapur; Jawaharlal Nehru
   Technological University - Anantapur
RP Rao, RV (corresponding author), Jawaharlal Nehru Technol Univ Anantapur JNTUA, Dept Elect & Commun Engn, Ananthapuramu, Andhra Pradesh, India.
EM rvr712@gmail.com
RI Talari, Jayachandra Prasad/P-2766-2019
OI Talari, Jayachandra Prasad/0000-0002-7804-982X
CR Agarwal M, 2019, PATTERN ANAL APPL, V22, P1585, DOI 10.1007/s10044-019-00787-2
   Alsmadi MK, 2020, ARAB J SCI ENG, V45, P3317, DOI 10.1007/s13369-020-04384-y
   [Anonymous], 2020, NEMA CT IMAGE DATABA
   Ashraf R, 2020, MULTIMED TOOLS APPL, V79, P8553, DOI 10.1007/s11042-018-5961-1
   Biji KR., 2020, MULTIMED TOOLS APPL, V79, P1
   Biswas R, 2019, INT J MULTIMED INF R, V8, P217, DOI 10.1007/s13735-019-00176-9
   Bressan RS, 2019, NEUROCOMPUTING, V357, P1, DOI 10.1016/j.neucom.2019.05.041
   Cai YH, 2019, IEEE ACCESS, V7, P51877, DOI 10.1109/ACCESS.2019.2911630
   Das P, 2017, INT J MULTIMED INF R, V6, P271, DOI 10.1007/s13735-017-0135-x
   Deep G, 2016, ENG SCI TECHNOL, V19, P1895, DOI 10.1016/j.jestch.2016.05.006
   Dubey SR, 2020, NEURAL COMPUT APPL, V32, P7539, DOI 10.1007/s00521-019-04279-6
   Dubey SR, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493446
   Gai S, 2019, VISUAL COMPUT, V35, P109, DOI 10.1007/s00371-017-1456-8
   Garg M, 2021, NEURAL COMPUT APPL, V33, P1311, DOI 10.1007/s00521-020-05017-z
   Ghrabat MJJ, 2019, HUM-CENTRIC COMPUT I, V9, DOI 10.1186/s13673-019-0191-8
   Hussain CA, 2021, EVOL INTELL, V14, P1449, DOI 10.1007/s12065-020-00401-z
   Jeyakumar V., 2019, INTELLIGENT DATA ANA, P121, DOI [doi:10.1016/B978-0-12-815553-0.00006-9, DOI 10.1016/B978-0-12-815553-0.00006-9]
   Jia F, 2021, IEEE INT CONF COMP V, P354, DOI 10.1109/ICCVW54120.2021.00044
   Jia F, 2021, IEEE SIGNAL PROC LET, V28, P1600, DOI 10.1109/LSP.2021.3100263
   Karthik K, 2021, VISUAL COMPUT, V37, P1837, DOI 10.1007/s00371-020-01941-2
   Kasban H, 2019, MULTIMED TOOLS APPL, V78, P35211, DOI 10.1007/s11042-019-08100-3
   Lan RS, 2018, COMPUT ELECTR ENG, V69, P669, DOI 10.1016/j.compeleceng.2018.01.027
   Lan RS, 2017, IEEE J BIOMED HEALTH, V21, P1338, DOI 10.1109/JBHI.2016.2623840
   Lo P, 2012, IEEE T MED IMAGING, V31, P2093, DOI 10.1109/TMI.2012.2209674
   Mandal M, 2019, IET COMPUT VIS, V13, P31, DOI 10.1049/iet-cvi.2018.5206
   Mezzoudj S, 2021, J KING SAUD UNIV-COM, V33, P141, DOI 10.1016/j.jksuci.2019.01.003
   Mirasadi MS, 2019, INT J MULTIMED INF R, V8, P233, DOI 10.1007/s13735-019-00179-6
   Mistry YD., 2020, INT J, V3, P1
   Murala S, 2015, NEUROCOMPUTING, V149, P1502, DOI 10.1016/j.neucom.2014.08.042
   Murala S, 2014, IEEE J BIOMED HEALTH, V18, P929, DOI 10.1109/JBHI.2013.2288522
   Murala S, 2013, NEUROCOMPUTING, V119, P399, DOI 10.1016/j.neucom.2013.03.018
   Murala S, 2012, J MED SYST, V36, P2865, DOI 10.1007/s10916-011-9764-4
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   Nair LR, 2020, MULTIMED TOOLS APPL, V79, P10123, DOI 10.1007/s11042-019-08090-2
   Owais M, 2019, J CLIN MED, V8, DOI 10.3390/jcm8040462
   Öztürk S, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113693
   Sanchez A, 2021, VISUAL COMPUT, V37, P203, DOI 10.1007/s00371-020-01793-w
   Sathiamoorthy S, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1941-y
   Sezavar A, 2019, MULTIMED TOOLS APPL, V78, P20895, DOI 10.1007/s11042-019-7321-1
   Shamna P, 2019, J BIOMED INFORM, V91, DOI 10.1016/j.jbi.2019.103112
   Shinde A, 2019, BIOMED ENG LETT, V9, P387, DOI 10.1007/s13534-019-00112-0
   Shinde A, 2019, MULTIMED TOOLS APPL, V78, P23489, DOI 10.1007/s11042-019-7697-y
   Shinde AA, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P154, DOI 10.1109/SIPROCESS.2017.8124524
   Shinde AA, 2017, INT J MULTIMED INF R, V6, P281, DOI 10.1007/s13735-017-0132-0
   Sundararajan SK, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1305-6
   Swati ZNK, 2019, IEEE ACCESS, V7, P17809, DOI 10.1109/ACCESS.2019.2892455
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang R, 2018, IEEE ACCESS, V6, P63737, DOI 10.1109/ACCESS.2018.2875270
NR 49
TC 4
Z9 4
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1797
EP 1813
DI 10.1007/s00371-022-02446-w
EA MAR 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000776062200001
DA 2024-07-18
ER

PT J
AU Guan, ZY
   Wang, ZQ
   Zhu, YS
   Liu, GC
AF Guan, Zhenyu
   Wang, Ziqi
   Zhu, Yisheng
   Liu, Guangcan
TI Presswork defect inspection using only defect-free high-resolution
   images
SO VISUAL COMPUTER
LA English
DT Article
DE Presswork defect detection; Defect generation; Imbanlance; Segmentation
AB Efficient automated presswork defect detection is valuable to the printing industry since such defects dramatically depress the presswork grad and manually detecting them is cost-ineffective. Instead of using real defect annotating data, the goal of this paper is to just use defect-free high-resolution images to complete the detection of defect in presswork (SWDF). In this paper, we first propose an end-to-end cropping method to balance the number of samples which cropped from different patterns in the defect-free high-resolution images, and thus, we can solve the quantity imbalance between different patterns (image level). Secondly, we designed an simple but effective loss function to solve the severe imbalance which caused by the larger quantity difference between the defective pixels and background pixels (pixel level). In order to solve the absence of the real defect annotating data and replace the time-consuming acquisition of defect data, we designed a high-speed defect generation algorithm, which directly generate defect on defect-free samples (obtained from the cropping process). As for the detection of defects, we use self-attention to design a novel and effective semantic segmentation head (GST), which can exploit global information from the feature map to repair the detection results, so as to obtain a better performance. In the experimental part, we will use a presswork defect dataset from the Zhentu Cup Competition (ZCC) and a public dataset DAGM 2007 to test the effect of our scheme. Especially, compared with existing supervised models, our model has also reached the state of the art in DAGM 2007.
C1 [Guan, Zhenyu; Wang, Ziqi; Zhu, Yisheng; Liu, Guangcan] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol B DAT, Nanjing, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Guan, ZY (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol B DAT, Nanjing, Peoples R China.
EM 20191222016@nuist.edu.cn
RI Liu, Guangcan/J-1391-2014; wang, ziqi/IQS-5011-2023; Zhu,
   Yisheng/GYA-3445-2022
OI Liu, Guangcan/0000-0002-9428-4387; guan, zhenyu/0000-0002-0053-5465
CR Bozic, 2020, ARXIV PREPRINT ARXIV
   Bozic J, 2021, COMPUT IND, V129, DOI 10.1016/j.compind.2021.103459
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Dai W., 2021, VISUAL COMPUT, P1
   Kim S, 2017, IEEE IJCNN, P2517, DOI 10.1109/IJCNN.2017.7966162
   Kingma D. P., 2013, ARXIV13126114
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Mei S, 2018, IEEE T INSTRUM MEAS, V67, P1266, DOI 10.1109/TIM.2018.2795178
   Mei S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041064
   Qin Y, 2020, VISUAL COMPUT, V36, P621, DOI 10.1007/s00371-019-01644-3
   Racki D, 2018, IEEE WINT CONF APPL, P1331, DOI 10.1109/WACV.2018.00150
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Shyam, ARXIV PREPRINT ARXIV
   Timm F., 2011, Proc. SPIE, V7877
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2020, VISUAL COMPUT, V36, P2471, DOI 10.1007/s00371-020-01887-5
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Weimer D, 2013, PROC CIRP, V7, P347, DOI 10.1016/j.procir.2013.05.059
   Weimer D, 2016, CIRP ANN-MANUF TECHN, V65, P417, DOI 10.1016/j.cirp.2016.04.072
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yuan Y., 2019, ARXIV PREPRINT ARXIV
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao ZX, 2018, LECT NOTES ARTIF INT, V11013, P473, DOI 10.1007/978-3-319-97310-4_54
NR 25
TC 1
Z9 1
U1 0
U2 18
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2023
VL 39
IS 4
BP 1271
EP 1282
DI 10.1007/s00371-022-02403-7
EA MAR 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C3OY5
UT WOS:000771842100001
DA 2024-07-18
ER

PT J
AU Xu, HP
   Jia, XN
   Cheng, LB
   Huang, HY
AF Xu, Huaping
   Jia, Xiaoning
   Cheng, Libo
   Huang, Heyan
TI Affine non-local Bayesian image denoising algorithm
SO VISUAL COMPUTER
LA English
DT Article
DE Elliptical patches; Affine invariant patch similarity measure; Bayesian;
   Image denoising
ID MULTISCALE ANALYSIS; SPARSE; SIMILARITIES; FRAMEWORK
AB This paper proposes an extension of the non-local Bayesian denoising algorithm. The idea is to use elliptical patches instead of regular square patches in the grouping process. We calculate the elliptical patches by an iterative method. We then use an affine invariant patch similarity measure to calculate the distance between two elliptical patches. Since the elliptical patch is a shape-adaptive patch and this similarity measure performs a patch comparison by automatically adapting the size and shape of the patches, so more similar patches are found and used for image denoising. This algorithm denoising procedure goes through two identical iterations to further improve the denoising performance. Experimental results on test images demonstrate that this algorithm achieves state-of-the-art denoising performance in terms of numerical results and subjective visual quality, compared with the non-local Bayesian.
C1 [Xu, Huaping; Jia, Xiaoning; Cheng, Libo] Changchun Univ Sci & Technol, Sch Math & Stat, Dept Math, Changchun, Peoples R China.
   [Huang, Heyan] Shanghai Inst Technol, Coll Sci, Shanghai, Peoples R China.
C3 Changchun University of Science & Technology; Shanghai Institute of
   Technology
RP Jia, XN (corresponding author), Changchun Univ Sci & Technol, Sch Math & Stat, Dept Math, Changchun, Peoples R China.
EM jiaxiaoning@cust.edu.cn
FU National Natural Science Foundation of China [12171054]; fund of the
   "Thirteen Five" Scientific and Technological Research Planning Project
   of the Department of Education of Jilin Province [JJKH20200726KJ]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 12171054, and the fund of the "Thirteen Five"
   Scientific and Technological Research Planning Project of the Department
   of Education of Jilin Province (JJKH20200726KJ). The authors would like
   to thank the anonymous reviewers and editor for their helpful feedback
   and suggestions.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Altinsoy E, 2022, VISUAL COMPUT, V38, P2139, DOI 10.1007/s00371-021-02273-5
   Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   An FP, 2022, VISUAL COMPUT, V38, P541, DOI 10.1007/s00371-020-02033-x
   Arias P, 2017, LECT NOTES COMPUT SC, V10302, P208, DOI 10.1007/978-3-319-58771-4_17
   Ballester C, 2014, MULTISCALE MODEL SIM, V12, P616, DOI 10.1137/130926833
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Chudasama V, 2022, VISUAL COMPUT, V38, P3643, DOI 10.1007/s00371-021-02193-4
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deledalle CA, 2012, J MATH IMAGING VIS, V43, P103, DOI 10.1007/s10851-011-0294-y
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fedorov V., 2016, VISAPP, P48
   Fedorov V., 2016, AFFINE INVARIANT IMA
   Fedorov V, 2018, IMAGE PROCESS ON LIN, V8, P490, DOI 10.5201/ipol.2018.202
   Fedorov V, 2017, IEEE T IMAGE PROCESS, V26, P2137, DOI 10.1109/TIP.2017.2681421
   Fedorov V, 2015, SIAM J IMAGING SCI, V8, P2021, DOI 10.1137/141000002
   Gai S, 2019, VISUAL COMPUT, V35, P109, DOI 10.1007/s00371-017-1456-8
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Kheradmand A, 2014, IEEE T IMAGE PROCESS, V23, P5136, DOI 10.1109/TIP.2014.2362059
   Lebrun M, 2013, SIAM J IMAGING SCI, V6, P1665, DOI 10.1137/120874989
   Lebrun M, 2013, IMAGE PROCESS ON LIN, V3, P1, DOI 10.5201/ipol.2013.16
   Liu XW, 2019, VISUAL COMPUT, V35, P1883, DOI 10.1007/s00371-018-1581-z
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Madhuranga D, 2021, VISUAL COMPUT, V37, P1263, DOI 10.1007/s00371-020-01864-y
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2010, PATTERN RECOGN, V43, P1531, DOI 10.1016/j.patcog.2009.09.023
NR 34
TC 5
Z9 5
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 99
EP 118
DI 10.1007/s00371-021-02316-x
EA FEB 2022
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000758950400002
DA 2024-07-18
ER

PT J
AU Ji, JY
   Xiang, K
   Wang, XY
AF Ji, Jiayu
   Xiang, Ke
   Wang, Xuanyin
TI SCVS: blind image quality assessment based on spatial correlation and
   visual saliency
SO VISUAL COMPUTER
LA English
DT Article
DE NR-IQA; Visual saliency; Gaussian function; Spatial correlation
ID MODEL
AB We propose a no-reference image quality assessment (NR-IQA) approach to predict the perceptual quality score of a given image without using any reference image. Our model consists of two steps and trains two similar convolutional neural networks (CNN) progressively. In order to consider the quality of different blocks in the whole picture, the first CNN takes the weighted average of the FR-IQA score of each patch and the differential mean opinion scores of the whole image as target output. The second CNN considers the interaction of the adjacent patches in an image. This paper not only uses visual saliency to address the importance of different patches, but also considers the spatial interaction of adjacent patches using Gaussian function. We compare the prediction results with several up-to-date proposed methods in six databases, and demonstrate the advance of our method.
C1 [Ji, Jiayu; Wang, Xuanyin] Zhejiang Univ, Hangzhou, Peoples R China.
   [Xiang, Ke] Zhejiang Sunny Opt Intelligence Technol Co Ltd, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Wang, XY (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.
EM 11625039@zju.edu.cn; xiangke@sunnyoptical.com; xywang@zju.edu.cn
RI wang, xuan/GXF-3679-2022; wang, xuan/JBJ-6948-2023
FU National Natural Science Foundation of China [52075483]
FX This project is supported by National Natural Science Foundation of
   China (Grant No. 52075483).
CR Abbas N, 2020, ARAB J SCI ENG, V45, P3387, DOI 10.1007/s13369-020-04414-9
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Huang Z., 2020, IEEE T CIRC SYST VID
   Huang Z, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107757
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Joshi P, 2018, VISUAL COMPUT, V34, P1739, DOI 10.1007/s00371-017-1460-z
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li J, 2016, SIGNAL IMAGE VIDEO P, V10, P609, DOI 10.1007/s11760-015-0784-2
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Rezaie F., 2017, MULTIMED TOOLS APPL, V77, P1
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Singh VK, 2020, VISUAL COMPUT, V36, P1423, DOI 10.1007/s00371-019-01750-2
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Ye P, 2013, PROC CVPR IEEE, P987, DOI 10.1109/CVPR.2013.132
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Yildiz ZC, 2020, VISUAL COMPUT, V36, P127, DOI 10.1007/s00371-018-1592-9
   Zhang L, 2013, IEEE IMAGE PROC, P171, DOI 10.1109/ICIP.2013.6738036
   Zhang L, 2014, IEEE MULTIMEDIA, V21, P67, DOI 10.1109/MMUL.2014.50
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu HC, 2022, IEEE T CIRC SYST VID, V32, P1048, DOI 10.1109/TCSVT.2021.3073410
NR 52
TC 6
Z9 6
U1 3
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 443
EP 458
DI 10.1007/s00371-021-02340-x
EA JAN 2022
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000742981500002
DA 2024-07-18
ER

PT J
AU Jin, Y
   Jiang, ZW
   Huang, MJ
   Xue, ZZ
AF Jin, Yan
   Jiang, Zhiwei
   Huang, Mengjia
   Xue, Zhizhong
TI Low-dose CT image restoration based on noise prior regression network
SO VISUAL COMPUTER
LA English
DT Article
DE Low-dose CT; Deep learning; Image restoration; Prior knowledge
ID RECONSTRUCTION
AB Low-dose CT image (LDCT) restoration is a challenging task attracting the interest of researchers extensively. However, reducing the radiation dose may lead to increased noise and artifacts. Over the past years, deep learning has produced impressive results in low-dose CT image restoration by learning a nonlinear mapping function. However, the limited number of image pairs may be unavailable in medical applications. And the mapping space from high-resolution images (HR) to low-resolution images (LR) is extremely large, which can hardly find a good result. Furthermore, it is difficult to directly remove noise due to the lack of prior knowledge. Therefore, we introduce a noise prior regression network (NPRN) by providing a prior of the noise distribution and introducing a constraint to estimate the HR -> LR mapping space. Furthermore, the noise prior makes the network focus on the noisy regions but also explicitly assesses the local consistency of the recovered regions. Simultaneously, the regression process does not depend on the pair images. We also compare our method with some state-of-the-art algorithms. The experimental results show that the proposed NPRN recovers structural textures effectively.
C1 [Jin, Yan; Jiang, Zhiwei; Huang, Mengjia; Xue, Zhizhong] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Jin, Y (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
EM jy@zjut.edu.cn
OI Jin, Yan/0000-0001-8956-7684; zhiwei, jiang/0000-0001-7314-2083
CR Aapm, 2017, Low Dose CT Grand Challenge
   Batson J, 2019, PR MACH LEARN RES, V97
   Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11
   Elbakri IA, 2002, IEEE T MED IMAGING, V21, P89, DOI 10.1109/42.993128
   Fan JF, 2019, MED IMAGE ANAL, V54, P193, DOI 10.1016/j.media.2019.03.006
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Hanchate V, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1937-7
   Hara AK, 2009, AM J ROENTGENOL, V193, P764, DOI 10.2214/AJR.09.2397
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herzog P, 2004, LANCET, V363, P340, DOI 10.1016/S0140-6736(04)15470-6
   Jia XX, 2019, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2019.00621
   Jiang XB, 2021, VISUAL COMPUT, V37, P2419, DOI 10.1007/s00371-020-01996-1
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma DP, 2015, ADV NEUR IN, V28
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   LEWITT RM, 1990, J OPT SOC AM A, V7, P1834, DOI 10.1364/JOSAA.7.001834
   Li ZB, 2014, MED PHYS, V41, DOI 10.1118/1.4851635
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu YF, 2021, VISUAL COMPUT, V37, P2513, DOI 10.1007/s00371-021-02204-4
   NAIDICH DP, 1990, RADIOLOGY, V175, P729, DOI 10.1148/radiology.175.3.2343122
   Ramírez RR, 2011, NEUROIMAGE, V56, P78, DOI 10.1016/j.neuroimage.2011.02.002
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Singh S, 2010, RADIOLOGY, V257, P373, DOI 10.1148/radiol.10092212
   Su S.-Y., 2019, DUAL SUPERVISED LEAR
   TCGA-KICH, CANC GEN ATL KIDN CH
   Tian Z, 2011, PHYS MED BIOL, V56, P5949, DOI 10.1088/0031-9155/56/18/011
   Tran L. D., 2020, P AS C COMP VIS
   Wang J, 2006, IEEE T MED IMAGING, V25, P1272, DOI 10.1109/TMI.2006.882141
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Xia YC, 2017, PR MACH LEARN RES, V70
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   You CY, 2020, IEEE T MED IMAGING, V39, P188, DOI 10.1109/TMI.2019.2922960
   Yu HC, 2017, IEEE IMAGE PROC, P3944, DOI 10.1109/ICIP.2017.8297022
   Zhang YB, 2017, IEEE T MED IMAGING, V36, P142, DOI 10.1109/TMI.2016.2600249
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 40
TC 2
Z9 2
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 459
EP 471
DI 10.1007/s00371-021-02341-w
EA JAN 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000740220700003
DA 2024-07-18
ER

PT J
AU Bai, C
   Liu, GS
   Li, XR
   Li, RY
   Sun, S
AF Bai, Chun
   Liu, Guangshuai
   Li, Xurui
   Li, Ruoyu
   Sun, Si
TI Orienting unorganized points and extracting isosurface for implicit
   surface reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud; Orientation; Isosurface extraction; Implicit surface
   reconstruction
ID MARCHING CUBES; ALGORITHM; ORIENTATION
AB We address the problems of consistent orientation of unorganized point cloud with complicated surfaces, especially close-by and sharp surfaces, as well as efficiency of isosurface extraction for implicit surface reconstruction. Our method focuses on enhancing the association of the same region and the stable propagation between different regions and the changes of the search strategy in the respective orientation and isosurface extraction processes, which includes two new developments. First, an optimized priority-driven normal propagation scheme is introduced, where lower possibility across different regions and an improved flip criterion can ensure reliable normal direction. Next, instead of scanning the entire voxel space, an automatic isosurface growing algorithm is proposed, which can automatically search for boundary voxels without additional conditions, thus speeding up the extraction process of isosurface. Plentiful experiments show that our method can generate consistent orientation even under various difficult scenarios and remarkably improve the extraction efficiency for implicit surface reconstruction.
C1 [Bai, Chun; Liu, Guangshuai; Li, Xurui; Li, Ruoyu; Sun, Si] Southwest Jiaotong Univ, Sch Mech Engn, Chengdu 610031, Sichuan, Peoples R China.
C3 Southwest Jiaotong University
RP Liu, GS (corresponding author), Southwest Jiaotong Univ, Sch Mech Engn, Chengdu 610031, Sichuan, Peoples R China.
EM motorliu7810@swjtu.edu.cn
RI Li, Ruoyu/HOC-2804-2023; Li, Roy/AAK-9800-2021
OI Li, Ruoyu/0000-0003-0754-2817; Li, Roy/0000-0002-2145-7590
FU National Natural Science Foundation of China [51275431]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Number 51275431).
CR Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   Cao JJ, 2011, COMPUT GRAPH-UK, V35, P733, DOI 10.1016/j.cag.2011.03.026
   Chen LS, 2011, IEICE T INF SYST, VE94D, P1289, DOI 10.1587/transinf.E94.D.1289
   Congote J, 2010, COMM COM INF SC, V68, P35, DOI 10.1007/978-3-642-11840-1_3
   Custodio Lis, 2019, Journal of the Brazilian Computer Society, V25, DOI 10.1186/s13173-019-0086-6
   Desbrun, 2007, P 5 EUROGRAPHICS S G, V7, P39
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618522
   Jakob J, 2019, COMPUT GRAPH FORUM, V38, P163, DOI 10.1111/cgf.13797
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Lee TY, 2001, COMPUT MED IMAG GRAP, V25, P405, DOI 10.1016/S0895-6111(00)00084-7
   Liu J, 2014, COMPUT AIDED DESIGN, V55, P26, DOI 10.1016/j.cad.2014.05.006
   Liu SJ, 2013, VISUAL COMPUT, V29, P627, DOI 10.1007/s00371-013-0801-9
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Masala GL, 2013, COMPUT PHYS COMMUN, V184, P777, DOI 10.1016/j.cpc.2012.09.030
   Mehra R, 2010, COMPUT GRAPH-UK, V34, P219, DOI 10.1016/j.cag.2010.03.002
   Mura C, 2018, VISUAL COMPUT, V34, P961, DOI 10.1007/s00371-018-1542-6
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Ohtake Y, 2003, ACM T GRAPHIC, V22, P463, DOI 10.1145/882262.882293
   Ohtake Y, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P153
   Seversky LM, 2011, COMPUT GRAPH-UK, V35, P492, DOI 10.1016/j.cag.2011.03.012
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Tsuzuki Mde SG Sato AK Ueda EK Martins Tde C Takimoto RY, 2017, VISUAL COMPUT, V34, P1
   Wang J, 2012, VISUAL COMPUT, V28, P163, DOI 10.1007/s00371-011-0607-6
   Wang MN, 2020, J MECH MED BIOL, V20, DOI 10.1142/S0219519420400023
   Weiss V, 2002, COMPUT AIDED GEOM D, V19, P19, DOI 10.1016/S0167-8396(01)00086-3
   Xia SB, 2020, IEEE J-STARS, V13, P685, DOI 10.1109/JSTARS.2020.2969119
   Xie H, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P91, DOI 10.1109/VISUAL.2003.1250359
   Xu MF, 2018, COMPUT GRAPH-UK, V75, P36, DOI 10.1016/j.cag.2018.06.002
   Zhang J, 2019, IEEE T VIS COMPUT GR, V25, P1693, DOI 10.1109/TVCG.2018.2827998
NR 35
TC 2
Z9 2
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2022
VL 38
IS 6
BP 1945
EP 1956
DI 10.1007/s00371-021-02258-4
EA AUG 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1C6YN
UT WOS:000689653900001
DA 2024-07-18
ER

PT J
AU Li, HT
   Todd, Z
   Bielski, N
   Carroll, F
AF Li, Heyang Thomas
   Todd, Zachary
   Bielski, Nikolas
   Carroll, Felix
TI 3D lidar point-cloud projection operator and transfer machine learning
   for effective road surface features detection and segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Machine learning; Object segmentation; Object classification; Geometric
   parameter estimation; Road segmentation; Deep neural network
ID EXTRACTION; MARKINGS; CLASSIFICATION; FRAMEWORK; NETWORK
AB The classification and extraction of road markings and lanes are of critical importance to infrastructure assessment, planning and road safety. We present a pipeline for the accurate segmentation and extraction of rural road surface objects in 3D lidar point-cloud, as well as a method to extract geometric parameters belonging to tar seal. To decrease the computational resources needed, the point-clouds were aggregated into a 2D image space before being transformed using affine transformations. The Mask R-CNN algorithm is then applied to the transformed image space to localize, segment and classify the road objects. The segmentation results for road surfaces and markings can then be used for geometric parameter estimation such as road widths estimation, while the segmentation results show that the efficacy of the existing Mask R-CNN to segment needle-type objects is improved by our proposed transformations.
C1 [Li, Heyang Thomas; Todd, Zachary; Bielski, Nikolas; Carroll, Felix] Univ Canterbury, Coll Engn, Sch Math & Stat, Christchurch 8041, New Zealand.
C3 University of Canterbury
RP Li, HT (corresponding author), Univ Canterbury, Coll Engn, Sch Math & Stat, Christchurch 8041, New Zealand.
EM thomas.li@canterbury.ac.nz
RI Bielski, Nikolas/HLP-9126-2023
OI Bielski, Nikolas/0000-0001-8493-4908; Li, Heyang/0000-0001-9794-5075
FU KiwiNet
FX Thomas Li has received funding from KiwiNet for this work. The dataset
   used for this research was given by NZTA. The Titan Xp GPU used for this
   research was donated by the NVIDIA Corporation. We thank Adrian Busch,
   David Humm and the Research and Innovation team for providing support.
   We are also grateful to Phil Davies and Nicholas Steyn for providing
   useful discussion. Other than those acknowledgements noted.
CR Abass, 2016, COMPUT INF SCI, DOI 10.5539/cis.v5n1p97
   BADRINARAYANAN V, 1998, IEEE T PATTERN ANAL
   Chow M., 2017, SCIENCE, V11, P50
   Chunpeng Wu, 2016, IET Cyber-Physical Systems: Theory & Applications, V1, P78, DOI 10.1049/iet-cps.2016.0027
   Danescu Radu, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P433, DOI 10.1109/ITSC.2010.5625261
   Dutta Abhishek, 2019, MM '19: Proceedings of the 27th ACM International Conference on Multimedia, P2276, DOI 10.1145/3343031.3350535
   Gargoum SA, 2018, AUTOMAT CONSTR, V95, P260, DOI 10.1016/j.autcon.2018.08.015
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonzalez-Jorge H., 2012, SURVEYING ROAD SLOPE, DOI 10.22260/isarc2012/0015
   Guan HY, 2014, ISPRS J PHOTOGRAMM, V87, P93, DOI 10.1016/j.isprsjprs.2013.11.005
   He K., MASK R CNNCITE ARXIV
   Holgado-Barco A, 2014, ISPRS J PHOTOGRAMM, V96, P28, DOI 10.1016/j.isprsjprs.2014.06.017
   Husain A, 2019, REMOTE SENS APPL, V13, P375, DOI 10.1016/j.rsase.2018.12.007
   Jung J, 2019, ISPRS J PHOTOGRAMM, V147, P1, DOI 10.1016/j.isprsjprs.2018.11.012
   Jung JY, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7110276
   Kheyrollahi A, 2012, MACH VISION APPL, V23, P123, DOI 10.1007/s00138-010-0289-5
   KIM J, 2016, ELSEVIER INT NEURAL, V87, P2555
   Kumar P, 2014, INT J APPL EARTH OBS, V32, P125, DOI 10.1016/j.jag.2014.03.023
   LEI G, 2017, COMPUT ELECTR ENG, V70, P895
   MA L, 2018, REMOTE SENS-BASEL, V10, P9
   Microsoft, 2018, SAT IM SEGM SUST FAR
   NZTA, 2010, MAN TRAFF SIGNS MA 1
   NZTA, 2008, TRAFF CONTR DEV MAN
   Soilán M, 2017, ISPRS J PHOTOGRAMM, V123, P94, DOI 10.1016/j.isprsjprs.2016.11.011
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Waleed A, 2017, GITHUB REPOSITORY
   Wang HY, 2015, IEEE GEOSCI REMOTE S, V12, P2085, DOI 10.1109/LGRS.2015.2449074
   Wang J, 2015, INT J REMOTE SENS, V36, P3144, DOI 10.1080/01431161.2015.1054049
   Wen CL, 2019, ISPRS J PHOTOGRAMM, V147, P178, DOI 10.1016/j.isprsjprs.2018.10.007
   Yadav M., 2018, REMOTE SENS APPL, V10, P18, DOI [10.1016/j.rsase.2018.02.003, DOI 10.1016/J.RSASE.2018.02.003]
   Yang MM, 2018, OPT LASER TECHNOL, V107, P192, DOI 10.1016/j.optlastec.2018.05.027
   YUAN C, 2018, IEEE ACCESS, V4, P2169
NR 32
TC 21
Z9 22
U1 3
U2 39
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2022
VL 38
IS 5
BP 1759
EP 1774
DI 10.1007/s00371-021-02103-8
EA JUN 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0M9JP
UT WOS:000667600400001
OA hybrid
DA 2024-07-18
ER

PT J
AU Soma, P
   Jatoth, RK
AF Soma, Prathap
   Jatoth, Ravi Kumar
TI An efficient and contrast-enhanced video de-hazing based on transmission
   estimation using HSL color model
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time image; video de-hazing; HSL color model; Raspberry Pi3; Jetson
   Nano; Transmission map
ID IMAGE
AB This paper proposed a fast and efficient video de-hazing system with reduced computational complexity for real-time computer vision applications. Video de-hazing is an important task and extensively researched in image/video processing and computer vision. The proposed method initially developed and verified for single images and later extended for real-time video's. The first key aspect of the proposed method is estimating the accurate transmission map using the hue, saturation, and light color model together with red, green, and blue color space. The second relevant aspect is preserving the edges and avoiding halos and artifacts by employing the median of pixels. These aspects reduce the number of computations. It does not require the most computationally complex step of refine transmission map. The advantage of this method is evaluated with five existing classical methods in terms of the average time constant (ATC), peak signal-to-noise ratio, percentage of haze improvement, average contrast of the output image, mean square error and structural similarity index. The comparative experiment shows that the proposed method is two times faster than the existing methods. The qualitative and quantitative analysis demonstrated that the proposed method can attain better de-hazing results and can be efficiently used for real-time video de-hazing applications. Based on comparative analysis, we mapped the proposed method on Raspberry Pi3 and Jetson Nano (GPU) with 24 fps (frames per second) without noticeable delay from input to output and demonstrated for the real-time video.
C1 [Soma, Prathap; Jatoth, Ravi Kumar] Natl Inst Technol, Dept ECE, Warangal 506004, Andhra Pradesh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Warangal
RP Soma, P (corresponding author), Natl Inst Technol, Dept ECE, Warangal 506004, Andhra Pradesh, India.
EM prathap.soma@student.nitw.ac.in; ravikumar@nitw.ac.in
RI Jatoth, Ravi Kumar/GLV-6625-2022
OI Jatoth, Ravi Kumar/0000-0002-7634-6951; Soma,
   Prathap/0000-0003-2350-4458
FU Science and Engineering Research Board (SERB) India [EEQ/2016/000556]
FX This work is supported by Science and Engineering Research Board (SERB)
   India, under the Grant of EEQ/2016/000556
CR Ancuti CO, 2011, LECT NOTES COMPUT SC, V6493, P501
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], 2002, NEW METRIC QUALITY A
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cass S, 2020, IEEE SPECTRUM, V57, P14, DOI 10.1109/MSPEC.2020.9126102
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Ju MY, 2017, VISUAL COMPUT, V33, P1613, DOI 10.1007/s00371-016-1305-1
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim JH, 2011, INT CONF ACOUST SPEE, P1273
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Preetham AJ, 1999, COMP GRAPH, P91, DOI 10.1145/311535.311545
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Singh, 2016, 2016 5 INT C WIR NET
   Soma P, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-2254-x
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Umesh, 2012, CSI COMMUNICATIONS
   Wallace, 2012, GETTING STARTED RASP
   Wang Z, 2014, COMPUT ELECTR ENG, V40, P785, DOI 10.1016/j.compeleceng.2013.06.009
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yang JC, 2017, J REAL-TIME IMAGE PR, V13, P479, DOI 10.1007/s11554-017-0671-x
   ZHU H, 2019, IEEE T CYBERN
   Zhu MZ, 2018, IEEE SIGNAL PROC LET, V25, P174, DOI 10.1109/LSP.2017.2780886
NR 31
TC 7
Z9 7
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2569
EP 2580
DI 10.1007/s00371-021-02132-3
EA APR 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000642836600001
DA 2024-07-18
ER

PT J
AU Baiju, PS
   Antony, SL
   George, SN
AF Baiju, P. S.
   Antony, Sherin Lisa
   George, Sudhish N.
TI An intelligent framework for transmission map estimation in image
   dehazing using total variation regularized low-rank approximation
SO VISUAL COMPUTER
LA English
DT Article
DE Single-image dehazing; Scene transmission estimation; Low-rank
   approximation; Split Bregman method
ID CONTRAST ENHANCEMENT; HISTOGRAM EQUALIZATION; COLOR; RETINEX;
   RESTORATION; LIGHTNESS; VISION; GRAY
AB The presence of haze affects a multitude of applications that require detection of image features, such as target tracking, object recognition and camera-based advanced driving assistance systems. In this paper, an optimization framework is proposed to efficiently estimate the scene transmission map which aids the dehazing process in an effective manner. In the formulated optimization model, low-rank approximation using weighted nuclear norm minimization is introduced to smoothen the coarse transmission map obtained from hazy data in order to avoid the visual artifacts in the dehazed image. Total variation regularization is employed to preserve the prominent edges and salient structural details in the transmission map. Moreover, the inclusion of l(1) norm minimization helps to obtain a finer transmission map by enhancing the minute sparse structural details, thereby providing good dehazing results. The beauty of the proposed model is confined in the efficient formulation of a unified optimization model for the estimation of transmission map with fine-tuned regularization terms which is not reported until now in the direction of image dehazing. The extensive experiments prove that the proposed method surpasses the state-of-the-art methods in image dehazing.
C1 [Baiju, P. S.; George, Sudhish N.] Natl Inst Technol Calicut, Dept Elect & Commun Engn, Calicut, Kerala, India.
   [Antony, Sherin Lisa] Capgemini Technol Serv India Ltd, A5 Co, Bangalore, Karnataka, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut
RP Baiju, PS (corresponding author), Natl Inst Technol Calicut, Dept Elect & Commun Engn, Calicut, Kerala, India.
EM baiju_p170023ec@nitc.ac.in; sherin-lisa.antony@capgemini.com;
   sudhish@nitc.ac.in
CR Ancuti Codruta O., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P501, DOI 10.1007/978-3-642-19309-5_39
   Ancuti C, 2018, IEEE COMPUT SOC CONF, P1004, DOI 10.1109/CVPRW.2018.00134
   Baiju PS, 2018, NATL CONF COMMUN
   Berthold KP, 1974, Comput. Graph. Image Process., V3, P277, DOI [DOI 10.1016/0146-664X(74)90022-7, 10.1016/0146-664X(74)90022-7]
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bouwmans T, 2014, Background modeling and foreground detection for video surveillance
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   DALEJONES R, 1993, PATTERN RECOGN, V26, P1373, DOI 10.1016/0031-3203(93)90143-K
   Deng XY, 2019, IEEE ACCESS, V7, P114297, DOI 10.1109/ACCESS.2019.2936029
   Fan Guo, 2013, International Journal of Digital Content Technology and its Applications, V7, P19, DOI 10.4156/jdcta.vol7.issue1.3
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Feng C, 2013, IEEE IMAGE PROC, P2363, DOI 10.1109/ICIP.2013.6738487
   Gibson KB, 2013, IEEE IMAGE PROC, P714, DOI 10.1109/ICIP.2013.6738147
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hautiere N, 2007, PROC CVPR IEEE, P2216
   He JX, 2016, IEEE IMAGE PROC, P2246, DOI 10.1109/ICIP.2016.7532758
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2013, ENG APPL ARTIF INTEL, V26, P1487, DOI 10.1016/j.engappai.2012.11.011
   HURLBERT A, 1986, J OPT SOC AM A, V3, P1684, DOI 10.1364/JOSAA.3.001684
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2008, AEROSP CONF PROC, P2322
   Kamila Narendra Kumar, 2015, HDB RES EMERGING PER
   Keng Zhang, 2013, ICTIS 2013. Improving Multimodal Transportation Systems - Information, Safety and Integration. Proceedings of the Second International Conference on Transportation Information and Safety, P861
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Koschmieder H., 1924, Beitraege Phys. Atmosp., P33
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1983, P NATL ACAD SCI USA, V80, P5163, DOI 10.1073/pnas.80.16.5163
   Lee C., 2020, ARXIV PREPRINT ARXIV
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li JJ, 2018, IEEE ACCESS, V6, P26831, DOI 10.1109/ACCESS.2018.2833888
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Liu SL, 2019, 2019 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ENVIRONMENT, ENERGY AND APPLICATIONS (IAEA 2019), P1, DOI [10.1145/3323716.3323717, 10.1109/globalsip45357.2019.8969491]
   Liu Z, 2019, IEEE SIGNAL PROC LET, V26, P833, DOI 10.1109/LSP.2019.2910403
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Meng GD, 2013, IEEE INT C SOL DIEL, P662, DOI 10.1109/ICSD.2013.6619828
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   QI, 2020, DEFOGGING ALGORITHM
   Rahman ZU, 1996, P SOC PHOTO-OPT INS, V2847, P183, DOI 10.1117/12.258224
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Salazar-Colores S, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0447-2
   Schaul L, 2009, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2009.5413700
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Seow MJ, 2006, NEUROCOMPUTING, V69, P954, DOI 10.1016/j.neucom.2005.07.003
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tang QF, 2021, COMPUT VIS IMAGE UND, V202, DOI 10.1016/j.cviu.2020.103086
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Verma M, 2015, PROCEEDING OF THE THIRD INTERNATIONAL SYMPOSIUM ON WOMEN IN COMPUTING AND INFORMATICS (WCI-2015), P426, DOI 10.1145/2791405.2791513
   Wang WC, 2017, IEEE-CAA J AUTOMATIC, V4, P410, DOI 10.1109/JAS.2017.7510532
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang YK, 2014, IEEE T IMAGE PROCESS, V23, P4826, DOI 10.1109/TIP.2014.2358076
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xu Y, 2016, IEEE ACCESS, V4, P165, DOI 10.1109/ACCESS.2015.2511558
   Xuan, 2010, J COMPUT AIDED DES C, V6
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yu J, 2010, INT CONF SIGN PROCES, P1048, DOI 10.1109/ICOSP.2010.5655901
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang Y, 2020, INT J ADV ROBOT SYST, V17, DOI 10.1177/1729881420961643
   Zhu L, 2017, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2017.60
   Zhu L, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.13019
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 73
TC 4
Z9 4
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2357
EP 2372
DI 10.1007/s00371-021-02117-2
EA APR 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000640468900001
DA 2024-07-18
ER

PT J
AU Wang, CM
   He, C
   Xu, MF
AF Wang, Chunmeng
   He, Chen
   Xu, Minfeng
TI Fast exposure fusion of detail enhancement for brightest and darkest
   regions
SO VISUAL COMPUTER
LA English
DT Article
DE High dynamic range image; Exposure fusion; Detail enhancement; Fast
   local Laplacian filtering
AB Multi-exposure fusion is the common approach to generate high dynamic range (HDR) images that combines multi-exposure images captured for the same scene, but the traditional multi-exposure fusion algorithms lose details in the brightest and darkest regions of the scene. Therefore, many detail enhancement-based exposure fusion algorithms have been proposed to extract these details. However, these algorithms have low efficiency because of the complexity of detail enhancement mechanism, and most of them excessively enhance all the pixels besides of the necessary brightest and darkest pixels. We propose a local detail enhancement mechanism to enhance only the details of brightest and darkest regions by using fast local Laplacian filtering (FLLF). A large number of experiments show that the proposed algorithm has much more high efficiency than the current detail enhancement-based exposure fusion algorithms, and the brightest and darkest details in the high dynamic range scene are preserved well.
C1 [Wang, Chunmeng] Jinling Inst Technol, Sch Comp Engn, Nanjing 211169, Jiangsu, Peoples R China.
   [He, Chen] Weifang Univ, Media & Commun Coll, Weifang 261061, Shandong, Peoples R China.
   [Xu, Minfeng] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Shandong, Peoples R China.
C3 Jinling Institute of Technology; Weifang University; Shandong University
   of Finance & Economics
RP Wang, CM (corresponding author), Jinling Inst Technol, Sch Comp Engn, Nanjing 211169, Jiangsu, Peoples R China.
EM wchm87@jit.edu.cn
RI Chen, Xupeng/KFA-5959-2024; liang, shuang/JOK-5869-2023; He,
   Chen/JLM-5059-2023
OI Xu, Minfeng/0000-0002-6553-5191
FU Project of High-level Talents Research Foundation of Jinling Institute
   of Technology [jit-b201802]; General Program of Natural Science
   Foundation of the Jiangsu Higher Education Institutions of China
   [19KJB520007]
FX This study was funded by the Project of High-level Talents Research
   Foundation of Jinling Institute of Technology (No. jit-b201802), the
   General Program of Natural Science Foundation of the Jiangsu Higher
   Education Institutions of China (No. 19KJB520007).
CR Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chalmers, 2020, VISUAL COMPUT, V17, P1
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kou F, 2018, J VIS COMMUN IMAGE R, V53, P235, DOI 10.1016/j.jvcir.2018.03.020
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P5805, DOI 10.1109/TIP.2020.2987133
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ZG, 2012, IEEE T IMAGE PROCESS, V21, P4672, DOI 10.1109/TIP.2012.2207396
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Nejati M, 2017, IEEE IMAGE PROC, P2234, DOI 10.1109/ICIP.2017.8296679
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Raman S., 2009, EUROGRAPHICS, P1
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Shen JB, 2012, VISUAL COMPUT, V28, P463, DOI 10.1007/s00371-011-0642-3
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Song ML, 2012, IEEE T IMAGE PROCESS, V21, P341, DOI 10.1109/TIP.2011.2157514
   Zhang W, 2010, PROC CVPR IEEE, P530, DOI 10.1109/CVPR.2010.5540168
NR 25
TC 11
Z9 13
U1 5
U2 41
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1233
EP 1243
DI 10.1007/s00371-021-02079-5
EA FEB 2021
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000619688600001
DA 2024-07-18
ER

PT J
AU Mohsin, N
   Payandeh, S
AF Mohsin, Nasreen
   Payandeh, Shahram
TI Clustering and Identification of key body extremities through
   topological analysis of multi-sensors 3D data
SO VISUAL COMPUTER
LA English
DT Article
DE Geodesic mapping; Key body part identification; Topological clustering;
   Pose recognition
ID SEGMENTATION
AB This paper presents a framework of a marker-less human pose recognition system by identifying key body extremity parts through a network of calibrated low-cost depth sensors. The usage of depth sensors overcomes challenges related to low illuminations which usually compromises the information from the RGB cameras. Furthermore, the addition of multiple depth sensors complements the existing information with more visibility and less self-occlusion. A simple algorithm was applied which finds the connections between aligned and updated meshes produced from multiple sensors. These connections help to fuse the meshes into one large geodesic graph network. On this graph, a novel algorithm is applied to identify key body extremities such as head, hands, and feet of a human subject. A geodesic mapping is applied to the fused point cloud to produce a set of distinct topological clusters of 3D points. These clusters generate a hierarchical skeleton tree graph (Reeb graph) and produce a set of features for semantic identification of key body extremities. The combination of both the shape model and semantic classification finally leads to pose recognition. The paper presents the assessment of the proposed framework and its comparison with another available technique in a succession of experimental configurations.
C1 [Mohsin, Nasreen; Payandeh, Shahram] Simon Fraser Univ, Sch Engn Sci, Networked Robot & Sensing Lab, Burnaby, BC, Canada.
C3 Simon Fraser University
RP Mohsin, N (corresponding author), Simon Fraser Univ, Sch Engn Sci, Networked Robot & Sensing Lab, Burnaby, BC, Canada.
EM nmohsin@sfu.com; shahram_payandeh@sfu.com
RI Payandeh, Shahram/IZQ-1865-2023
OI Payandeh, Shahram/0000-0001-6846-7289; MOHSIN,
   NASREEN/0000-0001-8917-9286
CR [Anonymous], 2015, LIBFREENECT2 OPEN SO
   [Anonymous], 2018, AIAA J
   Auvinet E., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P478, DOI 10.1109/ISSPA.2012.6310598
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Brandao A, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P367
   Carpenter J, 1999, IEE P-RADAR SON NAV, V146, P2, DOI 10.1049/ip-rsn:19990255
   Chen LL, 2013, PATTERN RECOGN LETT, V34, P1995, DOI 10.1016/j.patrec.2013.02.006
   Chi-Hung Chuang, 2012, 2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), P367, DOI 10.1109/IIH-MSP.2012.95
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Dijkstra EW., 1959, NUMER, V1, P269, DOI 10.1007/BF01386390
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Ganapathi V, 2012, LECT NOTES COMPUT SC, V7577, P738, DOI 10.1007/978-3-642-33783-3_53
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Grest D, 2005, LECT NOTES COMPUT SC, V3663, P285
   Handrich S, 2015, LECT NOTES COMPUT SC, V9386, P287, DOI 10.1007/978-3-319-25903-1_25
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hong S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113865
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Kanazawa Angjoo., 2019, Learning 3D Human Dynamics from Video
   Karmakar N, 2016, THEOR COMPUT SCI, V624, P25, DOI 10.1016/j.tcs.2015.11.013
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Mohsin N, 2017, IEEE SYS MAN CYBERN, P2736, DOI 10.1109/SMC.2017.8123040
   Natali M, 2011, GRAPH MODELS, V73, P151, DOI 10.1016/j.gmod.2011.03.002
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Park S, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P105, DOI 10.1109/MOTION.2002.1182221
   Phan A, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P567, DOI 10.1109/MVA.2015.7153256
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Pons-Moll G, 2010, PROC CVPR IEEE, P663, DOI 10.1109/CVPR.2010.5540153
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Saha S., 2016, MICROCHANNEL PHASE C, P1
   Schwarz LA, 2012, IMAGE VISION COMPUT, V30, P217, DOI 10.1016/j.imavis.2011.12.001
   Shafaei A, 2016, 2016 13TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P24, DOI 10.1109/CRV.2016.25
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Shuai L, 2017, IEEE T VIS COMPUT GR, V23, P1085, DOI 10.1109/TVCG.2016.2520926
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Thome N., 2006, Video and Signal Based Surveillance, P38
   Tierny J, 2008, VISUAL COMPUT, V24, P155, DOI 10.1007/s00371-007-0181-0
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Werghi N, 2006, IEEE T SYST MAN CY B, V36, P153, DOI 10.1109/TSMCB.2005.854503
   Wu, 2019, VISUAL COMPUT, P1
   Xiao YJ, 2004, INT C PATT RECOG, P131, DOI 10.1109/ICPR.2004.1334486
   YANG H, 2018, MATH PROBL ENG
   Zarchan P., 2000, PROGR ASTRONAUT AERO
   Zhang HB, 2016, INTELL AUTOM SOFT CO, V22, P483, DOI 10.1080/10798587.2015.1095419
   Zhang WH, 2019, J VIS COMMUN IMAGE R, V59, P272, DOI 10.1016/j.jvcir.2019.01.028
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
NR 50
TC 3
Z9 3
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 1097
EP 1120
DI 10.1007/s00371-021-02070-0
EA FEB 2021
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000618961200001
DA 2024-07-18
ER

PT J
AU Huang, YB
   Qing, LB
   Xu, SY
   Wang, L
   Peng, YH
AF Huang, Yibo
   Qing, Linbo
   Xu, Shengyu
   Wang, Lu
   Peng, Yonghong
TI HybNet: a hybrid network structure for pain intensity estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Pain intensity estimation; Deep learning; Convolutional neural network;
   1D; 2D; 3D convolution; Facial landmark
ID RECOGNITION
AB Automatic pain intensity estimation has great potential in current rehabilitation medicine, and patients' health status information can be obtained through the analysis of facial images. At present, deep convolutional neural networks (CNNs) have made great progress in many fields, including natural language processing, image classification and action recognition. Motivated by the current achievements, a novel end-to-end hybrid network is proposed to extract multidimensional features from image sequences, which is composed of 3D convolution, 2D convolution and 1D convolution. Specifically, the 3D convolutional neural network (3D CNN) is designed to capture the spatiotemporal features, and the 2D convolutional neural network (2D CNN) is designed to capture the spatial features, while the 1D convolutional neural network (1D CNN) is mainly used to capture the geometric information from facial landmarks. Finally, the features obtained by the three different networks are fused together for regression. The proposed HybNet is evaluated on UNBC-McMaster Shoulder Pain Expression Archive Database, and the experimental results show that it can effectively extract the discriminative high-level features and can achieve competitive performance with the state-of-the-art methods.
C1 [Huang, Yibo; Qing, Linbo; Xu, Shengyu; Wang, Lu] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
   [Peng, Yonghong] Manchester Metropolitan Univ, Dept Comp & Math, Manchester M15 6BH, Lancs, England.
C3 Sichuan University; Manchester Metropolitan University
RP Qing, LB (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
EM qing_lb@scu.edu.cn
RI Peng, Yonghong/ABD-5633-2021; Xu, Shengyu/AFN-8288-2022
OI Xu, Shengyu/0000-0002-4415-8542
FU National Natural Science Foundation of China [61871278]
FX This work was supported by the National Natural Science Foundation of
   China under the grant 61871278.
CR Adibuzzaman M, 2015, P INT COMP SOFTW APP, P726, DOI 10.1109/COMPSAC.2015.150
   Alharbi S, 2017, IEEE IPCCC, DOI 10.1109/TCYB.2017.2662199
   Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007
   Brahnam S, 2007, STUD COMPUT INTELL, V48, P225
   Brown JE, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024124
   Chen JX, 2012, IEEE IMAGE PROC, P2621, DOI 10.1109/ICIP.2012.6467436
   Ekman P, 1978, CONSULT PSYCHOL PRES, V12, P274
   Florea C, 2015, LECT NOTES COMPUT SC, V8927, P778, DOI 10.1007/978-3-319-16199-0_54
   Gholami B, 2009, IEEE ENG MED BIO, P2176, DOI 10.1109/IEMBS.2009.5332437
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hammal Z, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P47, DOI 10.1145/2388676.2388688
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong XP, 2016, NEUROCOMPUTING, V184, P99, DOI 10.1016/j.neucom.2015.07.134
   Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36
   Khan R. A., 2013, 2013 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI [10.1109/ICME.2013.6607608, DOI 10.1109/ICME.2013.6607608]
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Kingma D. P., 2014, arXiv
   Liu ZY, 2020, VISUAL COMPUT, V36, P843, DOI 10.1007/s00371-019-01659-w
   Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462
   Lucey Patrick, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P12, DOI 10.1109/CVPR.2009.5204279
   Lucey Patrick, 2009, Int Conf Affect Comput Intell Interact Workshops, V2009, P1
   Lucey P, 2011, IEEE T SYST MAN CY B, V41, P664, DOI 10.1109/TSMCB.2010.2082525
   Mauricio A, 2019, LECT NOTES COMPUT SC, V11754, P295, DOI 10.1007/978-3-030-34995-0_27
   Neshov N, 2015, INT WORKSH INT DATA, P251, DOI 10.1109/IDAACS.2015.7340738
   Pedersen H, 2015, LECT NOTES COMPUT SC, V9163, P128, DOI 10.1007/978-3-319-20904-3_12
   Rathee N, 2015, J VIS COMMUN IMAGE R, V33, P247, DOI 10.1016/j.jvcir.2015.09.007
   Roy S., 2007, STOIC: A database of dynamic and static faces expressing highly recognizable emotions
   Rudovic O, 2013, LECT NOTES COMPUT SC, V8034, P234, DOI 10.1007/978-3-642-41939-3_23
   Schulz E, 2012, CEREB CORTEX, V22, P1118, DOI 10.1093/cercor/bhr186
   Tavakolian M, 2018, IEEE IMAGE PROC, P1952, DOI 10.1109/ICIP.2018.8451681
   Walter Steffen, 2013, 2013 IEEE International Conference on Cybernetics (CYBCO), P128, DOI 10.1109/CYBConf.2013.6617456
   Wu, 2019, VISUAL COMPUT, P1
   Xie S, 2018, PROC INT CONF RECON
   Zafar Z, 2014, INT C PATT RECOG, P4696, DOI 10.1109/ICPR.2014.803
   Zhang Y, 2018, PROC CVPR IEEE, P7034, DOI 10.1109/CVPR.2018.00735
   Zhao R, 2016, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2016.377
   Zhou J, 2016, IEEE COMPUT SOC CONF, P1535, DOI 10.1109/CVPRW.2016.191
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 38
TC 17
Z9 17
U1 4
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2022
VL 38
IS 3
BP 871
EP 882
DI 10.1007/s00371-021-02056-y
EA FEB 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZQ8YX
UT WOS:000614677500001
DA 2024-07-18
ER

PT J
AU Wang, HD
AF Wang, Huidi
TI On extended progressive and iterative approximation for least squares
   fitting
SO VISUAL COMPUTER
LA English
DT Article
DE The ELSPIA method; Convergence; Relaxation parameters; Least square
   fitting
ID B-SPLINE CURVE; INTERPOLATION; SURFACE; CONVERGENCE; BASES
AB The progressive and iterative approximation method for least square fitting (LSPIA) (Deng and Lin in Comput Aided Des 47:32-44, 2014) is an efficient method for fitting a large number of data points. By introducing the global and local relaxation parameters, we develop an extended LSPIA (ELSPIA) method which includes the LSPIA method as its special case. The ELSPIA method constructs the sequence of curves and surfaces by adjusting the control points with the outer and inner iteration. It is proved that the sequence of curves and surfaces converges to the least square fitting curve and surface, respectively, even when the collocation matrix is not of full column rank. The ELSPIA method is flexible to allow the local adjustment of the control points. Moreover, the convergence rate of the ELSPIA method can be faster than that of the LSPIA method under the same assumption. Numerical results verify this phenomenon.
C1 [Wang, Huidi] China Jiliang Univ, Hangzhou, Peoples R China.
C3 China Jiliang University
RP Wang, HD (corresponding author), China Jiliang Univ, Hangzhou, Peoples R China.
EM hdwang@cjlu.edu.cn
FU National Natural Science Foundation of China [11871430]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 11871430.
CR [Anonymous], 1971, ITERATIVE SOLUTION L
   Bermudez A. J., 1994, SAVMA Symposium 1994 Proceedings., P1
   Carnicer JM, 2011, COMPUT AIDED GEOM D, V28, P523, DOI 10.1016/j.cagd.2011.09.005
   Carnicer JM, 1996, J COMPUT APPL MATH, V71, P365, DOI 10.1016/0377-0427(95)00240-5
   Chen J, 2011, COMPUT AIDED DESIGN, V43, P889, DOI 10.1016/j.cad.2011.03.012
   Chen YL, 2005, APPL MATH COMPUT, V167, P930, DOI 10.1016/j.amc.2004.06.143
   Chen ZX, 2008, COMPUT GRAPH FORUM, V27, P1823, DOI 10.1111/j.1467-8659.2008.01328.x
   Cheng FH, 2009, J COMPUT SCI TECH-CH, V24, P39, DOI 10.1007/s11390-009-9199-2
   Cheng F, 2008, GEOMETRIC MODELING & IMAGING: MODERN TECHNIQUES AND APPLICATIONS, P27, DOI 10.1109/GMAI.2008.15
   Delgado J, 2007, COMPUT AIDED GEOM D, V24, P10, DOI 10.1016/j.cagd.2006.10.001
   Deng CY, 2014, COMPUT AIDED DESIGN, V47, P32, DOI 10.1016/j.cad.2013.08.012
   Deng CY, 2012, COMPUT AIDED DESIGN, V44, P424, DOI 10.1016/j.cad.2011.12.001
   Deng CY, 2010, VISUAL COMPUT, V26, P137, DOI 10.1007/s00371-009-0393-6
   Ebrahimi A, 2019, J COMPUT APPL MATH, V359, P1, DOI 10.1016/j.cam.2019.03.025
   [李莎莎 Li Shasha], 2019, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V31, P1574
   Lin HW, 2018, COMPUT AIDED DESIGN, V95, P40, DOI 10.1016/j.cad.2017.10.002
   Lin HW, 2013, SIAM J SCI COMPUT, V35, pA3052, DOI 10.1137/120888569
   Lin HW, 2011, COMPUT GRAPH-UK, V35, P967, DOI 10.1016/j.cag.2011.07.003
   Lin HW, 2010, COMPUT AIDED DESIGN, V42, P505, DOI 10.1016/j.cad.2010.01.006
   Lin HW, 2010, COMPUT AIDED GEOM D, V27, P322, DOI 10.1016/j.cagd.2010.01.003
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Lin HW, 2004, SCI CHINA SER F, V47, P315, DOI 10.1360/02yf0529
   Liu CZ, 2020, J COMPUT APPL MATH, V366, DOI 10.1016/j.cam.2019.112389
   Liu MZ, 2018, J COMPUT APPL MATH, V327, P175, DOI 10.1016/j.cam.2017.06.013
   Lu LZ, 2010, COMPUT AIDED GEOM D, V27, P129, DOI 10.1016/j.cagd.2009.11.001
   Maekawa T, 2007, COMPUT AIDED DESIGN, V39, P313, DOI 10.1016/j.cad.2006.12.008
   [史利民 SHI Limin], 2006, [数学研究与评论, Journal of Mathematical Research and Exposition], V26, P735
   Xie J, 2020, COMPUT AIDED GEOM D, V80, DOI 10.1016/j.cagd.2020.101867
   Xiong YH, 2012, COMPUT GRAPH-UK, V36, P884, DOI 10.1016/j.cag.2012.07.002
   Zhang L, 2018, J COMPUT APPL MATH, V329, P331, DOI 10.1016/j.cam.2017.05.034
   Zhang L, 2016, VISUAL COMPUT, V32, P1109, DOI 10.1007/s00371-015-1170-3
   [张蒙 Zhang Meng], 2020, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V32, P568
   Zheng B, 2009, LINEAR ALGEBRA APPL, V431, P808, DOI 10.1016/j.laa.2009.03.033
NR 33
TC 17
Z9 20
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 591
EP 602
DI 10.1007/s00371-020-02036-8
EA JAN 2021
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000604815000001
DA 2024-07-18
ER

PT J
AU Gonçalves, AR
   Muñoz, JE
   Gouveia, ÉR
   Cameirao, MD
   Badia, SBI
AF Goncalves, Afonso Rodrigues
   Munoz, John Edison
   Gouveia, Elvio Rubio
   Cameirao, Monica da Silva
   Badia, Sergi Bermudez i
TI Effects of prolonged multidimensional fitness training with exergames on
   the physical exertion levels of older adults
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Virtual Worlds and Games for Serious
   Applications (VS-Games)
CY SEP 05-07, 2018
CL Wurzburg, GERMANY
DE Exergames; Activity trackers; Perceived exertion; Multidimensional
   training; Older adults
AB While exergames have been used with older adults in an attempt to promote higher physical activity (PA) levels, and its subsequent health benefits, there is a lack of research that objectively quantifies the PA levels that custom-made exergames can produce throughout an extended training program. In this paper, we describe a 3-month intervention study that aimed to measure the participants PA levels during exergames' sessions and their effectiveness in eliciting the recommended activity levels. Over the course of the study, two groups of older adults participated in either a conventional multidimensional fitness training program of two sessions of exercise per week (n = 16) or in an equivalent combined program (n = 15), of one conventional and one exergame session per week. Both the objective PA levels (through accelerometry) and subjective effort (perceived exertion) were collected in each session. Results revealed that while participants spent more time in moderate-to-vigorous PA during exergaming than during conventional sessions, they also spent less energy, thus working out at lower intensities but for a more sustained amount of time. The self-reported exertion was consistently higher for the sessions of the combined exercise program. We showed that a set of custom-made exergames can be successfully used by trainers to set up personalized training sessions and can be used in combination with regular exercise for sustained long-term training, exposing differences between the two training regimes in terms of efficiency, elicited PA, and perceived effort.
C1 [Goncalves, Afonso Rodrigues; Gouveia, Elvio Rubio; Cameirao, Monica da Silva; Badia, Sergi Bermudez i] LARSyS & Madeira Interact Technol Inst M ITI, Funchal, Portugal.
   [Goncalves, Afonso Rodrigues; Cameirao, Monica da Silva; Badia, Sergi Bermudez i] Univ Madeira, Fac Ciencias Exatas & Engn, Funchal, Portugal.
   [Gouveia, Elvio Rubio] Univ Madeira, Fac Ciencias Sociais, Funchal, Portugal.
   [Munoz, John Edison] Univ Waterloo, Syst Design & Engn Dept, Waterloo, ON, Canada.
C3 Universidade da Madeira; Universidade da Madeira; University of Waterloo
RP Gonçalves, AR (corresponding author), LARSyS & Madeira Interact Technol Inst M ITI, Funchal, Portugal.; Gonçalves, AR (corresponding author), Univ Madeira, Fac Ciencias Exatas & Engn, Funchal, Portugal.
EM afonso.goncalves@m-iti.org; john.munoz.hci@uwaterloo.ca; erubiog@uma.pt;
   monica.cameirao@m-iti.org; sergi.bermudez@m-iti.org
RI Badia, Sergi Bermúdez i/C-8681-2018; Cameirao, Monica/C-8675-2018;
   Cameirao, Monica/P-2078-2019; Gouveia, Elvio/F-9156-2015; Cameirao,
   Monica/KQV-2089-2024
OI Badia, Sergi Bermúdez i/0000-0003-4452-0414; Cameirao,
   Monica/0000-0002-5352-0128; Cameirao, Monica/0000-0002-5352-0128;
   Gouveia, Elvio/0000-0003-0927-692X; Cameirao,
   Monica/0000-0002-5352-0128; Goncalves, Afonso/0000-0003-3196-2678
FU Fundação para a Ciência e a Tecnologia [CMUP-ERI/HCI/0046/2013] Funding
   Source: FCT
CR Aguilar-Farías N, 2014, J SCI MED SPORT, V17, P293, DOI 10.1016/j.jsams.2013.07.002
   [Anonymous], 2018, P 51 HAW INT C SYST
   Barnett A, 2016, BMC GERIATR, V16, DOI 10.1186/s12877-016-0380-5
   Bronner S, 2016, J SCI MED SPORT, V19, P267, DOI 10.1016/j.jsams.2015.03.003
   Bushman B., 2017, ACSM's Complete Guide to Fitness Health
   Chao YY, 2015, J AGING HEALTH, V27, P379, DOI 10.1177/0898264314551171
   Chaput JP, 2014, NUTR DIABETES, V4, DOI 10.1038/nutd.2014.14
   Chodzko-Zajko WJ, 2009, MED SCI SPORT EXER, V41, P1510, DOI 10.1249/MSS.0b013e3181a0c95c
   Chu AHY, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172535
   Chuang TY, 2006, PHYS THER, V86, P1369, DOI 10.2522/ptj.20050335
   Copeland JL, 2009, J AGING PHYS ACTIV, V17, P17, DOI 10.1123/japa.17.1.17
   DeSmet A, 2014, PREV MED, V69, P95, DOI 10.1016/j.ypmed.2014.08.026
   FOLSTEIN MF, 1975, J PSYCHIAT RES, V12, P189, DOI 10.1016/0022-3956(75)90026-6
   Goncalves A., 2017, ICSPORTS 2017
   Graves LEF, 2010, J PHYS ACT HEALTH, V7, P393, DOI 10.1123/jpah.7.3.393
   Guderian B, 2010, J SPORT MED PHYS FIT, V50, P436
   Heyward V., 2014, ADV FITNESS ASSESSME
   Höchsmann C, 2016, SPORTS MED, V46, P845, DOI 10.1007/s40279-015-0455-z
   Jones C.J., 2005, Physical activity instruction of older adults
   Keefe FJ, 2012, PAIN, V153, P2163, DOI 10.1016/j.pain.2012.05.030
   Klompstra LV, 2014, EUR J CARDIOVASC NUR, V13, P388, DOI 10.1177/1474515113512203
   Konstantinidis EI, 2016, IEEE J BIOMED HEALTH, V20, P189, DOI 10.1109/JBHI.2014.2378814
   Larsen LH, 2013, GAMES HEALTH J, V2, P205, DOI 10.1089/g4h.2013.0036
   Lee S, 2018, SIMULAT GAMING, V49, P538, DOI 10.1177/1046878118776043
   Maillot P, 2012, PSYCHOL AGING, V27, P589, DOI 10.1037/a0026268
   Munoz J., 2018, P 10 INT C VIRT WORL
   Muñoz JE, 2016, INT CONF GAMES VIRTU
   O'Leary KC, 2011, CLIN NEUROPHYSIOL, V122, P1518, DOI 10.1016/j.clinph.2011.01.049
   Oesch P, 2017, BMC GERIATR, V17, DOI 10.1186/s12877-017-0467-7
   Peng W, 2011, CYBERPSYCH BEH SOC N, V14, P681, DOI 10.1089/cyber.2010.0578
   Ravenek KE, 2016, DISABIL REHABIL-ASSI, V11, P445, DOI 10.3109/17483107.2015.1029538
   Rikli R., 2012, The Senior Fitness Test, V2nd ed.
   Simao H., 2017, Proceedings of the 5th International Congress on Sport Sciences Research and Technology Support, Funchal, Portugal, V1, P151, DOI DOI 10.5220/0006606601510162
   Sinclair J., 2009, P 6 AUSTR C INT ENT, P11
   Skjæret N, 2016, INT J MED INFORM, V85, P1, DOI 10.1016/j.ijmedinf.2015.10.008
   Sween J, 2014, J PHYS ACT HEALTH, V11, P864, DOI 10.1123/jpah.2011-0425
   Taylor L, 2018, GAMES HEALTH J, V7, P37, DOI 10.1089/g4h.2017.0084
   Taylor LM, 2012, ARCH PHYS MED REHAB, V93, P2281, DOI 10.1016/j.apmr.2012.03.034
   Uzor S, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2813, DOI 10.1145/2556288.2557160
   Velazquez A, 2017, PERVASIVE MOB COMPUT, V34, P60, DOI 10.1016/j.pmcj.2016.09.002
   Wu YZ, 2019, GERIATR GERONTOL INT, V19, P147, DOI 10.1111/ggi.13575
NR 41
TC 9
Z9 10
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 19
EP 30
DI 10.1007/s00371-019-01736-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA QE3UO
UT WOS:000616134400003
DA 2024-07-18
ER

PT J
AU Lyu, Q
   Chen, MH
   Chen, X
AF Lyu, Qing
   Chen, Minghao
   Chen, Xiang
TI Learning color space adaptation from synthetic to real images of cirrus
   clouds
SO VISUAL COMPUTER
LA English
DT Article
DE Color space; Synthetic-to-real; Domain adaptation; Cirrus clouds;
   Segmentation; Style transfer
ID CLASSIFICATION
AB Cloud segmentation plays a crucial role in image analysis for climate modeling. Manually labeling the training data for cloud segmentation is time-consuming and error-prone. We explore to train segmentation networks with synthetic data due to the natural acquisition of pixel-level labels. Nevertheless, the domain gap between synthetic and real images significantly degrades the performance of the trained model. We propose a color space adaptation method to bridge the gap, by training a color-sensitive generator and discriminator to adapt synthetic data to real images in color space. Instead of transforming images by general convolutional kernels, we adopt a set of closed-form operations to make color-space adjustments while preserving the labels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and demonstrate the adaptation efficacy on the semantic segmentation task of cirrus clouds. With our adapted synthetic data for training the semantic segmentation, we achieve an improvement of 6.59% when applied to real images, superior to alternative methods.
C1 [Lyu, Qing; Chen, Minghao; Chen, Xiang] Zhejiang Univ, State Key Lab CAD&CG, Zijingang Campus, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Chen, X (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Zijingang Campus, Hangzhou 310058, Zhejiang, Peoples R China.
EM lyuqing@zju.edu.cn; minghaochen01@gmail.com; xchen.cs@gmail.com
OI Chen, Xiang/0000-0002-6955-8729
FU National Natural Science Foundation of China [61772024, 61732016]
FX This study was funded by National Natural Science Foundation of China
   (Grant Nos. 61772024, 61732016).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2010, Proc. BMVC, DOI [10.5244/C.24.106, DOI 10.5244/C.24.106, 10.5244/C.24.106.]
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.470
   Bi L, 2018, VISUAL COMPUT, V34, P1043, DOI 10.1007/s00371-018-1519-5
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Bianco S, 2019, LECT NOTES COMPUT SC, V11418, P209, DOI 10.1007/978-3-030-13940-7_16
   Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chai Y, 2020, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV45572.2020.9093321
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Chollet F, 2015, KERAS
   Christodoulou CI, 2003, IEEE T GEOSCI REMOTE, V41, P2662, DOI 10.1109/TGRS.2003.815404
   Dev S., 2019, P IEEE INT S ANT PRO
   Dev S, 2017, IEEE J-STARS, V10, P231, DOI 10.1109/JSTARS.2016.2558474
   Dev S, 2015, IEEE IMAGE PROC, P636, DOI 10.1109/ICIP.2015.7350876
   Dianne G., 2019, P DIG IM COMP TECHN, P1
   Dobashi Y, 2010, COMPUT GRAPH FORUM, V29, P2083, DOI 10.1111/j.1467-8659.2010.01795.x
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gong JM, 2009, PROCEEDINGS OF THE ASME PRESSURE VESSELS AND PIPING CONFERENCE - 2008, VOL 6, PT A AND B, P69
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Huang H, 2010, VISUAL COMPUT, V26, P933, DOI 10.1007/s00371-010-0498-y
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jakob Wenzel, 2010, Mitsuba renderer
   Kingma D. P., 2014, arXiv
   Kolouri S, 2019, ADV NEUR IN, V32
   Kuanar S., 2019, ARXIV190200855
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liebelt J, 2010, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2010.5539836
   Limmer M, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P61, DOI [10.1109/ICMLA.2016.0019, 10.1109/ICMLA.2016.114]
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long M, 2015, AUST HUMANIT REV, P93
   Mahrooghy M, 2012, IEEE J-STARS, V5, P1356, DOI 10.1109/JSTARS.2012.2201449
   Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A., 2015, ARXIV
   Reed S, 2016, PR MACH LEARN RES, V48
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G., 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, P3234, DOI DOI 10.1109/CVPR.2016.352
   Sixt L., 2017, P INT C LEARN REPR I
   Snderby CasperKaae., 2017, ICLR
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Sun B., 2014, Proceedings of the British Machine Vision Conference 2014
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Tobin Josh, 2017, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P23, DOI 10.1109/IROS.2017.8202133
   Wang JH, 2017, VISUAL COMPUT, V33, P1587, DOI 10.1007/s00371-016-1302-4
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yuan F, 2014, 2014 INTERNATIONAL SYMPOSIUM ON ANTENNAS AND PROPAGATION (ISAP), P383, DOI 10.1109/ISANP.2014.7026691
   Yuan F, 2014, IEEE ANTENNAS PROP, P259, DOI 10.1109/APS.2014.6904461
   Yuanming Hu, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3181974
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 66
TC 1
Z9 1
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2341
EP 2353
DI 10.1007/s00371-020-01990-7
EA OCT 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000577249300001
DA 2024-07-18
ER

PT J
AU Qiu, YH
   Hao, PY
AF Qiu, Yunhao
   Hao, Pengyi
TI Self-supervised deep subspace clustering network for faces in videos
SO VISUAL COMPUTER
LA English
DT Article
DE Video face clustering; Prior knowledge; SDSCN
AB Video face clustering is a challenging task with wide applications. Unlike ordinary image clustering, faces in videos usually exist as a series of tracks, which provide prior knowledge. Specifically, faces from the same track are considered to be the same person while faces from the different tracks appearing in the same frame are considered to be different people. Based on this prior knowledge, we propose the self-supervised deep subspace clustering network (SDSCN). SDSCN adopts autoencoder to nonlinearly map the faces into latent space and adds the fully connected layer between the encoder and decoder to explore the self-expressiveness property. Prior knowledge is automatically incorporated into the loss function to guide the training. We further propose efficient training strategy for our network and clustering. The experiments on the two public datasets (BBT0101 and Notting-Hill) demonstrate the advantages of our method. Specifically, our method achieves about 3-17% improvement in clustering accuracy on BBT0101 and about 6-23% improvement on Notting-Hill compared to the state-of-the-art methods.
C1 [Qiu, Yunhao] Zhejiang Univ, Sch Math Sci, Hangzhou, Peoples R China.
   [Hao, Pengyi] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University of Technology
RP Hao, PY (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM qyhfxjy@gmail.com; haopy@zjut.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [LY18F020034];
   Natural Science Foundation of China [61801428]
FX The work was supported by Zhejiang Provincial Natural Science Foundation
   of China under Grant No. LY18F020034, and Natural Science Foundation of
   China under Grant No. 61801428.
CR Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Chapelle O, 2006, PROBABILISTIC SEMI S, P73
   Cinbis RG, 2011, IEEE I CONF COMP VIS, P1559, DOI 10.1109/ICCV.2011.6126415
   Du M, 2012, LECT NOTES COMPUT SC, V7578, P167, DOI 10.1007/978-3-642-33786-4_13
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y, 2011, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2011.5995500
   Ji P, 2014, IEEE WINT CONF APPL, P461, DOI 10.1109/WACV.2014.6836065
   Klein D., 2002, Tech. rep., P307
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CY, 2012, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTER VISION IN REMOTE SENSING, P34
   Mehto A., 2019, INT CONF COMPUT, P1, DOI DOI 10.1109/icccnt45670.2019.8944908
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Pan J., 2017, 2017 C WORKSH NEUR I
   Patel VM, 2015, IEEE J-STSP, V9, P691, DOI 10.1109/JSTSP.2015.2402643
   Patel VM, 2014, IEEE IMAGE PROC, P2849, DOI 10.1109/ICIP.2014.7025576
   Patel VM, 2013, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2013.35
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Pini S, 2019, MULTIMED TOOLS APPL, V78, P14007, DOI 10.1007/s11042-018-7040-z
   Roth M, 2012, INT C PATT RECOG, P1012
   Sharma V, 2019, INT CONF AWARE SCI, P353, DOI [10.1109/icawst.2019.8923204, 10.1109/CLEOE-EQEC.2019.8871743]
   Wang SP, 2016, IEEE CONF COMPUT
   Wu BY, 2013, PROC CVPR IEEE, P3507, DOI 10.1109/CVPR.2013.450
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xiao SJ, 2014, LECT NOTES COMPUT SC, V8694, P123, DOI 10.1007/978-3-319-10599-4_9
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zhang YF, 2016, IEEE T IMAGE PROCESS, V25, P5780, DOI 10.1109/TIP.2016.2601491
   Zhang ZP, 2016, LECT NOTES COMPUT SC, V9907, P236, DOI 10.1007/978-3-319-46487-9_15
NR 28
TC 8
Z9 8
U1 1
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2253
EP 2261
DI 10.1007/s00371-020-01984-5
EA OCT 2020
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000575724700001
DA 2024-07-18
ER

PT J
AU Zhang, W
   Lin, ZY
   Cheng, J
   Ma, CX
   Deng, XM
   Wang, HG
AF Zhang, Wei
   Lin, Zeyi
   Cheng, Jian
   Ma, Cuixia
   Deng, Xiaoming
   Wang, Hongan
TI STA-GCN: two-stream graph convolutional network with spatial-temporal
   attention for hand gesture recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Hand gesture recognition; Graph convolutional network; Spatial-temporal
   attention
AB Skeleton-based hand gesture recognition is an active research topic in computer graphics and computer vision and has a wide range of applications in VR/AR and robotics. Although the spatial-temporal graph convolutional network has been successfully used in skeleton-based hand gesture recognition, these works often use a fixed spatial graph according to the hand skeleton tree or use a fixed graph on the temporal dimension, which may not be optimal for hand gesture recognition. In this paper, we propose a two-stream graph attention convolutional network with spatial-temporal attention for hand gesture recognition. We adopt pose stream and motion stream as the two input streams for our network. In pose stream, we use the joint in each frame as the input; In motion stream, we use the joint offsets between neighboring frames as the input. We propose a new temporal graph attention module to model the temporal dependency and also use a spatial graph attention module to construct dynamic skeleton graph. For each stream, we adopt graph convolutional network with spatial-temporal attention to extract the features. Then, we concatenate the feature of the pose stream and motion stream for gesture recognition. We achieve the competitive performance on the main hand gesture recognition benchmark datasets, which demonstrates the effectiveness of our method.
C1 [Zhang, Wei; Lin, Zeyi; Cheng, Jian; Ma, Cuixia; Deng, Xiaoming; Wang, Hongan] Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing 100190, Peoples R China.
   [Zhang, Wei; Lin, Zeyi; Cheng, Jian; Ma, Cuixia; Deng, Xiaoming; Wang, Hongan] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Deng, XM; Wang, HG (corresponding author), Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing 100190, Peoples R China.; Deng, XM; Wang, HG (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
EM xiaoming@iscas.ac.cn; hongan@iscas.ac.cn
RI , chengjian/KGL-5551-2024
OI , chengjian/0000-0003-1289-2758
FU National Key Research and Development Plan [2016YFB1001200,
   2018YFC0809300]; Natural Science Foundation of China [61473276,
   61872346]; Natural Science Foundation of Beijing [L182052]
FX This work was supported by the National Key Research and Development
   Plan (2016YFB1001200, 2018YFC0809300), Natural Science Foundation of
   China (61473276, 61872346), and Natural Science Foundation of Beijing
   (L182052).
CR [Anonymous], 2017, arXiv
   Bhat D, 2015, IEEE INT CONF MULTI
   Boulahia SY, 2017, INT CONF IMAG PROC
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Chen XH, 2017, IEEE IMAGE PROC, P2881, DOI 10.1109/ICIP.2017.8296809
   Chen Y., 2019, P BRIT MACHINE VISIO
   De Smedt Q, 2019, COMPUT VIS IMAGE UND
   De Smedt Q, 2016, IEEE COMPUT SOC CONF, P1206, DOI 10.1109/CVPRW.2016.153
   DeSmedt Q, 2017, P WORKSHOP 3D OBJECT
   DeSmedt Q, 2017, INT WORKSH UND HUM A
   Devineau G, 2018, P IEEE INT C AUTOMAT
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   Freeman W.T, 1995, P INT WORKSHOP AUTOM
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Hou J., 2018, Proceedings of the European Conference on Computer Vision (ECCV), P0
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kingma D. P., 2014, arXiv
   Li C, 2017, P INT C MULTIMEDIA E
   Lin Z, 2020, P IEEE INT C AUTOMAT
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Mehran M, 2018, P INT S VISUAL COMPU
   Molchanov P, 2015, IEEE COMPUT SOC CONF
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Nunez J.C, 2018, PATTERN RECOGN
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oberweger M, 2015, ARXIV
   Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang P, 2017, IEEE T CIRC SYST VID
   Wen Y, 2019, 33 AAAI C ART INT
   Weng JW, 2018, LECT NOTES COMPUT SC, V11211, P142, DOI 10.1007/978-3-030-01234-2_9
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang X, 2018, IEEE T NEUR NET LEAR
   2017, IEEE INT CONF COMP V, P585
NR 43
TC 28
Z9 28
U1 5
U2 52
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2433
EP 2444
DI 10.1007/s00371-020-01955-w
EA AUG 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000563612200001
DA 2024-07-18
ER

PT J
AU Zhang, D
   Wu, ZK
   Wang, XC
   Lv, CL
   Liu, N
AF Zhang, Dan
   Wu, Zhongke
   Wang, Xingce
   Lv, Chenlei
   Liu, Na
TI 3D skull and face similarity measurements based on a harmonic wave
   kernel signature
SO VISUAL COMPUTER
LA English
DT Article
DE Laplace-Beltrami operator; Harmonic wave kernel signature; 3D skull
   similarity; 3D face similarity
ID CRANIOFACIAL RECONSTRUCTION; REGRESSION; IDENTIFICATION; RECOGNITION;
   MODEL
AB Research regarding the similarity measurements of 3D craniofacial models (including 3D skull and face models) is an important research direction in fields such as archaeology, forensic science, and anthropology and represents a meaningful and challenging task. Its major challenges are the fact that 3D skulls are geometric models with multiple holes and complex topologies, there are facial expression changes on the 3D faces. Therefore, the general 3D shape similarity measurements, which are sensitive to boundaries and expression changes, make it impossible to simultaneously measure skull and face similarity. In this paper, we define a 3D signature to describe the pure intrinsic structure and distinguish the similar basic shape and complex topology of 3D skulls and faces: the harmonic wave kernel signature (HWKS). The HWKS is a point descriptor involving the Laplace-Beltrami operator, which is able to effectively extract geometrical and topological information from 3D skulls and faces. Based on the HWKS, we provide an effective pipeline for 3D skull and face similarity measurement by calculating the cosine distance between the HWKS values of 3D skulls and faces. By making comparisons with the wave kernel signature, the HWKS simultaneously describes both local and global properties of a shape. Results from a number of experiments have already shown that our framework is suitable for both measure 3D skull similarity and face similarity, and more importantly, measuring skull similarity and face similarity are two independent processes although using the same framework. By using the same measurement method for 3D skull similarity and face similarity, we observe an effective craniofacial relationship under unified metrics: The change rate of skull similarity is generally consistent with the corresponding face similarity, indicating the correlation between the shape of the 3D skull and its corresponding 3D face shape. And our experimental results show the rationality and effectiveness of this method, which refers to the previous researchers measure the similarity of the reconstructed face from the original skull to reflect the similarity of the original skull.
C1 [Zhang, Dan; Wu, Zhongke; Wang, Xingce; Lv, Chenlei; Liu, Na] Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Zhang, Dan; Wu, Zhongke; Wang, Xingce; Lv, Chenlei; Liu, Na] Beijing Normal Univ, Engn Res Ctr Virtual Real & Applicat, Beijing Key Lab Digital Preservat & Virtual Real, Minist Educ, Beijing 100875, Peoples R China.
C3 Beijing Normal University; Beijing Normal University
RP Wang, XC (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.; Wang, XC (corresponding author), Beijing Normal Univ, Engn Res Ctr Virtual Real & Applicat, Beijing Key Lab Digital Preservat & Virtual Real, Minist Educ, Beijing 100875, Peoples R China.
EM danz@mail.bnu.edu.cn; zwu@bnu.edu.cn; wangxingce@bnu.edu.cn;
   chenleilv@mail.bnu.edu.cn; lna@mail.bnu.edu.cn
FU BRICS Program of China [2017YFE0100500]; National Key R&D Program of
   China [2017YFB1002604, 2017YFB1402105, 2017YFB1002804]; Beijing Natural
   Science Foundation of China [4172033]
FX The authors like to thank the support of National Key Cooperation
   between the BRICS Program of China (No.2017YFE0100500); National Key R&D
   Program of China (No.2017YFB1002604, No.2017YFB1402105,
   No.2017YFB1002804); Beijing Natural Science Foundation of China (No.
   4172033).
CR Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Berar M, 2005, ISPA 2005: PROCEEDINGS OF THE 4TH INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P365, DOI 10.1109/ISPA.2005.195439
   Berar M, 2011, FORENSIC SCI INT, V210, P228, DOI 10.1016/j.forsciint.2011.03.010
   Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y
   Damas S, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978806
   Duan FQ, 2014, IEEE T INF FOREN SEC, V9, P1322, DOI 10.1109/TIFS.2014.2332981
   Emambakhsh M, 2017, IEEE T PATTERN ANAL, V39, P995, DOI 10.1109/TPAMI.2016.2565473
   Fenton TW, 2008, J FORENSIC SCI, V53, P34, DOI 10.1111/j.1556-4029.2007.00624.x
   Giurazza F, 2012, FORENSIC SCI INT, V222, DOI 10.1016/j.forsciint.2012.06.008
   Hou XN, 2016, VISUAL COMPUT, V32, P479, DOI 10.1007/s00371-015-1079-x
   Hu JX, 2009, VISUAL COMPUT, V25, P667, DOI 10.1007/s00371-009-0340-6
   Jin W-X, 2013, APPL RES COMPUT, V10, P61
   Jun-Li Zhao, 2018, Journal of Computer Science and Technology, V33, P207, DOI 10.1007/s11390-018-1814-7
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Kermi A., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P924, DOI 10.1109/ISSPA.2012.6310686
   Lei YJ, 2014, PATTERN RECOGN, V47, P509, DOI 10.1016/j.patcog.2013.07.018
   Lv CL, 2019, PATTERN RECOGN, V88, P458, DOI 10.1016/j.patcog.2018.12.006
   Mansour RF, 2020, MULTIMED TOOLS APPL, V79, P22065, DOI 10.1007/s11042-017-5015-0
   Melzi S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356524
   Mendonca DA, 2013, J CRANIOFAC SURG, V24, P1106, DOI 10.1097/SCS.0b013e31828dcdcb
   Ovsjanikov M, 2008, COMPUT GRAPH FORUM, V27, P1341, DOI 10.1111/j.1467-8659.2008.01273.x
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pei YR, 2016, IEEE IMAGE PROC, P2941, DOI 10.1109/ICIP.2016.7532898
   Pei Y, 2008, COMPUT GRAPH FORUM, V27, P1711, DOI 10.1111/j.1467-8659.2008.01315.x
   Quatrehomme G, 1997, J FORENSIC SCI, V42, P649
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shrimpton S, 2014, FORENSIC SCI INT, V234, P103, DOI 10.1016/j.forsciint.2013.10.021
   Shui WY, 2017, COMPUT BIOL MED, V90, P33, DOI 10.1016/j.compbiomed.2017.08.023
   Smeets D, 2009, 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P68
   Soltanpour S, 2017, PATTERN RECOGN, V72, P391, DOI 10.1016/j.patcog.2017.08.003
   Spradley MK, 2011, J FORENSIC SCI, V56, P289, DOI 10.1111/j.1556-4029.2010.01635.x
   Suetens P., 2006, CIT J COMPUTING INFO, V14, P21
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vanezis P, 2000, FORENSIC SCI INT, V108, P81, DOI 10.1016/S0379-0738(99)00026-2
   Xia J, 2000, INT J ORAL MAX SURG, V29, P11, DOI 10.1016/S0901-5027(00)80116-2
   Xu GL, 2004, COMPUT AIDED GEOM D, V21, P767, DOI 10.1016/j.cagd.2004.07.007
   Zhang D, 2019, 2019 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P77, DOI 10.1109/CW.2019.00021
   Zhang L, 2006, VISUAL COMPUT, V22, P43, DOI 10.1007/s00371-005-0352-9
   Zhao LP, 2012, ARCH PLAST SURG-APS, V39, P309, DOI 10.5999/aps.2012.39.4.309
NR 39
TC 6
Z9 7
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 749
EP 764
DI 10.1007/s00371-020-01946-x
EA AUG 2020
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000558137100002
DA 2024-07-18
ER

PT J
AU Laco, M
   Polatsek, P
   Dekrét, S
   Benesova, W
   Baránková, M
   Strnádelová, B
   Koróniová, J
   Gablíková, M
AF Laco, Miroslav
   Polatsek, Patrik
   Dekret, Simon
   Benesova, Wanda
   Barankova, Martina
   Strnadelova, Bronislava
   Koroniova, Jana
   Gablikova, Maria
TI Effects of individual's emotions on saliency and visual search
SO VISUAL COMPUTER
LA English
DT Article
DE Visual attention; Emotions; Saliency modelling
ID NEGATIVE AFFECT; POSITIVE MOOD; ATTENTION; PERCEPTION; INCREASES;
   STIMULI; BROADEN; IMAGERY; MEMORY
AB Psychological studies have already proven a connection between emotional stimuli and visual attention. However, there is a lack of evidence of how much an individual's mood influences visual information processing of emotionally neutral stimuli. In contrast to prior studies, we explored the effect of positive mood on bottom-up low-level visual saliency. In our extensive experimental studies, we induced positive or neutral emotions in 106 subjects using a psychological method of autobiographical memories and listening to positive music and then performed an eye-tracking study with three task types-free viewing, memorizing the image content and visual search tasks. We explored differences in the human gaze behaviour between both emotions and relate them to the bottom-up attention correlation between emotionally aroused and non-aroused subjects. We observed that positive emotions induce a strong broadening and engagement effect on the bottom-up attention while performing all task types. On the other hand, we found out that certain tasks are solved less efficiently when experiencing a positive mood. Therefore, we claim that positive mood may also act as a distractor while performing certain types of tasks.
C1 [Laco, Miroslav; Polatsek, Patrik; Dekret, Simon; Benesova, Wanda] Slovak Univ Technol Bratislava, Fac Informat & Informat Technol, Bratislava, Slovakia.
   [Barankova, Martina; Gablikova, Maria] EmotionID, Bratislava, Slovakia.
   [Barankova, Martina; Strnadelova, Bronislava; Koroniova, Jana; Gablikova, Maria] Comenius Univ, Fac Social & Econ Sci, Inst Appl Psychol, Bratislava, Slovakia.
   [Koroniova, Jana] Slovak Acad Sci, Ctr Social & Psychol Sci, Inst Expt Psychol, Bratislava, Slovakia.
C3 Slovak University of Technology Bratislava; Comenius University
   Bratislava; Slovak Academy of Sciences
RP Benesova, W (corresponding author), Slovak Univ Technol Bratislava, Fac Informat & Informat Technol, Bratislava, Slovakia.
EM vanda_benesova@stuba.sk
RI Laco, Miroslav/AHA-8134-2022; Benesova, Wanda/AAD-8074-2019
OI Laco, Miroslav/0000-0002-4820-1640; Benesova, Wanda/0000-0001-6929-9694;
   Strnadelova, Bronislava/0000-0003-2463-4457
FU Research and Development Operational Programme [ITMS 26240220084];
   European Regional Development Fund; UVP Project STU;  [VEGA 1/075/19
   2019-2022]
FX The authors would like to thank for financial contribution from UVP
   Project STU and from the Research and Development Operational Programme
   for the Project "University Science Park of STU Bratislava", ITMS
   26240220084, co-funded by the European Regional Development Fund and
   Grant VEGA 1/075/19 2019-2022.
CR [Anonymous], 2018, IEEE T PATTERN ANAL
   Armony JL, 2002, NEUROPSYCHOLOGIA, V40, P817, DOI 10.1016/S0028-3932(01)00178-6
   Barbot A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23686-8
   Becker MW, 2011, EMOTION, V11, P1248, DOI 10.1037/a0023524
   Borji A., 2015, P CVPR WORKSH FUT DA
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Bylinskii Z, 2016, ARXIV160403605
   Ding X, 2016, INT C MULT MOD, P197
   Egermann H, 2015, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01341
   Eysenck M.W., 2013, Cognitive psychology: A student's handbook
   Fan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P217, DOI 10.1145/3123266.3123445
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Ferrer RA, 2016, J BEHAV DECIS MAKING, V29, P245, DOI 10.1002/bdm.1871
   Fredrickson BL, 2005, COGNITION EMOTION, V19, P313, DOI 10.1080/02699930441000238
   Fredrickson BL, 2004, PHILOS T R SOC B, V359, P1367, DOI 10.1098/rstb.2004.1512
   Goldstein E.B., 2010, Encyclopedia of perception
   Grol M, 2014, PSYCHOL RES-PSYCH FO, V78, P566, DOI 10.1007/s00426-013-0508-6
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Holmes E, 2004, MEMORY, V12, P387, DOI 10.1080/09658210444000124
   Hsu JL, 2018, MULTIMEDIA SYST, V24, P195, DOI 10.1007/s00530-017-0542-0
   Jefferies LN, 2008, PSYCHOL SCI, V19, P290, DOI 10.1111/j.1467-9280.2008.02082.x
   Jiang J, 2011, INT J PSYCHOL, V46, P214, DOI 10.1080/00207594.2010.541255
   Judd T, 2012, MIT TECHNICAL REPORT
   Karim J, 2011, PROCD SOC BEHV, V15, P2016, DOI 10.1016/j.sbspro.2011.04.046
   Kaspar K, 2015, COMPUT HUM BEHAV, V53, P332, DOI 10.1016/j.chb.2015.07.020
   Killgore WDS, 2007, INT J NEUROSCI, V117, P643, DOI 10.1080/00207450600773848
   Krahé B, 2012, J APPL SOC PSYCHOL, V42, P271, DOI 10.1111/j.1559-1816.2011.00887.x
   LANG PJ, 1980, PSYCHOPHYSIOLOGY, V17, P179, DOI 10.1111/j.1469-8986.1980.tb00133.x
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HY, 2016, IEEE T NEUR NET LEAR, V27, P1201, DOI 10.1109/TNNLS.2016.2553579
   Maekawa T, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0195865
   Mills C, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0095837
   Miyazawa S, 2009, JPN PSYCHOL RES, V51, P13, DOI 10.1111/j.1468-5884.2009.00384.x
   Most SB, 2007, COGNITION EMOTION, V21, P964, DOI 10.1080/02699930600959340
   Öhman A, 2001, J EXP PSYCHOL GEN, V130, P466, DOI 10.1037/0096-3445.130.3.466
   Olivers CNL, 2005, PSYCHOL SCI, V16, P265, DOI 10.1111/j.0956-7976.2005.01526.x
   Olsen A, 2012, TOBII 1 VT FIXATION
   Pêcher C, 2009, SAFETY SCI, V47, P1254, DOI 10.1016/j.ssci.2009.03.011
   Phelps EA, 2006, PSYCHOL SCI, V17, P292, DOI 10.1111/j.1467-9280.2006.01701.x
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   Rowe G, 2007, P NATL ACAD SCI USA, V104, P383, DOI 10.1073/pnas.0605198104
   Smilek D, 2006, VIS COGN, V14, P543, DOI 10.1080/13506280500193487
   Sutherland MR, 2017, EMOTION, V17, P700, DOI 10.1037/emo0000245
   Talarico J, 2009, COGNITION EMOTION, V23, P380, DOI 10.1080/02699930801993999
   Tsujimura A, 2009, J SEX MED, V6, P1011, DOI 10.1111/j.1743-6109.2008.01031.x
   Wadlinger HA, 2006, MOTIV EMOTION, V30, P89, DOI 10.1007/s11031-006-9021-1
   Watkins ER, 2009, BEHAV RES THER, V47, P48, DOI 10.1016/j.brat.2008.10.014
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Xu YY, 2019, IEEE T PATTERN ANAL, V41, P2975, DOI 10.1109/TPAMI.2018.2866563
   Xu YY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3887
   Zadra JR, 2011, WIRES COGN SCI, V2, P676, DOI 10.1002/wcs.147
NR 52
TC 0
Z9 1
U1 2
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1581
EP 1592
DI 10.1007/s00371-020-01912-7
EA JUL 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SO2LW
UT WOS:000553760200003
DA 2024-07-18
ER

PT J
AU Paris, A
   Peytavie, A
   Guérin, E
   Dischler, JM
   Galin, E
AF Paris, A.
   Peytavie, A.
   Guerin, E.
   Dischler, J-M
   Galin, E.
TI Modeling rocky scenery using implicit blocks
SO VISUAL COMPUTER
LA English
DT Article
DE Implicit surfaces; Terrain synthesis; Procedural modeling
ID GENERATION
AB We present a novel geologically based method to generate vertical walls of rocky cliffs, crags or promontories. Our method procedurally generates a distribution of fractures in the bedrock to create a set of tiling blocks defined as implicit volumetric primitives. Blocks are in turn implicitly replicated over the vertical parts of the terrain and combined together to obtain a consistent volumetric representation of the fractured bedrock patterns using generalized union and blending operators. Our framework provides multiple levels of control: In addition to automatically generated blocks, the geometry of specific ones can be prescribed by the user using implicit primitives or construction trees, the shape of the blocks can be controlled by several parameters, and the placement rules may adapt according to the underlying geological strata and geometry of the terrain.
C1 [Paris, A.; Peytavie, A.; Guerin, E.; Galin, E.] Univ Lyon, LIRIS, CNRS, Lyon, France.
   [Dischler, J-M] Univ Strasbourg, CNRS, ICube, Strasbourg, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Centre National de la
   Recherche Scientifique (CNRS); Universites de Strasbourg Etablissements
   Associes; Universite de Strasbourg
RP Paris, A (corresponding author), Univ Lyon, LIRIS, CNRS, Lyon, France.
EM axel.paris@liris.cnrs.fr
OI Paris, Axel/0000-0003-4457-1801
FU Agence Nationale de la Recherche [HDWANR-16-CE33-0001]
FX This work was part of the project HDWANR-16-CE33-0001, supported by
   Agence Nationale de la Recherche.
CR Barthe L., 1998, P CSG C SERIES, P17
   Beardall Matthew, 2007, P 3 EUROGRAPHICS C N, P7
   Becher M, 2019, IEEE T VIS COMPUT GR, V25, P1283, DOI 10.1109/TVCG.2017.2762304
   Culik K, 1996, DISCRETE MATH, V160, P245, DOI 10.1016/S0012-365X(96)00118-5
   Dearman W, 1991, BUTTERWORTHS ADV SER
   Ebert DS, 1998, MORGAN KAUFMANN SERI, V3
   Galin E, 2019, COMPUT GRAPH FORUM, V38, P553, DOI 10.1111/cgf.13657
   Gamito M. N., 2001, P PORT COMP GRAPH M
   GEISS RYAN., 2008, GPU GEMS, P7
   Gourmel O, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451238
   Guérin E, 2016, COMPUT GRAPH FORUM, V35, P257, DOI 10.1111/cgf.13023
   Ito T, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P244, DOI 10.1109/CGI.2003.1214475
   Jones MD, 2010, IEEE T VIS COMPUT GR, V16, P81, DOI 10.1109/TVCG.2009.39
   Lagae A, 2006, ACM T GRAPHIC, V25, P1442, DOI 10.1145/1183287.1183296
   MA C., 2011, ACM T GRAPHIC, V30, P4, DOI DOI 10.1145/2010324.1964957
   Neyret F, 1998, IEEE T VIS COMPUT GR, V4, P55, DOI 10.1109/2945.675652
   Palmstrom A., 2001, MEASUREMENT CHARACTE, P49
   Paris A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3342765
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K.H., 1989, 16 ANN C COMP GRAPH, P253, DOI 10.1145/74333.74359
   Peytavie A, 2009, COMPUT GRAPH FORUM, V28, P1801, DOI 10.1111/j.1467-8659.2009.01557.x
   Peytavie A, 2009, COMPUT GRAPH FORUM, V28, P457, DOI 10.1111/j.1467-8659.2009.01385.x
   Stolte N, 2002, SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P105, DOI 10.1109/SMI.2002.1003534
   Sugihara M, 2010, COMPUT GRAPH-UK, V34, P282, DOI 10.1016/j.cag.2010.03.008
   Worley S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P291, DOI 10.1145/237170.237267
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346
   Yuksel C, 2019, COMPUT GRAPH FORUM, V38, P535, DOI 10.1111/cgf.13656
   Zanni C., 2012, EUROGRAPHICS SHORT P, P49
NR 29
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2020
VL 36
IS 10-12
BP 2251
EP 2261
DI 10.1007/s00371-020-01905-6
EA JUL 2020
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CX
UT WOS:000552590100001
DA 2024-07-18
ER

PT J
AU Vidanpathirana, M
   Sudasingha, I
   Vidanapathirana, J
   Kanchana, P
   Perera, I
AF Vidanpathirana, Madhawa
   Sudasingha, Imesha
   Vidanapathirana, Jayan
   Kanchana, Pasindu
   Perera, Indika
TI Tracking and frame-rate enhancement for real-time 2D human pose
   estimation
SO VISUAL COMPUTER
LA English
DT Article
DE Human pose estimation; Real-time; Tracking; Optical flow; Frame-rate
   enhancement
AB We propose a near real-time solution for frame-rate enhancement that enables the use of existing sophisticated pose estimation solutions at elevated frame rates. Our approach couples a keypoint human pose estimator with optical flow using a multistage system of queues operating in a multi-threaded environment. As additional contributions, we propose a pose tracking solution and an approach to overcome errors caused by optical flow. A reduction in error in the range of 30-35% is observed at practical frame rates of pose estimator (4-6 frames per second) while processing 1920x1080 resolution 30 frames-per-second videos at native frame rate. Slower frame rates have increased the reduction of error up to 50%, thereby promoting the use of cheaper hardware and sharing of expensive hardware. Thus, while improving accuracy by enabling sophisticated pose estimation models to operate at above-par frame rates, our approach reduces cost per frame by promoting efficient resource utilization.
C1 [Vidanpathirana, Madhawa] Simon Fraser Univ, Sch Comp Sci, Fac Appl Sci, Burnaby, BC, Canada.
   [Sudasingha, Imesha; Vidanapathirana, Jayan; Kanchana, Pasindu; Perera, Indika] Univ Moratuwa, Dept Comp Sci & Engn, Katubedda, Sri Lanka.
C3 Simon Fraser University; University Moratuwa
RP Vidanpathirana, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, Fac Appl Sci, Burnaby, BC, Canada.
EM madhawa_vidanapathirana@sfu.ca; imesha.13@cse.mrt.ac.lk
RI Perera, Indika/H-7514-2016
OI Vidanapathirana, Madhawa/0000-0002-2615-4978; Perera,
   Indika/0000-0001-5660-248X
CR [Anonymous], 2016, P EUROPEAN C COMPUTE
   [Anonymous], 2016, ARXIV160300831CS
   [Anonymous], 2018, BRIT MACHINE VISION
   [Anonymous], 2017, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2017.143
   [Anonymous], 2014, P COMP VIS ECCV 2014
   [Anonymous], 2014, CVPR
   [Anonymous], 2017, ABS170404861 CORR
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Chen Y, 2017, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2017.137
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Handa A, 2012, LECT NOTES COMPUT SC, V7578, P222, DOI 10.1007/978-3-642-33786-4_17
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Kim W., 2018, ABS180903605 CORR
   Knuth D, OPENPOSE
   Li YR, 2019, VISUAL COMPUT, V35, P1143, DOI 10.1007/s00371-019-01692-9
   Lucas B.D., 1981, IJCAI 1981
   Ma A, CHINA IS WATCHING IT
   OMALLEY M, 1993, MED BIOL ENG COMPUT, V31, P392, DOI 10.1007/BF02446694
   Osokin Daniil, 2018, ABS181112004 CORR
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Song MH, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150993
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wan CK, 2008, VISUAL COMPUT, V24, P373, DOI 10.1007/s00371-007-0195-7
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Ye S, 2018, MULTIMED TOOLS APPL, V77, P24983, DOI 10.1007/s11042-018-5736-8
   Zimmermann Christian, 2018, IEEE INT C ROB AUT I
NR 31
TC 15
Z9 15
U1 0
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1501
EP 1519
DI 10.1007/s00371-019-01757-9
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000015
DA 2024-07-18
ER

PT J
AU Wu, JZ
   Hu, DW
   Xiang, FT
   Yuan, XS
   Su, JM
AF Wu, Jianzhai
   Hu, Dewen
   Xiang, Fengtao
   Yuan, Xingsheng
   Su, Jiongming
TI 3D human pose estimation by depth map
SO VISUAL COMPUTER
LA English
DT Article
DE 3D human pose estimation; Convolutional neural networks; Deep learning;
   Depth
AB We present a new approach for 3D human pose estimation from a single image. State-of-the-art methods for 3D pose estimation have focused on predicting a full-body pose of a single person and have not given enough attention to the challenges in application: incompleteness of body pose and existence of multiple persons in image. In this paper, we introduce depth maps to solve these problems. Our approach predicts the depths of human pose over all spatial grids, which supports 3D poses estimation for incomplete or full bodies of multiple persons. The proposed depth maps encode depths of limbs rather than joints. They are more informative and reversibly convertible to depths of joints. The unified network is trained end to end using mixed 2D and 3D annotated samples. The experiments reveal that our algorithm achieves the state of the art on Human3.6M, the largest publicly available 3D pose estimation benchmark. Moreover, qualitative results have been reported to demonstrate the effectiveness of our approach for 3D pose estimation for incomplete human bodies and multiple persons.
C1 [Wu, Jianzhai; Hu, Dewen; Xiang, Fengtao; Yuan, Xingsheng; Su, Jiongming] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Wu, JZ (corresponding author), Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
EM wjz_gfkd@163.com; dwhu@nudt.edu.cn; xiangfengtao@nudt.edu.cn;
   yuanxingsheng@nudt.edu.cn; sujiongming07@nudt.edu.cn
RI Hu, Dewen/D-1978-2015; Hu, Dewen/AAN-8511-2020
FU Natural Science Foundation of China [61603402, 91420302, 61603403,
   61703417, 61806212]
FX These studies of Jianzhai Wu, Dewen Hu, FengTao Xiang, Xingsheng Yuan
   and Jiongming Su are funded by the Natural Sci-ence Foundation of China
   (Grant Nos. 61603402, 91420302, 61603403, 61703417 and 61806212,
   respectively).
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2017, CVPR
   [Anonymous], ARXIV161105708
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Ching-Hang Chen, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5759, DOI 10.1109/CVPR.2017.610
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Howard A. G., 2017, PREPRINT
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Katircioglu I, 2018, INT J COMPUT VISION, V126, P1326, DOI 10.1007/s11263-018-1066-6
   Li SJ, 2017, INT J COMPUT VISION, V122, P149, DOI 10.1007/s11263-016-0962-x
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Liang H, 2013, VISUAL COMPUT, V29, P837, DOI 10.1007/s00371-013-0822-4
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma CY, 2018, VISUAL COMPUT, V34, P1053, DOI 10.1007/s00371-018-1556-0
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D., 2016, ARXIV161109813
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Popa AI, 2017, PROC CVPR IEEE, P4714, DOI 10.1109/CVPR.2017.501
   Rogez G, 2018, INT J COMPUT VISION, V126, P993, DOI 10.1007/s11263-018-1071-9
   Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134
   Sanzari M, 2016, LECT NOTES COMPUT SC, V9912, P566, DOI 10.1007/978-3-319-46484-8_34
   Sarafianos N, 2016, COMPUT VIS IMAGE UND, V152, P1, DOI 10.1016/j.cviu.2016.09.002
   Saxena A, 2007, IEEE I CONF COMP VIS, P1
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Tekin B., 2016, P BRIT MACH VIS C BM, DOI [DOI 10.5244/C.30.130, 10.5244/C.30.130]
   Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Wang CY, 2019, IEEE T PATTERN ANAL, V41, P1227, DOI 10.1109/TPAMI.2018.2828427
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17
NR 47
TC 7
Z9 8
U1 2
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1401
EP 1410
DI 10.1007/s00371-019-01740-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000008
DA 2024-07-18
ER

PT J
AU Shi, JF
   Ulrich, S
   Ruel, S
AF Shi, Jian-Feng
   Ulrich, Steve
   Ruel, Stephane
TI Real-time saliency detection for greyscale and colour images
SO VISUAL COMPUTER
LA English
DT Article
DE Image saliency; Image segmentation; Image features
ID VISUAL-ATTENTION; OBJECT DETECTION; CLASSIFICATION; SEGMENTATION;
   SYSTEM; CUE
AB Unsupervised salient image generation without the aid of prior assumptions has many applications in computer vision. We present three unique real-time saliency generation algorithms that provide state-of-the-art performance for greyscale and colour images. Our fastest method run under 50 ms per frame on average. Our algorithm introduces a novel weighted histogram of orientation feature to supplement image intensity for monochromatic image manifold ranking. We also provide a method of dimensional reduction for the non-normalized optimal affinity matrix (OAM) using principal components analysis; this novel technique allows faster computation and stabilization of the OAM inversion process. We compare our methods with 18 traditional and recent techniques using three standard and custom datasets including ECSSD, DUT-OMRON and MSRA10K totalling 32,536 images for colour and greyscale variations. The results show our method to be more than 10 x faster than the RC and GMR models and having similar or better precision performances.
C1 [Shi, Jian-Feng; Ulrich, Steve] Carleton Univ, Dept Mech & Aerosp Engn, Ottawa, ON, Canada.
   [Ruel, Stephane] MDA Neptec Design Grp Ltd, Dev, Ottawa, ON, Canada.
C3 Carleton University
RP Shi, JF (corresponding author), Carleton Univ, Dept Mech & Aerosp Engn, Ottawa, ON, Canada.
EM jianfeng.shi@carleton.ca; steve.ulrich@carleton.ca; sruel@neptec.com
RI Shi, Jian-Feng/AAB-7175-2022
OI Shi, Jian-Feng/0000-0002-1268-9947
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   AYE H, 2017, IEEE INT C SOFTW ENG
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Borenstein Eran., 2006, P COMPUTER VISION PA, P969, DOI DOI 10.1109/CVPR.2006.276
   Borji A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247706
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Fattal AK, 2017, IEEE INT C INTELL TR
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FUJIWARA Y, 2014, P VLDB ENDOWMENT, V8
   Guo CL, 2008, PROC CVPR IEEE, P2908
   Hold-Geoffroy Y, 2015, IEEE INT CONF COMPUT
   Hou XD, 2007, PROC CVPR IEEE, P2280
   HOWARD R, 2008, IEEE AER C BIG SKY
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   Jung C, 2014, IMAGE VISION COMPUT, V32, P405, DOI 10.1016/j.imavis.2014.04.001
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li C, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P8, DOI 10.1109/ICIVC.2017.7984449
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li J, 2016, INT J COMPUT VISION, V120, P44, DOI 10.1007/s11263-016-0892-7
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Li L, 2010, IEEE T IMAGE PROCESS, V19, P1, DOI 10.1109/TIP.2009.2032341
   Li L, 2018, NEUROCOMPUTING, V301, P46, DOI 10.1016/j.neucom.2018.03.049
   Li SQ, 2017, SIGNAL PROCESS-IMAGE, V55, P93, DOI 10.1016/j.image.2017.03.023
   Liu M, 2017, INT C ELECTR MACH SY
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Liu ZY, 2020, VISUAL COMPUT, V36, P843, DOI 10.1007/s00371-019-01659-w
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Luo Z, 2017, IEEE C ELEC DEVICES
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   MEYER F, 1992, IEE CONF PUBL, V354, P303
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Otsu N., 2007, IEEE T SYS MAN CYBER, V9, P66, DOI [DOI 10.1109/TSMC.1979.4310076, 10.1109/TSMC.1979.4310076]
   Qi W., 2015, Comput. Vis. Media, V1, P309, DOI DOI 10.1007/s41095-015-0028-y
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   SHI J, 2018, P AIAA SPAC C EXH
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Strand R, 2013, COMPUT VIS IMAGE UND, V117, P429, DOI 10.1016/j.cviu.2012.10.011
   Tian ZQ, 2014, IET COMPUT VIS, V8, P16, DOI 10.1049/iet-cvi.2012.0189
   Van den Bergh M, 2015, INT J COMPUT VISION, V111, P298, DOI 10.1007/s11263-014-0744-2
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4
   Wan XJ, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2903
   Wang JP, 2015, NEUROCOMPUTING, V152, P359, DOI 10.1016/j.neucom.2014.10.056
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wang ZC, 2016, APPL INTELL, V45, P1, DOI 10.1007/s10489-015-0739-x
   WEI Y, 2012, ECCV, P29
   Wu X, 2017, IEEE INT SYMP PHYS
   Xia C., 2017, PROC CVPR IEEE, P4142, DOI DOI 10.1109/CVPR.2017.468
   Xu B, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P525
   Xu Y, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P361, DOI 10.1109/ICIVC.2017.7984578
   Yacoob Y, 2007, IEEE I CONF COMP VIS, P801
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang CL, 2019, VISUAL COMPUT, V35, P473, DOI 10.1007/s00371-018-1475-0
   Yang JM, 2017, IEEE T PATTERN ANAL, V39, P576, DOI 10.1109/TPAMI.2016.2547384
   Ye LW, 2016, IEEE SIGNAL PROC LET, V23, P838, DOI 10.1109/LSP.2016.2558489
   Ye RC, 2017, IEEE INT CON MULTI, P415, DOI 10.1109/ICME.2017.8019514
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang JM, 2017, MULTIMED TOOLS APPL, V76, P25713, DOI 10.1007/s11042-017-4473-8
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhao JF, 2015, OPT ENG, V54, DOI 10.1117/1.OE.54.4.043101
   Zhao TX, 2013, IEEE C ELEC DEVICES
   Zhengqin L., 2015, PROC CVPR IEEE, P1356, DOI DOI 10.1109/CVPR.2015.7298741
   Zhou DY, 2004, ADV NEUR IN, V16, P169
NR 91
TC 3
Z9 3
U1 2
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2021
VL 37
IS 6
BP 1277
EP 1296
DI 10.1007/s00371-020-01865-x
EA JUN 2020
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SO2LW
UT WOS:000539941100001
DA 2024-07-18
ER

PT J
AU Sonawane, B
   Sharma, P
AF Sonawane, Bhakti
   Sharma, Priyanka
TI Review of automated emotion-based quantification of facial expression in
   Parkinson's patients
SO VISUAL COMPUTER
LA English
DT Review
DE Facial expression; Parkinson; Hypomimia; Convolutional neural networks
   (CNNs)
ID FACE DETECTION; ACTION UNITS; HEALTH-CARE; DISEASE; RECOGNITION;
   SEQUENCES; FEATURES; SYSTEM; CLASSIFICATION; ALGORITHMS
AB Among various means of communication, the human face is utmost powerful. Persons suffering from Parkinson's disease (PD) experience hypomimia which often leads to reduction in facial expression. Hypomimia affects in social interaction and has a highly undesirable impact on patient's as well as his relative's quality of life. To track the longitudinal progression of PD, usually Movement Disorder Society's Unified Parkinson's Disease Rating Scale (MDS-UPDRS) is used in clinical studies and item 3.2 (i.e., facial expression) of MDS-UPDRS defines hypomimia levels. Assessment of facial expressions has traditionally relied on an observer-based scale which can be time-consuming. Computational analysis techniques for facial expressions can assist the clinician in decision making. Intention of such techniques is to predict objective and accurate score for facial expression. The aim of this paper is to present up-to-date review on computational analysis techniques for measurement of emotional facial expression of people with PD (PWP) along with an overview on clinical applications of automated facial expression analysis. This led us to examine a pilot experimental work for masked face detection in PD. For the same, a deep learning-based model was trained on NVIDIA GeForce 920M GPU. It was observed that deep learning-based model yields 85% accuracy on the testing images.
C1 [Sonawane, Bhakti; Sharma, Priyanka] Nirma Univ, Ahmadabad, Gujarat, India.
C3 Nirma University
RP Sonawane, B (corresponding author), Nirma Univ, Ahmadabad, Gujarat, India.
EM 15extphde153@nirmauni.ac.in; Priyanka.sharma@nirmauni.ac.in
RI Sonawane, bhakti/T-8756-2019; Sonawane, Bhakti/R-8759-2019; Sonawane,
   Bhakti/AAH-5333-2020; Sonawane, Bhakti/KHT-4569-2024
OI Sonawane, bhakti/0000-0002-2154-9014; 
CR Abdulrahman M, 2015, SIG PROCESS COMMUN, P276, DOI 10.1109/SIU.2015.7129813
   Adams WR, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0188226
   Agarwal A, 2016, 2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT), P3776, DOI 10.1109/ICEEOT.2016.7755419
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Ali L, 2019, 2019 INT C EL COMM C
   Ali L, 2019, IEEE J TRANSL ENG HE, V7, DOI 10.1109/JTEHM.2019.2940900
   Ali L, 2019, EXPERT SYST APPL, V137, P22, DOI 10.1016/j.eswa.2019.06.052
   Ali L, 2019, IEEE ACCESS, V7, P116480, DOI 10.1109/ACCESS.2019.2932037
   Almaev T.R., 2013, 2013 HUM ASS C AFF C
   Alvino C, 2007, J NEUROSCI METH, V163, P350, DOI 10.1016/j.jneumeth.2007.03.002
   Amin M.A., 2008, 2008 INT C MACH LEAR, V6
   [Anonymous], 2014, 2 INT C E HLTH TEL C
   [Anonymous], 2014, THESIS
   [Anonymous], 2011, ARXIV11114052
   [Anonymous], 2012, P SPIE
   [Anonymous], 2015, INT J INF TECHNOL CO
   Asgari M, 2010, IEEE ENG MED BIO, P5201, DOI 10.1109/IEMBS.2010.5626104
   Bache K., 2013, UCI MACHINE LEARNING, V28
   Bächlin M, 2010, IEEE T INF TECHNOL B, V14, P436, DOI 10.1109/TITB.2009.2036165
   Bakar Z. A., 2012, 2012 IEEE 8th International Colloquium on Signal Processing & its Applications, P63, DOI 10.1109/CSPA.2012.6194692
   Bandini A, 2017, J NEUROSCI METH, V281, P7, DOI 10.1016/j.jneumeth.2017.02.006
   Benba A., 2015, Int. J. Electr. Eng. Inform, V7, P297, DOI DOI 10.15676/IJEEI.2015.7.2.10
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   BETTADAPURA V, 2012, COMPUTER SCI
   Bishay M, 2021, IEEE T AFFECT COMPUT, V12, P949, DOI 10.1109/TAFFC.2019.2907628
   Bologna M, 2013, J NEUROL NEUROSUR PS, V84, P681, DOI 10.1136/jnnp-2012-303993
   Boughrara H, 2016, MULTIMED TOOLS APPL, V75, P709, DOI 10.1007/s11042-014-2322-6
   Bowers D, 2006, J INT NEUROPSYCH SOC, V12, P765, DOI 10.1017/S135561770606111X
   Chang KY, 2013, IEEE SYS MAN CYBERN, P3157, DOI 10.1109/SMC.2013.538
   Chang W.-Y., 2007, P AS C COMP VIS
   Chang WY, 2007, LECT NOTES COMPUT SC, V4844, P621
   Chen HL, 2013, EXPERT SYST APPL, V40, P263, DOI 10.1016/j.eswa.2012.07.014
   Chereshnev R., 2017, INT C AN IM SOC NETW
   Cho CW, 2009, EXPERT SYST APPL, V36, P7033, DOI 10.1016/j.eswa.2008.08.076
   Cirean DC, 2013, INT C MED IM COMP CO
   Clark US, 2008, NEUROPSYCHOLOGIA, V46, P2300, DOI 10.1016/j.neuropsychologia.2008.03.014
   Clawson K, 2017, 2017 INT SYST C INTE
   Cohn Jeffrey F., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204260
   Connie Tee, 2017, Multi-disciplinary Trends in Artificial Intelligence. 11th International Workshop, MIWAI 2017. Proceedings: LNAI 10607, P139, DOI 10.1007/978-3-319-69456-6_12
   Dagar D, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES (ICACCCT), P77, DOI 10.1109/ICACCCT.2016.7831605
   Das R, 2010, EXPERT SYST APPL, V37, P1568, DOI 10.1016/j.eswa.2009.06.040
   Delannoy J., 2008, IEEE INT C AUT FAC G
   Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905
   Drotár P, 2013, IEEE INT C BIOINF BI, DOI 10.1109/BIBE.2013.6701692
   Friesen E., 1978, Environmental Psychology & Nonverbal Behavior, V3, P5, DOI 10.1037/t27734-000
   Galaz Z, 2016, 2016 39TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P503, DOI 10.1109/TSP.2016.7760930
   Georgescu MI, 2019, IEEE ACCESS, V7, P64827, DOI 10.1109/ACCESS.2019.2917266
   Ghimire D, 2014, J INF PROCESS SYST, V10, P443, DOI 10.3745/JIPS.02.0004
   Ghimire D, 2013, SENSORS-BASEL, V13, P7714, DOI 10.3390/s130607714
   Goetz CG, 2003, MOVEMENT DISORD, V18, P738, DOI 10.1002/mds.10473
   Gunnery SD, 2017, COGENT PSYCHOL, V4, DOI 10.1080/23311908.2017.1376425
   Hamm J, 2011, J NEUROSCI METH, V200, P237, DOI 10.1016/j.jneumeth.2011.06.023
   Hjelmås E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Hong K., 2017, INT J ENG RES TECHNO, V6, P94
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Izard C.E., 1979, MAXIMALLY DISCRIMINA
   Izard C.E., 1982, MEASURING EMOTIONS I, P97
   Jaiswal S., 2016, ARXIV161202374CSCV
   Joshi A., 2016, P 9 ACM INT C PERV T, P1
   Joshi A, 2018, IEEE INT CONF AUTOMA, P278, DOI 10.1109/FG.2018.00048
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   KATSIKITIS M, 1988, J NEUROL NEUROSUR PS, V51, P362, DOI 10.1136/jnnp.51.3.362
   Kim M., 2010, EUR C COMP VIS
   Kotsia I, 2007, IEEE T IMAGE PROCESS, V16, P172, DOI 10.1109/TIP.2006.884954
   Kulkarni SS, 2009, BIOMED ENG ONLINE, V8, DOI 10.1186/1475-925X-8-16
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li DY, 2018, MULTIMED TOOLS APPL, V77, P15251, DOI 10.1007/s11042-017-5105-z
   Li M.H., VISION BASED ASSESSM, P8
   Lien JJJ, 2000, ROBOT AUTON SYST, V31, P131, DOI 10.1016/S0921-8890(99)00103-7
   Lin CY, 2016, BEHAV NEUROL, V2016, DOI 10.1155/2016/9287092
   Little MA, 2007, BIOMED ENG ONLINE, V6, DOI 10.1186/1475-925X-6-23
   Littlewort G, 2004, 2004 C COMP VIS PATT
   Littlewort G., 2004, C COMP VIS PATT REC
   Liu Q., 2010, 2 INT AS C INF CONTR, V2
   Lumaka A, 2017, CLIN GENET, V92, P166, DOI 10.1111/cge.12948
   Marek K, 2011, PROG NEUROBIOL, V95, P629, DOI 10.1016/j.pneurobio.2011.09.005
   Mazilu Sinziana, 2013, Machine Learning and Data Mining in Pattern Recognition. 9th International Conference, MLDM 2013. Proceedings: LNCS 7988, P144, DOI 10.1007/978-3-642-39712-7_11
   Mazilu S., 2012, P 6 INT C PERV COMP
   Minaee S., 2019, Deep-emotion: Facial expression recognition using attentional convolutional network
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Muhammad G, 2017, IEEE ACCESS, V5, P10871, DOI 10.1109/ACCESS.2017.2712788
   Nonis F, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183904
   Pantic M., 1999, Proceedings 11th International Conference on Tools with Artificial Intelligence, P113, DOI 10.1109/TAI.1999.809775
   Pell MD, 2005, COGNITIVE BRAIN RES, V23, P327, DOI 10.1016/j.cogbrainres.2004.11.004
   Pereira CR, 2016, SIBGRAPI, P340, DOI [10.1109/SIBGRAPI.2016.054, 10.1109/SIBGRAPI.2016.51]
   Pereira CR, 2015, COMP MED SY, P171, DOI 10.1109/CBMS.2015.34
   Pollina DA, 2006, ANN BIOMED ENG, V34, P1182, DOI 10.1007/s10439-006-9143-3
   Prashanth R, 2018, INT J MED INFORM, V119, P75, DOI 10.1016/j.ijmedinf.2018.09.008
   Rajoub BA, 2014, IEEE T INF FOREN SEC, V9, P1015, DOI 10.1109/TIFS.2014.2317309
   Ramanathan S, 2006, 2006 INT C IM PROC
   Ramkumar G, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES (ICACCCT), P12, DOI 10.1109/ICACCCT.2016.7831590
   Ranjan R., 2016, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Ravì D, 2017, IEEE J BIOMED HEALTH, V21, P56, DOI 10.1109/JBHI.2016.2633287
   Ricciardi L, 2015, J NEUROL SCI, V358, P125, DOI 10.1016/j.jns.2015.08.1516
   RINN WE, 1984, PSYCHOL BULL, V95, P52, DOI 10.1037/0033-2909.95.1.52
   Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292
   Sakar BE, 2013, IEEE J BIOMED HEALTH, V17, P828, DOI 10.1109/JBHI.2013.2245674
   Shrivastava P, 2017, COMPUT METH PROG BIO, V139, P171, DOI 10.1016/j.cmpb.2016.07.029
   Simons G, 2004, J INT NEUROPSYCH SOC, V10, P521, DOI 10.1017/S135561770410413X
   Simons G, 2003, COGNITION EMOTION, V17, P759, DOI 10.1080/02699930302280
   Singh S.K., 2003, TAMKANG J SCI ENG, V6, P227
   Sonawane Bhakti, 2018, Intelligent Computing and Information and Communication. Proceedings of 2nd International Conference, ICICC 2017. Advances in Intelligent Systems and Computing (AISC 673), P257, DOI 10.1007/978-981-10-7245-1_26
   Song KT, 2012, IEEE SYS MAN CYBERN, P3123, DOI 10.1109/ICSMC.2012.6378271
   Suk H-I, 2013, INT C MED IM COMP CO
   Suresh K, 2016, IIOAB J, V7, P305
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thevenot J, 2018, IEEE J BIOMED HEALTH, V22, P1497, DOI 10.1109/JBHI.2017.2754861
   Tian Y.-L., 2005, Facial Expression Analysis, P247, DOI DOI 10.1007/0-387-27257-7_12
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tickle-Degnen L, 2004, SOC SCI MED, V58, P603, DOI 10.1016/S0277-9536(03)00213-2
   Tickle-Degnen L, 2011, SOC SCI MED, V73, P95, DOI 10.1016/j.socscimed.2011.05.008
   Tivatansakul S, 2014, 2014 IEEE Symposium on Computational Intelligence in Healthcare and e-health (CICARE), P40, DOI 10.1109/CICARE.2014.7007832
   Tsanas A, 2012, IEEE T BIO-MED ENG, V59, P1264, DOI 10.1109/TBME.2012.2183367
   Tsanas A, 2010, IEEE T BIO-MED ENG, V57, P884, DOI 10.1109/TBME.2009.2036000
   Vinokurov N, 2015, INT S PERV COMP PAR
   Wang P, 2008, J NEUROSCI METH, V168, P224, DOI 10.1016/j.jneumeth.2007.09.030
   Wang T, 2014, BIO-MED MATER ENG, V24, P2751, DOI 10.3233/BME-141093
   Wojtusch J, 2015, IEEE-RAS INT C HUMAN, P74, DOI 10.1109/HUMANOIDS.2015.7363534
   Wolf K, 2015, DIALOGUES CLIN NEURO, V17, P457
   Wu P, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/427826
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yow KC, 1997, IMAGE VISION COMPUT, V15, P713, DOI 10.1016/S0262-8856(97)00003-6
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zafeiriou S, 2015, COMPUT VIS IMAGE UND, V138, P1, DOI 10.1016/j.cviu.2015.03.015
   Zeng W, 2013, IEEE INT CONF AUTOMA
   Zhang L, 2007, LECT NOTES COMPUT SC, V4642, P11
NR 130
TC 27
Z9 27
U1 7
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 1151
EP 1167
DI 10.1007/s00371-020-01859-9
EA JUN 2020
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA RV2QF
UT WOS:000538960600002
DA 2024-07-18
ER

PT J
AU Chen, CJ
   Xia, Q
   Li, S
   Qin, H
   Hao, AM
AF Chen, Chengju
   Xia, Qing
   Li, Shuai
   Qin, Hong
   Hao, Aimin
TI Compressing animated meshes with fine details using local spectral
   analysis and deformation transfer
SO VISUAL COMPUTER
LA English
DT Article
DE Animated mesh compression; Manifold harmonic basis; Deformation
   transfer; Linear prediction coding
ID 3D; APPROXIMATION; MATRIX
AB Geometry-centric shape animation, usually represented as dynamic meshes with fixed connectivity and time-deforming geometry, is becoming ubiquitous in digital entertainment and other relevant graphics applications. However, digital animation with fine details, which requires more diversity of texture on meshed geometry, always consumes a significant amount of storage space, and compactly storing and efficiently transmitting these meshes still remain technically challenging. In this paper, we propose a novel key-frame-based dynamic meshes compression method, wherein we decompose the meshes into the low-frequency and high-frequency parts by applying piece-wise manifold harmonic bases to reduce spatial-temporal redundancy of primary poses and by using deformation transfer to recover high-frequency details. First of all, we partition the animated meshes into several clusters with similar poses, and the primary poses of meshes in each cluster can be characterized as a linear combination of manifold harmonic bases derived from the key-frame of that cluster. Second, we recover the geometric details on each primary pose using the deformation transfer technique which reconstructs the details from the key-frames. Thus, we only need to store a very small number of key-frames and a few harmonic coefficients for compressing time-varying meshes, which would reduce a significant amount of storage in contrast with traditional methods where bases were stored explicitly. Finally, we employ the state-of-the-art static mesh compression method to store the key-frames and apply a second-order linear prediction coding to the harmonics coefficients to further reduce the spatial-temporal redundancy. Our comprehensive experiments and thorough evaluations on various datasets have manifested that, our novel method could obtain a high compression ratio while preserving high-fidelity geometry details and guaranteeing limited human perceived distortion rate simultaneously, as quantitatively characterized by the popular Karni-Gotsman error and our newly devised local rigidity error metrics.
C1 [Chen, Chengju; Xia, Qing; Li, Shuai; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, New York, NY USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Li, S (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM lishuaiouc@126.com; qin@cs.stonybrook.edu
OI Xia, Qing/0000-0002-0328-7882
FU National Natural Science Foundation of China [61672077, 61672149,
   61532002]; National Key R&D Program of China [2017YFF010 6407]; Applied
   Basic Research Program of Qingdao [161013xx]; National Science
   Foundation of USA [IIS-0949467, IIS-1047715, IIS-1715985, IIS-1049448];
   capital health research and development of special [2016-1-4011];
   Excellence Foundation of BUAA [2017043]
FX This research is supported in part by National Natural Science
   Foundation of China (Nos. 61672077, 61672149 and 61532002), National Key
   R&D Program of China (No. 2017YFF010 6407), Applied Basic Research
   Program of Qingdao (No. 161013xx), National Science Foundation of USA
   (Nos. IIS-0949467, IIS-1047715, IIS-1715985, and IIS-1049448), the
   capital health research and development of special 2016-1-4011, and the
   Excellence Foundation of BUAA for PhD Students (No. 2017043).
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   [Anonymous], 2018, P COMP GRAPH INT 201
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bici MO, 2011, J VIS COMMUN IMAGE R, V22, P577, DOI 10.1016/j.jvcir.2011.07.006
   de Goes F, 2008, COMPUT GRAPH FORUM, V27, P1349, DOI 10.1111/j.1467-8659.2008.01274.x
   Fröhlich S, 2011, COMPUT GRAPH FORUM, V30, P2246, DOI 10.1111/j.1467-8659.2011.01974.x
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Hajizadeh M, 2016, COMPUT ANIMAT VIRT W, V27, P556, DOI 10.1002/cav.1685
   Heu JH, 2009, J VIS COMMUN IMAGE R, V20, P439, DOI 10.1016/j.jvcir.2009.05.003
   Hou JH, 2017, IEEE T CIRC SYST VID, V27, P1043, DOI 10.1109/TCSVT.2015.2513698
   Hou JH, 2016, COMPUT AIDED GEOM D, V43, P211, DOI 10.1016/j.cagd.2016.02.002
   Hou JH, 2015, IEEE T CIRC SYST VID, V25, P51, DOI 10.1109/TCSVT.2014.2329376
   Hou JH, 2014, IEEE T CIRC SYST VID, V24, P1541, DOI 10.1109/TCSVT.2014.2313890
   Ibarria L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P126
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Lee PF, 2007, IEICE T INF SYST, VE90D, P1073, DOI 10.1093/ietisy/e90-d.7.1073
   Luo GL, 2013, COMPUT ANIMAT VIRT W, V24, P365, DOI 10.1002/cav.1522
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Mamou K, 2006, COMPUT ANIMAT VIRT W, V17, P337, DOI 10.1002/cav.137
   Mullen P, 2008, COMPUT GRAPH FORUM, V27, P1487, DOI 10.1111/j.1467-8659.2008.01289.x
   Neumann T, 2014, COMPUT GRAPH FORUM, V33, P35, DOI 10.1111/cgf.12429
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Sorkine O., 2003, Symposium on Geometry Processing, P42
   Sorkine Olga., 2007, Symposium on Geometry processing, V4, page, P30
   STEFANOSKI N, 2007, 3DTV C 2007, P1, DOI DOI 10.1109/3DTV.2007.4379461
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Stefanoski N, 2008, IEEE IMAGE PROC, P2696, DOI 10.1109/ICIP.2008.4712350
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Vása L, 2014, COMPUT GRAPH FORUM, V33, P145, DOI 10.1111/cgf.12304
   Vása L, 2009, COMPUT GRAPH FORUM, V28, P1529, DOI 10.1111/j.1467-8659.2008.01304.x
   Vasa L, 2007, 3DTV CONF, P49, DOI 10.1109/3DTV.2007.4379408
   Vása L, 2013, IEEE T VIS COMPUT GR, V19, P1467, DOI 10.1109/TVCG.2013.22
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Winkler T, 2010, COMPUT GRAPH FORUM, V29, P309, DOI 10.1111/j.1467-8659.2009.01600.x
   Zhang H, 2007, J GEOPHYS RES-SPACE, V112, DOI 10.1029/2006JA011774
   Zhong M, 2014, VISUAL COMPUT, V30, P751, DOI 10.1007/s00371-014-0971-0
NR 41
TC 2
Z9 2
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2020
VL 36
IS 5
BP 1029
EP 1042
DI 10.1007/s00371-019-01650-5
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LJ3BL
UT WOS:000530043100012
DA 2024-07-18
ER

PT J
AU Pan, X
   Cheng, ZH
   Liu, FC
   Zhang, SY
AF Pan, Xiang
   Cheng, Zhihao
   Liu, Fuchang
   Zhang, Sanyuan
TI Maximum spatial-temporal isometric cluster for dynamic surface
   correspondence
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic surface correspondence; Maximum spatial-temporal isometric
   cluster; Quadratic programming problem
AB Isometric correspondence is an important technique for surface correspondence. Recently, numerous algorithms have been proposed to build isometric mapping. However, those methods tend to be error prone due to the topological variation and noises of the dynamic surface. To address this issue, we propose a dynamic surface correspondence method by computing maximum spatial-temporal isometric cluster. Firstly, the algorithm defines a maximum isometric cluster score to measure the correspondence quality of each cluster in the product space. Then, the maximum problem is formulated into a quadratic programming problem. Furthermore, we define a similarity function which explicitly encodes the spatial-temporal consistence of the dynamic surface. It can greatly reduce the solving dimension, and improve the correspondence accuracy. Finally, the result is extended to the dense correspondence by a geodesic distance vector. Experimental results show that our algorithm can generate consistent correspondence on three databases of surface sequences, which outperforms existing state-of-the-art algorithms.
C1 [Pan, Xiang; Cheng, Zhihao] Zhejiang Univ Technol, Dept Comp Sci, Hangzhou, Peoples R China.
   [Liu, Fuchang] Hangzhou Normal Univ, Hangzhou Inst Serv Engn, Hangzhou, Peoples R China.
   [Zhang, Sanyuan] Zhejiang Univ, Dept Comp Sci, Hangzhou, Peoples R China.
C3 Zhejiang University of Technology; Hangzhou Normal University; Zhejiang
   University
RP Liu, FC (corresponding author), Hangzhou Normal Univ, Hangzhou Inst Serv Engn, Hangzhou, Peoples R China.
EM panx@zjut.edu.cn; 2512370979@qq.com; 252444679@qq.com;
   syzhang@cs.zju.edu.cn
FU National Natural Science Foundation of China [61502133]
FX This study was funded by National Natural Science Foundation of China
   (Grant No.: 61502133).
CR Aflalo Y, 2014, SIAM J IMAGING SCI, V8, P1579
   Alhashim I, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818088
   [Anonymous], ARXIV170100669
   Bogo F., 2017, IEEE C COMP VIS PATT
   Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103
   Chen QF, 2015, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2015.236
   Cosmo L, 2017, COMPUT GRAPH FORUM, V36, P209, DOI 10.1111/cgf.12796
   Crane K., 2013, ACM T GRAPHIC, V32, P13
   Feng W, 2013, VISUAL COMPUT, V29, P53, DOI 10.1007/s00371-012-0674-3
   Guo KW, 2015, IEEE I CONF COMP VIS, P3083, DOI 10.1109/ICCV.2015.353
   Huang QX, 2008, COMPUT GRAPH FORUM, V27, P1449, DOI 10.1111/j.1467-8659.2008.01285.x
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Jiang T, 2017, VISUAL COMPUT, V33, P891, DOI 10.1007/s00371-017-1390-9
   Kezurer I, 2015, COMPUT GRAPH FORUM, V34, P115, DOI 10.1111/cgf.12701
   KIM V, 2011, ACM T GRAPHIC, V30, P123, DOI DOI 10.1145/1964921.1964974
   Kolmogorov V, 2009, MATH PROGRAM COMPUT, V1, P43, DOI 10.1007/s12532-009-0002-8
   Küpcü E, 2017, IEEE INT CONF COMP V, P1266, DOI 10.1109/ICCVW.2017.152
   Lahner Z, 2016, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR.2016.240
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Lipman Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531378
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570
   Ovsjanikov M, 2010, COMPUT GRAPH FORUM, V29, P1555, DOI 10.1111/j.1467-8659.2010.01764.x
   Parashar S, 2018, IEEE T PATTERN ANAL, V40, P2442, DOI 10.1109/TPAMI.2017.2760301
   Ruggeri MR, 2010, INT J COMPUT VISION, V89, P248, DOI 10.1007/s11263-009-0250-0
   Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959
   Sahillioglu Y, 2014, COMPUT GRAPH FORUM, V33, P63, DOI 10.1111/cgf.12278
   SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Smeets D, 2013, COMPUT VIS IMAGE UND, V117, P158, DOI 10.1016/j.cviu.2012.10.002
   Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68
   Tung T, 2014, IEEE T PATTERN ANAL, V36, P901, DOI 10.1109/TPAMI.2013.179
   Vestner M, 2017, INT CONF 3D VISION, P517, DOI 10.1109/3DV.2017.00065
   Vlasic D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618520
   Xu K, 2017, COMPUT GRAPH FORUM, V36, P101, DOI 10.1111/cgf.12790
   Zeng Y, 2010, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2010.5540189
NR 36
TC 0
Z9 0
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 785
EP 798
DI 10.1007/s00371-019-01655-0
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800010
DA 2024-07-18
ER

PT J
AU Vomácka, T
   Kolingerová, I
   Manák, M
AF Vomacka, Tomas
   Kolingerova, Ivana
   Manak, Martin
TI Kinetic locally minimal triangulation: theoretical evaluation and
   combinatorial analysis
SO VISUAL COMPUTER
LA English
DT Article
DE Kinetic data structures; Delaunay triangulation; Locally minimal
   triangulation; Computational geometry; Planar graphs
AB Kinetic data structures represent an extension to ordinary data structures, where the underlying data become time dependent (e.g. moving points). In this paper, we define the kinetic locally minimal triangulation (KLMT) as a kinetic data structure extension to the locally minimal triangulation in the Euclidean plane. We explore the general properties of this data structure in order to show what types of events need to be considered during its life cycle; we also describe the predicates associated with these events. To describe the general kinetic features, we prove that KLMT is responsive, compact, efficient and non-local. In the combinatorial analysis of KLMT, we briefly describe the mathematical apparatus commonly used to investigate computational complexity properties of kinetic data structures and use it to establish the bounds on the number of events processed during the life cycle of this data structure. Finally, the obtained results are compared to the kinetic Delaunay triangulation showing that KLMT may provide some benefits over kinetic Delaunay triangulation, namely simplifying the mathematical equations that need to be computed in order to obtain the times of events.
C1 [Vomacka, Tomas; Kolingerova, Ivana; Manak, Martin] Univ West Bohemia, Fac Appl Sci, Plzen, Czech Republic.
C3 University of West Bohemia Pilsen
RP Vomácka, T (corresponding author), Univ West Bohemia, Fac Appl Sci, Plzen, Czech Republic.
EM tvomacka@kiv.zcu.cz; kolinger@kiv.zcu.cz; manak@kiv.zcu.cz
RI Manak, Martin/O-6300-2019
OI Manak, Martin/0000-0002-1248-7042; Kolingerova,
   Ivana/0000-0003-4556-2771
FU Czech Science Foundation [17-07690S]; Czech Ministry of Education, Youth
   and Sports [SGS-2019-016]
FX This work was funded by the Czech Science Foundation (Project Number
   17-07690S) and by the Czech Ministry of Education, Youth and Sports
   (Project Number SGS-2019-016).
CR Agarwal PK, 2000, DISCRETE COMPUT GEOM, V24, P721, DOI 10.1007/s004540010060
   Albers G, 1998, INT J COMPUT GEOM AP, V8, P365, DOI 10.1142/S0218195998000187
   [Anonymous], 1995, Davenport-Schinzel Sequences and Their Geometric Applications
   [Anonymous], 1996, P 12 EUR WORKSH COMP
   Basch J, 1999, J ALGORITHM, V31, P1, DOI 10.1006/jagm.1998.0988
   Basch J., 1997, Proceedings of the Thirteenth Annual Symposium on Computational Geometry, P388, DOI 10.1145/262839.263016
   Basch J., 1999, THESIS
   Beni L.H, 2009, THESIS
   Bose P, 2002, INT J COMPUT GEOM AP, V12, P445, DOI 10.1142/S0218195902000979
   Dickerson M. T., 1996, Proceedings of the Twelfth Annual Symposium on Computational Geometry, FCRC '96, P204, DOI 10.1145/237218.237364
   Erickson J, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P327
   Goldenstein S, 2001, COMPUT GRAPH-UK, V25, P983, DOI 10.1016/S0097-8493(01)00153-4
   Goralski IR, 2007, ISVD 2007: THE 4TH INTERNATIONAL SYMPOSIUM ON VORONOI DIAGRAMS IN SCIENCE AND ENGINEERING 2007, PROCEEDINGS, P84
   Guibas LJ, 1998, ROBOTICS: THE ALGORITHMIC PERSPECTIVE, P191
   Guibas LJ, 2001, IEEE INT CONF ROBOT, P2903, DOI 10.1109/ROBOT.2001.933062
   Heigeas L., 2003, GRAPHICON
   JAROMCZYK JW, 1992, P IEEE, V80, P1502, DOI 10.1109/5.163414
   Kamphuis A., 2004, SCA '04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, P19
   Kolingerova Ivana, 2018, Transactions on Computational Science XXXIII. Lecture Notes in Computer Science (LNCS 10990), P115, DOI 10.1007/978-3-662-58039-4_7
   Kolingerova I, 2017, NEAREST NEIGHBOUR GR, P455
   Okabe A., 2000, Spatial tessellations: concepts and applications of Voronoi diagrams, V43
   Overmars M, 2009, P 25 EUR WORKSH COMP, P263
   Russel D, 2007, THESIS
   Shewchuk JR, 1997, DISCRETE COMPUT GEOM, V18, P305, DOI 10.1007/PL00009321
   SUD A., 2008, PROF SIGGRAPH 08, P1, DOI DOI 10.1145/1401132.1401207
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Veltkamp R. C., 1992, Computational Geometry: Theory and Applications, V1, P227, DOI 10.1016/0925-7721(92)90003-B
   Vomaka T., 2008, SIGRAD 2008, P57
NR 28
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2020
VL 36
IS 4
BP 757
EP 765
DI 10.1007/s00371-019-01657-y
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KW0AL
UT WOS:000520835800008
DA 2024-07-18
ER

PT J
AU Aslan, S
AF Aslan, Selcuk
TI Modified artificial bee colony algorithms for solving multiple circle
   detection problem
SO VISUAL COMPUTER
LA English
DT Article
DE ABC algorithm; Multiple circle detection; Candidate list generation
ID OPTIMIZATION; IMAGES
AB Determining circular shapes from digital images is one of the most required operations in computer vision and its applications. Various techniques including evolutionary and swarm intelligence-based algorithms have been introduced and successfully used over the past decade for detecting single or multiple circles. In this study, two different circle candidate list generation approaches that are based on generating a combination between abandoned food sources and the obtained food sources in the final colony have been proposed and integrated into the workflow of the standard implementation of the artificial bee colony (ABC) algorithm. Experimental studies on a set of real and synthetic images showed that the proposed list generation approaches improved the solving capabilities of ABC algorithm and decreased the total error values related to the discovered circles compared to the ABC-based implementation for which a circle or circles are selected from a candidate list containing only abandoned food sources, genetic algorithm, bacterial foraging optimization algorithm and an improved version of the Hough transform-based circle detection technique called randomized Hough transform.
C1 [Aslan, Selcuk] Ondokuz Mayis Univ, Dept Comp Engn, Samsun, Turkey.
C3 Ondokuz Mayis University
RP Aslan, S (corresponding author), Ondokuz Mayis Univ, Dept Comp Engn, Samsun, Turkey.
EM selcuk.aslan@omu.edu.tr
RI Aslan, Selcuk/AAT-9375-2021
OI Aslan, Selcuk/0000-0002-9145-239X
CR Akay B, 2015, SIGNAL IMAGE VIDEO P, V9, P967, DOI 10.1007/s11760-015-0758-4
   Akay B, 2013, APPL SOFT COMPUT, V13, P3066, DOI 10.1016/j.asoc.2012.03.072
   [Anonymous], 2014, INT C ADV COMP SCI E
   [Anonymous], J INF COMPUT SCI
   [Anonymous], INT J COMPUT ENG RES
   [Anonymous], INT J COMPUT SCI ENG
   [Anonymous], INT C ADV COMP EL EN
   Ayala-Ramirez V, 2006, PATTERN RECOGN LETT, V27, P652, DOI 10.1016/j.patrec.2005.10.003
   Badem H, 2017, NEUROCOMPUTING, V266, P506, DOI 10.1016/j.neucom.2017.05.061
   CAO YF, 2012, ADV SCI LETT, V6, P841
   Celik M, 2016, INT J ARTIF INTELL T, V25, DOI 10.1142/S0218213015500281
   CHANDRAKALA D, 2012, ISRN ARTIF INTELL
   Charansiriphaisan K, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/927591
   CUEVAS E, 2018, METAHEURISTIC ALGORI, P57
   Cuevas E, 2015, INT J BIO-INSPIR COM, V7, P402, DOI 10.1504/IJBIC.2015.073178
   Cuevas E, 2012, J INTELL ROBOT SYST, V66, P359, DOI 10.1007/s10846-011-9611-3
   Cuevas E, 2012, SOFT COMPUT, V16, P281, DOI 10.1007/s00500-011-0741-0
   Cuevas E, 2012, EXPERT SYST APPL, V39, P713, DOI 10.1016/j.eswa.2011.07.063
   Cuevas E, 2011, PATTERN ANAL APPL, V14, P93, DOI 10.1007/s10044-010-0183-9
   Dasgupta S, 2010, SOFT COMPUT, V14, P1151, DOI 10.1007/s00500-009-0508-z
   Díaz-Cortés MA, 2017, INTEL SYST REF LIBR, V129, P143, DOI 10.1007/978-3-319-57813-2_8
   Dinerstein J, 2007, VISUAL COMPUT, V23, P25, DOI 10.1007/s00371-006-0085-4
   Horng MH, 2011, EXPERT SYST APPL, V38, P13785, DOI 10.1016/j.eswa.2011.04.180
   Karaboga D, 2016, GENET MOL RES, V15, DOI 10.4238/gmr.15028645
   Karaboga D, 2019, NAT COMPUT, V18, P333, DOI 10.1007/s11047-018-9674-1
   Karaboga N, 2007, 2007 IEEE 15TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS, VOLS 1-3, P145
   Karaboga N, 2009, J FRANKLIN I, V346, P328, DOI 10.1016/j.jfranklin.2008.11.003
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Lin JzauSheng Lin JzauSheng, 2012, Research Journal of Applied Sciences, Engineering and Technology, V4, P2973
   López A, 2018, IET COMPUT VIS, V12, P1188, DOI 10.1049/iet-cvi.2018.5193
   Luque-Chang A, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/6843923
   Manda K, 2012, ADV INTEL SOFT COMPU, V132, P29
   Öztürk C, 2016, INT J DATA MIN BIOIN, V14, P332, DOI 10.1504/IJDMB.2016.075823
   Ozturk C, 2015, PATTERN ANAL APPL, V18, P587, DOI 10.1007/s10044-014-0365-y
   Sahin O, 2016, APPL SOFT COMPUT, V49, P1202, DOI 10.1016/j.asoc.2016.09.045
   Shanthi S., 2014, INT J SCI ENG TECHNO, V3, P1664
   Trung-Thien Tran, 2016, Visual Computer, V32, P1205, DOI 10.1007/s00371-015-1157-0
   Wang GY, 2014, VISUAL COMPUT, V30, P539, DOI 10.1007/s00371-013-0879-0
   Xiao YH, 2012, INT J COMPUT APPL T, V43, P343, DOI 10.1504/IJCAT.2012.047159
   Yilmaz B, 2014, TURK J ELECTR ENG CO, V22, P1540, DOI 10.3906/elk-1209-31
   Zhang Y, 2011, PROG ELECTROMAGN RES, V116, P65, DOI 10.2528/PIER11031709
NR 42
TC 5
Z9 5
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2021
VL 37
IS 4
SI SI
BP 843
EP 856
DI 10.1007/s00371-020-01834-4
EA MAR 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RL0LI
UT WOS:000520668900001
DA 2024-07-18
ER

PT J
AU France, SL
   Akkucuk, U
AF France, Stephen L.
   Akkucuk, Ulas
TI A review, framework, and R toolkit for exploring, evaluating, and
   comparing visualization methods
SO VISUAL COMPUTER
LA English
DT Review
DE Dimensionality reduction; Mapping; Solution quality; Model selection
ID NONLINEAR DIMENSION REDUCTION; NEAREST NEIGHBOR ANALYSIS; PROJECTION
   METHODS; ROC CURVE; QUALITY; AREA; ROTATION; METRICS
AB This paper gives a review and synthesis of methods of evaluating dimensionality reduction techniques. Particular attention is paid to rank-order neighborhood evaluation metrics. A framework is created for exploring dimensionality reduction quality through visualization. An associated toolkit is implemented in R. The toolkit includes scatterplots, heat maps, loess smoothing, performance lift diagrams, and animation. The overall rationale is to help researchers compare dimensionality reduction techniques and use visual insights to help select and improve techniques. Examples are given for dimensionality reduction in manifolds and for dimensionality reduction applied to fashion image and consumer survey datasets.
C1 [France, Stephen L.] Mississippi State Univ, Mississippi State, MS 39762 USA.
   [Akkucuk, Ulas] Bogazici Univ, Dept Managment, TR-34342 Istanbul, Turkey.
C3 Mississippi State University; Bogazici University
RP France, SL (corresponding author), Mississippi State Univ, Mississippi State, MS 39762 USA.
EM sfrance@business.msstate.edu; ulas.akkucuk@boun.edu.tr
RI Akkucuk, Ulas/I-1068-2018
OI Akkucuk, Ulas/0000-0001-6731-0634; France, Stephen/0000-0002-7226-2750
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Akkucuk U, 2006, J CLASSIF, V23, P221, DOI 10.1007/s00357-006-0014-2
   Alvarez-Meza AM, 2017, NEUROCOMPUTING, V222, P36, DOI 10.1016/j.neucom.2016.10.004
   [Anonymous], 2005, ARXIV14041100
   [Anonymous], 2014, Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization, DOI [DOI 10.1145/2669557.2669579, 10.1145/2669557.26695792,9, DOI 10.1145/2669557.26695792,9]
   [Anonymous], 1958, Theory and Methods of Scaling
   [Anonymous], 2011, P WORKSH NEW CHALL N
   [Anonymous], 2004, THESIS
   [Anonymous], 2003, Advances in Neural Informaiton Processing Systems
   Aupetit M, 2007, NEUROCOMPUTING, V70, P1304, DOI 10.1016/j.neucom.2006.11.018
   Barbosa A, 2016, IEEE T VIS COMPUT GR, V22, P1314, DOI 10.1109/TVCG.2015.2464797
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bertini E, 2011, IEEE T VIS COMPUT GR, V17, P2203, DOI 10.1109/TVCG.2011.229
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Boulesteix A.L., 2018, ARXIV180910496
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Buja A, 2008, J COMPUT GRAPH STAT, V17, P444, DOI 10.1198/106186008X318440
   Busing F. M. T. A., 1997, SoftStat '97. Advances in Statistical Software 6. 9th Conference on the Scientific Use of Statistical Software, P67
   Campello R.J.G.B., 2019, CEUR WORKSHOP P, P4
   Carpendale S, 2008, LECT NOTES COMPUT SC, V4950, P19, DOI 10.1007/978-3-540-70956-5_2
   CARROLL J. D., 1974, HDB PERCEPTION, VII, P391
   Cavallo M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174209
   Chen L, 2006, THESIS
   Chen LS, 2013, J MACH LEARN RES, V14, P1145
   Chen LS, 2009, J AM STAT ASSOC, V104, P209, DOI 10.1198/jasa.2009.0111
   Cleveland W., 1991, Local regression models in statistical models in s, P309
   CLEVELAND WS, 1984, J AM STAT ASSOC, V79, P807, DOI 10.2307/2288711
   CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Coimbra DB, 2016, INFORM VISUAL, V15, P154, DOI 10.1177/1473871615600010
   Cutura Rene., 2018, 26 EUROPEAN S ARTIFI
   de Leeuw J, 2009, J STAT SOFTW, V31, P1
   Demartines P, 1997, IEEE T NEURAL NETWOR, V8, P148, DOI 10.1109/72.554199
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Etemadpour R, 2015, IEEE T VIS COMPUT GR, V21, P81, DOI 10.1109/TVCG.2014.2330617
   France SL, 2019, SHAPE SPACE MARKETIN, P1
   France SL, 2013, PROPERTIES GEN MEASU
   France S, 2007, LECT NOTES ARTIF INT, V4571, P499
   France SL, 2019, EXPERT SYST APPL, V119, P456, DOI 10.1016/j.eswa.2018.11.002
   France SL, 2011, IEEE T SYST MAN CY C, V41, P644, DOI 10.1109/TSMCC.2010.2078502
   GROENEN PJF, 1995, J CLASSIF, V12, P3, DOI 10.1007/BF01202265
   Hahsler M, 2008, J STAT SOFTW, V25, P1
   Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Hinton G. E., 2002, Advances in Neural InformationProcessing Systems, P857
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Iacobucci D., 2017, J MARK ANAL, V5, P81, DOI [10.1057/s41270-017-0022-6, DOI 10.1057/S41270-017-0022-6]
   Ingram P, 2010, COMPARATIVE ADMINISTRATIVE CHANGE AND REFORM: LESSONS LEARNED, P3
   Jankun-Kelly TJ, 2007, IEEE T VIS COMPUT GR, V13, P357, DOI 10.1109/TVCG.2007.28
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   Joyce J.M., 2011, Kullback-Leibler Divergence, P720, DOI [DOI 10.1007/978-3-642-04898-2_327, DOI 10.1007/978-3-642-04898-2327, 10.1007/978-3-642-04898-2327]
   Karatzoglou A., 2004, J STAT SOFTW, V11, P1, DOI DOI 10.18637/JSS.V011.I09
   Kaski S, 2003, BMC BIOINFORMATICS, V4, DOI 10.1186/1471-2105-4-48
   Kraemer G, 2017, PACKAGE DIMRED
   Krijthe J., 2018, Package 'Rtsne'
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   LASKOWSKI PH, 1989, AM CARTOGRAPHER, V16, P123, DOI 10.1559/152304089783875497
   Lee JA, 2008, JMLR WORKSHOP C P, P21, DOI DOI 10.5555/3053814.3053816
   Lee JA, 2015, NEUROCOMPUTING, V169, P246, DOI 10.1016/j.neucom.2014.12.095
   Lee JA, 2010, PATTERN RECOGN LETT, V31, P2248, DOI 10.1016/j.patrec.2010.04.013
   Lee JA, 2009, NEUROCOMPUTING, V72, P1431, DOI 10.1016/j.neucom.2008.12.017
   Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x
   Liiv Innar, 2010, Statistical Analysis and Data Mining, V3, P70, DOI 10.1002/sam.10071
   Lu Y, 2019, VISUAL COMPUT, V35, P1683, DOI 10.1007/s00371-019-01637-2
   Ma Y, 2012, MANIFOLD LEARNING THEORY AND APPLICATIONS, P1
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   Martins RM, 2015, COMPUTER GRAPHICS VI, P1
   Matute J, 2018, IEEE T VIS COMPUT GR, V24, P542, DOI 10.1109/TVCG.2017.2744339
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, DOI 10.48550/ARXIV.1802.03426, 10.21105/joss.00861]
   McKenna S, 2014, IEEE T VIS COMPUT GR, V20, P2191, DOI 10.1109/TVCG.2014.2346331
   Moere AV, 2011, INFORM VISUAL, V10, P356, DOI 10.1177/1473871611415996
   Mokbel B, 2013, NEUROCOMPUTING, V112, P109, DOI 10.1016/j.neucom.2012.11.046
   Motta R, 2015, NEUROCOMPUTING, V150, P583, DOI 10.1016/j.neucom.2014.09.063
   North C, 2006, IEEE COMPUT GRAPH, V26, P6, DOI 10.1109/MCG.2006.70
   Ntoutsi E., 2019, 1 WORKSH EV EXP DES, P1
   Pagliosa P, 2015, NEUROCOMPUTING, V150, P599, DOI 10.1016/j.neucom.2014.07.072
   Pandey AV, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3659, DOI 10.1145/2858036.2858155
   Paulovich FV, 2010, IEEE T VIS COMPUT GR, V16, P1281, DOI 10.1109/TVCG.2010.207
   PEAY ER, 1988, PSYCHOMETRIKA, V53, P199, DOI 10.1007/BF02294132
   Plaisant C., 2004, Proceedings of the Working Conference on Advanced Visual Interfaces, AVI'04, page, P109, DOI [10.1145/989863.9898802, DOI 10.1145/989863.9898802, 10.1145/989863.989880, DOI 10.1145/989863.989880]
   Pryke A, 2007, LECT NOTES COMPUT SC, V4403, P361
   Qu ZN, 2016, BEYOND TIME AND ERRORS: NOVEL EVALUATION METHODS FOR VISUALIZATION, BELIV 2016, P44, DOI 10.1145/2993901.2993910
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Rasul  K., 2017, FASHION MNIST
   Ringel DM, 2016, MARKET SCI, V35, P511, DOI 10.1287/mksc.2015.0950
   Ringel DM, 2014, P ANN HICSS, P3129, DOI 10.1109/HICSS.2014.388
   Rohde G, 2005, RESP RES, V6, DOI 10.1186/1465-9921-6-150
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sacha D, 2016, IEEE T VIS COMPUT GR, V22, P240, DOI 10.1109/TVCG.2015.2467591
   SCHONEMANN PH, 1970, PSYCHOMETRIKA, V35, P245, DOI 10.1007/BF02291266
   Sedlmair M, 2013, IEEE T VIS COMPUT GR, V19, P2634, DOI 10.1109/TVCG.2013.153
   Seifert C., 2010, P 1 EUR S VIS AN SCI, V1, P1
   SIBSON R, 1978, J ROY STAT SOC B MET, V40, P234
   Snyder JP., 1993, FLATTENING EARTH 200
   Sobczyk A., 1941, DUKE MATH J, V8, P78
   Spathis D, 2019, LECT NOTES COMPUT SC, V11132, P550, DOI 10.1007/978-3-030-11018-5_44
   Stahnke J, 2016, IEEE T VIS COMPUT GR, V22, P629, DOI 10.1109/TVCG.2015.2467717
   Steinley D, 2004, PSYCHOL METHODS, V9, P386, DOI 10.1037/1082-989X.9.3.386
   Tatu A., 2010, P INT C ADV VISUAL I, P49, DOI DOI 10.1145/1842993.1843002
   Ten Berge JMF, 1977, PSYCHOMETRIKA, V42, P267, DOI 10.1007/BF02294053
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Torgerson WS, 1952, PSYCHOMETRIKA, V17, P401
   Tory M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P151, DOI 10.1109/INFVIS.2004.59
   Trosset MW, 1998, J CLASSIF, V15, P15, DOI 10.1007/s003579900018
   Dang TN, 2014, IEEE PAC VIS SYMP, P73, DOI 10.1109/PacificVis.2014.42
   Dang TN, 2014, IEEE T VIS COMPUT GR, V20, P1624, DOI 10.1109/TVCG.2014.2346572
   Tukey J. W., 1985, Proceedings of the Sixth Annual Conference and Exposition: Computer Graphics '85. Vol. III, Technical Sessions, P773
   Tukey J.W., 1977, EXPLORATORY DATA ANA
   TVERSKY A, 1986, PSYCHOL REV, V93, P3, DOI 10.1037/0033-295X.93.1.3
   TVERSKY A, 1983, J MATH PSYCHOL, V27, P235, DOI 10.1016/0022-2496(83)90008-1
   UPSON C, 1989, IEEE COMPUT GRAPH, V9, P30, DOI 10.1109/38.31462
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   vander Maaten L., 2009, J MACH LEARN RES, V10, P13, DOI [10.1080/13506280444000102, DOI 10.1080/13506280444000102]
   Venna J, 2001, LECT NOTES COMPUT SC, V2130, P485
   Venna J, 2006, NEURAL NETWORKS, V19, P889, DOI 10.1016/j.neunet.2006.05.014
   Vidal R., 2016, GEN PRINCIPAL COMPON, P25, DOI [10.1007/978-0-387-87811-9_2, DOI 10.1007/978-0-387-87811-9_2]
   Viégas FB, 2007, LECT NOTES COMPUT SC, V4564, P182
   Wedel M., 2012, MARKET SEGMENTATION
   Wilkinson L, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P157, DOI 10.1109/INFVIS.2005.1532142
   Yang L., 2006, WORKING PAPER
   YOUNG FW, 1978, BEHAV RES METH INSTR, V10, P451, DOI 10.3758/BF03205177
   Zeileis A, 2009, COMPUT STAT DATA AN, V53, P3259, DOI 10.1016/j.csda.2008.11.033
   Zhang P, 2012, NEUROCOMPUTING, V97, P251, DOI 10.1016/j.neucom.2012.05.013
NR 124
TC 7
Z9 7
U1 6
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 457
EP 475
DI 10.1007/s00371-020-01817-5
EA FEB 2020
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QY5QT
UT WOS:000516216800001
DA 2024-07-18
ER

PT J
AU Ji, LE
   Yuan, YM
   Gao, F
AF Ji, Lianen
   Yuan, Yaming
   Gao, Fang
TI Multi-level and multi-perspective visual correlation analysis between
   general courses and program courses
SO VISUAL COMPUTER
LA English
DT Article
DE General and program courses; Multi-perspective; Multi-level;
   Correlation; Visual analytics
ID VISUALIZATION; ANALYTICS; SYSTEMS; FIT
AB Exploring the potential impact of important general courses on program-specific courses in universities can help to improve the entire teaching and learning process for an academic major. However, the large number of courses and multiple factors affecting students' course grades makes it difficult to reveal and analyze the complicated relationship between the two types of courses only from a single perspective or at a single level. Thence, this paper starts with analysis of historical course grades data within an undergraduate program and then presents an interactive visual analytic system, MVCAS, which is designed to demonstrate and explore the various correlations between these two types of courses at different levels and from different perspectives. The major contributions of this work include: (1) a multi-angle preprocessing of course grades data, including decomposition, extraction and conversion; (2) multiple coordinated analysis views which make it possible to effectively explore the overall, categorical and pairwise course correlations and further link courses with students, instructors and semesters together; and (3) a top-down correlation analysis process for general courses and program ones. The effectiveness and usefulness of MVCAS have been preliminarily demonstrated through a case study, in which the field experts use this tool to investigate different levels of correlations between the focused mathematics and program-specific courses in a computer science major comprehensively.
C1 [Ji, Lianen; Yuan, Yaming] China Univ Petr, Beijing Key Lab Petr Data Min, Dept Comp Sci, Beijing, Peoples R China.
   [Gao, Fang] China Mobile Commun Co Ltd, Res Inst, Beijing, Peoples R China.
C3 China University of Petroleum; China Mobile
RP Ji, LE (corresponding author), China Univ Petr, Beijing Key Lab Petr Data Min, Dept Comp Sci, Beijing, Peoples R China.
EM jilianen@cup.edu.cn; 664602532@qq.com
OI Ji, Lianen/0000-0003-0503-0009
FU National Natural Science Foundation of China [60873093]
FX This study was funded by the National Natural Science Foundation of
   China (Grant No. 60873093).
CR Albo Y, 2016, IEEE T VIS COMPUT GR, V22, P569, DOI 10.1109/TVCG.2015.2467322
   Gómez-Aguilar DA, 2015, COMPUT HUM BEHAV, V47, P60, DOI 10.1016/j.chb.2014.11.001
   Bergin S., 2005, SIGCSE Bulletin, V37, P411, DOI 10.1145/1047124.1047480
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Byron L, 2008, IEEE T VIS COMPUT GR, V14, P1245, DOI 10.1109/TVCG.2008.166
   Collins C, 2009, COMPUT GRAPH FORUM, V28, P1039, DOI 10.1111/j.1467-8659.2009.01439.x
   Ding S, 2009, ANAL TRAINING QUALIT
   Elbadrawy A, 2016, COMPUTER, V49, P61, DOI 10.1109/MC.2016.119
   Few S, 2012, SHOW ME NUMBERS DESI
   Fu SW, 2017, IEEE T VIS COMPUT GR, V23, P201, DOI 10.1109/TVCG.2016.2598444
   Gama S, 2014, IEEE INT CONF INF VI, P102, DOI 10.1109/IV.2014.65
   Ghoniem M., 2005, Information Visualization, V4, P114, DOI 10.1057/palgrave.ivs.9500092
   Goolsby C.B., 1988, Research and Teaching in Developmental Education, V4, P18
   Gratzl S, 2014, IEEE T VIS COMPUT GR, V20, P2023, DOI 10.1109/TVCG.2014.2346260
   Henry N, 2006, IEEE T VIS COMPUT GR, V12, P677, DOI 10.1109/TVCG.2006.160
   Hudson H T., 1981, J. Res. Sci. Teach, V18, P291, DOI [DOI 10.1002/TEA.3660180403, 10.1002/tea.3660180403]
   Inselberg A, 1985, VISUAL COMPUT, V1, P69, DOI 10.1007/BF01898350
   Isenberg T, 2013, IEEE T VIS COMPUT GR, V19, P2818, DOI 10.1109/TVCG.2013.126
   Ji Lianen, 2018, Journal of Computer Aided Design & Computer Graphics, V30, P44, DOI 10.3724/SP.J.1089.2018.16924
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P255, DOI 10.1109/TVCG.2002.1021578
   Knauf R, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P655, DOI 10.1109/SITIS.2012.99
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Li J, 2010, INFORM VISUAL, V9, P13, DOI 10.1057/ivs.2008.13
   Malapati A, 2013, PROCEEDINGS OF THE 2013 IEEE INTERNATIONAL CONFERENCE IN MOOC, INNOVATION AND TECHNOLOGY IN EDUCATION (MITE), P325, DOI 10.1109/MITE.2013.6756359
   Mazza R, 2007, INT J HUM-COMPUT ST, V65, P125, DOI 10.1016/j.ijhcs.2006.08.008
   Mühlbacher T, 2013, IEEE T VIS COMPUT GR, V19, P1962, DOI 10.1109/TVCG.2013.125
   Munzner T, 2009, IEEE T VIS COMPUT GR, V15, P921, DOI 10.1109/TVCG.2009.111
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Pryke A, 2007, LECT NOTES COMPUT SC, V4403, P361
   Qu HM, 2015, IEEE COMPUT GRAPH, V35, P69, DOI 10.1109/MCG.2015.137
   Qu H, 2007, IEEE T VIS COMPUT GR, V13, P1408, DOI 10.1109/TVCG.2007.70523
   Raji M, 2021, IEEE T BIG DATA, V7, P510, DOI 10.1109/TBDATA.2018.2840986
   Ritsos P.D., 2014, Proceedings of the 5th EuroVis Workshop on Visual Analytics, P61
   Romero C, 2008, COMPUT EDUC, V51, P368, DOI 10.1016/j.compedu.2007.05.016
   Siirtola Harri, 2013, 2013 17th International Conference on Information Visualisation, P108, DOI 10.1109/IV.2013.13
   Singh I, 2016, 2016 6TH INTERNATIONAL CONFERENCE - CLOUD SYSTEM AND BIG DATA ENGINEERING (CONFLUENCE), P294, DOI 10.1109/CONFLUENCE.2016.7508131
   Sukharev J, 2009, IEEE PAC VIS SYMP, P161, DOI 10.1109/PACIFICVIS.2009.4906852
   Tessema MT, 2005, INT J HUMANIT SOC SC, V2, P34
   Viegas FB., 2008, INTERACTIONS, V15, P49, DOI [DOI 10.1145/1374489.1374501, 10.1145/1374489.1374501]
   Vieira C, 2018, COMPUT EDUC, V122, P119, DOI 10.1016/j.compedu.2018.03.018
   Wang YC, 2019, VISUAL COMPUT, V35, P1567, DOI 10.1007/s00371-018-1558-y
   Wortman D, 2007, SIGCSE 2007: PROCEEDINGS OF THE THIRTY-EIGHTH SIGCSE TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, P430, DOI 10.1145/1227504.1227458
   Zhang ZY, 2015, IEEE T VIS COMPUT GR, V21, P289, DOI 10.1109/TVCG.2014.2350494
   Zhang ZY, 2012, IEEE PAC VIS SYMP, P17
NR 45
TC 2
Z9 2
U1 3
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 477
EP 495
DI 10.1007/s00371-020-01818-4
EA FEB 2020
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000520062800001
DA 2024-07-18
ER

PT J
AU Lu, RR
   Zhu, F
   Wu, QX
   Fu, XY
AF Lu, Rongrong
   Zhu, Feng
   Wu, Qingxiao
   Fu, Xingyin
TI Search inliers based on redundant geometric constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Correspondence grouping; Geometric constraints; Correspondence voting;
   3D object recognition
ID OBJECT RECOGNITION; PERFORMANCE EVALUATION; 3D; REGISTRATION;
   HISTOGRAMS; ALGORITHM; FEATURES; IMAGES
AB This paper presents an efficient correspondence grouping algorithm to search inliers from an initial set of feature matches. The novelty lies in the proposal of a scoring technique for measuring the reliability of a triple combination (three pairs of matches) based on redundant geometric constraints. According to the proposed scoring method, several top-ranking triple combinations are selected for estimating the transformation hypotheses between two 3D shapes. For each transformation hypothesis, a correspondence from a selected correspondence set should cast a vote whether it is satisfying the geometric constraint with it. Finally, the transformation hypothesis with the most votes is considered as the best transformation and the correspondences from the initial correspondence set agreeing with the best transformation are grouped as inliers. We performed both comparative experiments and real application experiments to evaluate the performance of our proposed method on five popular datasets. The experimental results show the superior performance of our method with respect to different levels of noise, point density variation, partial overlap, clutter and occlusion. In addition, our proposed method can boost the performance of a feature-based 3D object recognition algorithm, giving an increase in both high recognition rate and computational efficiency.
C1 [Lu, Rongrong; Zhu, Feng; Wu, Qingxiao; Fu, Xingyin] Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.
   [Lu, Rongrong; Fu, Xingyin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Lu, Rongrong; Zhu, Feng; Wu, Qingxiao; Fu, Xingyin] Chinese Acad Sci, Key Lab Optoelect Informat Proc, Shenyang 110016, Peoples R China.
   [Lu, Rongrong; Zhu, Feng; Wu, Qingxiao; Fu, Xingyin] Key Lab Image Understanding & Comp Vis, Shenyang 110016, Liaoning, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences
RP Lu, RR (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.; Lu, RR (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.; Lu, RR (corresponding author), Chinese Acad Sci, Key Lab Optoelect Informat Proc, Shenyang 110016, Peoples R China.; Lu, RR (corresponding author), Key Lab Image Understanding & Comp Vis, Shenyang 110016, Liaoning, Peoples R China.
EM lurongrong@sia.cn; fzhu@sia.cn
OI Lu, Rongrong/0000-0001-6878-4049
FU NSFC [U1713216]; Autonomous subject of the State Key Laboratory of
   Robotics [2017-Z21]
FX The authors would like to acknowledge the Stan-ford 3D Scanning
   Repository, the University of Western Australia and the University of
   Bologna for providing their datasets. We also thank Dr. Jiaqi Yang for
   sharing the code to us. This work is jointly supported by the NSFC
   (U1713216) and Autonomous subject of the State Key Laboratory of
   Robotics (2017-Z21).
CR Aldoma A, 2012, LECT NOTES COMPUT SC, V7574, P511, DOI 10.1007/978-3-642-33712-3_37
   [Anonymous], 2014, P IEEE C COMP VIS PA
   [Anonymous], VIS COMPUT
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Boyer E., 2011, Proceedings of the 4th Eurographics Conference on 3D Object Retrieval, P71
   Buch AG, 2014, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR.2014.266
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Cho M, 2009, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2009.5459322
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319
   Ferencz IH, 2017, INT CONF 3D VISION, P374, DOI 10.1109/3DV.2017.00050
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu RR, 2017, OPT ENG, V56, DOI 10.1117/1.OE.56.12.123109
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Novatnack J, 2008, LECT NOTES COMPUT SC, V5304, P440, DOI 10.1007/978-3-540-88690-7_33
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Tombari F, 2013, INT J COMPUT VISION, V102, P198, DOI 10.1007/s11263-012-0545-4
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Tombari Federico, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology, P349, DOI DOI 10.1109/PSIVT.2010.65
   Yang JQ, 2017, INT CONF 3D VISION, P467, DOI 10.1109/3DV.2017.00060
   Yu Zhong, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P689, DOI 10.1109/ICCVW.2009.5457637
   Zhou W, 2019, VISUAL COMPUT, V35, P489, DOI 10.1007/s00371-018-1478-x
NR 34
TC 5
Z9 5
U1 1
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2020
VL 36
IS 2
BP 253
EP 266
DI 10.1007/s00371-018-1605-8
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ2TH
UT WOS:000511910300003
DA 2024-07-18
ER

PT J
AU Zhang, SD
   He, FZ
AF Zhang, Shengdong
   He, Fazhi
TI DRCDN: learning deep residual convolutional dehazing networks
SO VISUAL COMPUTER
LA English
DT Article
DE Residual learning; Dehazing; Image restoration; Global structure
   information; Deep learning
ID IMAGE; EFFICIENT; ALGORITHM; FUSION
AB Single image dehazing, which is the process of removing haze from a single input image, is an important task in computer vision. This task is extremely challenging because it is massively ill-posed. In this paper, we propose a novel end-to-end deep residual convolutional dehazing network (DRCDN) based on convolutional neural networks for single image dehazing, which consists of two subnetworks: one network is used for recovering a coarse clear image, and the other network is used to refine the result. The DRCDN firstly predicts the coarse clear image via a context aggregation subnetwork, which can capture global structure information. Subsequently, it adopts a novel hierarchical convolutional neural network to further refine the details of the clean image by integrating the local context information. The DRCDN is directly trained using complete images and the corresponding ground-truth haze-free images. Experimental results on synthetic datasets and natural hazy images demonstrate that the proposed method performs favorably against the state-of-the-art methods.
C1 [Zhang, Shengdong; He, Fazhi] Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
C3 Wuhan University
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
EM fzhe@whu.edu.cn
RI He, Fazhi/Q-3691-2018
FU NSFC [61472289, 41571436]
FX We thank anonymous reviewers very much for their suggestive comments.
   This work is partially supported by the NSFC (No. 61472289, 41571436).
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen X, 2019, MULTIMED TOOLS APPL, V78, P11173, DOI 10.1007/s11042-018-6690-1
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Doyle L, 2019, VISUAL COMPUT, V35, P1489, DOI 10.1007/s00371-018-1513-y
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fazl-Ersi E, 2019, VISUAL COMPUT, V35, P1447, DOI 10.1007/s00371-018-1510-1
   Fu X, 2017, INT CONF ACOUST SPEE, P5855, DOI 10.1109/ICASSP.2017.7953279
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou N, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8184-3
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li HR, 2019, APPL MATH SER B, V34, P1, DOI 10.1007/s11766-019-3706-1
   Li HR, 2020, SOFT COMPUT, V24, P6851, DOI 10.1007/s00500-019-04324-5
   Li K, 2019, FRONT COMPUT SCI-CHI, V13, P1116, DOI 10.1007/s11704-018-6442-4
   Li K, 2018, J COMPUT SCI TECH-CH, V33, P223, DOI 10.1007/s11390-017-1764-5
   Liu Ding., 2017, International Joint Conference on Artificial Intelligence, P842
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Luo JK, 2020, INTELL DATA ANAL, V24, P581, DOI 10.3233/IDA-194641
   Mbelwa JT, 2020, VISUAL COMPUT, V36, P1173, DOI 10.1007/s00371-019-01727-1
   Mehta Sachin, 2018, P EUR C COMP VIS ECC, P552, DOI DOI 10.1007/978-3-030-01249-6_34
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Oliver I, 2018, 2018 11TH INTERNATIONAL CONFERENCE ON THE QUALITY OF INFORMATION AND COMMUNICATIONS TECHNOLOGY (QUATIC), P229, DOI 10.1109/QUATIC.2018.00041
   Pan YT, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8123-3
   Pan YT, 2020, APPL INTELL, V50, P314, DOI 10.1007/s10489-019-01542-0
   Pan YT, 2019, NEUROCOMPUTING, V332, P137, DOI 10.1016/j.neucom.2018.12.025
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Sulami M, 2014, IEEE INT CONF COMPUT
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Tan RT, 2008, COMPUTER VISION PATT, P1
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Umer S, 2019, VISUAL COMPUT, V35, P1327, DOI 10.1007/s00371-018-1544-4
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Yan YY, 2019, IEEE T INF FOREN SEC, V14, P5, DOI 10.1109/TIFS.2018.2834155
   Yong JS, 2019, APPL MATH SER B, V34, P480, DOI 10.1007/s11766-019-3714-1
   You S, 2018, 2018 32ND INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P706, DOI 10.1109/ICOIN.2018.8343210
   Yu F., 2015, ARXIV
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Yu HP, 2019, MULTIMED TOOLS APPL, V78, P11779, DOI 10.1007/s11042-018-6735-5
   Yu HP, 2018, MULTIMED TOOLS APPL, V77, P24097, DOI 10.1007/s11042-018-5697-y
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang SD, 2018, IEEE INT CON MULTI
   Zhang SD, 2020, VISUAL COMPUT, V36, P305, DOI 10.1007/s00371-018-1612-9
   Zhang WX, 2019, PROC CVPR IEEE, P12428, DOI 10.1109/CVPR.2019.01272
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 64
TC 140
Z9 142
U1 5
U2 60
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2020
VL 36
IS 9
BP 1797
EP 1808
DI 10.1007/s00371-019-01774-8
EA NOV 2019
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NA2KY
UT WOS:000499416600002
DA 2024-07-18
ER

PT J
AU Xue, ZC
   Dong, J
   Zhao, YX
   Liu, C
   Chellali, R
AF Xue, Zhichao
   Dong, Jing
   Zhao, Yuxin
   Liu, Chang
   Chellali, Ryad
TI Low-rank and sparse matrix decomposition via the truncated nuclear norm
   and a sparse regularizer
SO VISUAL COMPUTER
LA English
DT Article
DE Low-rank and sparse decomposition; Truncated nuclear norm; Sparse
   regularizer; Face image shadow removal; Video background subtraction
ID THRESHOLDING ALGORITHM; BACKGROUND SUBTRACTION; FACE RECOGNITION; ROBUST
   PCA; RECOVERY
AB Recovering the low-rank and sparse components from a given matrix is a challenging problem that has many real applications. This paper proposes a novel algorithm to address this problem by introducing a sparse prior on the low-rank component. Specifically, the low-rank component is assumed to be sparse in a transform domain and a sparse regularizer formulated as an l(1)-norm term is employed to promote the sparsity. The truncated nuclear norm is used to model the low-rank prior, rather than the nuclear norm used in most existing methods, to achieve a better approximation to the rank of the considered matrix. Furthermore, an efficient solving method based on a two-stage iterative scheme is developed to address the raised optimization problem. The proposed algorithm is applied to deal with synthetic data and real applications including face image shadow removal and video background subtraction, and the experimental results validate the effectiveness and accuracy of the proposed approach as compared with other methods.
C1 [Xue, Zhichao; Dong, Jing; Chellali, Ryad] Nanjing Tech Univ, Coll Elect Engn & Control Sci, Nanjing, Jiangsu, Peoples R China.
   [Zhao, Yuxin; Liu, Chang] Harbin Engn Univ, Coll Automat, Harbin, Heilongjiang, Peoples R China.
C3 Nanjing Tech University; Harbin Engineering University
RP Dong, J (corresponding author), Nanjing Tech Univ, Coll Elect Engn & Control Sci, Nanjing, Jiangsu, Peoples R China.
EM jingdong.njtech@outlook.com
RI Zhao, Yuxin/HNS-3187-2023; chellali, ryad/KLC-5875-2024
OI chellali, ryad/0000-0003-3395-2254
FU Natural Science Foundation of the Higher Education Institutions of
   Jiangsu Province of China [17KJB510025]; Natural Science Foundation of
   China [41676088]; Major Basic Research Program for National Security of
   China (973 Program for National Defence) [613317]
FX This work was supported by the Natural Science Foundation of the Higher
   Education Institutions of Jiangsu Province of China (17KJB510025), the
   Natural Science Foundation of China (41676088) and the Major Basic
   Research Program for National Security of China (973 Program for
   National Defence, No. 613317).
CR [Anonymous], COMPUT ADV MULTISENS
   [Anonymous], 2002, THESIS STANFORD U
   [Anonymous], ARXIV171109492
   [Anonymous], 2010, 100920105055 ARXIV
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2012, AS C COMP VIS
   [Anonymous], PREPRINT
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bhardwaj A, 2016, VISUAL COMPUT, V32, P591, DOI 10.1007/s00371-015-1075-1
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Cao FL, 2017, NEURAL NETWORKS, V85, P10, DOI 10.1016/j.neunet.2016.09.005
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Fei LK, 2017, PATTERN RECOGN, V67, P252, DOI 10.1016/j.patcog.2017.02.017
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   Goldfarb D, 2013, MATH PROGRAM, V141, P349, DOI 10.1007/s10107-012-0530-2
   Grant M., 2008, GLOB OPTIM, V1, P155
   Hai YA, 2015, DISCOV MED, V19, P311
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   Javed S, 2017, IEEE T IMAGE PROCESS, V26, P5840, DOI 10.1109/TIP.2017.2746268
   Javed S, 2015, LECT NOTES COMPUT SC, V9280, P340, DOI 10.1007/978-3-319-23234-8_32
   Kang Z, 2015, IEEE DATA MINING, P211, DOI 10.1109/ICDM.2015.15
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liang Xiong, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P844, DOI 10.1109/ICDM.2011.52
   Liu YG, 2013, VISUAL COMPUT, V29, P421, DOI 10.1007/s00371-012-0745-5
   Liu YY, 2013, NEURAL NETWORKS, V48, P8, DOI 10.1016/j.neunet.2013.06.013
   Luan X, 2014, PATTERN RECOGN, V47, P495, DOI 10.1016/j.patcog.2013.06.031
   Mansour H, 2015, INT CONF ACOUST SPEE, P4065, DOI 10.1109/ICASSP.2015.7178735
   McClain K, 2009, STUD MATH THINK LEAR, P56
   Merhav N, 1998, IEEE T CIRC SYST VID, V8, P378, DOI 10.1109/76.709404
   Oliver N., 1999, Computer Vision Systems. First International Conference, ICVS'99. Proceedings, P255
   Porwik P., 2004, Machine Graphics & Vision, V13, P79
   Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P2004, DOI 10.1109/TSP.2017.2649482
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Rodriguez P, 2016, J MATH IMAGING VIS, V55, P1, DOI 10.1007/s10851-015-0610-z
   Seidel F, 2014, MACH VISION APPL, V25, P1227, DOI 10.1007/s00138-013-0555-4
   Shan G, 2017, VISUAL COMPUT, V2, P1
   Sobral A., 2015, P 12 IEEE INT C ADV, P1, DOI DOI 10.1109/AVSS.2015.7301753
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Sobral Andrews., Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing
   Tütüncü RH, 2003, MATH PROGRAM, V95, P189, DOI 10.1007/s10107-002-0347-5
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419
   Yadong Mu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2609, DOI 10.1109/CVPR.2011.5995369
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983
   Yuan X., 2009, SPARSE LOW RANK MATR
   Zhang HY, 2015, NEURAL COMPUT, V27, P1915, DOI 10.1162/NECO_a_00762
   Zhou T., 2011, P 28 INT C MACHINE L, P33
NR 57
TC 32
Z9 39
U1 8
U2 48
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2019
VL 35
IS 11
SI SI
BP 1549
EP 1566
DI 10.1007/s00371-018-1555-1
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JD5IZ
UT WOS:000490018000005
DA 2024-07-18
ER

PT J
AU Kang, N
   Bai, JX
   Pan, JJ
   Qin, H
AF Kang, Ning
   Bai, Junxuan
   Pan, Junjun
   Qin, Hong
TI Interactive animation generation of virtual characters using single
   RGB-D camera
SO VISUAL COMPUTER
LA English
DT Article
DE Character animation; Motion capture; Retargeting; RGB-D camera
ID HUMAN MOTION
AB The rapid creation of 3D character animation by commodity devices plays an important role in enriching visual content in virtual reality. This paper concentrates on addressing the challenges of current motion imitation for human body. We develop an interactive framework for stable motion capturing and animation generation based on single Kinect device. In particular, we focus our research efforts on two cases: (1) The participant is facing the camera; or (2) the participant is turning around or is side facing the camera. Using existing methods, camera could obtain a profile view of the body, but it frequently leads to less satisfactory result or even failure due to occlusion. In order to reduce certain artifacts appeared at the side view, we design a mechanism to refine the movement of the human body by integrating an adaptive filter. After specifying the corresponding joints between the participant and the virtual character, the captured motion could be retargeted in a quaternion-based manner. To further improve the animation quality, inverse kinematics are brought into our framework to constrain the target's positions. A large variety of motions and characters have been tested to validate the performance of our framework. Through experiments, it shows that our method could be applied to real-time applications, such as physical therapy and fitness training.
C1 [Kang, Ning; Bai, Junxuan; Pan, Junjun] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Kang, Ning; Pan, Junjun] Beihang Univ, Qingdao Inst, Qingdao, Shandong, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Beihang University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook
RP Pan, JJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Pan, JJ (corresponding author), Beihang Univ, Qingdao Inst, Qingdao, Shandong, Peoples R China.
EM pan_junjun@hotmail.com; qin@cs.stonybrook.edu
RI Bai, Junxuan/P-7282-2018; Pan, Junjun/A-1316-2013
OI Bai, Junxuan/0000-0002-7941-0584; 
FU National Key R&D Program of China [2017YFB1002602]; National Natural
   Science Foundation of China [61532002, 61672149, 61872020, 61872347];
   NSF [IIS-1715985, IIS-1812606]
FX This research has been supported by National Key R&D Program of China
   (Grant No. 2017YFB1002602), NationalNatural Science Foundation of China
   (Grant Nos. 61532002, 61672149, 61872020, 61872347), NSF IIS-1715985,
   NSF IIS-1812606.
CR Abdul-Massih M, 2017, COMPUT GRAPH FORUM, V36, P86, DOI 10.1111/cgf.12860
   [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   [Anonymous], 2010, Animating non-humanoid characters with human motion data
   Baciu G, 2006, COMPUT ANIMAT VIRT W, V17, P41, DOI 10.1002/cav.72
   Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265
   Celikcan U, 2015, COMPUT GRAPH FORUM, V34, P216, DOI 10.1111/cgf.12507
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   Fang AC, 2003, ACM T GRAPHIC, V22, P417, DOI 10.1145/882262.882286
   Grochow K, 2004, ACM T GRAPHIC, V23, P522, DOI 10.1145/1015706.1015755
   Guo SH, 2015, VISUAL COMPUT, V31, P497, DOI 10.1007/s00371-014-0943-4
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Kwon T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2983616
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Liu ZG, 2016, IEEE T VIS COMPUT GR, V22, P2437, DOI 10.1109/TVCG.2015.2510000
   Lv XL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982440
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Monzani JS, 2000, COMPUT GRAPH FORUM, V19, pC11, DOI 10.1111/1467-8659.00393
   Mousas C, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P57, DOI 10.1109/VR.2018.8446498
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Park MJ, 2004, COMPUT ANIMAT VIRT W, V15, P245, DOI 10.1002/cav.27
   Popovic Z, 1999, COMP GRAPH, P11, DOI 10.1145/311535.311536
   Rhodin H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818082
   Rhodin H, 2014, COMPUT GRAPH FORUM, V33, P273, DOI 10.1111/cgf.12325
   Roth D, 2016, P IEEE VIRT REAL ANN, P275, DOI 10.1109/VR.2016.7504760
   Villegas R, 2018, PROC CVPR IEEE, P8639, DOI 10.1109/CVPR.2018.00901
   Wang KK, 2017, IEEE T IMAGE PROCESS, V26, P5966, DOI 10.1109/TIP.2017.2740624
   Ye M, 2016, IEEE T PATTERN ANAL, V38, P1517, DOI 10.1109/TPAMI.2016.2557783
NR 27
TC 10
Z9 10
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 849
EP 860
DI 10.1007/s00371-019-01678-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200007
OA Bronze
DA 2024-07-18
ER

PT J
AU Sathish, D
   Kamath, S
   Prasad, K
   Kadavigere, R
AF Sathish, Dayakshini
   Kamath, Surekha
   Prasad, Keerthana
   Kadavigere, Rajagopal
TI Role of normalization of breast thermogram images and automatic
   classification of breast cancer
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 6th International Conference on Virtual Reality and Visualization
   (ICVRV)
CY SEP 24-26, 2016
CL Hangzhou, PEOPLES R CHINA
SP China Soc Image & Graph, China Comp Federat, China Syst Simulat Assoc, IEEE Comp Soc, Connected Universal Experiences Labs Inc, China Soc Image & Graph, VR Comm, China Comp Federat, VR & Visualizat Comm, China Syst Simulat Assoc, VR Comm, China Syst Simulat Assoc, Digital Entertainment Comm, China Syst Simulat Assoc, Surgery Simulat Comm
DE Breast cancer; Breast thermography; Asymmetry analysis; Normalization;
   Wavelet local energy; Random subset feature selection; Genetic algorithm
ID INFRARED THERMOGRAPHY; ASYMMETRY ANALYSIS; TEXTURE FEATURES; EXTRACTION
AB Breast thermography is a non-invasive imaging technique used for early detection of breast cancer based on temperatures. Temperature matrix of breast provides minute variations in temperatures, which is significant in early detection of breast cancer. The minimum, maximum temperatures and the their range may be different for each breast thermogram. Normalization of temperature matrices of breast thermograms is essential to bring the different range of temperatures to the common scale. In this article, we demonstrate the importance of temperature matrix normalization of breast thermograms. This paper also proposes a novel method for automatically classifying breast thermogram images using local energy features of wavelet sub-bands. A significant subset of features is selected by a random subset feature selection (RSFS) and genetic algorithm. Features selected by RSFS method are found to be relevant in detection of asymmetry between right and left breast. We have obtained an accuracy of 91%, sensitivity 87.23% and specificity 94.34% using SVM Gaussian classifier for normalized breast thermograms. Accuracy of classification between a set of hundred normalized and corresponding set of non-normalized breast thermograms are compared. An increase in accuracy of 16% is obtained for normalized breast thermograms in comparison with non-normalized breast thermograms.
C1 [Sathish, Dayakshini; Kamath, Surekha] Manipal Univ, ICE Dept, MIT Manipal, Manipal 576104, Karnataka, India.
   [Prasad, Keerthana] Manipal Univ, SOIS Manipal, Manipal 576104, Karnataka, India.
   [Kadavigere, Rajagopal] Manipal Univ, KMC Manipal, Manipal 576104, Karnataka, India.
C3 Manipal Academy of Higher Education (MAHE); Manipal Academy of Higher
   Education (MAHE); Manipal Academy of Higher Education (MAHE); Kasturba
   Medical College, Manipal
RP Kamath, S (corresponding author), Manipal Univ, ICE Dept, MIT Manipal, Manipal 576104, Karnataka, India.
EM dayakshini@gmail.com; surekha.kamath@manipal.edu;
   keerthana.prasad@manipal.edu; rajagopalkv@yahoo.com
RI Sathish, Dayakshini/ABH-3934-2020; Kamath, S/HLP-9093-2023; ,
   Dayakshini/Q-8309-2019
OI Sathish, Dayakshini/0000-0002-0952-9176; Kamath,
   surekha/0000-0001-5567-166X
CR Acharya UR, 2014, EXPERT SYST, V31, P37, DOI 10.1111/j.1468-0394.2012.00654.x
   Acharya UR, 2012, J MED SYST, V36, P1503, DOI 10.1007/s10916-010-9611-z
   Ali MAS, 2015, ACSIS-ANN COMPUT SCI, V5, P255, DOI 10.15439/2015F318
   Amirolad A, 2016, VISUAL COMPUT, V32, P1633, DOI 10.1007/s00371-016-1220-5
   Araujo M. C. D., 2008, 12 BRAZ C THERM ENG
   Araújo MC, 2014, EXPERT SYST APPL, V41, P6728, DOI 10.1016/j.eswa.2014.04.027
   Bagavathiappan S, 2013, INFRARED PHYS TECHN, V60, P35, DOI 10.1016/j.infrared.2013.03.006
   Berbar MA, 2014, VISUAL COMPUT, V30, P19, DOI 10.1007/s00371-013-0774-8
   Bezerra LA, 2013, SIGNAL PROCESS, V93, P2851, DOI 10.1016/j.sigpro.2012.06.002
   Borchartt T. B., 2011, P 21 BRAZ C MECH ENG, P24
   Borchartt TB, 2013, SIGNAL PROCESS, V93, P2785, DOI 10.1016/j.sigpro.2012.08.012
   Brioschi ML, 2011, FLIR TECH SER APPL N, V8, P1
   Devi VS, 2013, PATTERN RECOGNITION
   EtehadTavakol M, 2013, INT J THERM SCI, V69, P21, DOI 10.1016/j.ijthermalsci.2013.03.001
   Fernández-Cuevas I, 2015, INFRARED PHYS TECHN, V71, P28, DOI 10.1016/j.infrared.2015.02.007
   de Souza GAGR, 2015, EINSTEIN-SAO PAULO, V13, P518, DOI 10.1590/S1679-45082015AO3392
   Gogoi UR, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ADVANCED COMPUTING AND COMMUNICATION (ISACC), P258, DOI 10.1109/ISACC.2015.7377351
   Gonzalez R.C., 2005, Digital image processing
   Jayaraman S., 2012, DIGITAL IMAGE PROCES
   Krawczyk B, 2014, APPL SOFT COMPUT, V20, P112, DOI 10.1016/j.asoc.2013.11.011
   Kumar V., 2014, SMART COMPUTING REV, V4, P211, DOI [10.6029/smartcr.2014.03.007, DOI 10.6029/SMARTCR.2014.03.007, DOI 10.1145/2740070.2626320]
   Lahiri BB, 2012, INFRARED PHYS TECHN, V55, P221, DOI 10.1016/j.infrared.2012.03.007
   Lashkari AmirEhsan, 2016, J Med Signals Sens, V6, P12
   Ludwig O, 2010, IEEE T NEURAL NETWOR, V21, P972, DOI 10.1109/TNN.2010.2046423
   Martis R.J., 2014, Machine learning in healthcare informatics, P25, DOI DOI 10.1007/978-3-642-40017-9_2
   Milosevic M, 2014, EXCLI J, V13, P1204
   Ng EYK, 2004, BMC CANCER, V4, DOI 10.1186/1471-2407-4-17
   Nicandro CR, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/264246
   Pohjalainen J, 2015, COMPUT SPEECH LANG, V29, P145, DOI 10.1016/j.csl.2013.11.004
   Pramanik S, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ADVANCED COMPUTING AND COMMUNICATION (ISACC), P205, DOI 10.1109/ISACC.2015.7377343
   Qi HR, 2000, P ANN INT IEEE EMBS, V22, P1227, DOI 10.1109/IEMBS.2000.897952
   Sathish Dayakshini, 2016, International Journal of Medical Engineering and Informatics, V8, P275
   Sathish D, 2017, SIGNAL IMAGE VIDEO P, V11, P745, DOI 10.1007/s11760-016-1018-y
   Schaefer G, 2009, PATTERN RECOGN, V42, P1133, DOI 10.1016/j.patcog.2008.08.007
   Siegel RL., 2019, ANTI-CANCER DRUG, V69, P7, DOI [DOI 10.3322/caac.20115, DOI 10.1097/CAD.0000000000000617]
   Silva LF, 2014, J MED IMAG HEALTH IN, V4, P92, DOI 10.1166/jmihi.2014.1226
   Sonka M., 2004, IMAGE PROCESSING ANA
   Suganthi SS, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0101-6
   Tang XW, 2005, P ANN INT IEEE EMBS, P1680
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Zhuo L, 2015, INT ARCH PHOTOGRAMM, VXXXVII, P397
NR 41
TC 33
Z9 34
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2019
VL 35
IS 1
BP 57
EP 70
DI 10.1007/s00371-017-1447-9
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HL6ZO
UT WOS:000458885900006
DA 2024-07-18
ER

PT J
AU Khmag, A
   Al Haddad, SAR
   Ramlee, RA
   Kamarudin, N
   Malallah, FL
AF Khmag, Asem
   Al Haddad, S. A. R.
   Ramlee, R. A.
   Kamarudin, Noraziahtulhidayu
   Malallah, Fahad Layth
TI Natural image noise removal using nonlocal means and hidden Markov
   models in transform domain
SO VISUAL COMPUTER
LA English
DT Article
DE Gaussian noise; Nonlocal means; hidden Markov model; Spatial filter;
   Noise removal; Classification clustering
ID DENOISING FILTER; CLASSIFICATION; ALGORITHM
AB Nonlocal means (NLM) which utilizes the self-similarity is considered as one of the most popular denoising techniques. Although NLM can attain significant performance, it shows a few loopholes, such as its computational complexity when it comes to similarity measurements, and the small number of sufficient candidates that use to choose the target patches which have complicated textures. In this paper, the use of clustering based on moment invariants and the hidden Markov model (HMM) is proposed to achieve preclassification and thus capture the dependency between additive white Gaussian noise pixel and its neighbors on the wavelet transform. The HMM also allows hidden states to connect to one another to capture the dependencies among coefficients in the transform domain. In the practical part, the experimental results present that the proposed algorithm has the ability to show denoised images better than the results of state-of-the-art denoising methods both objectively in peak signal-to-noise ratio and structural similarity and subjectively using visual results, especially when the noise level is high.
C1 [Khmag, Asem] Zawia Univ, Fac Engn, Zawiya, Libya.
   [Al Haddad, S. A. R.; Ramlee, R. A.; Kamarudin, Noraziahtulhidayu] Univ Putra Malaysia, Fac Engn, Serdang, Malaysia.
   [Malallah, Fahad Layth] Cihan Univ, Fac Comp Sci, Sulaimaniya, Iraq.
C3 Universiti Putra Malaysia; Cihan University-Erbil
RP Khmag, A (corresponding author), Zawia Univ, Fac Engn, Zawiya, Libya.
EM khmaj2002@gmail.com; sar@upm.my; ridza@utem.edu.my;
   hidayu.kamarudin@gmail.com; fahad.layth.86@gmail.com
RI kamarudin, noraziahtulhidayu/AAQ-8508-2021; Al-Haddad, S. A.
   R./AAM-6449-2020; , null/ACB-2995-2022; Malallah, Fahad
   Layth/G-8406-2019; Khmag, Asem/AAH-1051-2019
OI Malallah, Fahad Layth/0000-0001-6067-7302; Khmag,
   Asem/0000-0002-1360-5346; Kamarudin,
   Noraziahtulhidayu/0000-0001-7467-4348
CR [Anonymous], INDIAN J SCI TECHNOL
   [Anonymous], 2017, VISUAL COMPUTER
   [Anonymous], THESIS
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen GY, 2011, IEEE T GEOSCI REMOTE, V49, P973, DOI 10.1109/TGRS.2010.2075937
   Coupé P, 2008, IEEE T MED IMAGING, V27, P425, DOI 10.1109/TMI.2007.906087
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Demir B, 2011, IEEE GEOSCI REMOTE S, V8, P220, DOI 10.1109/LGRS.2010.2058996
   Goossens B., 2008, 2008 International Workshop on Local and Non-Local Approximation in Image Processing, P143
   Grewenig S, 2011, J VIS COMMUN IMAGE R, V22, P117, DOI 10.1016/j.jvcir.2010.11.001
   Khmag A, 2017, VISUAL COMPUT, V33, P1141, DOI 10.1007/s00371-016-1273-5
   Khmag A, 2016, IEEJ T ELECTR ELECTR, V11, P339, DOI 10.1002/tee.22223
   Khmag A, 2015, J MED IMAG HEALTH IN, V5, P1261, DOI 10.1166/jmihi.2015.1523
   Mahmoudi M, 2005, IEEE SIGNAL PROC LET, V12, P839, DOI 10.1109/LSP.2005.859509
   Pang C., 2009, IEEE INT WORKSHOP MU, P1
   Salmon J, 2010, IEEE SIGNAL PROC LET, V17, P269, DOI 10.1109/LSP.2009.2038954
   Shao L, 2008, IEEE T IMAGE PROCESS, V17, P1772, DOI 10.1109/TIP.2008.2002162
   Thaipanich T, 2010, IEEE T CONSUM ELECTR, V56, P2623, DOI 10.1109/TCE.2010.5681149
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang J, 2006, IEEE IMAGE PROC, P1429, DOI 10.1109/ICIP.2006.312698
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu G, 2017, COMPUT AIDED DESIGN, V91, P1, DOI 10.1016/j.cad.2017.04.002
   Yan RM, 2012, J DISP TECHNOL, V8, P212, DOI 10.1109/JDT.2011.2181487
NR 23
TC 36
Z9 36
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1661
EP 1675
DI 10.1007/s00371-017-1439-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Lopes, DS
   Parreira, PF
   Mendes, AR
   Pires, VM
   Paulo, SF
   Sousa, C
   Jorge, JA
AF Lopes, Daniel Simoes
   Parreira, Pedro F.
   Mendes, Ana R.
   Pires, Vasco M.
   Paulo, Soraia F.
   Sousa, Carlos
   Jorge, Joaquim A.
TI Explicit design of transfer functions for volume-rendered images by
   combining histograms, thumbnails, and sketch-based interaction
SO VISUAL COMPUTER
LA English
DT Article
DE Transfer function; Volume rendering; 3D medical images; Sketch-based
   interfaces; Thumbnails; User study
ID DATA SETS; VISUALIZATION; SEE; EXPLORATION; BOUNDARIES
AB Visual quality of volume rendering for medical imagery strongly depends on the underlying transfer function. Conventional Windows-Icons-Menus-Pointer interfaces typically refer the user to browse a lengthy catalog of predefined transfer functions or to pain-staking refine the transfer function by clicking and dragging several independent handles. To turn the standard design process less difficult and tedious, this paper proposes novel interactions on a sketch-based interface that supports the design of 1D transfer functions via touch gestures to directly control voxel opacity and easily assign colors. User can select different types of transfer function shapes including ramp function, free hand curve drawing, and slider bars similar to those of a mixing table. An assorted array of thumbnails provides an overview of the data when editing the transfer function. User performance is evaluated by comparing the time and effort necessary to complete a number of tests with sketch-based and conventional interfaces. Users were able to more rapidly explore and understand volume data using the sketch-based interface, as the number of design iterations necessary to obtain a desirable transfer function was reduced. In addition, informal evaluation sessions carried out with professionals (two senior radiologists, a general surgeon and two scientific illustrators) provided valuable feedback on how suitable the sketch-based interface is for illustration, patient communication and medical education.
C1 [Lopes, Daniel Simoes; Paulo, Soraia F.; Jorge, Joaquim A.] INESC ID Lisboa, IST Taguspk,Room 2N9-1,Ave Prof Cavaco Silva, P-2744016 Porto Salvo, Portugal.
   [Parreira, Pedro F.; Mendes, Ana R.; Pires, Vasco M.; Jorge, Joaquim A.] Univ Lisbon, Inst Super Tecn, Av Rovisco Pais 1, P-1049001 Lisbon, Portugal.
   [Sousa, Carlos] Hosp Prof Doutor Fernando Fonseca, Ctr Res & Creat Informat, EPE, IC19, P-2720276 Amadora, Portugal.
C3 INESC-ID; Universidade de Lisboa; Hospital Professor Doutor Fernando
   Fonseca, EPE
RP Lopes, DS (corresponding author), INESC ID Lisboa, IST Taguspk,Room 2N9-1,Ave Prof Cavaco Silva, P-2744016 Porto Salvo, Portugal.
EM daniel.lopes@inesc-id.pt; pedro.f.parreira@tecnico.ulisboa.pt;
   ana.rita.monteiro.mendes@tecnico.ulisboa.pt;
   vasco.pires@tecnico.ulisboa.pt; soraiafpaulo@inesc-id.pt;
   carlos.sousa@hff.min-saude.pt; jorgej@acm.org
RI Simoes Lopes, Daniel/M-2930-2015; Jorge, Joaquim/C-5596-2008
OI Simoes Lopes, Daniel/0000-0003-0917-9396; Jorge,
   Joaquim/0000-0001-5441-4637; F Paulo, Soraia/0000-0002-0812-1072
FU Portuguese Foundation for Science and Technology (FCT); national funds
   through FCT [UID/CEC/50021/2013, IT-MEDEX PTDC/EEI-SII/6038/2014]; 
   [SFRH/BPD/97449/2013]; Fundação para a Ciência e a Tecnologia
   [SFRH/BPD/97449/2013] Funding Source: FCT
FX All authors are thankful for the financial support given by Portuguese
   Foundation for Science and Technology (FCT). In particular, the first
   author thanks for the postdoctoral grant SFRH/BPD/97449/2013. This work
   was also partially supported by national funds through FCT with
   reference UID/CEC/50021/2013 and IT-MEDEX PTDC/EEI-SII/6038/2014.
CR Arens S., 2010, Proceedings of the 8th IEEE/EG International Conference on Volume Graphics, VG'10, P77
   Brix T., 2015, P EUR WORKSH VIS COM, P83
   Chevalier F, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P281
   Corcoran A, 2010, COMPUT GRAPH-UK, V34, P388, DOI 10.1016/j.cag.2010.03.014
   Dolnicar S, 2011, INT J MARKET RES, V53, P231, DOI 10.2501/IJMR-53-2-231-252
   Freiman M, 2007, INT J CARS, V2, P125
   Guo HQ, 2013, IEEE PAC VIS SYMP, P65, DOI 10.1109/PacificVis.2013.6596129
   Guo HQ, 2011, IEEE T VIS COMPUT GR, V17, P2106, DOI 10.1109/TVCG.2011.261
   Higuera FV, 2004, PROC SPIE, V5367, P275, DOI 10.1117/12.535534
   Hurter C, 2014, IEEE PAC VIS SYMP, P225, DOI 10.1109/PacificVis.2014.61
   Ip CY, 2012, IEEE T VIS COMPUT GR, V18, P2355, DOI 10.1109/TVCG.2012.231
   Isenberg T., 2011, INRIA, P24
   Jönsson D, 2016, IEEE T VIS COMPUT GR, V22, P896, DOI 10.1109/TVCG.2015.2467294
   Kindlmann G., 2002, COURSE NOTES ACM SIG, V3
   Kniss J, 2001, IEEE VISUAL, P255, DOI 10.1109/VISUAL.2001.964519
   Latulipe C, 2006, SYMMETRIC INTERACTIO
   Li L, 2016, COMPUT METH PROG BIO, V126, P76, DOI 10.1016/j.cmpb.2015.11.014
   Liu BC, 2010, GRAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P254
   LUNDSTROM C, 2006, P EUR IEEE VGTC S VI
   Marks J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P389, DOI 10.1145/258734.258887
   Prani J. S., 2009, P 14 INT FALL WORKSH, P285
   Prassni JS, 2010, IEEE PAC VIS SYMP, P9, DOI 10.1109/PACIFICVIS.2010.5429624
   Roettger S., 2005, Eurographics IEEE VGTC Symposium on Visualization, P271, DOI DOI 10.2312/VISSYM/EUROVIS05/271-278
   Ropinski Timo., 2008, IEEEEG INT S VOLUME, P41
   Sereda P, 2006, IEEE T VIS COMPUT GR, V12, P208, DOI 10.1109/TVCG.2006.39
   Vilanova A, 2001, SPRING CONFERENCE ON COMPUTER GRAPHICS, PROCEEDINGS, P241, DOI 10.1109/SCCG.2001.945360
   Wiebel A, 2012, IEEE T VIS COMPUT GR, V18, P2236, DOI 10.1109/TVCG.2012.292
   Wu YC, 2007, IEEE T VIS COMPUT GR, V13, P1027, DOI 10.1109/TVCG.2007.1051
NR 28
TC 3
Z9 3
U1 0
U2 20
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2018
VL 34
IS 12
BP 1713
EP 1723
DI 10.1007/s00371-017-1448-8
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GY3WM
UT WOS:000448487400008
DA 2024-07-18
ER

PT J
AU Son, TG
   Lee, J
   Lim, J
   Lee, K
AF Son, Tae-geun
   Lee, Jusung
   Lim, Jeonghun
   Lee, Kunwoo
TI Reassembly of fractured objects using surface signature
SO VISUAL COMPUTER
LA English
DT Article
DE Reassembly; Broken objects; Fractured surface; Surface signature
ID 3D; PUZZLES
AB As 3D object acquisition technology has improved, research has been actively conducted on the virtual reassembly of broken 3D objects. However, because fractured surfaces are bumpy and complicated, it is difficult to extract salient features from them. In this paper, we propose a new simple descriptor, the surface signature, to delineate a fractured surface and to find the counterpart fractured surface effectively. This descriptor is based on the convex/concave information of a point on the fractured surface. To apply the descriptor to reassembly, feature curves are extracted from the boundaries of the surface signature. The similarity between two fractured surfaces is calculated based on the spin images of feature curve points and the distance and normal deviation between two feature curves to find the matching fractured surface. Several reassembling experiments using the surface signature are performed on real-world objects, and it is shown that the proposed descriptor can be effectively applied to the reassembly process.
C1 [Son, Tae-geun; Lee, Jusung] Seoul Natl Univ, Seoul, South Korea.
   [Lim, Jeonghun] Seoul Natl Univ, Human Ctr CAD Lab, Seoul, South Korea.
   [Lee, Kunwoo] Seoul Natl Univ, Sch Mech & Aerosp Engn, Seoul, South Korea.
   [Lee, Kunwoo] Seoul Natl Univ, Coll Engn, Seoul, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU); Seoul
   National University (SNU); Seoul National University (SNU)
RP Lee, K (corresponding author), Seoul Natl Univ, Sch Mech & Aerosp Engn, Seoul, South Korea.; Lee, K (corresponding author), Seoul Natl Univ, Coll Engn, Seoul, South Korea.
EM kunwoohccl@gmail.com
CR Altantsetseg E, 2014, VISUAL COMPUT, V30, P929, DOI 10.1007/s00371-014-0959-9
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], CARTOGRAPHICA INT J
   Au OKC, 2012, IEEE T VIS COMPUT GR, V18, P1125, DOI 10.1109/TVCG.2011.131
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Johnson A., 1997, Thesis
   Johnson AE, 1998, IMAGE VISION COMPUT, V16, P635, DOI 10.1016/S0262-8856(98)00074-2
   Kong WX, 2001, PROC CVPR IEEE, P583
   Lavoué G, 2007, APGV 2007: SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, PROCEEDINGS, P57
   Papaioannou G, 2003, IMAGE VISION COMPUT, V21, P401, DOI 10.1016/S0262-8856(03)00008-8
   Papaionnou G, 2001, IEEE COMPUT GRAPH, V21, P53, DOI 10.1109/38.909015
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   Üçoluk G, 1999, COMPUT GRAPH-UK, V23, P573, DOI 10.1016/S0097-8493(99)00075-8
   Willis A, 2004, INT C PATT RECOG, P96, DOI 10.1109/ICPR.2004.1333714
   Winkelbach S, 2008, INT J COMPUT VISION, V78, P1, DOI 10.1007/s11263-007-0121-5
NR 19
TC 7
Z9 8
U1 2
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1371
EP 1381
DI 10.1007/s00371-017-1419-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400008
DA 2024-07-18
ER

PT J
AU Bi, L
   Feng, DG
   Kim, J
AF Bi, Lei
   Feng, Dagan
   Kim, Jinman
TI Dual-Path Adversarial Learning for Fully Convolutional Network
   (FCN)-Based Medical Image Segmentation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Adversarial learning; Fully convolutional networks (FCNs); Segmentation;
   Regions of interest (ROI)
ID CHEST RADIOGRAPHS; NEURAL-NETWORKS
AB Segmentation of regions of interest (ROIs) in medical images is an important step for image analysis in computer-aided diagnosis systems. In recent years, segmentation methods based on fully convolutional networks (FCNs) have achieved great success in general images. FCN performance is primarily due to it leveraging large labeled datasets to hierarchically learn the features that correspond to the shallow appearance as well as the deep semantics of the images. However, such dependence on large dataset does not translate well into medical images where there is a scarcity of annotated medical training data, and FCN results in coarse ROI detections and poor boundary definitions. To overcome this limitation, medical image-specific FCN methods have been introduced with post-processing techniques to refine the segmentation results; however, the performance of these methods is reliant on the appropriate tuning of a large number of parameters and dependence on data-specific post-processing techniques. In this study, we leverage the state-of-the-art image feature learning method of generative adversarial network (GAN) for its inherent ability to produce consistent and realistic images features by using deep neural networks and adversarial learning concept. We improve upon GAN such that ROI features can be learned at different levels of complexities (simple and complex), in a controlled manner, via our proposed dual-path adversarial learning (DAL). The outputs from our DAL are then augmented to the learned ROI features into the existing FCN training data, which increases the overall feature diversity. We conducted experiments on three public datasets with a variety of visual characteristics. Our results demonstrate that our DAL can improve FCN-based segmentation methods and outperform or be competitive in performances to the state-of-the-art methods without using medical image-specific optimizations.
C1 [Bi, Lei; Feng, Dagan; Kim, Jinman] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
   [Feng, Dagan] Shanghai Jiao Tong Univ, Med X Res Inst, Shanghai, Peoples R China.
C3 University of Sydney; Shanghai Jiao Tong University
RP Kim, J (corresponding author), Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
EM jinman.kim@sydney.edu.au
RI Kim, Jin Man/HJO-8987-2023; Kim, Jin/AAS-5810-2021
OI Kim, Jin/0000-0002-7667-9588; kim, jinman/0000-0001-5960-1060; Feng,
   Dagan/0000-0002-3381-214X; , Lei Bi/0000-0001-9759-0200
FU Australia Research Council (ARC) grants
FX This work was supported in part by Australia Research Council (ARC)
   grants.
CR [Anonymous], 2014, Clinical decisionsupport systems, DOI [10.1007/978-1-4471-4474-8_22, DOI 10.1007/978-1-4471-4474-822]
   [Anonymous], EPFLREPORT82802
   [Anonymous], BR J RADIOL
   [Anonymous], IEEE T IND ELECT
   BenTaieb Aicha, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P460, DOI 10.1007/978-3-319-46723-8_53
   Bi L, 2017, COMPUT MED IMAG GRAP, V60, P3, DOI 10.1016/j.compmedimag.2016.11.008
   Bi L, 2017, IEEE T BIO-MED ENG, V64, P2065, DOI 10.1109/TBME.2017.2712771
   Bi L, 2017, VISUAL COMPUT, V33, P1061, DOI 10.1007/s00371-017-1379-4
   Candemir S, 2014, IEEE T MED IMAGING, V33, P577, DOI 10.1109/TMI.2013.2290491
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen H, 2017, MED IMAGE ANAL, V36, P135, DOI 10.1016/j.media.2016.11.004
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XJ, 2012, IEEE T IMAGE PROCESS, V21, P2035, DOI 10.1109/TIP.2012.2186306
   Dai W., 2017, arXiv:1703.08770
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doi K, 2007, COMPUT MED IMAG GRAP, V31, P198, DOI 10.1016/j.compmedimag.2007.02.002
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jung Y, 2013, VISUAL COMPUT, V29, P805, DOI 10.1007/s00371-013-0833-1
   Kumar A, 2017, IEEE J BIOMED HEALTH, V21, P31, DOI 10.1109/JBHI.2016.2635663
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Novikov A. A., 2017, Fully convolutional architectures for multi-class segmentation in chest radiographs, P1
   Qi Dou, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P149, DOI 10.1007/978-3-319-46723-8_18
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Shiraishi J, 2000, AM J ROENTGENOL, V174, P71, DOI 10.2214/ajr.174.1.1740071
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sirinukunwattana K, 2015, IEEE T MED IMAGING, V34, P2366, DOI 10.1109/TMI.2015.2433900
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   van Ginneken B, 2006, MED IMAGE ANAL, V10, P19, DOI 10.1016/j.media.2005.02.002
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Yang W, 2018, IEEE J BIOMED HEALTH, V22, P842, DOI 10.1109/JBHI.2017.2687939
   Yin BJ, 2015, MED IMAGE ANAL, V26, P232, DOI 10.1016/j.media.2015.09.002
   Yuan YD, 2017, IEEE T MED IMAGING, V36, P1876, DOI 10.1109/TMI.2017.2695227
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 40
TC 55
Z9 56
U1 1
U2 53
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1043
EP 1052
DI 10.1007/s00371-018-1519-5
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400026
DA 2024-07-18
ER

PT J
AU Xu, K
   Wang, X
   Yang, X
   He, SF
   Zhang, Q
   Yin, BC
   Wei, XP
   Lau, RWH
AF Xu, Ke
   Wang, Xin
   Yang, Xin
   He, Shengfeng
   Zhang, Qiang
   Yin, Baocai
   Wei, Xiaopeng
   Lau, Rynson W. H.
TI Efficient image super-resolution integration
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 35th Computer Graphics International conference (CGI)
CY JUN 11-14, 2018
CL INDONESIA
SP Comp Graph Soc, Nanyang Technol Univ
DE Image super-resolution; Image processing; Gaussian conditional random
   fields
AB The super-resolution (SR) problem is challenging due to the diversity of image types with little shared properties as well as the speed required by online applications, e.g., target identification. In this paper, we explore the merits and demerits of recent deep learning-based and conventional patch-based SR methods and show that they can be integrated in a complementary manner, while balancing the reconstruction quality and time cost. Motivated by this, we further propose an integration framework to take the results from FSRCNN and A+ methods as inputs and directly learn a pixel-wise mapping between the inputs and the reconstructed results using the Gaussian conditional random fields. The learned pixel-wise integration mapping is flexible to accommodate different upscaling factors. Experimental results show that the proposed framework can achieve superior SR performance compared with the state of the arts while being efficient.
C1 [Xu, Ke; Yang, Xin] Dalian Univ Technol, Dept Comp Sci, Dalian, Peoples R China.
   [Xu, Ke; Wang, Xin; Zhang, Qiang; Wei, Xiaopeng] Dalian Univ Technol, Dalian, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Comp Sci, Dalian, Peoples R China.
   [He, Shengfeng] South China Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Xu, Ke; Lau, Rynson W. H.] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University of Technology; South China University of Technology; City
   University of Hong Kong
RP Wang, X (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.; He, SF (corresponding author), South China Univ Technol, Guangzhou, Guangdong, Peoples R China.
EM kkangwing@mail.dlut.edu.cn; wangxinlp@gmail.com; xinyang@dlut.edu.cn;
   hesfe@scut.edu.cn; zhangq@dlut.edu.cn; ybc@dlut.edu.cn;
   weixp@dlut.edu.cn; rynson.lau@cityu.edu.hk
RI jiang, lei/IWE-1124-2023; wei, xiao/ISB-6027-2023; zhang,
   qiang/HZJ-9551-2023; He, Shengfeng/E-5682-2016; , kk/AAH-8764-2020;
   Zhang, Qiang/IWU-5000-2023; Jiang, Tao/IWM-7503-2023; Zhang,
   Qiang/GXF-3105-2022
OI He, Shengfeng/0000-0002-3802-4644; , Xin/0000-0002-8046-722X; XU,
   Ke/0000-0001-5855-3810; Zhang, Qiang/0000-0003-3776-9799
FU SRG grant from City University of Hong Kong [7004889]; NSFC grant from
   National Natural Science Foundation of China [91748104, 61632006,
   61425002, 61702194]
FX This study was funded by an SRG grant from City University of Hong Kong
   (Ref. 7004889), and by NSFC grant from National Natural Science
   Foundation of China (Ref. 91748104, 61632006, 61425002, 61702194).
CR [Anonymous], 2014, ACCV
   [Anonymous], 2005, ICCV
   [Anonymous], 2012, ECCV
   [Anonymous], 2013, ICCV
   [Anonymous], 2004, CVPR
   [Anonymous], 2010, INT C CURV SURF
   [Anonymous], 2017, CVPR
   [Anonymous], ICCV
   [Anonymous], 2015, NIPS
   [Anonymous], 2016, ECCV
   [Anonymous], 2014, P EUR C COMP VIS ZUR
   [Anonymous], 2016, ECCV
   [Anonymous], 2013, ICCV
   [Anonymous], 2001, ICCV
   [Anonymous], 2015, ICCV
   [Anonymous], 2015, CVPR
   [Anonymous], 2016, CVPR
   [Anonymous], 2015, CVPR
   [Anonymous], 2013, ICCV
   [Anonymous], 2007, CVPR
   Chang H., 2004, Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, V1, P1063
   Dai D., 2015, EG
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Farsiu S, 2004, INT J IMAG SYST TECH, V14, P47, DOI 10.1002/ima.20007
   Fattal R., 2007, ACM TOG
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman W.T., 2011, ADV MARKOV RANDOM FI, V1, P3
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Jancsary J., 2012, CVPR
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim K., 2004, ECCV WORKSH STAT LEA
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Köhler T, 2016, IEEE T COMPUT IMAG, V2, P42, DOI 10.1109/TCI.2016.2516909
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Schmidt U., 2013, CVPR
   Simonyan K., 2014, 14091556 ARXIV
   Tappen MarshallF., 2007, CVPR
   Thévenaz P, 2000, BIOMED EN S, P393
   Xie Z., 2014, CGF
   Xu K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980224
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 44
TC 11
Z9 11
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2018
VL 34
IS 6-8
SI SI
BP 1065
EP 1076
DI 10.1007/s00371-018-1554-2
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GH6MC
UT WOS:000433557400028
DA 2024-07-18
ER

PT J
AU Maiseli, BJ
   Bai, LF
   Yang, XQ
   Gu, YF
   Gao, HJ
AF Maiseli, Baraka Jacob
   Bai, LiFei
   Yang, Xianqiang
   Gu, Yanfeng
   Gao, Huijun
TI Robust cost function for optimizing chamfer masks
SO VISUAL COMPUTER
LA English
DT Article
DE Chamfering; Euclidean; Mean absolute error; Optimization
ID EUCLIDEAN DISTANCE TRANSFORMATION; ALGORITHMS; ERROR
AB Chamfering, a mask-driven technique, refers to a process of propagating local distances over an image to estimate a reference metric. Performance of the technique depends on the design of chamfer masks using cost functions. To date, most scholars have been using a mean absolute error and a mean squared error to formulate optimization problems for estimating weights in the chamfer masks. However, studies have shown that these optimization functions endure some potential weaknesses, including biasedness and sensitivity to outliers. Motivated by the weaknesses, the present work proposes an alternative difference function, RLog, that is unbiased, symmetrical, and robust. RLog takes the absolute logarithm of the relative accuracy of the estimated distance to compute optimal chamfer weights. Also, we have proposed an algorithm to map entries of the designed real-valued chamfer masks into integers. Analytical and experimental results demonstrate that chamfering based on our weights generate polygons and distance maps with lower errors. Methods and results of our work may be useful in robotics to address the matching problem.
C1 [Maiseli, Baraka Jacob; Gu, Yanfeng] Harbin Inst Technol, Sch Elect & Informat Engn, Harbin 150001, Heilongjiang, Peoples R China.
   [Maiseli, Baraka Jacob] Univ Dar Es Salaam, Dept Elect & Telecommun Engn, Coll Informat & Commun Technol, Dar Es Salaam 00255, Tanzania.
   [Bai, LiFei; Yang, Xianqiang; Gao, Huijun] Harbin Inst Technol, Res Inst Intelligent Control & Syst, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology; University of Dar es Salaam; Harbin
   Institute of Technology
RP Maiseli, BJ (corresponding author), Harbin Inst Technol, Sch Elect & Informat Engn, Harbin 150001, Heilongjiang, Peoples R China.; Maiseli, BJ (corresponding author), Univ Dar Es Salaam, Dept Elect & Telecommun Engn, Coll Informat & Commun Technol, Dar Es Salaam 00255, Tanzania.
EM barakamaiseli@yahoo.com; lifeibai@hit.edu.cn; xianqiangyang@hit.edu.cn;
   guyf@hit.edu.cn; huijungao@gmail.com
RI Maiseli, Baraka/ABD-6700-2020; zhang, ye/HKN-5128-2023; Gao,
   Huijun/B-6853-2013; Maiseli, Baraka Jacob/M-2408-2016; Gu,
   Yanfeng/F-7781-2015
OI Maiseli, Baraka Jacob/0000-0002-7551-0107; 
FU China Postdoctoral Science Foundation
FX Funding was provided by China Postdoctoral Science Foundation.
CR Bailey DG, 2004, LECT NOTES COMPUT SC, V3322, P394
   BREU H, 1995, IEEE T PATTERN ANAL, V17, P529, DOI 10.1109/34.391389
   Butt MA, 1998, IEEE T IMAGE PROCESS, V7, P1477, DOI 10.1109/83.718487
   Elizondo-Leal JC, 2013, INT J ADV ROBOT SYST, V10, DOI 10.5772/56581
   Cuisenaire O, 1999, COMPUT VIS IMAGE UND, V76, P163, DOI 10.1006/cviu.1999.0783
   de Myttenaere A, 2016, NEUROCOMPUTING, V192, P38, DOI 10.1016/j.neucom.2015.12.114
   Nguyen DT, 2014, PROC CVPR IEEE, P2425, DOI 10.1109/CVPR.2014.311
   Foss T, 2003, IEEE T SOFTWARE ENG, V29, P985, DOI 10.1109/TSE.2003.1245300
   Franses PH, 2016, INT J FORECASTING, V32, P20, DOI 10.1016/j.ijforecast.2015.03.008
   Grevera GJ, 2007, TOP BIOMED ENG, P33, DOI 10.1007/978-0-387-68413-0_2
   Jian Dong, 2013, Intelligence Science and Big Data Engineering. 4th International Conference, IScIDE 2013. Revised Selected Papers: LNCS 8261, P875, DOI 10.1007/978-3-642-42057-3_110
   Kaliamoorthi P, 2013, IEEE SIGNAL PROC LET, V20, P1151, DOI 10.1109/LSP.2013.2283254
   Linnér E, 2014, LECT NOTES COMPUT SC, V8668, P88, DOI 10.1007/978-3-319-09955-2_8
   Liu WP, 2013, IEEE T PARALL DISTR, V24, P1763, DOI 10.1109/TPDS.2012.300
   Ma TY, 2010, LECT NOTES COMPUT SC, V6315, P450
   Majidpour M, 2016, APPL ENERG, V163, P134, DOI 10.1016/j.apenergy.2015.10.184
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   Mishchenko Y, 2015, SIGNAL IMAGE VIDEO P, V9, P19, DOI 10.1007/s11760-012-0419-9
   Montanvert A., 2007, ARXIV07053343
   Muñoz A, 2007, VISUAL COMPUT, V23, P493, DOI 10.1007/s00371-007-0122-y
   PAGLIERONI DW, 1992, CVGIP-GRAPH MODEL IM, V54, P56, DOI 10.1016/1049-9652(92)90034-U
   Saha PK, 2016, PATTERN RECOGN LETT, V76, P3, DOI 10.1016/j.patrec.2015.04.006
   SAITO T, 1994, PATTERN RECOGN, V27, P1551, DOI 10.1016/0031-3203(94)90133-3
   Salvi D, 2015, IEEE WINT CONF APPL, P757, DOI 10.1109/WACV.2015.106
   Shih FY, 2004, COMPUT VIS IMAGE UND, V93, P195, DOI 10.1016/j.cviu.2003.09.004
   Shouhong Ding, 2017, Visual Computer, V33, P355, DOI 10.1007/s00371-015-1205-9
   Song CF, 2018, VISUAL COMPUT, V34, P243, DOI 10.1007/s00371-016-1331-z
   THIEL E, 1992, VISUAL FORM, P537
   Tofallis C, 2015, J OPER RES SOC, V66, P1352, DOI 10.1057/jors.2014.103
   Tzionas D, 2013, LECT NOTES COMPUT SC, V8142, P131, DOI 10.1007/978-3-642-40602-7_14
   VERWER BJH, 1991, PATTERN RECOGN LETT, V12, P671, DOI 10.1016/0167-8655(91)90004-6
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Xu Dong, 2013, Res Comput Mol Biol, V7821, P304, DOI 10.1007/978-3-642-37195-0_30
   Zhang HL, 2016, VISUAL COMPUT, V32, P31, DOI 10.1007/s00371-014-1053-z
NR 35
TC 1
Z9 1
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 617
EP 632
DI 10.1007/s00371-017-1367-8
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100003
DA 2024-07-18
ER

PT J
AU Wolberg, G
   Zokai, S
AF Wolberg, George
   Zokai, Siavash
TI PhotoSketch: a photocentric urban 3D modeling system
SO VISUAL COMPUTER
LA English
DT Article
DE Image-based modeling; Phototextured 3D models; Structure and motion;
   Multiview geometry; 3D photography; Camera calibration
ID SCENES
AB Online mapping services from Google, Apple, and Microsoft are exceedingly popular applications for exploring 3D urban cities. Their explosive growth provides impetus for photorealistic 3D modeling of urban scenes. Although classical algorithms such as multiview stereo and laser range scanners are traditional sources for detailed 3D models of existing structures, they generate heavyweight models that are not appropriate for the streaming data that these navigation applications leverage. Instead, lightweight models as produced by interactive image-based tools are better suited for this domain. The contribution of this work is that it merges the benefits of multiview geometry, an intuitive sketching interface, and dynamic texture mapping to produce lightweight photorealistic 3D models of buildings. We present experimental results from urban scenes using our PhotoSketch system.
C1 [Wolberg, George] CUNY, City Coll New York, New York, NY 10031 USA.
   [Zokai, Siavash] Brainstorm Technol LLC, New York, NY 10001 USA.
C3 City University of New York (CUNY) System; City College of New York
   (CUNY)
RP Wolberg, G (corresponding author), CUNY, City Coll New York, New York, NY 10031 USA.
EM wolberg@cs.ccny.cuny.edu; zokai@brainstormllc.com
FU US Department of Energy [DE-NA0002492]
FX This work was supported by a grant from the US Department of Energy
   (DE-NA0002492).
CR [Anonymous], C OPT 3D MEAS TECHN
   [Anonymous], SAMANTHA STRUCTURE M
   [Anonymous], 2004, An invitation to 3-D vision
   [Anonymous], COMPUTER GRAPHICS SI
   [Anonymous], VISUALSFM
   [Anonymous], P CIPA 2001 INT S
   [Anonymous], OTHERS OPENMVG OPEN
   [Anonymous], P IEEE CVPR
   [Anonymous], AUTOMATIC EXTRACTION
   [Anonymous], SIGGRAPH ASIA 08
   [Anonymous], CLOSE RANGE PHOTOGRA
   Arikan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421642
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Faugeras O., 2001, The geometry of multiple images: the laws that govern the formation of multiple images of a scene and some of their applications
   Furukawa Y., 2007, IEEE C COMPUTER VISI, P1
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hou F, 2016, VISUAL COMPUT, V32, P151, DOI 10.1007/s00371-015-1061-7
   Li ML, 2016, INT J DIGIT EARTH, V9, P806, DOI 10.1080/17538947.2016.1143982
   Liu L., 2006, IEEE C COMPUTER VISI, V2, P2293, DOI [10.1109/CVPR.2006.204, DOI 10.1109/CVPR.2006.204]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luhmann Thomas., 2006, Close range photogrammetry: principles, techniques and applications
   Mathias M., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P304, DOI 10.1109/3DIMPVT.2011.45
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Müller P, 2006, ACM T GRAPHIC, V25, P614, DOI 10.1145/1141911.1141931
   Musialski P, 2013, COMPUT GRAPH FORUM, V32, P146, DOI 10.1111/cgf.12077
   Nan LL, 2015, COMPUT GRAPH FORUM, V34, P217, DOI 10.1111/cgf.12554
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Samavati F, 2016, VISUAL COMPUT, V32, P1293, DOI 10.1007/s00371-016-1227-y
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stamos I, 2002, COMPUT VIS IMAGE UND, V88, P94, DOI 10.1006/cviu.2002.0963
   Stamos I, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P731, DOI 10.1109/ICCV.2001.937699
   Stamos I, 2008, INT J COMPUT VISION, V78, P237, DOI 10.1007/s11263-007-0089-1
   van den Hengel A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239537, 10.1145/1276377.1276485]
   Vanegas CA, 2010, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.2010.5540190
   Verdie Y, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732527
   Weihong Li, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P124, DOI 10.1109/3DIMPVT.2011.23
   Wu FZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601162
   Zhang JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2167076.2167079
NR 40
TC 8
Z9 8
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2018
VL 34
IS 5
BP 605
EP 616
DI 10.1007/s00371-017-1365-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3AW
UT WOS:000429657100002
DA 2024-07-18
ER

PT J
AU Cheng, Y
   Jiao, LB
   Cao, XH
   Li, ZY
AF Cheng, Yong
   Jiao, Liangbao
   Cao, Xuehong
   Li, Zuoyong
TI Illumination-insensitive features for face recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Illumination variation; Local reflectance normalization; Face
   recognition
ID NONSUBSAMPLED CONTOURLET TRANSFORM; VARYING ILLUMINATION; HISTOGRAM
   EQUALIZATION; INVARIANT; REPRESENTATION; NORMALIZATION; ENHANCEMENT;
   EXTRACTION; MODELS
AB Illumination variation is one of the most challenging problems for robust face recognition. In this paper, after investigating the ratio relationship between two neighboring pixels in a digital image, we proposed two illumination-insensitive features, i.e., the non-directional local reflectance normalization (NDLRN) and the fused multi-directional local reflectance normalization (fMDLRN), which not only effectively reduce illumination difference among facial images under different illumination conditions, but also preserve the facial details. Experimental results show that NDLRN and fMDLRN can significantly alleviate the adverse effect of complex illumination on face recognition.
C1 [Cheng, Yong] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Cheng, Yong; Jiao, Liangbao; Cao, Xuehong] Nanjing Inst Technol, Sch Commun Engn, Nanjing 211167, Jiangsu, Peoples R China.
   [Cheng, Yong; Li, Zuoyong] Minjiang Univ, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou 350121, Fujian, Peoples R China.
C3 Southeast University - China; Nanjing Institute of Technology; Minjiang
   University
RP Li, ZY (corresponding author), Minjiang Univ, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou 350121, Fujian, Peoples R China.
EM chengyong@njit.edu.cn; fzulzytdq@126.com
FU Natural Science Foundation of Jiangsu Province [BK20131342]; National
   Natural Science Foundation of China (NSFC) [61305011]; Fuzhou Science
   and Technology Planning Project [2016-S-116, 2015-PT-91]; Technology
   Project of Provincial University of Fujian Province [JK2014040]; Program
   for New Century Excellent Talents in Fujian Province University
   (NCETFJ); Program for Young Scholars in Minjiang University
   [Mjqn201601]; Key Project of College Youth Natural Science Foundation of
   Fujian Province [JZ160467]; Open Fund Project of Fujian Provincial Key
   Laboratory of Information Processing and Intelligent Control (Minjiang
   University) [MJUKF201712]
FX Natural Science Foundation of Jiangsu Province (BK20131342); National
   Natural Science Foundation of China (NSFC) (61305011); Fuzhou Science
   and Technology Planning Project (2016-S-116 and 2015-PT-91); Technology
   Project of Provincial University of Fujian Province (JK2014040); Program
   for New Century Excellent Talents in Fujian Province University
   (NCETFJ); Program for Young Scholars in Minjiang University
   (Mjqn201601); Key Project of College Youth Natural Science Foundation of
   Fujian Province (JZ160467); the Open Fund Project of Fujian Provincial
   Key Laboratory of Information Processing and Intelligent Control
   (Minjiang University) (No. MJUKF201712).
CR Baradarani A, 2013, PATTERN RECOGN, V46, P57, DOI 10.1016/j.patcog.2012.06.007
   Cao X, 2012, PATTERN RECOGN, V45, P1299, DOI 10.1016/j.patcog.2011.09.010
   Chen T, 2006, IEEE T PATTERN ANAL, V28, P1519, DOI 10.1109/TPAMI.2006.195
   Cheng Y, 2010, NEUROCOMPUTING, V73, P2217, DOI 10.1016/j.neucom.2010.01.012
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gross R, 2003, LECT NOTES COMPUT SC, V2688, P10
   Hu HF, 2015, IET COMPUT VIS, V9, P163, DOI 10.1049/iet-cvi.2013.0342
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Lee PH, 2012, IEEE T IMAGE PROCESS, V21, P4280, DOI 10.1109/TIP.2012.2202670
   Li Q, 2010, VISUAL COMPUT, V26, P41, DOI 10.1007/s00371-009-0375-8
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Luo Y, 2015, APPL OPTICS, V54, P6887, DOI 10.1364/AO.54.006887
   Nikan S, 2015, IET IMAGE PROCESS, V9, P12, DOI 10.1049/iet-ipr.2013.0792
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Savvides M, 2003, LECT NOTES COMPUT SC, V2688, P549
   Shan SG, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P157
   Shao M, 2010, INT CONF ACOUST SPEE, P1114, DOI 10.1109/ICASSP.2010.5495355
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Vageeswaran P, 2013, IEEE T IMAGE PROCESS, V22, P1362, DOI 10.1109/TIP.2012.2228498
   Vitomir S., INFACE TOOLBOX V2 1
   Wang B, 2011, IEEE SIGNAL PROC LET, V18, P462, DOI 10.1109/LSP.2011.2158998
   Wang HT, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P819
   Wu Y, 2014, NEUROCOMPUTING, V136, P262, DOI 10.1016/j.neucom.2014.01.006
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Xie XD, 2005, PATTERN RECOGN, V38, P221, DOI 10.1016/j.patcog.2004.07.002
   Xie XH, 2011, IEEE T IMAGE PROCESS, V20, P1807, DOI 10.1109/TIP.2010.2097270
   Xie XH, 2010, PATTERN RECOGN, V43, P4177, DOI 10.1016/j.patcog.2010.06.019
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang TP, 2009, PATTERN RECOGN, V42, P251, DOI 10.1016/j.patcog.2008.03.017
   Zhang TP, 2009, IEEE T IMAGE PROCESS, V18, P2599, DOI 10.1109/TIP.2009.2028255
   Zhou Y, 2013, OPT EXPRESS, V21, P11294, DOI 10.1364/OE.21.011294
NR 35
TC 15
Z9 15
U1 0
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2017
VL 33
IS 11
BP 1483
EP 1493
DI 10.1007/s00371-017-1357-x
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FJ0TP
UT WOS:000412423100010
DA 2024-07-18
ER

PT J
AU Wang, WM
   Li, BJ
   Qian, SC
   Liu, YJ
   Wang, CCL
   Liu, LG
   Yin, BC
   Liu, XP
AF Wang, Weiming
   Li, Baojun
   Qian, Sicheng
   Liu, Yong-Jin
   Wang, Charlie C. L.
   Liu, Ligang
   Yin, Baocai
   Liu, Xiuping
TI Cross section-based hollowing and structural enhancement
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 34th Conference on Computer Graphics International (CGI)
CY JUN 27-30, 2017
CL Yokohama, JAPAN
SP Keio Univ, Fac Sci & Technol
DE 3D printing; Cross section; Adaptive hollowing; Structural enhancement
ID UPRIGHT ORIENTATION; 3D; OPTIMIZATION
AB Recently, 3D printing has become a powerful tool for personal fabrication. However, the price of some materials is still high which limits its applications in home users. To optimize the volume of the model, while not largely affecting the strength of the objects, researchers propose algorithms to divide the model with different kinds of lightweight structures, such as frame structure, honeycomb cell structure, truss structure, medial axis tree. However, these algorithms are not suitable for the model whose internal space needs to be reused. In addition, the structural strength and static stability of the models, obtained with modern 3D model acquirement methods, are not guaranteed. In consequence, some models are too fragile to print and cannot be survived in daily usage, handling, and transportation or cannot stand in a stable. To handle the mentioned problems, an algorithm system is proposed based on cross sections in this work. The structural weak cross sections are enhanced, and structural strong cross sections are adaptively hollowed to meet a given structural strength, static stability, printability, etc., while the material usage is minimized. The proposed algorithm system has been tested on several typical 3D models. The experimental results demonstrate the effectiveness and practicability of our system.
C1 [Wang, Weiming; Li, Baojun; Yin, Baocai; Liu, Xiuping] Dalian Univ Technol, Dalian, Peoples R China.
   [Qian, Sicheng] Dalian Univ Technol, Math Sci, Dalian, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Beijing, Peoples R China.
   [Wang, Charlie C. L.] Delft Univ Technol, Delft, Netherlands.
   [Liu, Ligang] Univ Sci & Technol China, Hefei, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology;
   Tsinghua University; Delft University of Technology; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Liu, XP (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.
EM wwmdlut@dlut.edu.cn; bjli@dlut.edu.cn; liuyongjin@tsinghua.edu.cn;
   c.c.wang@tudelft.nl; lgliu@ustc.edu.cn; ybc@dlut.edu.cn;
   xpliu@dlut.edu.cn
RI Li, Baojun/H-6254-2018; Wang, Weiming/H-4944-2017; Liu,
   Xiu/IYJ-9134-2023; Liu, Yong/GWQ-6163-2022; Liu, Xiufang/I-8003-2015;
   Wang, Charlie C. L./B-3730-2010
OI Li, Baojun/0000-0001-8325-3772; Wang, Weiming/0000-0001-6289-0094; Wang,
   Charlie C. L./0000-0003-4406-8480
FU China Postdoctoral Science Foundation [2016M601308]; One Hundred Talent
   Project of the Chinese Academy of Sciences; Fundamental Research Fund
   [DUT16RC(3)061]; National Natural Science Foundation of China [61370143,
   61432003, 61661130156, 61672482, 11626253, 11472073]; Hong Kong RGC GRF
   [14207414]; Royal Society-Newton Advanced Fellowship [NA150431]
FX We would like to thank the reviewers for their detailed comments and
   suggestions which greatly improved the manuscript. The research leading
   to these results has received funding from China Postdoctoral Science
   Foundation (2016M601308), the One Hundred Talent Project of the Chinese
   Academy of Sciences, Fundamental Research Fund (DUT16RC(3)061), National
   Natural Science Foundation of China (61370143, 61432003, 61661130156,
   61672482, 11626253, 11472073), Hong Kong RGC GRF (14207414), and Royal
   Society-Newton Advanced Fellowship (NA150431).
CR [Anonymous], COMPUTER GRAPHICS FO
   Bächer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601157
   Bächer M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185543
   Cacciola F., 2013, CGAL USER REFERENCE
   Ceylan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508400
   Chen Y, 2011, COMPUT AIDED DESIGN, V43, P31, DOI 10.1016/j.cad.2010.09.002
   Dumas J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601153
   Fu HB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360641
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Hildebrand K, 2013, COMPUT GRAPH-UK, V37, P669, DOI 10.1016/j.cag.2013.05.011
   Hoffmann C., 1989, Geometric and Solid Modeling: An Introduction
   Liu SJ, 2011, IEEE T AUTOM SCI ENG, V8, P347, DOI 10.1109/TASE.2010.2066563
   Lu L, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601168
   Sá AME, 2015, VISUAL COMPUT, V31, P799, DOI 10.1007/s00371-015-1109-8
   MEYERS D, 1992, ACM T GRAPHIC, V11, P228, DOI 10.1145/130881.131213
   Musialski P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925886
   Musialski P, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766955
   Nocedal J, 2006, SPRINGER SER OPER RE, P135
   Pintus R, 2010, VISUAL COMPUT, V26, P831, DOI 10.1007/s00371-010-0488-0
   Porumbescu SD, 2005, ACM T GRAPHIC, V24, P626, DOI 10.1145/1073204.1073239
   Préost R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461957
   Schmidt R., 2014, ACM SIGGRAPH 2014 ST, P9
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Stava O, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185544
   Su-Jin Kim, 2004, Computer-Aided Design and Applications, V1, P285
   Sun T, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766961
   Umetani N., 2013, SIGGRAPH Asia Technical Briefs, P5, DOI DOI 10.1145/2542355.2542361
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P322, DOI 10.1111/cgf.12353
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P117, DOI 10.1111/cgf.12437
   Wang CCL, 2013, COMPUT AIDED DESIGN, V45, P321, DOI 10.1016/j.cad.2012.10.015
   Wang L., 2016, Computer Graphics Forum, V35, P49, DOI 10.1111/cgf.12810
   Wang WM, 2016, COMPUT GRAPH FORUM, V35, P59, DOI 10.1111/cgf.12811
   Wang WM, 2015, COMPUT GRAPH FORUM, V34, P148, DOI 10.1111/cgf.12527
   Wang WM, 2014, J MECH SCI TECHNOL, V28, P2469, DOI 10.1007/s12206-014-0604-6
   Wang WM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508382
   Xie Y, 2015, COMPUT AIDED GEOM D, V35-36, P163, DOI 10.1016/j.cagd.2015.03.019
   Xu W., 2016, J COMPUT SC IN PRESS, V31
   YAMANAKA D., 2014, SIGGRAPH ASIA 2014 T, P7
   Zhang XL, 2015, COMPUT AIDED GEOM D, V35-36, P149, DOI 10.1016/j.cagd.2015.03.012
   Zhang XT, 2016, COMPUT GRAPH FORUM, V35, P157, DOI 10.1111/cgf.12972
   Zhou QY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461919
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 42
TC 6
Z9 9
U1 1
U2 50
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2017
VL 33
IS 6-8
BP 949
EP 960
DI 10.1007/s00371-017-1386-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EX1EY
UT WOS:000402964800025
DA 2024-07-18
ER

PT J
AU Jang, JS
   Choi, SH
   Jung, GS
   Jung, SK
AF Jang, Jae Seok
   Choi, Soo Ho
   Jung, Gi Sook
   Jung, Soon Ki
TI Focused augmented mirror based on human visual perception
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented mirror; Augmented reality; Depth-of-field; Camera calibration
ID HUMAN EYE
AB A mirror is used in various aspects of daily life; thus, most people can use a mirror-metaphor augmented reality (AR) system naturally, without wearing a head-mounted display or AR glasses. An augmented mirror, one of the mirror metaphor AR display, is considered a more appropriate system than video-based virtual mirror display for personalized and immersive interaction in AR because it can show a combined scene in which a rendered virtual world is superimposed on the reflected real world based the viewpoint of a user. In this paper, we propose a focused augmented mirror that implements both viewpoint and depth-of-field matchings. In particular, we design a focused augmented mirror by concentrating on how the depth-of-field influences human visual perception in the augmented mirror system. To compare the differences between the theoretical and practical results, we perform two types of experiments; calculating the focus measure of the combined image photographed at user's viewpoint and evaluating user experience with randomly selected non-expert people. In addition, we suggest an efficient user workspace that is limited by the tracker's geometric configuration and the target object's depth of field. Finally, we describe the physical limitation of our system and propose its solution as a future work.
C1 [Jang, Jae Seok; Choi, Soo Ho; Jung, Gi Sook; Jung, Soon Ki] Kyungpook Natl Univ, Sch Comp Sci & Engn, 80 Daehak Ro, Daegu, South Korea.
C3 Kyungpook National University
RP Jung, SK (corresponding author), Kyungpook Natl Univ, Sch Comp Sci & Engn, 80 Daehak Ro, Daegu, South Korea.
EM skjung@knu.ac.kr
RI Jung, Soon Ki/P-7687-2018
OI Jung, Soon Ki/0000-0003-0239-6785
FU Ministry of Culture, Sports and Tourism (MCST); Korea Creative Content
   Agency (KOCCA) in the Culture Technology (CT) Research & Development
   Program (Immersive Game Contents CT Co-Research Center)
FX This research is supported by Ministry of Culture, Sports and Tourism
   (MCST) and Korea Creative Content Agency (KOCCA) in the Culture
   Technology (CT) Research & Development Program (Immersive Game Contents
   CT Co-Research Center).
CR [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Bimber O, 2001, IEEE COMPUT GRAPH, V21, P48, DOI 10.1109/38.963460
   BRENNER JF, 1976, J HISTOCHEM CYTOCHEM, V24, P100, DOI 10.1177/24.1.1254907
   Campbell F. W., 1957, OPT ACTA, V4, P157, DOI [DOI 10.1080/713826091, 10.1080/713826091]
   Combes B, 2008, P IEEE C COMP VIS PA, P1, DOI 10.1109/CVPR.2008.4587605
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Eggert DW, 1997, MACH VISION APPL, V9, P272, DOI 10.1007/s001380050048
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   Jang J.S., 2015, P COMP GRAPH INT 201
   Jang JS, 2014, P IEEE, V102, P196, DOI 10.1109/JPROC.2013.2294253
   Krishnamurthy V., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P313, DOI 10.1145/237170.237270
   Lueder E, 2012, WILEY-SID SER DISPL, P1
   Marcos S, 1999, VISION RES, V39, P2039, DOI 10.1016/S0042-6989(98)00317-4
   Melzer J.E., 1997, HEAD MOUNTED DISPLAY
   Mir H., 2014, P SPIE DIGITAL PHOTO, V9023
   NAYAR SK, 1994, IEEE T PATTERN ANAL, V16, P824, DOI 10.1109/34.308479
   Ng KC, 2001, IEEE INT CONF ROBOT, P2791, DOI 10.1109/ROBOT.2001.933045
   Pachoulakis I., 2012, INT J MULTIMEDIA ITS, V4, P35
   Reichelt S., 2010, 3 DIMENSIONAL IMAGIN
   Rolland J.P., 1994, P SPIE TELEMANIPULAT, V2351, P1321
   Russ J.C., 2002, IMAGE PROCESSING HDB
   Saakes D., 2015, 42 INT C EXH COMP GR
   Tenebaum J.M., 1970, THESIS
   Ukita N., 2014, INT C PHYS COMP SYST
   Wang B, 2006, SURV OPHTHALMOL, V51, P75, DOI 10.1016/j.survophthal.2005.11.003
   Wang L., 2012, P IEEE C COMP VIS PA
   Witten I.H., 1999, Managing Gigabytes: Compressing and Indexing Documents and Images
   Yousefi S, 2011, IEEE T CONSUM ELECTR, V57, P1003, DOI 10.1109/TCE.2011.6018848
NR 29
TC 3
Z9 4
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2017
VL 33
IS 5
BP 625
EP 636
DI 10.1007/s00371-016-1212-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER4KB
UT WOS:000398767300007
DA 2024-07-18
ER

PT J
AU Ding, SH
   Sheng, B
   Xie, ZF
   Ma, LZ
AF Ding, Shouhong
   Sheng, Bin
   Xie, Zhifeng
   Ma, Lizhuang
TI Intrinsic image estimation using near- sparse optimization
SO VISUAL COMPUTER
LA English
DT Article
DE Intrinsic image decomposition; Reflectance; Shading; L-0 sparsity
ID REPRESENTATION; DECOMPOSITION; LIGHTNESS; RETINEX
AB The objective of intrinsic images estimation is to decompose an input image into its intrinsic shading and reflectance components. This is a well-known under-constrained problem that has long been an open challenge. This paper proposes a novel approach for automatic intrinsic images decomposition that uses a new reflectance sparsity prior. On the basis of the observation that the reflectance of natural objects is commonly piecewise constant, we formalize this constraint on the entire reflectance image using the sparse loss function that enforces the variation in reflectance images to be of high-frequency and sparse. This new sparsity constraint significantly improves the quality of Retinex intrinsic images estimation. It also functions effectively by combining a class of global sparsity priors on reflectance. Experimental results on MIT benchmark dataset as well as various real-world images and synthetic images demonstrate the effectiveness and versatility of our approach.
C1 [Ding, Shouhong; Sheng, Bin; Ma, Lizhuang] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn CSE, Shanghai, Peoples R China.
   [Xie, Zhifeng] Shanghai Univ, Dept Film & Televis Engn, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai University
RP Ma, LZ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn CSE, Shanghai, Peoples R China.
EM dingsh1987@gmail.com; shengbin@cs.sjtu.edu.cn; ma-lz@cs.sjtu.edu.cn
RI Sun, Peng/KDO-4243-2024
FU National Natural Science Foundation of China [61133009, 61472245,
   61572316, 61303093]; National High-tech R&D Program of China (863
   Program) [2015AA011604]
FX The work is supported by the National Natural Science Foundation of
   China (Nos. 61133009, 61472245, 61572316 and 61303093), National
   High-tech R&D Program of China (863 Program) (Grant No. 2015AA011604).
   The authors would also like to thank the reviewers for their helpful
   suggestions.
CR [Anonymous], 1978, Recovering intrinsic scene characteristics.
   [Anonymous], TECH REP
   Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Barron JT, 2012, LECT NOTES COMPUT SC, V7575, P57, DOI 10.1007/978-3-642-33765-9_5
   Beigpour S, 2013, IEEE IMAGE PROC, P285, DOI 10.1109/ICIP.2013.6738059
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   Berthold KP, 1974, Comput. Graph. Image Process., V3, P277, DOI [DOI 10.1016/0146-664X(74)90022-7, 10.1016/0146-664X(74)90022-7]
   BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476
   Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Finlayson G.D., 2002, ISTSID 10 COLOR IMAG, P73
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   FUNT BV, 1992, LECT NOTES COMPUT SC, V588, P124
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Gehler P., 2011, P ADV NEUR INF PROC, P765
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   Koltun Vladlen, 2013, INT C MACH LEARN
   Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112
   Laffont PY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366221
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li CM, 2009, IEEE I CONF COMP VIS, P702, DOI 10.1109/ICCV.2009.5459239
   Serra M, 2012, PROC CVPR IEEE, P278, DOI 10.1109/CVPR.2012.6247686
   SHEN L., 2008, CVPR
   Shen L, 2013, IEEE T PATTERN ANAL, V35, P2904, DOI 10.1109/TPAMI.2013.136
   Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738
   Tappen M., 2006, IEEE COMPUTER VISION, P1992
   Tappen M. F., 2004, PATTERN ANAL MACH IN, V27
   Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606
   Werman M., 2004, COMP VIS PATT REC CV
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77
NR 35
TC 4
Z9 5
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2017
VL 33
IS 3
BP 355
EP 369
DI 10.1007/s00371-015-1205-9
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EL1KM
UT WOS:000394379300009
DA 2024-07-18
ER

PT J
AU Quesada, C
   González, D
   Alfaro, I
   Cueto, E
   Huerta, A
   Chinesta, F
AF Quesada, C.
   Gonzalez, D.
   Alfaro, I.
   Cueto, E.
   Huerta, A.
   Chinesta, F.
TI Real-time simulation techniques for augmented learning in science and
   engineering
SO VISUAL COMPUTER
LA English
DT Article
DE Augmented learning; Real-time simulation; Model order reduction; Proper
   generalized decomposition
ID PROPER GENERALIZED DECOMPOSITION; MODEL ORDER REDUCTION; INTERPOLATION
   METHOD; SOFT-TISSUES; SURGERY; DEFORMATIONS; SOLVERS; FAMILY
AB In this paper, we present the basics of a novel methodology for the development of simulation-based and augmented learning tools in the context of applied science and engineering. It is based on the extensive use of model order reduction, and particularly, of the so-called proper generalized decomposition method. This method provides a sort of meta-modeling tool without the need for prior computer experiments that allows the user to obtain real-time response in the solution of complex engineering or physical problems. This real-time capability also allows for its implementation in deployed, touch screen, handheld devices or even to be immersed into electronic textbooks. We explore here the basics of the proposed methodology and give examples on a few challenging applications never until now explored, up to our knowledge.
C1 [Quesada, C.; Gonzalez, D.; Alfaro, I.; Cueto, E.] Univ Zaragoza, Aragon Inst Engn Res I3A, Maria de Luna 3, Zaragoza 50018, Spain.
   [Huerta, A.] Univ Politecn Cataluna, Lab Calcul Numer LaCaN, Dept Matemat Aplicada 3, Jordi Girona 1-3, ES-08034 Barcelona, Spain.
   [Huerta, A.] Swansea Univ, Coll Engn, Zienkiewicz Ctr Computat Engn, Swansea SA2 8PP, W Glam, Wales.
   [Chinesta, F.] Inst Univ France, UMR CNRS, GEM, Cent Nantes, 1 Rue Noe,BP 92101, F-44321 Nantes 3, France.
C3 University of Zaragoza; Universitat Politecnica de Catalunya; Swansea
   University; Institut Universitaire de France; Nantes Universite; Ecole
   Centrale de Nantes; Centre National de la Recherche Scientifique (CNRS)
RP Cueto, E (corresponding author), Univ Zaragoza, Aragon Inst Engn Res I3A, Maria de Luna 3, Zaragoza 50018, Spain.
EM ecueto@unizar.es
RI Gonzalez, David/IPU-3884-2023; Alfaro, Iciar/K-7500-2015; GONZALEZ,
   DAVID/JDD-2892-2023; González, David/G-1536-2011; Chinesta,
   Francisco/ABD-1528-2021; Quesada, Carlos/AAB-9888-2021; Cueto,
   Elias/A-2452-2010; Huerta, Antonio/B-7563-2008
OI González, David/0000-0003-3003-5856; Quesada,
   Carlos/0000-0003-3294-8093; Cueto, Elias/0000-0003-1017-4381; Huerta,
   Antonio/0000-0003-4198-3798
FU IUF (Institut Universitaire de France); Spanish Ministry of Economy and
   Competitiveness [DPI2011-27778-C02-01/02, DPI2014-51844-C2-1/2-R]
FX This work has been partially supported by the IUF (Institut
   Universitaire de France) and by the Spanish Ministry of Economy and
   Competitiveness, through Grants Number DPI2011-27778-C02-01/02 and
   DPI2014-51844-C2-1/2-R.
CR Alastrué V, 2006, J BIOMECH ENG-T ASME, V128, P150, DOI 10.1115/1.2132368
   Ammar A, 2007, J NON-NEWTON FLUID, V144, P98, DOI 10.1016/j.jnnfm.2007.03.009
   Ammar A, 2006, J NON-NEWTON FLUID, V139, P153, DOI 10.1016/j.jnnfm.2006.07.007
   Ammar A, 2011, INT J MULTISCALE COM, V9, P17, DOI 10.1615/IntJMultCompEng.v9.i1.30
   Ammar A, 2010, COMPUT METHOD APPL M, V199, P1872, DOI 10.1016/j.cma.2010.02.012
   Ammar A, 2013, INT J NUMER METH ENG, V93, P887, DOI 10.1002/nme.4413
   Ammar A, 2012, INT J NUMER METH ENG, V90, P569, DOI 10.1002/nme.3331
   Ammar A, 2009, EUR J COMPUT MECH, V18, P445, DOI 10.3166/EJCM.18.445-463
   Amsallem D, 2008, AIAA J, V46, P1803, DOI 10.2514/1.35374
   [Anonymous], 1996, STOCHASTIC PROCESSES
   Authors V., 2006, TECH REP
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Barbic J, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P171
   Barrault M, 2004, CR MATH, V339, P667, DOI 10.1016/j.crma.2004.08.006
   Bellotti G, 2007, COAST ENG, V54, P680, DOI 10.1016/j.coastaleng.2007.02.002
   Bird R.B., 1987, DYNAMICS OF POLYMERI, V2
   Bognet B, 2012, COMPUT METHOD APPL M, V201, P1, DOI 10.1016/j.cma.2011.08.025
   Bro-Nielsen M., 1996, Computer Graphics Forum, V15, pC57, DOI 10.1111/1467-8659.1530057
   Cancs E., 2003, Handbook of Numerical Analysis, Volume X: Special Volume: Computational Chemistry, P3, DOI [10.1016/S1570-8659(03)10003-8, DOI 10.1016/S1570-8659(03)10003-8]
   Chaturantabut S, 2010, SIAM J SCI COMPUT, V32, P2737, DOI 10.1137/090766498
   Chinesta F, 2013, ARCH COMPUT METHOD E, V20, P31, DOI 10.1007/s11831-013-9080-x
   Chinesta F, 2010, INT J NUMER METH ENG, V83, P1114, DOI 10.1002/nme.2794
   Chinesta F., 2014, PGD BASED MODELING O
   Chinesta F, 2011, ARCH COMPUT METHOD E, V18, P395, DOI 10.1007/s11831-011-9064-7
   Chinesta F, 2010, ARCH COMPUT METHOD E, V17, P327, DOI 10.1007/s11831-010-9049-y
   Cotin S, 1999, IEEE T VIS COMPUT GR, V5, P62, DOI 10.1109/2945.764872
   Courtecuisse H, 2010, PROG BIOPHYS MOL BIO, V103, P159, DOI 10.1016/j.pbiomolbio.2010.09.016
   Delingette F, 2004, HDBK NUM AN, V12, P453
   Delingette H, 2005, COMMUN ACM, V48, P31, DOI 10.1145/1042091.1042116
   Doswell J.T., 2006, 6 IEEE INT C ADV LEA, P1182
   FUNG YC, 1993, BIOMECHANICS MECHANI
   Ghnatios C, 2012, COMPUT METHOD APPL M, V213, P29, DOI 10.1016/j.cma.2011.11.018
   Ghnatios C, 2011, COMPOS PART A-APPL S, V42, P1169, DOI 10.1016/j.compositesa.2011.05.001
   González D, 2012, MATH COMPUT SIMULAT, V82, P1677, DOI 10.1016/j.matcom.2012.04.001
   González D, 2010, INT J NUMER METH ENG, V81, P637, DOI 10.1002/nme.2710
   Hegland M, 2007, J COMPUT APPL MATH, V205, P708, DOI 10.1016/j.cam.2006.02.053
   Heyberger C, 2012, COMPUT MECH, V49, P277, DOI 10.1007/s00466-011-0646-x
   Inwood MJ, 2005, CLIN ANAT, V18, P613, DOI 10.1002/ca.20140
   Karhunen K, 1946, Ann. Acad. Sci. Fennicae, AI, P34
   Ketelhut DJ, 2010, J SCI EDUC TECHNOL, V19, P212
   Klopfer E, 2004, J COMPUT ASSIST LEAR, V20, P347, DOI 10.1111/j.1365-2729.2004.00094.x
   Klopfer E, 2008, ETR&D-EDUC TECH RES, V56, P203, DOI 10.1007/s11423-007-9037-6
   Koh C, 2010, J ENG EDUC, V99, P237, DOI 10.1002/j.2168-9830.2010.tb01059.x
   Kopfler E., 2008, AUGMENTED LEARNING
   Ladevèze P, 2010, COMPUT METHOD APPL M, V199, P1287, DOI 10.1016/j.cma.2009.06.023
   Ladeveze P., 1999, MECH ENG S, V1st
   Ladevèze P, 2011, COMPUT METHOD APPL M, V200, P2032, DOI 10.1016/j.cma.2011.02.019
   Laughlin RB, 2000, P NATL ACAD SCI USA, V97, P28, DOI 10.1073/pnas.97.1.28
   Le Bris C, 2009, CONSTR APPROX, V30, P621, DOI 10.1007/s00365-009-9071-1
   Loeve M.M., 1963, THE UNIVERSITY SERIE
   Lorenz E.N., 1956, SCIENTIFIC REPORT NU
   Nguyen NC, 2008, INT J NUMER METH ENG, V73, P521, DOI 10.1002/nme.2086
   Niroomandi S, 2008, COMPUT METH PROG BIO, V91, P223, DOI 10.1016/j.cmpb.2008.04.008
   Niroomandi S, 2013, INT J NUMER METH BIO, V29, P586, DOI 10.1002/cnm.2544
   Niroomandi S, 2012, COMPUT METH PROG BIO, V105, P1, DOI 10.1016/j.cmpb.2010.06.012
   Niroomandi S, 2010, INT J NUMER METH ENG, V81, P1180, DOI 10.1002/nme.2733
   Nouy A, 2010, COMPUT METHOD APPL M, V199, P1603, DOI 10.1016/j.cma.2010.01.009
   Price S, 2004, COMPUT EDUC, V43, P137, DOI 10.1016/j.compedu.2003.12.009
   Pruliere E, 2010, MATH COMPUT SIMULAT, V81, P791, DOI 10.1016/j.matcom.2010.07.015
   Quesada C., INTERACTIVE SIMULATI
   Ryckelynck D, 2006, ARCH COMPUT METHOD E, V13, P91, DOI 10.1007/BF02905932
   Seitz AR, 2009, NEURON, V61, P700, DOI 10.1016/j.neuron.2009.01.016
   Selwyn N, 2010, INFORM COMMUN SOC, V13, P788, DOI 10.1080/13691181003792616
   Squire K, 2007, J LEARN SCI, V16, P371, DOI 10.1080/10508400701413435
   Swaak J., 1996, Studies in Educational Evaluation, V22, P341
   Talbot D, 2012, GIVEN TABLETS BUT NO
   Taylor ZA, 2009, MED IMAGE ANAL, V13, P234, DOI 10.1016/j.media.2008.10.001
   Wang P, 2007, COMPUT STRUCT, V85, P331, DOI 10.1016/j.compstruc.2006.11.021
NR 68
TC 3
Z9 4
U1 0
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2016
VL 32
IS 11
BP 1465
EP 1479
DI 10.1007/s00371-015-1134-7
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EA2BS
UT WOS:000386397000009
DA 2024-07-18
ER

PT J
AU Lee, H
   Kyung, MH
AF Lee, Hyunho
   Kyung, Min-Ho
TI Parallel mesh simplification using embedded tree collapsing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 33rd Conference on Computer Graphics International (CGI)
CY JUN 28-JUL 01, 2016
CL Heraklion, GREECE
SP Fdn Res Technol
DE Mesh simplification; Edge collapsing; Parallel algorithm; GPU
AB We present a novel parallel algorithm for mesh simplification that can reduce an input triangle mesh with highly improved performance. To take full advantage of the GPU comprising many computing cores, we enable collapsing of connected edges to be processed at one time by breaking data dependency in the update of the mesh data structure. Our solution is a lazy update method, which temporarily stores edge update information in a table and then updates the mesh data with it in the next step. Thanks to the lazy update method, we can more freely choose a large number of edges in the form of small trees for collapsing. The constructed trees are split to satisfy an error constraint, prevent normal flipping, and preserve the mesh topology. In experiments performed on several test models of various scales, we found that our algorithm consistently outperformed the prior GPU algorithm of Papageorgiou and Platis (Vis Comput 31(2):235-244, 2015) by a factor of 10 or higher.
C1 [Lee, Hyunho; Kyung, Min-Ho] Ajou Univ, Dept Digital Media, Suwon, South Korea.
C3 Ajou University
RP Kyung, MH (corresponding author), Ajou Univ, Dept Digital Media, Suwon, South Korea.
EM kyung@ajou.ac.kr
CR [Anonymous], 1993, Modeling in Computer Graphics
   [Anonymous], 2008, MESHLAB OPEN SOURCE, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136
   Cabiddu D, 2015, COMPUT GRAPH-UK, V51, P81, DOI 10.1016/j.cag.2015.05.015
   Cellier F, 2012, WEB3D 2012, P73
   DeCoro C, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P161
   Dey Tamal., 1999, Publications de lInstitut Mathmatique, V66, P23
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Grund Nico., 2011, Proceedings of the Vision, Modeling, and Visualization Workshop 2011, Berlin, Germany, 4-6 October, 2011, P293
   Gueziec A., 1995, Second Annual International Symposium on Medical Robotics and Computer Assisted Surgery, MRCAS '95, P132
   Hjelmervik J, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P91, DOI 10.1109/SMI.2007.17
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hoppe H., 1993, 930101 TR U WASH DEP
   Kobbelt L, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P43
   Lindstrom P, 2000, COMP GRAPH, P259, DOI 10.1145/344779.344912
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   Papageorgiou A, 2015, VISUAL COMPUT, V31, P235, DOI 10.1007/s00371-014-1039-x
   Salinas D, 2015, COMPUT GRAPH FORUM, V34, P211, DOI 10.1111/cgf.12531
   Schaefer S., 2003, P SIAM GEOMETRICDESI
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Shontz SM, 2012, P 21 INT MESH ROUNDT P 21 INT MESH ROUNDT, P475
   Xiong H., 2008, P 8 EUR C PAR GRAPH, P33
NR 21
TC 8
Z9 11
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2016
VL 32
IS 6-8
BP 967
EP 976
DI 10.1007/s00371-016-1242-z
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DP8ET
UT WOS:000378731600028
DA 2024-07-18
ER

PT J
AU Liu, Q
   Zhang, CM
   Guo, Q
   Xu, H
   Zhou, YF
AF Liu, Qian
   Zhang, Caiming
   Guo, Qiang
   Xu, Hui
   Zhou, Yuanfeng
TI Adaptive sparse coding on PCA dictionary for image denoising
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference (CVM)
CY APR 16-17, 2015
CL Tsinghua Univ, Beijing, PEOPLES R CHINA
HO Tsinghua Univ
DE Image denoising; Sparse coding; Iterative shrinkage; Principal component
   analysis
ID TRANSFORM; ALGORITHM; DOMAIN
AB Sparse coding is a popular technique in image denoising. However, owing to the ill-posedness of denoising problems, it is difficult to obtain an accurate estimation of the true code. To improve denoising performance, we collect the sparse coding errors of a dataset on a principal component analysis dictionary, make an assumption on the probability of errors and derive an energy optimization model for image denoising, called adaptive sparse coding on a principal component analysis dictionary (ASC-PCA). The new method considers two aspects. First, with a PCA dictionary-related observation of the probability distributions of sparse coding errors on different dimensions, the regularization parameter balancing the fidelity term and the nonlocal constraint can be adaptively determined, which is critical for obtaining satisfying results. Furthermore, an intuitive interpretation of the constructed model is discussed. Second, to solve the new model effectively, a filter-based iterative shrinkage algorithm containing the filter-based back-projection and shrinkage stages is proposed. The filter in the back-projection stage plays an important role in solving the model. As demonstrated by extensive experiments, the proposed method performs optimally in terms of both quantitative and visual measurements.
C1 [Liu, Qian; Zhang, Caiming; Guo, Qiang; Xu, Hui; Zhou, Yuanfeng] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
   [Zhang, Caiming; Guo, Qiang] Shandong Univ Finance & Econ, Shandong Prov Key Lab Digital Media Technol, Jinan 250014, Peoples R China.
C3 Shandong University; Shandong University of Finance & Economics
RP Zhang, CM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.; Zhang, CM (corresponding author), Shandong Univ Finance & Econ, Shandong Prov Key Lab Digital Media Technol, Jinan 250014, Peoples R China.
EM alicesdu@gmail.com; czhang@sdu.edu.cn
RI Wu, Yiping/JJF-6185-2023; liu, xinyi/KFB-4466-2024; Zhang,
   Caiming/AHD-6558-2022; dong, zhao/JHS-9392-2023; Guo, Qiang/I-2949-2019
OI Wu, Yiping/0009-0000-6223-5786; Zhang, Caiming/0000-0002-6365-6221; Guo,
   Qiang/0000-0003-4219-3528
FU National Natural Science Foundation of China [61332015, 61373078,
   61272245, 61472220, 61202148]; NSFC-Guangdong Joint Fund [U1201258]
FX We thank all the anonymous reviewers for their helpful suggestions. This
   work was supported by the National Natural Science Foundation of China
   (61332015, 61373078, 61272245, 61472220, 61202148) and the
   NSFC-Guangdong Joint Fund (U1201258).
CR Amer A, 2005, IEEE T CIRC SYST VID, V15, P113, DOI 10.1109/TCSVT.2004.837017
   [Anonymous], MATH MODELS COMPUTER
   Bini AA, 2014, VISUAL COMPUT, V30, P311, DOI 10.1007/s00371-013-0857-6
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633
   Dabov K., 2009, WORKSH SIGN PROC AD
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Ignacio UA, 2007, VISUAL COMPUT, V23, P733, DOI 10.1007/s00371-007-0139-2
   Liang MY, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4943-1
   Liu W, 2013, IEEE T IMAGE PROCESS, V22, P872, DOI 10.1109/TIP.2012.2219544
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Muresan DD, 2003, IEEE IMAGE PROC, P101
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rajwade A, 2013, IEEE T PATTERN ANAL, V35, P849, DOI 10.1109/TPAMI.2012.140
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shi JJ, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-011-4422-5
   Shin DH, 2005, IEEE T CONSUM ELECTR, V51, P218, DOI 10.1109/TCE.2005.1405723
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI [10.1109/TIP.2002.1014998, 10.1117/12.408568]
   Tan S, 2007, INT J COMPUT VISION, V75, P209, DOI 10.1007/s11263-006-0019-7
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang L, 2010, PATTERN RECOGN, V43, P1531, DOI 10.1016/j.patcog.2009.09.023
NR 29
TC 18
Z9 21
U1 2
U2 24
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2016
VL 32
IS 4
BP 535
EP 549
DI 10.1007/s00371-015-1087-x
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI6YD
UT WOS:000373645200011
DA 2024-07-18
ER

PT J
AU Martínez, J
   Pla, N
   Vigo, M
AF Martinez, Jones
   Pla, Nuria
   Vigo, Marc
TI The three-dimensional cube and scale cube skeleton
SO VISUAL COMPUTER
LA English
DT Article
DE Skeletal representations; Orthogonal polyhedra
ID MEDIAL AXIS; VORONOI DIAGRAM; COMPUTATION; REPRESENTATIONS; SEGMENTS;
   DOMAINS; 2D
AB The recently introduced cube and scale cube skeleton of Martinez et al. (Graph Models 75:189-207, 2013) are a new type of skeletal representations for polygons or polyhedra enclosed by axis-aligned edges or faces. In this paper, we present efficient algorithms to compute the three-dimensional cube and scale cube skeleton. In addition, we analyze the combinatorial complexity of the three-dimensional cube skeleton. We also introduce the three-dimensional interior cube skeleton, which is homotopically equivalent to the input shape. Finally, we experimentally evaluate the efficiency and robustness of all the presented algorithms and compare the obtained skeletons with other relevant skeletal representations.
C1 [Martinez, Jones; Pla, Nuria; Vigo, Marc] Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat, E-08028 Barcelona, Spain.
C3 Universitat Politecnica de Catalunya
RP Martínez, J (corresponding author), Univ Politecn Cataluna, Dept Llenguatges & Sistemes Informat, Edifici ETSEIB,Diagonal 647,8a Planta, E-08028 Barcelona, Spain.
EM jmartinez@lsi.upc.edu
RI Bayona, Jonàs Martínez/AAQ-4236-2020; Vigo, Marc/H-6818-2015
OI Bayona, Jonàs Martínez/0000-0001-8443-9624; Vigo,
   Marc/0000-0001-7412-0425
FU Spanish government [TIN2008-02903, TIN2011-24220]; IBEC (Bioengineering
   Institute of Catalonia)
FX We would like to thank Dani Lopez for generating some figures in the
   article. The armadillo, lion and dragon models are courtesy of the
   Stanford 3D scanning repository. The gargoyle, tyrannosaurus and buste
   models are courtesy of the Aim@Shape shape repository. This work has
   been partially supported by the project TIN2008-02903 and TIN2011-24220
   of the Spanish government and by the IBEC (Bioengineering Institute of
   Catalonia).
CR Aichholzer Oswin, 2012, Curves and Surfaces. 7th International Conference, Curves and Surfaces 2010. Revised Selected Papers, P1, DOI 10.1007/978-3-642-27413-8_1
   Amenta N, 2001, COMP GEOM-THEOR APPL, V20, P25, DOI 10.1016/S0925-7721(01)00033-5
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   Anglada MV, 1997, COMPUT GRAPH, V21, P215, DOI 10.1016/S0097-8493(96)00085-4
   [Anonymous], 1995, Davenport-Schinzel Sequences and Their Geometric Applications
   [Anonymous], 2002, P 7 ACM S SOLID MODE, DOI DOI 10.1145/566282.566333
   [Anonymous], 1995, Journal of Universal Computer Science, DOI DOI 10.3217/JUCS-001-12-0752
   Attali D, 1997, COMPUT VIS IMAGE UND, V67, P261, DOI 10.1006/cviu.1997.0536
   Attali D, 2009, MATH VIS, P109, DOI 10.1007/b106657_6
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Balaban I. J., 1995, Proceedings of the Eleventh Annual Symposium on Computational Geometry, P211, DOI 10.1145/220279.220302
   Barequet G, 2008, LECT NOTES COMPUT SC, V5193, P148, DOI 10.1007/978-3-540-87744-8_13
   Biasotti S, 2008, MATH VIS, P145, DOI 10.1007/978-3-540-33265-7_5
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   BRANDT JW, 1992, CVGIP-IMAG UNDERSTAN, V55, P329, DOI 10.1016/1049-9660(92)90030-7
   BRANDT JW, 1994, CVGIP-IMAG UNDERSTAN, V59, P116, DOI 10.1006/ciun.1994.1007
   Cao LX, 2011, COMPUT AIDED DESIGN, V43, P979, DOI 10.1016/j.cad.2011.03.001
   Chazal F, 2005, GRAPH MODELS, V67, P304, DOI 10.1016/j.gmod.2005.01.002
   CHEW LP, 1989, ALGORITHMICA, V4, P97, DOI 10.1007/BF01553881
   Choi HI, 1997, PAC J MATH, V181, P57, DOI 10.2140/pjm.1997.181.57
   Culver T, 2004, COMPUT AIDED GEOM D, V21, P65, DOI 10.1016/j.cagd.2003.07.008
   Demaine ED, 2005, LECT NOTES COMPUT SC, V3608, P205
   Etzion M., 1999, Proceedings of the fifth ACM symposium on Solid modeling and applications, P167
   Giesen J, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P106, DOI 10.1145/1542362.1542388
   HERSHBERGER J, 1989, INFORM PROCESS LETT, V33, P169, DOI 10.1016/0020-0190(89)90136-1
   KIRKPATRICK D, 1983, SIAM J COMPUT, V12, P28, DOI 10.1137/0212002
   Koltun V, 2003, SIAM J COMPUT, V32, P616, DOI 10.1137/S0097539702408387
   Koltun V., 2002, P 18 ANN ACM S COMP, P227
   Livesu M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508388
   Lopez Monterde D., 2014, INT C COMP GRAPH THE
   Martin T, 2012, COMPUT GRAPH FORUM, V31, P805, DOI 10.1111/j.1467-8659.2012.03061.x
   Martinez J, 2011, COMPUT GRAPH FORUM, V30, P1573, DOI 10.1111/j.1467-8659.2011.02031.x
   Martínez J, 2013, GRAPH MODELS, V75, P189, DOI 10.1016/j.gmod.2013.03.005
   Miklos B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778838
   Papadopoulou E, 2001, INT J COMPUT GEOM AP, V11, P503, DOI 10.1142/S0218195901000626
   Pizer SM, 2003, INT J COMPUT VISION, V55, P155, DOI 10.1023/A:1026135101267
   Preparata F.P., 1985, COMPUTATIONAL GEOMET, DOI DOI 10.1007/978-1-4612-1098-6
   REDDY JM, 1995, COMPUT AIDED DESIGN, V27, P677, DOI 10.1016/0010-4485(94)00025-9
   Schwarz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866201
   Sherbrooke EC, 1996, IEEE T VIS COMPUT GR, V2, P44, DOI 10.1109/2945.489386
   Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8
   SRINIVASAN V, 1987, IBM J RES DEV, V31, P361, DOI 10.1147/rd.313.0361
   Stolpner S, 2011, COMPUT VIS IMAGE UND, V115, P695, DOI 10.1016/j.cviu.2010.10.014
   Sud A., 2005, P 2005 ACM S SOL PHY, P39, DOI DOI 10.1145/1060244.1060250
   Vigo M., 2011, ORTO BREP
   Vigo M, 2012, GRAPH MODELS, V74, P61, DOI 10.1016/j.gmod.2012.03.004
   Ward AD, 2010, IEEE T PATTERN ANAL, V32, P1084, DOI 10.1109/TPAMI.2009.81
   Whitehead JHC, 1939, P LOND MATH SOC, V45, P243
   Yoshizawa S, 2007, COMPUT GRAPH FORUM, V26, P255, DOI 10.1111/j.1467-8659.2007.01047.x
NR 49
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2015
VL 31
IS 9
BP 1233
EP 1252
DI 10.1007/s00371-014-1008-4
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CO8AY
UT WOS:000359388100007
DA 2024-07-18
ER

PT J
AU Biswas, R
   Bhowmick, P
AF Biswas, Ranita
   Bhowmick, Partha
TI Layer the sphere For accurate and additive voxelation by integer
   operation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE 3D printing; Discrete sphere; Integer algorithm; Rapid prototyping;
   Sphere voxelation
ID HOLLOW SPHERES; MICROSPHERES; HYPERSPHERES; RADIOTHERAPY; ALGORITHM;
   CIRCLES; DESIGN; MODELS
AB Voxelation today is not only limited to discretization and representation of 3D objects, but has also been gaining tremendous importance in rapid prototyping through 3D printing. In this paper, we introduce a novel technique for discretization of a sphere in the integer space, which gives rise to a set of mathematically accurate, 3D-printable physical voxels up to the desired level of precision. The proposed technique is based on an interesting correspondence between the voxel set forming a discrete sphere and certain easy-to-compute integer intervals defined by voxel position and sphere dimension. It gives us several algorithmic leverages, such as computational sufficiency with simple integer operations and amenability to layer-by-layer additive fabrication. Theoretical analysis, prototype characteristics, and experimental results demonstrate its efficiency, versatility, and further prospects.
C1 [Biswas, Ranita; Bhowmick, Partha] Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur
EM bhowmick@gmail.com
OI Biswas, Ranita/0000-0002-5372-7890
CR ANDRES E, 1994, COMPUT GRAPH, V18, P695, DOI 10.1016/0097-8493(94)90164-3
   Andres E, 1997, IEEE T VIS COMPUT GR, V3, P75, DOI 10.1109/2945.582354
   Augustin C, 2009, MATER LETT, V63, P1109, DOI 10.1016/j.matlet.2009.01.015
   Bera S, 2014, LECT NOTES COMPUT SC, V8321, P49, DOI 10.1007/978-3-319-04126-1_5
   Biglino G, 2011, ADV APPL RAPID PROTO
   BRESENHAM J, 1977, COMMUN ACM, V20, P100, DOI 10.1145/359423.359432
   Brimkov VE, 2008, THEOR COMPUT SCI, V406, P24, DOI 10.1016/j.tcs.2008.07.014
   Chamizo F, 2012, ACTA MATH HUNG, V135, P97, DOI 10.1007/s10474-011-0144-9
   CHANDRU V, 1995, IEEE COMPUT GRAPH, V15, P42, DOI 10.1109/38.469516
   Cochran JK, 1998, CURR OPIN SOLID ST M, V3, P474, DOI 10.1016/S1359-0286(98)80010-7
   COHENOR D, 1995, GRAPH MODEL IM PROC, V57, P453, DOI 10.1006/gmip.1995.1039
   Coxeter HMS., 1973, Dover Books on Mathematics, V3rd
   Desimone J.M., 2014, US Patent, Patent No. 20140361463
   Fiorio C, 2006, PROC SPIE, V6066, DOI 10.1117/12.642976
   Fiorio C, 2006, LECT NOTES COMPUT SC, V4245, P425
   Foley J.D., 1993, COMPUTER GRAPHICS PR
   Ghahramani MR, 2014, APPL RADIAT ISOTOPES, V85, P87, DOI 10.1016/j.apradiso.2013.12.009
   Guo LM, 2009, MATER LETT, V63, P1141, DOI 10.1016/j.matlet.2009.01.064
   Hearn E.J., 1997, MECH MAT 1, V3rd
   Hiller J, 2010, RAPID PROTOTYPING J, V16, P241, DOI 10.1108/13552541011049252
   Hiller J, 2009, RAPID PROTOTYPING J, V15, P137, DOI 10.1108/13552540910943441
   Hong J.-Y., 2015, VISUAL COMPUT, P1, DOI DOI 10.1007/S0037101510724
   Jee HJ, 2000, ADV ENG SOFTW, V31, P97, DOI 10.1016/S0965-9978(99)00045-9
   Kamrani A.K., 2009, ENG DESIGN RAPID PRO
   Kawashita M, 2003, BIOMATERIALS, V24, P2955, DOI 10.1016/S0142-9612(03)00094-2
   Kim OS, 2014, IEEE T ANTENN PROPAG, V62, P3839, DOI 10.1109/TAP.2014.2317489
   Klette R, 2001, SPRING INT SER ENG C, V628, P33
   Klette R., 2004, DIGITAL GEOMETRY GEO
   Lipson H, 2000, NATURE, V406, P974, DOI 10.1038/35023115
   Maehara H, 2010, EUR J COMBIN, V31, P617, DOI 10.1016/j.ejc.2009.03.034
   Sá AME, 2014, VISUAL COMPUT, V30, P1321, DOI 10.1007/s00371-013-0883-4
   Melchels FPW, 2012, PROG POLYM SCI, V37, P1079, DOI 10.1016/j.progpolymsci.2011.11.007
   Montani C., 1990, GRAPHICS GEMS, P327
   Nanya T, 2013, J ADV MECH DES SYST, V7, P362, DOI 10.1299/jamdsm.7.362
   Pintus R, 2010, VISUAL COMPUT, V26, P831, DOI 10.1007/s00371-010-0488-0
   Roget B, 2013, J COMPUT PHYS, V241, P76, DOI 10.1016/j.jcp.2013.01.035
   Sene FF, 2008, J NON-CRYST SOLIDS, V354, P4887, DOI 10.1016/j.jnoncrysol.2008.04.041
   Steingart R., 2013, US Patent, Patent No. 8509933
   Toutant JL, 2013, DISCRETE APPL MATH, V161, P2662, DOI 10.1016/j.dam.2013.06.001
   Waag U., 2000, MET POWDER REPORT, P29, DOI DOI 10.1016/S0026-0657(00)87339-7
   Zheng MB, 2006, MATER LETT, V60, P2991, DOI 10.1016/j.matlet.2006.02.030
NR 41
TC 6
Z9 7
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 787
EP 797
DI 10.1007/s00371-015-1101-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500005
DA 2024-07-18
ER

PT J
AU Pan, JJ
   Zhao, CK
   Zhao, X
   Hao, AM
   Qin, H
AF Pan, Junjun
   Zhao, Chengkai
   Zhao, Xin
   Hao, Aimin
   Qin, Hong
TI Metaballs-based physical modeling and deformation of organs for virtual
   surgery
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 32nd Computer Graphics International CGI 15 Conference
CY JUN 24-26, 2015
CL INSA Strasbourg Univ Strasbourg, Strasbourg, FRANCE
SP CNRS, iCUBE, Univ Strasbourg, CGS, Springer, Acm Incooperation, IGG, ACMSIGGRAPH, KIST Europe, Region Alsace, Visteon
HO INSA Strasbourg Univ Strasbourg
DE Metaballs; Optimization; Organ; Deformation; Skinning
ID SIMULATION
AB Prior research on metaballs-based modeling solely focuses on shape geometry and its processing for organic objects. This paper takes a different approach by exploring a new metaballs-based physical modeling method for digital organs that are imperative to support virtual surgery. We propose a novel hybrid physical model comprising both surface mesh and the metaballs which occupy organs' interior. The finer surface mesh with high-precision geometric information and texture is necessary to represent the boundary structure of organs. Through the use of metaballs, the organ interior is geometrically simplified via a set of overlapping spheres with different radii. This work's novelty hinges upon the integration of metaballs and position-based dynamics (PBD) which enables metaballs-based organs to serve as physical models and participate in dynamic simulation. For the metaballs construction, we develop an adaptive approach based on Voronoi Diagram for model initialization. Using global optimization, an electrostatic attraction model is proposed to drive the metaballs to best match with the organ's boundary. Using PBD, we devise a novel metaballs-based deformation algorithm, which preserves two local shape properties via constraints on Laplacian coordinates and local volume. To retain the organ's smooth deformation, we propose a new skinning method based on distance field, and it is employed to build the mapping between the metaballs and organ boundary. This metaballs-based deformation technique has already been integrated into a VR-based laparoscopic surgery simulator.
C1 [Pan, Junjun; Zhao, Chengkai; Zhao, Xin; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Comp Sci, Stony Brook, NY USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Pan, JJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM pan_junjun@hotmail.com; qin@cs.stonybrook.edu
RI Pan, Junjun/A-1316-2013
FU National Natural Science Foundation of China [61402025, 61190120,
   61190121, 61190125]; National Science Foundation of USA [IIS-0949467,
   1047715, 1049448]; Fundamental Research Funds for the Central
   Universities; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1047715, 1049448] Funding Source:
   National Science Foundation
FX We thank Junxuan Bai and Chen Yang for their work in the experiments.
   This research is supported by National Natural Science Foundation of
   China (No. 61402025, 61190120, 61190121, 61190125), National Science
   Foundation of USA (No. IIS-0949467, 1047715, 1049448) and the
   Fundamental Research Funds for the Central Universities
CR Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Bradshaw G, 2004, ACM T GRAPHIC, V23, P1, DOI 10.1145/966131.966132
   Bradshaw G., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P33, DOI DOI 10.1145/545261.545267
   Cueto E., 2014, Advanced Modeling and Simulation in Engineering Sciences, V1, P1, DOI DOI 10.1186/2213-7467-1-11
   France L., 2002, ESAIM P NOV 12, P42
   Gianluca D. N., 2010, INT C APPL BION BIOM, P23
   Hubbard PM, 1996, ACM T GRAPHIC, V15, P179, DOI 10.1145/231731.231732
   Jerabkova L, 2009, IEEE COMPUT GRAPH, V29, P61, DOI 10.1109/MCG.2009.32
   Jones B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2560795
   Liu T., 2013, ACM T GRAPH SIGGRAPH, V32, p[209, 1], DOI DOI 10.1145/2508363.2508406
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   MURAKI S, 1991, COMP GRAPH, V25, P227, DOI 10.1145/127719.122743
   Pan JJ, 2011, INT J MED ROBOT COMP, V7, P304, DOI 10.1002/rcs.399
   Pan JJ, 2009, COMPUT ANIMAT VIRT W, V20, P121, DOI 10.1002/cav.284
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Sorkine-Hornung O., 2004, EUR S GEOM PROC, P1
   Steinemann D., 2006, PROC ACM SIGGRAPHEUR, P63
   Suzuki S, 2004, IEEE T MED IMAGING, V23, P714, DOI 10.1109/TMI.2004.826947
   Wei Y., 2012, ANEURYSM, DOI DOI 10.5772/48635
   Wu J., 2014, EUROGRAPHICS 2014 ST, P1
   Wu J, 2013, VISUAL COMPUT, V29, P739, DOI 10.1007/s00371-013-0810-8
NR 21
TC 9
Z9 13
U1 1
U2 21
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2015
VL 31
IS 6-8
BP 947
EP 957
DI 10.1007/s00371-015-1106-y
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CM2CN
UT WOS:000357487500020
DA 2024-07-18
ER

PT J
AU McGraw, T
AF McGraw, Tim
TI Fast Bokeh effects using low-rank linear filters
SO VISUAL COMPUTER
LA English
DT Article
DE Bokeh; Blur; Filter; Depth-of-field
ID FIELD SIMULATION; DEPTH; GENERATION
AB We present a method for faster and more flexible approximation of camera defocus effects given a focused image of a virtual scene and depth map. Our method leverages the advantages of low-rank linear filtering by reducing the problem of 2D convolution to multiple 1D convolutions, which significantly reduces the computational complexity of the filtering operation. In the case of rank 1 filters (e.g., the box filter and Gaussian filter), the kernel is described as 'separable' since it can be implemented as a horizontal 1D convolution followed by a 1D vertical convolution. While many filter kernels which result in bokeh effects cannot be approximated closely by separable kernels, they can be effectively approximated by low-rank kernels. We demonstrate the speed and flexibility of low-rank filters by applying them to image blurring, tilt-shift postprocessing, and depth-of-field simulation, and also analyze the approximation error for several aperture shapes.
C1 Purdue Univ, Comp Graph Technol, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP McGraw, T (corresponding author), Purdue Univ, Comp Graph Technol, W Lafayette, IN 47907 USA.
EM tmcgraw@purdue.edu
OI McGraw, Tim/0000-0001-6704-6351
CR [Anonymous], 1998, ANISOTROPIC DIFFUSIO
   [Anonymous], DIGITAL IMAGE PROCES
   Bertalmío M, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P767, DOI 10.1109/TDPVT.2004.1335393
   Buhler J., 2002, ACM SIGGRAPH 2002 conference abstracts and applications, P142
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Demers J, 2004, GPU GEMS, V1, pU390
   Greene S., 2003, GAM DEV C, V6, P8
   Haeberli P., 1990, Computer Graphics, V24, P309, DOI 10.1145/97880.97913
   Hammon E., 2007, GPU GEMS, P583
   Hensley J, 2005, COMPUT GRAPH FORUM, V24, P547, DOI 10.1111/j.1467-8659.2005.00880.x
   Kass M., 2006, 2 PIX AN STUD
   Kraus M, 2011, GRAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P153
   Krivánek J, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P196, DOI 10.1109/CGI.2003.1214466
   Lee S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778802
   Lee S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618480
   Lee S, 2009, IEEE T VIS COMPUT GR, V15, P453, DOI 10.1109/TVCG.2008.106
   Lee S, 2008, COMPUT GRAPH FORUM, V27, P1955, DOI 10.1111/j.1467-8659.2008.01344.x
   McIntosh L, 2012, COMPUT GRAPH FORUM, V31, P1810, DOI 10.1111/j.1467-8659.2012.02097.x
   Merklinger H. M., 1997, PHOTOTECHNIQUES, V18
   Pharr M., 2010, PHYS BASED RENDERING
   Potmesil M., 1982, ACM Transactions on Graphics (TOG), V1, P85
   ROKITA P, 1993, COMPUT GRAPH, V17, P593, DOI 10.1016/0097-8493(93)90010-7
   Scheuermann T., 2004, GDC, V8
   Scofield B., 1992, GRAPHICS GEMS, P36
   SHINYA M, 1994, GRAPH INTER, P59
   Soler C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516529
   Tuttle S., 2012, PHOTOCRAFT CREATIVE
   Zhou TS, 2007, COMPUT GRAPH FORUM, V26, P15, DOI 10.1111/j.1467-8659.2007.00935.x
NR 28
TC 14
Z9 19
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 601
EP 611
DI 10.1007/s00371-014-0986-6
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CH2CF
UT WOS:000353831400008
DA 2024-07-18
ER

PT J
AU Zheng, Y
   Nie, XC
   Meng, ZP
   Feng, W
   Zhang, K
AF Zheng, Yan
   Nie, Xuecheng
   Meng, Zhaopeng
   Feng, Wei
   Zhang, Kang
TI Layered modeling and generation of Pollock's drip style
SO VISUAL COMPUTER
LA English
DT Article
DE Jackson Pollock; Abstract painting; Layered modeling; Processing;
   Generative art
ID ART
AB In this paper, we propose a layered approach to model Jackson Pollock's dripping style of paintings. Having analyzed fractal-based algorithms and observed the details of Pollock's paintings, we designed a layered modeling approach that divides Pollock's artwork into four layers: from bottom up are background layer, irregular shape layer, line layer and water drop layer. The layers are drawn sequentially and independent, forming the desired Pollock style. We have developed a program using Processing to generate artworks of the dripping style. The parameters of our program can be randomly generated or tuned by the user, supporting high flexibility and effectiveness. Experimental results show that our layered modeling approach can systematically generate images resembling Pollock's dripping style.
C1 [Zheng, Yan; Nie, Xuecheng; Meng, Zhaopeng; Zhang, Kang] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
   [Feng, Wei] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Visual Comp Lab, Richardson, TX 75083 USA.
C3 Tianjin University; Tianjin University; University of Texas System;
   University of Texas Dallas
RP Zhang, K (corresponding author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM yanzheng@tju.edu.cn; xcnie@tju.edu.cn; mengzp@tju.edu.cn;
   wfeng@tju.edu.cn; kzhang@utdallas.edu
RI Feng, Wei/E-3985-2016; Li, Guo/JNR-1700-2023
OI ZHENG, YAN/0000-0003-2741-058X
CR [Anonymous], 1998, Fractals Everywhere
   Borgeat L, 2007, IEEE COMPUT GRAPH, V27, P60, DOI 10.1109/MCG.2007.162
   Coddington J., 2008, P COMP IM AN STUD AR
   Fairbanks M., 2010, P EL IM SPIE 10, V7531, P1
   Fogleman M, 2011, PROCEDURALLY GENERAT
   HAEBERLI P, 1990, SIGGRAPH
   Irfan M., 2009, P IS SPIE EL IM
   Jensen HJ, 2002, INTERDISCIPL SCI REV, V27, P45, DOI 10.1179/030801802225000000
   Johnson CR, 2008, IEEE SIGNAL PROC MAG, V25, P37, DOI 10.1109/MSP.2008.923513
   KIRSCH JL, 1988, LEONARDO, V21, P437, DOI 10.2307/1578708
   Lyu S, 2004, P NATL ACAD SCI USA, V101, P17006, DOI 10.1073/pnas.0406398101
   Mandelbrot B., 1734, NEW SCI, V1990, P38
   Marcos AF, 2007, IEEE COMPUT GRAPH, V27, P98, DOI 10.1109/MCG.2007.123
   Mureika J.R., 2012, IEEE SIGNAL PROCESS, V93, P573
   Mureika JR, 2005, CHAOS, V15, DOI 10.1063/1.2121947
   Mureika JR, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.046101
   Mureika JR, 2004, LEONARDO, V37, P53, DOI 10.1162/002409404772828139
   Naifeh Steven, 1989, J POLLOCK AM SAGA
   NOLL AM, 1966, PSYCHOL REC, V16, P1
   Reas C., 2007, PROGRAMMING HDB VISU
   Redies C, 2008, SPATIAL VISION, V21, P137, DOI 10.1163/156856808782713825
   Singh G, 2005, IEEE COMPUT GRAPH, V25, P4, DOI 10.1109/MCG.2005.57
   SINGH G, 2009, IEEE COMPUT GRAPH, V29, P4
   Taylor R, 1999, PHYS WORLD, V12, P25
   Taylor RP, 2007, PATTERN RECOGN LETT, V28, P695, DOI 10.1016/j.patrec.2006.08.012
   Taylor Richard, 2006, Hist. cienc. saude-Manguinhos, V13, P109, DOI 10.1590/S0104-59702006000500007
   Taylor Richard P, 2011, Front Hum Neurosci, V5, P60, DOI 10.3389/fnhum.2011.00060
   Taylor RP, 1999, NATURE, V399, P422, DOI 10.1038/20833
   Taylor RP, 2002, LEONARDO, V35, P203, DOI 10.1162/00240940252940603
   Taylor RP, 2002, SCI AM, V287, P116, DOI 10.1038/scientificamerican1202-116
   Zhang K., 2014, GENERATION IN PRESS
   Zhang K, 2007, IEEE COMPUT GRAPH, V27, P12, DOI 10.1109/MCG.2007.58
   Zhang K, 2012, LEONARDO, V45, P243, DOI 10.1162/LEON_a_00366
   Zhao M., 2010, PROC INT S NONPHOTOR, P99, DOI DOI 10.1145/1809939.1809951
NR 34
TC 12
Z9 13
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2015
VL 31
IS 5
BP 589
EP 600
DI 10.1007/s00371-014-0985-7
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA CH2CF
UT WOS:000353831400007
DA 2024-07-18
ER

PT J
AU Tung, T
   Matsuyama, T
AF Tung, Tony
   Matsuyama, Takashi
TI Invariant shape descriptor for 3D video encoding
SO VISUAL COMPUTER
LA English
DT Article
DE Invariant shape descriptor; Dynamic surface; Geometry image; 3D video;
   Reeb graph
ID RECOGNITION; COMPRESSION; SURFACE; DEFORMATION; GEOMETRY; CAPTURE
AB This paper presents a novel approach to represent spatio-temporal visual information. We introduce a surface-based shape model whose structure is invariant to surface variations over time to describe 3D dynamic surfaces (e.g., 3D video obtained from multiview video capture). The descriptor is defined as a graph lying on object surfaces and anchored to invariant local features (e.g., surface point extrema). Geodesic consistency-based priors are used as cues within a probabilistic framework to maintain the graph invariant, even though the surfaces undergo non-rigid deformations. Our contribution brings to 3D geometric data a temporally invariant structure that relies only on intrinsic surface properties, and is independent of surface parameterization (i.e., surface mesh connectivity). The proposed descriptor can therefore be used for efficient dynamic surface encoding, through transformation into 2D (geometry) images, as its structure can provide an invariant representation for dynamic 3D mesh models. Various experiments on challenging publicly available datasets are performed to assess invariant property and performance of the descriptor.
C1 [Tung, Tony; Matsuyama, Takashi] Kyoto Univ, Grad Sch Informat, Sakyo Ku, Kyoto, Japan.
C3 Kyoto University
RP Tung, T (corresponding author), Kyoto Univ, Grad Sch Informat, Sakyo Ku, Kyoto, Japan.
EM tony2ng@gmail.com; tm@i.kyoto-u.ac.jp
FU JST-CREST
FX This work was supported in part by the JST-CREST project "Creation of
   Human-Harmonized Information Technology for Convivial Society". The
   authors thank Dr. Lyndon Hill for his preliminary work on this project.
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Allard J., 2007, MARKERLESS 3D INTERA
   Alliez P, 2005, MATH VIS, P3, DOI 10.1007/3-540-26808-1_1
   [Anonymous], 2006, CVPR
   Baran I., 2007, ACM T GRAPHIC, V26, P27
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Briceno H. M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P136
   Bronstein AM, 2007, IEEE T VIS COMPUT GR, V13, P902, DOI 10.1109/TVCG.2007.1041
   Cagniart C., 2010, P EUR C COMP VIS
   Carr N., 2006, P EUROGRAPHICSSIGGRA, P181
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cornea ND, 2005, VISUAL COMPUT, V21, P945, DOI 10.1007/s00371-005-0308-0
   de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697
   Edelsbrunner H., 2004, P S COMP GEOM
   Erickson J, 2004, DISCRETE COMPUT GEOM, V31, P37, DOI 10.1007/s00454-003-2948-z
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   FORSYTH D, 1991, IEEE T PATTERN ANAL, V13, P971, DOI 10.1109/34.99233
   Franco J., 2004, P IEEE C COMP VIS PA
   Gu XF, 2002, ACM T GRAPHIC, V21, P355
   Guo Y.W., 2005, SPRINGER LNCS
   Habe H., 2004, P PICT COD S
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Huang P., 2010, P 3DPVT
   Jiang H., 2012, P EUR C COMP VIS
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Kavan L, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P39
   Klein T., 2005, P TOP BAS METH VIS
   Mamou K., 2008, P IEEE INT C MULT EX
   Matsuyama T, 2004, COMPUT VIS IMAGE UND, V96, P393, DOI 10.1016/j.cviu.2004.03.012
   Matsuyama Takashi., 2012, 3D Video and Its Applications
   Mémoli F, 2005, FOUND COMPUT MATH, V5, P313, DOI 10.1007/s10208-004-0145-y
   Morse M., 1934, CALCULUS VARIATIONS, P18
   MORTARA M, 2002, P SHAP MOD INT
   Mundy J., 1992, GEOMETRIC INVARIANCE
   Palágyi K, 1999, GRAPH MODEL IM PROC, V61, P199, DOI 10.1006/gmip.1999.0498
   Pascucci V, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239509
   REEB G, 1946, CR HEBD ACAD SCI, V222, P847
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Saboret L., 2012, CGAL REFERENCE MANUA
   Sander P. V., 2003, Symposium on Geometry Processing, P146
   Starck J., 2005, P IEEE INT C COMP VI
   Starck J, 2007, IEEE COMPUT GRAPH, V27, P21, DOI 10.1109/MCG.2007.68
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Tung T., 2005, International Journal of Shape Modeling, V11, P91, DOI 10.1142/S0218654305000748
   Tung T., 2007, P IEEE C COMP VIS PA
   Tung T., 2012, P AS C COMP VIS
   Tung T., 2010, P IEEE C COMP VIS PA
   Tung T, 2012, IEEE T PATTERN ANAL, V34, P1645, DOI 10.1109/TPAMI.2011.258
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   [No title captured]
NR 52
TC 2
Z9 2
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2015
VL 31
IS 3
BP 311
EP 324
DI 10.1007/s00371-014-0925-6
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC3DB
UT WOS:000350223900006
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Ling, J
   Zhang, XH
   Xie, H
AF Zhang, Yun
   Ling, Jian
   Zhang, Xiaohong
   Xie, Hao
TI Image copy-and-paste with optimized gradient
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computational Visual Media Conference
CY 2013
CL Geneva, SWITZERLAND
DE Copy-and-paste; Gradient transition map; Smooth transition; Image
   cloning
AB This paper presents a novel approach to image copy-and-paste with optimized gradient. We improve the traditional gradient domain cloning by creating smooth transition from source patches to target images. We first specify a source patch and select its foreground region by strokes. Next, we create a gradient transition map in the cloning area. Finally, we reconstruct the gradient of the source patch according to the gradient transition map, and propose an interpolation based method to efficiently calculate the composition results, which avoids solving a large linear system. Experimental results demonstrate the effectiveness of our method, which can produce more natural and satisfying results.
C1 [Zhang, Yun; Ling, Jian; Zhang, Xiaohong] Zhejiang Univ Media & Commun, Hangzhou 310018, Zhejiang, Peoples R China.
   [Xie, Hao] Zhejiang Univ, Inst Artificial Intelligence, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Communication University of Zhejiang; Zhejiang University
RP Zhang, Y (corresponding author), Zhejiang Univ Media & Commun, Hangzhou 310018, Zhejiang, Peoples R China.
EM zhangyun_zju@zju.edu.cn
RI Zhang, Xiaohong/A-3060-2015
FU National Basic Research Program of China [2011CB302205]; National
   High-Tech Research and Development Program of China [2013AA013903];
   Zhejiang Provincial Natural Science Foundation of China [LY13F020036,
   LY12F02032]
FX We thank all anonymous reviewers for their valuable comments. This work
   was supported by National Basic Research Program of China (No.
   2011CB302205), National High-Tech Research and Development Program of
   China (No. 2013AA013903), Zhejiang Provincial Natural Science Foundation
   of China (No. LY13F020036) and (No. LY12F02032).
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bie XH, 2011, J COMPUT SCI TECH-CH, V26, P1011, DOI 10.1007/s11390-011-1197-5
   Bie XH, 2013, VISUAL COMPUT, V29, P599, DOI 10.1007/s00371-013-0826-0
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Du H, 2013, VISUAL COMPUT, V29, P217, DOI 10.1007/s00371-012-0722-z
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   He KM, 2010, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR.2010.5539896
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Liu JY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531375
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sun X, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185570
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Wang J., 2007, IEEE C COMP VIS PATT
   Wang R, 2010, J ZHEJIANG U-SCI C, V11, P690, DOI 10.1631/jzus.C1000067
   Xie ZF, 2010, VISUAL COMPUT, V26, P1123, DOI 10.1007/s00371-010-0466-6
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   Yang WX, 2009, IEEE T IMAGE PROCESS, V18, P2584, DOI 10.1109/TIP.2009.2027365
   Zhang FL, 2012, IEEE T VIS COMPUT GR, V18, P1849, DOI 10.1109/TVCG.2012.68
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
NR 31
TC 6
Z9 7
U1 0
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2014
VL 30
IS 10
BP 1169
EP 1178
DI 10.1007/s00371-013-0897-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA AP6NF
UT WOS:000342193800010
DA 2024-07-18
ER

PT J
AU Jiang, XB
   Zhong, F
   Peng, QS
   Qin, XY
AF Jiang, Xinbo
   Zhong, Fan
   Peng, Qunsheng
   Qin, Xueying
TI Online robust action recognition based on a hierarchical model
SO VISUAL COMPUTER
LA English
DT Article
DE Robust online action recognition; Hierarchical model; Bottom-up approach
AB Action recognition solely based on video data has known to be very sensitive to background activity, and also lacks the ability to discriminate complex 3D motion. With the development of commercial depth cameras, skeleton-based action recognition is becoming more and more popular. However, the skeleton-based approach is still very challenging because of the large variation in human actions and temporal dynamics. In this paper, we propose a hierarchical model for action recognition. To handle confusing motions, a motion-based grouping method is proposed, which can efficiently assign each video a group label, and then for each group, a pre-trained classifier is used for frame-labeling. Unlike previous methods, we adopt a bottom-up approach that first performs action recognition for each frame. The final action label is obtained by fusing the classification to its frames, with the effect of each frame being adaptively adjusted based on its local properties. To achieve online real-time performance and suppressing noise, bag-of-words is used to represent the classification features. The proposed method is evaluated using two challenge datasets captured by a Kinect. Experiments show that our method can robustly recognize actions in real-time.
C1 [Jiang, Xinbo; Zhong, Fan; Qin, Xueying] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Peng, Qunsheng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310003, Zhejiang, Peoples R China.
   [Qin, Xueying] Shandong Prov Key Lab Network Based Intelligent C, Jinan, Peoples R China.
C3 Shandong University; Zhejiang University
RP Zhong, F (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
EM zhongfan@sdu.edu.cn; qxy@sdu.edu.cn
RI jiang, xinbo/AAL-7727-2021; Qin, Xueying/AAM-8775-2021
OI Qin, Xueying/0000-0003-0057-295X
FU 973 program of China [2009CB320802]; NSF of China [U1035004, 61173070,
   61202149]; Key Projects in the National Science & Technology Pillar
   Program [2013BAH39F00]
FX The authors gratefully acknowledge the anonymous reviewers for their
   comments to help us to improve our paper, and also thank Guofeng Wang
   for his enormous help in revising this paper. This work is supported by
   973 program of China (No. 2009CB320802), NSF of China (Nos. U1035004,
   61173070, 61202149), Key Projects in the National Science & Technology
   Pillar Program (No. 2013BAH39F00).
CR [Anonymous], 2009, WORKSH VID OR OBJ EV
   [Anonymous], 2008, C COMP VIS PATT REC
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Chatzis SP, 2013, PATTERN RECOGN, V46, P1569, DOI 10.1016/j.patcog.2012.11.028
   Ellis C, 2013, INT J COMPUT VISION, V101, P420, DOI 10.1007/s11263-012-0550-7
   Fathi A., 2008, CVPR
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Ke Y, 2007, IEEE I CONF COMP VIS, P1424
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Negin F., 2013, DECISION FOREST BASE
   Shechtman E, 2007, IEEE T PATTERN ANAL, V29, P2045, DOI 10.1109/TPAMI.2007.1119
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Yang X., 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
NR 16
TC 25
Z9 27
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2014
VL 30
IS 9
BP 1021
EP 1033
DI 10.1007/s00371-014-0923-8
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO5GP
UT WOS:000341372200006
DA 2024-07-18
ER

PT J
AU Lee, S
   Kim, Y
   Lee, J
   Kim, K
   Lee, K
   Noh, J
AF Lee, Sangwoo
   Kim, Younghui
   Lee, Jungjin
   Kim, Kyehyun
   Lee, Kyunghan
   Noh, Junyong
TI Depth manipulation using disparity histogram analysis for stereoscopic
   3D
SO VISUAL COMPUTER
LA English
DT Article
DE Stereoscopic 3D; Depth manipulation; Disparity retargeting; Depth
   editing
AB The importance of post-production for stereoscopic 3D is increasing rapidly. In particular, depth manipulation is essential, as there are many situations in which the captured depth requires further adjustment. Nonlinear disparity mapping has been a popular choice for efficient depth manipulation. However, most existing work requires users to have a deep understanding of how stereo works. This paper proposes a novel and very intuitive-to-use nonlinear disparity mapping technique. A commonly used multirigging technique inspired this work. Specifically, our method creates multiple depth layers using the Gaussian Mixture Model (GMM) and a histogram analysis. The depth position and volume are then manipulated with simple parameters at each layer individually, achieving complex nonlinearity in terms of depth control. The employed optimization scheme ensures the preservation of the original depth order. A user study shows that our method is very easy to use and simple to control. We demonstrate the versatility of our method with various practical applications.
C1 [Lee, Sangwoo; Kim, Kyehyun; Lee, Kyunghan; Noh, Junyong] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
   [Kim, Younghui] Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Visual Media Lab, Taejon 305701, South Korea.
   [Lee, Jungjin] Korea Adv Inst Sci & Technol, Grad Scholl Culture Technol, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Korea Advanced
   Institute of Science & Technology (KAIST); Korea Advanced Institute of
   Science & Technology (KAIST)
RP Noh, J (corresponding author), Korea Adv Inst Sci & Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
EM sangwoolee@kaist.ac.kr; junyongnoh@kaist.ac.kr
RI Noh, Junyong/C-1663-2011; Lee, Jungjin/HZL-2693-2023
OI Lee, Jungjin/0000-0003-3471-4848
FU MKE [10040959]
FX This work was supported by MKE (10040959, Development of Compositing
   Software Supporting 4K Images).
CR [Anonymous], IJCV
   [Anonymous], ACM T GRAPHICS
   Bleyer M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3081, DOI 10.1109/CVPR.2011.5995581
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Dejohn M., 2007, 2 SERIES TECHNICAL R
   Du S., 2013, CHANGING PERSPECTIVE
   Engle R., 2008, P SPIE, V6803
   Foundry T., 2011, NUKE COMMERCIAL COMP
   Gonzalez R.C., 2018, DIGITAL IMAGE PROCES, V4th
   Iyer Kiran Nanjunda, 2008, 2008 Second International Conference on Future Generation Communication and Networking Symposia (FGCNS), P31, DOI 10.1109/FGCNS.2008.46
   Kim, 2012, COMMUNICATION
   Kim Y, 2011, COMPUT GRAPH FORUM, V30, P2067, DOI 10.1111/j.1467-8659.2011.02061.x
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Lamberti F, 2006, IEEE T CONSUM ELECTR, V52, P966, DOI 10.1109/TCE.2006.1706495
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lipton L., 1982, FDN STEREOSCOPIC CIN
   Luo SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366201
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Neuman Robert, 2009, Proceedings of SPIE, V7237
   Niu YZ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366202
   Scharstein D., 2002, EVALUATION DENSE 2 F
   Smith BM, 2009, PROC CVPR IEEE, P485, DOI 10.1109/CVPRW.2009.5206793
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tong R.f, 2012, IEEE T VIS COMPUT GR
   WANG C., 2008, P SPIE, V6803
   Wang L., 2008, CVPR
   Wu ZQ, 2006, IEE P-VIS IMAGE SIGN, V153, P512, DOI 10.1049/ip-vis:20050159
   Yoon H., 2009, INT J ELECT ELECT EN
NR 28
TC 10
Z9 10
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2014
VL 30
IS 4
BP 455
EP 465
DI 10.1007/s00371-013-0868-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AD3SW
UT WOS:000333167500008
DA 2024-07-18
ER

PT J
AU Lu, X
   Wang, YN
   Xu, HX
   Zhou, XY
   Zhao, K
AF Lu, Xiao
   Wang, Yaonan
   Xu, Haixia
   Zhou, Xuanyu
   Zhao, Ke
TI A new method for camera stratified self-calibration under circular
   motion
SO VISUAL COMPUTER
LA English
DT Article
DE Stratified self-calibration; Circular motion; Plane at infinity; SIPSO
ID SEQUENCES
AB We consider the stratified self-calibration (affine and metric reconstruction) problem from images acquired with a camera with unchanging internal parameters undergoing circular motion. The general stratified method (modulus constraints) is known to fail with this motion. In this paper we give a novel constraint on the plane at infinity in projective reconstruction for circular motion, the constant inter-frame motion constraint on the plane at infinity between every two adjacent views and a fixed view of the motion sequences, by making use of the facts that in many commercial systems rotation angles are constant. An initial solution can be obtained by using the first three views of the sequence, and Stratified Iterative Particle Swarm Optimization (SIPSO) is proposed to get an accurate and robust solution when more views are at hand. Instead of using the traditional optimization algorithm as the last step to obtain an accurate solution, in this paper, the whole motion sequence information is exploited before computing the camera calibration matrix, this results in a more accurate and robust solution. Once the plane at infinity is identified, the calibration matrices of the camera and a metric reconstruction can be readily obtained. Experiments on both synthetic and real image sequence are given, showing the accuracy and robustness of the new algorithm.
C1 [Lu, Xiao; Wang, Yaonan; Zhou, Xuanyu] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Xu, Haixia] Xiangtan Univ, Coll Informat Engn, Xiangtan 411105, Peoples R China.
   [Zhou, Xuanyu] Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
C3 Hunan University; Xiangtan University; Wuhan University
RP Lu, X (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
EM xlu_hnu@163.com; yaonan@hnu.cn; xhxia2002@126.com;
   zhouxuanyu@whu.edu.cn; zhao_ke001@163.com
RI lu, xiao/AAS-2542-2020
FU National Natural Science Foundation of China [61175075]; National
   High-tech Research and Development Projects [2012AA112312]
FX This work is supported by the National Natural Science Foundation of
   China (Grant nos. 61175075) and National High-tech Research and
   Development Projects (Grant nos. 2012AA112312). The authors also thank
   the anonymous reviewers for providing us constructive suggestions.
CR Boyer E., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P109
   Cao XC, 2006, COMPUT VIS IMAGE UND, V102, P227, DOI 10.1016/j.cviu.2006.01.004
   FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321
   Fitzgibbon A. W., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P155
   Hartley R. I., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P510, DOI 10.1109/ICCV.1999.791264
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P133, DOI 10.1109/34.574792
   HEYDEN A, 1996, P INT C PATT REC
   Jiang G., 2001, IEEE T PATTERN ANAL, V25, P604
   Kahl F, 2000, J MATH IMAGING VIS, V13, P131, DOI 10.1023/A:1026524030731
   Kutulakos K.N., 1998, INT J COMPUT VISION, V38, P199
   Liu Y, 2000, INT C PATT RECOG, P865, DOI 10.1109/ICPR.2000.903680
   MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171
   NIEM W, 1994, P SOC PHOTO-OPT INS, V2182, P388, DOI 10.1117/12.171088
   Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285
   Schaffalitzky F., 2000, Proc. Indian Conf. on Computer Vision, P314
   Sturm P, 1997, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.1997.609467
   Sturm P., 1996, EUROPEAN C COMPUTER, P709, DOI DOI 10.1007/3-540-61123-1
   Sullivan S, 1998, IEEE T PATTERN ANAL, V20, P1091, DOI 10.1109/34.722621
   Szeliski R., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P625, DOI 10.1109/CVPR.1991.139764
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Zhang H, 2009, IEEE T PATTERN ANAL, V31, P5, DOI 10.1109/TPAMI.2008.56
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
NR 22
TC 1
Z9 1
U1 0
U2 13
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2013
VL 29
IS 11
BP 1107
EP 1119
DI 10.1007/s00371-012-0754-4
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 236PL
UT WOS:000325811300001
DA 2024-07-18
ER

PT J
AU Zhao, X
   Kaufman, A
AF Zhao, Xin
   Kaufman, Arie
TI Structure revealing techniques based on parallel coordinates plot
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Conference on the Computer Graphics International (CGI)
CY 2012
CL Bournemouth, ENGLAND
DE Parallel coordinates plot; Dimension sorting optimization; Visual
   representation
ID VISUALIZATION
AB Parallel coordinates plot (PCP) is an excellent tool for multivariate visualization and analysis, but it may fail to reveal inherent structures for complex and large datasets. Therefore, polyline clustering and coordinate sorting are inevitable for the accurate data exploration and analysis. In this paper, we propose a suite of novel clustering and dimension sorting techniques in PCP, to reveal and highlight hidden trend and correlation information of polylines. Spectrum theory is first introduced to specifically design clustering and sorting techniques for a clear view of clusters in PCP. We also provide an efficient correlation based sorting technique to optimize the ordering of coordinates to reveal correlated relations, and show how our view-range metrics, generated based on the aggregation constraints, can be used to make a clear view for easy data perception and analysis. Experimental results generated using our framework visually represent meaningful structures to guide the user, and improve the efficiency of the analysis, especially for the complex and noisy data.
C1 [Zhao, Xin; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Zhao, X (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM xinzhao@cs.stonybrook.edu; ari@cs.stonybrook.edu
CR [Anonymous], 2012, HOMEPAGE XMDVTOOL MU
   [Anonymous], 1988, ALGORITHMS CLUSTERIN
   Artero AO, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P81, DOI 10.1109/INFVIS.2004.68
   Bach FR, 2004, ADV NEUR IN, V16, P305
   Chung F., 1997, C BOARD MATH SCI, P88
   Dasgupta A, 2010, IEEE T VIS COMPUT GR, V16, P1017, DOI 10.1109/TVCG.2010.184
   DONATH WE, 1973, IBM J RES DEV, V17, P420, DOI 10.1147/rd.175.0420
   FIEDLER M, 1973, CZECH MATH J, V23, P298
   Friendly M, 2002, AM STAT, V56, P316, DOI 10.1198/000313002533
   Friendly M., 2002, COMPUT STAT DATA AN, V37, P47
   Fua YH, 2000, IEEE T VIS COMPUT GR, V6, P150, DOI 10.1109/2945.856996
   HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993
   Hauser H, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P127, DOI 10.1109/INFVIS.2002.1173157
   INSELBERG A, 1990, PROCEEDINGS OF THE FIRST IEEE CONFERENCE ON VISUALIZATION - VISUALIZATION 90, P361, DOI 10.1109/VISUAL.1990.146402
   Inselberg A., 2009, Parallel Coordinates. Visual Multidimensional Geometry and its Applications, DOI DOI 10.1007/978-0-387-68628-8
   Johansson J, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P125, DOI 10.1109/INFVIS.2005.1532138
   Johansson J., 2007, Proceedings of Eurographics/IEEE-VGTC Symposium on Visualization, P35
   Keim DA, 2000, IEEE T VIS COMPUT GR, V6, P59, DOI 10.1109/2945.841121
   McDonnell KT, 2008, COMPUT GRAPH FORUM, V27, P1031, DOI 10.1111/j.1467-8659.2008.01239.x
   Meila M, 2003, LECT NOTES ARTIF INT, V2777, P173, DOI 10.1007/978-3-540-45167-9_14
   NOVOTNY M, 2004, CENTR EUR SEM COMP G
   Novotny M, 2006, IEEE T VIS COMPUT GR, V12, P893, DOI 10.1109/TVCG.2006.170
   Peng W, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P89
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   WEGMAN E., 1997, COMPUTER SCI STAT, V28, P352
   Ying-Huey Fua, 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P43, DOI 10.1109/VISUAL.1999.809866
   Zhou H, 2008, COMPUT GRAPH FORUM, V27, P1047, DOI 10.1111/j.1467-8659.2008.01241.x
NR 27
TC 8
Z9 12
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2012
VL 28
IS 6-8
BP 541
EP 551
DI 10.1007/s00371-012-0713-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 947EW
UT WOS:000304411500003
DA 2024-07-18
ER

PT J
AU Lien, JM
   Camelli, F
   Wong, D
   Lu, YY
   McWhorter, B
AF Lien, Jyh-Ming
   Camelli, Fernando
   Wong, David
   Lu, Yanyan
   McWhorter, Benjamin
TI Creating building ground plans via robust <i>K</i>-way union A step
   toward large-scale simulation in urban environment
SO VISUAL COMPUTER
LA English
DT Article
DE Urban geometry objects; Geometry processing; Numerical robustness;
   Simulation
ID ALGORITHMS; GEOMETRY
AB Due to collaborative efforts and advances in data acquisition technology, a large volume of geometric models describing urban buildings has become available in public domain via "Digital Earth" software like ESRI ArcGlobe and Google Earth. As a consequence, almost every major international city has been reconstructed in the virtual world. Although mostly created for visualization, we believe that these urban models can benefit many applications beyond visualization including video games, city scale evacuation plans, traffic simulations, and earth phenomenon simulations. However, before these urban models can be used in these applications, they require tedious manual preparation that usually takes weeks, if not months. In this paper, we present a framework that produces disjoint 2D ground plans from these urban models, an important step in the preparation process. Designing algorithms that can robustly and efficiently handle unstructured urban models at city scale is the main technical challenge. In this work, we show both theoretically and empirically that our method is resolution complete, efficient, and numerically stable.
C1 [Lien, Jyh-Ming; Lu, Yanyan; McWhorter, Benjamin] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Camelli, Fernando] George Mason Univ, Computat Data Dept, Fairfax, VA 22030 USA.
   [Wong, David] George Mason Univ, Dept Geog & GeoInformat Sci, Fairfax, VA 22030 USA.
C3 George Mason University; George Mason University; George Mason
   University
RP Lien, JM (corresponding author), George Mason Univ, Dept Comp Sci, 4400 Univ Dr, Fairfax, VA 22030 USA.
EM jmlien@gmu.edu; fcamelli@gmu.edu; dwong2@gmu.edu; ylu4@gmu.edu;
   bmcwhort@gmu.edu
OI /0000-0002-0525-0071
FU NSF [IIS-096053]; College of Science at George Mason University
FX This work is supported in part by NSF IIS-096053 and College of Science
   Interdisciplinary Seed Grant, "Bringing Together Computational Fluid
   Dynamics Models and Geospatial Modeling" at George Mason University.
CR Agarwal PK, 2008, CONTEMP MATH, V453, P9
   [Anonymous], P ROB SCI SYST 4 ZUR
   [Anonymous], 2009, ISPRS WORKSH CIT MOD
   [Anonymous], P IEEE RSJ INT C INT
   BENTLEY JL, 1979, IEEE T COMPUT, V28, P643, DOI 10.1109/TC.1979.1675432
   Camelli F, 2006, 6 S URB ENV
   Chang R, 2008, IEEE COMPUT GRAPH, V28, P27, DOI 10.1109/MCG.2008.56
   Cheng L., 2008, ISPRS08, pB3b
   Cignoni P, 2007, COMPUT GRAPH FORUM, V26, P405, DOI 10.1111/j.1467-8659.2007.01063.x
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   ESRI E., 1998, SHAP TECHN DESCR
   Fabri A, 2000, SOFTWARE PRACT EXPER, V30, P1167, DOI 10.1002/1097-024X(200009)30:11<1167::AID-SPE337>3.0.CO;2-B
   FLATO E, 2000, THESIS TEL AVIV U
   Fousse L, 2007, ACM T MATH SOFTWARE, V33, DOI 10.1145/1236463.1236468
   Frueh C., 2003, P IEEE COMP SOC C CO, V2
   Hanna SR, 2006, B AM METEOROL SOC, V87, P1713, DOI 10.1175/BAMS-87-12-1713
   Haunert J.-H., 2008, PROC 21 ISPRS C, P372
   Haverkort Herman J, 2004, THESIS UTRECHT U
   HWANG YK, 1992, COMPUT SURV, V24, P219, DOI 10.1145/136035.136037
   KADA M., 2006, INT ARCH PHOTOGRAMME, V36
   Kolbe T. H., 2009, 3D GEOINFORMATION SC, P15, DOI [DOI 10.1007/978-3-540-87395-2_2, 10.1007/978-3-540-87395-2_2]
   Laidlaw D., 1986, ACM SIGGRAPH COMPUTE, V20, P170
   LaValle SM, 1997, IEEE INT CONF ROBOT, P731, DOI 10.1109/ROBOT.1997.620122
   Lee D, 2004, ICA WORKSH GEN MULT
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P579, DOI 10.1145/1141911.1141926
   LICHTNER W, 1979, GEO-PROCESSING, V1, P183
   Lien J.M., 2011, P COMP GRAPH INT CGI
   Löhner R, 2008, COMPUT METHOD APPL M, V197, P2173, DOI 10.1016/j.cma.2007.09.010
   Lu Y., 2011, SIAM C GEOM PHYS MOD
   Neidhart H, INT ARCH PHOTOGRAMME, V37, P405
   ORourke J, 1998, Computational Geometry in C
   Rainsford D, 2002, ADVANCES IN SPATIAL DATA HANDLING, P137
   RICCI A, 1973, COMPUT J, V16, P157, DOI 10.1093/comjnl/16.2.157
   RICHARDS FM, 1977, ANNU REV BIOPHYS BIO, V6, P151, DOI 10.1146/annurev.bb.06.060177.001055
   Rodriguez S, 2010, IEEE INT CONF ROBOT, P350, DOI 10.1109/ROBOT.2010.5509502
   ROSSIGNAC JR, 1991, COMPUT AIDED DESIGN, V23, P21, DOI 10.1016/0010-4485(91)90078-B
   Sester M., 2000, INT ARCH PHOTOGRA B4, V33, P931, DOI DOI 10.1080/136588100240930
   Teschner M, 2003, VISION, MODELING, AND VISUALIZATION 2003, P47
   Thiemann F., 2006, P GICON
   Wang P, 2004, ISPRS C
   Wein R, 2006, LECT NOTES COMPUT SC, V4168, P829
NR 42
TC 0
Z9 0
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2012
VL 28
IS 4
BP 401
EP 412
DI 10.1007/s00371-011-0645-0
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 926EE
UT WOS:000302813300006
DA 2024-07-18
ER

PT J
AU Livny, Y
   Bauman, G
   El-Sana, J
AF Livny, Yotam
   Bauman, Gilad
   El-Sana, Jihad
TI Displacement patches for view-dependent rendering
SO VISUAL COMPUTER
LA English
DT Article
DE GPU processing; Level-of-detail rendering; View-dependent rendering;
   Subdivision surfaces
ID INTERPOLATION; GEOMETRY; DETAIL
AB In this paper we present a new approach for interactive view-dependent rendering of large polygonal data sets which relies on advanced features of modern graphics hardware. Our preprocessing algorithm starts by generating a simplified representation of the input mesh. It then builds a multiresolution hierarchy for the simplified model. For each face in the hierarchy, it generates and assigns a displacement map that resembles the original surface represented by that face. At runtime, the multiresolution hierarchy is used to select a coarse view-dependent level-of-detail representation, which is sent to the graphics hardware. The GPU then refines the coarse representation by replacing each face with a planar tile, which is elevated according to the assigned displacement map. Our results show that our implementation achieves quality images at high frame rates.
C1 [Bauman, Gilad; El-Sana, Jihad] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.
C3 Ben Gurion University
RP El-Sana, J (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.
EM livnyy@cs.bgu.ac.il; baumang@cs.bgu.ac.il; el-sana@cs.bgu.ac.il
CR Asirvatham A., 2005, GPU GEMS 2, V2, P27
   Baboud L, 2006, PROC GRAPH INTERF, P195
   Bolz J., 2002, COMPUTATIONAL GEOMET
   Cignoni P, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P147, DOI 10.1109/VISUAL.2003.1250366
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   Cignoni P, 1999, VISUAL COMPUT, V15, P519, DOI 10.1007/s003710050197
   Cohen J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P115, DOI 10.1145/280814.280832
   Dachsbacher C., 2004, EUROGRAPHICS S GEOME, P138
   De Floriani L, 1998, VISUALIZATION '98, PROCEEDINGS, P43, DOI 10.1109/VISUAL.1998.745283
   DeCoro C, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P363, DOI 10.1109/VISUAL.2002.1183796
   Doggett Michael, 2000, P ACM SIGGRAPH EUROG, P59
   Donnelly W., 2005, GPU GEMS 2, V1st, P123
   El-Sana J, 1999, COMPUT GRAPH FORUM, V18, pC83, DOI 10.1111/1467-8659.00330
   El-Sana J, 2001, IEEE VISUAL, P371, DOI 10.1109/VISUAL.2001.964534
   El-Sana J, 2000, COMPUT GRAPH FORUM, V19, pC139, DOI 10.1111/1467-8659.00406
   ERIKSON C., 2001, I3D 01 P 2001 S INTE, P111, DOI [10.1145/364338.364376, DOI 10.1145/364338.364376]
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guskov I, 2000, COMP GRAPH, P95, DOI 10.1145/344779.344831
   Hirche J, 2004, PROC GRAPH INTERF, P153
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Hoppe H., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P189, DOI 10.1145/258734.258843
   Hwa LM, 2005, IEEE T VIS COMPUT GR, V11, P355, DOI 10.1109/TVCG.2005.65
   Ji JF, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P108
   Kautz Jan., 2001, GRIN'01, P61
   Kim J., 2001, Graphics Interface, P101
   Lee A, 2000, COMP GRAPH, P85, DOI 10.1145/344779.344829
   Livny Y, 2008, GRAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P181
   Livny Y, 2008, VISUAL COMPUT, V24, P139, DOI 10.1007/s00371-007-0180-1
   Losasso F., 2003, Symposium on Geometry Processing, P138
   Luebke D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P199, DOI 10.1145/258734.258847
   Mann S, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P35
   Mao ZH, 2005, LECT NOTES COMPUT SC, V3480, P776
   NIELSON GM, 1979, J APPROX THEORY, V25, P318, DOI 10.1016/0021-9045(79)90020-0
   Pajarola R, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P22, DOI 10.1109/PCCGA.2001.962854
   Policarpo F., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, P155, DOI DOI 10.1145/1053427.1053453
   Schein S, 2005, VISUAL COMPUT, V21, P791, DOI 10.1007/s00371-005-0338-7
   Schneider J, 2006, JOURNAL WSCG, V14, P49
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Southern R, 2003, COMPUT GRAPH FORUM, V22, P35, DOI 10.1111/1467-8659.t01-1-00644
   Wagner D., 2004, SHADERX2 SHADER PROG
   YOON S, 2003, P VIS 03
   Yoon SE, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P131, DOI 10.1109/VISUAL.2004.86
NR 43
TC 0
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2012
VL 28
IS 3
BP 247
EP 263
DI 10.1007/s00371-011-0609-4
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 896QS
UT WOS:000300585400002
DA 2024-07-18
ER

PT J
AU Ma, J
   Bae, SW
   Choi, S
AF Ma, Jaehwan
   Bae, Sang Won
   Choi, Sunghee
TI 3D medial axis point approximation using nearest neighbors and the
   normal field
SO VISUAL COMPUTER
LA English
DT Article
DE Medial axis; Nearest neighbor; GPU
ID COMPUTATION; SKELETONS
AB We present a novel method to approximate medial axis points given a set of points sampled from a surface and the normal vectors to the surface at those points. For each sample point, we find its maximal tangent ball containing no other sample points, by iteratively reducing its radius using nearest neighbor queries. We prove that the center of the ball constructed by our algorithm converges to a true medial axis point as the sampling density increases to infinity. We also propose a simple heuristic to handle noisy samples. By simple extensions, our method is applied to medial axis point simplification, local feature size estimation and feature-sensitive point decimation. Our algorithm is simple, easy to implement, and suitable for parallel computation using GPU because the iteration process for each sample point runs independently. Experimental results show that our method is efficient both in time and in space.
C1 [Ma, Jaehwan; Choi, Sunghee] Korea Adv Inst Sci & Technol, Geometr Comp Lab GCLAB, Taejon 305701, South Korea.
   [Bae, Sang Won] Kyouggi Univ, Suwon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Choi, S (corresponding author), Korea Adv Inst Sci & Technol, Geometr Comp Lab GCLAB, Taejon 305701, South Korea.
EM kaede@jupiter.kaist.ac.kr; swbae@kgu.ac.kr; sunghee@jupiter.kaist.ac.kr
RI Choi, Sunghee/C-1617-2011; Bae, Sang Won/R-8161-2019
OI Bae, Sang Won/0000-0002-8802-4247
FU National Research Foundation of Korea (NRF); Korea government (MEST)
   [2010-0024092]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MEST) (No. 2010-0024092).
CR Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Amenta N, 2001, COMP GEOM-THEOR APPL, V19, P127, DOI 10.1016/S0925-7721(01)00017-7
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   Attali D, 1997, COMPUT VIS IMAGE UND, V67, P261, DOI 10.1006/cviu.1997.0536
   Attali D., 2007, MATH FDN SCI VISUALI
   BENTLEY JL, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P187, DOI 10.1145/98524.98564
   Blum H., 1967, MODELS PERCEPTION SP, P362, DOI DOI 10.1142/S0218654308001154
   Chandramouli M., 2008, INTEGRATION GA BASED, P1, DOI [DOI 10.1201/9781420080599.CH1, 10,1201/9781420080599.chl]
   Choi HI, 1997, PAC J MATH, V181, P57, DOI 10.2140/pjm.1997.181.57
   Choi S, 2002, SIAM PROC S, P135
   CHOSET H, 1997, 1 CGC WORKSH COMP GE
   Culver T, 2004, COMPUT AIDED GEOM D, V21, P65, DOI 10.1016/j.cagd.2003.07.008
   Dey TK, 2004, ALGORITHMICA, V38, P179, DOI 10.1007/s00453-003-1049-y
   DEY TK, 2001, P 13 CAN C COMP GEOM, P85
   DEY TK, 2002, SMA 02, P356
   ETZION M, 1999, COMPUTING VORONOI SK
   ETZION M, 1999, SOL MOD 99
   Foskey M., 2003, J. Comput. Inf. Sci. Eng., V3, P274, DOI DOI 10.1145/781606.781623
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   GUIBAS L, 1999, IEEE RSJ P INT C INT
   GURSOY HN, 1991, ADV ENG SOFTW WORKST, V13, P287, DOI 10.1016/0961-3552(91)90033-Z
   Hoff KE, 1999, COMP GRAPH, P277, DOI 10.1145/311535.311567
   HOLLEMAN C, 1997, P INT C ROB AUT, P1408
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Hou QM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360618
   LAM L, 1992, IEEE T PATTERN ANAL, V14, P869, DOI 10.1109/34.161346
   Leymarie FF, 2007, IEEE T PATTERN ANAL, V29, P313, DOI 10.1109/TPAMI.2007.44
   Ma CM, 1996, COMPUT VIS IMAGE UND, V64, P420, DOI 10.1006/cviu.1996.0069
   Ming-Ching Chang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1473, DOI 10.1109/ICCVW.2009.5457437
   NIBLACK CW, 1992, CVGIP-GRAPH MODEL IM, V54, P420, DOI 10.1016/1049-9652(92)90026-T
   OGNIEWICZ RL, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P746, DOI 10.1109/CVPR.1994.323891
   Pauly M, 2003, COMPUT GRAPH FORUM, V22, P281, DOI 10.1111/1467-8659.00675
   REDDY JM, 1995, COMPUT AIDED DESIGN, V27, P677, DOI 10.1016/0010-4485(94)00025-9
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Sheehy DJ, 1996, IEEE T VIS COMPUT GR, V2, P62, DOI 10.1109/2945.489387
   Sheffer A, 1999, ENG COMPUT-GERMANY, V15, P248, DOI 10.1007/s003660050020
   Sherbrooke EC, 1996, GRAPH MODEL IM PROC, V58, P574, DOI 10.1006/gmip.1996.0047
   SHERBROOKE EC, 1995, SMA 95, P187
   Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8
   VIDAL SF, 2000, P INT C PATT REC, V1, P1712
   Vleugels J, 1998, INT J COMPUT GEOM AP, V8, P201, DOI 10.1142/S0218195998000114
   WILMARTH S, 1999, ACM S COMP GEOM, P173
   WOLTER F, 1993, 922 MIT DES LAB US N
   Wolter FE, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P137, DOI 10.1109/CGI.2000.852329
   Yang Yuandong., 2004, SM '04: Proceedings of the ninth ACM symposium on Solid modeling and applications, P15
NR 45
TC 53
Z9 57
U1 0
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2012
VL 28
IS 1
BP 7
EP 19
DI 10.1007/s00371-011-0594-7
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 874YQ
UT WOS:000298995000002
DA 2024-07-18
ER

PT J
AU Rustamov, RM
AF Rustamov, Raif M.
TI Interpolated eigenfunctions for volumetric shape processing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd Eurographics Workshop on 3D Object Retrieval
CY MAY 02, 2010
CL Norrkoping, SWEDEN
DE Volumetric shape processing; Laplace-Beltrami eigenfunctions;
   Barycentric interpolation; Heat kernel signature
ID SIMPLIFICATION; TOOL
AB This paper introduces a set of volumetric functions suitable for geometric processing of volumes. We start with Laplace-Beltrami eigenfunctions on the bounding surface and interpolate them into the interior using barycentric coordinates. The interpolated eigenfunctions: (1) can be computed efficiently by using the boundary mesh only; (2) can be seen as a shape-aware generalization of barycentric coordinates; (3) can be used for efficiently representing volumetric functions; (4) can be naturally plugged into existing spectral embedding constructions such as the diffusion embedding to provide their volumetric counterparts. Using the interior diffusion embedding, we define the interior Heat Kernel Signature (iHKS) and examine its performance for the task of volumetric point correspondence. We show that the three main qualities of the surface Heat Kernel Signature-being informative, multiscale, and insensitive to pose-are inherited by this volumetric construction. Next, we construct a bag of features based shape descriptor that aggregates the iHKS signatures over the volume of a shape, and evaluate its performance on a public shape retrieval benchmark. We find that while, theoretically, strict isometry invariance requires concentrating on the intrinsic surface properties alone, yet, practically, pose insensitive shape retrieval can be achieved using volumetric information.
C1 Drew Univ, Madison, NJ 07940 USA.
C3 Drew University
RP Rustamov, RM (corresponding author), Drew Univ, 36 Madison Ave, Madison, NJ 07940 USA.
EM rrustamov@drew.edu
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2008, EUR S GEOM PROC SGP
   [Anonymous], binvox
   Belyaev A., 2006, SGP, P89, DOI 10.2312/SGP/SGP06/089-099
   Ben-Chen M, 2005, ACM T GRAPHIC, V24, P60, DOI 10.1145/1037957.1037961
   Berger M., 2003, PANORAMIC VIEW RIEMA
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P31, DOI 10.1016/j.acha.2005.07.005
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Garland M, 2005, ACM T GRAPHIC, V24, P209, DOI 10.1145/1061347.1061350
   Iyer N, 2005, COMPUT AIDED DESIGN, V37, P509, DOI 10.1016/j.cad.2004.07.002
   JOSHI P, 2007, TOG SIGGRAPH, P71
   JU T, 2005, TOG SIGGRAPH, P561
   LEVY B, 2006, SHAPE MODELING INT
   LIAN Z, SHREC 11 TRACK SHAPE, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971
   Liu YS, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-157
   Mémoli F, 2011, APPL COMPUT HARMON A, V30, P363, DOI 10.1016/j.acha.2010.09.005
   MEYER M, 2002, P VIS MATH
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Raviv D., 2010, ACM WORKSHOP 3D OBJE, P39
   REUTER M, 2005, SPM 05, P101
   Reuter M, 2009, COMPUT AIDED DESIGN, V41, P739, DOI 10.1016/j.cad.2009.02.007
   RUSTAMOV R, 2009, COMPUT GRAPH FORUM S, V28
   RUSTAMOV R, 2008, MANIFOLD LEARNING ME
   Rustamov R. M., 2007, S GEOM PROC
   Shen Y, 2010, VISUAL COMPUT, V26, P1229, DOI 10.1007/s00371-009-0404-7
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   YU Y, 2004, TOG SIGGRAPH, P644
NR 35
TC 12
Z9 12
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2011
VL 27
IS 11
SI SI
BP 951
EP 961
DI 10.1007/s00371-011-0629-0
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 831MW
UT WOS:000295736400002
DA 2024-07-18
ER

PT J
AU Kasap, M
   Magnenat-Thalmann, N
AF Kasap, Mustafa
   Magnenat-Thalmann, Nadia
TI Skeleton-aware size variations in digital mannequins
SO VISUAL COMPUTER
LA English
DT Article
DE Anthropometry; Deformation; Human body; Skeleton
AB The general trend in character modeling is toward the personalization of models with higher levels of visual realism. This becomes possible with the development of commodity computation resources that are capable of processing massive data in parallel across multiple processors. On the other hand, there is always a trade-off between the quantity of the model features that are simulated and the plausibility of the visual realism because of the limited computation resources. Also, to keep the resources' to be used efficiently within the other modeling approaches such as skin reflectance, aging, animation, etc., one must consider the efficiency of the method being used in the simulation. In this paper, we present an efficient method to customize the size of a human body model to personalize it with industry standard parameters. One of the major contributions of this method is that it is possible to generate a range of different size body models by using anthropometry surveys. This process is not limited by data-driven mesh deformation but also adapts the skeleton and motion to keep the consistency between different body layers.
C1 [Kasap, Mustafa; Magnenat-Thalmann, Nadia] Univ Geneva, MIRALab, Geneva, Switzerland.
   [Magnenat-Thalmann, Nadia] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
C3 University of Geneva; Nanyang Technological University
RP Kasap, M (corresponding author), Univ Geneva, MIRALab, Geneva, Switzerland.
EM mustafa.kasap@miralab.ch; thalmann@miralab.ch
RI Thalmann, Nadia/AAK-5195-2021
OI Thalmann, Nadia/0000-0002-1459-5960
FU EU [247688]; Swiss National Research Foundation
FX This work is supported by the EU project 3DLife (Grant No. 247688) and
   AerialCrowds project funded by Swiss National Research Foundation.
CR Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Barr A. H., 1981, IEEE Computer Graphics and Applications, V1, P11, DOI 10.1109/MCG.1981.1673799
   BORREL P, 1994, ACM T GRAPHIC, V13, P137, DOI 10.1145/176579.176581
   CHADWICK JE, 1989, SIGGRAPH 89, P243
   CHOI KJ, 1999, P INT PAC GRAPH, P32
   GORDON CC, 1989, NATICKTR89027 US ARM
   GROSSO M, 1987, ANTHROPOMETRY COMPUT
   JIANHUA S, 1994, COMPUTER GRAPHICS IN, P9
   Kasap M, 2009, VRST 09 P 16 ACM S V, P123, DOI [10.1145/1643928.1643956, DOI 10.1145/1643928.1643956]
   Kasap M, 2007, 2007 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P160, DOI 10.1109/CW.2007.14
   Magnenat-Thalmann Nadia, 1988, P GRAPHICS INTERFACE
   Maïm J, 2009, IEEE COMPUT GRAPH, V29, P82, DOI 10.1109/MCG.2009.129
   SCHEEPERS F, 1997, SIGGRAPH 97, P163
   Seo H, 2004, GRAPH MODELS, V66, P1, DOI 10.1016/j.gmod.2003.07.004
NR 15
TC 13
Z9 14
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2011
VL 27
IS 4
SI SI
BP 263
EP 274
DI 10.1007/s00371-011-0547-1
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 739IC
UT WOS:000288707700003
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hulusic, V
   Debattista, K
   Aggarwal, V
   Chalmers, A
AF Hulusic, Vedad
   Debattista, Kurt
   Aggarwal, Vibhor
   Chalmers, Alan
TI Maintaining frame rate perception in interactive environments by
   exploiting audio-visual cross-modal interaction
SO VISUAL COMPUTER
LA English
DT Article
DE Cross-modal; Perception; High-fidelity rendering
ID TEMPORAL VENTRILOQUISM; VIRTUAL ENVIRONMENTS; DIVIDED ATTENTION;
   AUDITORY FLUTTER; VISUAL FLICKER; HEARING; VISION; MOTION; SOUND
AB The entertainment industry, primarily the video games industry, continues to dictate the development and performance requirements of graphics hardware and computer graphics algorithms. However, despite the enormous progress in the last few years, it is still not possible to achieve some of industry's demands, in particular high-fidelity rendering of complex scenes in real-time, on a single desktop machine. A realisation that sound/music and other senses are important to entertainment led to an investigation of alternative methods, such as cross-modal interaction in order to try and achieve the goal of "realism in real-time". In this paper we investigate the cross-modal interaction between vision and audition for reducing the amount of computation required to compute visuals by introducing movement related sound effects. Additionally, we look at the effect of camera movement speed on temporal visual perception. Our results indicate that slow animations are perceived as smoother than fast animations. Furthermore, introducing the sound effect of footsteps to walking animations further increased the animation smoothness perception. This has the consequence that for certain conditions, the number of frames that need to be rendered each second can be reduced, saving valuable computation time, without the viewer being aware of this reduction. The results presented are another step towards the full understanding of the auditory-visual cross-modal interaction and its importance for helping achieve "realism int real-time".
C1 [Hulusic, Vedad; Debattista, Kurt; Aggarwal, Vibhor; Chalmers, Alan] Univ Warwick, WMG, Int Digital Lab, Coventry CV4 7AL, W Midlands, England.
C3 University of Warwick
RP Hulusic, V (corresponding author), Univ Warwick, WMG, Int Digital Lab, Coventry CV4 7AL, W Midlands, England.
EM v.hulusic@warwick.ac.uk
OI Hulusic, Vedad/0000-0002-3675-095X
CR Alais D, 2006, P R SOC B, V273, P1339, DOI 10.1098/rspb.2005.3420
   ALLPORT DA, 1972, Q J EXP PSYCHOL, V24, P225, DOI 10.1080/00335557243000102
   [Anonymous], 2007, COMPUT ENTERTAINMENT
   Arif M., 2004, MEAS SCI REV, V4, P29
   Bonneel N., 2009, ACM T APPL PERCEPT
   Bonneel N, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658350
   Bonnel AM, 1998, PERCEPT PSYCHOPHYS, V60, P179, DOI 10.3758/BF03206027
   Burr D, 2006, PROG BRAIN RES, V155, P243, DOI 10.1016/S0079-6123(06)55014-9
   Cater K., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P270
   CHOE CS, 1975, PERCEPT PSYCHOPHYS, V18, P55, DOI 10.3758/BF03199367
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Duncan J, 1997, NATURE, V387, P808, DOI 10.1038/42947
   FUNKHOUSER TA, 1993, SIGGRAPH 93, P247
   GEBHARD JW, 1959, AM J PSYCHOL, V72, P521, DOI 10.2307/1419493
   Getzmann S, 2007, PERCEPTION, V36, P1089, DOI 10.1068/p5741
   Grelaud D., 2009, P ACM SIGGRAPH S INT
   Howard I.P., 1966, Human Spatial Orientation
   Hulusic V, 2008, WSCG 2008, FULL PAPERS, P41
   Hulusic V., 2009, SPRING C COMP GRAPH, P167
   Hulusic V., 2010, P IEEE C GAM VIRT WO
   Humphreys GlynW., 1989, Visual Cognition: Computational, Experimental, and Neuropsychological Perspectives
   James W., 1890, The Principles of Psychology, V1
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kamitani Y, 2001, J VIS, V1, P478, DOI DOI 10.1167/1.3.478
   Kayser C, 2005, CURR BIOL, V15, P1943, DOI 10.1016/j.cub.2005.09.040
   Keetels M, 2008, PERCEPT PSYCHOPHYS, V70, P765, DOI 10.3758/PP.70.5.765
   Kelly M.C., 2002, P AES 112 CONV MUN G
   Larsen A, 2003, PERCEPT PSYCHOPHYS, V65, P568, DOI 10.3758/BF03194583
   Lécuyer A, 2006, P IEEE VIRT REAL ANN, P11, DOI 10.1109/VR.2006.31
   Mack Arien, 1998, Inattentional Blindness
   MASSARO DW, 1977, PERCEPT PSYCHOPHYS, V21, P569, DOI 10.3758/BF03198739
   Mastoropoulou G, 2004, THEORY AND PRACTICE OF COMPUTER GRAPHICS 2004, PROCEEDINGS, P128, DOI 10.1109/TPCG.2004.1314462
   Mastoropoulou G., 2006, EFFECT AUDIO VISUAL
   MASTOROPOULOU G, 2005, APGV 05, P9, DOI DOI 10.1145/1080402.1080404
   MASTOROPOULOU G, 2005, GRAPHITE 05, P363, DOI DOI 10.1145/1101389.1101462
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Moeck T, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P189
   Moore BrianC. J., 1982, An Introduction to the Psychology of Hearing
   Morein-Zamir S, 2003, COGNITIVE BRAIN RES, V17, P154, DOI 10.1016/S0926-6410(03)00089-2
   Ramic-Brkic B., 2009, SPRING C COMP GRAPH, P175
   Recanzone GH, 2003, J NEUROPHYSIOL, V89, P1078, DOI 10.1152/jn.00706.2002
   Sekuler R, 1997, NATURE, V385, P308, DOI 10.1038/385308a0
   Shams L, 2000, NATURE, V408, P788, DOI 10.1038/35048669
   Shams L, 2002, COGNITIVE BRAIN RES, V14, P147, DOI 10.1016/S0926-6410(02)00069-1
   Shams L., 2004, The Handbook of Multisensory Processes, P27
   SHIPLEY T, 1964, SCIENCE, V145, P1328, DOI 10.1126/science.145.3638.1328
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Suied C., 2009, EXP BRAIN RES
   Tsingos N, 2004, ACM T GRAPHIC, V23, P249, DOI 10.1145/1015706.1015710
   Vroomen J., 1998, VISUAL INFLUENCE DIS
   Vroomen J., 2004, HDB MULTISENSORY PRO, P140
   Wada Y, 2003, INT J PSYCHOPHYSIOL, V50, P117, DOI 10.1016/S0167-8760(03)00128-4
   WELCH RB, 1980, PSYCHOL BULL, V88, P638, DOI 10.1037/0033-2909.88.3.638
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 54
TC 14
Z9 16
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2011
VL 27
IS 1
BP 57
EP 66
DI 10.1007/s00371-010-0514-2
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 700YI
UT WOS:000285781100005
DA 2024-07-18
ER

PT J
AU Martínez, F
   Rueda, AJ
   Feito, FR
AF Martinez, Francisco
   Rueda, Antonio J.
   Feito, Francisco R.
TI The multi-LREP decomposition of solids and its application to a
   point-in-polyhedron inclusion test
SO VISUAL COMPUTER
LA English
DT Article
DE Spatial data structures; Solid modeling; Geometric algorithms
ID REPRESENTATION
AB This paper presents a scheme for decomposing polyhedra called multi-LREP. The scheme is based on the L-REP decomposition, which classifies the triangular faces of a polyhedron into a set of layered tetrahedra. In the multi-LREP these layered tetrahedra are grouped into regions of a space subdivision. The paper also describes an efficient method for constructing the L-REP decomposition and how the multi-LREP can be applied to speed up two L-REP applications: the point-in-polyhedron inclusion test and the ray-scene intersection. An experimental comparison with other point-in-polyhedron tests is presented as well.
C1 [Martinez, Francisco; Rueda, Antonio J.; Feito, Francisco R.] Univ Jaen, Dept Informat, Jaen 23071, Spain.
C3 Universidad de Jaen
RP Martínez, F (corresponding author), Univ Jaen, Dept Informat, Campus Las Lagunillas S-N, Jaen 23071, Spain.
EM fmartin@ujaen.es; ajrueda@ujaen.es; ffeito@ujaen.es
RI Rueda-Ruiz, Antonio Jesús/AAY-5298-2021; Martínez-del-Río,
   Francisco/I-2855-2015; Feito, Francisco/M-1672-2014
OI Rueda-Ruiz, Antonio Jesús/0000-0001-7692-454X; Martínez-del-Río,
   Francisco/0000-0002-5206-1898; Feito, Francisco/0000-0001-8230-6529
FU Ministerio de Ciencia y Tecnologia of Spain; European Union
   [TIN2007-67474-CO3-03, P06-TIC-01403, P07-TIC-02773]; Consejeria de
   Innovacion, Ciencia y Empresa of the Junta de Andalucia
FX This work has been partially granted by the Ministerio de Ciencia y
   Tecnologia of Spain and the European Union by means of the ERDF funds,
   under the research project TIN2007-67474-CO3-03, and by the Consejeria
   de Innovacion, Ciencia y Empresa of the Junta de Andalucia and the
   European Union by means of the ERDF funds, under the research projects
   P06-TIC-01403 and P07-TIC-02773.
CR AKENINE M, 2002, REAL TIME RENDERING, P835
   [Anonymous], J GRAPHICS TOOLS
   [Anonymous], 2000, COMPUT GEOM ALGORITH
   Feito FR, 1997, COMPUT GRAPH, V21, P23, DOI 10.1016/S0097-8493(96)00067-2
   GLASSNER AS, 1984, IEEE COMPUT GRAPH, V4, P15, DOI 10.1109/MCG.1984.6429331
   Haines E., 1994, GRAPHICS GEMS, P24, DOI [10.1016/B978-0-12-336156-1.50013-6, DOI 10.1016/B978-0-12-336156-1.50013-6]
   Martínez F, 2006, COMPUT GRAPH-UK, V30, P947, DOI 10.1016/j.cag.2006.08.015
   Ogayar CJ, 2005, COMPUT GRAPH-UK, V29, P616, DOI 10.1016/j.cag.2005.05.012
   Rueda AJ, 2005, VISUAL COMPUT, V21, P406, DOI 10.1007/s00371-005-0302-6
   Rueda AJ, 2002, COMPUT GRAPH-UK, V26, P805, DOI 10.1016/S0097-8493(02)00135-8
   Samet H., 2006, FDN MULTIDIMENSIONAL, P993
   Wang W, 2008, IEEE T VIS COMPUT GR, V14, P73, DOI 10.1109/TVCG.2007.70407
NR 12
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2010
VL 26
IS 11
BP 1361
EP 1368
DI 10.1007/s00371-009-0413-6
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 653UV
UT WOS:000282123700003
DA 2024-07-18
ER

PT J
AU Toldo, R
   Castellani, U
   Fusiello, A
AF Toldo, Roberto
   Castellani, Umberto
   Fusiello, Andrea
TI The <i>bag of words</i> approach for retrieval and categorization of 3D
   objects
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 2nd Eurographics Workshop on 3D Object Retrieval
CY MAR 29, 2009
CL Munich, GERMANY
DE 3D object retrieval; 3D object categorization; 3D segmentation
AB In this paper, we propose a novel framework for 3D object retrieval and categorization. The object is modeled in terms of its subparts as an histogram of 3D visual word occurrences. We introduce an effective method for hierarchical 3D object segmentation driven by the minima rule that combines spectral clustering-for the selection of seed-regions-with region growing based on fast marching. Descriptors attached to the regions allow the definition of the visual words. After coding of each object according to the Bag-of-Words paradigm, retrieval can be performed by matching with a suitable kernel, or categorization by learning a Support Vector Machine. Several examples on the Aim@Shape watertight dataset and on the Tosca dataset demonstrate the versatility of the proposed method in working with either 3D objects with articulated shape changes or partially occluded or compound objects. Results are encouraging as shown by the comparison with other methods for each of the analyzed scenarios.
C1 [Toldo, Roberto; Castellani, Umberto; Fusiello, Andrea] Univ Verona, Dipartimento Informat, I-37134 Verona, Italy.
C3 University of Verona
RP Castellani, U (corresponding author), Univ Verona, Dipartimento Informat, Str Grazie 15, I-37134 Verona, Italy.
EM roberto.toldo@univr.it; umberto.castellani@univr.it;
   andrea.fusiello@univr.it
RI Fusiello, Andrea/GOJ-9893-2022; Castellani, Umberto/H-5101-2013
OI Fusiello, Andrea/0000-0003-2963-0316; Castellani,
   Umberto/0000-0002-6099-5682
CR [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], 1983, INTRO MODERN INFORM
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Belongie S, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P20, DOI 10.1109/IVL.2000.853834
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   CORNEA ND, 2005, IEEE INT C SHAP MOD
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Duda R. O., 2000, PATTERN CLASSIFICATI
   FERREIRA A, 2008, INT J COMPUT VISION, P1573
   Funkhouser T, 2005, COMMUN ACM, V48, P58, DOI 10.1145/1064830.1064859
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Grauman K, 2007, J MACH LEARN RES, V8, P725
   HOFFMAN DD, 1987, COGNITION, P65
   Iyer N, 2005, COMPUT AIDED DESIGN, V37, P509, DOI 10.1016/j.cad.2004.07.002
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   LI Y, 2006, INT C COMP VIS PATT
   LIN X, 2008, ISVC 08, V5358, P349
   OHBUCHI R, 2008, INT C SHAP MOD APPL
   Ovsjanikov M., 2009, P NORDIA
   Petitjean S, 2002, ACM COMPUT SURV, V34, P211, DOI 10.1145/508352.508354
   SHAMIR A, 2008, COMPUT GRAPH FORUM
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SHILANE P, 2006, INT C SHAP MOD APPL
   Tam GKL, 2007, IEEE T VIS COMPUT GR, V13, P470, DOI 10.1109/TVCG.2007.1011
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Tung T, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P157, DOI 10.1109/SMI.2004.1314503
   Veltkamp RC, 2007, UUCS2007015 DEP INF
NR 31
TC 44
Z9 47
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2010
VL 26
IS 10
BP 1257
EP 1268
DI 10.1007/s00371-010-0519-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 649SI
UT WOS:000281793200003
DA 2024-07-18
ER

PT J
AU Egges, A
   van Basten, B
AF Egges, Arjan
   van Basten, Ben
TI One step at a time: animating virtual characters based on foot placement
SO VISUAL COMPUTER
LA English
DT Article
DE Motion synthesis; Speed warping; Locomotion; Footprint; Motion editing
AB Especially in a constrained virtual environment, precise control of foot placement during character locomotion is crucial to avoid collisions and to ensure a natural locomotion. In this paper, we present an extension of the step space: a novel technique for generating animations of a character walking over a set of desired foot steps in real time. We use an efficient greedy nearest-neighbor approach and warp the resulting animation so that it adheres to both spatial and temporal constraints. We will show that our technique can generate realistic locomotion animations over an input path very efficiently even though we impose many constraints on the animation. We also present a simple footstep planning technique that automatically plans regular stepping and sidestepping based on an input path with clearance information generated by a path planner.
C1 [Egges, Arjan; van Basten, Ben] Univ Utrecht, Games & Virtual Worlds Grp, Utrecht, Netherlands.
C3 Utrecht University
RP Egges, A (corresponding author), Univ Utrecht, Games & Virtual Worlds Grp, Utrecht, Netherlands.
EM egges@cs.uu.nl; basten@cs.uu.nl
FU Netherlands Organization for Scientific Research (NWO); Netherlands ICT
   Research and Innovation Authority (ICT Regie)
FX This research has been supported by the GATE project, funded by the
   Netherlands Organization for Scientific Research (NWO) and the
   Netherlands ICT Research and Innovation Authority (ICT Regie).
CR Autodesk, 3D STUD MAX
   Boulic R., 1990, Visual Computer, V6, P344, DOI 10.1007/BF01901021
   BRUDERLIN A, 1989, P ACM SIGGRAPH, P233
   Choi MG, 2003, ACM T GRAPHIC, V22, P182, DOI 10.1145/636886.636889
   Chung SK, 1999, COMP ANIM CONF PROC, P4, DOI 10.1109/CA.1999.781194
   COROS S, 2008, ACM SIGGRAPH ASIA 20, P113
   Geraerts R, 2007, COMPUT ANIMAT VIRT W, V18, P107, DOI 10.1002/cav.166
   Heck R, 2007, I3D 2007: ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P129
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar Lucas., 2002, SCA 2002: Proceedings of the 2002 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, P97
   Multon F, 1999, J VISUAL COMP ANIMAT, V10, P39, DOI 10.1002/(SICI)1099-1778(199901/03)10:1<39::AID-VIS195>3.0.CO;2-2
   Oshita M, 2008, COMPUT GRAPH FORUM, V27, P1909, DOI 10.1111/j.1467-8659.2008.01339.x
   Patla AE, 2003, IEEE ENG MED BIOL, V22, P48, DOI 10.1109/MEMB.2003.1195695
   Torkos N, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P151
   Treuille A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239458
   van Basten Ben JH, 2009, P 4 INT C FDN DIGITA, P199
   VANBASTEN BJH, 2010, 23 ANN C COMP AN SOC
   vandePanne M, 1997, COMPUT GRAPH FORUM, V16, P211, DOI 10.1111/1467-8659.00181
   WU CC, 2008, P 4 INT S ADV VIS CO, P106
NR 19
TC 5
Z9 5
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 497
EP 503
DI 10.1007/s00371-010-0443-0
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800011
DA 2024-07-18
ER

PT J
AU Jang, T
   Kim, H
   Bae, J
   Seo, J
   Noh, J
AF Jang, Taekwon
   Kim, Heeyoung
   Bae, Jinhyuk
   Seo, Jaewoo
   Noh, Junyong
TI Multilevel vorticity confinement for water turbulence simulation
SO VISUAL COMPUTER
LA English
DT Article
DE Physically based simulation; Water turbulence; Natural phenomena;
   Vorticity
ID SCALE VELOCITY SIMULATION
AB Physically based fluid simulation can provide realism, but simulating water turbulence remains challenging. Recently, there have been much work on gas turbulence, but these algorithms mostly rely on the Kolmogorov theory which is not directly applicable to water turbulence simulation. This paper presents a novel technique for simulating water turbulence. We show that sub-grid turbulence can be created by employing a flow-scale separation technique. We adopted the multi-scale flow separation method to derive a special small-scale equation. Small-scale velocities are then generated and manipulated by the equation. To simulate the turbulence effect, this work employed the vorticity confinement method. By extending the original method to multi-level, we effectively simulate energy cascading effects.
C1 [Jang, Taekwon; Kim, Heeyoung; Bae, Jinhyuk; Seo, Jaewoo; Noh, Junyong] Korea Adv Inst Sci & Technol, Visual Media Lab, Grad Sch Culture Technol, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Jang, T (corresponding author), Korea Adv Inst Sci & Technol, Visual Media Lab, Grad Sch Culture Technol, Taejon 305701, South Korea.
EM taekwon@kaist.ac.kr
RI Noh, Junyong/C-1663-2011
CR [Anonymous], 2008, P 2008 ACM SIGGRAPHE
   [Anonymous], 2008, STRUCTURALLY ORDERED
   Cook RL, 2005, ACM T GRAPHIC, V24, P803, DOI 10.1145/1073204.1073264
   Davies PJ, 2004, PLANT HORMONES: BIOSYNTHESIS, SIGNAL TRANSDUCTION, ACTION, P1
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Kemenov KA, 2007, J COMPUT PHYS, V222, P673, DOI 10.1016/j.jcp.2006.08.002
   Kemenov KA, 2006, J COMPUT PHYS, V220, P290, DOI 10.1016/j.jcp.2006.05.006
   Kim B, 2007, IEEE T VIS COMPUT GR, V13, P135, DOI 10.1109/TVCG.2007.3
   KIM D, 2009, SIGGRAPH ASIA 09 ACM, P1, DOI DOI 10.1145/1661412.1618466
   Kim D, 2008, COMPUT GRAPH FORUM, V27, P467, DOI 10.1111/j.1467-8659.2008.01144.x
   Losasso F, 2004, ACM T GRAPHIC, V23, P457, DOI 10.1145/1015706.1015745
   LYNN F, 2007, 18 AIAA COMP FLUID D, P1
   MULLEN P, 2009, SIGGRAPH 09 ACM SIGG, P1, DOI DOI 10.1145/1576246.1531344
   Narain R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409119
   Nielsen MB, 2006, J SCI COMPUT, V26, P261, DOI 10.1007/s10915-005-9062-8
   Park S. I., 2005, Computer Animation, Conference Proceedings, P261, DOI [DOI 10.1145/1073368.1073406, 10.1145/1073368.1073406]
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Perlin K., 2001, Siggraph Technical Sketches and Applications, P187
   Pfaff T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618467
   SCHECHTER H, 2008, SCA 08, P1
   SELLE A, 2005, SIGGRAPH 05 ACM SIGG, P910, DOI DOI 10.1145/1186822.1073282
   Selle A, 2008, J SCI COMPUT, V35, P350, DOI 10.1007/s10915-007-9166-4
   Song OY, 2007, IEEE T VIS COMPUT GR, V13, P711, DOI 10.1109/TVCG.2007.1022
   Song OY, 2005, ACM T GRAPHIC, V24, P81, DOI 10.1145/1037957.1037962
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   STAM J, 1993, SIGGRAPH 93, P369, DOI DOI 10.1145/166117.166163
   STEINHOFF J, 1994, PHYS FLUIDS, V6, P2738, DOI 10.1063/1.868164
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 28
TC 11
Z9 16
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 873
EP 881
DI 10.1007/s00371-010-0487-1
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800047
DA 2024-07-18
ER

PT J
AU Jin, Y
   Liu, LG
   Wu, QB
AF Jin, Yong
   Liu, Ligang
   Wu, Qingbiao
TI Nonhomogeneous scaling optimization for realtime image resizing
SO VISUAL COMPUTER
LA English
DT Article
DE Content-aware image resizing; Image retargeting; Visual saliency;
   Triangulation; Feature preserving
AB We present a novel approach for interactive content-aware image resizing. The resizing is performed on warping a triangular mesh over the image, which captures the image saliency information as well as the underlying image features. The warped triangular mesh and the horizontal and vertical scales of all triangles are simultaneously obtained by a quadratic optimization which can be achieved by solving a sparse linear system. Our approach can preserve the shapes of curved features in the resized images. The resizing operation can be performed in an interactive rate which makes the proposed approach practically useful for realtime image resizing. To guarantee a foldover free resizing result, we modify the optimization to a standard quadratic programming. A number of experimental results have shown that our approach has obtained pleasing results and outperforms the previous approaches.
C1 [Jin, Yong; Liu, Ligang; Wu, Qingbiao] Zhejiang Univ, Dept Math, Hangzhou 310003, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Liu, LG (corresponding author), Zhejiang Univ, Dept Math, Hangzhou 310003, Zhejiang, Peoples R China.
EM jsnjkl@zju.edu.cn; ligangliu@zju.edu.cn; qbwu@zju.edu.cn
FU National Natural Science Foundation of China [60776799, 10871178];
   Technology Department of Zhejiang Province [2008C01048-3]; 973 National
   Key Basic Research Foundation of China [2009CB320801]
FX Thanks to the Photo. net, Flickr.com, and PhotoVillage users for sharing
   their images which were used in this paper. We would like to thank Ariel
   Shamir, Michael Rubinstein, and Yanwen Guo for providing the images in
   the paper. This work is supported by the National Natural Science
   Foundation of China (60776799,10871178), Technology Department of
   Zhejiang Province (No. 2008C01048-3), and the 973 National Key Basic
   Research Foundation of China (No. 2009CB320801).
CR [Anonymous], 2007, ACM T GRAPH
   [Anonymous], P ICCV
   [Anonymous], CVX MATLAB SOFTWARE
   [Anonymous], 2010, INT MATH KERN LIB
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Fernandes LAF, 2008, PATTERN RECOGN, V41, P299, DOI 10.1016/j.patcog.2007.04.003
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   KIM H, 2007, P SPIE, V6696
   Liu LG, 2008, COMPUT GRAPH FORUM, V27, P1495, DOI 10.1111/j.1467-8659.2008.01290.x
   REIBMAN A, 2008, P ICIP, P1184
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Sederberg T.W., 1993, em Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, P15, DOI DOI 10.1145/166117.166118.ENDERE
   Seidel R., 1988, CONSTRAINED DELAUNAY
   SHAMIR A, 2009, SIGGRAPH AS COURS
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wolf L., 2007, Computer Vision and Pattern Recognition, P1
   Zhang L, 2010, ACM T SENSOR NETWORK, V6, DOI 10.1145/1777406.1777414
NR 23
TC 38
Z9 50
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 769
EP 778
DI 10.1007/s00371-010-0472-8
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800037
DA 2024-07-18
ER

PT J
AU Knauer, E
   Bärz, J
   Müller, S
AF Knauer, Erik
   Baerz, Jakob
   Mueller, Stefan
TI A hybrid approach to interactive global illumination and soft shadows
SO VISUAL COMPUTER
LA English
DT Article
DE Global illumination; Soft shadows; Photon mapping; Radiosity
AB We present a hybrid approach to simulate global illumination and soft shadows at interactive frame rates. The strengths of hardware-accelerated GPU techniques are combined with CPU methods to achieve physically consistent results while maintaining reasonable performance. The process of image synthesis is subdivided into multiple passes accounting for the different illumination effects. While direct lighting is rendered efficiently by rasterization, soft shadows are simulated using a novel approach combining the speed of shadow mapping and the accuracy of visibility ray tracing. A shadow refinement mask is derived from the result of the direct lighting pass and from a small number of shadow maps to identify the penumbral region of an area light source. This region is accurately rendered by ray tracing. For diffuse indirect illumination, we introduce radiosity photons to profit from the flexibility of a point-based sampling while maintaining the benefits of interpolation over scattered data approximation or density estimation. A sparse sampling of the scene is generated by particle tracing. An area is approximated for each point sample to compute the radiosity solution using a relaxation approach. The indirect illumination is interpolated between neighboring radiosity photons, stored in a multidimensional search tree. We compare different neighborhood search algorithms in terms of image quality and performance. Our method yields interactive frame rates and results consistent with path tracing reference solutions.
C1 [Knauer, Erik; Baerz, Jakob; Mueller, Stefan] Univ Koblenz, D-56070 Koblenz, Germany.
C3 University of Koblenz & Landau
RP Bärz, J (corresponding author), Univ Koblenz, Univ Str 1, D-56070 Koblenz, Germany.
EM eknauer@uni-koblenz.de; jbaerz@uni-koblenz.de; stefanm@uni-koblenz.de
CR [Anonymous], 1993, P 3 INT C COMP GRAPH
   [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Arvo, 1986, ACM SIGGRAPH COURSE, V12, P259
   Cohen M. F., 1988, Computer Graphics, V22, P75, DOI 10.1145/378456.378487
   Cohen M. F., 1985, Computer Graphics, V19, P31, DOI 10.1145/325165.325171
   Cook R. L., 1984, Computers & Graphics, V18, P137
   Dobashi Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P152, DOI 10.1109/PCCGA.2004.1348345
   Goral C. M., 1984, Computers & Graphics, V18, P213
   HANRAHAN P, 1991, COMP GRAPH, V25, P197
   Heckbert P. S., 1990, Computer Graphics, V24, P145, DOI 10.1145/97880.97895
   JENSEN HW, 1995, COMPUT GRAPH, V19, P215, DOI 10.1016/0097-8493(94)00145-O
   Jensen HW., 2001, REALISTIC IMAGE SYNT, DOI [10.1201/9780429294907, DOI 10.1201/9780429294907]
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   LEHTINEN J, 2007, TMLB7 HELS U TECHN
   Lehtinen J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1289603.1289604
   MCGUIRE M, 2009, HPG 09, P77
   Nichols G, 2009, COMPUT GRAPH FORUM, V28, P1141, DOI 10.1111/j.1467-8659.2009.01491.x
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   RITSCHEL T, 2008, ACM T GRAPHIC, P27
   STAMMINGER M, 2000, P VIS MOD VIS 2000, P263
   Veach E., 1998, THESIS STANFORD U
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Ward G. J., 1988, Computer Graphics, V22, P85, DOI 10.1145/378456.378490
   Whitted T., 1979, P 6 ANN C COMPUTER G, VVolume 13, P14
NR 25
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2010
VL 26
IS 6-8
BP 565
EP 574
DI 10.1007/s00371-010-0437-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 602JQ
UT WOS:000278135800017
DA 2024-07-18
ER

PT J
AU Song, YQ
AF Song, Yuqing
TI Boundary fitting for 2D curve reconstruction
SO VISUAL COMPUTER
LA English
DT Article
DE Curve fitting; Boundary fitting; Voronoi tree; Isolation compactness;
   Boundary compactness
ID POINTS; MAPS
AB In this paper we present a 3-step algorithm for reconstructing curves from unorganized points: data clustering to filter out the noise, data confining to get the boundary, and region thinning to find the skeleton curve. The method is effective in removing far-from-the-shape noise and in handling a shape of changing density. The algorithm takes O(nlog n) time and O(n) space for a set of n points.
C1 Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Song, YQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, POB 2704-28, Beijing 100190, Peoples R China.
EM yqsong7@hotmail.com
CR Albou LP, 2009, PROTEINS, V76, P1, DOI 10.1002/prot.22301
   [Anonymous], 1975, P 16 ANN IEEE S FDN
   ARCELLI C, 1992, PATTERN RECOGN LETT, V13, P237, DOI 10.1016/0167-8655(92)90074-A
   Attali D, 1997, COMPUT VIS IMAGE UND, V67, P261, DOI 10.1006/cviu.1997.0536
   BLUM H, 1967, S MOD PERC SPEECH VI, P362
   Cheng SW, 2005, COMP GEOM-THEOR APPL, V31, P63, DOI 10.1016/j.comgeo.2004.07.004
   De-Alarcón PA, 2002, BIOPHYS J, V83, P619, DOI 10.1016/S0006-3495(02)75196-5
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Gower J.C., 1969, Appl.Stats, V18, P54, DOI DOI 10.2307/2346439
   JAROMCZYK JW, 1992, P IEEE, V80, P1502, DOI 10.1109/5.163414
   Jonyer I, 2002, J MACH LEARN RES, V2, P19, DOI [10.1142/S0218213001000441, 10.1162/153244302760185234]
   Kawaji H, 2004, BIOINFORMATICS, V20, P243, DOI 10.1093/bioinformatics/btg397
   Krasnoshchekov D, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P279, DOI 10.1109/SMI.2008.4548006
   Lee IK, 2000, COMPUT AIDED GEOM D, V17, P161, DOI 10.1016/S0167-8396(99)00044-8
   Lee YH, 1996, 10TH INTERNATIONAL PARALLEL PROCESSING SYMPOSIUM - PROCEEDINGS OF IPPS '96, P424, DOI 10.1109/IPPS.1996.508090
   Levin D, 1998, MATH COMPUT, V67, P1517, DOI 10.1090/S0025-5718-98-00974-0
   Lin HW, 2005, VISUAL COMPUT, V21, P418, DOI 10.1007/s00371-005-0304-4
   Ohbuchi R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P293, DOI 10.1109/PCCGA.2003.1238271
   OLSON CF, 1995, PARALLEL COMPUT, V21, P1313, DOI 10.1016/0167-8191(95)00017-I
   Shi Y, 2005, IEEE T KNOWL DATA EN, V17, P1389, DOI 10.1109/TKDE.2005.157
   West D.B., 1996, Introduction to Graph Theory, V2
   ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083
NR 22
TC 7
Z9 8
U1 1
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2010
VL 26
IS 3
BP 187
EP 204
DI 10.1007/s00371-009-0395-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 558CW
UT WOS:000274719200003
DA 2024-07-18
ER

PT J
AU Agathos, A
   Pratikakis, I
   Perantonis, S
   Sapidis, NS
AF Agathos, Alexander
   Pratikakis, Ioannis
   Perantonis, Stavros
   Sapidis, Nickolas S.
TI Protrusion-oriented 3D mesh segmentation
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh segmentation; Prominent feature extraction; Core approximation
AB In this paper, we present a segmentation algorithm which partitions a mesh based on the premise that a 3D object consists of a core body and its constituent protrusible parts. Our approach is based on prominent feature extraction and core approximation and segments the mesh into perceptually meaningful components. Based upon the aforementioned premise, we present a methodology to compute the prominent features of the mesh, to approximate the core of the mesh and finally to trace the partitioning boundaries which will be further refined using a minimum cut algorithm. Although the proposed methodology is aligned with a general framework introduced by Lin et al. (IEEE Trans. Multimedia 9(1):46-57, 2007), new approaches have been introduced for the implementation of distinct stages of the framework leading to improved efficiency and robustness. The evaluation of the proposed algorithm is addressed in a consistent framework wherein a comparison with the state of the art is performed.
C1 [Agathos, Alexander; Pratikakis, Ioannis; Perantonis, Stavros] NCSR Demokritos, Computat Intelligence Lab, Inst Informat & Telecommun, Athens, Greece.
   [Agathos, Alexander; Sapidis, Nickolas S.] Univ Aegean, Dept Prod & Syst Design Engn, Mitilini, Greece.
C3 National Centre of Scientific Research "Demokritos"; University of
   Aegean
RP Agathos, A (corresponding author), NCSR Demokritos, Computat Intelligence Lab, Inst Informat & Telecommun, Athens, Greece.
EM agalex@iit.demokritos.gr; ipratika@iit.demokritos.gr;
   sper@iit.demokritos.gr; sapidis@aegean.gr
RI Sapidis, Nickolas S./E-5584-2010; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688
FU Greek Secretariat of Research and Technology
FX This research was supported by the Greek Secretariat of Research and
   Technology (PENED "3D Graphics search and retrieval" 03 ED 520).
CR [Anonymous], 3DPVT
   [Anonymous], 2007, Computer-Aided Design Applications, DOI DOI 10.1080/16864360.2007.10738515
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   ATTENE M., 2006, IEEE INT C SHAP MOD
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HOFFMAN DD, 1984, COGNITION, V18, P65, DOI 10.1016/0010-0277(84)90022-2
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   KATZ S, 2005, VISUAL COMPUT, V21, P639
   Kim DH, 2005, PATTERN RECOGN, V38, P673, DOI 10.1016/j.patcog.2004.10.003
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Lévy B, 2002, ACM T GRAPHIC, V21, P362, DOI 10.1145/566570.566590
   Li X., 2001, P 2001 S INT 3D GRAP, P35, DOI DOI 10.1145/364338.364343
   Lin HYS, 2007, IEEE T MULTIMEDIA, V9, P46, DOI 10.1109/TMM.2006.886344
   Page DL, 2003, PROC CVPR IEEE, P27
   SHAMIR A, 2006, STAT OF THE ART REP
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SHLAFMAN S, 2002, EUROGRAPHICS, P219
   Valette S., 2005, WORKSHOP SEMANTIC VI, P68
   Wu KN, 1997, IEEE T PATTERN ANAL, V19, P1223, DOI 10.1109/34.632982
   Zhang H., 2005, PROC C VISION MODELI, P429
   Zhang Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P273
   Zuckerberger E, 2002, COMPUT GRAPH-UK, V26, P733, DOI 10.1016/S0097-8493(02)00128-0
NR 24
TC 23
Z9 33
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2010
VL 26
IS 1
BP 63
EP 81
DI 10.1007/s00371-009-0383-8
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 526WD
UT WOS:000272325300006
DA 2024-07-18
ER

PT J
AU Lee, CH
   Liu, A
   Caudell, TP
AF Lee, Chang Ha
   Liu, Alan
   Caudell, Thomas P.
TI A study of locomotion paradigms for immersive medical simulation
   environments
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual environments; User interaction; Locomotion paradigms; User
   study; Medical simulation
AB Immersive virtual environments are increasingly used for medical training and rehearsal. Immersive environments can provide realistic context for team training, where success relies on practiced coordination between individual members. Using immersive virtual environments, medical teams can practice in situations that would otherwise be difficult or expensive to create. It has been shown that individuals perform poorly when the training environment differs significantly from practice 2005. Efforts have been made to close this gap using virtual environments. Interacting in a virtual space requires a robust locomotion paradigm. Locomotion paradigms are methods that allow an individual to move and navigate through virtual environments. Locomotion paradigms should be intuitive to the user, and not distract from the central task of medical training. In this paper, we describe and evaluate four locomotion paradigms, Look & Go, Push & Go, Point & Go, and Grab & Drag, using objective metrics to evaluate navigational efficiency. This study was performed with 98 volunteers predominantly of clinical backgrounds. With the comparison between the performances of game-playing and non-game-playing subjects, we have shown that game-playing experiences do not significantly affect the locomotion performances with the four proposed paradigms. The results of this study suggests the Grab & Drag as the best method among four locomotion paradigms in triage/trauma scenarios, where trainees need to find and help patients scattered in a large area.
C1 [Lee, Chang Ha] Chung Ang Univ, Sch Comp Sci & Engn, Seoul 156756, South Korea.
   [Liu, Alan] Uniformed Serv Univ Hlth Sci, Natl Capital Area, Surg Simulat Lab, Med Simulat Ctr, Bethesda, MD USA.
   [Caudell, Thomas P.] Univ New Mexico, Dept Elect & Comp Engn, Albuquerque, NM 87131 USA.
C3 Chung Ang University; Uniformed Services University of the Health
   Sciences - USA; University of New Mexico
RP Lee, CH (corresponding author), Chung Ang Univ, Sch Comp Sci & Engn, Seoul 156756, South Korea.
EM chlee@cau.ac.kr
FU Office for the Advancement of Telehealth, Health Resources and Services
   Administration, Department of Health and Human Services [D1B TM
   00003-02]; Henry Jackson Foundation [103932]; US Army Medical Research
   Acquisition Activity [W81XWH-04-1-087]; Seoul Development Institute of
   Korean government [CR070019]; Korea Science & Engineering Foundation
   [ROA-2008-000-20060-0]
FX The authors would like to thank Dr. Ken Summers for conceiving of the
   Grab & Drag metaphor and Mr. James Holten III for assistance with the
   implementation. The project described was supported partially by grant 2
   D1B TM 00003-02 from the Office for the Advancement of Telehealth,
   Health Resources and Services Administration, Department of Health and
   Human Services. It was also partially supported by the Henry Jackson
   Foundation project number 103932 and funded by the US Army Medical
   Research Acquisition Activity, ID W81XWH-04-1-087. Its contents are
   solely the responsibility of the authors and do not necessarily
   represent the official views of the Health Resources and Services
   Administration, the Henry Jackson Foundation, and/or the US Army. This
   work was also partially supported by a grant (CR070019) from Seoul R& BD
   Program funded by the Seoul Development Institute of Korean government
   and the NRL Program (Grant ROA-2008-000-20060-0) from the Korea Science
   & Engineering Foundation.
CR Alverson D.C., 2005, Journal of the International Association of Medical Science Educators, V15, P19
   Bowman D. A., 1998, Virtual Reality, V3, P120, DOI 10.1007/BF01417673
   Bowman DA, 1997, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VRAIS.1997.583043
   Caudell Thomas P., 2003, Anatomical Record, V270B, P23, DOI 10.1002/ar.b.10007
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   Darken R.P., 1999, PRESENCE, V8, P3
   Hollerbach JM, 2002, HUM FAC ER, P239
   Iwata H, 1999, IEEE COMPUT GRAPH, V19, P30, DOI 10.1109/38.799737
   Johnson KC, 2006, INT J CANCER, V119, P240, DOI 10.1002/ijc.21777
   KAUFMAN M, 2005, INT IND TRAIN SIM ED
   KAUFMAN M, 2006, P MED MEETS VIRT REA
   Mason A. H., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P426, DOI 10.1145/365024.365308
   MINE MR, 1997, COMPUTER GRAPHICS, V31, P19
   MINE MR, 1995, TR95018 UNC DEP COMP
   Poupyrev I., 1996, P 9 ANN ACM S USER I, P79, DOI [DOI 10.1145/237091.237102, 10.1145/237091.237102]
   POUPYREV I, 1998, GRAPH FORUM, V17, P41
   SCERBO M, 2005, NATO RTO HUMAN FACTO
   Templeman JN, 1999, PRESENCE-TELEOP VIRT, V8, P598, DOI 10.1162/105474699566512
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Whitton MC, 2005, P IEEE VIRT REAL ANN, P123
   WIEDERHOLD MD, 2004, P CYB THER
NR 21
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2009
VL 25
IS 11
BP 1009
EP 1018
DI 10.1007/s00371-009-0356-y
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 508EQ
UT WOS:000270911700004
DA 2024-07-18
ER

PT J
AU Parari, AEC
   Esperança, C
   Oliveira, AAF
AF Cuno Parari, Alvaro E.
   Esperanca, Claudio
   Oliveira, Antonio A. F.
TI Shape-sensitive MLS deformation
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 21st Brazilian Symposium on Computer Graphics and Image Processing
CY OCT 12-15, 2008
CL Campo Grande, BRAZIL
SP Brazilian Comput Soc
DE Moving least squares; As-rigid-as-possible deformation; Animation
AB This work presents methods for deforming meshes in a shape-sensitive way using Moving Least Squares (MLS) optimization. It extends an approach for deforming space (Cuno et al. in Proceedings of the 27th Computer Graphics International Conference, pp. 115-122, 2007) by showing how custom distance metrics may be used to achieve deformations which preserve the overall mesh shape. Several variant formulations are discussed and demonstrated, including the use of geodesic distances, distances constrained to paths contained in the mesh, the use of skeletons, and a reformulation of the MLS scheme which makes it possible to affect the bending behavior of the deformation. Finally, aspects of the implementation of these techniques in parallel architectures such as GPUs (graphics processing units) are described and compared with CPU-only implementations.
C1 [Cuno Parari, Alvaro E.; Esperanca, Claudio; Oliveira, Antonio A. F.] Univ Fed Rio de Janeiro, COPPE, Programa Engn Sistemas, BR-21941972 Rio De Janeiro, Brazil.
C3 Universidade Federal do Rio de Janeiro
RP Esperança, C (corresponding author), Univ Fed Rio de Janeiro, COPPE, Programa Engn Sistemas, Cidade Univ,Ctr Tecnol,Bloco H,Sala 319, BR-21941972 Rio De Janeiro, Brazil.
EM alvaro@lcg.ufrj.br; esperanc@lcg.ufrj.br; oliveira@lcg.ufrj.br
OI Cuno, Alvaro/0000-0001-9595-471X
CR Alexa M, 2006, ACM SIGGRAPH COURSES
   [Anonymous], 1987, Art gallery theorems and algorithms
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Au OKC, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239534, 10.1145/1276377.1276481]
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Botsch M, 2005, COMPUT GRAPH FORUM, V24, P611, DOI 10.1111/j.1467-8659.2005.00886.x
   Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054
   Botsch M, 2007, COMPUT GRAPH FORUM, V26, P339, DOI 10.1111/j.1467-8659.2007.01056.x
   Buck I, 2004, ACM T GRAPHIC, V23, P777, DOI 10.1145/1015706.1015800
   Cornea ND, 2007, IEEE T VIS COMPUT GR, V13, P530, DOI 10.1109/TVCG.2007.1002
   Cuno A., 2007, Proceedings of the 27th Computer Graphics International Conference, P115
   FEDOR M, 2003, SCCG 03, P203
   Fu HB, 2007, COMPUT GRAPH FORUM, V26, P34, DOI 10.1111/j.1467-8659.2007.00940.x
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   KANATANI K, 1994, IEEE T PATTERN ANAL, V16, P543, DOI 10.1109/34.291441
   *LAWR LIV NAT LAB, 1995, POSIX THREADS PROGR
   McCool M, 2004, ACM T GRAPHIC, V23, P787, DOI 10.1145/1015706.1015801
   MCCOOL MD, 2006, P ACM IEEE C SUP NY, P222
   Nealen A, 2005, ACM T GRAPHIC, V24, P1142, DOI 10.1145/1073204.1073324
   *NVIDIA CORP, 2007, CUDA ENV COMP UN DEV
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   PAPAKIPOS M, 2007, P LCI C HIGH PERF CL
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   SORKINE O, 2007, SGP 07, P109
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   TARDITI D, 2006, 12 INT C ARCH SUPP P, P325
   WALKER MW, 1991, CVGIP-IMAG UNDERSTAN, V54, P358, DOI 10.1016/1049-9660(91)90036-O
NR 28
TC 4
Z9 4
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2009
VL 25
IS 10
SI SI
BP 911
EP 922
DI 10.1007/s00371-009-0369-6
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 500TP
UT WOS:000270328200003
DA 2024-07-18
ER

PT J
AU Luboz, V
   Blazewski, R
   Gould, D
   Bello, F
AF Luboz, Vincent
   Blazewski, Rafal
   Gould, Derek
   Bello, Fernando
TI Real-time guidewire simulation in complex vascular models
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Workshop on 3D Physiological Human
CY DEC, 2008
CL Zermatt, SWITZERLAND
DE Simulation; Interventional radiology; Training; Mass-spring model
AB The base of all training in interventional radiology aims at the development of the core skills in manipulating the instruments. Computer simulators are emerging to help in this task. This paper extends our previous framework with more realistic instrument behaviour and more complex vascular models. The instrument is modelled as a hybrid mass-spring particle system while the vasculature is a triangulated surface mesh segmented from patient data sets. A specially designed commercial haptic device allows the trainee to use real instruments to guide the simulation through the vasculature selected from a database of 23 different patients. A new collision detection algorithm allows an efficient computation of the contacts, therefore leaving more time to deal with the collision response for a realistic simulation in real time. The behaviour of our simulated instruments has been visually compared with the real ones and assessed by experienced interventional radiologists. Preliminary results show close correlations and a realistic behaviour.
C1 [Luboz, Vincent; Blazewski, Rafal; Bello, Fernando] Univ London Imperial Coll Sci Technol & Med, St Marys Hosp, Biosurg & Surg Technol Dept, SORA, London W2 1NY, England.
   [Gould, Derek] Royal Liverpool Hosp, Liverpool L7 8XP, Merseyside, England.
C3 Imperial College London; Royal Liverpool & Broadgreen University
   Hospitals NHS Trust; Royal Liverpool University Hospital
RP Luboz, V (corresponding author), Univ London Imperial Coll Sci Technol & Med, St Marys Hosp, Biosurg & Surg Technol Dept, SORA, London W2 1NY, England.
EM v.luboz@imperial.ac.uk; dgould@liverpool.ac.uk; fernando@imperial.ac.uk
RI Bello, Fernando/E-3081-2013
OI Bello, Fernando/0000-0003-4136-0355
FU EPSRC [EP/E005020/1] Funding Source: UKRI
CR Alderliesten T, 2007, IEEE T BIO-MED ENG, V54, P29, DOI 10.1109/TBME.2006.886659
   Chui CK, 2002, STUD HEALTH TECHNOL, V85, P96
   Cotin S, 2005, LECT NOTES COMPUT SC, V3750, P534, DOI 10.1007/11566489_66
   Luboz V, 2008, LECT NOTES COMPUT SC, V5104, P215, DOI 10.1007/978-3-540-70521-5_25
   WANG F, 2007, P 29 ANN INT C IEEE
NR 5
TC 32
Z9 37
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2009
VL 25
IS 9
BP 827
EP 834
DI 10.1007/s00371-009-0312-x
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 478KA
UT WOS:000268585100002
DA 2024-07-18
ER

PT J
AU Chen, YL
   Lai, SH
AF Chen, Yi-Ling
   Lai, Shang-Hong
TI Creating MPU implicit surfaces from unoriented point sets with
   orientation inference
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT Computer Graphics International Conference 2009
CY MAY 26-29, 2009
CL Victoria, CANADA
DE Implicit surfaces; Orientation inference; Sharp features
ID PARTITION; UNITY
AB In this paper, we extend the MPU implicits algorithm to deal with unoriented point sets while preserving its desirable properties, such as sharp feature preservation. An orientation inference algorithm is introduced to orientate the local implicit patches by solving a graph labeling problem through energy minimization. Sign consistency between local functions is exploited to infer the globally consistent orientation. To precisely model the features, we employ the affinity propagation clustering algorithm to identify the local surface patches composing the features by considering orientation consistency between data points. Sharp features can then be accurately reconstructed by performing piecewise smooth surface fitting. Experimental results are shown to demonstrate the performance of the proposed algorithm.
C1 [Chen, Yi-Ling; Lai, Shang-Hong] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
C3 National Tsing Hua University
RP Chen, YL (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, 101 Sect 2,Kuang Fu Rd, Hsinchu 30013, Taiwan.
EM yilin@cs.nthu.edu.tw; lai@cs.nthu.edu.tw
RI Chen, Yi/HIR-2608-2022; Chen, yf/JMR-4435-2023; Chen, Yi/JBR-7728-2023;
   Chen, Yuxuan/IWL-8267-2023; chen, ye Xiao/HSF-9650-2023; chen,
   yue/JXW-9556-2024; Lai, Shang-Hong/AAS-4002-2020
OI Lai, Shang-Hong/0000-0002-5092-993X
CR Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   [Anonymous], P 5 EUR S GEOM PROC
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   CHEN YL, 2007, P 3 D DIG IM MOD, P400
   Desbrun, 2007, P 5 EUROGRAPHICS S G, V7, P39
   DEY TK, 2005, P S GEOM PROC
   Dinh HQ, 2002, IEEE T PATTERN ANAL, V24, P1358, DOI 10.1109/TPAMI.2002.1039207
   FLEISHMAN S, 2005, ACM SIGGRAPH
   FRANKE R, 1980, INT J NUMER METH ENG, V15, P1691, DOI 10.1002/nme.1620151110
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   KAZHDAN M, 2006, P EUR S GEOM PROC SA
   KOBBELT LP, 2001, ACM SIGGRAPH
   KOLLURI R, 2005, P ACM SIAM S DISCR A
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Ohtake Y, 2006, GRAPH MODELS, V68, P15, DOI 10.1016/j.gmod.2005.08.001
   OHTAKE Y, 2003, ACM SIGGRAPH
   OHTAKE Y, 2005, S GEOM PROC
   OHTAKE Y, 2004, ACM SIGGRAPH
   PAULY M, 2003, P EUR, V22
   Stylianou G, 2004, IEEE T VIS COMPUT GR, V10, P536, DOI 10.1109/TVCG.2004.24
   Taubin G., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P658, DOI 10.1109/ICCV.1993.378149
   Tobor I, 2006, GRAPH MODELS, V68, P25, DOI 10.1016/j.gmod.2005.09.003
   Turk G, 2002, ACM T GRAPHIC, V21, P855, DOI 10.1145/571647.571650
   Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585
   Xie H, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P259, DOI 10.1109/VISUAL.2004.101
   Xie H, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P91, DOI 10.1109/VISUAL.2003.1250359
NR 28
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2009
VL 25
IS 5-7
BP 391
EP 399
DI 10.1007/s00371-009-0354-0
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 438ES
UT WOS:000265539300003
DA 2024-07-18
ER

PT J
AU Crespo, J
   Zorrilla, M
   Bernardos, P
   Mora, E
AF Luis Crespo, Jose
   Zorrilla, Marta
   Bernardos, Pilar
   Mora, Eduardo
TI Moving objects forecast in image sequences using autoregressive
   algorithms
SO VISUAL COMPUTER
LA English
DT Article
DE Tracking and prediction; Image sequence analysis; Causal images;
   Autoregressive model
ID PREDICTION; MOTION; MODEL
AB The objective of this paper is to present an overall approach to forecasting the future position of the moving objects of an image sequence after processing the images previous to it. The proposed method makes use of classical techniques such as optical flow to extract objects' trajectories and velocities, and autoregressive algorithms to build the predictive model. Our method can be used in a variety of applications, where videos with stationary cameras are used, moving objects are not deformed and change their position with time. One of these applications is traffic control, which is used in this paper as a case study with different meteorological conditions to compare with.
C1 [Zorrilla, Marta] Univ Cantabria, Dept Stat & Computat Math, E-39005 Santander, Spain.
   [Luis Crespo, Jose; Bernardos, Pilar; Mora, Eduardo] Univ Cantabria, Dept Appl Math & Comp Sci, E-39005 Santander, Spain.
C3 Universidad de Cantabria; Universidad de Cantabria
RP Zorrilla, M (corresponding author), Univ Cantabria, Dept Stat & Computat Math, E-39005 Santander, Spain.
EM crespoj@unican.es; marta.zorrilla@unican.es
RI zorrilla, marta/I-7617-2012
OI zorrilla, marta/0000-0002-0475-8834
FU Spanish Inter-ministerial Board for Science and Technology (Comision
   Interministerial de Ciencia y Tecnolog a CICYT) [TIC2002-01306]
FX The authors are deeply grateful to the Spanish Inter-ministerial Board
   for Science and Technology (Comision Interministerial de Ciencia y
   Tecnolog a CICYT) which supports Project TIC2002-01306, as a part of
   which this paper has been produced.
CR [Anonymous], 2001, Sequential Monte Carlo methods in practice
   [Anonymous], 2001, Pyramidal implementation of the Lucas Kanade Feature Tracker Description of the Algorithm
   Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141
   Bergeron C, 1991, IEEE T CIRC SYST VID, V1, P72, DOI 10.1109/76.109148
   Bors AG, 2000, IEEE T IMAGE PROCESS, V9, P1441, DOI 10.1109/83.855440
   Box GEP, 1976, TIME SERIES ANAL FOR
   Carpenter J, 1999, IEE P-RADAR SON NAV, V146, P2, DOI 10.1049/ip-rsn:19990255
   Crespo JL, 2007, VISUAL COMPUT, V23, P419, DOI 10.1007/s00371-007-0114-y
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Elnagar A, 1998, IEEE T SYST MAN CY A, V28, P803, DOI 10.1109/3468.725351
   ERKELENS J, 1996, THESIS DELFT U TECHN
   GLOYER B, 1995, P SPIE IMAGE VIDEO P, V3, P173
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   *GROUP HH NAG I AL, F FIUKT TRAFF INT SE
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hsiao YT, 2006, IMAGE VISION COMPUT, V24, P1123, DOI 10.1016/j.imavis.2006.04.002
   *IST, 200137540 IST
   KEHTARNAVAZ N, 1990, COMPUT VISION GRAPH, V49, P95, DOI 10.1016/0734-189X(90)90165-R
   LABIT C, 1991, NOV P VCIP BOST, V1605, P697
   LO B, 2001, P INT S INT MULT VID, V1, P158
   Lucas B. D., 1981, P IJCAI, P674
   Mitiche L, 2004, SIGNAL PROCESS, V84, P1805, DOI 10.1016/j.sigpro.2004.05.029
   Pece AEC, 2006, IMAGE VISION COMPUT, V24, P1218, DOI 10.1016/j.imavis.2005.06.013
   PFEIFER PE, 1980, TECHNOMETRICS, V22, P35, DOI 10.2307/1268381
   PICCARDI M, 2004, P IEEE SMC 2004 INT, V1, P3099
   Rao TS, 2004, IMA V MATH, P123
   SMITH SM, 1997, TR97SMS1
   Turkmen I, 2006, NEUROCOMPUTING, V69, P2309, DOI 10.1016/j.neucom.2005.04.014
   Weigend A. S., 1993, Time Series Prediction
NR 29
TC 3
Z9 3
U1 0
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD APR
PY 2009
VL 25
IS 4
BP 309
EP 323
DI 10.1007/s00371-008-0270-8
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 413YH
UT WOS:000263830600002
DA 2024-07-18
ER

PT J
AU Xiao, CX
   Fu, HB
   Tai, CL
AF Xiao, Chunxia
   Fu, Hongbo
   Tai, Chiew-Lan
TI Hierarchical aggregation for efficient shape extraction
SO VISUAL COMPUTER
LA English
DT Article
DE Mesh segmentation; Geometric similarity measure; Shape matching; Shape
   extraction
ID MESH SEGMENTATION; POINT
AB This paper presents an efficient framework which supports both automatic and interactive shape extraction from surfaces. Unlike most of the existing hierarchical shape extraction methods, which are based on computationally expensive top-down algorithms, our framework employs a fast bottom-up hierarchical method with multiscale aggregation. We introduce a geometric similarity measure, which operates at multiple scales and guarantees that a hierarchy of high-level features are automatically found through local adaptive aggregation. We also show that the aggregation process allows easy incorporation of user-specified constraints, enabling users to interactively extract features of interest. Both our automatic and the interactive shape extraction methods do not require explicit connectivity information, and thus are applicable to unorganized point sets. Additionally, with the hierarchical feature representation, we design a simple and effective method to perform partial shape matching, allowing efficient search of self-similar features across the entire surface. Experiments show that our methods robustly extract visually meaningful features and are significantly faster than related methods.
C1 [Xiao, Chunxia] Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
   [Fu, Hongbo; Tai, Chiew-Lan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Wuhan University; Hong Kong University of Science & Technology
RP Xiao, CX (corresponding author), Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
EM cxxiao@whu.edu.cn; fuhb@cse.ust.hk; taicl@cse.ust.hk
OI FU, Hongbo/0000-0002-0284-726X
FU Research Grant Council of the Hong Kong Special Administrative Region,
   China [HKUST620107]; NSF of China [40701154]
FX We would like to thank Ligang Liu for the code of the easy mesh cutting
   algorithm [12]. We would also like to thank the anonymous reviewers for
   their insightful comments. This work was supported by a grant from the
   Research Grant Council of the Hong Kong Special Administrative Region,
   China (Project No. HKUST620107) and a grant from the NSF of China (No.
   40701154).
CR Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Attene M, 2006, VISUAL COMPUT, V22, P181, DOI 10.1007/s00371-006-0375-x
   Cohen-Steiner D., 2003, PROC 19 ANN ACM SYMP, P237
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Funkhouser T, 2004, ACM T GRAPHIC, V23, P652, DOI 10.1145/1015706.1015775
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Gatzke T, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P244, DOI 10.1109/SMI.2005.13
   Gatzke T, 2006, LECT NOTES COMPUT SC, V4077, P578
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Ji ZP, 2006, COMPUT GRAPH FORUM, V25, P283, DOI 10.1111/j.1467-8659.2006.00947.x
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Katz S, 2003, ACM T GRAPHIC, V22, P954, DOI 10.1145/882262.882369
   Lai Y.K., 2006, PROC ACM S SOLID PHY, P17
   LANGE C, 2005, SPEC ISSUE COMPUT AI, V22
   Lee Y, 2005, COMPUT AIDED GEOM D, V22, P444, DOI 10.1016/j.cagd.2005.04.002
   Lee Y, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P279
   LI X, 2005, S GEOM PROC, P217
   Liu R, 2004, 12TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P298
   Liu R, 2007, COMPUT GRAPH FORUM, V26, P385, DOI 10.1111/j.1467-8659.2007.01061.x
   Liu R, 2006, LECT NOTES COMPUT SC, V4035, P172
   Mangan AP, 1999, IEEE T VIS COMPUT GR, V5, P308, DOI 10.1109/2945.817348
   MORTARA M, 2004, 9 ACM S SOL MOD APPL, P339
   Ohtake Y, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P31, DOI 10.1109/SMI.2004.1314491
   Page DL, 2003, PROC CVPR IEEE, P27
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   PAULY M, 2003, EUROGRAPHICS, P281
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   SHAMIR A, 2006, STATE OF ART REPORT, P137
   Sharf A, 2006, VISUAL COMPUT, V22, P835, DOI 10.1007/s00371-006-0068-5
   SHARON E, 2001, COMPUTER VISION PATT, V1, P70
   SHARON E, 2000, CVPR, V1, P70
   Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Vieira M, 2005, COMPUT AIDED GEOM D, V22, P771, DOI 10.1016/j.cagd.2005.03.006
   Xiao CX, 2006, VISUAL COMPUT, V22, P210, DOI 10.1007/s00371-006-0377-8
   YAMAZAKI I, 2006, SHAPE MODELING APPL, P46
   ZELINKA S, 2004, S GEOM PROC, P209
   Zelinka S, 2006, PROC GRAPH INTERF, P107
   Zhang Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P273
NR 41
TC 11
Z9 13
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2009
VL 25
IS 3
BP 267
EP 278
DI 10.1007/s00371-008-0226-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 403RH
UT WOS:000263099200006
DA 2024-07-18
ER

PT J
AU Haciomeroglu, M
   Laycock, RG
   Day, AM
AF Haciomeroglu, M.
   Laycock, R. G.
   Day, A. M.
TI Automatic spatial analysis and pedestrian flow control for real-time
   crowd simulation in an urban environment
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT International Conference on Cyberworlds
CY OCT 24-27, 2007
CL Hannover, GERMANY
SP Welfenlab, Gottfried Wilhelm Leibniz Univ, EuroGraphics, ACM SIGWEB, ACM SIGART
DE real-time crowd simulation; pedestrian flow
AB Distributing a collection of virtual humans throughout a large urban environment, where limited semantic information is available, poses a problem when attempting to create a visually realistic real-time environment. Randomly positioning agents within an urban environment will not cover the environment with virtual humans in a plausible way. For example, areas of the urban space that are more frequently used should have a higher population density both at the start and during the simulation. It is infeasible to manually identify all the areas in the urban environment which should be crowded or sparsely populated when considering a scalable method, suitable for large environments. Consequently, this paper combines and extends techniques from spatial analysis and virtual agent behaviour simulations to develop a system capable of automatically distributing pedestrians in an urban environment. In particular, it extends the Point-Based Space Syntax technique to enable the automatic analysis of a large urban environment in the presence of limited contextual information. This analysis specifies a set of population densities for areas in the environment and these values are used to initialise the locations of all the virtual humans in the environment. In addition to the initialisation stage the population densities in each area are consulted to ensure that the correct distribution of virtual humans is maintained throughout the simulation. The system is tested on an arbitrary section of a real city and comparisons of the characteristic parts of the test environment are correlated with the pedestrian movements.
C1 [Haciomeroglu, M.; Laycock, R. G.; Day, A. M.] Univ E Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
C3 University of East Anglia
RP Haciomeroglu, M (corresponding author), Univ E Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
EM muratm@cmp.uea.ac.uk
FU EPSRC [EP/E035639/1] Funding Source: UKRI
CR AICHOLZER O, 1995, J UNIVERS COMPUT SCI
   Amato NM, 1998, ROBOTICS: THE ALGORITHMIC PERSPECTIVE, P155
   ARIKAN O, 2001, P EUR WORKSH COMP AN, P151
   AVNEESH S, 2007, IEEE VIRTUAL REALITY
   FELKEL P, 1998, 14 SPRING C COMP GRA, V14, P210
   Gibson J. J., 2014, The ecological approach to visual perception, Vclassic
   Hillier B., 1984, SOCIAL LOGIC SPACE, DOI DOI 10.1017/CBO9780511597237
   Hillier B., 1996, Space is the Machine
   Jiang B., 2002, T GIS, V6, P295, DOI [10.1111/1467-9671.00112, DOI 10.1111/1467-9671.00112]
   Kavraki LE, 1998, PRACTICAL MOTION PLANNING IN ROBOTICS, P33
   Lamarche F, 2004, COMPUT GRAPH FORUM, V23, P509, DOI 10.1111/j.1467-8659.2004.00782.x
   Loscos C, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P122
   MICHAEL D, 2003, SHORT PAPER EUROGRAP
   PETTRE J, 2005, 1 INT WORKSH CROWD S
   RENAULT O, 2001, J VISUAL COMP ANIMAT, V1, P18
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Ryder G, 2006, EUROGR TECH REP SER, P37
   Ryder G, 2005, COMPUT GRAPH FORUM, V24, P203, DOI 10.1111/j.1467-8659.2005.00844.x
   Sakuma T, 2005, COMPUT ANIMAT VIRT W, V16, P343, DOI 10.1002/cav.105
   SHAO W, 2005, SCA 05, P19
   STYLIANOU S, 2004, VRST 04, P65
   Tecchia F, 2002, COMPUT GRAPH FORUM, V21, P753, DOI 10.1111/1467-8659.00633
   Turner A, 2002, ENVIRON PLANN B, V29, P473, DOI 10.1068/b12850
   TURNER A, 2007, EVOLVING DIRECT PERC, P411
   WEIS MA, 1995, ALGORITHMS DATA STRU
   Wilmarth SA, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1024, DOI 10.1109/ROBOT.1999.772448
NR 26
TC 1
Z9 1
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2008
VL 24
IS 10
BP 889
EP 899
DI 10.1007/s00371-008-0291-3
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 347IQ
UT WOS:000259134200005
DA 2024-07-18
ER

PT J
AU Wang, XZ
   Devarajan, V
AF Wang, Xiuzhong
   Devarajan, Venkat
TI Improved 2D mass-spring-damper model with unstructured triangular meshes
SO VISUAL COMPUTER
LA English
DT Article
DE mass-spring-damper model; parameter optimization; preload; axisymmetric
   bending/stretching
ID PARTICLE-SYSTEM; ANIMATION
AB In this paper, we investigate both the visual realism and the physical accuracy of the 2D mass-spring-damper (MSD) model with general unstructured triangular meshes for the simulation of rigid cloth. For visual realism, the model should, at a minimum, bend smoothly under pure bending load conditions. For physical accuracy, it should bend approximately the same amount and shape as dictated by continuum mechanics. By matching the 2D MSD model with an elastic plate, we obtain a series of constraints on the parameters of the model. We find that for a 2D unstructured MSD model, it is necessary to apply preloads on the springs for accurate modeling of bending resistance. By simultaneously applying the constraints for both visual realism and physical accuracy, we can optimize the parameters of the model to enhance its fidelity. The simulation shows that the deformation of the optimized MSD model with preload is very close to the result obtained by the finite element method (FEM) under either point load condition or pressure load condition. With a much smaller computational burden compared with FEM, the optimized MSD model is especially suitable for real time haptic applications.
C1 Ikonisys Inc, New Haven, CT 06511 USA.
   Univ Texas Arlington, Arlington, TX 76019 USA.
C3 Ikonisys Inc; University of Texas System; University of Texas Arlington
RP Wang, XZ (corresponding author), Ikonisys Inc, 5 Sci Pk, New Haven, CT 06511 USA.
EM xiuzhongw@yahoo.com
CR [Anonymous], 1975, THEORIES ELASTIC PLA
   Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821
   Bhat Kiran S., 2003, Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, P37
   Breen David E., 1994, Proceedings of the 21st annual conference on Computer graphics and interactive techniques, P365
   BRIDSON R, 2003, P 2003 ACM SIGGRAPH, P28
   BROWN J, 2001, P 4 INT C MED IM COM, P137
   CARIGNAN M, 1992, COMP GRAPH, V26, P99, DOI 10.1145/142920.134017
   Chen YY, 2003, IEEE T VIS COMPUT GR, V9, P43, DOI 10.1109/TVCG.2003.1175096
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   Devarajan V, 2006, INT J MED ROBOT COMP, V2, P312, DOI 10.1002/rcs.107
   Eberhardt B, 1996, IEEE COMPUT GRAPH, V16, P52, DOI 10.1109/38.536275
   Etzmuss O, 2003, IEEE T VIS COMPUT GR, V9, P538, DOI 10.1109/TVCG.2003.1260747
   FENNER RT, 1993, MECH SOLIDS
   GELDER A, 1998, J GRAPHICS TOOLS, V3, P21
   Grinspun E., 2003, P 2003 ACM SIGGRAPH, P62
   Jojic Nebojsa., 1997, INT WORKSHOP SYNTHET, P73
   Lee Y., 1995, SIGGRAPH, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]
   MACIEL A, 2003, P INT S SURG SIM SOF, P74
   Miller G. S. P., 1988, Computer Graphics, V22, P169, DOI 10.1145/378456.378508
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   PROVOT X, 1995, GRAPH INTER, P147
   Raghupathi L, 2004, IEEE T VIS COMPUT GR, V10, P708, DOI 10.1109/TVCG.2004.36
   Solecki R., 2003, Advanced Mechanics of Materials
   Terzopoulos D., 1990, Journal of Visualization and Computer Animation, V1, P73, DOI 10.1002/vis.4340010208
   Terzopoulos D., 1989, Proceeding of Graphics Interface, P219
   Terzopoulos D., 1987, COMPUT GRAPH, P205, DOI DOI 10.1145/37402.37427
   TU X., 1994, P ACM SIGGRAPH 94, P43, DOI DOI 10.1145/192161.192170
   Vassilev T, 2001, COMPUT GRAPH FORUM, V20, pC260, DOI 10.1111/1467-8659.00518
   Volino P., 2005, Computer-Aided Design and Applications, V2, P645
   VOLINO P, 1995, P SIGGRAPH 95, P137
   WANG X, 2004, ACM SIGGRAPH INT C V, P317
   Wang XH, 2005, VISUAL COMPUT, V21, P429, DOI 10.1007/s00371-005-0303-5
   Wang XZ, 2006, STUD HEALTH TECHNOL, V119, P568
   Zhang Y, 2004, IEEE T VIS COMPUT GR, V10, P339, DOI 10.1109/TVCG.2004.1272733
NR 34
TC 5
Z9 7
U1 0
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2008
VL 24
IS 1
BP 57
EP 75
DI 10.1007/s00371-007-0179-7
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 234GU
UT WOS:000251149500005
DA 2024-07-18
ER

PT J
AU Huang, ZJ
   Wang, GP
AF Huang, Zhangjin
   Wang, Guoping
TI Improved error estimate for extraordinary Catmull-Clark subdivision
   surface patches
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 10th International Conference on Computer-Aided Design and Computer
   Graphics
CY OCT 15-18, 2007
CL Peking Univ, Beijing, PEOPLES R CHINA
SP China Comp Federat, IEEE Beijing Sect, Peking Univ, Inst Comp Sci & Technol, Peking Univ, Sch EECS, Natl Nat Sci Fdn China, Microsoft Res Asia, Peking Univ, Natl Lab Machine Percept, Key Lab High Confidence Software Technologies, Minist Educ
HO Peking Univ
DE Catmull-Clark subdivision surfaces; error estimate; subdivision depth;
   adaptive subdivision
ID DEPTH COMPUTATION
AB Based on an optimal estimate of the convergence rate of the second order norm, an improved error estimate for extraordinary Catmull-Clark subdivision surface (CCSS) patches is proposed. If the valence of the extraordinary vertex of an extraordinary CCSS patch is even, a tighter error bound and, consequently, a more precise subdivision depth for a given error tolerance, can be obtained. Furthermore, examples of adaptive subdivision illustrate the practicability of the error estimation approach.
C1 Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University
RP Huang, ZJ (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM hzj@graphics.pku.edu.cn; wgp@graphics.pku.edu.cn
RI wang, guoping/KQU-3394-2024; Huang, Zhangjin/I-7929-2014
CR CATMULL E, 1978, COMPUT AIDED DESIGN, V10, P350, DOI 10.1016/0010-4485(78)90110-0
   Chen G, 2006, LECT NOTES COMPUT SC, V4077, P545
   Cheng F., 2006, Computer-Aided Design and Applications, V3, P485
   Cheng FH, 2006, LECT NOTES COMPUT SC, V4035, P404
   DOO D, 1978, COMPUT AIDED DESIGN, V10, P356, DOI 10.1016/0010-4485(78)90111-2
   Huang Z.J., 2007, P 2007 ACM S SOL PHY, P233
   Loop C, 1987, THESIS U UTAH
   Lutterkort D, 2001, NUMER MATH, V89, P735, DOI 10.1007/S002110100181
   Stam J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P395, DOI 10.1145/280814.280945
NR 9
TC 6
Z9 6
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2007
VL 23
IS 12
BP 1005
EP 1014
DI 10.1007/s00371-007-0173-0
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 232EP
UT WOS:000251001400006
DA 2024-07-18
ER

PT J
AU Ott, R
   Thalmann, D
   Vexo, F
AF Ott, Renaud
   Thalmann, Daniel
   Vexo, Ferderic
TI Haptic feedback in mixed-reality environment
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 25th Computer Graphics International Conference (CGI)
CY MAY 30-JUN 02, 2007
CL Petropolis, BRAZIL
DE haptic; mixed-reality; (Novel) user interfaces
ID VIRTUAL OBJECTS
AB The training process in industries is assisted with computer solutions to reduce costs. Normally, computer systems created to simulate assembly or machine manipulation are implemented with traditional Human-Computer interfaces (keyboard, mouse, etc). But, this usually leads to systems that are far from the real procedures, and thus not efficient in term of training. Two techniques could improve this procedure: mixed-reality and haptic feedback. We propose in this paper to investigate the integration of both of them inside a single framework. We present the hardware used to design our training system. A feasibility study allows one to establish testing protocol. The results of these tests convince us that such system should not try to simulate realistically the interaction between real and virtual objects as if it was only real objects.
C1 Ecole Polytech Fed Lausanne, VRLab, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Ott, R (corresponding author), Ecole Polytech Fed Lausanne, VRLab, CH-1015 Lausanne, Switzerland.
EM daniel.thalmann@epfl.ch; frederic.vexo@epfl.ch
RI Thalmann, Daniel/AAL-1097-2020; Thalmann, Daniel/A-4347-2008
OI Thalmann, Daniel/0000-0002-0451-7491; 
CR Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   BIANCHI G, 2006, P EUROHAPTICS C
   CRISON F, 2005, P IEEE INT C VIRT RE
   LECUYER A, 2001, P IEEE INT WORKSH RO
   LECUYER A, 2005, P WORLDHAPTIC
   MELLETDHUART D, 2004, P VIRT REAL C VRIC
   Nojima T, 2002, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2002.996506
   Ott R, 2005, WORLD HAPTICS CONFERENCE: FIRST JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRUTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P401
   PETENIER A, 2006, P EDUTAINMENT
   Salisbury JK, 1997, IEEE COMPUT GRAPH, V17, P6, DOI 10.1109/MCG.1997.1626171
   Tamura H, 2001, IEEE COMPUT GRAPH, V21, P64, DOI 10.1109/38.963462
   Tuceryan M, 2000, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDING, P149, DOI 10.1109/ISAR.2000.880938
   Vacchetti L., 2004, VIRTUAL REALITY AUGM, P125, DOI [10.1007/978-1-4471-3873-0_8, DOI 10.1007/978-1-4471-3873-0_8]
   Walairacht S, 2002, PRESENCE-TELEOP VIRT, V11, P134, DOI 10.1162/105474602317396011
   Zauner J, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P237, DOI 10.1109/ISMAR.2003.1240707
NR 15
TC 14
Z9 16
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2007
VL 23
IS 9-11
SI SI
BP 843
EP 849
DI 10.1007/s00371-007-0159-y
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 206UE
UT WOS:000249207600024
DA 2024-07-18
ER

PT J
AU Liu, SG
   Wang, ZY
   Gong, Z
   Peng, QS
AF Liu, Shiguang
   Wang, Zhangye
   Gong, Zheng
   Peng, Qunsheng
TI Real time simulation of a tornado
SO VISUAL COMPUTER
LA English
DT Article
DE natural phenomenon; tornado scene; Reynold-average two-fluid model
   (RATFM)
AB We present a novel method for simulating a tornado scene and its damage on the environment in real time, which is recognized as a challenging task for researchers of computer graphics. The method adopts a Reynold-average two-fluid model (RATFM) for modeling the motion of a tornado. In RATFM, the air flow (wind field) is simulated by Reynold-average Navier-Stokes equations. The motion of dust particles is approximated as a continuous fluid and is modeled by non-viscosity Navier-Stokes equations. An interaction force is introduced to simulate the interaction between these two-fluid systems efficiently. Considering the data structure of our method, we design a RATFM solver on the GPU to achieve real time simulation. We also adopt new features of the GPU to accelerate our algorithm. Then, an efficient method is proposed to simulate the tornado's interaction with surrounding large objects such as a car, a bus, a house, etc. In our model, the objects in the tornado scene are represented by connected voxels and a corresponding graph storing the link information. Compared with the photographs of real tornado displays, our simulated results are quite satisfactory.
C1 Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 Zhejiang University; Tianjin University
RP Wang, ZY (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM lsg@cad.zju.edu.cn; zywang@cad.zju.edu.cn; gongzheng@cad.zju.edu.cn;
   peng@cad.zju.edu.cn
RI Zhou, Hong/JKJ-1067-2023
CR [Anonymous], P C GRAPH HARDW
   BELL WN, 2005, P ACM SIGGRAPH EUROG, P77
   DING X, 2004, PHYS BASED SIMULATIO
   ENRIGHT D, 2001, P SIGGRAPH, P736
   Fedkiw R, 2001, COMP GRAPH, P15, DOI 10.1145/383259.383260
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Hong JM, 2003, COMPUT GRAPH FORUM, V22, P253, DOI 10.1111/1467-8659.00672
   LIU S, 2006, PHYS BASED MODELING, V7, P1099
   Losasso F, 2006, ACM T GRAPHIC, V25, P812, DOI 10.1145/1141911.1141960
   Mihalef Viorel., 2006, S COMPUTER ANIMATION, P317
   Mizuno R, 2003, 11TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P440, DOI 10.1109/PCCGA.2003.1238291
   Muller M, 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Nguyen DQ, 2002, ACM T GRAPHIC, V21, P721, DOI 10.1145/566570.566643
   Rotunno R., 1993, The Tornado: Its Structure, Dynamics, Prediction, and Hazards, P57
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   TRAPP RL, 1993, TORNADO STRUCTURE DY, V79, P49
   Yngve GD, 2000, COMP GRAPH, P29, DOI 10.1145/344779.344801
   Zheng W., 2006, Proc. of the ACM Siggraph/Eurographics Symposium on Computer Animation, P325
   ZHU HB, 2006, P COMPUTER ANIMATION, P403
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 20
TC 8
Z9 17
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2007
VL 23
IS 8
BP 559
EP 567
DI 10.1007/s00371-007-0118-7
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 189HE
UT WOS:000247979200003
DA 2024-07-18
ER

PT J
AU Lu, J
   Ye, ZX
   Zou, YR
AF Lu, Jian
   Ye, Zhongxing
   Zou, Yuru
TI Automatic generation of colorful patterns with wallpaper symmetries from
   dynamics
SO VISUAL COMPUTER
LA English
DT Article
DE wallpaper groups; density function; colormap
ID CHAOTIC ATTRACTORS
AB A new algorithm is presented in this paper for the generation of colorful images with wallpaper symmetries by means of dynamical systems. Invariant functions are employed to construct density functions for the creation of colorful images.
C1 Shanghai Jiao Tong Univ, Dept Math, Shanghai 200240, Peoples R China.
   Shenzhen Univ, Coll Math & Computat Sci, Shenzhen 518060, Peoples R China.
C3 Shanghai Jiao Tong University; Shenzhen University
RP Lu, J (corresponding author), Shanghai Jiao Tong Univ, Dept Math, Shanghai 200240, Peoples R China.
EM jianlu@sjtu.edu.cn; zxye@sjtu.edu.cn; yrzou@163.com
OI Zou, Yuru/0000-0001-5184-739X
CR [Anonymous], 1988, Groups and Symmetry
   Carter NC, 1998, CHAOS SOLITON FRACT, V9, P2031, DOI 10.1016/S0960-0779(97)00157-4
   Chung KW, 2004, INT J BIFURCAT CHAOS, V14, P3249, DOI 10.1142/S0218127404011314
   Dumont JP, 2001, CHAOS SOLITON FRACT, V12, P761, DOI 10.1016/S0960-0779(00)00040-0
   Field M, 2001, COMPUT AIDED DESIGN, V33, P349, DOI 10.1016/S0010-4485(00)00127-5
   Field M.J., 1992, SYMMETRY IN CHAOS
   Jones KC, 2000, COMPUT GRAPH-UK, V24, P271, DOI 10.1016/S0097-8493(99)00161-2
   Lu JA, 2005, COMPUT GRAPH-UK, V29, P787, DOI 10.1016/j.cag.2005.08.008
   Pickover Clifford., 1990, Computers Patterns Chaos and Beauty
   ROBINSON C, 1994, DYNAMICAL SYSTEMS
NR 10
TC 15
Z9 16
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2007
VL 23
IS 6
BP 445
EP 449
DI 10.1007/s00371-007-0116-9
PG 5
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 165BC
UT WOS:000246277800006
DA 2024-07-18
ER

PT J
AU Kunii, TL
   Ohmori, K
AF Kunii, Tosiyasu L.
   Ohmori, Kenji
TI Cyberworlds: architecture and modeling by an incrementally modular
   abstraction hierarchy
SO VISUAL COMPUTER
LA English
DT Article
DE cyberworlds; cyberspaces; incrementally modular abstraction hierarchy;
   algebraic topology; fiber bundles; homotopy; adjunction spaces; cellular
   spaces
AB An incrementally modular abstraction hierarchy, which specifies, models and visualizes the architecture of cyberworlds from general to specific, is presented. The hierarchy, consisting of a homotopy level with fiber bundles, a set theoretical space level, a topological space level, an adjunction space level, a cellular space level, and presentation and view-levels, is described theoretically with examples of online book shopping such as e-commerce, seat assembling as such e-manufacturing, and accounting such as e-economy. Sharing invariants defined at each level contributes to robust architecture and modeling for designing, analyzing, implementing and visualizing cyberworlds, which results in a fault-free reduction of time and cost.
C1 Kanazawa Inst Technol, IT Inst, Shibuya Ku, Tokyo 1500001, Japan.
   Hosei Univ, Fac Comp & Informat Sci, Tokyo 1848584, Japan.
C3 Hosei University
RP Kunii, TL (corresponding author), Kanazawa Inst Technol, IT Inst, Shibuya Ku, 1-15-13 Jingumae, Tokyo 1500001, Japan.
EM tosi@kunii.info; ohmori@k.hosei.ac.jp
CR [Anonymous], 1997, A User's Guide to Algebraic Topology (Mathematics and Its Applications)
   Barabasi A., 2002, How Everything Is Connected to Everything Else and What It Means for Business, Science, and Everyday Life
   CHAUCHARD P, 1957, PRECIS BIOL HUMAINE
   Drucker P., 1969, The age of discontinuity: Guidelines to our changing society
   Hatcher A., 2003, ALGEBRAIC TOPOLOGY
   Huxley J., 1993, PROBLEMS RELATIVE GR
   Huxley J. S., 1936, Nature London, V137, P780, DOI 10.1038/137780b0
   Kennedy Paul, 1987, RISE FALL GREAT POWE
   Kunii T. L., 1999, International Journal of Shape Modeling, V5, P123, DOI 10.1142/S0218654399000137
   Kunii TL, 2005, 2005 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P3, DOI 10.1109/CW.2005.35
   Kunii TL, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P2, DOI 10.1109/CW.2004.63
   Kunii TL, 2005, LECT NOTES COMPUT SC, V3433, P204
   Kunii TL, 2005, IEICE T INF SYST, VE88D, P790, DOI 10.1093/ietisy/e88-d.5.790
   Kunii TL, 2003, 2003 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, pXX
   Kunii TL, 2003, LECT NOTES COMPUT SC, V2822, P86
   Kunii TL, 2002, LECT NOTES COMPUT SC, V2544, P58
   Kunii TL, 2003, IEICE T INF SYST, VE86D, P1181
   Kunii TL, 2002, FIRST INTERNATIONAL SYMPOSIUM ON CYBER WORLDS, PROCEEDINGS, P3, DOI 10.1109/CW.2002.1180853
   Kunii TL, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P130, DOI 10.1109/CGI.1999.777925
   KUNII TL, 1989, P 7 ANN C AUSTR SOC, P28
   KUNII TL, 1998, CYBERWORLDS, P5
   KUNII TL, 1985, VID P GRAPH INT 85 M
   KUNII TL, 1998, CYBERWORLDS, P19
   KUNII TL, 1984, VID CULT CAN 2 ANN I
   KUNII TL, 2001, PRACTICING GLOBAL OP
   KUNII TL, 1988, PAX JAPONICA
   KUNII TL, 1969, J MATH SCI, P54
   Kurtzman Joel., 1993, DEATH MONEY
   Morrow Glenn R., 1992, Proclus: A Commentary on the First Book of Euclid's Elements
   Ohmori K, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P126, DOI 10.1109/SMA.2001.923383
   Ohmori K, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P117, DOI 10.1109/CGI.2000.852327
   Pacioli L., 1963, PACIOLO ACCOUNTING
   SCHMALENBACH E, 1981, DYNAMIC ACCOUNTING
   Sieradski A.J., 1992, INTRO TOPOLOGY HOMOT
   Soros G., 1998, CRISIS GLOBAL CAPITA
   Spanier E., 1966, Algebraic Topology
   Stallman R., 2002, Free software, free society: Selected essays of Richard M. Stallman
   TEISSIER G., 1937, C R SOC BIOL, V124, P1071
   Whitehead J.H.C., 1950, Proceedings of the International Congress of Mathematicians, V2, P354
   [No title captured]
NR 40
TC 13
Z9 13
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2006
VL 22
IS 12
BP 949
EP 964
DI 10.1007/s00371-006-0040-4
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 120HE
UT WOS:000243074300002
DA 2024-07-18
ER

PT J
AU Jong, BS
   Tseng, JL
   Yang, WH
AF Jong, BS
   Tseng, JL
   Yang, WH
TI An efficient and low-error mesh simplification method based on torsion
   detection
SO VISUAL COMPUTER
LA English
DT Article
DE mesh simplification; quadric error metric; torsion detection
AB To preserve the major characteristics of the simplified model, this study proposes the use of torsion detection to improve the quadric error metric of vertex-pair contraction, and retain the physical features of the models. Besides keeping the physical features of the models, the proposed method also decreases the preprocessing time cost associated with analysis. To verify the conclusion, this research not only presents the effects of simplification and compares them with the vertex-pair contraction, but also employs Metro detection and image comparison to verify the error measurements. The experimental results demonstrate that the proposed method improves the error rate and keeps the precision of the object features efficiently.
C1 Chung Yuan Christian Univ, Dept Informat & Comp Engn, Chungli 32023, Taiwan.
   Chung Yuan Christian Univ, Dept Elect Engn, Chungli 32023, Taiwan.
   Chin Min Inst Technol, Dept Management Informat Syst, Tou Fen, Miao Li, Taiwan.
   Chin Min Inst Technol, Dept Elect Engn, Tou Fen, Miao Li, Taiwan.
C3 Chung Yuan Christian University; Chung Yuan Christian University
RP Chung Yuan Christian Univ, Dept Informat & Comp Engn, 22 Pu Jen,Pu Chung Li, Chungli 32023, Taiwan.
EM bsjong@ice.cycu.edu.tw; arthur@cg.ice.cycu.edu.tw;
   ericy@cg.ice.cycu.edu.tw
RI Tseng, Juin-Ling/AAE-5663-2022
OI Tseng, Juin-Ling/0000-0002-9569-8987
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   BERND H, 1994, COMPUT AIDED GEOM D, V11, P197, DOI DOI 10.1016/0167-8396(94)90032-9
   Chen BY, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P34, DOI 10.1109/CGI.2003.1214445
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Garland M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P117, DOI 10.1109/VISUAL.2002.1183765
   GARLAND M, 1999, CMUCS99105
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Garland M., 2001, I3D 01, P49, DOI [DOI 10.1145/364338.364345, 10.1145/364338.364345]
   Gieng TS, 1998, IEEE T VIS COMPUT GR, V4, P145, DOI 10.1109/2945.694956
   Gueziec A., 1995, Second Annual International Symposium on Medical Robotics and Computer Assisted Surgery, MRCAS '95, P132
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Kho Y., 2003, P S INTERACTIVE 3D G, P123
   LINSTROM P, 2000, P SIGGRAPH 2000 JUL, P259, DOI DOI 10.1145/344779.344912
   LINSTROM P, 2000, ACM T GRAPHIC, V19, P2004, DOI DOI 10.1145/353981.35995
   Liu YJ, 2003, VISUAL COMPUT, V19, P565, DOI 10.1007/s00371-003-0222-2
   Lodha SK, 2003, VISUAL COMPUT, V19, P493, DOI 10.1007/s00371-003-0214-2
   Rossignac J., 1993, Geometric Modeling in Computer Graphics, P455
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Spivak M., 1999, A Comprehensive Introduction toDifferential Geometry, V4
   Yan JQ, 2004, IEEE T VIS COMPUT GR, V10, P142, DOI 10.1109/TVCG.2004.1260766
NR 20
TC 18
Z9 20
U1 0
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2006
VL 22
IS 1
BP 56
EP 67
DI 10.1007/s00371-005-0356-5
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 991NT
UT WOS:000233821400006
DA 2024-07-18
ER

PT J
AU Nielsen, F
AF Nielsen, F
TI Surround video: a multihead camera approach
SO VISUAL COMPUTER
LA English
DT Article
DE virtual reality; stitching; environment mapping
ID CALIBRATION
AB We describe algorithms for creating, storing and viewing high-resolution immersive surround videos. Given a set of unit cameras designed to be almost aligned at a common nodal point, we first present a versatile process for stitching seamlessly synchronized streams of videos into a single surround video corresponding to the video of the multihead camera. We devise a general registration process onto raymaps based on minimizing a tailored objective function. We review and introduce new raymaps with good sampling properties. We then give implementation details on the surround video viewer and present experimental results on both real-world acquired and computer-graphics rendered full surround videos. We conclude by mentioning potential applications and discuss ongoing related activities.
C1 Sony Corp, Comp Sci Labs, Tokyo, Japan.
C3 Sony Corporation
RP Sony Corp, Comp Sci Labs, Tokyo, Japan.
EM Frank.Nielsen@acm.org
OI Nielsen, Frank/0000-0001-5728-0726
CR Aliaga DG, 2003, IEEE COMPUT GRAPH, V23, P22, DOI 10.1109/MCG.2003.1242379
   *AUSTR CTR MOV IM, 2005, MAG MACH HIST MOV IM
   Benosman R., 2001, PANORAMIC VISION SEN
   Cabral B, 1999, COMP GRAPH, P165, DOI 10.1145/311535.311553
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Conrady AE, 1918, MON NOT R ASTRON SOC, V79, P0384
   Coorg S, 2000, INT J COMPUT VISION, V37, P259, DOI 10.1023/A:1008184124789
   El-Melegy MT, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P554
   Grossberg MD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P108, DOI 10.1109/ICCV.2001.937611
   Hsu S, 2002, IEEE COMPUT GRAPH, V22, P44, DOI 10.1109/38.988746
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   MILGRAM DL, 1975, IEEE T COMPUT, V24, P1113, DOI 10.1109/T-C.1975.224142
   Nalwa V., 1996, A true omnidirectional viewer
   Nayar SK, 2000, PROC CVPR IEEE, P388
   NEUMANN J, 2003, IEEE COMP SOC C COMP, V2, P294
   Nielsen F, 2003, IEEE IMAGE PROC, P793
   SMOLIC A, 2004, ISOIECJTC1SC29WG11
   SNYDER JP, 2000, MAP PROJECTION TRANS
   Swaminathan R, 2000, IEEE T PATTERN ANAL, V22, P1172, DOI 10.1109/34.879797
   Szeliski Richard., 1997, P SIGGRAPH 97 COMPUT, P251, DOI DOI 10.1145/258734.258861
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Wong T-T, 1997, J GRAPHICS TOOLS, V2, P9, DOI DOI 10.1080/10867651.1997.10487471
   Xiong YL, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P69, DOI 10.1109/ACV.1998.732860
NR 23
TC 26
Z9 31
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2005
VL 21
IS 1-2
BP 92
EP 103
DI 10.1007/s00371-004-0273-z
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 911OG
UT WOS:000228013300007
DA 2024-07-18
ER

PT J
AU Park, SC
AF Park, SC
TI Triangular mesh intersection
SO VISUAL COMPUTER
LA English
DT Article
DE triangular mesh; intersection; self-intersection; computational geometry
ID POLYHEDRAL MODELS; SPHERE; TOOL
AB This paper proposes an efficient algorithm for finding self-intersections of a triangular mesh. It is very important to restrict, as much as possible, when and where the basic triangle-to-triangle intersection (TTI) algorithm is applied by taking advantage of the geometry and topology of a triangular mesh. To reduce the number of triangle pairs to be checked for intersection, the suggested algorithm employs the visibility information of triangles together with a conventional space-partitioning method. The visibility method works by topology, while the space-partitioning method works by geometry. The complementary nature of the two techniques enables additional improvement of the triangular mesh intersection process. The proposed algorithm has been implemented and tested with various examples. Some examples have been provided to illustrate the efficiency of the algorithm.
C1 Ajou Univ, Dept Ind Informat & Syst Engn, Suwon 443749, South Korea.
C3 Ajou University
RP Ajou Univ, Dept Ind Informat & Syst Engn, San 5, Suwon 443749, South Korea.
EM scpark@ajou.ac.kr
CR [Anonymous], 1952, Geometry and the Imagination
   CHEN LL, 1992, J MECH DESIGN, V114, P288, DOI 10.1115/1.2916945
   Choi B.K., 1998, Sculptured Surface Machining
   Jiménez P, 2003, COMPUT AIDED DESIGN, V35, P693, DOI 10.1016/S0010-4485(02)00099-4
   Jiménez P, 2001, COMPUT GRAPH-UK, V25, P269, DOI 10.1016/S0097-8493(00)00130-8
   Jiménez P, 2001, INT J ROBOT RES, V20, P466, DOI 10.1177/02783640122067499
   Johnson David E, 2001, P 2001 S INTERACTIVE, P129
   LEE YS, 1995, COMPUT AIDED DESIGN, V27, P715, DOI 10.1016/0010-4485(94)00021-5
   MOLLER T, 2002, REAL TIME RENDERING
   Park SC, 2002, COMPUT GRAPH-UK, V26, P341, DOI 10.1016/S0097-8493(02)00060-2
   Shamos M.I., 1976, 17 ANN S FDN COMP SC, P208, DOI [10.1109/SFCS.1976.16, DOI 10.1109/SFCS.1976.16]
   TOMAS M, 1997, J GRAPH TOOLS, V2, P25
   VOLINO P, 1995, P 6 EUR WORKSH COMP, P55
   WOO TC, 1994, COMPUT AIDED DESIGN, V26, P6, DOI 10.1016/0010-4485(94)90003-5
NR 14
TC 8
Z9 9
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2004
VL 20
IS 7
BP 448
EP 456
DI 10.1007/s00371-004-0251-5
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 863IZ
UT WOS:000224556200002
DA 2024-07-18
ER

PT J
AU Steiner, D
   Fischer, A
AF Steiner, D
   Fischer, A
TI Finding and defining the generators of genus-<i>n</i> objects for
   constructing topological and cut graphs
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 4th Israel-Korea Bi-National Conference on Geometric Modeling and
   Computer Graphics
CY FEB 12-14, 2003
CL Tel Aviv Univ, Tel Aviv, ISRAEL
HO Tel Aviv Univ
DE shape classification; topological graph; cut graph; longitude; meridian
AB The topology of an object is commonly represented through a topological graph or a cut graph (polygonal scheme). Over the past few years, many studies have focused on extracting the topological and cut graphs of complex freeform objects that are represented by meshes. For an object with genus-n, the topological graph has n cycles, while the cut graph contains 2n cycles. These loops, however, do not always explicitly represent the holes in the objects. That is, a cycle in the graph can be a cycle around a solid (meridian), a cycle around a hole (longitude), or almost any combination of the two. The task of classifying the cycles (generators) as cycles around holes (longitude) and cycles around solids (meridians) on the mesh is not straightforward.
   Every closed orientable 2-manifold with genus-n can be seen as a collection of n toruses stitched together, so that each hole in the object can be referred to as a torus with two generators. This paper proposes a method that extracts the generators from which the longitudes and the meridians are found. The topological graph is defined by the longitudes and by a spanning tree constructed between them. The cut graph is constructed using the same concept. The advantage of the proposed method over other methods is that each loop in the topological graph explicitly represents a hole in the object.
C1 Technion Israel Inst Technol, Dept Mech Engn, CMSR Lab Comp Graph & CAD, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Technion Israel Inst Technol, Dept Mech Engn, CMSR Lab Comp Graph & CAD, IL-32000 Haifa, Israel.
EM ovir@technion.ac.il; meranath@technion.ac.il
CR BIASOTTI S, 2001, P CENTR EUR SEM COMP, P10
   CORMEN HT, 1998, INTRO ALGORITHMS
   de Berg M., 2000, COMPUTATIONAL GEOMET
   DEY TK, 1995, DISCRETE COMPUT GEOM, V14, P93, DOI 10.1007/BF02570697
   Dey TK, 1999, J COMPUT SYST SCI, V58, P297, DOI 10.1006/jcss.1998.1619
   FISCHER A, 1999, P IEEE PAC GRAPH 99
   Floater MS, 1997, COMPUT AIDED GEOM D, V14, P231, DOI 10.1016/S0167-8396(96)00031-3
   Hatcher A., 2003, ALGEBRAIC TOPOLOGY
   HELIO H, 2002, P 7 ACM C SOL MOD AP
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   JEFF E, 2002, P S COMP GEOM BARC S
   Kartasheva E., 1999, International Journal of Shape Modeling, V5, P179, DOI 10.1142/S0218654399000162
   Kawauchi A., 1996, Survey on knot theory
   LAZARUS F, 1999, P ACM SOL MOD 99 ANN
   Lazarus F., 2001, P 17 ANN S COMPUTATI, P80, DOI DOI 10.1145/378583.378630
   LEE Y, 2001, P 3 ISR KOR BIN C GE, P105
   MUNKERS JR, 1984, ELEMENTS ALGEBRIC TO
   Sheffer Alla., 2000, P 9 INT MESHING ROUN, P161
   SHINAGAWA Y, 1991, IEEE COMPUT GRAPH, V11, P66, DOI 10.1109/38.90568
   Sorkine O, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P355, DOI 10.1109/VISUAL.2002.1183795
   Steiner D, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P82, DOI 10.1109/PCCGA.2001.962860
   STEINER D, 2002, P 7 ACM C SOL MOD AP
   Tai CL, 1998, COMPUT GRAPH, V22, P255, DOI 10.1016/S0097-8493(98)00036-3
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Tutte W.T., 1963, Proc. London Math. Soc., V13, P743, DOI DOI 10.1112/PLMS/S3-13.1.743
   ULRIKE A, 1998, MATH VISUALIZATION, P223
   VEGTER G, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P102, DOI 10.1145/98524.98546
   XIANFENG G, 2002, P SIGGRAPH 2002 SAN, P362
NR 28
TC 8
Z9 9
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2004
VL 20
IS 4
BP 266
EP 278
DI 10.1007/s00371-003-0232-0
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 834XR
UT WOS:000222444700006
DA 2024-07-18
ER

PT J
AU Schmitt, B
   Pasko, A
   Schlick, C
AF Schmitt, B
   Pasko, A
   Schlick, C
TI Constructive sculpting of heterogeneous volumetric objects using
   trivariate B-splines
SO VISUAL COMPUTER
LA English
DT Article
DE volume sculpting; heterogeneous object; trivariate B-spline; function
   representation; extended space mapping
AB This paper deals with modeling heterogeneous volumetric objects as point sets with attributes using trivariate B-splines. In contrast to homogeneous volumes with uniform distribution of material and other properties, a heterogeneous volumetric object has a number of variable attributes assigned at each point. An attribute is a mathematical model of an object property of an arbitrary nature (material, photometric, physical, statistical, etc.). In our approach, the function representation (FRep) is used as the basic model for both object geometry and attributes represented independently using real-valued scalar functions of point coordinates. While FRep directly defines object geometry, for an attribute it specifies a space partition used to define the attribute function. We propose a volume sculpting scheme with multiresolution capability based on trivariate B-spline functions to define both object geometry and its attributes. A new trivariate B-spline primitive is proposed that can be used as a leaf in an FRep constructive tree. An interactive volume modeler based on trivariate B-splines and other simple primitives is described, with a real-time repolygonization of the surface during modeling. We illustrate that the space partition obtained in the modeling process can be applied to define attributes for the objects with an arbitrary geometry model such as BRep or homogeneous volume models.
C1 Univ Bordeaux, LaBRI, Talence, France.
   Hosei Univ, Dept Digital Media, Koganei, Tokyo 1848584, Japan.
C3 Universite de Bordeaux; Centre National de la Recherche Scientifique
   (CNRS); Hosei University
RP Univ Bordeaux, LaBRI, 351 Cours Liberat, Talence, France.
RI Pasko, Alexander/H-9344-2017
OI Pasko, Alexander/0000-0002-4785-7066
CR Arata H, 1999, SHAPE MODELING INTERNATIONAL '99 - INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P242, DOI 10.1109/SMA.1999.749346
   BAERENTZEN A, 2002, J SHAPE MODELL, V8, P79
   BAERENTZEN A, 1998, IEEE VIS 98 LAT BREA, P9
   Chen M, 2000, COMPUT GRAPH FORUM, V19, P281, DOI 10.1111/1467-8659.00464
   COQUILLART S, 1988, COMPUT GRAPH, V24, P205
   DEROSE TD, 1995, COMPUT GRAPH APPL, P75
   Elber G., 1995, Proceedings of the Third Pacific Conference on Computer Graphics and Applications Pacific Graphics '95. Computer Graphics and Applications, P267
   Farin G., 1993, Curves and Surfaces for Computer Aiaded Geometric Design, V3
   Ferley E, 2000, VISUAL COMPUT, V16, P469, DOI 10.1007/PL00007216
   Forsey D. R., 1988, Computer Graphics, V22, P205, DOI 10.1145/378456.378512
   Hua J, 2002, IEEE/ACM SIGGRAPH SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2002, PROCEEDINGS, P55, DOI 10.1109/SWG.2002.1226510
   Kumar V, 1999, COMPUT AIDED DESIGN, V31, P541, DOI 10.1016/S0010-4485(99)00051-2
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   MARTIN W, 2001, 6 ACM S SOL MOD APPL, P234
   MENON J, 1994, IEEE COMPUT GRAPH, V14, P22, DOI 10.1109/38.279039
   MIURA KT, 1996, P CSG 96, P217
   PARENT R, 1988, COMPUT GRAPH, V11, P138
   PARK SM, 2001, 6 ACM S SOL MOD APPL, P216
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Pasko A, 2001, GRAPH MODELS, V63, P413, DOI 10.1006/gmod.2001.0560
   PASKO A, 2001, TRNCCA200101 BOURN U, P34
   PASKO AA, 1988, COMPUT GRAPH, V12, P457, DOI 10.1016/0097-8493(88)90070-2
   Raviv A, 2000, COMPUT AIDED DESIGN, V32, P513, DOI 10.1016/S0010-4485(00)00039-7
   RAVIV A, 1999, CIS9903
   REQUICHA AAG, 1980, COMPUT SURV, V12, P436
   ROSSIGNAC J, 1994, GEOMETRIC MODELING A, P1
   Savchenko V, 1998, VISUAL COMPUT, V14, P257, DOI 10.1007/s003710050139
   Schmitt B, 2000, CISST'2000: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY, VOLS I AND II, P475
   SCHMITT B, 2001, 6 ACM S SOL MOD APPL, P321
   Schmitt Benjamin., 2003, GRAPHITE P 1 INT C C, P127
   Schmuki P, 1999, ELEC SOC S, V99, P25
   SNYDER J, 1992, ACM SIGGRAPH COMPUT, V26
   WANG S, 1995, S INT 3D GRAPH, P151
   WINTER A, 2001, P 2 INT WORKSH VOL G, P81
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   HYPERFUN PROJECT LAN
   POVRAY PERSISTANCE V
   TCL TK
   [No title captured]
   MAM VRS VIRTUAL REND
NR 40
TC 9
Z9 11
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 130
EP 148
DI 10.1007/s00371-003-0236-9
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900004
DA 2024-07-18
ER

PT J
AU Wu, CH
   Lai, YY
   Tai, WK
AF Wu, CH
   Lai, YY
   Tai, WK
TI A hybrid-based texture synthesis approach
SO VISUAL COMPUTER
LA English
DT Article
DE texture synthesis; image processing; patch-based sampling
AB Texture synthesis has been an active research area in computer graphics, vision, and image processing. A hybrid texture synthesis approach that combines the strength of Ashikhmin's [1] and Liang's [11] algorithms is presented in this paper. Using patches from the input sample texture to extend the input itself, more suitable valid candidate pixels in the input sample can be more precisely determined. An extra original position array is used to record the valid pixel position in the extended region of input sample texture such that the candidate pixel can be retrieved efficiently. Pasting patches with more global information of texture features to initialize the output texture, the texture synthesis process tends to grow larger pieces in the output texture so that the perceptual similarity is reserved as much as possible. Many classes of textures have been used to test our approach. As the experimental results showed, the proposed hybrid approach effectively and efficiently synthesizes high-quality textures from a wide variety of textures.
C1 Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Shoufeng 974, Hualien, Taiwan.
C3 National Dong Hwa University
RP Tai, WK (corresponding author), Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, 1 Sect 2,Da Hsueh Rd, Shoufeng 974, Hualien, Taiwan.
EM wkdai@mail.ndhu.edu.tw
CR [Anonymous], P SIGGRAPHS, DOI DOI 10.1145/218380.218446
   ASHIKHMIN A, 2001, P ACM S INT 3D GRAPH, P217
   DEBONET JS, 1997, P SIGGRAPH 97, P361, DOI DOI 10.1145/258734.258882
   Dischler JM, 2002, COMPUT GRAPH FORUM, V21, P401, DOI 10.1111/1467-8659.t01-1-00600
   EFORS AA, 2001, P ACM SIGGRAPH, P341
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   FOURNIER A, 1982, COMMUN ACM, V25, P371, DOI 10.1145/358523.358553
   HARALICK RM, 1986, HDB PATTERN RECOGNIT, P247
   HERTZMANN A, 2001, P ACM SIGGRAPH 2001, P12
   IVERSEN H, 1994, PATTERN RECOGN LETT, V15, P575, DOI 10.1016/0167-8655(94)90018-3
   Liang L, 2001, ACM T GRAPHIC, V20, P127, DOI 10.1145/501786.501787
   Mount D.M., 1998, ANN programming manual
   Rao A., 1990, TAXONOMY TEXTURE DES
   Simoncelli EP, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P62, DOI 10.1109/ICIP.1998.723417
   Tong X, 2002, ACM T GRAPHIC, V21, P665, DOI 10.1145/566570.566634
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   WITKIN A, 1991, COMP GRAPH, V25, P299, DOI 10.1145/127719.122750
   Worley S., 1996, P 23 ANN C COMPUTER, P291, DOI [DOI 10.1145/237170.237267, 10.1145/237170.237267]
   Xu Y., 2000, Technical Report MSR-TR-2000-32
   Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627
NR 20
TC 2
Z9 4
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2004
VL 20
IS 2-3
BP 106
EP 129
DI 10.1007/s00371-003-0235-x
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 818ZV
UT WOS:000221283900003
DA 2024-07-18
ER

PT J
AU Nasri, AH
   Kim, TW
   Lee, K
AF Nasri, AH
   Kim, TW
   Lee, K
TI Polygonal mesh regularization for subdivision surfaces interpolating
   meshes of curves
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on Shape Modeling and Applications (SMI
   2001)
CY MAY 07-11, 2001
CL GENOA, ITALY
SP Consiglio Nazl Ricerche
DE subdivision surfaces; curve interpolation; B-spline; fairing; polyhedral
   meshes; Laplacian; curvature
ID RECURSIVE SUBDIVISION; CONSTRAINTS; TAXONOMY; SCHEME
AB A recursive-subdivision surface interpolating a mesh of curves can be generated from a given polygonalmesh (or a polyhedron) and some tagged control polygons by constructing a polygonal complex for each of these polygons. This process will modify the geometry, and possibly the topology, of the polygonal mesh and could result in poorly shaped surfaces across the interpolated curves. The problem can be minimized by applying some fairing procedure that regularly repositions the vertices of the mesh. This paper provides an approach to regularize a polygonal mesh based on the Laplacian and mean curvature of the vertices. The results are useful, especially if further constraints such as normal or cross curvature are imposed across the interpolated curves, where more irregularity can be introduced on the polygonalmesh.
C1 Amer Univ Beirut, Dept Math & Comp Sci, Beirut, Lebanon.
   Seoul Natl Univ, Dept Naval Architecture & Ocean Engn, Seoul 151742, South Korea.
   Seoul Natl Univ, Sch Mech & Aerosp Engn, Seoul 151, South Korea.
C3 American University of Beirut; Seoul National University (SNU); Seoul
   National University (SNU)
RP Amer Univ Beirut, Dept Math & Comp Sci, POB 11-236, Beirut, Lebanon.
EM anasri@aub.edu.lb; kimtw@gong.snu.ac.kr; kunwoo@snu.ac.kr
RI Kim, Tae-wan/E-9982-2011
OI Nasri, Ahmad/0000-0002-2047-6693
CR [Anonymous], 1996, THESIS U WASHINGTON
   BALL A, 1984, P COMPUTER AIDED DES, V84, P112
   Biermann H, 2000, COMP GRAPH, P113, DOI 10.1145/344779.344841
   Catmull E., 1998, SEMINAL GRAPHICS, P183, DOI DOI 10.1145/280811.280992
   DeRose T., 1998, Computer Graphics (SIGGRAPH 98 Proceedings), P85, DOI DOI 10.1145/280814.280826
   Doo D., 1998, SEMINAL GRAPHICS POI, P177
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Kobbelt L, 2000, COMP GRAPH, P103, DOI 10.1145/344779.344835
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Levin A, 1999, COMP GRAPH, P57, DOI 10.1145/311535.311541
   Loop C, 1987, THESIS U UTAH
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Nasri A, 1997, COMPUT AIDED GEOM D, V14, P13, DOI 10.1016/S0167-8396(96)00018-0
   Nasri A. H., 1991, Computer-Aided Geometric Design, V8, P89, DOI 10.1016/0167-8396(91)90051-C
   Nasri A. H., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P262, DOI 10.1109/GMAP.2000.838258
   Nasri AH, 2000, COMPUT AIDED GEOM D, V17, P595, DOI 10.1016/S0167-8396(00)00015-7
   Nasri AH, 2002, VISUAL COMPUT, V18, P382, DOI 10.1007/s003710100155
   Nasri AH, 2002, VISUAL COMPUT, V18, P259, DOI 10.1007/s003710100154
   Ohtake Y., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P229, DOI 10.1109/GMAP.2000.838255
   Peters J, 1997, ACM T GRAPHIC, V16, P420, DOI 10.1145/263834.263851
   PETERS J, 2000, MATH SURFACES, V9
   PRAUTZSCH H, 1995, ANAL CSTAR SUBDIVISI
   REIF U, 1995, COMPUT AIDED GEOM D, V12, P153, DOI 10.1016/0167-8396(94)00007-F
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Weimer H, 1999, COMP GRAPH, P111, DOI 10.1145/311535.311547
   ZHENG X, 2002, FILLET OPERATIONS RE
   Zorin D., 2000, Subdivision for Modeling and Animation
NR 27
TC 4
Z9 4
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 80
EP 93
DI 10.1007/s00371-002-0178-7
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 691AU
UT WOS:000183583300002
DA 2024-07-18
ER

PT J
AU Ros, L
   Sugihara, K
   Thomas, F
AF Ros, L
   Sugihara, K
   Thomas, F
TI Towards shape representation using trihedral mesh projections
SO VISUAL COMPUTER
LA English
DT Article
DE shape representation; polygonization; trihedral mesh; reconstruction
   from projections; AND/OR graphs
ID POLYGONIZATION; POLYHEDRA; ALGORITHM
AB This paper explores the possibility of approximating a surface by a trihedral polygonal mesh plus some triangles at strategic places. The presented approximation has attractive properties. It turns out that the Z-coordinates of the vertices are completely governed by the Z-coordinates assigned to four selected ones. This allows describing the spatial polygonal mesh with just its 2D projection plus the heights of four vertices. As a consequence, these projections essentially capture the "spatial meaning" of the given surface, in the sense that, whatever spatial interpretations are drawn from them, they all exhibit essentially the same shape.
C1 UPC, Inst Robot & Informat Ind, CSIC, Barcelona 08028, Spain.
   Univ Tokyo, Dept Math Informat, Bunkyo Ku, Tokyo 1138656, Japan.
C3 Universitat Politecnica de Catalunya; Consejo Superior de
   Investigaciones Cientificas (CSIC); CSIC - Institut de Robotica i
   Informatica Industrial (IRII); University of Tokyo
RP UPC, Inst Robot & Informat Ind, CSIC, Llorens Artigas 4-6, Barcelona 08028, Spain.
EM llros@iri.upc.es; sugihara@simplex.t.u-tokyo.ac.jp; fthomas@iri.upc.es
RI Torras, Carme/M-1794-2014; Sugihara, Kokichi/IWM-4256-2023; Ros,
   Lluis/L-7914-2014
OI Torras, Carme/0000-0002-2933-398X; Ros, Lluis/0000-0002-8338-6062
CR Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   BOOR C, 1987, COMPUT AIDED GEOM D, V4, P125
   Chuang JH, 1998, VISUAL COMPUT, V14, P455, DOI 10.1007/s003710050155
   Coxeter H. S. M., 1987, PROJECTIVE GEOMETRY, V2nd
   Fox DE, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P426, DOI 10.1109/CGI.1998.694296
   Heckbert P., 1997, Multiresolution Surface Modeling Course Notes of SIGGRAPH '97
   HOSCHEK J, 1983, SURFACES COMPUTER AI, P147
   Jiménez P, 2000, ARTIF INTELL, V124, P1, DOI 10.1016/S0004-3702(00)00063-1
   Ros L, 2002, IEEE T PATTERN ANAL, V24, P456, DOI 10.1109/34.993554
   ROS L, 2000, THESIS POLYTECHNIC U
   Seibold W, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P416, DOI 10.1109/CGI.1998.694295
   Sugihara K, 1999, DISCRETE COMPUT GEOM, V21, P243, DOI 10.1007/PL00009419
   SUGIHARA K, 1984, ARTIF INTELL, V23, P59, DOI 10.1016/0004-3702(84)90005-5
   Sugihara K., 1986, MACHINE INTERPRETATI
   *U MINN, 2002, QHULL HOM PAG GEOM
   WHITELEY W, 1989, DISCRETE COMPUT GEOM, V4, P75, DOI 10.1007/BF02187716
   WHITELEY W, 1994, J INTELL ROBOT SYST, V11, P135, DOI 10.1007/BF01258299
   Wünsche B, 1999, VISUAL COMPUT, V15, P36, DOI 10.1007/s003710050161
NR 18
TC 3
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2003
VL 19
IS 2-3
BP 139
EP 150
DI 10.1007/s00371-002-0176-9
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 691AU
UT WOS:000183583300007
DA 2024-07-18
ER

PT J
AU Chang, YX
   Shih, ZC
AF Chang, YX
   Shih, ZC
TI The synthesis of rust in seawater
SO VISUAL COMPUTER
LA English
DT Article
DE texture synthesis; metallic corrosion; L-systems
AB Computer graphics researchers have recently revealed that the presence of imperfections is an essential part of the illusion of photorealism. An improved model to simulate the rust development of ferrous metals in seawater is proposed. The tide and the current, under the gravity of the sun and the moon, affect the stability of the environment at the bottom of the ocean. A simple current model is employed to elucidate the periodic transition of currents in the ocean. L-systems, coupled with the current model, modulate the diffusion of rust according to tendencies based on the object geometry and environmental factors. Changing the underlying distributions of tendency allows images of a variety of metallic rusts to be generated.
C1 Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 30050, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Natl Chiao Tung Univ, Dept Comp & Informat Sci, Hsinchu 30050, Taiwan.
EM gis82548@cis.nctu.edu.tw; zcshih@cc.nctu.edu.tw
CR Becket W., 1990, Journal of Visualization and Computer Animation, V1, P26, DOI 10.1002/vis.4340010108
   Blinn J. F., 1982, Computer Graphics, V16, P21, DOI 10.1145/965145.801255
   Calladine C. R., 1986, Mathematics of Surfaces. Proceedings of a Conference, P179
   CARIG BD, 1989, HDB CORROSION DATA
   Chandler K.A., 1985, Marine and Offshore Corrosion
   Chang YX, 2000, COMPUT GRAPH FORUM, V19, pC109, DOI 10.1111/1467-8659.00403
   Dorsey J, 2000, SCI AM, V282, P64, DOI 10.1038/scientificamerican0200-64
   DORSEY J, 1999, COMPUT GRAPH, V33, P225
   DORSEY J, 1996, COMPUT GRAPH, V30, P387
   DORSEY J, 1996, COMPUT GRAPH, V30, P411
   Fontana Mars G., 1986, Corrosion Engineering, V3rd
   Gobron S, 1999, J VISUAL COMP ANIMAT, V10, P143, DOI 10.1002/(SICI)1099-1778(199907/09)10:3<143::AID-VIS204>3.0.CO;2-W
   HAASE CS, 1992, ACM T GRAPHIC, V11, P305, DOI 10.1145/146443.146452
   Hall R., 1989, ILLUMINATION COLOR C
   Hanan JS, 1992, THESIS U REGINA
   HECKBERT PS, 1986, IEEE COMPUT GRAPH, V6, P56, DOI 10.1109/MCG.1986.276672
   HSU SC, 1995, IEEE COMPUT GRAPH, V15, P18, DOI 10.1109/38.364957
   Jones D.A., 1992, PRINCIPLES PREVENTIO
   Judd DB., 1975, Color in business, science, and industry /
   LINDENMAYER A, 1968, J THEOR BIOL, V18, P280, DOI 10.1016/0022-5193(68)90079-9
   MECH R, 1996, COMPUTER GRAPHICS, V30, P397
   MERCER AD, 1990, CORROSION SEAWATER S
   Merillou S., 2001, P GRAPHICS INTERFACE, P167
   MILLER G, 1994, COMPUT GRAPHICS, V28, P319
   NEWMAN RC, 1994, SCIENCE, V263, P1708, DOI 10.1126/science.263.5154.1708
   Noser H, 1999, IEEE T VIS COMPUT GR, V5, P281, DOI 10.1109/2945.817347
   PARISH YIH, 2001, COMPUT GRAPH, V35, P301
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   Prusinkiewicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P351, DOI 10.1145/192161.192254
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   ROGERS TH, 1971, MARINE CORROSION
   ROSS TK, 1977, METAL CORROSION
   SHREIR LL, 1976, CORROSION METAL ENV
   Touloukian Y.S., 1966, Thermophysical Properties of High Temperature Solid Materials
   Trethewey K.R., 1988, CORROSION STUDENTS S
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Warniers R, 1998, COMPUT GRAPH WORLD, V21, P50
   WHITAKER JW, 1980, PHYSIOLOGIST, V23, P6
   WONG TT, 1997, P 8 EUR WORKSH REND, P139
NR 39
TC 19
Z9 24
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2003
VL 19
IS 1
BP 50
EP 66
DI 10.1007/s00371-002-0172-0
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 671MB
UT WOS:000182467600005
DA 2024-07-18
ER

PT J
AU Xi, CC
   Zhang, KB
   He, X
   Hu, YT
   Chen, JG
AF Xi, Chenchen
   Zhang, Kaibing
   He, Xin
   Hu, Yanting
   Chen, Jinguang
TI Soft-edge-guided significant coordinate attention network for scene text
   image super-resolution
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Scene text image super-resolution; Scene text recognition; Soft edge;
   Significant coordinate attention
ID NEURAL-NETWORK; CLASSIFICATION
AB Scene text image super-resolution (STISR) aims to enhance the resolution and visual quality of low-resolution scene text images, thereby improving the performance of some text-related downstream vision tasks. However, many existing STISR methods treat scene text images as general images while ignoring text-specific properties such as the particular structure of text images. Although some methods elaborated on introducing a certain edge detection operator to obtain the hard edges for improving the quality of super-resolved images, the extracted hard edges are binary and prone to generate aliasing edges. In view of the above considerations, we propose a novel soft-edge-guided significant coordinate attention network for STISR. Specifically, we apply soft edges to assist text image super-resolution, which is the probabilistic edges that can reflect a complete edge description on text images. In addition, some proposed approaches exploit both channel and spatial attention for effective image enhancement, but they all ignore the location information hiding in text images. To explore the key position-dependent features embedded in scene text images, we elaborately incorporate the coordinate attention into the process of STISR, which can capture long-term dependencies in one spatial direction while retaining precise position information in another one. Furthermore, we propose a new attention mechanism, called significant coordinate attention, to enable the network to focus more on the significant text region. The extensive experimental results demonstrate that our newly proposed method performs favorably against state-of-the-art methods in terms of both quantitative and qualitative assessments. The code will be available at https://github.com/kbzhang0505/SegSCoAN.
C1 [Xi, Chenchen; Zhang, Kaibing; He, Xin] Xian Polytech Univ, Sch Elect & Informat, Xian 710048, Peoples R China.
   [Zhang, Kaibing; Chen, Jinguang] Xian Polytech Univ, Sch Comp Sci, Shaanxi Key Lab Clothing Intelligence, Xian 710048, Peoples R China.
   [Zhang, Kaibing; Chen, Jinguang] Xian Polytech Univ, Sch Elect & Informat, Xian 710048, Peoples R China.
   [Hu, Yanting] Xinjiang Med Univ, Sch Med Engn & Technol, Urumqi 830054, Peoples R China.
C3 Xi'an Polytechnic University; Xi'an Polytechnic University; Xi'an
   Polytechnic University; Xinjiang Medical University
RP Zhang, KB (corresponding author), Xian Polytech Univ, Sch Elect & Informat, Xian 710048, Peoples R China.; Zhang, KB (corresponding author), Xian Polytech Univ, Sch Comp Sci, Shaanxi Key Lab Clothing Intelligence, Xian 710048, Peoples R China.; Zhang, KB (corresponding author), Xian Polytech Univ, Sch Elect & Informat, Xian 710048, Peoples R China.
EM xichenchen@stu.xpu.edu.cn; zhangkaibing@xpu.edu.cn;
   hexin@stu.xpu.edu.cn; yantinghu2012@gmail.com; xacjg@163.com
RI He, Xin/JYQ-1624-2024; 胡, 艳婷/AAB-7048-2021; Zhang, Kaibing/AFS-4658-2022
OI 胡, 艳婷/0000-0003-4848-6137; 
FU This work was supported in part by the National Natural Science
   Foundation of China under Grant 61971339, Grant 62061047, and Grant
   61471161, in part by the Textile Intelligent Equipment Information and
   Control Innovation Team of Shaanxi Innovation Ability [61971339,
   62061047, 61471161]; National Natural Science Foundation of China
   [2021TD-29]; Textile Intelligent Equipment Information and Control
   Innovation Team of Shaanxi Innovation Ability Support Program; Textile
   Intelligent Equipment Information and Control Innovation Team of Shaanxi
   Innovation Team of Universities [2020D01C157]; Natural Science
   Foundation of Xinjiang Uygur Autonomous Region [2018JZ6002]; Key Project
   of the Natural Science Foundation of Shaanxi Province
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61971339, Grant 62061047, and Grant
   61471161, in part by the Textile Intelligent Equipment Information and
   Control Innovation Team of Shaanxi Innovation Ability Support Program
   under Grant 2021TD-29, in part by the Textile Intelligent Equipment
   Information and Control Innovation Team of Shaanxi Innovation Team of
   Universities, in part by the Natural Science Foundation of Xinjiang
   Uygur Autonomous Region under Grant 2020D01C157, and in part by the Key
   Project of the Natural Science Foundation of Shaanxi Province under
   Grant 2018JZ6002.
CR Amaranageswarao G, 2022, VISUAL COMPUT, V38, P31, DOI 10.1007/s00371-020-01998-z
   Ben Salah K, 2022, VISUAL COMPUT, V38, P1833, DOI 10.1007/s00371-021-02108-3
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Chudasama V, 2022, VISUAL COMPUT, V38, P3643, DOI 10.1007/s00371-021-02193-4
   Cui MM, 2021, LECT NOTES COMPUT SC, V12824, P156, DOI 10.1007/978-3-030-86337-1_11
   Dong C, 2015, Arxiv, DOI arXiv:1506.02211
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang CT, 2021, NEUROCOMPUTING, V455, P88, DOI 10.1016/j.neucom.2021.05.060
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Gao WS, 2010, INT CONF COMP SCI, P67, DOI 10.1109/ICCSIT.2010.5563693
   Gou Yuanbiao, 2020, P ADV NEUR INF PROC, V33, P17129, DOI DOI 10.5555/3495724.3497161
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li BY, 2022, PROC CVPR IEEE, P17431, DOI 10.1109/CVPR52688.2022.01693
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AQ, 2023, VISUAL COMPUT, V39, P3837, DOI 10.1007/s00371-022-02519-w
   Liu Y, 2022, VISUAL COMPUT, V38, P3377, DOI 10.1007/s00371-022-02551-w
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Ma J., 2021, arXiv
   Mao XJ, 2016, ADV NEUR IN, V29
   Peyrard C, 2015, PROC INT CONF DOC, P1201, DOI 10.1109/ICDAR.2015.7333951
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi WL, 2021, VISUAL COMPUT, V37, P1569, DOI 10.1007/s00371-020-01903-8
   Pham TA, 2023, VISUAL COMPUT, V39, P927, DOI 10.1007/s00371-021-02375-0
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang F., 2022, Vis. Comput., P1
   Wang WJ, 2019, Arxiv, DOI arXiv:1909.07113
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wenjia Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P650, DOI 10.1007/978-3-030-58607-2_38
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu XY, 2017, IEEE I CONF COMP VIS, P251, DOI 10.1109/ICCV.2017.36
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Yao C, 2016, Arxiv, DOI arXiv:1511.09207
   Yongqiang Mou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P158, DOI 10.1007/978-3-030-58555-6_10
   Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang YT, 2019, J APPL REMOTE SENS, V13, DOI 10.1117/1.JRS.13.026507
   Zhao S, 2024, VISUAL COMPUT, V40, P559, DOI 10.1007/s00371-023-02801-5
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhu HY, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10101187
NR 51
TC 1
Z9 1
U1 9
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD 2023 OCT 8
PY 2023
DI 10.1007/s00371-023-03111-6
EA OCT 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T4FU5
UT WOS:001077570200001
DA 2024-07-18
ER

PT J
AU Roy, M
   Mukhopadhyay, S
AF Roy, Manali
   Mukhopadhyay, Susanta
TI A DCT-based multiscale framework for 2D greyscale image fusion using
   morphological differential features
SO VISUAL COMPUTER
LA English
DT Article
DE Multi-focus; Multi-sensor; Information fusion; Multi-exposure;
   Multiscale morphology; DCT
ID DISCRETE COSINE TRANSFORM; TOP-HAT TRANSFORM; FOCUS MEASURE; DENSE SIFT;
   EXTRACTION; ENSEMBLE; REGIONS; DOMAIN
AB Image fusion refers to the process of synergistic combination of useful sensory information from multiple images to synthesize a composite image with greater information content and increased practical value. It aims to maximize pertinent information specific to a sensor while minimizing uncertainty and redundancy in the fused output. In this paper, the authors have proposed a simple yet cohesive framework for 2D greyscale image fusion using morphological differential features. The features are extracted using morphological open-close filters applied at multiple scales using an isotropic structuring element which brings out categorical bright and dark features from the source images. At each scale, the bright (and dark) differential features are mutually compared using higher-valued AC coefficients obtained in the DCT domain within a block. The scale-specific fused features are recursively added to form an image containing high-frequency information from all conceivable scales. The fused image is achieved by superimposing the cumulative feature image onto a suitable base image. The base image is obtained by using a morphological weighted version of pseudomedian filter over the source images using the largest homothetic of the structuring element. The superiority of the framework is empirically verified in different domains of fusion, i.e. multi-focus, multi-sensor, multi-exposure, and multi-spectral image fusion. The proposed approach has surpassed the state-of-the-art unified fusion algorithms in terms of qualitative and quantitative evaluation with a perfect resource-time trade-off. Furthermore, the proposed method has been extended to greyscale-colour and colour-colour image pairs qualifying it for anatomical-functional image fusion.
C1 [Roy, Manali; Mukhopadhyay, Susanta] Indian Inst Technol ISM, Dept CSE, Dhanbad 826004, Jharkhand, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (Indian School of Mines) Dhanbad
RP Roy, M (corresponding author), Indian Inst Technol ISM, Dept CSE, Dhanbad 826004, Jharkhand, India.
EM manalir66@gmail.com; msushanta2001@gmail.com
OI Roy, Manali/0000-0001-9968-3806
CR Aiazzi B, 2007, IEEE T GEOSCI REMOTE, V45, P3230, DOI 10.1109/TGRS.2007.901007
   Amin-Naji M., 2017, ARXIV
   Amin-Naji M, 2017, IRAN CONF MACH, P45, DOI 10.1109/IranianMVIP.2017.8342367
   Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bai XZ, 2015, INFRARED PHYS TECHN, V71, P77, DOI 10.1016/j.infrared.2015.03.001
   Bai XZ, 2013, INFRARED PHYS TECHN, V60, P81, DOI 10.1016/j.infrared.2013.03.002
   Bai XZ, 2013, OPTIK, V124, P1660, DOI 10.1016/j.ijleo.2012.06.029
   Bai XZ, 2013, DIGIT SIGNAL PROCESS, V23, P542, DOI 10.1016/j.dsp.2012.11.001
   Bai XZ, 2011, IMAGE VISION COMPUT, V29, P829, DOI 10.1016/j.imavis.2011.09.003
   Cao L, 2015, IEEE SIGNAL PROC LET, V22, P220, DOI 10.1109/LSP.2014.2354534
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Choi J, 2011, IEEE T GEOSCI REMOTE, V49, P295, DOI 10.1109/TGRS.2010.2051674
   Choi Y, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020245
   De I, 2006, SIGNAL PROCESS, V86, P924, DOI 10.1016/j.sigpro.2005.06.015
   De I, 2013, INFORM FUSION, V14, P136, DOI 10.1016/j.inffus.2012.01.007
   Dou JF, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.9.097002
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fang C., 2022, P IEEE CVF C COMP VI, p20,677
   Fang C., 2022, ARXIV
   Feng Y, 2022, IET CIRC DEVICE SYST, V16, P483, DOI 10.1049/cds2.12121
   Garzelli A, 2009, IEEE GEOSCI REMOTE S, V6, P662, DOI 10.1109/LGRS.2009.2022650
   Gong MG, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3139077
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P744, DOI 10.1016/j.compeleceng.2011.07.012
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P789, DOI 10.1016/j.compeleceng.2011.04.016
   Hayat N, 2019, J VIS COMMUN IMAGE R, V62, P295, DOI 10.1016/j.jvcir.2019.06.002
   Hermessi H, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108036
   Huang ZB, 2022, LECT NOTES COMPUT SC, V13678, P539, DOI 10.1007/978-3-031-19797-0_31
   Jiang Y, 2014, INFORM FUSION, V18, P107, DOI 10.1016/j.inffus.2013.06.001
   Jin X, 2018, INFRARED PHYS TECHN, V88, P1, DOI 10.1016/j.infrared.2017.10.004
   Kaur H, 2021, ARCH COMPUT METHOD E, V28, P4425, DOI 10.1007/s11831-021-09540-7
   Kumar B., 2013, P CAN C EL COMP ENG, P1, DOI DOI 10.1109/CCECE.2013.6567721
   Li JW, 2023, INFRARED PHYS TECHN, V128, DOI 10.1016/j.infrared.2022.104486
   Li W, 2018, OPTIK, V172, P1, DOI 10.1016/j.ijleo.2018.06.123
   Liu J., 2022, VISUAL COMPUT, V1, P1
   Liu J., 2022, P IEEECVF C COMPUTER, P5802
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu JY, 2021, IEEE SIGNAL PROC LET, V28, P1818, DOI 10.1109/LSP.2021.3109818
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, J VIS COMMUN IMAGE R, V31, P208, DOI 10.1016/j.jvcir.2015.06.021
   Liu Y, 2013, IEEE CONF IMAGING SY, P288, DOI 10.1109/IST.2013.6729708
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   MATSOPOULOS GK, 1995, J VIS COMMUN IMAGE R, V6, P196, DOI 10.1006/jvci.1995.1018
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Nie XX, 2021, NEUROCOMPUTING, V465, P93, DOI 10.1016/j.neucom.2021.08.109
   Paramanandham N, 2018, INFRARED PHYS TECHN, V88, P13, DOI 10.1016/j.infrared.2017.11.006
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Purkait P, 2012, IEEE T IMAGE PROCESS, V21, P4029, DOI 10.1109/TIP.2012.2201492
   Ramlal SD, 2018, SIGNAL IMAGE VIDEO P, V12, P1479, DOI 10.1007/s11760-018-1303-z
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Restaino R, 2016, IEEE T IMAGE PROCESS, V25, P2882, DOI 10.1109/TIP.2016.2556944
   Schulze M. A., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P57, DOI 10.1109/ICASSP.1993.319746
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Shreyamsha Kumar BK, 2013, SIGNAL IMAGE VIDEO P, V7, P1125, DOI 10.1007/s11760-012-0361-x
   Tang JS, 2004, DIGIT SIGNAL PROCESS, V14, P218, DOI 10.1016/j.dsp.2003.06.001
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Tushar, 2022, ICDSMLA 2020: Proceedings of the 2nd International Conference on Data Science, Machine Learning and Applications. Lecture Notes in Electrical Engineering (783), P1607, DOI 10.1007/978-981-16-3690-5_152
   Wald L., 2002, Data Fusion: Definitions and Architectures: Fusion of Images of Different Spatial Resolutions
   Wang D, 2022, ARXIV
   Wang R, 2022, ADV APPL CLIFFORD AL, V32, DOI 10.1007/s00006-021-01197-6
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Xu H., 2022, P IEEE CVF C COMP VI, P688
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu ZP, 2014, INFORM FUSION, V19, P38, DOI 10.1016/j.inffus.2013.01.001
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang B, 2008, CONF CYBERN INTELL S, P55
   Yilmaz CS, 2022, INFORM FUSION, V79, P1, DOI 10.1016/j.inffus.2021.10.001
   Zafar I., 2006, MULTIEXPOSURE MULTIF, DOI [10.1049/cp:20060600, DOI 10.1049/CP:20060600]
   Zhan K, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.063004
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang XC, 2021, INFORM FUSION, V74, P111, DOI 10.1016/j.inffus.2021.02.005
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang YZ, 2014, IEEE W CONTR MODEL, DOI 10.1109/COMPEL.2014.6877120
NR 79
TC 2
Z9 2
U1 4
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2024
VL 40
IS 5
BP 3569
EP 3590
DI 10.1007/s00371-023-03052-0
EA AUG 2023
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OC3D9
UT WOS:001050556700002
DA 2024-07-18
ER

PT J
AU Xue, FY
   Zhou, M
   Zhang, CJ
   Shao, YH
   Wei, YP
   Wang, ML
AF Xue, Feiyu
   Zhou, Min
   Zhang, Chengjie
   Shao, Yahui
   Wei, Yongping
   Wang, MeiLi
TI Rt-swinir: an improved digital wallchart image super-resolution with
   attention-based learned text loss
SO VISUAL COMPUTER
LA English
DT Article
DE Digital wallchart; Super-resolution; Learned text loss; Degradation
   process; Attention mechanism
ID CNN
AB In recent years, image super-resolution (SR) has made remarkable progress in areas such as natural images or text images. However, in the field of digital wallchart image super-resolution, existing methods have failed to preserve the finer details of text regions while restoring graphics. To address this challenge, we present a new model called Real Text-SwinIR (RT-SwinIR), which employs a novel plug-and-play Attention-based Learned Text Loss (LTL) technique to enhance the architecture's ability to render clear text structure while preserving the clarity of graphics. To evaluate the effectiveness of our method, we have collected a dataset of digital wallcharts and subjected them to a two-order degradation process that simulates real-world damage, including creases and stains on wallcharts, as well as noise and blurriness caused by compression during computer network transmission. On the proposed dataset, RT-SwinIR achieves the best 0.58 on Learned Text Loss and 0.11 on LPIPS, reduced by an average of 41.4% and 35.3%, respectively. Experiments have shown that our method outperforms prior works in digital wallchart image super-resolution, indicating its superior visual perceptual performance.
C1 [Xue, Feiyu; Zhou, Min; Zhang, Chengjie; Shao, Yahui; Wei, Yongping; Wang, MeiLi] Northwest A&F Univ, Xianyang, Peoples R China.
C3 Northwest A&F University - China
RP Wang, ML (corresponding author), Northwest A&F Univ, Xianyang, Peoples R China.
EM meili_w@nwsuaf.edu.cn
FU National College Student Innovation and Entrepreneurship Training
   Program [202210712194]
FX AcknowledgementsThis paper is supported by National College Student
   Innovation and Entrepreneurship Training Program under the project
   number 202210712194. Special thanks go to Yao Zhou.
CR Agustsson Eirikur, 2017, IEEE CVF C COMP VIS, P126, DOI DOI 10.1109/CVPRW.2017.150
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bell-Kligler S, 2019, ADV NEUR IN, V32
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chen JY, 2022, AAAI CONF ARTIF INTE, P285
   Chen X., 2022, ARXIV
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du Y, 2020, ARXIV200909941
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Gu SH, 2012, INT C PATT RECOG, P3128
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, Advances in neural information processing systems, V33, P6840
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang Y., 2020, P ADV NEUR INF PROC, V33, P5632, DOI DOI 10.48550/ARXIV.2010.02631
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee J, 2022, PROC CVPR IEEE, P1928, DOI 10.1109/CVPR52688.2022.00197
   Lee J, 2020, IEEE COMPUT SOC CONF, P2326, DOI 10.1109/CVPRW50498.2020.00281
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2018, arXiv
   Ma JQ, 2023, IEEE T IMAGE PROCESS, V32, P1341, DOI 10.1109/TIP.2023.3237002
   Martin D., 2001, P ICCV, P416, DOI [DOI 10.1109/ICCV.2001.937655, 10.1109/ICCV.2001.937655]
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Miyato T., 2018, ARXIV
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2021, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR46437.2021.00869
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang X, 2021, PROC CVPR IEEE, P1905, DOI 10.1109/CVPR46437.2021.00194
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Ward C.M., 2017, APPL DIGITAL IMAGE P, V10396, P19
   Wenjia Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P650, DOI 10.1007/978-3-030-58607-2_38
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yu K., 2016, ARXIV
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou S., 2020, P ADV NEUR INF PROC, V33, P3499
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 60
TC 2
Z9 2
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3467
EP 3479
DI 10.1007/s00371-023-03017-3
EA JUL 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001036798500001
DA 2024-07-18
ER

PT J
AU Wang, JL
   Xiang, N
   Kukreja, N
   Yu, LY
   Liang, HN
AF Wang, Jialin
   Xiang, Nan
   Kukreja, Navjot
   Yu, Lingyun
   Liang, Hai-Ning
TI LVDIF: a framework for real-time interaction with large volume data
SO VISUAL COMPUTER
LA English
DT Article
DE Volume graphics; Large volume data; Volume editing; Volume manipulation;
   Framework
ID SIGNED DISTANCE FIELDS; MARCHING CUBES; MODELS
AB The interest in real-time volume graphics has grown rapidly in the last few years, driven by the increasing demands from both academia and industry. GPU-based volume rendering has been used in a wide variety of fields, including scientific visualization, visual effects, and video games. Similarly, real-time volume editing has been used to build terrain and create visual effects during game development; it has even become an integral part of gameplay in various video games (e.g., Minecraft). Nowadays, as the size of volume data increases, processing large volume data in real time is inevitable in many modern application scenarios. However, manipulation and editing of large volume data are associated with various challenges, such as dramatically increasing memory usage and computational burden. In this paper, we present a framework for interactive manipulation and editing of large volume data to address these challenges. A robust and efficient method for large signed distance function (SDF) volume generation is presented and incorporated into the framework. Also, a complete implementation with specialized GPU optimization is introduced to demonstrate its usefulness and effectiveness-it is included in the framework as well. The framework can be an easy-to-use middleware or a plugin that is able to integrate into game engines for the development of various types of applications (e.g., video games). It can also contribute to the research looking at large volume data from a user-centered perspective (e.g., for human-computer interaction researchers).
C1 [Wang, Jialin; Xiang, Nan; Yu, Lingyun; Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.
   [Wang, Jialin; Kukreja, Navjot] Univ Liverpool, Dept Comp Sci, Liverpool, England.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.
EM haining.Liang@xjtlu.edu.cn
RI Wang, Jialin/KFS-9745-2024; Yang, Ning/KHD-1133-2024
OI Wang, Jialin/0000-0002-1990-1293; Liang, Hai-Ning/0000-0003-3600-8955
FU Xi'an Jiaotong-Liverpool University Special Key Fund [KSF-A-03];
   National Natural Science Foundation of China [62272396]; XJTLU Research
   Development Fund [RDF-21-02-065, RDF-19-02-11]
FX AcknowledgementsWe thank the reviewers for their valuable time and
   insightful comments that helped improve our paper. This research was
   partly funded by Xi'an Jiaotong-Liverpool University Special Key Fund
   (#KSF-A-03), the National Natural Science Foundation of China
   (#62272396), and XJTLU Research Development Fund (#RDF-21-02-065,
   #RDF-19-02-11).
CR Boada I, 2001, VISUAL COMPUT, V17, P185, DOI 10.1007/PL00013406
   Bærentzen JA, 2005, VOLUME GRAPHICS 2005, P167
   Bürger K, 2008, IEEE T VIS COMPUT GR, V14, P1388, DOI 10.1109/TVCG.2008.120
   Chittenden T, 2018, J CONTEMP PAINTING, V4, P381, DOI 10.1386/jcp.4.2.381_1
   Cirne M.V.M., 2013, Journal of the Brazilian Computer Society, V19, P223, DOI DOI 10.1007/S13173-012-0097-Z
   Cristie V., 2015, CITYHEAT, P1, DOI [10.1145/2818517.2818527, DOI 10.1145/2818517.2818527]
   Dyken C, 2008, COMPUT GRAPH FORUM, V27, P2028, DOI 10.1111/j.1467-8659.2008.01182.x
   Gibson SFF, 1998, LECT NOTES COMPUT SC, V1496, P888, DOI 10.1007/BFb0056277
   Goetz F., 2005, P EUR, V2005, P1
   Guthe S, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P53, DOI 10.1109/VISUAL.2002.1183757
   Hoetzlein R., 2009, Proceedings of the Symposium on Interactive 3D Graphics and Games (I3D'09), P107
   Hormann K, 2001, COMP GEOM-THEOR APPL, V20, P131, DOI 10.1016/S0925-7721(01)00012-8
   Hu E.J., 2021, LORA LOW RANK ADAPTA, P1
   Jacobson A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461916
   Jeremias P., 2013, ACM SIGGRAPH 2013 Computer Animation Festival, P1, DOI DOI 10.1145/2503541.2503644
   Johansson G., 2006, P 2006 C CTR ADV STU, P39
   Jones MW, 2006, IEEE T VIS COMPUT GR, V12, P581, DOI 10.1109/TVCG.2006.56
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   KASSON JM, 1993, P SOC PHOTO-OPT INS, V1909, P127, DOI 10.1117/12.149035
   Krayer B, 2019, VISUAL COMPUT, V35, P961, DOI 10.1007/s00371-019-01683-w
   Laine S, 2011, IEEE T VIS COMPUT GR, V17, P1048, DOI 10.1109/TVCG.2010.240
   Liu FC, 2014, IEEE T VIS COMPUT GR, V20, P714, DOI 10.1109/TVCG.2013.268
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Mawhorter Peter, 2010, 2010 IEEE Information Theory Workshop (ITW 2010), P351, DOI 10.1109/ITW.2010.5593333
   Museth K., 2013, ACM SIGGRAPH 2013 CO, DOI [DOI 10.1145/2504435.2504454, 10.1145/2504435.2504454]
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006
   Qin J, 2010, IEEE COMPUT GRAPH, V30, P45, DOI 10.1109/MCG.2009.83
   Schaefer S, 2005, COMPUT GRAPH FORUM, V24, P195, DOI 10.1111/j.1467-8659.2005.00843.x
   Schneider J, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P293, DOI 10.1109/VISUAL.2003.1250385
   Sherbondy A, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P171, DOI 10.1109/VISUAL.2003.1250369
   Shi R., 2023, 2023 CHI C HUM FACT, DOI DOI 10.1145/3544549.3585615
   Shi RK, 2021, INT SYM MIX AUGMENT, P118, DOI 10.1109/ISMAR52148.2021.00026
   Treib M, 2012, COMPUT GRAPH FORUM, V31, P383, DOI 10.1111/j.1467-8659.2012.03017.x
   Tzevanidis K., 2010, P 11 EUROPEAN C TREN, VII, P384, DOI DOI 10.1007/978-3-642-35740-4-30
   Wang JL, 2023, IEEE T VIS COMPUT GR, V29, P2478, DOI 10.1109/TVCG.2023.3247057
   Wang JL, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522610
   Wu J, 2010, J COMPUT INF SCI ENG, V10, DOI 10.1115/1.3402759
NR 38
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3373
EP 3386
DI 10.1007/s00371-023-02976-x
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001026397800003
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, Y
   Guo, Z
   Guo, HJ
   Xiao, HX
AF Liu, Yu
   Guo, Zhe
   Guo, Haojie
   Xiao, Huaxin
TI Zoom-GAN: learn to colorize multi-scale targets
SO VISUAL COMPUTER
LA English
DT Article
DE Image colorization; Instance normalization; Generative adversarial
   networks; Multi-scale target
ID IMAGE SYNTHESIS
AB In recent years, the research of image colorization based on deep learning has made great progress. Most of the existing methods have achieved impressive colorizing performance over the entire region of a given image. However, we notice that the colorizing results of existing methods suffer from color disorder on small target region or boundary. For colorizing multi-scale targets, we propose a feature scaling network in this paper called Zoom-GAN to improve the colorizing consistency for small objects and boundary. Specifically, the Zoom-GAN proposes a zoom instance normalization layer to introduce scale information in color feature. Meanwhile, multi-scale structure is adopted in the generator and discriminator to improve the colorizing performance for various targets. Experiments on three public datasets Oxford102, Bird100 and Hero show that our Zoom-GAN achieves state-of-the-art on three subjective and objective evaluation metrics.
C1 [Liu, Yu; Xiao, Huaxin] Natl Univ Def Technol, Coll Syst Engn, Deya Rd 109, Changsha 410073, Hunan, Peoples R China.
   [Guo, Zhe; Guo, Haojie] Northwestern Polytech Univ, Sch Elect & Informat, Youyixi Rd 127, Xian 710072, Shaanxi, Peoples R China.
C3 National University of Defense Technology - China; Northwestern
   Polytechnical University
RP Xiao, HX (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Deya Rd 109, Changsha 410073, Hunan, Peoples R China.
EM jasonyuliu@nudt.edu.cn; guozhe@nwpu.edu.cn; ghjxin@mail.nwpu.edu.cn;
   xiaohuaxin@nudt.edu.cn
FU National Natural Science Foundation of China [62071384, 61906206]; Key
   Research and Development Project of Shaanxi Province [2023-YBGY-239]
FX & nbsp;This work was supported in part by the National Natural Science
   Foundation of China under Grants 62071384 and 61906206, and Key Research
   and Development Project of Shaanxi Province under Grant 2023-YBGY-239.
CR Bahng H, 2018, LECT NOTES COMPUT SC, V11216, P443, DOI 10.1007/978-3-030-01258-8_27
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dong X, 2022, IEEE T VIS COMPUT GR, V28, P1469, DOI 10.1109/TVCG.2020.3022480
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo HJ, 2021, PROC INT C TOOLS ART, P729, DOI 10.1109/ICTAI52525.2021.00116
   Guo Z, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14153740
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   IIZUKA S, 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925974
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kaiser L, 2017, One Model To Learn Them All
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee YW, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10203818
   Li YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P991, DOI 10.1145/3394171.3413684
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liu S., 2023, IMAGE VIDEO COLOR ED
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Paszke A, 2019, ADV NEUR IN, V32
   Peters A F., 2015, The color thief: A family's story of depression (K. Littlewood
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Reed S, 2016, PR MACH LEARN RES, V48
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheng ZH, 2023, IEEE T IMAGE PROCESS, V32, P905, DOI 10.1109/TIP.2023.3235536
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Taunk K, 2019, PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICCS), P1255, DOI [10.1109/iccs45141.2019.9065747, 10.1109/ICCS45141.2019.9065747]
   Ulyanov Dmitry, 2016, arXiv
   Vitoria P, 2020, IEEE WINT CONF APPL, P2434, DOI [10.1109/WACV45572.2020.9093389, 10.1109/wacv45572.2020.9093389]
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Weng SC, 2022, LECT NOTES COMPUT SC, V13667, P1, DOI 10.1007/978-3-031-20071-7_1
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang Y., 2022, VISUAL COMPUT, P1
   Zhang YF, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10224347
NR 48
TC 1
Z9 1
U1 1
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3299
EP 3310
DI 10.1007/s00371-023-02941-8
EA JUL 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001023990600003
DA 2024-07-18
ER

PT J
AU Wang, X
   Fujisawa, M
   Mikawa, M
AF Wang, Xu
   Fujisawa, Makoto
   Mikawa, Masahiko
TI XProtoSphere: an eXtended multi-sized sphere packing algorithm driven by
   particle size distribution
SO VISUAL COMPUTER
LA English
DT Article
DE ProtoSphere; Multi-sized sphere packing; Particle size distribution;
   Discrete element-based relaxation; Physically-based simulation
AB The sphere packing problem, which involves filling an arbitrarily shaped geometry with the maximum number of non-overlapping spheres, is a critical research challenge. ProtoSphere is a prototype-oriented algorithm designed for solving sphere packing problems. Due to its easily parallelizable design, it exhibits high versatility and has wide-ranging applications. However, the controllable regulation of particle size distribution (PSD) produced by ProtoSphere is often neglected, which limits its application on algorithm. This paper proposes a novel PSD-driven technique that extends the ProtoSphere algorithm to achieve multi-sized sphere packing with distribution-specific characteristics, as dictated by a pre-defined cumulative distribution function. The proposed approach improves the controllability and flexibility of the packing process, and enables users to generate packing configurations that meet their specific requirements. In addition, by combining the relaxation method with the ProtoSphere algorithm, we can further improve the packing density and ensure the average overlap below 1%. Our method generates multi-sized particles that can be used to simulate the behavior of various granular materials, including sand-like and clay-like soils.
C1 [Wang, Xu; Fujisawa, Makoto; Mikawa, Masahiko] Univ Tsukuba, Tsukuba, Ibaraki, Japan.
C3 University of Tsukuba
RP Wang, X (corresponding author), Univ Tsukuba, Tsukuba, Ibaraki, Japan.
EM wang.xu.lm@alumni.tsukuba.ac.jp
CR Adams B, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276437, 10.1145/1239451.1239499]
   AURENHAMMER F, 1987, SIAM J COMPUT, V16, P78, DOI 10.1137/0216006
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821
   Bell N., 2005, P 2005 ACM SIGGRAPH, P77, DOI DOI 10.1145/1073368.1073379
   Bonneau F, 2021, COMPUT PART MECH, V8, P201, DOI 10.1007/s40571-020-00324-7
   Borkovec M, 1994, FRACTALS, V2, P521, DOI 10.1142/S0218348X94000739
   Bridson R., 2007, Fast Poisson disk sampling in arbitrary dimensions, V10, P1, DOI DOI 10.1145/1278780.1278807
   Conway JH., 2013, Grundlehren der mathematischen Wissenschaften
   Cui L, 2003, GRANUL MATTER, V5, P135, DOI 10.1007/s10035-003-0145-7
   CUNDALL PA, 1979, GEOTECHNIQUE, V29, P47, DOI 10.1680/geot.1979.29.1.47
   Desbrun M., 1999, Space-Time Adaptive Simulation of Highly Deformable Substances
   Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443
   George P., 1998, DELAUNAY TRIANGULATI
   Hales TC, 2005, ANN MATH, V162, P1065, DOI 10.4007/annals.2005.162.1065
   Hentz S, 2004, COMPUT STRUCT, V82, P2509, DOI 10.1016/j.compstruc.2004.05.016
   Hifi M, 2009, ADV OPER RES, V2009, DOI 10.1155/2009/150624
   Jerier JF, 2010, COMPUT METHOD APPL M, V199, P1668, DOI 10.1016/j.cma.2010.01.016
   Jerier JF, 2009, GRANUL MATTER, V11, P43, DOI 10.1007/s10035-008-0116-0
   Jiang M, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1798
   Jiang M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818102
   Kita N, 2016, VISUAL COMPUT, V32, P1035, DOI 10.1007/s00371-016-1248-6
   Lopes LGO, 2021, COMPUT PART MECH, V8, P931, DOI 10.1007/s40571-020-00378-7
   Lopes LGO, 2021, COMPUT PART MECH, V8, P169, DOI 10.1007/s40571-020-00320-x
   Schechter H, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185557
   Schwarz M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866201
   Smilauer V., YADE FORMULATION
   Teuber J., 2013, GI AR VRWORKSHOP
   Torquato S., 2002, Appl. Mech. Rev, V55, pB62
   Tsuzuki S., 2014, 26 IEEE ACM INT C HI
   Wang X., 2022, IIEEJ T IMAGE ELECT, V10, P150
   Wang X, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3480141
   Weller R., 2009, P 2009 ACM SIGGRAPH, P151, DOI DOI 10.1145/1581073.1581097
   Weller R., 2010, ACM SIGGRAPH ASIA 20, P1
   Winchenbach R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3363555
   Winchenbach R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073713
   Wong KM, 2017, VISUAL COMPUT, V33, P823, DOI 10.1007/s00371-017-1382-9
   Yan DM, 2015, J COMPUT SCI TECH-CH, V30, P439, DOI 10.1007/s11390-015-1535-0
   Zhang KY, 2020, POWDER TECHNOL, V366, P448, DOI 10.1016/j.powtec.2020.01.079
   Zheng X., 2016, P 33 COMP GRAPH INT, P77
NR 40
TC 0
Z9 0
U1 2
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2023
VL 39
IS 8
SI SI
BP 3333
EP 3346
DI 10.1007/s00371-023-02977-w
EA JUL 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P2DS6
UT WOS:001023823500001
DA 2024-07-18
ER

PT J
AU Aslam, N
   Kolekar, MH
AF Aslam, Nazia
   Kolekar, Maheshkumar H.
TI DeMAAE: deep multiplicative attention-based autoencoder for
   identification of peculiarities in video sequences
SO VISUAL COMPUTER
LA English
DT Article
DE Anomaly detection; Attention mechanism; Deep autoencoder; Temporal
   attention
ID ANOMALY DETECTION
AB In videos, anomaly detection is challenging due to its diverse nature in different application domains. Reconstruction and prediction-based methods have been widely employed to detect anomalies. Due to the generalization capability of a deep neural network, sometimes, it recreates irregular patterns along with regular ones. This paper presents a novel autoencoder-based framework called deep multiplicative attention-based autoencoder (DeMAAE) to detect anomalies in a video sequence. The global attention mechanism is used at the decoder side of DeMAAE for better feature learning during the decoding phase. An attention map is created by taking the dot product between all encoder's hidden states and the previously generated decoder's hidden state. After that, the final output of the decoder is determined by the context vector. The context vector is computed using the weighted summation of all encoder's hidden states and attention weight. DeMAAE delivers an improved runtime of 0.015 s (similar to 67 fps) for detecting anomalies during testing. Extensive experiments have been performed on the two diversified and widely used datasets (UCSD Pedestrian and CUHK Avenue) to compare the efficacy of DeMAAE with different state-of-the-art methods.
C1 [Aslam, Nazia] Indian Inst Technol Patna, Dept Elect Engn, Video Surveillance Lab, Bihta 801106, India.
   [Kolekar, Maheshkumar H.] Indian Inst Technol Patna, Dept Elect Engn, Bihta 801106, India.
C3 Indian Institute of Technology (IIT) - Patna; Indian Institute of
   Technology (IIT) - Patna
RP Aslam, N (corresponding author), Indian Inst Technol Patna, Dept Elect Engn, Video Surveillance Lab, Bihta 801106, India.
EM n.aslam921@gmail.com; mahesh@iitp.ac.in
RI Aslam, Nazia/AGZ-4243-2022
OI Aslam, Nazia/0000-0002-8381-9702
CR Aslam N, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103598
   Aslam N, 2022, MULTIMED TOOLS APPL, V81, P42457, DOI 10.1007/s11042-022-13496-6
   Aslam N, 2017, 2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P1071, DOI 10.1109/ICCSP.2017.8286540
   Astrid M, 2021, IEEE INT CONF COMP V, P207, DOI 10.1109/ICCVW54120.2021.00028
   Ba J., 2014, ARXIV PREPRINT ARXIV
   Bansod SD, 2020, VISUAL COMPUT, V36, P609, DOI 10.1007/s00371-019-01647-0
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Cosar S, 2017, IEEE T CIRC SYST VID, V27, P683, DOI 10.1109/TCSVT.2016.2589859
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Feng JC, 2021, PROC CVPR IEEE, P14004, DOI 10.1109/CVPR46437.2021.01379
   Georgescu MI, 2021, PROC CVPR IEEE, P12737, DOI 10.1109/CVPR46437.2021.01255
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Guansong Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12170, DOI 10.1109/CVPR42600.2020.01219
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Ji XL, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207231
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Kingma D. P., 2014, arXiv
   Kumar D, 2017, VISUAL COMPUT, V33, P265, DOI 10.1007/s00371-015-1192-x
   Li QN, 2018, VISUAL COMPUT, V34, P229, DOI 10.1007/s00371-016-1330-0
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13568, DOI 10.1109/ICCV48922.2021.01333
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Luong M.-T., 2015, P 2015 C EMPIRICAL M, DOI DOI 10.18653/V1/D15-1166
   Ma H, 2023, J GENDER STUD, V32, P562, DOI 10.1080/09589236.2022.2070463
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Medel J. R., 2016, Anomaly detection in video using predictive convolutional long short-term memory networks
   Nawaratne R, 2020, IEEE T IND INFORM, V16, P393, DOI 10.1109/TII.2019.2938527
   Ramachandra B, 2020, IEEE WINT CONF APPL, P2558, DOI [10.1109/WACV45572.2020.9093457, 10.1109/wacv45572.2020.9093457]
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Song H, 2020, IEEE T MULTIMEDIA, V22, P2138, DOI 10.1109/TMM.2019.2950530
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Wang L, 2018, IEEE IMAGE PROC, P2276, DOI 10.1109/ICIP.2018.8451070
   Wang X, 2018, INT CONF SIGN PROCES, P474, DOI 10.1109/ICSP.2018.8652354
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xi PC, 2020, VISUAL COMPUT, V36, P1869, DOI 10.1007/s00371-019-01775-7
   Yan SY, 2020, IEEE T COGN DEV SYST, V12, P30, DOI 10.1109/TCDS.2018.2883368
   Yang B, 2019, IEEE T COGN DEV SYST, V11, P473, DOI 10.1109/TCDS.2018.2866838
   Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899
   Zhang JQ, 2019, IEEE T MULTIMEDIA, V21, P1332, DOI 10.1109/TMM.2018.2871421
   Zhang Y., 2020, IEEE T CIRC SYST VID
   Zhang Y.-F., 2021, arXiv
   Zhao B, 2011, PROC CVPR IEEE
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhao Y, 2016, IEEE IMAGE PROC, P3354, DOI 10.1109/ICIP.2016.7532981
   Zheng L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4419
   Zhou JT, 2020, IEEE T CIRC SYST VID, V30, P4639, DOI 10.1109/TCSVT.2019.2962229
NR 54
TC 6
Z9 6
U1 5
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1729
EP 1743
DI 10.1007/s00371-023-02882-2
EA MAY 2023
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000993235200001
DA 2024-07-18
ER

PT J
AU Salar, A
   Ahmadi, A
AF Salar, Ali
   Ahmadi, Ali
TI Improving loss function for deep convolutional neural network applied in
   automatic image annotation
SO VISUAL COMPUTER
LA English
DT Article
DE Image annotation; Deep learning; Unsymmetrical loss function; Threshold
   estimation
ID CNN
AB Automatic image annotation (AIA) is a mechanism for describing the visual content of an image with a list of semantic labels. Typically, there is a massive imbalance between positive and negative tags in a picture-in other words, an image includes much fewer positive labels than negative ones. This imbalance can negatively affect the optimization process and diminish the emphasis on gradients from positive labels during training. Although traditional annotation models mainly focus on model structure design, we propose a novel unsymmetrical loss function for a deep convolutional neural network (CNN) that performs differently on positives and negatives, which leads to a reduction in the loss contribution from negative labels and also highlights the contribution of positive ones. During the annotation process, we specify a threshold for each label separately based on the Matthews correlation coefficient (MCC). Extensive experiments on high-vocabulary datasets like Corel 5k, IAPR TC-12, and Esp Game reveal that despite ignoring the semantic relationships between labels, our suggested approach achieves remarkable results compared to the state-of-the-art automatic image annotation models.
C1 [Salar, Ali; Ahmadi, Ali] K N Toosi Univ Technol, Fac Comp Engn, Tehran, Iran.
C3 K. N. Toosi University of Technology
RP Ahmadi, A (corresponding author), K N Toosi Univ Technol, Fac Comp Engn, Tehran, Iran.
EM parham.slrst@email.kntu.ac.ir; ahmadi@kntu.ac.ir
RI ahmadi, ali/JMC-5690-2023
CR Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng QM, 2018, PATTERN RECOGN, V79, P242, DOI 10.1016/j.patcog.2018.02.017
   Chicco D, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-019-6413-7
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Feng LN, 2016, IEEE T PATTERN ANAL, V38, P785, DOI 10.1109/TPAMI.2015.2469281
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Gong Y., 2013, ARXIV
   Grubinger M., 2007, ANAL EVALUATION VISU
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Jeon J., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119, DOI DOI 10.1145/860435.860459
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Jing XY, 2016, IEEE T IMAGE PROCESS, V25, P2712, DOI 10.1109/TIP.2016.2549459
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Khatchatoorian AG, 2020, IET COMPUT VIS, V14, P214, DOI 10.1049/iet-cvi.2019.0500
   Khatchatoorian AG, 2018, 2018 2ND INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (ICDSP 2018), P88, DOI 10.1145/3193025.3193035
   Kipf TN, 2016, ARXIV
   Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199
   Li ZX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426974
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   LOTFI F, 2021, P 5 INT C PATTERN RE, P1, DOI DOI 10.1109/IPRIA53572.2021.9483536
   Ma YC, 2019, MULTIMED TOOLS APPL, V78, P3767, DOI 10.1007/s11042-018-6038-x
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Murthy V.N., 2014, ICMR 2014 P ACM INT, P369, DOI DOI 10.1145/2578726.2578774
   Murthy VN, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P603, DOI 10.1145/2671188.2749391
   Niu YL, 2019, IEEE T IMAGE PROCESS, V28, P1720, DOI 10.1109/TIP.2018.2881928
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Ridnik T, 2021, IEEE WINT CONF APPL, P1399, DOI 10.1109/WACV48630.2021.00144
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   Wang Mei, 2008, Journal of Software, V19, P2449, DOI 10.3724/SP.J.1001.2008.02449
   Wu BY, 2018, PROC CVPR IEEE, P7967, DOI 10.1109/CVPR.2018.00831
   Wu BY, 2015, IEEE I CONF COMP VIS, P4157, DOI 10.1109/ICCV.2015.473
   Wu BY, 2014, INT C PATT RECOG, P1964, DOI 10.1109/ICPR.2014.343
   Xue LX, 2020, VISUAL COMPUT, V36, P1325, DOI 10.1007/s00371-019-01731-5
   Zhang WF, 2018, NEURAL PROCESS LETT, V48, P1503, DOI 10.1007/s11063-017-9753-9
   Zhang Y.-F., 2021, arXiv
   Zhu G., 2010, P INT C MULTIMEDIA, P461, DOI [10.1145/1873951.1874028., DOI 10.1145/1873951.1874028]
NR 43
TC 0
Z9 0
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2024
VL 40
IS 3
BP 1617
EP 1629
DI 10.1007/s00371-023-02873-3
EA MAY 2023
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HY8O3
UT WOS:000986398700002
DA 2024-07-18
ER

PT J
AU Liang, XL
   Luo, ZZ
   Han, YK
AF Liang, Xiaoliang
   Luo, Zhengzhi
   Han, Yike
TI TFTSVM: near color recognition of polishing red lead via SVM based on
   threshold and feature transform
SO VISUAL COMPUTER
LA English
DT Article
DE Mould polishing; Machine vision; HSV color space; Support vector
   machine; Sample transformation; Near-color recognition
ID SEGMENTATION
AB With the extensive application ofmachine vision in themanufacturing industry, target region recognition in complex industrial scenes is becoming a vital research territory. In the automatic polishing of molds, polishing red lead, as an auxiliary tool for polishing positioning, can intuitively determine the areas to be polished. Its bright color information are very suitable for vision-based recognition. Due to the interference of the near color in the polishing environment, the traditional color recognition method has the appearance of over-segmentation. In this paper, we propose a novel near-color recognition method via SVM based on threshold and feature transform (TFTSVM) to improve the identification accuracy of polishing red lead. Specifically, this method adopts a threshold-based color recognition algorithm to extract two kinds of color features of red lead color and its near color in HSV color space and skillfully finds it is distinguishable in three dimensions. To reduce the computational complexity, a machine learning segmentation model is constructed, which realizes dimension reduction by integrating the feature transformation method of sample transformation and projection transformation to achieve the best segmentation effect. Experimental results on self-established dataset demonstrate that the proposed method has an excellent identification effect on the red lead area in the field polishing environment and also shows good robustness under the condition that there are reflections on the mold surface. It meets the requirements of mechanical arm polishing and improves the safety and reliability of automatic polishing. In addition, we also compare different machine learning algorithms and advanced studies to verify the correctness of the algorithm. This method also provides a reference for realizing near-color recognition in complex industrial environments.
C1 [Liang, Xiaoliang; Luo, Zhengzhi; Han, Yike] Southwest Jiaotong Univ, Inst Adv Design & Mfg, Chengdu 610000, Peoples R China.
C3 Southwest Jiaotong University
RP Luo, ZZ (corresponding author), Southwest Jiaotong Univ, Inst Adv Design & Mfg, Chengdu 610000, Peoples R China.
EM zhzhluo@swjtu.edu.cn
FU Sichuan Science and Technology Program [2021YFG0194]
FX This work was supported by the Sichuan Science and Technology Program
   (2021YFG0194).
CR Amirkhani A, 2023, VISUAL COMPUT, V39, P5293, DOI 10.1007/s00371-022-02660-6
   Castillo-Martínez MA, 2020, COMPUT ELECTRON AGR, V178, DOI 10.1016/j.compag.2020.105783
   Chang GS, 2022, INT J ADV MANUF TECH, V122, P697, DOI 10.1007/s00170-022-09943-1
   Chiu CC, 2000, COMPUT METH PROG BIO, V61, P77, DOI 10.1016/S0169-2607(99)00031-0
   Chouksey M, 2020, MULTIMED TOOLS APPL, V79, P19075, DOI 10.1007/s11042-019-08138-3
   Ferraguti F, 2019, ROBOT CIM-INT MANUF, V59, P158, DOI 10.1016/j.rcim.2019.04.007
   Fu HY, 2020, NEUROCOMPUTING, V395, P178, DOI 10.1016/j.neucom.2018.02.111
   Hanmandlu M, 2013, NEUROCOMPUTING, V120, P235, DOI 10.1016/j.neucom.2012.09.043
   Lei L, 2021, SPECTROCHIM ACTA A, V262, DOI 10.1016/j.saa.2021.120119
   Liu YJ, 2012, VISUAL COMPUT, V28, P75, DOI 10.1007/s00371-011-0605-8
   Long K, 2021, MEASUREMENT, V176, DOI 10.1016/j.measurement.2021.109169
   Ma JW, 2023, VISUAL COMPUT, V39, P4065, DOI 10.1007/s00371-022-02573-4
   Malhotra J, 2021, PRECIS ENG, V72, P690, DOI 10.1016/j.precisioneng.2021.07.013
   Mou T, 2023, VISUAL COMPUT, V39, P6151, DOI 10.1007/s00371-022-02718-5
   Penumuru DP, 2020, J INTELL MANUF, V31, P1229, DOI 10.1007/s10845-019-01508-6
   Rafael C.Gonzalez., 2007, DIGITAL IMAGE PROCES, V2nd
   Segreto T, 2015, PROC CIRP, V33, P333, DOI 10.1016/j.procir.2015.06.075
   Segreto T, 2015, PROC CIRP, V28, P22, DOI 10.1016/j.procir.2015.04.005
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Sheahan C., 2006, J. Manuf. Process., V8, P133, DOI [10.1016/S1526-6125(06)80008-3, DOI 10.1016/S1526-6125(06)80008-3]
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Suh HK, 2020, COMPUT ELECTRON AGR, V179, DOI 10.1016/j.compag.2020.105819
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Upadhyay P, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2019.105522
   Wang K, 2019, PROCEDIA MANUF, V38, P1499, DOI 10.1016/j.promfg.2020.01.137
   Wang NN, 2021, J MANUF PROCESS, V66, P281, DOI 10.1016/j.jmapro.2021.04.014
   Wang YG, 2007, PATTERN RECOGN LETT, V28, P11, DOI 10.1016/j.patrec.2006.06.004
   Wu BH, 2009, J MATER PROCESS TECH, V209, P3241, DOI 10.1016/j.jmatprotec.2008.07.031
   [徐志强 Xu Zhiqiang], 2017, [表面技术, Surface Technology], V46, P99
   Yin H, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107015
   Yin YX, 2020, J MANUF PROCESS, V57, P268, DOI 10.1016/j.jmapro.2020.06.035
   Zhang HH, 2019, MULTIMED TOOLS APPL, V78, P35313, DOI 10.1007/s11042-019-08164-1
   Zhang HY, 2022, J MANUF PROCESS, V74, P441, DOI 10.1016/j.jmapro.2021.12.028
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
NR 34
TC 0
Z9 0
U1 5
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2024
VL 40
IS 2
BP 717
EP 730
DI 10.1007/s00371-023-02811-3
EA MAR 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE6E8
UT WOS:000948095300001
DA 2024-07-18
ER

PT J
AU Haggui, O
   Bayd, H
   Magnier, B
AF Haggui, Olfa
   Bayd, Hamza
   Magnier, Baptiste
TI Centroid human tracking via oriented detection in overhead fisheye
   sequences
SO VISUAL COMPUTER
LA English
DT Article
DE Human detection; Tracking; Moving fisheye camera; Deep learning; YOLOv3;
   Centroid tracker
AB Pedestrian tracking is highly relevant to the understanding of static and moving scenes in video sequences. The increasing demand for people's safety and security has resulted in more research on intelligent visual surveillance in a wide range of applications, such as moving human detection and tracking. With the great success of deep learning methods, researchers decided to switch from traditional methods based on hand-crafted feature extractors to recent deep-learning-based techniques in order to detect and track people. In this work, the topic of person detection using a Top-view moving fisheye camera is addressed using a deep learning detector combined with a centroid technique in order to track a selected person. Although the fisheye camera is a useful tool for video monitoring, most object detection techniques use classical perspective cameras, with (or without) deep learning. However, due to the distortions of fisheye images, we expect to have higher requirements and challenges on pedestrian detection using this type of device. In this paper, an end-to-end people detection learning method is proposed; it is based on a YOLOv3 detector that detects people using oriented bounding boxes. The proposed model customizes the traditional YOLOv3 for the detection of oriented bounding boxes, by regressing the angle of each bounding box using a periodic loss function. With rotation bounding box prediction, the new approach is efficient, reaching 98.1% of true detection. This detection model is combined with a centroid tracker in order to track a single person by identifying the trajectory, estimated angle of rotation and target distance. Finally, the proposed method is evaluated on a new available dataset where rotated bounding boxes represent annotations from several fisheye videos.
C1 [Haggui, Olfa; Bayd, Hamza; Magnier, Baptiste] Univ Montpellier, EuroMov Digital Hlth Mot, IMT Mines Ales, Ales, France.
C3 IMT - Institut Mines-Telecom; IMT Mines Ales; Universite de Montpellier
RP Haggui, O (corresponding author), Univ Montpellier, EuroMov Digital Hlth Mot, IMT Mines Ales, Ales, France.
EM olfa.haggui@mines-ales.fr; hamza.bayd@mines-ales.fr;
   Baptiste.Magnier@mines-ales.fr
CR Ahmad M, 2020, INT J DISTRIB SENS N, V16, DOI 10.1177/1550147720934738
   Ahmed I, 2021, INT J MACH LEARN CYB, V12, P3053, DOI 10.1007/s13042-020-01220-5
   Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583
   Baek I, 2018, IEEE INT VEH SYM, P447, DOI 10.1109/IVS.2018.8500455
   Bertozzi M, 2015, IEEE INT VEH SYM, P132, DOI 10.1109/IVS.2015.7225675
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Boui M, 2016, IEEE IMAGE PROC, P604, DOI 10.1109/ICIP.2016.7532428
   Chen CW, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351436
   Chiang A.-T., 2014, ICME WORKSH, P1
   Chiang SH, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104069
   Demirkus M, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P141, DOI 10.5220/0006094701410148
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Duan ZH, 2020, IEEE COMPUT SOC CONF, P2700, DOI 10.1109/CVPRW50498.2020.00326
   Gözen D, 2021, INT C PATT RECOG, P10082, DOI 10.1109/ICPR48806.2021.9413316
   Haggui O, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733674
   Haggui O, 2021, EUR W VIS INF PROCES, DOI 10.1109/EUVIP50544.2021.9483957
   Nguyen HD, 2019, MULTIMED TOOLS APPL, V78, P4563, DOI 10.1007/s11042-018-6141-z
   Hansen P, 2010, INT J ROBOT RES, V29, P267, DOI 10.1177/0278364909356484
   Huang CB, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (RCAR), P543, DOI 10.1109/RCAR.2017.8311919
   Jingrui Yu, 2019, Current Directions in Biomedical Engineering, V5, P239, DOI 10.1515/cdbme-2019-0061
   KRAMS O, 2017, 14 IEEE INT C ADV VI, P1
   Kubo Yohei, 2007, SICE '07. 46th SICE Annual Conference, P2013
   Kumar V.R., ARXIV
   Kumler J, 2000, P SOC PHOTO-OPT INS, V4093, P360, DOI 10.1117/12.405226
   Li SY, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI [10.1109/avss.2019.8909877, 10.1109/IRCE.2019.00008]
   Lin HL, 2018, 2018 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MACHINE LEARNING (SPML 2018), P55, DOI 10.1145/3297067.3297069
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Magnier B., 2010, IEEE INT C IM SYST T, P229
   Ramezani-Kebrya A, 2021, ARXIV
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rosebrock A., 2018, SIMPLE OBJECT TRACKI
   Saeidi M, 2022, VISUAL COMPUT, V38, P2223, DOI 10.1007/s00371-021-02280-6
   Saito Mamoru, 2013, Transactions of the Society of Instrument and Control Engineers, V49, P319
   Scaramuzza D., 2021, OMNIDIRECTIONAL CAME
   Sio C. H., 2019, P ACM MULT AS, P1
   Srisamosorn V, 2020, VISUAL COMPUT, V36, P1443, DOI 10.1007/s00371-019-01749-9
   Tamura M, 2019, IEEE WINT CONF APPL, P1989, DOI 10.1109/WACV.2019.00216
   Nguyen VT, 2016, I C INF COMM TECH CO, P840, DOI 10.1109/ICTC.2016.7763311
   Vandewiele F., 2013, INT C DISTR SMART CA, P1
   VENKATESWARLU R, 1992, P SOC PHOTO-OPT INS, V1697, P520, DOI 10.1117/12.138205
   Wang T, 2019, CONF TECHNOL APPL, DOI 10.1109/taai48200.2019.8959887
   Wang T, 2017, I S INTELL SIG PROC, P719, DOI 10.1109/ISPACS.2017.8266570
   Wang W, 2015, IEEE WINT CONF APPL, P17, DOI 10.1109/WACV.2015.10
   Wang XZ, 2016, CHIN CONT DECIS CONF, P329, DOI 10.1109/CCDC.2016.7531004
   Wei LX, 2021, VISUAL COMPUT, V37, P133, DOI 10.1007/s00371-019-01787-3
   Wu JH, 2014, COMM COM INF SC, V484, P255
   Yu Hsiang-Fu, 2010, KDD Cup, DOI DOI 10.1016/J.NEULET.2010.03.079.PUBMED
   Zhang HX, 2021, VISUAL COMPUT, V37, P2433, DOI 10.1007/s00371-020-01997-0
   Zhen Chen, 2019, 2019 4th International Conference on Robotics and Automation Engineering (ICRAE), P125, DOI 10.1109/ICRAE48301.2019.9043800
NR 56
TC 4
Z9 4
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 407
EP 425
DI 10.1007/s00371-023-02790-5
EA FEB 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000934682000002
DA 2024-07-18
ER

PT J
AU Aldin, SS
   Aldin, NB
   Aykaç, M
AF Aldin, Shaima Safa
   Aldin, Noor Baha
   Aykac, Mahmut
TI Enhanced image classification using edge CNN (E-CNN)
SO VISUAL COMPUTER
LA English
DT Article
DE CNN; Classification; Training time; Edge
ID NEURAL-NETWORKS
AB Recently, deep learning has become a hot topic in wide fields, especially in the computer vision that proved its efficiency in processing images. However, it tends to overfit or consumes a long learning time in many platforms. The causes behind these issues return to the huge number of learning parameters and lack or incorrect training samples. In this work, two levels of deep convolutional neural network (DCNN) are proposed for classifying the images. The first one is enhancing the training images with removing unnecessary details, and the second one is detecting the edges of the processed images for further reduction of learning time in the DCNN. The proposed work is inspired by the human eye's way in recognizing an object, where a piece of object can be helpful in the recognition and not necessarily the whole object or full colors. The goal is to speed up the learning process of CNN based on the preprocessed training samples that are precise and lighter to work well in real-time applications. The obtained results proved to be more significant for real-time classification as it reduced the learning process by (94%) in Animals10 dataset with a validation accuracy of (99.2%) in accordance with the classical DCNNs.
C1 [Aldin, Shaima Safa] Al Nahrain Univ, Continuing Educ Ctr, Baghdad, Iraq.
   [Aldin, Noor Baha] Hasan Kalyoncu Univ, Dept Elect & Elect Engn, Gaziantep, Turkiye.
   [Aykac, Mahmut] Gaziantep Univ, Dept Elect & Elect Engn, Gaziantep, Turkiye.
C3 Al-Nahrain University; Hasan Kalyoncu University; Gaziantep University
RP Aldin, SS (corresponding author), Al Nahrain Univ, Continuing Educ Ctr, Baghdad, Iraq.
EM shaima.safaaldin@nahrainuniv.edu.iq; noor.aldin@hku.edu.tr;
   maykac@gantep.edu.tr
RI AYKAÇ, Mahmut/JOJ-8225-2023; BAHA ALDIN, NOOR/JES-2617-2023; Baha aldin,
   Shaima safa aldin/V-3616-2019
OI AYKAÇ, Mahmut/0000-0003-2977-9719; BAHA ALDIN, NOOR/0000-0002-7351-4083;
   Baha aldin, Shaima safa aldin/0000-0003-0911-6934
CR Aldin NB, 2022, 2022 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ICEEE 2022), P316, DOI 10.1109/ICEEE55327.2022.9772551
   Alessio Corrado, 2019, Animals-10
   [Anonymous], 2017, ARXIV
   [Anonymous], 2008, ACM ICMR MIR 08
   Aresta G, 2019, MED IMAGE ANAL, V56, P122, DOI 10.1016/j.media.2019.05.010
   Coates A., 2011, JMLR W CP, V15
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   DeVries T, 2017, PREPRINT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang K, 2020, VISUAL COMPUT, V36, P1355, DOI 10.1007/s00371-019-01734-2
   Ioffe S., 2015, P 2015 INT C MLIS
   Julie P., 2020, SICARA          1126
   Kane Aditya, 2019, IMAGENETTE2
   Kang G, 2017, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang XD, 2016, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2016.75
   Liu ZY, 2020, VISUAL COMPUT, V36, P1823, DOI 10.1007/s00371-019-01778-4
   Macoadha O., 2018, INCEPTION V3 MODEL
   Nazeri K, 2018, LECT NOTES COMPUT SC, V10882, P717, DOI 10.1007/978-3-319-93000-8_81
   NEOZ RO., 2020, KAGGLE
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Roy P., 2018, NATURAL IMAGES
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   SACHIN, 2020, CATS VS DOGS
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Sparsh G., 2020, FLOWERS DATASET
   Srivastava N., 2012, INT C MACH LEARN WOR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang W, 2022, IEEE T IMAGE PROCESS, V31, P5134, DOI 10.1109/TIP.2022.3193288
   Vesal S, 2018, LECT NOTES COMPUT SC, V10882, P812, DOI 10.1007/978-3-319-93000-8_92
   Wan L, 2013, P 30 INT C MACH LEAR, P1058, DOI DOI 10.5555/3042817.3043055
   Wang RG, 2017, J VIS COMMUN IMAGE R, V49, P213, DOI 10.1016/j.jvcir.2017.07.004
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang FL, 2012, VISUAL COMPUT, V28, P175, DOI 10.1007/s00371-011-0616-5
   Yun S., 2019, ARXIV
   Zhang H., 2017, ARXIV
   Zhang L, 2014, VISUAL COMPUT, V30, P1359, DOI 10.1007/s00371-013-0891-4
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
NR 41
TC 1
Z9 1
U1 17
U2 42
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 319
EP 332
DI 10.1007/s00371-023-02784-3
EA FEB 2023
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000925971100002
DA 2024-07-18
ER

PT J
AU Lu, HX
   Liu, ZB
   Pan, XP
   Lan, RS
   Wang, WH
AF Lu, Haoxiang
   Liu, Zhenbing
   Pan, Xipeng
   Lan, Rushi
   Wang, Wenhao
TI Enhancing infrared images via multi-resolution contrast stretching and
   adaptive multi-scale detail boosting
SO VISUAL COMPUTER
LA English
DT Article
DE Image enhancement; Image fusion; Histogram equalization; Adaptive gamma
   correction
ID ENHANCEMENT; EQUALIZATION; DOMAIN; FILTER
AB Infrared imaging technology has attracted numerous interests in military, security, transportation, etc. However, infrared images suffer from low contrast and blurry detail, limiting its application. To handle these issues, this paper presents a novel approach based on multi-resolution contrast stretching and adaptive multi-scale detail boosting. Specifically, the hybrid-spatial Gamma function can create the high-brightness feature map; multi-resolution analysis fully discovers the information in different screen resolutions for reconstructing the high-contrast feature map, i.e., the weighted adaptive limited contrast histogram equalization stretches the image's approximate map and the hybrid Gamma function and histogram processes the detailed maps at different directions; the edge-preserving Gaussian blurring function can fully explore the details at different scale space for yielding the feature map with clearer details. Finally, the Laplace pyramid fuses different feature maps obeying the image-matching degree and average gradient to reconstruct pleasing visual images. Extensive experiments have shown that our method has an average increased by 13.0411, 1.6737, 23.3651, and 20.8085 in terms of average gradient (AG), information entropy (IE), enhancement by IE (EME), and image sharpness (IS), respectively, which is a proven approach for infrared image enhancement.
C1 [Lu, Haoxiang; Liu, Zhenbing; Pan, Xipeng; Lan, Rushi; Wang, Wenhao] Guilin Univ Elect Technol, Sch Comp & Informat Secur, Guilin 541004, Guangxi, Peoples R China.
C3 Guilin University of Electronic Technology
RP Liu, ZB (corresponding author), Guilin Univ Elect Technol, Sch Comp & Informat Secur, Guilin 541004, Guangxi, Peoples R China.
EM hxlu10005@163.com; zbliu@guet.edu.cn; pxp201@guet.edu.cn;
   rslan2016@163.com; whwang2018@163.com
RI Wang, Wenhao/AAG-3120-2021
OI Wang, Wenhao/0000-0003-4198-6681
FU National Natural Science Foundation of China [61866009, 62002082];
   Guangxi Science and Technology Project [AB21220037]; Guangxi Natural
   Science Foundation [2020GXNSFBA238014, 2020GXNSFAA297061]; Innovation
   Project of Guangxi Graduate Education [YCBZ2022112]
FX This work was supported by the National Natural Science Foundation of
   China under grants (61866009, 62002082), the Guangxi Science and
   Technology Project (AB21220037), the Guangxi Natural Science Foundation
   under grants (2020GXNSFBA238014, 2020GXNSFAA297061), and the Innovation
   Project of Guangxi Graduate Education (YCBZ2022112).
CR Acharya UK, 2021, OPTIK, V247, DOI 10.1016/j.ijleo.2021.167904
   Alhaidery MMA, 2022, MULTIMED TOOLS APPL, V81, P8745, DOI 10.1007/s11042-022-12237-z
   Bai XZ, 2011, INFRARED PHYS TECHN, V54, P61, DOI 10.1016/j.infrared.2010.12.001
   Bhandari AK, 2022, NEURAL COMPUT APPL, V34, P7053, DOI 10.1007/s00521-021-06858-y
   Bhandari AK, 2022, MULTIMED TOOLS APPL, V81, P6009, DOI 10.1007/s11042-021-11347-4
   Bulut F, 2022, MATH COMPUT SIMULAT, V197, P277, DOI 10.1016/j.matcom.2022.02.006
   Chen BH, 2019, IEEE T CIRC SYST VID, V29, P38, DOI 10.1109/TCSVT.2017.2773461
   Chen WJ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14010233
   Da P, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102092
   [董丽丽 Dong Lili], 2018, [电子学报, Acta Electronica Sinica], V46, P2367
   Fan GD, 2021, APPL INTELL, V51, P7262, DOI 10.1007/s10489-021-02236-2
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Goh HH, 2022, APPL SOFT COMPUT, V118, DOI 10.1016/j.asoc.2022.108487
   Harish, 2020, J COMPUT THEOR NANOS, V17, P2405
   Herrera-Arellano M, 2021, IEEE T IMAGE PROCESS, V30, P4962, DOI 10.1109/TIP.2021.3077310
   Hinder F, 2022, LECT NOTES COMPUT SC, V13205, P157, DOI 10.1007/978-3-031-01333-1_13
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Huang ZH, 2022, INFRARED PHYS TECHN, V121, DOI 10.1016/j.infrared.2021.104014
   Jebadass JR, 2022, SOFT COMPUT, V26, P4949, DOI 10.1007/s00500-021-06539-x
   Jiawei Luo, 2021, 2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA), P332, DOI 10.1109/ICIBA52610.2021.9688030
   Kalake L, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062123
   KATIRCIO LU F., 2020, EL CEZERI FEN MUHEND, V7, P1201, DOI DOI 10.31202/ECJSE.733519
   Kaur P, 2021, MATER TODAY-PROC, V46, P4025, DOI 10.1016/j.matpr.2021.02.543
   Kim Y, 2015, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2015.7351031
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Kwan C, 2022, SIGNAL IMAGE VIDEO P, V16, P93, DOI 10.1007/s11760-021-01970-x
   Liu CW, 2021, MATH PROBL ENG, V2021, DOI 10.1155/2021/6681202
   Liu GJ, 2022, INFRARED PHYS TECHN, V121, DOI 10.1016/j.infrared.2021.104005
   Lu HX, 2020, IEEE ACCESS, V8, P156763, DOI 10.1109/ACCESS.2020.3017499
   Lu ZW, 2018, IEEE SIGNAL PROC LET, V25, P1585, DOI 10.1109/LSP.2018.2867896
   Luo Y, 2022, OPTIK, V258, DOI 10.1016/j.ijleo.2022.168914
   Mannam V, 2022, OPTICA, V9, P335, DOI 10.1364/OPTICA.448287
   Nickfarjam AM, 2017, APPL INTELL, V47, P1132, DOI 10.1007/s10489-017-0931-2
   Nnolim UA, 2021, INT J IMAGE GRAPH, V21, DOI 10.1142/S0219467821500327
   Paul A, 2022, OPTIK, V259, DOI 10.1016/j.ijleo.2022.168899
   Qian K, 2021, APPL INTELL, V51, P1108, DOI 10.1007/s10489-020-01873-3
   Qin Z, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102028
   Qu Z, 2021, MULTIMEDIA SYST, V27, P33, DOI 10.1007/s00530-020-00691-4
   Ravikumar M, 2022, INT J IMAGE GRAPH, V22, DOI 10.1142/S0219467822500103
   Ravirathinam P, 2021, IEEE ACCESS, V9, P31053, DOI 10.1109/ACCESS.2021.3059498
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Sharma, 2021, J AMB INTEL HUM COMP, V13, P1941
   Singh D, 2019, APPL INTELL, V49, P4276, DOI 10.1007/s10489-019-01504-6
   Subramani B, 2020, COLOR RES APPL, V45, P644, DOI 10.1002/col.22502
   Suresha M, 2022, INT J IMAGE GRAPH, V22, DOI 10.1142/S0219467822500036
   WAN MJ, 2018, REMOTE SENS-BASEL, V10
   Wang B., 2015, SPIE, V9817, P188
   Wang Y, 2021, SIGNAL PROCESS, V189, DOI 10.1016/j.sigpro.2021.108254
   Wang Yuanbin., 2019, J PHYS C SERIES, V1302
   Wang YK, 2022, IEEE T INF FOREN SEC, V17, P500, DOI 10.1109/TIFS.2022.3146766
   Wang YW, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3145361
   Wu HB, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14020185
   Xianhong L., 2017, ACTA OPT SINICA, V37
   Xing S, 2022, ADV SCI, V9, DOI 10.1002/advs.202105113
   Xu LL, 2022, IEEE SIGNAL PROC LET, V29, P1953, DOI 10.1109/LSP.2022.3205271
   Yang CX, 2022, OPTIK, V253, DOI 10.1016/j.ijleo.2022.168592
   Yang DX, 2022, NEUROCOMPUTING, V491, P132, DOI 10.1016/j.neucom.2022.03.061
   Yin JL, 2021, IEEE T MULTIMEDIA, V23, P1049, DOI 10.1109/TMM.2020.2992962
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhang H, 2022, INFRARED PHYS TECHN, V120, DOI 10.1016/j.infrared.2021.104000
   Zhang J, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102040
   Zhang XB, 2021, MULTIMED TOOLS APPL, V80, P29745, DOI 10.1007/s11042-021-11184-5
NR 62
TC 0
Z9 0
U1 4
U2 23
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2024
VL 40
IS 1
BP 53
EP 71
DI 10.1007/s00371-022-02765-y
EA FEB 2023
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5B8
UT WOS:000926051700001
DA 2024-07-18
ER

PT J
AU Yang, H
   Zhang, Y
AF Yang, Hao
   Zhang, Yi
TI A context- and level-aware feature pyramid network for object detection
   with attention mechanism
SO VISUAL COMPUTER
LA English
DT Article
DE Object detection; Contextual information; Attention mechanism; Feature
   fusion
AB An object detection task includes classification and localization, which require large receptive field and high-resolution input, respectively. How to strike a balance between the two conflicting needs remains a difficult problem in this field. Fortunately, feature pyramid network (FPN) realizes the fusion of low-level and high-level features, which alleviates this dilemma to some extent. However, existing FPN-based networks overlooked the importance of features of different levels during fusion process. Their simple fusion strategies can easily cause overwritten of important information, leading to serious aliasing effect. In this paper, we propose an improved object detector based on context- and level-aware feature pyramid networks. Experiments have been conducted on mainstream datasets to validate the effectiveness of our network, where it exhibits superior performances than other state-of-the-art works.
C1 [Yang, Hao; Zhang, Yi] Sichuan Univ, Coll Comp Sci, Chengdu 610041, Peoples R China.
C3 Sichuan University
RP Zhang, Y (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610041, Peoples R China.
EM 515294355@qq.com; yi.zhang@scu.edu.cn
CR Cao JL, 2020, IEEE T CIRC SYST VID, V30, P3372, DOI 10.1109/TCSVT.2019.2950526
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen XY, 2021, IEEE T CIRC SYST VID, V31, P715, DOI 10.1109/TCSVT.2020.2987465
   Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5
   Dai JF, 2016, ADV NEUR IN, V29
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gupta AK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3108503
   Gupta AK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3064423
   Gupta AK, 2021, PATTERN ANAL APPL, V24, P625, DOI 10.1007/s10044-020-00925-1
   Gupta AK, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22101174
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jing YH, 2022, J HYDROL, V613, DOI 10.1016/j.jhydrol.2022.128388
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Leng JX, 2022, APPL INTELL, V52, P2621, DOI 10.1007/s10489-020-02037-z
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li L, 2022, APPL INTELL, V52, P1, DOI 10.1007/s10489-021-02377-4
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo YH, 2022, MULTIMED TOOLS APPL, V81, P30685, DOI 10.1007/s11042-022-11940-1
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Park H, 2022, IEEE ACCESS, V10, P38742, DOI 10.1109/ACCESS.2022.3166928
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2021, IEEE ACCESS, V9, P107024, DOI 10.1109/ACCESS.2021.3100369
   Wang JJ, 2022, APPL INTELL, V52, P12844, DOI 10.1007/s10489-021-03147-y
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Xie J, 2023, IEEE T MULTIMEDIA, V25, P2153, DOI 10.1109/TMM.2022.3143707
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Zhang JF, 2023, APPL INTELL, V53, P2173, DOI 10.1007/s10489-022-03487-3
   Zhang WC, 2021, VISUAL COMPUT, V37, P881, DOI 10.1007/s00371-020-01839-z
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280
NR 48
TC 5
Z9 5
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6711
EP 6722
DI 10.1007/s00371-022-02758-x
EA JAN 2023
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000916562400001
DA 2024-07-18
ER

PT J
AU Wei, C
   Liang, JZ
   Liu, H
   Hou, ZJ
   Huan, Z
AF Wei, Cheng
   Liang, Jiuzhen
   Liu, Hao
   Hou, Zhenjie
   Huan, Zhan
TI Multi-stage unsupervised fabric defect detection based on DCGAN
SO VISUAL COMPUTER
LA English
DT Article
DE Fabric defect detection; Generative adversarial network; Unsupervised
   learning; Image reconstruction
ID SYSTEM
AB The diversity of fabric defects and the lack of defect samples make detecting fabric defects an important and challenging problem. Currently, unsupervised algorithms are widely used for surface defect detection as they do not require annotated data and therefore reduce the cost of data acquisition. This paper presents a multi-stage unsupervised fabric defect detection method based on DCGAN. The method consists of three stages: the GAN training, the encoder training, and the classifier training. The first two stages allow our model to reconstruct the test images. In the image reconstruction process, we use a linear weighted fusion method to reduce the interference of defects. When the reconstructed image is subtracted from the original, we get a residual map that highlights the defects. This pixel-level detection makes it easier to detect different types of defects. In addition, we introduce a classifier training phase to generate a likelihood map for the test images. Each pixel value in the likelihood map represents the probability of the original map having a defect in that location region. Finally, we fuse the residual map with the likelihood map and further perform threshold segmentation on the fused residual map. Our method uses a separate training strategy at each stage and learns from a set of image patches cropped out online. The experimental results are compared with other methods in recent years and validate the method's superiority in terms of f-score metrics.
C1 [Wei, Cheng; Liang, Jiuzhen; Liu, Hao; Hou, Zhenjie; Huan, Zhan] Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou 213164, Peoples R China.
   [Liang, Jiuzhen] Changzhou Univ, Jiangsu Engn Res Ctr Digital Twinning Technol Key, Changzhou 213164, Peoples R China.
C3 Changzhou University; Changzhou University
RP Liang, JZ (corresponding author), Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou 213164, Peoples R China.; Liang, JZ (corresponding author), Changzhou Univ, Jiangsu Engn Res Ctr Digital Twinning Technol Key, Changzhou 213164, Peoples R China.
EM jzliang@cczu.edu.cn
RI Hou, Zhenjie/HKW-7644-2023
OI Liang, Jiuzhen/0000-0003-3511-3374
FU Open Project of Key Laboratory of Ministry of Public Security for Road
   Traffic Safety; Jiangsu Engineering Research Center of Digital Twinning
   Technology for Key Equipment in Petrochemical Process; 
   [2021ZDSYSKFKT04];  [DT2020720]
FX AcknowledgementsThis study was supported by Open Project of Key
   Laboratory of Ministry of Public Security for Road Traffic Safety (No.
   2021ZDSYSKFKT04), Jiangsu Engineering Research Center of Digital
   Twinning Technology for Key Equipment in Petrochemical Process (No.
   DT2020720).
CR Abouelela A, 2005, PATTERN RECOGN LETT, V26, P1435, DOI 10.1016/j.patrec.2004.11.016
   Bao XY, 2022, VISUAL COMPUT, V38, P2707, DOI 10.1007/s00371-021-02148-9
   Cao JJ, 2016, INT J CLOTH SCI TECH, V28, P516, DOI 10.1108/IJCST-10-2015-0117
   Cheng ZY, 2022, LECT NOTES COMPUT SC, V13698, P514, DOI 10.1007/978-3-031-19839-7_30
   Cui Y., 2022, WACV, P3411, DOI 10.1109/WACV51458
   Cui YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8118, DOI 10.1109/ICCV48922.2021.00803
   Deotale NT, 2019, 3D RES, V10, DOI 10.1007/s13319-019-0215-1
   Ding Shumin, 2011, 2011 International Conference on Multimedia Technology, P2903
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Habib MT, 2016, 2016 INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE (IWCI), P101, DOI 10.1109/IWCI.2016.7860347
   Hanbay K, 2016, OPTIK, V127, P11960, DOI 10.1016/j.ijleo.2016.09.110
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu G.H., 2018, J ENG FIBER FABR, V13
   Hu GH, 2020, TEXT RES J, V90, P247, DOI 10.1177/0040517519862880
   Ji X, 2020, J ENG FIBER FABR, V15, DOI 10.1177/1558925020957654
   Jing JF, 2022, TEXT RES J, V92, P30, DOI 10.1177/0040517520928604
   Kang XJ, 2020, IEEE ACCESS, V8, P221808, DOI 10.1109/ACCESS.2020.3041849
   Karlekar VV, 2015, 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION ICCUBEA 2015, P712, DOI 10.1109/ICCUBEA.2015.145
   Li C, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/9948808
   Li CL, 2019, IEEE ACCESS, V7, P83962, DOI 10.1109/ACCESS.2019.2925196
   Li MY, 2020, MED PHYS, V47, P1139, DOI 10.1002/mp.14003
   Li YD, 2017, IEEE T AUTOM SCI ENG, V14, P1256, DOI 10.1109/TASE.2016.2520955
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Liu DF, 2021, PROC CVPR IEEE, P9811, DOI 10.1109/CVPR46437.2021.00969
   Liu DF, 2021, AAAI CONF ARTIF INTE, V35, P6101
   Liu DF, 2020, NEUROCOMPUTING, V409, P1, DOI 10.1016/j.neucom.2020.05.027
   Liu JH, 2020, IEEE T IMAGE PROCESS, V29, P3388, DOI 10.1109/TIP.2019.2959741
   Mei S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041064
   Ngan HYT, 2011, IMAGE VISION COMPUT, V29, P442, DOI 10.1016/j.imavis.2011.02.002
   Radford A., 2016, ARXIV, DOI [10.48550/ARXIV.1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Raheja JL, 2013, OPTIK, V124, P6469, DOI 10.1016/j.ijleo.2013.05.004
   Raheja JL, 2013, OPTIK, V124, P5280, DOI 10.1016/j.ijleo.2013.03.038
   Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Shi BS, 2019, IEEE ACCESS, V7, P130423, DOI 10.1109/ACCESS.2019.2939843
   Shi W., 2022, SOC SCI ELECT PUBL, DOI [10.2139/ssrn.4061500, DOI 10.2139/SSRN.4061500]
   Tsang CSC, 2016, PATTERN RECOGN, V51, P378, DOI 10.1016/j.patcog.2015.09.022
   Wu QF, 2020, IEEE ACCESS, V8, P98716, DOI 10.1109/ACCESS.2020.2997001
   Xing P., 2022, ARXIV, DOI DOI 10.48550/ARXIV.2209.12441
   Xing P., 2022, ARXIV
   Yan LQ, 2022, IEEE T CIRC SYST VID, V32, P6642, DOI [10.1109/TCSVT.2022.3177320, 10.1109/tcsvt.2022.3177320]
   Zhu DD, 2015, AUTEX RES J, V15, P226, DOI 10.1515/aut-2015-0001
NR 42
TC 4
Z9 5
U1 18
U2 63
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6655
EP 6671
DI 10.1007/s00371-022-02754-1
EA DEC 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000902032900001
DA 2024-07-18
ER

PT J
AU Lan, JY
   Ye, FH
   Ye, ZH
   Xu, PP
   Ling, WK
   Huang, GH
AF Lan, Jiaying
   Ye, Fenghua
   Ye, Zhenghua
   Xu, Pingping
   Ling, Wing-Kuen
   Huang, Guoheng
TI Unsupervised style-guided cross-domain adaptation for few-shot stylized
   face translation
SO VISUAL COMPUTER
LA English
DT Article
DE GAN; Image-to-image translation; Few-shot learning; Domain adaptation
AB When a domain-to-domain translation-based generative model is trained on a target domain with limited instances and no paired samples, two key issues will emerge: the introduction of noticeable artifacts and the severe overfitting. Existing methods require several thousands of images to perform the proper training, and they tend to consider only a few key elements to control the generation of a single style, while the noticeable artifacts are introduced in the local shapes of their generated faces. When the total number of available images is reduced to just a few images, the training samples are usually overfitted or of poor quality. To address these issues, we propose the portrait style conversion-generative adversarial network to translate one portrait of a photo into varieties of the specific style appearances. Firstly, for the purpose of reducing the noticeable artifacts, a visual content disentangled module is put forward to extract the disentangled visual representations, preserving both the similarities and differences among the instances of the source images. At the same time, a cross-domain generative module is proposed to complete the cross-domain adaptation from source to target. Then, for the purpose of reducing the overfitting, an anchor-based strategy called the adaptive joint patch discriminative module is presented to encourage different levels of realism over different regions in the latent space. Lastly, experiments and ablation studies of the existing state-of-the-art GAN-based model performance comparison were conducted to validate the efficacy of our approach. The proposed method preserves more global characteristics of source image and improves the total image perceptual quality significantly. The results show high superiority of our method compared to the existing state-of-the-art models.
C1 [Lan, Jiaying; Xu, Pingping; Huang, Guoheng] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Ye, Fenghua] Guangdong Univ Technol, Sch Art & Design, Guangzhou 510006, Peoples R China.
   [Ye, Zhenghua] Guangzhou Acad Fine Arts, City Coll, Guangzhou 510006, Peoples R China.
   [Ling, Wing-Kuen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology;
   Guangzhou Academy of Fine Arts; Guangdong University of Technology
RP Huang, GH (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.; Ye, FH (corresponding author), Guangdong Univ Technol, Sch Art & Design, Guangzhou 510006, Peoples R China.
EM yefenghua@gdut.edu.cn; kevinwong@gdut.edu.cn
RI Xu, ping/KDN-1175-2024; s, x/JXY-8160-2024
OI Huang, Guoheng/0000-0002-3640-3229; Xu, Pingping/0000-0002-7966-5317
FU Natural Science Foundation of Guangdong Province [2021A1515011888];
   Science and technology research in key areas in Foshan [2020001006832];
   Key-Area Research and Development Program of Guangdong Province
   [2018B010109007, 2019B010153002]; Guangzhou R &D Programme in Key Areas
   of Science and Technology Projects [202007040006]; Guangdong Provincial
   Key Laboratory of Cyber-Physical System [2020B1212060069]; Program of
   Marine Economy Development (Six Marine Industries) Special Foundation of
   Department of Natural Resources of Guangdong Province [GDNRC [2020]056]
FX This work was supported in part by the Natural Science Foundation of
   Guangdong Province under Grant 2021A1515011888, and the Science and
   technology research in key areas in Foshan under Grant 2020001006832,
   the Key-Area Research and Development Program of Guangdong Province
   under Grant 2018B010109007 and 2019B010153002, and the Guangzhou R &D
   Programme in Key Areas of Science and Technology Projects under Grant
   202007040006, and the Guangdong Provincial Key Laboratory of
   Cyber-Physical System under Grant 2020B1212060069, and the Program of
   Marine Economy Development (Six Marine Industries) Special Foundation of
   Department of Natural Resources of Guangdong Province under Grant GDNRC
   [2020]056.
CR Bai J, 2020, VISUAL COMPUT, V36, P2145, DOI 10.1007/s00371-020-01943-0
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Cao B, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108446
   Cao K., 2018, ACM Trans. on Graph., V37, P1, DOI [10.1145/3272127.3275046, DOI 10.1145/3272127.3275046]
   Chen R., 2020, PROC IEEECVF C COMPU, P8165, DOI DOI 10.1109/CVPR42600.2020.00819
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Esser P, 2019, IEEE I CONF COMP VIS, P2699, DOI 10.1109/ICCV.2019.00279
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1172, DOI 10.1145/3240508.3240655
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Islam NU, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9030395
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jahanian A, 2019, ARXIV190707171
   Jha AH, 2018, LECT NOTES COMPUT SC, V11207, P829, DOI 10.1007/978-3-030-01219-9_49
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2018, P INT C LEARN REPR I
   Karras T., 2020, arXiv
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim Junho, 2019, ICLR
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li Y., 2020, ARXIV
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu YF, 2021, VISUAL COMPUT, V37, P2513, DOI 10.1007/s00371-021-02204-4
   Mejjati YA, 2018, ADV NEUR IN, V31
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mo S., 2020, ARXIV
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nizan Ori, 2020, CVPR
   Noguchi A, 2019, IEEE I CONF COMP VIS, P2750, DOI 10.1109/ICCV.2019.00284
   Nozawa N, 2022, VISUAL COMPUT, V38, P1317, DOI 10.1007/s00371-020-02024-y
   Park Taesung, 2020, Advances in Neural Information Processing Systems, V33, P7198
   Radford A, 2021, PR MACH LEARN RES, V139
   Saharia C., 2022, arXiv
   Salimans T, 2016, ADV NEUR IN, V29
   Shi YC, 2019, PROC CVPR IEEE, P10754, DOI 10.1109/CVPR.2019.01102
   Sun Q, 2022, VISUAL COMPUT, V38, P1283, DOI 10.1007/s00371-021-02219-x
   Ul Islam N, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9050743
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   vandenOord Aaron, 2018, ARXIV180703748
   Wang LD, 2018, IEEE INT CONF AUTOMA, P83, DOI 10.1109/FG.2018.00022
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Yaxing, 2020, P IEEE CVF C COMP VI
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Zhang SY, 2019, VISUAL COMPUT, V35, P1157, DOI 10.1007/s00371-019-01691-w
   Zhao S., 2020, Advances in Neural Information Processing Systems, V33, P7559
   Zhao Yihao, 2020, ECCV
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 64
TC 4
Z9 3
U1 6
U2 17
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6167
EP 6181
DI 10.1007/s00371-022-02719-4
EA NOV 2022
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000884709200001
DA 2024-07-18
ER

PT J
AU Weng, KC
   Lin, ST
   Hu, CC
   Soong, RT
   Chi, MT
AF Weng, Kai-Chun
   Lin, Shu-Ting
   Hu, Chen-Chi
   Soong, Ru-Tai
   Chi, Ming-Te
TI Multi-view approach for drone light show
SO VISUAL COMPUTER
LA English
DT Article
DE 3D ambigram; Drone light show; Integer programming
ID COOPERATIVE COLLISION-AVOIDANCE
AB Drones, or precisely quadrotors, have been increasingly used in the field of robotics and also in entertainment. Coordinated multiple drones that form visual presentations through their equipped LEDs are known as drone light shows (Waibel, M., Keays, B., Augugliaro, F.: in Drone shows: Creative potential and best practices. ETH Zurich, 2017). Such a performance offers visual enjoyment for a large audience, particularly in festivals. However, the majority of current drone light shows are manually coordinated by personnel using software. Drone light shows also have limited viewing range, thereby preventing the audience from getting a good view of the actual performance. This study proposes a method to provide multiple visual presentations in accordance with multiple viewing angles. We use visual hull to filter out the candidate areas that form input images, and takes projection error and classification values as weight for optimization. Consequently, the proposed method reduces the number of drones needed to form a multi-view structure for visual presentations. Furthermore, to meet the demands of performing the animation in multi-view structure, we implement the flight algorithm to locate the most suitable corresponding points between two different structures, and then generate the shortest flight paths without collision. Experiments conducted in our simulator provide additional insights and discussions, and each factor is visualized to provide an improved understanding of our approach for multi-view drone light shows.
C1 [Weng, Kai-Chun; Lin, Shu-Ting; Hu, Chen-Chi; Soong, Ru-Tai; Chi, Ming-Te] Natl Chengchi Univ, Dept Comp Sci, Taipei, Taiwan.
C3 National Chengchi University
RP Chi, MT (corresponding author), Natl Chengchi Univ, Dept Comp Sci, Taipei, Taiwan.
EM mtchi@cs.nccu.edu.tw
OI Chi, Ming-Te/0000-0002-9014-7561
FU Ministry of Science and Technology, Taiwan [MOST 110-2221-E-004-009,
   MOST 110-2634-F-004-001]
FX This work was supported by The Ministry of Science and Technology,
   Taiwan, under GRANT No. MOST 110-2221-E-004-009 and MOST
   110-2634-F-004-001 through Pervasive Artificial Intelligence Research
   (PAIR) Labs. We would also thank Sin-Fei Lee for the demo video editing.
CR Ali AA, 2016, ROBOT AUTON SYST, V75, P119, DOI 10.1016/j.robot.2015.10.010
   [Anonymous], 2011, Rvo2 library: Reciprocal collision avoidance for real-time multi-agent simulation
   Bradski G, 2000, DR DOBBS J, V25, P120
   Fujimori A, 2000, J ROBOTIC SYST, V17, P347, DOI 10.1002/1097-4563(200007)17:7<347::AID-ROB1>3.0.CO;2-A
   Hsiao KW, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275070
   Kolev K, 2009, INT J COMPUT VISION, V84, P80, DOI 10.1007/s11263-009-0233-1
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Mitra NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618502
   Ohta A., 2017, P SIGGRAPH 17 EM TEC, DOI [10.1145/3084822.3108158, DOI 10.1145/3084822.3108158]
   Perron L., OR TOOLS 7 2
   Rockwood AP, 1997, COMPUT AIDED DESIGN, V29, P279, DOI 10.1016/S0010-4485(96)00056-5
   Shah M.A., 2010, 9 IEEE INT C CYBERNE, P1
   Skrjanc I, 2007, ITI, P451, DOI 10.1109/ITI.2007.4283813
   Trager M, 2016, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2016.364
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   Waibel M., 2017, Tech. Rep., DOI DOI 10.3929/ETHZ-A-010831954
   Wikipedia Contributors., 2020, AMB WIK FREE ENC
   Xiong WD, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3198034
NR 19
TC 1
Z9 1
U1 6
U2 12
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5797
EP 5808
DI 10.1007/s00371-022-02696-8
EA NOV 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000881913500001
DA 2024-07-18
ER

PT J
AU Kumar, S
   Gupta, SK
   Gupta, U
   Agarwal, M
AF Kumar, Sanjeev
   Gupta, Suneet Kumar
   Gupta, Umesh
   Agarwal, Mohit
TI Non-overlapping block-level difference-based image forgery detection and
   localization (NB-localization)
SO VISUAL COMPUTER
LA English
DT Article
DE Image processing; Forged images; Original image; Copy-move forgery
ID SEGMENTATION; NETWORK
AB With advent of digital devices, we are surrounded by many digital images. We usually believe on digital images in whatever form presented to us. Therefore, we need to be careful as the images may be forged. There exist several image forgeries through which original intent of the image may be hidden and some other meaning is reflected through forgery. Copy-move forgery is one such forgery technique, where the manipulator copies certain portion of the image and duplicates it in some other portion of the same image. In this paper, we propose a novel approach to detect the copy-move forgery in images using non-overlapping block level pixel comparisons and that can achieve better detection and classification accuracy. This approach divides image into 4, 5, 6 or more such blocks and compare each block by moving sliding window over the entire image which is not overlapping with current block. It was found that with different number of blocks the forged region of different sizes can be easily found. We have used SSIM (structure similarity index) parameter to classify the image as forged or original. Algorithm is simulated on various datasets including (MICC, CASIA, coverage, and COMOFOD, etc.) and achieved maximum accuracy of 98% and also compared our result on precision, recall, FPR and FNR including other parameters.
C1 [Kumar, Sanjeev; Gupta, Suneet Kumar; Gupta, Umesh; Agarwal, Mohit] Bennett Univ, Greater Noida, India.
   [Kumar, Sanjeev] KIET Grp Inst, Delhi Ncr, Ghaziabad, India.
C3 KIET Group of Institutions
RP Kumar, S (corresponding author), Bennett Univ, Greater Noida, India.; Kumar, S (corresponding author), KIET Grp Inst, Delhi Ncr, Ghaziabad, India.
EM look4sanjeev@gmail.com
RI Gupta, Dr. SUneet/ABG-7279-2022; Kumar, Dr. Sanjeev/AAJ-7172-2021;
   GUPTA, UMESH/AAC-4589-2021
OI Gupta, Dr. SUneet/0000-0002-0757-1290; Kumar, Dr.
   Sanjeev/0000-0001-9977-8004; GUPTA, UMESH/0000-0002-1547-7974
CR Abbas MN, 2021, 2021 IEEE 19TH WORLD SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI 2021), P125, DOI 10.1109/SAMI50585.2021.9378690
   Agarwal R, 2022, EVOL SYST-GER, V13, P27, DOI 10.1007/s12530-021-09367-4
   Alamuru, 2021, MATER TODAY-PROC, DOI [10.1016/j.matpr.2021.03.154, DOI 10.1016/J.MATPR.2021.03.154]
   Alberry Hesham A., 2018, Future Computing and Informatics Journal, V3, P159, DOI 10.1016/j.fcij.2018.03.001
   Alharbi A, 2024, APPL COMPUT INFORM, V20, P89, DOI 10.1016/j.aci.2019.12.001
   Amerini I, 2017, IEEE COMPUT SOC CONF, P1865, DOI 10.1109/CVPRW.2017.233
   Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   Ansari MD, 2018, INT J SIGNAL IMAGING, V11, P44, DOI 10.1504/IJSISE.2018.10011742
   Barghout L., 2013, J. Vis, V13, P709, DOI DOI 10.1167/13.9.709
   Bi XL, 2019, IEEE COMPUT SOC CONF, P30, DOI 10.1109/CVPRW.2019.00010
   Bondi L, 2017, IEEE COMPUT SOC CONF, P1855, DOI 10.1109/CVPRW.2017.232
   Chen HP, 2022, MULTIMEDIA SYST, V28, P363, DOI 10.1007/s00530-021-00801-w
   Chen H, 2022, VISUAL COMPUT, V38, P2675, DOI 10.1007/s00371-021-02146-x
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Cozzolino D, 2014, IEEE IMAGE PROC, P5312, DOI 10.1109/ICIP.2014.7026075
   Dixit R, 2017, IET IMAGE PROCESS, V11, P301, DOI 10.1049/iet-ipr.2016.0537
   Elaskily MA, 2020, MULTIMED TOOLS APPL, V79, P19167, DOI 10.1007/s11042-020-08751-7
   Elaskily MA, 2019, MULTIMED TOOLS APPL, V78, P15353, DOI 10.1007/s11042-018-6891-7
   Ferrara P, 2012, IEEE T INF FOREN SEC, V7, P1566, DOI 10.1109/TIFS.2012.2202227
   Gan YN, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2021.102783
   Gupta D, 2021, MULTIMED TOOLS APPL, V80, P30091, DOI 10.1007/s11042-020-10242-8
   Hossein-Nejad Z, 2022, VISUAL COMPUT, V38, P1991, DOI 10.1007/s00371-021-02261-9
   Isaac MM, 2018, J INTELL FUZZY SYST, V34, P1679, DOI 10.3233/JIFS-169461
   Jain, 2021, ADV IMAGE SPLICING C, DOI [10.1109/Confluence51648.2021.9377104, DOI 10.1109/CONFLUENCE51648.2021.9377104]
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Kaliyar RK, 2020, COGN SYST RES, V61, P32, DOI 10.1016/j.cogsys.2019.12.005
   Kasban H, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106728
   Kaur, 2016, INT J ENG DEV RES, V4
   Krylov VA, 2016, IEEE T IMAGE PROCESS, V25, P4704, DOI 10.1109/TIP.2016.2593340
   Kumar B.S., 2018, Journal of computational and theoretical Nanoscience, V15, P2560, DOI [10.1166/jctn.2018.7498, DOI 10.1166/JCTN.2018.7498]
   Kumar Sanjeev, 2020, 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), P253, DOI 10.1109/ICRITO48877.2020.9197955
   Kumar S, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103644
   Lagouvardos P, 2018, J PROSTHODONT RES, V62, P503, DOI 10.1016/j.jpor.2018.07.005
   Li GH, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1750
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P1240, DOI 10.1109/TIFS.2017.2656823
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Lyu QY, 2021, J VIS COMMUN IMAGE R, V76, DOI 10.1016/j.jvcir.2021.103057
   Maind Rohini A, 2014, INT J SOFT COMPUT EN, V4, P49
   Manjunatha, 2017, CIIT INT J DIGIT IMA, V9
   Manu VT, 2018, SIGNAL IMAGE VIDEO P, V12, P549, DOI 10.1007/s11760-017-1191-7
   Meena KB, 2020, J INF SECUR APPL, V52, DOI 10.1016/j.jisa.2020.102481
   Meena KB, 2020, MULTIMED TOOLS APPL, V79, P8197, DOI 10.1007/s11042-019-08343-0
   Meena KB, 2019, MULTIMED TOOLS APPL, V78, P33505, DOI 10.1007/s11042-019-08082-2
   Mushtaq S, 2018, INT J FUTUR GENER CO, V11, P11, DOI 10.14257/ijfgcn.2018.11.2.02
   Pun CM, 2015, IEEE T INF FOREN SEC, V10, P1705, DOI 10.1109/TIFS.2015.2423261
   Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010
   Shrivastava VK, 2017, COMPUT METH PROG BIO, V150, P9, DOI 10.1016/j.cmpb.2017.07.011
   Sun Y, 2018, SECUR COMMUN NETW, DOI 10.1155/2018/1301290
   Swain Monalisa, 2022, Integration, The VLSI Journal, P12, DOI 10.1016/j.vlsi.2021.11.004
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Tyagi S, 2023, VISUAL COMPUT, V39, P813, DOI 10.1007/s00371-021-02347-4
   vcl, 2021, COMOFOD DAT REP
   Wang YL, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102536
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu, 2020, MULTIMED TOOLS APPL, V2, P57, DOI [10.1007/978-981-10-7644-2, DOI 10.1007/978-981-10-7644-2]
   Yeap YY, 2018, 2018 IEEE 14TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA 2018), P239, DOI 10.1109/CSPA.2018.8368719
   Zhang Y, 2016, CRYPTOL INF SEC SER, V14, P1, DOI 10.3233/978-1-61499-617-0-1
   Zhao XD, 2015, IEEE T CIRC SYST VID, V25, P185, DOI 10.1109/TCSVT.2014.2347513
   Zhong JL, 2020, IEEE T INF FOREN SEC, V15, P2134, DOI 10.1109/TIFS.2019.2957693
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
NR 61
TC 0
Z9 0
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2023
VL 39
IS 12
BP 6029
EP 6040
DI 10.1007/s00371-022-02710-z
EA NOV 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X7HK1
UT WOS:000877975900002
DA 2024-07-18
ER

PT J
AU Tang, KK
   Chen, YH
   Peng, WL
   Zhang, YL
   Fang, ME
   Wang, Z
   Song, P
AF Tang, Keke
   Chen, Yuhong
   Peng, Weilong
   Zhang, Yanling
   Fang, Meie
   Wang, Zheng
   Song, Peng
TI RepPVConv: attentively fusing reparameterized voxel features for
   efficient 3D point cloud perception
SO VISUAL COMPUTER
LA English
DT Article
DE Reparameterization; PVCNN; Point clouds; Attention; Efficient
AB Designing efficient deep learning models for 3D point clouds is an important research topic. Point-voxel convolution (Liu et al. in NeurIPS, 2019) is a pioneering approach in this direction, but it still has considerable room for improvement in terms of performance, since it has quite a few layers of simple 3D convolutions and linear point-voxel feature fusion operations. To resolve these issues, we propose a novel reparameterizable point-voxel convolution (RepPVConv) block. First, RepPVConv adopts two reparameterizable 3D convolution modules to extract more informative voxel features without introducing any extra computational overhead for inference. The rationale is that the reparameterizable 3D convolution modules are trained in high-capacity modes but are reparameterized into low-capacity modes during inference while losslessly maintaining the original performance. Second, RepPVConv attentively fuses the reparameterized voxel features with those of points. Since the proposed approach operates in a nonlinear manner, descriptive reparameterized voxel features can be better utilized. Extensive experimental results show that RepPVConv-based networks are efficient in terms of both GPU memory consumption and computational complexity and significantly outperform the state-of-the-art methods.
C1 [Tang, Keke; Chen, Yuhong; Peng, Weilong; Zhang, Yanling; Fang, Meie] Guangzhou Univ, Guangzhou, Peoples R China.
   [Wang, Zheng] Southern Univ Sci & Technol, Shenzhen, Peoples R China.
   [Song, Peng] Singapore Univ Technol & Design, Singapore, Singapore.
C3 Guangzhou University; Southern University of Science & Technology;
   Singapore University of Technology & Design
RP Peng, WL; Fang, ME (corresponding author), Guangzhou Univ, Guangzhou, Peoples R China.
EM wlpeng@gzhu.edu.cn; fme@gzhu.edu.cn
RI Wang, Zheng/AAN-4580-2020; Fang, Meie/IYS-4458-2023; zhang,
   yanling/GRJ-9862-2022; Tang, Keke/JPX-2411-2023; Peng,
   Weilong/GVS-5360-2022
OI Fang, Meie/0000-0003-4292-8889; Peng, Weilong/0000-0001-5820-889X; Tang,
   Keke/0000-0003-0377-1022
FU National Natural Science Foundation of China [62102105, 62072126];
   Guangdong Basic and Applied Basic Research Foundation [2020A1515110997,
   2022A1515011501, 2022A1515010138]; Science and Technology Program of
   Guangzhou [202002030263, 202102010419, 202201020229]; Open Project
   Program of the State Key Lab of CAD and CG [A2218]; Zhejiang University
FX We thank the reviewers for the valuable comments. This work was
   supported in part by the National Natural Science Foundation of China
   (62102105, 62072126), Guangdong Basic and Applied Basic Research
   Foundation (2020A1515110997, 2022A1515011501, and 2022A1515010138), the
   Science and Technology Program of Guangzhou (202002030263, 202102010419
   and 202201020229), and the Open Project Program of the State Key Lab of
   CAD and CG (A2218), Zhejiang University.
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   Armeni Iro, 2017, arXiv
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chang A. X., 2015, ARXIV
   Chen LF, 2023, VISUAL COMPUT, V39, P5229, DOI 10.1007/s00371-022-02656-2
   Chen YH, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2286818
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   cicek Ozgtin, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955
   Li YY, 2018, ADV NEUR IN, V31
   Lin N, 2022, IEEE ROBOT AUTOM LET, V7, P1387, DOI 10.1109/LRA.2021.3140127
   Liu ZJ, 2019, ADV NEUR IN, V32
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Noh J, 2021, PROC CVPR IEEE, P14600, DOI 10.1109/CVPR46437.2021.01437
   Paszke A, 2019, ADV NEUR IN, V32
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Que ZZ, 2021, PROC CVPR IEEE, P6038, DOI 10.1109/CVPR46437.2021.00598
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi S., 2021, ARXIV
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun YL, 2020, VISUAL COMPUT, V36, P2407, DOI 10.1007/s00371-020-01892-8
   Tang Haotian, 2020, EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-58604-1_41
   Tang KK, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3196129
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Veit A, 2016, ADV NEUR IN, V29
   Wang PS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275050
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei Y, 2021, PROC CVPR IEEE, P6950, DOI 10.1109/CVPR46437.2021.00688
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Zagoruyko S, 2017, ARXIV
   Zhang F., 2020, EUR C COMP VIS SPRIN, P644
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 53
TC 5
Z9 5
U1 2
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD NOV
PY 2023
VL 39
IS 11
BP 5577
EP 5588
DI 10.1007/s00371-022-02682-0
EA OCT 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W5SX6
UT WOS:000870126400001
DA 2024-07-18
ER

PT J
AU Galetto, F
   Deng, G
AF Galetto, Fernando
   Deng, Guang
TI Single image defocus map estimation through patch blurriness
   classification and its applications
SO VISUAL COMPUTER
LA English
DT Article
DE Defocus blur estimation; CNN; Adaptive image enhancement; Shallow depth
   of field
ID QUALITY ASSESSMENT; BLUR; DEPTH; SEGMENTATION
AB Depth information is useful in many image processing applications. However, since taking a picture is a process of projection of a 3D scene onto a 2D imaging sensor, the depth information is embedded in the image. Extracting the depth information from the image is a challenging task. A guiding principle is that the level of blurriness due to defocus is related to the distance between the object and the focal plane. Based on this principle and the widely used assumption that Gaussian blur is a good model for defocus blur, we formulate the problem of estimating the spatially varying defocus blurriness as a Gaussian blur classification problem. We solve the problem by training a deep neural network to classify image patches into one of the 20 levels of blurriness. We have created a dataset of more than 500,000 image patches of size 32 x 32 which does not require human labelling. The dataset is used to train and test several well-known network models. We find that MobileNetV2 is suitable for this application due to its low memory requirement and high accuracy. The trained model is used to determine the patch blurriness which is then refined by applying an iterative weighted guided filter. The result is a defocus map that carries the information of the degree of blurriness for each pixel. We compare the proposed method with state-of-the-art techniques and we demonstrate its successful applications in adaptive image enhancement and defocus magnification limited to images that present a clear distinction between defocus levels.
C1 [Galetto, Fernando; Deng, Guang] La Trobe Univ, Dept Engn, Bundoora, Vic 3086, Australia.
C3 La Trobe University
RP Galetto, F (corresponding author), La Trobe Univ, Dept Engn, Bundoora, Vic 3086, Australia.
EM f.galetto@latrobe.edu.au; d.deng@latrobe.edu.au
OI Galetto, Fernando/0000-0002-7456-201X
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Al-Obaidi FE., 2015, AM J SIGNAL PROCESS, V5, P51, DOI DOI 10.5923/J.AJSP.20150503.01
   Anwar S, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115978
   Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x
   Bailey SW, 2015, VISUAL COMPUT, V31, P1697, DOI 10.1007/s00371-014-1050-2
   Bohra M, 2020, SIGNAL IMAGE VIDEO P, V14, P1163, DOI 10.1007/s11760-020-01656-w
   Chakrabarti A, 2010, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2010.5539954
   Chan SH, 2011, IEEE IMAGE PROC, P677, DOI 10.1109/ICIP.2011.6116643
   Chen DJ, 2016, IEEE IMAGE PROC, P3962, DOI 10.1109/ICIP.2016.7533103
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Cheong H, 2015, SENSORS-BASEL, V15, P880, DOI 10.3390/s150100880
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Favaro P, 2005, IEEE T PATTERN ANAL, V27, P406, DOI 10.1109/TPAMI.2005.43
   Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu H, 2006, IEEE IMAGE PROC, P617, DOI 10.1109/ICIP.2006.312411
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Javaran TA, 2017, VISUAL COMPUT, V33, P151, DOI 10.1007/s00371-015-1166-z
   Jiang Z., 2020, PROC IEEE 22 INT WOR, P1
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Karaali A, 2018, IEEE T IMAGE PROCESS, V27, P1126, DOI 10.1109/TIP.2017.2771563
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kubota A, 2005, IEEE T IMAGE PROCESS, V14, P1848, DOI 10.1109/TIP.2005.854468
   Lateef F, 2019, NEUROCOMPUTING, V338, P321, DOI 10.1016/j.neucom.2019.02.003
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Lin JY, 2013, IEEE T IMAGE PROCESS, V22, P4545, DOI 10.1109/TIP.2013.2274389
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P5155, DOI 10.1109/TIP.2018.2847421
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Marais FV, 2007, SIGNAL PROCESS-IMAGE, V22, P833, DOI 10.1016/j.image.2007.06.003
   Mather G, 1996, P ROY SOC B-BIOL SCI, V263, P169, DOI 10.1098/rspb.1996.0027
   McGraw T, 2015, VISUAL COMPUT, V31, P601, DOI 10.1007/s00371-014-0986-6
   Min XK, 2016, IEEE INT CON MULTI
   MITRA SK, 1991, INT CONF ACOUST SPEE, P2525, DOI 10.1109/ICASSP.1991.150915
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nasonov A, 2019, INT ARCH PHOTOGRAMM, V42-2, P161, DOI 10.5194/isprs-archives-XLII-2-W12-161-2019
   Nasonova A, 2015, IEEE SIGNAL PROC LET, V22, P417, DOI 10.1109/LSP.2014.2361492
   Ou FZ, 2019, IEEE IMAGE PROC, P1004, DOI [10.1109/icip.2019.8803047, 10.1109/ICIP.2019.8803047]
   Park J, 2017, PROC CVPR IEEE, P2760, DOI 10.1109/CVPR.2017.295
   Pech-Pacheco JL, 2000, INT C PATT RECOG, P314, DOI 10.1109/ICPR.2000.903548
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Ramponi G, 1996, J ELECTRON IMAGING, V5, P353, DOI 10.1117/12.242618
   Ramponi G, 1998, SIGNAL PROCESS, V67, P211, DOI 10.1016/S0165-1684(98)00038-3
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   SAKAMOTO T, 1984, APPL OPTICS, V23, P1707, DOI 10.1364/AO.23.001707
   Sakurikar P, 2019, IEEE WINT CONF APPL, P1337, DOI 10.1109/WACV.2019.00147
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi JP, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818136
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Sun ZG, 2020, IEEE T IMAGE PROCESS, V29, P500, DOI 10.1109/TIP.2019.2928631
   Suwajanakorn S, 2015, PROC CVPR IEEE, P3497, DOI 10.1109/CVPR.2015.7298972
   Tai YW, 2009, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2009.5414620
   Tan M., 2019, arXiv
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P12063
   Tang C, 2021, IEEE T MULTIMEDIA, V23, P624, DOI 10.1109/TMM.2020.2985541
   Tang HX, 2017, PROC CVPR IEEE, P4773, DOI 10.1109/CVPR.2017.507
   Vu DT, 2014, IEEE T IMAGE PROCESS, V23, P3428, DOI 10.1109/TIP.2014.2329389
   Wadhwa N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201329
   Wang L, 2018, ARXIV
   Xin S., 2021, P IEEECVF INT C COMP, P2228
   Xue F, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2380174
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Ye W, 2018, IEEE T IMAGE PROCESS, V27, P4465, DOI 10.1109/TIP.2018.2838660
   Yi X, 2016, IEEE T IMAGE PROCESS, V25, P1626, DOI 10.1109/TIP.2016.2528042
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Zhai YP, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.107996
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang XX, 2016, J VIS COMMUN IMAGE R, V35, P257, DOI 10.1016/j.jvcir.2016.01.002
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhou CY, 2011, INT J COMPUT VISION, V93, P53, DOI 10.1007/s11263-010-0409-8
   Zhu X, 2013, IEEE T IMAGE PROCESS, V22, P4879, DOI 10.1109/TIP.2013.2279316
   Zhuo SJ, 2011, PATTERN RECOGN, V44, P1852, DOI 10.1016/j.patcog.2011.03.009
   Ziou D, 2001, COMPUT VIS IMAGE UND, V81, P143, DOI 10.1006/cviu.2000.0899
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 81
TC 2
Z9 2
U1 1
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2023
VL 39
IS 10
BP 4555
EP 4571
DI 10.1007/s00371-022-02609-9
EA JUL 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T1GK8
UT WOS:000829815500002
OA Green Published, hybrid
DA 2024-07-18
ER

PT J
AU Zhou, JX
   Chen, YZ
   Li, YH
   Cao, S
   Wu, Y
   Jin, XG
AF Zhou, Jixiang
   Chen, Yanzhen
   Li, Yuanheng
   Cao, Shun
   Wu, Yu
   Jin, Xiaogang
TI Fast probe-leaking elimination using mask decomposition
SO VISUAL COMPUTER
LA English
DT Article
DE Computer graphics; Rendering; Global illumination; Compression
AB Light leaking in Probe GI is typically solved by visibility tests, which cannot benefit from hardware-aided tri-linear sampling. We present Mask Decomposition, which decomposes the visibility into probe-group indicators and their corresponding masks, making it possible to use tri-linear sampling in its reconstruction. We prove that the rendering overhead is significantly reduced with the help of Mask Decomposition, making the rendering at least 3 x faster than the state-of-the-art visibility-test Probe GI, and even as fast as the original leaking probe GI. We also present an efficient algorithm to solve the Mask Decomposition problem and a simple compression method to minimize the spatial overhead of the mask textures, which is much lower than in compression methods like Moving Basis Decomposition (MBD).
C1 [Zhou, Jixiang; Chen, Yanzhen; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Li, Yuanheng; Cao, Shun; Wu, Yu] Tencent, Unit Two,Bldg C,Kexing Sci Pk,Kejizhongsan Ave, Shenzhen 518057, Peoples R China.
C3 Zhejiang University; Tencent
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM jin@cad.zju.edu.cn
RI cao, shun/GZL-9022-2022
OI Jin, Xiaogang/0000-0001-7339-2920; Zhou, Jixiang/0000-0002-7949-3869
FU Key R &D Program of Zhejiang Province [2022C03126]; Science and
   Technology Innovation 2025 Major Project of Ningbo [2020Z007]
FX We thank the anonymous reviewers for their constructive comments.
   Xiaogang Jin was supported by the Key R &D Program of Zhejiang Province
   (Grant No. 2022C03126) and the Science and Technology Innovation 2025
   Major Project of Ningbo (Grant No. 2020Z007).
CR [Anonymous], 2009, P S INT 3D GRAPH GAM, DOI 10.1145/1507149.1507161.5,7
   Garcia K., 2020, SPECIAL INTEREST GRO, P1
   Hu J., 2021, VISUAL COMPUT, P1
   Krivanek Jaroslav, 2009, Practical Global Illumination with Irradiance Caching
   Majercik Z., 2020, ARXIV PREPRINT ARXIV
   Majercik Z, 2019, J Comput Graph Tech, V8
   McGuire M., 2017, Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, I3D'17, p2:1, DOI DOI 10.1145/3023368.3023378
   Nishino K, 2005, IEEE T PATTERN ANAL, V27, P1675, DOI 10.1109/TPAMI.2005.193
   Silvennoinen A, 2021, COMPUT GRAPH FORUM, V40, P127, DOI 10.1111/cgf.14346
   Sloan PP, 2003, ACM T GRAPHIC, V22, P382, DOI 10.1145/882262.882281
   Wang Y, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317024
   William D., 2006, P 2006 S INT 3D GRAP, P161, DOI DOI 10.1145/1111411.1111440
NR 12
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2022
VL 38
IS 9-10
SI SI
BP 3279
EP 3288
DI 10.1007/s00371-022-02576-1
EA JUL 2022
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4N3NE
UT WOS:000827919200001
DA 2024-07-18
ER

PT J
AU Shi, MW
   Fan, LW
   Li, XM
   Zhang, CM
AF Shi, Miaowen
   Fan, Linwei
   Li, Xuemei
   Zhang, Caiming
TI A competent image denoising method based on structural information
   extraction
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Image denoising; Low-rank approximation; Structural information
   extraction; Kernel wiener filtering
ID LOW-RANK APPROXIMATION; REPRESENTATION; MINIMIZATION; RESTORATION;
   SPARSITY
AB The critical problem of image denoising is removing noise while remaining the complex structures of the restored image as much as possible. Therefore, the reconstruction of image structures influences the quality of denoised images. In this paper, we first develop a structure extraction model that detects image structure efficiently. Then the model is applied to stack the similar patch group matrix. Different from other patch grouping methods, this model focuses on the image structure similarity among patches. After this set, we introduce a novel denoising model that incorporates low-rank and kernel Wiener filter priors based on the structure extraction model. The new model takes full advantage of the corresponding patches and remains the fine details as much as possible. Furthermore, the proposed method can reduce the artifacts which are inevitable in most denoising methods. Finally, the optimization problem is solved by alternating direction method multipliers. Experimental results demonstrate the out-performance of the proposed method concerning numerical and visual measurements.
C1 [Shi, Miaowen; Li, Xuemei; Zhang, Caiming] Shandong Univ, Sch Software, Jinan, Peoples R China.
   [Fan, Linwei] Shandong Univ Finance & Econ, Jinan, Peoples R China.
C3 Shandong University; Shandong University of Finance & Economics
RP Zhang, CM (corresponding author), Shandong Univ, Sch Software, Jinan, Peoples R China.
EM czhang@sdu.edu.cn
RI Fan, Linwei/ABG-8736-2021
OI Fan, Linwei/0000-0001-9986-2396; Shi, Miaowen/0000-0002-1989-7108
FU National Natural Science Foundation of China [62072281, 62007017,
   62002200]; Science and Technology Innovation Program for Distributed
   Young Talents of Shandong Province Higher Education Institutions, China
   [2019KJN042]; Natural Science Foundation of Shandong Province, China
   [ZR2020QF012]
FX This work was supported partly by the National Natural Science
   Foundation of China under Grant Nos. 62072281, 62007017, 62002200, the
   Science and Technology Innovation Program for Distributed Young Talents
   of Shandong Province Higher Education Institutions, China underGrant No.
   2019KJN042, theNatural Science Foundation of Shandong Province, China
   under Grant No. ZR2020QF012.
CR [Anonymous], 2002, THESIS STANFORD U
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Candès EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Donoho DL, 2013, P NATL ACAD SCI USA, V110, P8405, DOI 10.1073/pnas.1306110110
   Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139
   Fan LW, 2022, J SCI COMPUT, V90, DOI 10.1007/s10915-021-01728-0
   Fan LW, 2019, VIS COMPUT IND BIOME, V2, DOI 10.1186/s42492-019-0016-7
   Fan LW, 2019, SIGNAL PROCESS, V164, P110, DOI 10.1016/j.sigpro.2019.06.004
   Fan LW, 2019, IET IMAGE PROCESS, V13, P680, DOI 10.1049/iet-ipr.2018.6357
   Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Iordache MD, 2011, IEEE T GEOSCI REMOTE, V49, P2014, DOI 10.1109/TGRS.2010.2098413
   Jain P, 2015, VISUAL COMPUT, V31, P657, DOI 10.1007/s00371-014-0993-7
   Jia XX, 2016, SIGNAL PROCESS, V129, P1, DOI 10.1016/j.sigpro.2016.05.026
   Ke QF, 2005, PROC CVPR IEEE, P739
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu J, 2019, J SCI COMPUT, V78, P607, DOI 10.1007/s10915-018-0785-8
   Liu RS, 2012, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2012.6247726
   Liu Z, 2018, J VIS COMMUN IMAGE R, V52, P159, DOI 10.1016/j.jvcir.2018.02.011
   Lu CY, 2014, PROC CVPR IEEE, P4130, DOI 10.1109/CVPR.2014.526
   Luo Q, 2021, VISUAL COMPUT, V37, P1899, DOI 10.1007/s00371-020-01951-0
   Morikawa C, 2021, VISUAL COMPUT, V37, P2931, DOI 10.1007/s00371-021-02200-8
   Rehman A, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-16
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shenlong Wang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P231, DOI 10.1007/978-3-642-37431-9_18
   Tartavel G, 2016, SIAM J IMAGING SCI, V9, P1726, DOI 10.1137/16M1067494
   Wang HY, 2018, IEEE T IMAGE PROCESS, V27, DOI 10.1109/TIP.2017.2781425
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Yoshino H, 2010, IEEE T NEURAL NETWOR, V21, P1719, DOI 10.1109/TNN.2010.2059042
   Zha ZY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1353, DOI 10.1109/ICASSP.2018.8461388
   Zhai L, 2018, BRAIN RES BULL, V142, P270, DOI 10.1016/j.brainresbull.2018.08.006
   Zhang DB, 2012, PROC CVPR IEEE, P2192, DOI 10.1109/CVPR.2012.6247927
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang ML, 2018, COMPUT VIS IMAGE UND, V171, P48, DOI 10.1016/j.cviu.2018.05.006
   Zhang YQ, 2020, NEURAL COMPUT APPL, V32, P12575, DOI 10.1007/s00521-020-04717-w
   Zhang YQ, 2018, INFORM SCIENCES, V462, P402, DOI 10.1016/j.ins.2018.06.028
   Zhou MY, 2012, IEEE T IMAGE PROCESS, V21, P130, DOI 10.1109/TIP.2011.2160072
   Zuo WM, 2014, IEEE T IMAGE PROCESS, V23, P2459, DOI 10.1109/TIP.2014.2316423
NR 47
TC 3
Z9 3
U1 3
U2 9
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2407
EP 2423
DI 10.1007/s00371-022-02491-5
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000815765300002
DA 2024-07-18
ER

PT J
AU Bath, U
   Shekhar, S
   Egbert, J
   Schmidt, J
   Semmo, A
   Döllner, J
   Trapp, M
AF Bath, Ulrike
   Shekhar, Sumit
   Egbert, Julian
   Schmidt, Julian
   Semmo, Amir
   Doellner, Juergen
   Trapp, Matthias
TI CERVI: collaborative editing of raster and vector images
SO VISUAL COMPUTER
LA English
DT Article
DE Human-centered computing; Collaborative interaction; Image processing;
   Web-based interaction
AB Various web-based image-editing tools and web-based collaborative tools exist in isolation. Research focusing to bridge the gap between these two domains is sparse. We respond to the above and develop prototype groupware for real-time collaborative editing of raster and vector images in a web browser. To better understand the requirements, we conduct a preliminary user study and establish communication and synchronization as key elements. The existing groupware for text documents or presentations handles the above through well-established techniques. However, those cannot be extended as it is for raster or vector graphics manipulation. To this end, we develop a document model that is maintained by a server and is delivered and synchronized to multiple clients. Our prototypical implementation is based on a scalable client-server architecture: using WebGL for interactive browser-based rendering and WebSocket connections to maintain synchronization. We evaluate our work qualitatively through a post-deployment user study for three different scenarios. For quantitative evaluation, we perform a thorough performance measure on both client and server side, thereby identifying design recommendations for future concurrent image-editing software(s).
C1 [Bath, Ulrike] Univ Potsdam, Hasso Plattner Inst, IT Syst Engn ITSE, Potsdam, Germany.
   [Shekhar, Sumit; Trapp, Matthias] Univ Potsdam, Hasso Plattner Inst, Potsdam, Germany.
   [Egbert, Julian; Schmidt, Julian] Univ Potsdam, Hasso Plattner Inst, IT Syst Engn, Potsdam, Germany.
   [Doellner, Juergen] Univ Potsdam, Hasso Plattner Inst, Anal Planning & Construct Complex Syst, Potsdam, Germany.
   [Semmo, Amir] Digital Masterpieces GmbH, Res & Dev, Potsdam, Germany.
C3 University of Potsdam; University of Potsdam; University of Potsdam;
   University of Potsdam
RP Shekhar, S (corresponding author), Univ Potsdam, Hasso Plattner Inst, IT Syst Engn ITSE, Potsdam, Germany.
EM ulrike.bath@student.hpi.uni-potsdam.de;
   sumit.shekhar@hpi.uni-potsdam.de;
   julian.egbert@student.hpi.uni-potsdam.de;
   julian.schmidt@student.hpi.uni-potsdm.de;
   amir.semmo@digitalmasterpieces.com; jurgen.dollner@hpi.uni-potsdam.de;
   matthias.trapp@hpi.uni-potsdam.de
RI Trapp, Matthias/J-4456-2014; Semmo, Amir/KPA-5814-2024
OI Trapp, Matthias/0000-0003-3861-5759; Semmo, Amir/0000-0002-1553-4940;
   Shekhar, Sumit/0000-0002-5683-2290
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   [Anonymous], GOOGLE DRAW
   [Anonymous], AGGIE IO
   [Anonymous], Photopea
   [Anonymous], DRAW CHAT
   [Anonymous], PIXLR
   [Anonymous], 2014, Google documents
   Bath U, 2021, 2021 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2021), P33, DOI 10.1109/CW52790.2021.00013
   Calabrese C, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925956
   Chengzheng Sun, 2002, ACM Transactions on Computer-Human Interaction, V9, P1, DOI 10.1145/505151.505152
   Dürschmid T, 2017, SA'17: SIGGRAPH ASIA 2017 MOBILE GRAPHICS & INTERACTIVE APPLICATIONS, DOI 10.1145/3132787.3139208
   Edwards W. E., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P139, DOI 10.1145/263407.263533
   ELLIS CA, 1989, SIGMOD REC, V18, P399, DOI 10.1145/66926.66963
   Gao LP, 2018, MULTIMED TOOLS APPL, V77, P5067, DOI 10.1007/s11042-017-5242-4
   Heer J, 2008, INFORM VISUAL, V7, P49, DOI 10.1057/palgrave.ivs.9500167
   Isenberg T., 2016, PROC NPAR SER EXPRES, P8996
   Jian Z, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P264, DOI 10.1109/MMMC.2005.6
   Juranek L, 2019, 2019 42ND INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P127, DOI [10.1109/TSP.2019.8768889, 10.1109/tsp.2019.8768889]
   LEWIS JR, 1995, INT J HUM-COMPUT INT, V7, P57, DOI 10.1080/10447319509526110
   Nováková K, 2013, ECAADE 2013: COMPUTATION AND PERFORMANCE, VOL 1, P213
   Rempt B., OPEN RASTER SPECIFIC
   Richter M, 2018, COMPUT SCI RES NOTES, V2802, P97, DOI 10.24132/CSRN.2018.2802.13
   Salvati G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818110
   Viggiato M., 2018, ARXIV180804836, P1
   Wegen O., 2019, COMPUTER SCI RES NOT, P127, DOI [10.24132/csrn.2019.2901.1.15, DOI 10.24132/CSRN.2019.2901.1.15]
   Wu CX, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0427-6
   이보람, 2017, [Archives of Design Research, 디자인학연구], V30, P17
NR 27
TC 1
Z9 1
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4057
EP 4070
DI 10.1007/s00371-022-02522-1
EA JUN 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000814960800002
OA hybrid
DA 2024-07-18
ER

PT J
AU Pu, GQ
   Wang, HJ
AF Pu, Gangqiang
   Wang, Huijuan
TI Review on research progress of machine lip reading
SO VISUAL COMPUTER
LA English
DT Review
DE Machine lip reading; Deep learning; Lip reading datasets; Feature
   extraction methods
ID RECOGNITION; EXTRACTION; FEATURES; MODELS
AB Machine lip reading recognizes text content through the speaker's lip motion information. Lip reading has significant research and application value. With the continuous breakthrough of deep learning technology, lip reading research is also developing rapidly, and researchers have published many related studies. This paper studies the development of lip reading in detail, especially the latest research results of lip reading. We focus on the lip reading datasets and their comparison, including some recently released datasets. At the same time, we introduce the feature extraction methods of lip reading and compare various methods in detail. Finally, the future development direction of lip reading is discussed and prospected.
C1 [Pu, Gangqiang; Wang, Huijuan] North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Hebei, Peoples R China.
C3 North China Institute of Aerospace Engineering
RP Wang, HJ (corresponding author), North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Hebei, Peoples R China.
EM 2114667598@qq.com; wanghj@nciae.edu.cn
RI Han, Liang/KFR-6745-2024; LI, Xiang-Yang/JZE-0275-2024; Wang,
   Xingyi/KHT-7171-2024
OI Wang, Huijuan/0000-0003-1598-819X
FU Scientific Research Key Project of Hebei Provincial Department of
   Education [ZD2020161]; Natural Science Foundation of Hebei Province
   [F2021409007]
FX This studywas funded by the Scientific Research Key Project of Hebei
   Provincial Department of Education (Grant No.ZD2020161) and the Natural
   Science Foundation of Hebei Province (Grant No.F2021409007).
CR Abdrakhmanova M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21103465
   Afouras T., 2019, ARXIV PREPRINT ARXIV
   Afouras T., 2018, arXiv preprint arXiv 1809. 02108
   Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Aleksic PS, 2006, P IEEE, V94, P2025, DOI 10.1109/JPROC.2006.886017
   Alizadeh S, 2008, INT CONF SIGN PROCES, P561, DOI 10.1109/ICOSP.2008.4697195
   Anina I, 2015, IEEE INT CONF AUTOMA
   [Anonymous], 2004, ICMI'04-Sixth International Conference on Multimodal Interfaces, DOI [DOI 10.1145/1027933.1027972, 10.1145/1027933.1027972]
   [Anonymous], 2000, AUDIO-VISUAL SPEECH RECOGNITION
   [Anonymous], 2014, ARXIV14123121
   Assael Y.M., 2016, Lipnet: Sentence-level lipreading
   Assael Yannis M, 2016, ARXIV161101599
   Bailly-Bailliére E, 2003, LECT NOTES COMPUT SC, V2688, P625
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Chen JY, 2008, LECT NOTES COMPUT SC, V5359, P236, DOI 10.1007/978-3-540-89646-3_23
   Chen XJ, 2020, SIGNAL IMAGE VIDEO P, V14, P981, DOI 10.1007/s11760-019-01630-1
   Chenhao W, 2019, INTHE 30 BRIT MACHIN, V2019
   Chuanzhen R., 2012, DATA ACQUISITION PRO, VS2, P7
   Chung JS, 2018, COMPUT VIS IMAGE UND, V173, P76, DOI 10.1016/j.cviu.2018.02.001
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cox S., 2008, The challenge of multispeaker lip-reading
   Ding RW, 2018, IEEE IMAGE PROC, P4138, DOI 10.1109/ICIP.2018.8451096
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Egorov E., 2021, ARXIV PREPRINT ARXIV
   Elrefaei Lamiaa A., 2019, Procedia Computer Science, V163, P400, DOI 10.1016/j.procs.2019.12.122
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Feng D., 2021, 2021 IEEE INT C MULT, P1
   Fernandez-Lopez A, 2018, IMAGE VISION COMPUT, V78, P53, DOI 10.1016/j.imavis.2018.07.002
   Fox NA., 2005, VALID NEW PRACTICAL
   Fung I, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2511, DOI 10.1109/ICASSP.2018.8462280
   Garg A., 2016, Technical report
   Hao MF, 2020, IEEE ACCESS, V8, P204518, DOI 10.1109/ACCESS.2020.3036865
   Huang J, 2004, SPEECH COMMUN, V44, P83, DOI 10.1016/j.specom.2004.10.007
   Huang X, 2021, VISUAL COMPUT, V37, P95, DOI 10.1007/s00371-020-01982-7
   Jha A, 2018, IEEE WINT CONF APPL, P150, DOI 10.1109/WACV.2018.00023
   Kass M., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P259, DOI 10.1007/BF00133570
   Khassanov Y., 2021, ARXIV PREPRINT ARXIV
   Kumar Y, 2019, AAAI CONF ARTIF INTE, P2588
   Lan Y., 2012, IEEE INT C AC
   Lee B., 2011, C SPOKEN LANGUAGE
   Lee D, 2017, LECT NOTES COMPUT SC, V10117, P290, DOI 10.1007/978-3-319-54427-4_22
   Li M, 2008, 2008 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, VOLS 1 AND 2, PROCEEDINGS, P361, DOI 10.1109/CIS.2008.214
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu J., 2020, FASTLR NONAUTOREGRES
   Liu M., ARXIV PREPRINT ARXIV
   Lu L, 2019, IEEE ACM T NETWORK, V27, P447, DOI 10.1109/TNET.2019.2891733
   Lubitz A., 2021, ARXIV PREPRINT ARXIV
   Lucey P.J., 2008, PATCH BASED ANAL VIS
   Luo MS, 2020, IEEE INT CONF AUTOMA, P273, DOI 10.1109/FG47880.2020.00010
   Ma P., END TO END AUDIO VIS
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7608, DOI 10.1109/ICASSP39728.2021.9415063
   Ma XJ, 2016, CHIN CONTR CONF, P6928, DOI 10.1109/ChiCC.2016.7554449
   Makino T, 2019, 2019 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU 2019), P905, DOI [10.1109/asru46091.2019.9004036, 10.1109/ASRU46091.2019.9004036]
   Mathulaprangsan S, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT), P22, DOI 10.1109/ICOT.2015.7498485
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   McCool C, 2012, IEEE INT CONF MULTI, P635, DOI 10.1109/ICMEW.2012.116
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mesbah A, 2019, IMAGE VISION COMPUT, V88, P76, DOI 10.1016/j.imavis.2019.04.010
   Messer K., 1999, 2 INT C AUD VID BAS
   Mirzaei MR, 2014, VISUAL COMPUT, V30, P245, DOI 10.1007/s00371-013-0841-1
   Movellan J. R., 1995, Advances in Neural Information Processing Systems 7, P851
   Ninomiya H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P563
   Noda K., LIPREADING USING CON
   Noda K, 2015, APPL INTELL, V42, P722, DOI 10.1007/s10489-014-0629-7
   Oghbaie M., 2021, ARXIV PREPRINT ARXIV
   Oliveira D., INT C AUTOMATIC FACE
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   Petajan E. D., 1984, IEEE Global Telecommunications Conference, GLOBECOM '84 Conference Record. `Communications in the Information Age' (Cat. No. 84CH2064-4), P265
   Petridis S, 2017, 14 INT C AUD VIS SPE, P36
   Petridis S., 2017, BRIT MACH VIS C
   Petridis S, 2018, IEEE W SP LANG TECH, P513, DOI 10.1109/SLT.2018.8639643
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6219, DOI 10.1109/ICASSP.2018.8461596
   Petridis S, 2017, INT CONF ACOUST SPEE, P2592, DOI 10.1109/ICASSP.2017.7952625
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Prajwal K.R., 2020 IEEE INT C ACOU, V2020
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rahmani MH, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND IMAGE ANALYSIS (IPRIA), P195, DOI 10.1109/PRIA.2017.7983045
   Rekik A, 2014, LECT NOTES COMPUT SC, V8815, P21, DOI 10.1007/978-3-319-11755-3-3
   Saitoh T, 2017, LECT NOTES COMPUT SC, V10117, P277, DOI 10.1007/978-3-319-54427-4_21
   Sanderson C., 2004, VIDTIMIT DATABASE
   Shillingford B, 2019, INTERSPEECH, P4135, DOI 10.21437/Interspeech.2019-1669
   Stafylakis T, 2018, LECT NOTES COMPUT SC, V11208, P536, DOI 10.1007/978-3-030-01225-0_32
   Stafylakis T, 2018, COMPUT VIS IMAGE UND, V176, P22, DOI 10.1016/j.cviu.2018.10.003
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Stafylakis T, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4974, DOI 10.1109/ICASSP.2018.8461347
   Sterpu G., 2020, ARXIV PREPRINT ARXIV
   Sterpu G, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P111, DOI 10.1145/3242969.3243014
   Stillittano S, 2013, MACH VISION APPL, V24, P1, DOI 10.1007/s00138-012-0445-1
   Torfi A, 2017, IEEE ACCESS, V5, P22081, DOI 10.1109/ACCESS.2017.2761539
   Vanegas O., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P343, DOI 10.1109/ICIP.1999.822914
   Wand M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3041, DOI 10.1109/ICASSP.2018.8461900
   Wand M, 2017, INTERSPEECH, P3662, DOI 10.21437/Interspeech.2017-421
   Wand M, 2016, INT CONF ACOUST SPEE, P6115, DOI 10.1109/ICASSP.2016.7472852
   Watanabe T., 2017, LIP READING MULTIVIE
   Weng X., 2019, 30 BRIT MACH VIS C, V2019
   Wiriyathammabhum P., 2020, SPOTFAST NETWORKS ME
   Xiao JY, 2020, IEEE INT CONF AUTOMA, P364, DOI 10.1109/FG47880.2020.00132
   Xu B., 2020, Discriminative multi-modality speech recognition
   Xu K, 2018, IEEE INT CONF AUTOMA, P548, DOI 10.1109/FG.2018.00088
   Yang S., 2019 14 IEEE INT C A
   Yanjun X., 2000, ACTA ACOUST SINICA, V25, P8
   Yao Y., 2019 INT C MULTIMODA, V2019
   Yuxuan Lan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P432, DOI 10.1109/ICME.2012.192
   Zhang XB, 2019, AAAI CONF ARTIF INTE, P9211
   Zhang Y., 2021, ACM MULTIMEDIA 2021
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao X, 2020, IEEE INT CONF AUTOMA, P420, DOI 10.1109/FG47880.2020.00133
   Zhao Y., MMASIA 19, V2019
   Zhao Y, 2020, AAAI CONF ARTIF INTE, V34, P6917
   Zhou P, 2019, INT CONF ACOUST SPEE, P6565, DOI 10.1109/ICASSP.2019.8683733
NR 116
TC 5
Z9 5
U1 15
U2 50
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 3041
EP 3057
DI 10.1007/s00371-022-02511-4
EA JUN 2022
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000811457600001
DA 2024-07-18
ER

PT J
AU Kim, J
   Eom, H
   Kim, J
   Kim, Y
   Noh, J
AF Kim, Jaedong
   Eom, Haegwang
   Kim, Jihwan
   Kim, Younghui
   Noh, Junyong
TI Real-time tunnel projection from a moving subway train
SO VISUAL COMPUTER
LA English
DT Article
DE Tunnel display system; Dynamic projection mapping; Tunnel projection;
   Train tracking
AB In this study, we present the first actual working system that can project content onto a tunnel wall from a moving subway train so that passengers can enjoy the display of digital content through a train window. Our stand-alone system can be easily deployed to existing trains because it does not assume any specific type of interface requiring data transfer with the train. To effectively estimate the position of the train in a tunnel, we propose counting sleepers, which are installed at regular interval along the railway, using a distance sensor. In the preprocessing step, the side depth variation along the tunnel is stored in synchronization with the train location in a tunnel profile. The tunnel profile is constructed using pointclouds captured by a depth camera installed next to the projector. The tunnel profile is used to identify projectable sections that will not contain too much interference by possible occluders. The tunnel profile is also used to retrieve the depth at a specific location so that a properly warped content can be projected for viewing by passengers through the window when the train is moving at runtime. Here, we show that the proposed system can operate on an actual train and evaluate the quality of the projection results through simulation.
C1 [Kim, Jaedong; Noh, Junyong] Korea Adv Inst Sci & Technol, Visual Media Lab, Daejeon, South Korea.
   [Eom, Haegwang] Weta Digital, Wellington, New Zealand.
   [Kim, Jihwan] Kakao Brain Corp, Daejeon, South Korea.
   [Kim, Younghui] Kai Inc, Daejeon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Kakao
RP Noh, J (corresponding author), Korea Adv Inst Sci & Technol, Visual Media Lab, Daejeon, South Korea.
EM jaedong27@gmail.com; gongguri858@gmail.com; jj.mize@kakaobrain.com;
   yh.kim@kaistudio.co.kr; junyongnoh@kaist.ac.kr
RI Noh, Junyong/C-1663-2011
OI Noh, Junyong/0000-0003-1925-3326
FU Ministry of Culture, Sports and Tourism and Korea Creative Content
   Agency [R2020040173]
FX This research is supported by Ministry of Culture, Sports and Tourism
   and Korea Creative Content Agency (Project Number: R2020040173).
CR Akaoka E., 2010, P 4 INT C TANG EMB E, P4956
   Akcil L., 2013, CONTROL AUTOMATION T, V1, P129
   [Anonymous], 2015, ADJUNCT P 28 ANN ACM
   Asayama H, 2018, IEEE T VIS COMPUT GR, V24, P1077, DOI 10.1109/TVCG.2017.2657634
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Bandyopadhyay D., 2001, Proceedings of IEEE/ACM international symposium on augmented reality (ISAR 01), P207216
   Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17
   Ehnes J, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P26, DOI 10.1109/ISMAR.2004.47
   Fernández-Rodicio E, 2017, J SENSORS, V2017, DOI 10.1155/2017/4853915
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fujimoto Y, 2014, IEEE T VIS COMPUT GR, V20, P540, DOI 10.1109/TVCG.2014.25
   Garcia-Dorado Ignacio., 2011, 2011 IEEE COMPUTER S, P29, DOI [10. 1109/CVPRW. 2011. 5981726, DOI 10.1109/CVPRW.2011.5981726]
   Grundhöfer A, 2018, COMPUT GRAPH FORUM, V37, P653, DOI 10.1111/cgf.13387
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hashimoto N, 2017, INT J COMPUT GAMES T, V2017, DOI 10.1155/2017/4936285
   International Association of Public Transport, WORLD METR FIG 2018
   Jones Brett, 2014, P 27 ANN ACM S US IN, P637, DOI [10.1145/2642918.2647383, DOI 10.1145/2642918.2647383]
   Jong-Hwi H., 2015, US Patent App., Patent No. [14/430,541, 14430541]
   Kagami S, 2019, IEEE T VIS COMPUT GR, V25, P3094, DOI 10.1109/TVCG.2019.2932248
   Katz B., 1996, US Patent, Patent No. [5,580,140, 5580140]
   Ke Cui, 2018, CICTP 2017. Transportation Reform and Change-Equity, Inclusiveness, Sharing, and Innovation. Proceedings of the 17th Cota International Conference of Transportation Professionals, P2053
   Kim J., 2021, PACIFIC GRAPHICS 202, P14
   Kitajima Y, 2017, IEEE T VIS COMPUT GR, V23, P2419, DOI 10.1109/TVCG.2017.2734478
   Koike Hideki, 2015, P 6 AUGMENTED HUMAN, P93
   Marais J, 2017, IEEE T INTELL TRANSP, V18, P2602, DOI 10.1109/TITS.2017.2658179
   Miyafuji S, 2016, PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES, (ISS 2016), P33, DOI 10.1145/2992154.2992181
   Molyneaux D., 2013, ACM T INTERACT INTEL, V3, P1, DOI 10.1145/2499474.2499476
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Narita G, 2017, IEEE T VIS COMPUT GR, V23, P1235, DOI 10.1109/TVCG.2016.2592910
   Punpongsanon P, 2015, VIRTUAL REAL-LONDON, V19, P45, DOI 10.1007/s10055-014-0256-y
   Raskar R, 2003, ACM T GRAPHIC, V22, P809, DOI 10.1145/882262.882349
   Resch C, 2014, INT SYM MIX AUGMENT, P151, DOI 10.1109/ISMAR.2014.6948421
   Sajadi B, 2012, IEEE T VIS COMPUT GR, V18, P381, DOI 10.1109/TVCG.2011.271
   Shilov, 2021, CHINA MAY REPLACE SU
   Siegl C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818111
   Spaulding W.J., 1992, US Patent, Patent No. [5,108,171, 5108171]
   Tone D, 2020, IEEE T VIS COMPUT GR, V26, P2030, DOI 10.1109/TVCG.2020.2973444
   Vaylagya T., 2008, US Patent App., Patent No. [11/394,293, 11394293]
   Veillard D, 2016, IEEE SENSOR
   Wang J., 2016, 2016 IEEERSJ INT C I
   Wang Q, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030645
   Watanabe Y, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P52, DOI 10.1109/ISMAR.2017.22
   Wiesmeyr C, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020448
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhou Y., 2016, P 2016 CHI C HUMAN F
NR 45
TC 0
Z9 0
U1 0
U2 5
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2023
VL 39
IS 7
BP 2711
EP 2724
DI 10.1007/s00371-022-02487-1
EA APR 2022
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L4HX1
UT WOS:000794955400001
DA 2024-07-18
ER

PT J
AU Mo, WY
   Pei, JH
AF Mo, Wenying
   Pei, Jihong
TI <sub>Sea-sky line detection in the infrared image based on the vertical
   grayscale distribution feature</sub>
SO VISUAL COMPUTER
LA English
DT Article
DE Infrared image; Sea-sky line detection; Vertical grayscale distribution
   feature; Probability feature map; CNN
ID HOUGH TRANSFORM
AB (Abstract) When detecting sea-sky line (SSL) in the infrared image, the blurry SSL, conspicuous sea clutter affects the accurate detection of SSL seriously. To solve these problems, we proposed a robust SSL detection algorithm based on the vertical grayscale distribution feature (VGDF). We divided the infrared image into sub-image blocks by sliding window. The sub-image blocks that contain SSL in the central area are labeled as positive samples, and those without any SSL are labeled as negative samples. To improve the separability of the samples, the vertical grayscale distribution feature map (VGDF map) transformation method is proposed to transform the gray sub-image blocks into the feature maps. The VGDF maps are used as the input of the convolutional neural network to train the SSL recognition model. This strategy can improve the separability of SSL image blocks from background image blocks. Then, we use the trained model to obtain the edge candidates and construct the SSL probability feature map. Finally, we detect the SSL by fitting a straight line with the greatest probability on the SSL probability feature map. The proposed algorithm realized 99.4% accuracy rate on the dataset containing 1320 frames of infrared images. The comparison results showed that our algorithm obtained higher detection accuracy than the existing state-of-the-art algorithms. Our algorithm performs well even when the SSL was blurred or there are obvious ship's wave wakes on the sea surface.
C1 [Mo, Wenying; Pei, Jihong] Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Mo, WY (corresponding author), Shenzhen Univ, Sch Elect & Informat Engn, Shenzhen 518060, Peoples R China.
EM weny_mo@163.com; jhpei@szu.edu.cn
RI mo, wy/JUF-6109-2023
FU National Natural Science Foundation of China [62071303, 61871269];
   Guangdong Basic and Applied Basic Research Foundation [2019A1515011861];
   Shenzhen Science and Technology Projection [JCYJ20190808151615540]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant (62071303, 61871269), Guangdong Basic
   and Applied Basic Research Foundation (2019A1515011861), and Shenzhen
   Science and Technology Projection (JCYJ20190808151615540).
CR Beeravolu AR, 2021, IEEE ACCESS, V9, P33438, DOI 10.1109/ACCESS.2021.3058773
   Ben Fredj H, 2021, VISUAL COMPUT, V37, P217, DOI 10.1007/s00371-020-01794-9
   Bnouni N, 2021, INT MULTICONF SYST, P264, DOI 10.1109/SSD52085.2021.9429422
   Dai Y., 2018, OPTO ELECT ENG, V45, P1
   Feng T., 2020, LASER OPTOELECTRON P, V57, P1
   Fu J, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3111099
   Jeong C, 2019, MULTIDIM SYST SIGN P, V30, P1187, DOI 10.1007/s11045-018-0602-4
   Li FX, 2021, SIGNAL IMAGE VIDEO P, V15, P139, DOI 10.1007/s11760-020-01733-0
   Liang D, 2020, IEEE T INSTRUM MEAS, V69, P45, DOI 10.1109/TIM.2019.2893008
   Lin C, 2020, J MAR SCI ENG, V8, DOI 10.3390/jmse8100799
   Liu RW, 2021, OCEAN ENG, V235, DOI 10.1016/j.oceaneng.2021.109435
   Liu XY, 2017, PROCEEDINGS OF 2017 13TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONIC MEASUREMENT & INSTRUMENTS (ICEMI), VOL 1, P486, DOI 10.1109/ICEMI.2017.8265991
   Liu ZY, 2016, IMAGE VISION COMPUT, V48-49, P14, DOI 10.1016/j.imavis.2015.12.005
   Ma DD, 2021, IEEE ACCESS, V9, P1439, DOI 10.1109/ACCESS.2020.3047736
   Ma T, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P700, DOI 10.1109/CompComm.2016.7924792
   Palmer P., 1993, IEE Colloquium on `Hough Transforms' (Digest No.106), p3/1
   Prasad DK, 2017, IEEE T INTELL TRANSP, V18, P1993, DOI 10.1109/TITS.2016.2634580
   [仇荣超 Qiu Rongchao], 2019, [兵工学报, Acta Armamentarii], V40, P1171
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   [邵旭慧 Shao Xuhui], 2019, [信号处理, Journal of Signal Processing], V35, P877
   Song HF, 2021, J RUSS LASER RES, V42, P318, DOI 10.1007/s10946-021-09965-2
   Suhr JK, 2015, IEEE T INTELL TRANSP, V16, P1528, DOI 10.1109/TITS.2014.2369002
   Sun X., 2017, ACTA OPT SINICA, V37, P94
   Tschopp F, 2021, IEEE ROBOT AUTOM LET, V6, P2745, DOI 10.1109/LRA.2021.3061404
   Wang B, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040543
   Wenying Mo, 2019, 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), P1397, DOI 10.1109/IAEAC47372.2019.8997603
   Xu ZZ, 2015, IEEE T IMAGE PROCESS, V24, P813, DOI 10.1109/TIP.2014.2387020
   Yang WH, 2019, INT J ADV ROBOT SYST, V16, DOI 10.1177/1729881419892116
   Zafarifar B., 2008, P SPIE INT SOC OPTIC, V6822, P6822
   Zardoua Y, 2023, VISUAL COMPUT, V39, P197, DOI 10.1007/s00371-021-02321-0
   Zhan Wei, 2019, Electronics Optics & Control, V26, P43, DOI 10.3969/j.issn.1671-637X.2019.01.010
   Zhang L, 2023, VISUAL COMPUT, V39, P73, DOI 10.1007/s00371-021-02313-0
   Zhang W, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.002
   Zhang X., 2018, APPL SCI TECHNOL, V45, P6
   Zhang Y, 2017, OCEAN ENG, V141, P53, DOI 10.1016/j.oceaneng.2017.06.022
   Zhang Z., 2020, J HUAZHONG U SCI TEC, V48, P1
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zhou AR, 2018, INFRARED PHYS TECHN, V91, P123, DOI 10.1016/j.infrared.2018.04.006
NR 39
TC 7
Z9 7
U1 2
U2 35
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2023
VL 39
IS 5
BP 1915
EP 1927
DI 10.1007/s00371-022-02455-9
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7ZK3
UT WOS:000781250600002
DA 2024-07-18
ER

PT J
AU Song, YN
   Shen, WM
   Peng, KK
AF Song, Yanan
   Shen, Weiming
   Peng, Kunkun
TI A novel partial point cloud registration method based on graph attention
   network
SO VISUAL COMPUTER
LA English
DT Article
DE Point cloud registration; Partial correspondence; Graph attention
   network; Deep learning
AB Point cloud registration is a challenging task due to sparsity and unknown initial correspondence information. The traditional registration methods tend to converge to local optimal solutions and rely on good initial correspondence information. Deep learning-based methods show good adaptability to initial information and noises, but they cannot effectively cope with partial-to-partial registration scenes. This paper proposes a partial point cloud registration method based on graph attention network. The context information of the point cloud is obtained by a message passing mechanism. The attention features of the key registration points are extracted by an attention network. The key matching points are chosen by a key point selection module. Virtual correspondences are generated based on these key points and their features. A rigid transformation is obtained based on the virtual registration by a singular value decomposition layer. The performance of the proposed method is evaluated in three scenarios based on the ModelNet40 dataset. Experimental results show that the proposed method is robust to arbitrary initial positions and noises. It obtains higher registration accuracy than traditional methods while maintaining low network complexity.
C1 [Song, Yanan] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Song, Yanan] Zhejiang Univ, Inst Comp Innovat, Hangzhou, Peoples R China.
   [Shen, Weiming] Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Wuhan, Peoples R China.
   [Peng, Kunkun] Wuhan Univ Sci & Technol, Sch Management, Wuhan, Peoples R China.
C3 Zhejiang University; Zhejiang University; Huazhong University of Science
   & Technology; Wuhan University of Science & Technology
RP Shen, WM (corresponding author), Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Wuhan, Peoples R China.
EM shenwm@hust.edu.cn
RI shen, Weiming/HJZ-2337-2023; Shen, Weiming/B-7400-2013
OI Shen, Weiming/0000-0001-5204-7992
FU China Postdoctoral Science Foundation [2021M692778]
FX This work was supported by China Postdoctoral Science Foundation [Grant
   Number 2021M692778].
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2015, Advances in neural information processing systems
   Aoki Y, 2019, PROC CVPR IEEE, P7156, DOI 10.1109/CVPR.2019.00733
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Choy C, 2020, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR42600.2020.00259
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dong K, 2022, VISUAL COMPUT, V38, P51, DOI 10.1007/s00371-020-01999-y
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Gojcic Z, 2020, PROC CVPR IEEE, P1756, DOI 10.1109/CVPR42600.2020.00183
   Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Huang SY, 2021, PROC CVPR IEEE, P4265, DOI 10.1109/CVPR46437.2021.00425
   Lee D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5663, DOI 10.1109/ICCV48922.2021.00563
   Li J., 2020, P 16 EUR C COMP VIS, Vvol 12369, P378, DOI 10.1007/978-3-030-58586-023
   Liu TR, 2022, VISUAL COMPUT, V38, P2303, DOI 10.1007/s00371-021-02112-7
   Liu YC, 2019, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2019.00534
   Luo S., 2021, ARXIV PREPRINT ARXIV
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Meng HY, 2019, IEEE I CONF COMP VIS, P8499, DOI 10.1109/ICCV.2019.00859
   Oztireli AC, 2008, VISUAL COMPUT, V24, P679, DOI 10.1007/s00371-008-0248-6
   Pais GD, 2020, PROC CVPR IEEE, P7191, DOI 10.1109/CVPR42600.2020.00722
   Prieto SA, 2020, VISUAL COMPUT, V36, P113, DOI 10.1007/s00371-018-1584-9
   Qi CR, 2017, ADV NEUR IN, V30
   Rosen DM, 2019, INT J ROBOT RES, V38, P95, DOI 10.1177/0278364918784361
   Rusinkiewicz S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323037
   Sarode Vinit, 2019, ARXIV190807906
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2022, VISUAL COMPUT, V38, P4279, DOI 10.1007/s00371-021-02295-z
   Wang Y, 2019, IEEE I CONF COMP VIS, P3522, DOI 10.1109/ICCV.2019.00362
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang YL, 2019, ADV NEUR IN, V32
   Wei HS, 2020, IEEE INT C INT ROBOT, P2678, DOI 10.1109/IROS45743.2020.9341249
   Wu J., 2021, P IEEECVF INT C COMP, P5530
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zi Jian Yew, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11821, DOI 10.1109/CVPR42600.2020.01184
NR 38
TC 16
Z9 16
U1 5
U2 51
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2023
VL 39
IS 3
BP 1109
EP 1120
DI 10.1007/s00371-021-02391-0
EA FEB 2022
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9E6QG
UT WOS:000752807000003
DA 2024-07-18
ER

PT J
AU Jia, SY
   Zhang, WZ
   Wang, GD
   Pan, ZK
   Yu, XK
AF Jia, Shiyu
   Zhang, Weizhong
   Wang, Guodong
   Pan, Zhenkuan
   Yu, Xiaokang
TI A real-time deformable cutting method using two levels of linked voxels
   for improved decoupling between collision and rendering
SO VISUAL COMPUTER
LA English
DT Article
DE Deformable object; Physically based modeling; Interactive cutting;
   Voxels
ID SIMULATION; OBJECTS; CUTS
AB The linked voxel model is commonly used to simulate real-time interactive cutting of deformable objects. Previous methods use a single level of voxels and result in tight coupling between collision and rendering. In this paper, a novel method using two levels of voxels is proposed. The object surface mesh is divided into an interface mesh constructed on a fine level voxel grid and a cut surface mesh constructed on a coarse level voxel grid. Collision uses a collision proxy constructed on the coarse level voxel grid and is therefore decoupled from the rendering quality of the interface mesh. The only drawback of our method is that collision is still coupled to the rendering quality of the cut surface mesh. Simulation tests show that simulations using our method have higher frame rates than those using a single level of fine voxels, while achieving comparable rendering qualities for the interface meshes. Although the rendering qualities of the cut surface meshes are only comparable to those using a single level of coarse voxels, the rendering qualities of the boundary lines between different materials on the cut surface and the outer edges of the cut surface are better.
C1 [Jia, Shiyu; Zhang, Weizhong; Wang, Guodong; Pan, Zhenkuan; Yu, Xiaokang] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
C3 Qingdao University
RP Zhang, WZ (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
EM zhangwz_01@aliyun.com
RI Zhang, Weizhong/S-9082-2019; jia, shi/HHS-4172-2022
OI Zhang, Weizhong/0000-0002-1831-0862; 
FU Natural Science Foundation of Shandong Province of China [ZR2020MF043]
FX The work described in this paper was funded by the Natural Science
   Foundation of Shandong Province of China (Grant Number ZR2020MF043).
CR Aras R, 2016, ADV ENG SOFTW, V102, P40, DOI 10.1016/j.advengsoft.2016.08.011
   Berndt I, 2017, IEEE COMPUT GRAPH, V37, P24, DOI 10.1109/MCG.2017.45
   Courtecuisse H, 2014, MED IMAGE ANAL, V18, P394, DOI 10.1016/j.media.2013.11.001
   Dick C, 2011, IEEE T VIS COMPUT GR, V17, P1663, DOI 10.1109/TVCG.2010.268
   Jerábková L, 2010, PROG BIOPHYS MOL BIO, V103, P217, DOI 10.1016/j.pbiomolbio.2010.09.012
   Jerabkova L, 2009, IEEE COMPUT GRAPH, V29, P61, DOI 10.1109/MCG.2009.32
   Jia SY, 2017, J COMPUT SCI TECH-CH, V32, P1198, DOI 10.1007/s11390-017-1794-z
   Jia SY, 2020, VISUAL COMPUT, V36, P1017, DOI 10.1007/s00371-019-01716-4
   Jia SY, 2018, COMPUT GRAPH FORUM, V37, P45, DOI 10.1111/cgf.13162
   Jia SY, 2015, INT J COMPUT ASS RAD, V10, P1477, DOI 10.1007/s11548-014-1147-0
   Ju T, 2002, ACM T GRAPHIC, V21, P339
   Kaufmann P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531356
   Koschier D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073666
   Magnoux V, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1929
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   Pan JJ, 2019, VISUAL COMPUT, V35, P861, DOI 10.1007/s00371-019-01680-z
   Pan JJ, 2018, VISUAL COMPUT, V34, P105, DOI 10.1007/s00371-016-1317-x
   Paulus CJ, 2015, VISUAL COMPUT, V31, P831, DOI 10.1007/s00371-015-1123-x
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Qi D, 2021, VISUAL COMPUT, V37, P1113, DOI 10.1007/s00371-020-01856-y
   Seiler M, 2011, VISUAL COMPUT, V27, P519, DOI 10.1007/s00371-011-0561-3
   Sifakis E, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P73
   Steinemann D, 2009, GRAPH MODELS, V71, P209, DOI 10.1016/j.gmod.2008.12.004
   Turkiyyah GM, 2011, COMPUT AIDED DESIGN, V43, P809, DOI 10.1016/j.cad.2010.10.005
   Wang Y., 2014, Proc ACM SIGGRAPH/Eurographics Symp Comp Anim, SCA '14, P77
   Wu J., 2011, VRIPHYS, P29
   Wu J, 2015, COMPUT GRAPH FORUM, V34, P161, DOI 10.1111/cgf.12528
   Wu J, 2013, VISUAL COMPUT, V29, P739, DOI 10.1007/s00371-013-0810-8
NR 28
TC 3
Z9 3
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2023
VL 39
IS 2
BP 765
EP 783
DI 10.1007/s00371-021-02373-2
EA FEB 2022
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8T2TV
UT WOS:000752196300001
DA 2024-07-18
ER

PT J
AU Wang, XY
   Su, YN
   Liu, L
   Zhang, H
   Di, SH
AF Wang, Xingyuan
   Su, Yining
   Liu, Lin
   Zhang, Hao
   Di, Shuhong
TI Color image encryption algorithm based on Fisher-Yates scrambling and
   DNA subsequence operation
SO VISUAL COMPUTER
LA English
DT Article
DE Color image encryption; Fisher-Yates scrambling; DNA subsequence
   operation; Chaotic sequence
ID HYBRID GENETIC ALGORITHM; CHAOTIC SYSTEM; CML
AB In this paper, a color image encryption algorithm based on Fisher-Yates scrambling and DNA subsequence operation (elongation operation, truncation operation, deletion operation, insertion arithmetic) is proposed. Firstly, the three-dimensional color image is transformed into two-dimensional gray image, and the chaotic sequence generated by Chen system and Fisher-Yates scrambling method is used to scramble the plaintext images of R, G and B channels. Secondly, the three channel images of the scrambled plaintext image are transformed into three DNA sequence matrixes by using the DNA coding rules, and then the three DNA sequence matrixes are manipulated by using DNA subsequence operation and DNA addition, subtraction and XOR operation to destroy the scrambled plaintext information. Finally, the color encrypted image is obtained by using the DNA decoding rule. Experimental results and security analysis demonstrate that our encryption algorithm has good performance and may resist against various typical attacks.
C1 [Wang, Xingyuan; Su, Yining; Liu, Lin] Dalian Maritime Univ, Fac Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Zhang, Hao] Taiyuan Univ Technol, Coll Informat & Comp, Jinzhong 030600, Peoples R China.
   [Di, Shuhong] Tangshan Normal Univ, Sch Phys & Technol, Tangshan 063000, Peoples R China.
C3 Dalian Maritime University; Taiyuan University of Technology; Tangshan
   Normal University
RP Wang, XY (corresponding author), Dalian Maritime Univ, Fac Informat Sci & Technol, Dalian 116026, Peoples R China.
EM xywang@dlmu.edu.cn
RI Wang, Xing-yuan/I-6353-2015
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key R&D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 universities'
   Funding Projects Introducing Innovation Team Program [2019GXRC031]
FX This research is supported by the National Natural Science Foundation of
   China (No: 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No: MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No: XLYC1802013), Key R&D Projects of Liaoning Province (No:
   2019020105-JH2/103), Jinan City '20 universities' Funding Projects
   Introducing Innovation Team Program (No: 2019GXRC031).
CR Abdullah AH, 2012, AEU-INT J ELECTRON C, V66, P806, DOI 10.1016/j.aeue.2012.01.015
   Ali TS, 2020, MULTIMED TOOLS APPL, V79, P19853, DOI 10.1007/s11042-020-08850-5
   Enayatifar R, 2014, OPT LASER ENG, V56, P83, DOI 10.1016/j.optlaseng.2013.12.003
   Hasanzadeh E, 2020, MULTIMED TOOLS APPL, V79, P7279, DOI 10.1007/s11042-019-08342-1
   He XL, 2018, OPT LASER ENG, V107, P112, DOI 10.1016/j.optlaseng.2018.03.018
   Hosny KM, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10091066
   Hou YC, 2013, INFORM SCIENCES, V233, P290, DOI 10.1016/j.ins.2013.01.006
   Hu T, 2017, SIGNAL PROCESS, V134, P234, DOI 10.1016/j.sigpro.2016.12.008
   Kamal ST, 2021, IEEE ACCESS, V9, P37855, DOI 10.1109/ACCESS.2021.3063237
   Kocarev L., 2001, IEEE CIRC SYST MAG, V1, P6, DOI DOI 10.1109/7384.963463
   Kumar M, 2016, SIGNAL PROCESS, V125, P187, DOI 10.1016/j.sigpro.2016.01.017
   Li B, 2021, EUR PHYS J PLUS, V136, DOI 10.1140/epjp/s13360-020-01001-7
   Li XJ, 2021, OPT LASER TECHNOL, V140, DOI 10.1016/j.optlastec.2021.107074
   Liu HJ, 2015, OPT COMMUN, V338, P340, DOI 10.1016/j.optcom.2014.10.021
   Liu HJ, 2012, APPL SOFT COMPUT, V12, P1457, DOI 10.1016/j.asoc.2012.01.016
   Liu TM, 2021, CHAOS SOLITON FRACT, V145, DOI 10.1016/j.chaos.2021.110791
   Liu YS, 2014, OPT LASER TECHNOL, V60, P111, DOI 10.1016/j.optlastec.2014.01.015
   Luo YL, 2018, MULTIMED TOOLS APPL, V77, P26191, DOI 10.1007/s11042-018-5844-5
   Mansouri A, 2021, VISUAL COMPUT, V37, P189, DOI 10.1007/s00371-020-01791-y
   Parvaz R, 2018, OPT LASER TECHNOL, V101, P30, DOI 10.1016/j.optlastec.2017.10.024
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P1757, DOI 10.1007/s00371-020-01936-z
   Wang XY, 2016, BIOSYSTEMS, V144, P18, DOI 10.1016/j.biosystems.2016.03.011
   Wang XY, 2015, OPT LASER ENG, V73, P53, DOI 10.1016/j.optlaseng.2015.03.022
   Wang XY, 2010, NONLINEAR DYNAM, V62, P615, DOI 10.1007/s11071-010-9749-8
   Wang XY, 2021, INFORM SCIENCES, V574, P505, DOI 10.1016/j.ins.2021.06.032
   Wang XY, 2021, OPT LASER TECHNOL, V143, DOI 10.1016/j.optlastec.2021.107316
   Wang XY, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106393
   Wang XY, 2020, OPT LASER ENG, V125, DOI 10.1016/j.optlaseng.2019.105851
   Wu XJ, 2018, SIGNAL PROCESS, V148, P272, DOI 10.1016/j.sigpro.2018.02.028
   Wu XJ, 2017, NONLINEAR DYNAM, V90, P855, DOI 10.1007/s11071-017-3698-4
   Xian YJ, 2022, IEEE T CIRC SYST VID, V32, P4028, DOI 10.1109/TCSVT.2021.3108767
   Xu J, 2022, VISUAL COMPUT, V38, P1509, DOI 10.1007/s00371-021-02085-7
   Yang FF, 2020, OPT LASER ENG, V129, DOI 10.1016/j.optlaseng.2020.106031
   陈惟昌, 2001, 生物物理学报, V17, P542
NR 35
TC 23
Z9 23
U1 11
U2 55
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2023
VL 39
IS 1
BP 43
EP 58
DI 10.1007/s00371-021-02311-2
EA OCT 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F6UJ1
UT WOS:000713080200001
DA 2024-07-18
ER

PT J
AU Zhuang, HM
   Zhang, JX
   Liao, F
AF Zhuang, Huangming
   Zhang, Jixiang
   Liao, Fei
TI A systematic review on application of deep learning in digestive system
   image processing
SO VISUAL COMPUTER
LA English
DT Article; Proceedings Paper
CT ACIAT202
CY 2022
CL ELECTR NETWORK
DE Deep learning; Artificial intelligence; Digestive system; Endoscopic;
   Imaging; Pathology; Diagnosis
ID ARTIFICIAL-INTELLIGENCE; GASTRIC-CANCER; PERFORMANCE; DIAGNOSIS;
   DISEASE; CLASSIFICATION; ALGORITHM; NETWORKS
AB With the advent of the big data era, the application of artificial intelligence represented by deep learning in medicine has become a hot topic. In gastroenterology, deep learning has accomplished remarkable accomplishments in endoscopy, imageology, and pathology. Artificial intelligence has been applied to benign gastrointestinal tract lesions, early cancer, tumors, inflammatory bowel diseases, livers, pancreas, and other diseases. Computer-aided diagnosis significantly improve diagnostic accuracy and reduce physicians' workload and provide a shred of evidence for clinical diagnosis and treatment. In the near future, artificial intelligence will have high application value in the field of medicine. This paper mainly summarizes the latest research on artificial intelligence in diagnosing and treating digestive system diseases and discussing artificial intelligence's future in digestive system diseases. We sincerely hope that our work can become a stepping stone for gastroenterologists and computer experts in artificial intelligence research and facilitate the application and development of computer-aided image processing technology in gastroenterology.
C1 [Zhuang, Huangming; Zhang, Jixiang; Liao, Fei] Wuhan Univ, Renmin Hosp, Dept Gastroenterol, Wuhan 430060, Hubei, Peoples R China.
C3 Wuhan University
RP Liao, F (corresponding author), Wuhan Univ, Renmin Hosp, Dept Gastroenterol, Wuhan 430060, Hubei, Peoples R China.
EM feiliao@whu.edu.cn
OI Liao, Fei/0000-0002-6026-7976
CR Aslam MA, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83184-2
   Ben-Cohen A, 2019, ENG APPL ARTIF INTEL, V78, P186, DOI 10.1016/j.engappai.2018.11.013
   Ben-Haim S, 2009, J NUCL MED, V50, P88, DOI 10.2967/jnumed.108.054205
   Boellaard R, 2009, J NUCL MED, V50, p11S, DOI 10.2967/jnumed.108.057182
   Bora K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83788-8
   Budd S, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102062
   Byra M, 2022, J ULTRAS MED, V41, P175, DOI 10.1002/jum.15693
   Caroppo A, 2021, COMPUT MED IMAG GRAP, V88, DOI 10.1016/j.compmedimag.2020.101852
   Carreras-Torres R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-76361-2
   Chen W, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23060667
   Chen YH, 2020, MED PHYS, V47, P4971, DOI 10.1002/mp.14429
   Chen YJ, 2019, IEEE I CONF COMP VIS, P6960, DOI 10.1109/ICCV.2019.00706
   Colli A, 2017, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008759.pub2
   Corral JE, 2019, PANCREAS, V48, P805, DOI 10.1097/MPA.0000000000001327
   Curreri F, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030823
   de Souza LA Jr, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104029
   Dehkordi Azar B., 2019, Infectious Disorders - Drug Targets, V19, P101, DOI 10.2174/1871526518666180629134511
   Després JP, 2006, NATURE, V444, P881, DOI 10.1038/nature05488
   Eraslan G, 2019, NAT REV GENET, V20, P389, DOI 10.1038/s41576-019-0122-6
   Feng X, 2015, J HELMINTHOL, V89, P671, DOI 10.1017/S0022149X15000656
   Golemanov B, 2011, AM J TROP MED HYG, V84, P48, DOI 10.4269/ajtmh.2011.10-0312
   Govind D, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67880-z
   Becker BG, 2021, THER ADV GASTROINTES, V14, DOI 10.1177/2631774521990623
   Häggström I, 2019, MED IMAGE ANAL, V54, P253, DOI 10.1016/j.media.2019.03.013
   Hakim A, 2019, J HEPATOL, V70, P1214, DOI 10.1016/j.jhep.2019.01.036
   Hectors SJ, 2021, EUR RADIOL, V31, P3805, DOI 10.1007/s00330-020-07475-4
   Hong JS, 2017, IEEE ENG MED BIO, P2892, DOI 10.1109/EMBC.2017.8037461
   Hu HT, 2021, J GASTROEN HEPATOL, V36, P2875, DOI 10.1111/jgh.15522
   Hu H, 2021, GASTROINTEST ENDOSC, V93, P1333, DOI 10.1016/j.gie.2020.11.014
   Hu YJ, 2021, GASTRIC CANCER, V24, P868, DOI 10.1007/s10120-021-01158-9
   Huang BS, 2021, IEEE J BIOMED HEALTH, V25, P3498, DOI 10.1109/JBHI.2021.3070708
   Hwang SJ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082691
   Iakovidis DK, 2018, IEEE T MED IMAGING, V37, P2196, DOI 10.1109/TMI.2018.2837002
   Incetan K, 2021, MED IMAGE ANAL, V70, DOI 10.1016/j.media.2021.101990
   Itoh H, 2021, INT J COMPUT ASS RAD, V16, P989, DOI 10.1007/s11548-021-02398-x
   Jansen-Winkeln B, 2021, CANCERS, V13, DOI 10.3390/cancers13050967
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.21492, 10.3322/caac.20107, 10.3322/caac.20115]
   Jiang YM, 2021, JAMA NETW OPEN, V4, DOI 10.1001/jamanetworkopen.2020.32269
   Jimenez-Pastor A, 2021, EUR RADIOL, V31, P7876, DOI 10.1007/s00330-021-07838-5
   Klang E, 2020, GASTROINTEST ENDOSC, V91, P606, DOI 10.1016/j.gie.2019.11.012
   Klimov S, 2021, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.593211
   Kwak MS, 2021, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.619803
   Kwon J, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84761-1
   Lai LL, 2021, J BIOMED OPT, V26, DOI 10.1117/1.JBO.26.1.015001
   Laiz P, 2020, COMPUT MED IMAG GRAP, V86, DOI 10.1016/j.compmedimag.2020.101794
   Langner T, 2019, MAGN RESON MED, V81, P2736, DOI 10.1002/mrm.27550
   Le Berre C, 2020, GASTROENTEROLOGY, V158, P76, DOI 10.1053/j.gastro.2019.08.058
   le Clercq CMC, 2014, GUT, V63, P957, DOI 10.1136/gutjnl-2013-304880
   Lee CM, 2021, EUR RADIOL, V31, P3355, DOI 10.1007/s00330-020-07430-3
   Lee JY, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-65387-1
   Li T, 2021, CHEM RES TOXICOL, V34, P550, DOI 10.1021/acs.chemrestox.0c00374
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu KL, 2020, LANCET DIGIT HEALTH, V2, pE303, DOI 10.1016/S2589-7500(20)30078-9
   Liu SL, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.626626
   Ma BW, 2020, FRONT PHARMACOL, V11, DOI 10.3389/fphar.2020.572372
   Mahmood F, 2018, IEEE T MED IMAGING, V37, P2572, DOI 10.1109/TMI.2018.2842767
   Martin DR, 2020, ARCH PATHOL LAB MED, V144, P370, DOI 10.5858/arpa.2019-0004-OA
   Michalak A, 2016, PHARMACOL REP, V68, P837, DOI 10.1016/j.pharep.2016.04.016
   Min JK, 2019, GUT LIVER, V13, P388
   Mostafapour S, 2021, CLIN NUCL MED, V46, P609, DOI 10.1097/RLU.0000000000003585
   Ozyoruk KB, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102058
   Pan Y, 2020, CLIN TRANSL MED, V10, DOI 10.1002/ctm2.129
   Pannala Rahul, 2020, VideoGIE, V5, P598, DOI 10.1016/j.vgie.2020.08.013
   Park J, 2021, CLIN CANCER RES, V27, P719, DOI 10.1158/1078-0432.CCR-20-3159
   Roth HR, 2018, MED IMAGE ANAL, V45, P94, DOI 10.1016/j.media.2018.01.006
   Ryu H, 2021, EUR RADIOL, V31, P8733, DOI 10.1007/s00330-021-07850-9
   Saillard C, 2020, HEPATOLOGY, V72, P2000, DOI 10.1002/hep.31207
   Saito H, 2020, GASTROINTEST ENDOSC, V92, P144, DOI 10.1016/j.gie.2020.01.054
   Sanaat A, 2020, J NUCL MED, V61, P1388, DOI 10.2967/jnumed.119.239327
   Sato D, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-79021-7
   Seven G, 2022, DIGEST DIS SCI, V67, P273, DOI 10.1007/s10620-021-06830-9
   Shao W., 2021, IEEE T MED IMAGING, DOI [10.1109/TMI.2021.3097319, DOI 10.1109/TMI.2021.3097319]
   Shao W, 2020, IEEE T MED IMAGING, V39, P99, DOI 10.1109/TMI.2019.2920608
   Shiri I, 2020, EUR J NUCL MED MOL I, V47, P2533, DOI 10.1007/s00259-020-04852-5
   Srinidhi CL, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101813
   Struyvenberg MR, 2021, GASTROINTEST ENDOSC, V93, P89, DOI 10.1016/j.gie.2020.05.050
   Su TH, 2021, J GASTROEN HEPATOL, V36, P569, DOI 10.1111/jgh.15415
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tan JW, 2020, J CANCER, V11, P7224, DOI 10.7150/jca.46704
   Tizhoosh Hamid Reza, 2018, J Pathol Inform, V9, P38, DOI 10.4103/jpi.jpi_53_18
   Tsuboi A, 2020, DIGEST ENDOSC, V32, P382, DOI 10.1111/den.13507
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Udristoiu AL, 2021, J GASTROINTEST LIVER, V30, P59, DOI 10.15403/jgld-3212
   Uema R, 2021, J GASTROEN HEPATOL, V36, P2239, DOI 10.1111/jgh.15479
   Wang KS, 2021, BMC MED, V19, DOI 10.1186/s12916-021-01942-5
   Wang SJ, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101549
   Wang W, 2013, EUR RADIOL, V23, P2546, DOI 10.1007/s00330-013-2849-3
   Wang XD, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-21674-7
   Wang Y, 2018, NEUROIMAGE, V174, P550, DOI 10.1016/j.neuroimage.2018.03.045
   Wang YK, 2021, CANCERS, V13, DOI 10.3390/cancers13020321
   Watson MD, 2021, AM SURGEON, V87, P602, DOI 10.1177/0003134820953779
   Wong GLH, 2021, J GASTROEN HEPATOL, V36, P543, DOI 10.1111/jgh.15385
   Wu LL, 2021, ENDOSCOPY, V53, P1199, DOI 10.1055/a-1350-5583
   Wu M, 2022, J ULTRAS MED, V41, P163, DOI 10.1002/jum.15691
   Wu XL, 2020, INT J BIOL SCI, V16, P1551, DOI 10.7150/ijbs.44024
   Xiao W, 2021, LANCET DIGIT HEALTH, V3, pe88, DOI 10.1016/S2589-7500(20)30288-0
   Xie XH, 2010, EUR RADIOL, V20, P239, DOI 10.1007/s00330-009-1538-8
   Xie XZ, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2021.101985
   Xu M, 2021, GASTROINTEST ENDOSC, V94, P540, DOI 10.1016/j.gie.2021.03.013
   Yamada A, 2021, ENDOSCOPY, V53, P832, DOI 10.1055/a-1266-1066
   Yang SF, 2021, IEEE T MED IMAGING, V40, P38, DOI 10.1109/TMI.2020.3021560
   Yao LW, 2021, EBIOMEDICINE, V65, DOI 10.1016/j.ebiom.2021.103238
   Yi X, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101552
   Zhang XY, 2020, RADIOLOGY, V296, P56, DOI 10.1148/radiol.2020190936
   Zhang YQ, 2020, DIGEST LIVER DIS, V52, P566, DOI 10.1016/j.dld.2019.12.146
   Zhang YT, 2021, IEEE T MED IMAGING, V40, P1618, DOI 10.1109/TMI.2021.3062902
   Zhang YX, 2021, J MAGN RESON IMAGING, V54, P134, DOI 10.1002/jmri.27538
   Zhao BC, 2020, CURR PROB CANCER, V44, DOI 10.1016/j.currproblcancer.2020.100579
   Zhou GY, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231880
   Zhou WY, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-21466-z
   Zou J, 2019, NAT GENET, V51, P12, DOI 10.1038/s41588-018-0295-5
NR 112
TC 11
Z9 11
U1 22
U2 99
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2023
VL 39
IS 6
SI SI
BP 2207
EP 2222
DI 10.1007/s00371-021-02322-z
EA OCT 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA J1YY4
UT WOS:000713080200002
PM 34744231
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Fan, CX
   Zhang, RQ
   Ming, Y
AF Fan, Chunxiao
   Zhang, Runqing
   Ming, Yue
TI MP-LN: motion state prediction and localization network for visual
   object tracking
SO VISUAL COMPUTER
LA English
DT Article
DE Visual object tracking; Correlation filters; Reinforcement learning;
   Motion prediction; Policy gradient; Iterative localization
ID ROBUST; SIAMESE
AB Visual object tracking is an important topic in computer vision, where the methods extracting features from the appearance of the object have made a significant progress. However, occlusion and rapid motion cause an incomplete appearance of the object and an incorrect search area in a complex scene, which limits the precision of object localization. In this paper, we propose a novel motion state prediction and localization network, named MP-LN, for visual object tracking, which predicts and translates a reasonable search area depending on the continuous motion state. Specially, we design a motion state prediction model based on the reinforcement learning, which adopts the policy gradients to estimate the target motion and incorporates rewards to enhance the back-propagation of errors for more accurate motion state. After that, we utilize an iterative localization to fine-tune the identification of the target location, reducing the response suppression. Extensive experiments and results demonstrate the effectiveness and advancement of the proposed method on six challenging tracking datasets, DTB70, UAVDT, UAV123, LaSOT, GOT-10k, and OTB2015.
C1 [Fan, Chunxiao; Zhang, Runqing; Ming, Yue] Beijing Univ Posts & Telecommun, Beijing Key Lab Work Safety & Intelligent Monitor, Sch Elect Engn, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Ming, Y (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Work Safety & Intelligent Monitor, Sch Elect Engn, Beijing, Peoples R China.
EM fcxg100@163.com; zrq1993@bupt.edu.cn; myname35875235@126.com
OI Fan, Chunxiao/0000-0002-3607-4904
FU Natural Science Foundation of China [62076030]; Beijing Natural Science
   Foundation of China [L201023]
FX The work presented in this paper was partly supported by Natural Science
   Foundation of China (Grant No. 62076030), Beijing Natural Science
   Foundation of China (Grant No. L201023).
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cehovin L, 2016, IEEE WINT CONF APPL
   Chen YL, 2020, APPL SOFT COMPUT, V93, DOI 10.1016/j.asoc.2020.106335
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   DIRAC PAM, 1953, PHYSICA, V19, P888, DOI 10.1016/S0031-8914(53)80099-6
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu HC, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2020.103869
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Kashiani H, 2019, IMAGE VISION COMPUT, V83-84, P17, DOI 10.1016/j.imavis.2019.02.003
   Kristan M., 2014, 2014 INT C COMP VIS
   Kristan M., 2014, VOT2013 CHALLENGE OV, P1
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Martin, 2019, F S K M F ACCURATE T
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pan YT, 2020, WORLD WIDE WEB, V23, P2259, DOI 10.1007/s11280-020-00793-z
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Supancic J, 2017, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2017.43
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   van der Merwe R, 2001, ADV NEUR IN, V13, P584
   Veerapaneni R., 2020, C ROBOT LEARNING, P1439
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang R, 2016, VISUAL COMPUT, V32, P1379, DOI 10.1007/s00371-015-1206-8
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Ye H, 2019, IEEE T VEH TECHNOL, V68, P3163, DOI 10.1109/TVT.2019.2897134
   Yeo D, 2017, PROC CVPR IEEE, P511, DOI 10.1109/CVPR.2017.62
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhou J., 2018, ECCV, P684
NR 55
TC 5
Z9 5
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD DEC
PY 2022
VL 38
IS 12
BP 4291
EP 4306
DI 10.1007/s00371-021-02296-y
EA SEP 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7B7SH
UT WOS:000696167900001
DA 2024-07-18
ER

PT J
AU Chen, ZH
   Qiu, J
   Sheng, B
   Li, P
   Wu, EH
AF Chen, Zhihua
   Qiu, Jun
   Sheng, Bin
   Li, Ping
   Wu, Enhua
TI GPSD: generative parking spot detection using multi-clue recovery model
SO VISUAL COMPUTER
LA English
DT Article
DE Auto-parking; Parking spot detection; Multi-clue recovery model; Corner
   recognition
AB Due to various complex environmental factors and parking scenes, there are more stringent requirements for automatic parking than the manual one. The existing auto-parking technology is based on space or plane dimension, where the former usually ignores the ground parking spot lines which may cause parking at a wrong position, while the latter often costs a lot of time in object classification which may decreases the algorithm applicability. In this paper, we propose a Generative Parking Spot Detection algorithm which uses a multi-clue recovery model to reconstruct parking spots. In the proposed method, we firstly dismantle the parking spot geometrically for marking the location of its corresponding corners and then use a micro-target recognition network to find corners from the ground image taken by car cameras. After these, we use the multi-clue model to correct the fully pairing map so that the reliable true parking spot can be recovered correctly. The proposed algorithm is compared with several existing algorithms, and the experimental result shows that it has a higher accuracy than others which can reach more than 80% in most test cases.
C1 [Chen, Zhihua; Qiu, Jun] East China Univ Sci & Technol, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Macau, Peoples R China.
C3 East China University of Science & Technology; Shanghai Jiao Tong
   University; Hong Kong Polytechnic University; Chinese Academy of
   Sciences; Institute of Software, CAS; University of Macau
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.; Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing, Peoples R China.
EM czh@ecust.edu.cn; 2267638193@gq.com; shengbin@sjtu.edu.cn;
   p.li@polyu.edu.hk; ehwu@um.edu.mo
RI Li, Ping/AAO-2019-2020
OI Li, Ping/0000-0002-1503-0240; Sheng, Bin/0000-0001-8678-2784
FU National Natural Science Foundation of China [61672228, 62077037,
   61872241, 62072449, 61632003]; Shanghai Automotive Industry Science and
   Technology Development Foundation [1837]; Science and Technology
   Commission of Shanghai Municipality [18410750700, 17411952600]; Hong
   Kong Polytechnic University [P0030419, P0030929, P0035358]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672228, 62077037, 61872241, 62072449
   and 61632003, in part by the Shanghai Automotive Industry Science and
   Technology Development Foundation under Grant 1837, in part by the
   Science and Technology Commission of Shanghai Municipality under Grants
   18410750700 and 17411952600, and in part by The Hong Kong Polytechnic
   University under Grants P0030419, P0030929 and P0035358.
CR Athira A., 2019, 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI). Proceedings, P1184, DOI 10.1109/ICOEI.2019.8862517
   Bacchiani G., 2017, P IEEE 20 INT C INT, P1, DOI DOI 10.1109/ITSC.2017.8317764
   Bochkovskiy A, 2020, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.2004.10934
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Dixit M, 2020, 2020 43RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P170, DOI [10.1109/tsp49548.2020.9163467, 10.1109/TSP49548.2020.9163467]
   Hakim IM, 2019, 2019 IEEE 9TH SYMPOSIUM ON COMPUTER APPLICATIONS & INDUSTRIAL ELECTRONICS (ISCAIE), P222, DOI [10.1109/iscaie.2019.8743906, 10.1109/ISCAIE.2019.8743906]
   Hamada K, 2015, IEEE INT VEH SYM, P1106, DOI 10.1109/IVS.2015.7225832
   Huang JH, 2019, IEEE INT CON MULTI, P212, DOI 10.1109/ICME.2019.00045
   Jian DH, 2020, IEEE ICCE, P528, DOI [10.1109/AIEA51086.2020.00008, 10.1109/icce46568.2020.9043164]
   Lee M, 2019, IEEE T INTELL TRANSP, V20, P2389, DOI 10.1109/TITS.2018.2855183
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Panzani G, 2019, IEEE INT C INTELL TR, P2772, DOI [10.1109/itsc.2019.8916916, 10.1109/ITSC.2019.8916916]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scheunert Ullrich, 2007, Proceedings of the 2007 IEEE Intelligent Vehicles Symposium, P154
   Sedighi S, 2019, INT C CONTROL DECISI, P1711, DOI [10.1109/codit.2019.8820634, 10.1109/CoDIT.2019.8820634]
   Singh T, 2018, 2018 IEEE MTT-S INTERNATIONAL MICROWAVE WORKSHOP SERIES ON ADVANCED MATERIALS AND PROCESSES FOR RF AND THZ APPLICATIONS (IMWS-AMP)
   Song J, 2019, IET INTELL TRANSP SY, V13, P1557, DOI 10.1049/iet-its.2019.0049
   Suhr JK, 2018, ELECTRON LETT, V54, P445, DOI 10.1049/el.2018.0196
   Suhr JK, 2022, IEEE T INTELL TRANSP, V23, P4570, DOI 10.1109/TITS.2020.3046039
   Suhr JK, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.3.037203
   Suhr JK, 2010, MACH VISION APPL, V21, P163, DOI 10.1007/s00138-008-0156-9
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Unger C, 2014, MACH VISION APPL, V25, P561, DOI 10.1007/s00138-011-0385-1
   Wu ZZ, 2020, IEEE INT VEH SYM, P290, DOI 10.1109/IV47402.2020.9304776
   Yamamoto K, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P833, DOI [10.1109/icma.2019.8816556, 10.1109/ICMA.2019.8816556]
   Ye CB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1987, DOI 10.1109/ROBIO.2018.8664884
   Zhang L, 2018, IEEE T IMAGE PROCESS, V27, P5350, DOI 10.1109/TIP.2018.2857407
   Zhang S., 2019, ARXIV PREPRINT ARXIV
   Zinelli A, 2019, IEEE INT VEH SYM, P683, DOI 10.1109/IVS.2019.8813777
NR 31
TC 21
Z9 21
U1 1
U2 14
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD SEP
PY 2021
VL 37
IS 9-11
SI SI
BP 2657
EP 2669
DI 10.1007/s00371-021-02199-y
EA JUN 2021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL2RI
UT WOS:000663489600003
DA 2024-07-18
ER

PT J
AU Jiang, M
   Zhai, FH
   Kong, J
AF Jiang, Min
   Zhai, Fuhao
   Kong, Jun
TI Sparse Attention Module for optimizing semantic segmentation performance
   combined with a multi-task feature extraction network
SO VISUAL COMPUTER
LA English
DT Article
DE Semantic segmentation; Sparse Attention Module; Class attention
   features; Multi-task
AB In the task of semantic segmentation, researchers often use self-attention module to capture long-range contextual information. These methods are often effective. However, the use of the self-attention module will cause a problem that cannot be ignored, that is, the huge consumption of computing resources. Therefore, how to reduce the resource consumption of the self-attention module under the premise of ensuring performance is a very meaningful research topic. In this paper, we propose a Sparse Attention Model combined with a powerful multi-task feature extraction network for semantic segmentation. Compared with the classic self-attention model, our Sparse Attention Model does not calculate the inner product between pairs of all vectors. Instead, we first sparse the feature block Query and the feature block Key defined in self-attention module through the credit matrix generated by the pre-output. Then, we perform similarity modeling on the two sparse feature blocks. Meanwhile, to ensure that the vectors in Query could capture dense contextual information, we design a Class Attention Module and embed it into Sparse Attention Module. Note that, compared with Dual Attention Network for scene segmentation, our attention module greatly reduces the consumption of computing resources while ensuring the accuracy. Furthermore, in the stage of feature extraction, the use of downsampling will cause serious loss of detailed information and affect the segmentation performance of the network, so we adopt a multi-task feature extraction network. It learns semantic features and edge features in parallel, and we feed the learned edge features into the deep layer of the network to help restore detailed information for capturing high-quality semantic features. We do not use pure concatenation. Instead, we extract the edge features related to each channel by element-wise multiplication before concatenation. Finally, we conduct experiments on three datasets: Cityscapes, PASCAL VOC2012 and ADE20K, and obtain competitive results.
C1 [Jiang, Min; Zhai, Fuhao; Kong, Jun] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Jiang, M (corresponding author), Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Jiangsu, Peoples R China.
EM minjiang@jiangnan.edu.cn
FU Fundamental Research Funds for the Central Universities [JUSRP41908];
   National Natural Science Foundation of China [61201429, 61362030]; China
   Postdoctoral Science Foundation [2015M581720, 2016M600360]; Jiangsu
   Postdoctoral Science Foundation [1601216C]
FX This work is partially supported by the Fundamental Research Funds for
   the Central Universities (JUSRP41908), National Natural Science
   Foundation of China (61201429, 61362030), China Postdoctoral Science
   Foundation (2015M581720, 2016M600360) and Jiangsu Postdoctoral Science
   Foundation (1601216C).
CR [Anonymous], 2019, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.492
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bulò SR, 2018, PROC CVPR IEEE, P5639, DOI 10.1109/CVPR.2018.00591
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Chen LJ, 2018, ADV NEUR IN, V31
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Duda R., 1973, Pattern Classification and Scene Analysis
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HENGEL A, 2016, P IEEE C COMP VIS PA, P3194
   Huang L, 2019, ADV NEUR IN, V32
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Iandola F., 2014, DenseNet: Implementing efficient convnet descriptor pyramids
   Kong S, 2018, PROC CVPR IEEE, P956, DOI 10.1109/CVPR.2018.00106
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Nasr G.E., 2002, Forecasting gasoline demand, P381
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shajini M, 2021, VISUAL COMPUT, V37, P1517, DOI 10.1007/s00371-020-01885-7
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tan M., 2020, P IEEECVF C COMPUTER, P10781, DOI [10.48550/arXiv.1911.09070, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2021, VISUAL COMPUT, V37, P1101, DOI 10.1007/s00371-020-01855-z
   Wang J, 2019, ARXIV PREPRINT ARXIV
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu B., 2019, ARXIV PREPRINT ARXIV
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Yuan Y., 2018, ARXIV PREPRINT ARXIV
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng CX, 2018, VISUAL COMPUT, V34, P735, DOI 10.1007/s00371-017-1411-8
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou XN, 2019, VISUAL COMPUT, V35, P385, DOI 10.1007/s00371-018-1471-4
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 53
TC 17
Z9 17
U1 3
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2473
EP 2488
DI 10.1007/s00371-021-02124-3
EA MAY 2021
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000650092900002
DA 2024-07-18
ER

PT J
AU Lipovetsky, E
AF Lipovetsky, Evgeny
TI Subdivision of point-normal pairs with application to smoothing feasible
   robot path
SO VISUAL COMPUTER
LA English
DT Article
DE Subdivision of 2D point-normal pairs; Bezier quasi-average; 2D curve
   design; Modified Lane&#8211; Riesenfeld algorithm; Modified 4-point
   scheme; Subdivision in the environment with obstacles; Robot path
   smoothing
AB In a previous paper (Lipovetsky and Dyn in Comput Aided Geom Des 48:36-48, 2016), we introduced a weighted binary average of two 2D point-normal pairs, termed circle average, and investigated subdivision schemes based on it. These schemes refine point-normal pairs in 2D and converge to limit curves and limit normals. Such a scheme has the disadvantage that the limit normals are not the normals of the limit curve. In this paper, we address this problem by proposing a new averaging method and obtaining a new family of algorithms based on it. We demonstrate their new editing capabilities and apply this subdivision technique to smooth a precomputed feasible polygonal point robot path.
C1 [Lipovetsky, Evgeny] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
C3 Tel Aviv University
RP Lipovetsky, E (corresponding author), Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
EM eug@post.com
OI Lipovetsky, Evgeny/0000-0001-8783-614X
CR Chalmoviansky P, 2007, ADV COMPUT MATH, V27, P375, DOI 10.1007/s10444-005-9011-y
   Connors J, 2007, IEEE VTS VEH TECHNOL, P2565, DOI 10.1109/VETECS.2007.528
   Conti C, 2011, J COMPUT APPL MATH, V236, P461, DOI 10.1016/j.cam.2011.06.004
   de Berg M., 2008, COMPUTATIONAL GEOMET, V3rd, DOI [10.1007/978-3-540-77974-2_1, DOI 10.1007/978-3-540-77974]
   Dokken T., 1990, Computer-Aided Geometric Design, V7, P33, DOI 10.1016/0167-8396(90)90019-N
   Dyn N, 2002, ACT NUMERIC, V11, P73, DOI 10.1017/S0962492902000028
   Dyn N, 2017, J COMPUT APPL MATH, V311, P54, DOI 10.1016/j.cam.2016.07.008
   Dyn Nira, 2002, SERDICA MATH J, V28, P349
   Ewald T, 2015, CONSTR APPROX, V42, P425, DOI 10.1007/s00365-015-9305-3
   Gloderer M., 2010, Spline-Based Trajectory Optimization for Autonomous Vehicles with Ackerman drive
   Lipovetsky E, 2019, COMPUT AIDED GEOM D, V69, P45, DOI 10.1016/j.cagd.2019.01.001
   Lipovetsky E, 2016, COMPUT AIDED GEOM D, V48, P36, DOI 10.1016/j.cagd.2016.07.004
   Mao AH, 2016, VISUAL COMPUT, V32, P1085, DOI 10.1007/s00371-015-1175-y
   Micchelli CA, 1998, MATH Z, V229, P621, DOI 10.1007/PL00004676
   Prautzsch H., 2002, B ZIER B SPLINE TECH, DOI [10.1007/978-3-662-04919-8, DOI 10.1007/978-3-662-04919-8]
   Rahman IU, 2005, MULTISCALE MODEL SIM, V4, P1201, DOI 10.1137/050622729
   Thompson S, 2006, INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR MODELLING, CONTROL & AUTOMATION JOINTLY WITH INTERNATIONAL CONFERENCE ON INTELLIGENT AGENTS, WEB TECHNOLOGIES & INTERNET COMMERCE, VOL 1, PROCEEDINGS, P863
   Wallner J, 2005, COMPUT AIDED GEOM D, V22, P593, DOI 10.1016/j.cagd.2005.06.003
   Zhang A., 2010, 2 INT C NEV SCI, V4
NR 19
TC 2
Z9 3
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2022
VL 38
IS 7
BP 2271
EP 2284
DI 10.1007/s00371-021-02110-9
EA MAR 2021
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2B0WL
UT WOS:000635024500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, GQ
   Li, R
   Liu, YJ
   Wang, J
AF Yang, Guiqiang
   Li, Rui
   Liu, Yujun
   Wang, Ji
TI A robust nonrigid point set registration framework based on global and
   intrinsic topological constraints
SO VISUAL COMPUTER
LA English
DT Article
DE Nonrigid point set registration; Gaussian mixture model; Thin-plate
   spline; Graph Laplacian regularization; Expectation maximization
ID IMAGE; ALGORITHM; AFFINE
AB The problem of registering nonrigid point sets, with the aim of estimating the correspondences and learning the transformation between two given sets of points, often arises in computer vision tasks. This paper proposes a novel method for performing nonrigid point set registration on data with various types of degradation, in which the registration problem is formulated as a Gaussian mixture model (GMM)-based density estimation problem. Specifically, two complementary constraints are jointly considered for optimization in a GMM probabilistic framework. The first is a thin-plate spline-based regularization constraint that maintains global spatial motion consistency, and the second is a spectral graph-based regularization constraint that preserves the intrinsic structure of a point set. Moreover, the correspondences and the transformation are alternately optimized using the expectation maximization algorithm to obtain a closed-form solution. We first utilize local descriptors to construct the initial correspondences and then estimate the underlying transformation under the GMM-based framework. Experimental results on contour images and real images show the effectiveness and robustness of the proposed method.
C1 [Yang, Guiqiang; Li, Rui; Liu, Yujun; Wang, Ji] Dalian Univ Technol, Sch Naval Architecture, Dalian 116024, Peoples R China.
   [Li, Rui; Liu, Yujun; Wang, Ji] State Key Lab Struct Anal Ind Equipment, Dalian 116024, Peoples R China.
C3 Dalian University of Technology
RP Li, R (corresponding author), Dalian Univ Technol, Sch Naval Architecture, Dalian 116024, Peoples R China.; Li, R (corresponding author), State Key Lab Struct Anal Ind Equipment, Dalian 116024, Peoples R China.
EM lirui@dlut.edu.cn
RI Li, Rui/C-2155-2009
OI Yang, Guiqiang/0000-0003-3386-7201
FU Science and Technology Innovation Foundation of Dalian [2018J12GX057];
   National Natural Science Foundation of China [51979034]
FX This work was supported in part by the Science and Technology Innovation
   Foundation of Dalian (2018J12GX057) and in part by the National Natural
   Science Foundation of China (51979034).
CR Amberg Brian, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   [Anonymous], 2006, 2006 IEEE COMPUTER S
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374
   Choi J, 2020, VISUAL COMPUT, V36, P2039, DOI 10.1007/s00371-020-01902-9
   Choy C., 2020, CVPR, P11227
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   de Lima R, 2021, J REAL-TIME IMAGE PR, V18, P143, DOI 10.1007/s11554-020-00959-y
   de Vos BD, 2019, MED IMAGE ANAL, V52, P128, DOI 10.1016/j.media.2018.11.010
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng WX, 2018, VISUAL COMPUT, V34, P1399, DOI 10.1007/s00371-017-1444-z
   Moreno-García CF, 2017, PATTERN ANAL APPL, V20, P201, DOI 10.1007/s10044-015-0486-y
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Ge S, 2019, MACH VISION APPL, V30, P717, DOI 10.1007/s00138-019-01024-w
   Hammouda G, 2020, VISUAL COMPUT, V36, P279, DOI 10.1007/s00371-018-1604-9
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   Hu L, 2020, VISUAL COMPUT, V36, P669, DOI 10.1007/s00371-019-01648-z
   Jian B, 2005, IEEE I CONF COMP VIS, P1246
   Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572
   Kán P, 2019, VISUAL COMPUT, V35, P873, DOI 10.1007/s00371-019-01666-x
   Krejsa J, 2019, INT C MECH, P229
   Krishnakumar K, 2020, VISUAL COMPUT, V36, P1837, DOI 10.1007/s00371-019-01780-w
   Lati A, 2020, EVOL SYST-GER, V11, P717, DOI 10.1007/s12530-019-09279-4
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2019, PATTERN RECOGN, V92, P231, DOI 10.1016/j.patcog.2019.04.001
   Ma JY, 2019, IEEE T IMAGE PROCESS, V28, P4045, DOI 10.1109/TIP.2019.2906490
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528
   Ma JY, 2018, IEEE T GEOSCI REMOTE, V56, P4435, DOI 10.1109/TGRS.2018.2820040
   Ma JY, 2017, AAAI CONF ARTIF INTE, P4218
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Myronenko A., 2007, Advances in Neural Information Processing Systems, P1009
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Sedaghat A, 2019, INT J REMOTE SENS, V40, P2576, DOI 10.1080/01431161.2018.1528402
   Song Ge, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P126, DOI 10.1109/CVPRW.2015.7301306
   Tsin Y, 2004, LECT NOTES COMPUT SC, V3023, P558
   Ullah K, 2020, COMPLEX INTELL SYST, V6, P15, DOI 10.1007/s40747-019-0103-6
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wang G, 2017, IEEE T IMAGE PROCESS, V26, P1759, DOI 10.1109/TIP.2017.2658947
   Wang JH, 2019, SIGNAL PROCESS, V157, P225, DOI 10.1016/j.sigpro.2018.12.004
   Xiao D, 2010, COMPUT MED IMAG GRAP, V34, P321, DOI 10.1016/j.compmedimag.2009.12.003
   Yang CC, 2018, IEEE ACCESS, V6, P75947, DOI 10.1109/ACCESS.2018.2883689
   Yang CW, 2019, VISUAL COMPUT, V35, P695, DOI 10.1007/s00371-018-1504-z
   Yang GQ, 2020, IEEE ACCESS, V8, P130263, DOI 10.1109/ACCESS.2020.3009255
   Yang Y, 2015, PATTERN RECOGN, V48, P156, DOI 10.1016/j.patcog.2014.06.017
   Yuille A. L., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P344, DOI 10.1109/CCV.1988.590011
   YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430
   Zhang SW, 2020, SOFT COMPUT, V24, P4041, DOI 10.1007/s00500-019-04172-3
   Zhu J, 2012, IET COMPUT VIS, V6, P252, DOI 10.1049/iet-cvi.2011.0178
NR 60
TC 3
Z9 3
U1 0
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2022
VL 38
IS 2
BP 603
EP 623
DI 10.1007/s00371-020-02037-7
EA FEB 2021
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZE7IA
UT WOS:000620113400001
DA 2024-07-18
ER

PT J
AU Sole, A
   Guarnera, GC
   Farup, I
   Nussbaum, P
AF Sole, Aditya
   Guarnera, Giuseppe Claudio
   Farup, Ivar
   Nussbaum, Peter
TI Measurement and rendering of complex non-diffuse and goniochromatic
   packaging materials
SO VISUAL COMPUTER
LA English
DT Article
DE Visual appearance; Goniochromatism; Genetic algorithm-based
   optimisation; BRDF measurement; Retro-reflection
ID BRDF; APPEARANCE; PARAMETERIZATION; ACCURATE; MODEL
AB Realistic renderings of materials with complex optical properties, such as goniochromatism and non-diffuse reflection, are difficult to achieve. In the context of the print and packaging industries, accurate visualisation of the complex appearance of such materials is a challenge, both for communication and quality control. In this paper, we characterise the bidirectional reflectance of two homogeneous print samples displaying complex optical properties. We demonstrate that in-plane retro-reflective measurements from a single input photograph, along with genetic algorithm-based BRDF fitting, allow to estimate an optimal set of parameters for reflectance models, to use for rendering. While such a minimal set of measurements enables visually satisfactory renderings of the measured materials, we show that a few additional photographs lead to more accurate results, in particular, for samples with goniochromatic appearance.
C1 [Sole, Aditya; Guarnera, Giuseppe Claudio; Nussbaum, Peter] Norwegian Univ Sci & Technol, Gjovik, Norway.
   [Farup, Ivar] Norwegian Univ Sci & Technol, Comp Sci, Gjovik, Norway.
   [Guarnera, Giuseppe Claudio] Univ York, Comp Graph & Vis, York, N Yorkshire, England.
C3 Norwegian University of Science & Technology (NTNU); Norwegian
   University of Science & Technology (NTNU); University of York - UK
RP Sole, A (corresponding author), Norwegian Univ Sci & Technol, Gjovik, Norway.
EM aditya.sole@ntnu.no
RI Sole, Aditya/KBR-2975-2024; Farup, Ivar/HOC-8111-2023; Guarnera,
   Giuseppe Claudio/ABC-8635-2022
OI Farup, Ivar/0000-0003-3473-1138; Guarnera, Giuseppe
   Claudio/0000-0002-7703-5194; Sole, Aditya/0000-0002-7916-5363
FU NTNU Norwegian University of Science and Technology; St. Olavs Hospital
   - Trondheim University Hospital; "MUVApp" Project - Research Council of
   Norway [N-250293]; "Spectraskin" Project - Research Council of Norway
   [N-288670]; European Union [814158]; Marie Curie Actions (MSCA) [814158]
   Funding Source: Marie Curie Actions (MSCA)
FX Open Access funding provided by NTNU Norwegian University of Science and
   Technology (incl St. Olavs Hospital - Trondheim University Hospital).
   This work was supported by the "MUVApp" Project N-250293, "Spectraskin"
   Project N-288670 funded by the Research Council of Norway, and from the
   European Union's Horizon 2020 research and innovation programme under
   the Marie SkodowskaCurie Grant Agreement No. 814158 (ApPEARS Project
   [https://www.appears-itn.eu]).
CR [Anonymous], 2006, Symposium on Rendering, DOI [DOI 10.2312/EGWR/EGSR06/399-407, 10.2312/EGWR/EGSR06/399-407]
   [Anonymous], 1987, Proceedings of the 2nd International Conference on Genetic Algorithms, DOI DOI 10.1007/S10489-006-0018-Y
   Ashikhmin M., 2007, DISTRIBUTION B UNPUB, P10
   Bagher MM, 2012, COMPUT GRAPH FORUM, V31, P1509, DOI 10.1111/j.1467-8659.2012.03147.x
   Brady A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601193
   CHURCH EL, 1989, P SOC PHOTO-OPT INS, V1165, P136
   CHURCH EL, 1991, P SOC PHOTO-OPT INS, V1530, P71, DOI 10.1117/12.50498
   Cook R. L., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293
   Debattista K, 2018, COMPUT GRAPH FORUM, V37, P439, DOI 10.1111/cgf.13307
   Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059
   Ershov S, 2004, VISUAL COMPUT, V20, P586, DOI 10.1007/s00371-004-0248-0
   Ferrero A, 2013, OPT EXPRESS, V21, P26812, DOI 10.1364/OE.21.026812
   Fores A, 2012, COLOR IMAG CONF, P142
   Gitlina Y, 2020, COMPUT GRAPH FORUM, V39, P75, DOI 10.1111/cgf.14055
   Guarnera D, 2016, COMPUT GRAPH FORUM, V35, P625, DOI 10.1111/cgf.12867
   Guarnera D, 2020, IEEE T VIS COMPUT GR, V26, P2258, DOI 10.1109/TVCG.2018.2886877
   Guo J, 2018, GRAPH MODELS, V96, P38, DOI 10.1016/j.gmod.2018.01.002
   Guo J, 2017, PROC INT CONF SOFTW, P3, DOI 10.1109/ICSE.2017.9
   Havran V, 2016, COMPUT GRAPH FORUM, V35, P1, DOI 10.1111/cgf.12944
   Höpe A, 2010, METROLOGIA, V47, P295, DOI 10.1088/0026-1394/47/3/021
   Jakob Wenzel, 2010, Mitsuba renderer
   Kehren K, 2013, THESIS
   KURT M, 2020, VIS COMPUT
   Lafortune E. P. F., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P117, DOI 10.1145/258734.258801
   Lagunas M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323036
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   Lu R, 1998, APPL OPTICS, V37, P5974, DOI 10.1364/AO.37.005974
   Maile FJ, 2005, PROG ORG COAT, V54, P150, DOI 10.1016/j.porgcoat.2005.07.003
   Marschner SR, 1999, SPRING EUROGRAP, P131
   Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343
   McCamy CS, 1996, COLOR RES APPL, V21, P292, DOI 10.1002/(SICI)1520-6378(199608)21:4<292::AID-COL4>3.3.CO;2-7
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Ngan A., 2005, Rendering Techniques, V2005, P2
   Nicodemus F. E., 1978, Geometrical considerations and nomenclature for reflectance
   Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085
   Palmer J.M. Grant., 2010, ART RADIOMETRY
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242
   Sole A., 2014, COL IM C 2014, V2014, P91
   Sole A, 2018, J IMAGING, V4, DOI 10.3390/jimaging4110136
   Sole A, 2018, APPL OPTICS, V57, P1918, DOI 10.1364/AO.57.001918
   Soler C, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13348
   Sun TC, 2017, COMPUT GRAPH FORUM, V36, P47, DOI 10.1111/cgf.13223
   Tominaga S, 2000, IEEE COMPUT GRAPH, V20, P58, DOI 10.1109/38.865881
   Tongbuasirilai T, 2020, VISUAL COMPUT, V36, P855, DOI 10.1007/s00371-019-01664-z
   Toscani M, 2020, ACM T APPL PERCEPT, V17, DOI 10.1145/3380741
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Xu ZX, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982396
NR 48
TC 5
Z9 5
U1 8
U2 15
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2021
VL 37
IS 8
BP 2207
EP 2220
DI 10.1007/s00371-020-01980-9
EA OCT 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT5FX
UT WOS:000579329400001
OA Green Published, Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Talhaoui, MZ
   Wang, XY
   Talhaoui, A
AF Talhaoui, Mohamed Zakariya
   Wang, Xingyuan
   Talhaoui, Abdallah
TI A new one-dimensional chaotic map and its application in a novel
   permutation-less image encryption scheme
SO VISUAL COMPUTER
LA English
DT Article
DE Image encryption; One-dimensional chaotic map; Chaos theory; Real-time
   image processing; Cryptography
ID APPROXIMATE ENTROPY; SYSTEM
AB In this paper, we propose a new real one-dimensional cosine fractional (1-DCF) chaotic map. Several chaos-theory analysis tests demonstrate that the proposed map has many good cryptography properties, such as a highly chaotic behavior, a large chaotic range, an infinite number of unstable fixed points, and a widely superior sensitivity to the initial conditions than most of the low-dimensional chaotic maps. Regarding these attractive features, we use the 1-DCF map to design a novel fast image encryption scheme for real-time image processing. Unlike most of the existing encryption schemes, we adopt a permutation-less architecture to increase the encryption speed. Regardless of the permutation phase absence, a high-security level is obtained by using a substitution process with a high sensitivity to the plain image. Moreover, we replace the natural row-order encryption with a more secure random-like encryption order generated from the secret key. Experimentation and simulations show that the new scheme is better than many recently proposed encryption schemes in both security and rapidity.
C1 [Wang, Xingyuan] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Talhaoui, Mohamed Zakariya; Wang, Xingyuan] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Talhaoui, Abdallah] Ecole Natl Polytech Oran, Dept FPST, Oran 31000, Algeria.
C3 Dalian Maritime University; Dalian University of Technology; Ecole
   Nationale Polytechnique - Algeria
RP Wang, XY (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.; Wang, XY (corresponding author), Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
EM talhaouizakariya@mail.dlut.edu.cn; wangxy@dlut.edu.cn;
   abdellah.talhaoui@enp-oran.dz
RI Mohamed zakariya, Talhaoui/AAD-6018-2022; Wang, Xing-yuan/I-6353-2015
OI TALHAOUI, Mohamed Zakariya/0000-0001-9020-8590
FU National Natural Science Foundation of China [61672124]; Password Theory
   Project of the 13th Five-Year Plan National Cryptography Development
   Fund [MMJJ20170203]; Liaoning Province Science and Technology Innovation
   Leading Talents Program Project [XLYC1802013]; Key R&D Projects of
   Liaoning Province [2019020105-JH2/103]; Jinan City '20 universities'
   Funding Projects Introducing Innovation Team Program [2019GXRC031]
FX This research is supported by the National Natural Science Foundation of
   China (No. 61672124), the Password Theory Project of the 13th Five-Year
   Plan National Cryptography Development Fund (No. MMJJ20170203), Liaoning
   Province Science and Technology Innovation Leading Talents Program
   Project (No. XLYC1802013), Key R&D Projects of Liaoning Province (No.
   2019020105-JH2/103), Jinan City '20 universities' Funding Projects
   Introducing Innovation Team Program (No. 2019GXRC031).
CR Akhshani A, 2012, COMMUN NONLINEAR SCI, V17, P4653, DOI 10.1016/j.cnsns.2012.05.033
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Artiles JAP, 2019, SIGNAL PROCESS-IMAGE, V79, P24, DOI 10.1016/j.image.2019.08.014
   Boriga R, 2014, ADV MULTIMED, V2014, DOI 10.1155/2014/409586
   Boriga R, 2014, SIGNAL PROCESS-IMAGE, V29, P887, DOI 10.1016/j.image.2014.04.001
   Castro JCH, 2005, MATH COMPUT SIMULAT, V68, P1, DOI 10.1016/j.matcom.2004.09.001
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Chen W, 2010, OPT LETT, V35, P3817, DOI 10.1364/OL.35.003817
   COOPERSMITH D, 1994, IBM J RES DEV, V38, P243, DOI 10.1147/rd.383.0243
   El Assad S, 2016, SIGNAL PROCESS-IMAGE, V41, P144, DOI 10.1016/j.image.2015.10.004
   He D, 2001, IEEE T CIRCUITS-I, V48, P900, DOI 10.1109/81.933333
   Hua ZY, 2020, IEEE T IND INFORM, V16, P887, DOI 10.1109/TII.2019.2923553
   Hua ZY, 2020, IEEE T SIGNAL PROCES, V68, P1937, DOI 10.1109/TSP.2020.2979596
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Hua ZY, 2019, IEEE ACCESS, V7, P8660, DOI 10.1109/ACCESS.2018.2890116
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Kang XJ, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115670
   Li CQ, 2014, NONLINEAR DYNAM, V78, P1545, DOI 10.1007/s11071-014-1533-8
   Li YP, 2017, OPT LASER ENG, V90, P238, DOI 10.1016/j.optlaseng.2016.10.020
   Liu LF, 2018, MULTIMED TOOLS APPL, V77, P21445, DOI 10.1007/s11042-017-5594-9
   Mansouri A, 2020, INFORM SCIENCES, V520, P46, DOI 10.1016/j.ins.2020.02.008
   PINCUS S, 1995, CHAOS, V5, P110, DOI 10.1063/1.166092
   PINCUS SM, 1991, P NATL ACAD SCI USA, V88, P2297, DOI 10.1073/pnas.88.6.2297
   Rijmen V., 2001, P FED INF PROC STAND, V19, P22
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Shao ZH, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115662
   Stallings W., 2011, CRYPTOGRAPHY NETWORK
   Talhaoui MZ, 2021, VISUAL COMPUT, V37, P541, DOI 10.1007/s00371-020-01822-8
   Talhaoui MZ, 2021, J REAL-TIME IMAGE PR, V18, P85, DOI 10.1007/s11554-020-00948-1
   Tang JY, 2019, MULTIMED TOOLS APPL, V78, P24765, DOI 10.1007/s11042-019-7602-8
   Wang B, 2013, OPTIK, V124, P1773, DOI 10.1016/j.ijleo.2012.06.020
   Wang XY, 2019, MOD PHYS LETT B, V33, DOI 10.1142/S0217984919502634
   Wang XY, 2019, NONLINEAR DYNAM, V95, P2797, DOI 10.1007/s11071-018-4723-y
   WOLF A, 1985, PHYSICA D, V16, P285, DOI 10.1016/0167-2789(85)90011-9
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Yang YG, 2016, INFORM SCIENCES, V345, P257, DOI 10.1016/j.ins.2016.01.078
   Zhou NR, 2015, QUANTUM INF PROCESS, V14, P1193, DOI 10.1007/s11128-015-0926-z
NR 38
TC 41
Z9 43
U1 2
U2 66
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1757
EP 1768
DI 10.1007/s00371-020-01936-z
EA AUG 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000558137100001
DA 2024-07-18
ER

PT J
AU Abdelbaky, A
   Aly, S
AF Abdelbaky, Amany
   Aly, Saleh
TI Two-stream spatiotemporal feature fusion for human action recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Human action recognition; Spatiotemporal; Convolutional neural networks;
   Principal component analysis network; BoF; VLAD
ID BAG; DESCRIPTORS; HISTOGRAMS
AB Human action recognition is still a challenging topic in the computer vision field that has attracted a large number of researchers. It has a significant importance in varieties of applications such as intelligent video surveillance, sports analysis, and human-computer interaction. Recent works attempt to exploit the progress in deep learning architecture to learn spatial and temporal features from action video. However, it remains unclear how to combine spatial and temporal information with convolutional neural network. In this paper, we propose a novel human action recognition method by fusing spatial and temporal features learned from a simple unsupervised convolutional neural network called principal component analysis network (PCANet) in combination with bag-of-features (BoF) and vector of locally aggregated descriptors (VLAD ) encoding schemes. Firstly, both spatial and temporal features are learned via PCANet using a subset of frames and temporal templates for each video, while their dimensionality is reduced using whitening transformation (WT). The temporal templates are calculated using short-time motion energy images (ST-MEI) based on frame differencing. Then, the encoding scheme is applied to represent the final dual spatiotemporal PCANet features by feature fusion. Finally, the support vector machine (SVM) classifier is exploited for action recognition. Extensive experiments have been performed on two popular datasets, namely KTH and UCF sports, to evaluate the performance of proposed method. Our experimental results using leave-one-out evaluation strategy demonstrate that the proposed method presents satisfactory and comparable results on both datasets.
C1 [Abdelbaky, Amany; Aly, Saleh] Aswan Univ, Fac Engn, Dept Elect Engn, Aswan 81542, Egypt.
   [Aly, Saleh] Majmaah Univ, Dept Informat Technol, Coll Comp & Informat Sci, Majmaah 11952, Saudi Arabia.
C3 Egyptian Knowledge Bank (EKB); Aswan University; Majmaah University
RP Aly, S (corresponding author), Aswan Univ, Fac Engn, Dept Elect Engn, Aswan 81542, Egypt.; Aly, S (corresponding author), Majmaah Univ, Dept Informat Technol, Coll Comp & Informat Sci, Majmaah 11952, Saudi Arabia.
EM amany.abdelbaki@aswu.edu.eg; saleh@aswu.edu.eg
RI Ahmed, Amany/AAI-1552-2021; Aly, Saleh/B-9095-2019
OI Aly, Saleh/0000-0002-1772-4254
FU Deanship of Scientific Research at Majmaah University [RGP-2019-24]
FX The author SalehAly extends their appreciation to the Deanship of
   Scientific Research at Majmaah University for funding this work under
   project number No. (RGP-2019-24).
CR Abdelbaky A, 2020, PROCEEDINGS OF 2020 INTERNATIONAL CONFERENCE ON INNOVATIVE TRENDS IN COMMUNICATION AND COMPUTER ENGINEERING (ITCE), P257, DOI [10.1109/itce48509.2020.9047769, 10.1109/ITCE48509.2020.9047769]
   Abdelbaky A, 2020, NEURAL COMPUT APPL, V32, P12561, DOI 10.1007/s00521-020-04712-1
   Agahian S, 2019, VISUAL COMPUT, V35, P591, DOI 10.1007/s00371-018-1489-7
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aly S, 2019, MULTIMED TOOLS APPL
   Aly S, 2019, IEEE ACCESS, V7, P52024, DOI 10.1109/ACCESS.2019.2911851
   Aly W, 2019, IEEE ACCESS, V7, P123138, DOI 10.1109/ACCESS.2019.2938829
   [Anonymous], 4 UK COMP VIS STUD W
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arashloo SR, 2017, J VIS COMMUN IMAGE R, V43, P89, DOI 10.1016/j.jvcir.2016.12.015
   Asadi-Aghbolaghi M, 2017, IEEE INT CONF AUTOMA, P476, DOI 10.1109/FG.2017.150
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007
   Csurka G, 2011, COMM COM INF SC, V229, P28
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Han YM, 2018, PATTERN RECOGN LETT, V107, P83, DOI 10.1016/j.patrec.2017.08.015
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kessy A, 2018, AM STAT, V72, P309, DOI 10.1080/00031305.2016.1277159
   Khan FS, 2018, MACH VISION APPL, V29, P55, DOI 10.1007/s00138-017-0871-1
   Khan FS, 2013, INT J COMPUT VISION, V105, P205, DOI 10.1007/s11263-013-0633-0
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Li Y, 2015, VISUAL COMPUT, V31, P1383, DOI 10.1007/s00371-014-1020-8
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nazir S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19122790
   Nazir S, 2018, COMPUT ELECTR ENG, V72, P660, DOI 10.1016/j.compeleceng.2018.01.037
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Pei LS, 2016, VISUAL COMPUT, V32, P1395, DOI 10.1007/s00371-015-1090-2
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727
   Schindler K, 2008, PROC CVPR IEEE, P3025
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shapovalova N, 2012, LECT NOTES COMPUT SC, V7578, P55, DOI 10.1007/978-3-642-33786-4_5
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Shi J, 2017, IEEE J BIOMED HEALTH, V21, P1327, DOI 10.1109/JBHI.2016.2602823
   Shin A., 2016, ARXIV160309046
   Sun C, 2013, IEEE WORK APP COMP, P15, DOI 10.1109/WACV.2013.6474994
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang L, 2018, IEEE ACCESS, V6, P17913, DOI 10.1109/ACCESS.2018.2817253
   Wang TQ, 2014, IEEE T CIRC SYST VID, V24, P277, DOI 10.1109/TCSVT.2013.2276856
   Wu JZ, 2014, VISUAL COMPUT, V30, P1395, DOI 10.1007/s00371-013-0899-9
   Wu JS, 2017, IEEE ACCESS, V5, P3322, DOI 10.1109/ACCESS.2017.2675478
   Xu HY, 2016, MULTIMED TOOLS APPL, V75, P5701, DOI 10.1007/s11042-015-2536-2
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yan SY, 2017, SIGNAL PROCESS-IMAGE, V54, P118, DOI 10.1016/j.image.2017.03.010
   Yao GL, 2019, PATTERN RECOGN LETT, V118, P14, DOI 10.1016/j.patrec.2018.05.018
   Yuan CF, 2013, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2013.99
   Zhang KT, 2018, MULTIMED TOOLS APPL, V77, P16053, DOI 10.1007/s11042-017-5179-7
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhen XT, 2016, IMAGE VISION COMPUT, V50, P1, DOI 10.1016/j.imavis.2016.02.006
   Zhou BL, 2014, ADV NEUR IN, V27
NR 70
TC 33
Z9 33
U1 3
U2 34
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2021
VL 37
IS 7
BP 1821
EP 1835
DI 10.1007/s00371-020-01940-3
EA AUG 2020
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ5DG
UT WOS:000557738600002
DA 2024-07-18
ER

PT J
AU Zhao, HH
   Rosin, PL
   Lai, YK
   Wang, YN
AF Zhao, Hui-Huang
   Rosin, Paul L.
   Lai, Yu-Kun
   Wang, Yao-Nan
TI Automatic semantic style transfer using deep convolutional neural
   networks and soft masks
SO VISUAL COMPUTER
LA English
DT Article
DE Deep neural networks; Style transfer; Soft mask; Semantic segmentation
ID TEXTURE SYNTHESIS; IMAGE
AB This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image are often applied inappropriately to the content image, ignoring its semantic layout and ruining the transfer result. In order to reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their soft semantic masks from the style and content images, in order to preserve the structure of the content image while having the style transferred. Each soft mask of the style image represents a specific part of the style image, corresponding to the soft mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random field model. The results on various images show that our method outperforms the most recent techniques.
C1 [Zhao, Hui-Huang] Hengyang Normal Univ, Coll Comp Sci & Technol, Hengyang, Peoples R China.
   [Rosin, Paul L.; Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
   [Wang, Yao-Nan] Hunan Univ, Coll Elect & Informat Engn, Changsha, Peoples R China.
C3 Hengyang Normal University; Cardiff University; Hunan University
RP Zhao, HH (corresponding author), Hengyang Normal Univ, Coll Comp Sci & Technol, Hengyang, Peoples R China.
EM happyday.huihuang@gmail.com; RosinPL@cardiff.ac.uk; LaiY4@cardiff.ac.uk;
   yaonan@hnu.cn
RI Lai, Yu-Kun/D-2343-2010
FU National Natural Science Foundation of China [61503128]; Science and
   Technology Plan Project of Hunan Province [2016TP1020]; Scientific
   Research Fund of Hunan Provincial Education Department [16C0226,
   18A333]; Hengyang guided science and technology projects and
   Application-oriented Special Disciplines [Hengkefa [2018]60-31]; Double
   FirstClass University Project of Hunan Province [Xiangjiaotong
   [2018]469]; Hunan Province Special Funds of Central Government for
   Guiding Local Science and Technology Development [2018CT5001]; Subject
   Group Construction Project of Hengyang Normal University [18XKQ02,
   61733004]
FX This work was supported by National Natural Science Foundation of China
   (61503128), Science and Technology Plan Project of Hunan Province
   (2016TP1020), Scientific Research Fund of Hunan Provincial Education
   Department (16C0226, 18A333), Hengyang guided science and technology
   projects and Applicationoriented Special Disciplines (Hengkefa
   [2018]60-31), Double FirstClass University Project of Hunan Province
   (Xiangjiaotong [2018]469), Hunan Province Special Funds of Central
   Government for Guiding Local Science and Technology Development
   (2018CT5001) and Subject Group Construction Project of Hengyang Normal
   University (18XKQ02), Key Programme (61733004). We would like to thank
   NVIDIA for the GPU donation.
CR Azadi Samaneh, 2017, ARXIV171200516
   Baltruaitis T, 2016, 2016 IEEE WINT C APP, DOI [DOI 10.1109/WACV.2016.7477553, 10.1109/WACV.2016.7477553]
   Brancati N, 2017, COMPUT VIS IMAGE UND, V155, P33, DOI 10.1016/j.cviu.2016.12.001
   Champandard AJ, ARXIV160301768
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Deng X, 2018, IEEE SIGNAL PROC LET, V25, P571, DOI 10.1109/LSP.2018.2805809
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fiser J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073660
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gilbert A., 2018, IEEE C COMP VIS PATT
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gooch A., 1998, C COMP GRAPH INT TEC
   Hall P., 2015, Computational Visual Media, V1, P91, DOI DOI 10.1007/S41095-015-0017-1
   Isenberg T, 2013, CARTOGR J, V50, P8, DOI 10.1179/1743277412Y.0000000007
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim B.C., 2019, ARXIV190507442
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lerotic M, 2007, LECT NOTES COMPUT SC, V4792, P102
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YJ, 2017, ADV NEUR IN, V30
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Luft T., 2008, INT S COMP AESTH GRA
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Thoma M., 2016, SURVEY SEMANTIC SEGM
   Ulyanov D., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, V1, P6
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Yang Y, 2017, MULTIMED TOOLS APPL, V76, P523, DOI 10.1007/s11042-015-3063-x
   Zhang H., 2017, MULTISTYLE GENERATIV
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou X, 2018, IEEE CONF COMPUT
NR 47
TC 28
Z9 31
U1 2
U2 33
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUL
PY 2020
VL 36
IS 7
BP 1307
EP 1324
DI 10.1007/s00371-019-01726-2
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LO1PW
UT WOS:000533401000002
DA 2024-07-18
ER

PT J
AU Abdellatef, E
   Ismail, NA
   Abd Elrahman, SESE
   Ismail, KN
   Rihan, M
   Abd El-Samie, FE
AF Abdellatef, Essam
   Ismail, Nabil A.
   Abd Elrahman, Salah Eldin S. E.
   Ismail, Khalid N.
   Rihan, Mohamed
   Abd El-Samie, Fathi E.
TI Cancelable multi-biometric recognition system based on deep learning
SO VISUAL COMPUTER
LA English
DT Article
DE Face recognition; Deep learning; Fusion of features; Cancelable
   biometrics
ID FEATURES
AB In this paper, we propose a cancelable multi-biometric face recognition method that uses multiple convolutional neural networks (CNNs) to extract deep features from different facial regions. We also propose a new CNN architecture that exploits batch normalization, depth concatenation and a residual learning framework. The proposed method adopts a region-based technique in which face, eyes, nose and mouth regions are detected from the original face images. Multiple CNNs are used to extract deep features from each region, and then, a fusion network combines these features. Moreover, to provide user's privacy and increase the system resistance against spoof attacks, a cancelable biometric technique using bio-convolving encryption is performed on the final facial descriptor. Our experiments on the FERET, LFW and PaSC datasets show excellent and competitive results compared to state-of-the-art methods in terms of recognition accuracy, specificity, precision, recall and f(score).
C1 [Abdellatef, Essam] Delta Acad Engn, Elect & Commun Dept, Mansoura, Egypt.
   [Ismail, Nabil A.; Abd Elrahman, Salah Eldin S. E.] Menoufia Univ, Dept Comp Sci & Engn, Fac Elect Engn, Menoufia 32952, Egypt.
   [Ismail, Khalid N.] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Ismail, Khalid N.] Menoufia Univ, Dept Informat Technol, Fac Comp & Informat, Menoufia, Egypt.
   [Rihan, Mohamed; Abd El-Samie, Fathi E.] Menoufia Univ, Dept Elect & Elect Commun Engn, Fac Elect Engn, Menoufia 32952, Egypt.
C3 Egyptian Knowledge Bank (EKB); Menofia University; Durham University;
   Egyptian Knowledge Bank (EKB); Menofia University; Egyptian Knowledge
   Bank (EKB); Menofia University
RP Abdellatef, E (corresponding author), Delta Acad Engn, Elect & Commun Dept, Mansoura, Egypt.
EM essam_abdellatef@yahoo.com; Nabil.Ismail@el-eng.menofia.edu.eg;
   salaheldeen@el-eng.menofia.edu.eg; Khalid.n.ismail@gmail.com;
   mohamed.elmelegy@el-eng.menofia.edu.eg; fathi_sayed@yahoo.com
RI Sayed, Fathi/HRA-4752-2023; Elmeligy, Mohamed/AAW-3702-2020; Ismail,
   Nabil/AAV-4721-2021; Rihan, Mohamed/ACT-2475-2022; Elmeligy, Mohamed
   Rihan Emam/AAB-7907-2022
OI Sayed, Fathi/0000-0001-8749-9518; Ismail, Nabil/0000-0001-7818-2631;
   Elmeligy, Mohamed Rihan Emam/0000-0003-4030-2559; Ismail,
   Khalid/0009-0005-0114-8407
CR Adler A, 2003, CCECE 2003: CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3, PROCEEDINGS, P1163
   An FP, 2020, VISUAL COMPUT, V36, P483, DOI 10.1007/s00371-019-01635-4
   [Anonymous], 2013, P IEEE 6 INT C BIOM
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P IEEE CVPR
   [Anonymous], 0749 U MASS
   [Anonymous], 2012, INT J SOFTW SCI COMP, DOI DOI 10.4018/jssci.2012070102
   [Anonymous], VIS COMPUT
   [Anonymous], COMPUTER
   [Anonymous], 2017, ARXIV171000870
   [Anonymous], 2018, CVPR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, ARXIV150607310
   [Anonymous], 2017, CVPR
   [Anonymous], ARXIV180310758
   [Anonymous], 2018, Arcface: Additive angular margin loss for deep face recognition
   Bedad F, 2019, LECT NOTE NETW SYST, V62, P70, DOI 10.1007/978-3-030-04789-4_7
   Castrillón M, 2007, J VIS COMMUN IMAGE R, V18, P130, DOI 10.1016/j.jvcir.2006.11.004
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   Choi JY, 2018, VISUAL COMPUT, V34, P1535, DOI 10.1007/s00371-017-1429-y
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Feng YC, 2010, IEEE T INF FOREN SEC, V5, P103, DOI 10.1109/TIFS.2009.2038760
   Guan H, 2018, VISUAL COMPUT, V34, P1701, DOI 10.1007/s00371-017-1445-y
   Hasnat A., 2017, IEEE INT CONF COMP V, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin ATB, 2004, PATTERN RECOGN, V37, P2245, DOI 10.1016/j.patcog.2004.04.011
   Kabbai L, 2019, VISUAL COMPUT, V35, P679, DOI 10.1007/s00371-018-1503-0
   Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803
   Kurban OC, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA), P361, DOI 10.1109/INISTA.2017.8001186
   Li K, 2020, VISUAL COMPUT, V36, P391, DOI 10.1007/s00371-019-01627-4
   Li LF, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P238, DOI 10.23919/MVA.2017.7986845
   Li XJ, 2020, VISUAL COMPUT, V36, P39, DOI 10.1007/s00371-018-1582-y
   Liang DD, 2020, VISUAL COMPUT, V36, P499, DOI 10.1007/s00371-019-01636-3
   Lin W, 2017, IEEE ANTENNAS PROP, P547, DOI 10.1109/APUSNCURSINRSM.2017.8072316
   Liu BZ, 2018, VISUAL COMPUT, V34, P707, DOI 10.1007/s00371-017-1408-3
   Liu LG, 2018, DISS MATH, P1
   Liu XX, 2019, VISUAL COMPUT, V35, P445, DOI 10.1007/s00371-018-1566-y
   Liu YJ, 2019, VISUAL COMPUT, V35, P667, DOI 10.1007/s00371-018-1502-1
   Lobachev O, 2018, VISUAL COMPUT, V34, P1749, DOI 10.1007/s00371-017-1466-6
   Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523
   Natgunanathan I, 2018, MULTIMED TOOLS APPL, V77, P6753, DOI 10.1007/s11042-017-4596-y
   Park JK, 2019, VISUAL COMPUT, V35, P1615, DOI 10.1007/s00371-018-1561-3
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004
   Shu XB, 2016, PATTERN RECOGN, V59, P156, DOI 10.1016/j.patcog.2015.12.015
   Sun Y., 2015, Journal of Computational and Graphical Statistics
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Teoh ABJ, 2006, IEEE T PATTERN ANAL, V28, P1892, DOI 10.1109/TPAMI.2006.250
   Tripathi G, 2019, VISUAL COMPUT, V35, P753, DOI 10.1007/s00371-018-1499-5
   Wang YM, 2011, VISUAL COMPUT, V27, P333, DOI 10.1007/s00371-010-0530-2
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Wu X, 2019, 2019 26TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), P284, DOI [10.1109/ict.2019.8798810, 10.1109/ICT.2019.8798810]
   Xiangbo Shu, 2016, MultiMedia Modeling. 22nd International Conference, MMM 2016. Proceedings, P114, DOI 10.1007/978-3-319-27671-7_10
   Yang WC, 2018, J SUPERCOMPUT, V74, P4893, DOI 10.1007/s11227-018-2266-0
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhu XL, 2020, VISUAL COMPUT, V36, P743, DOI 10.1007/s00371-019-01660-3
NR 61
TC 17
Z9 17
U1 2
U2 25
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2020
VL 36
IS 6
BP 1097
EP 1109
DI 10.1007/s00371-019-01715-5
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LI0TO
UT WOS:000529199400002
DA 2024-07-18
ER

PT J
AU Ma, TS
   Tian, WH
AF Ma, Tingsong
   Tian, Wenhong
TI Back-projection-based progressive growing generative adversarial network
   for single image super-resolution
SO VISUAL COMPUTER
LA English
DT Article
DE Back-projection; Progressive growing; Generative adversarial networks;
   Single image super-resolution
AB Recent advanced deep learning studies have shown the positive role of feedback mechanism in image super-resolution task. However, current feedback mechanism only calculates residual errors of images with the same resolution without considering the useful features that may be carried by different resolution features. In this paper, to explore the potential of feedback mechanism, we design a new network structure (progressive up- and downsampling back-projection units) to construct a generative adversarial network for single image super-resolution and use progressive growing methodologies to train it. Unlike previous feedback structure, we use progressively increasing scale factor to build up- and down-projection units, which aims to learn fruitful features across scales. This method allows us to get more meaningful information from early feature maps. Additionally, we train our network progressively; in the process of training, we start from single layer network structure and add new layers as the training goes on. By this mean, the training process can be greatly accelerated and stabilized. Experiments on benchmark dataset with the state-of-the-art methods show that our network achieves 0.01 dB, 0.11 dB, 0.13 dB and 0.4 dB better PSNR results than that of RDN+, MDSR, D-DBPN and EDSR on 8x enlargement, respectively, and also achieves favorable performance against the state-of-the-art methods on 2xand 4x enlargement.
C1 [Ma, Tingsong; Tian, Wenhong] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Tian, WH (corresponding author), Univ Elect Sci & Technol China, Chengdu, Peoples R China.
EM 201811090811@std.uestc.edu.cn; tian_wenhong@uestc.edu.cn
FU National Key Research and Development Program [2018AAA003203]
FX This work was supported partially by National Key Research and
   Development Program (ID 2018AAA003203)
CR [Anonymous], 2016, MULTIMED TOOLS APPL
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Denton E, 2015, ADV NEUR IN, V28
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Gulrajani I., 2017, Advances in neural information processing systems, P5769
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Haris M, 2017, APPL OPTICS, V56, P6043, DOI 10.1364/AO.56.006043
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GL, 2017, IEEE ICC
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   KIM J, 2015, ARXIV151104491V2
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kravitz DJ, 2013, TRENDS COGN SCI, V17, P26, DOI 10.1016/j.tics.2012.10.011
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   LEDIG C, 2016, ARXIV160904802V5
   Li JZ, 2018, C ELECT INSUL DIEL P, P527, DOI 10.1109/CEIDP.2018.8544775
   LI Z, 2019, ARXIV190309814
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   PELEG T, 2014, TIP
   Radford A., 2015, ARXIV
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   ULG E, 2017, IEEE C COMP VIS PATT
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zeyde Roman., 2010, On single image scale‐up using sparse‐representations. In: International Conference on Curves and Surfaces, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2017, LECT NOTES COMPUT SC, V10636, P217, DOI 10.1007/978-3-319-70090-8_23
   Zhang GD, 2017, DESTECH TRANS ENG, P358
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang Kaibing, 2012, TIP
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zheng T, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278092
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 49
TC 15
Z9 16
U1 0
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAY
PY 2021
VL 37
IS 5
BP 925
EP 938
DI 10.1007/s00371-020-01843-3
EA APR 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV2QF
UT WOS:000529574600001
DA 2024-07-18
ER

PT J
AU Amit
   Raman, B
   Sadhya, D
AF Amit
   Raman, Balasubramanian
   Sadhya, Debanjan
TI Dynamic texture recognition using local tetra pattern-three orthogonal
   planes (LTrP-TOP)
SO VISUAL COMPUTER
LA English
DT Article
DE Dynamic texture; Local tetra pattern; Video classification
ID BINARY PATTERNS; CLASSIFICATION; SCHEME; DESCRIPTOR; SCALE; FLOW
AB Dynamic texture (DT) is the annexation of an altering series of images where temporal regularity is present. Characterization, classification and identification of DTs have been investigated by many researchers in the past 2 decades. Especially, several image texture descriptors based on local binary pattern (LBP) have been enhanced for DT classification in the spatiotemporal domain. Local tetra pattern (LTrP) is an extension of LBP in which the calculation of the feature code depends on the referenced central pixel as well as its neighbor's directions. In this work, we have proposed an LTrP-based novel spatiotemporal descriptor for characterization and subsequent identification of textures in videos. We term this descriptor as local tetra patterns on three orthogonal planes (LTrP-TOP). In our proposed work, the video frames are initially divided into three orthogonal planes on the basis of the XY, YT and XT directions. The direction of the center and neighborhood pixels are then detected in vertical and horizontal orientations using first-order derivatives. This sequence of operations computes the LTrP for each center pixel. Subsequently, we create the histogram for a particular plane by considering the LTrP codes for all the pixels in that plane. Finally, the histograms from the three orthogonal planes are concatenated for forming the final texture descriptor of the video. Comprehensive experimental assessments on benchmark DT databases (Dyntex++ and UCLA) demonstrate that the LTrP-TOP descriptor exhibits better performance than most of the other state-of-the-art DT descriptors (in terms of classification accuracy %).
C1 [Amit; Raman, Balasubramanian; Sadhya, Debanjan] Indian Inst Technol Roorkee, Dept Comp Sci & Engn, Roorkee, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee
RP Sadhya, D (corresponding author), Indian Inst Technol Roorkee, Dept Comp Sci & Engn, Roorkee, Uttar Pradesh, India.
EM amit@cs.iitr.ac.in; balarfma@iitr.ac.in; dipu8.cspdf2017@iitr.ac.in
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], ARXIV12013612 CORR
   [Anonymous], AISS
   [Anonymous], TEMPORAL TEXTURE ACT
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Baktashmotlagh M, 2014, IEEE T PATTERN ANAL, V36, P2353, DOI 10.1109/TPAMI.2014.2339851
   Chan AB, 2005, PROC CVPR IEEE, P846
   Chan AB, 2009, PROC CVPR IEEE, P1062, DOI 10.1109/CVPRW.2009.5206556
   Chan AB, 2009, IEEE T PATTERN ANAL, V31, P1862, DOI 10.1109/TPAMI.2009.110
   Chetverikov D, 2005, ADV SOFT COMP, P17
   Danelakis A, 2016, VISUAL COMPUT, V32, P257, DOI 10.1007/s00371-015-1142-7
   Derpanis KG, 2012, IEEE T PATTERN ANAL, V34, P1193, DOI 10.1109/TPAMI.2011.221
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Fazekas S, 2007, SIGNAL PROCESS-IMAGE, V22, P680, DOI 10.1016/j.image.2007.05.013
   Fazekas S, 2007, INT WORK CONTENT MUL, P25, DOI 10.1109/CBMI.2007.385388
   Ghanem B., 2010, LECT NOTES COMPUT SC, P223
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   NELSON RC, 1992, CVGIP-IMAG UNDERSTAN, V56, P78, DOI 10.1016/1049-9660(92)90087-J
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Péteri R, 2005, LECT NOTES COMPUT SC, V3523, P223
   Pietikainen M, 2011, COMPUT IMAGING VIS, V40, P1
   Qiao YL, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/762472
   Qiao YL, 2015, IEEE SIGNAL PROC LET, V22, P509, DOI 10.1109/LSP.2014.2362613
   Ravichandran A, 2013, IEEE T PATTERN ANAL, V35, P342, DOI 10.1109/TPAMI.2012.83
   Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, INT CONF ACOUST SPEE, P2400, DOI 10.1109/ICASSP.2013.6638085
   Ren JF, 2013, IEEE T IMAGE PROCESS, V22, P4049, DOI 10.1109/TIP.2013.2268976
   Saisan P, 2001, PROC CVPR IEEE, P58
   Shrivastava N, 2014, VISUAL COMPUT, V30, P1223, DOI 10.1007/s00371-013-0887-0
   Smith JR, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P437
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tiwari D, 2017, MULTIMED TOOLS APPL, V76, P6623, DOI 10.1007/s11042-016-3362-x
   Tiwari D, 2016, ADV INTELL SYST COMP, V434, P365, DOI 10.1007/978-81-322-2752-6_36
   Tiwari D, 2017, COMPUT ELECTR ENG, V62, P485, DOI 10.1016/j.compeleceng.2016.11.008
   Tiwari D, 2016, COMPUT VIS IMAGE UND, V150, P58, DOI 10.1016/j.cviu.2016.04.010
   Tiwari D, 2016, MULTIDIM SYST SIGN P, V27, P563, DOI 10.1007/s11045-015-0319-6
   Tung T, 2015, VISUAL COMPUT, V31, P311, DOI 10.1007/s00371-014-0925-6
   Wang Y, 2016, SOFT COMPUT, V20, P1977, DOI 10.1007/s00500-015-1618-4
   Wang Y, 2015, NEUROCOMPUTING, V154, P217, DOI 10.1016/j.neucom.2014.12.001
   Xu Y, 2012, COMPUT VIS IMAGE UND, V116, P999, DOI 10.1016/j.cviu.2012.05.003
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao G, 2007, LECT NOTES COMPUT SC, V4358, P165
   Zhao GY, 2012, IEEE T IMAGE PROCESS, V21, P1465, DOI 10.1109/TIP.2011.2175739
NR 48
TC 6
Z9 6
U1 0
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2020
VL 36
IS 3
BP 579
EP 592
DI 10.1007/s00371-019-01643-4
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KL4EO
UT WOS:000513378500010
DA 2024-07-18
ER

PT J
AU Clifford, RMS
   Engelbrecht, H
   Jung, S
   Oliver, H
   Billinghurst, M
   Lindeman, RW
   Hoermann, S
AF Clifford, Rory M. S.
   Engelbrecht, Hendrik
   Jung, Sungchul
   Oliver, Hamish
   Billinghurst, Mark
   Lindeman, Robert W.
   Hoermann, Simon
TI Aerial firefighter radio communication performance in a virtual training
   system: radio communication disruptions simulated in VR for <i>Air
   Attack Supervision</i>
SO VISUAL COMPUTER
LA English
DT Article
DE Virtual reality; Simulation; Aviation; Radio communications;
   Firefighting; CRM
ID AWARENESS; STRESS; REAL
AB Communication disruptions are frequent in aerial firefighting. Information is more easily lost over multiple radio channels, busy with simultaneous conversations. Such a high bandwidth of information throughput creates mental overload. Further problems with hardware or radio signals being disrupted over long distances or by mountainous terrain make it difficult to coordinate firefighting efforts. This creates stressful conditions and requires certain expertise to manage effectively. An experiment was conducted which tested the effects of disrupting users communications equipment and measured their stress levels as well as communication performance. This research investigated how realistic communication disruptions have an effect on behavioural changes in communication frequency, as well as physiological stress by means of measuring heart rate variability (HRV). Broken radio transmissions created a greater degree of stress than background chatter alone. Experts could maintain a more stable HRV during disruptions than novices, which was calculated on the change in HRV during the experiment. From this, we deduce that experts have a better ability to manage stress. We also noted strategies employed by experts such as relaying to overcome the radio challenges, as opposed to novices who would not find a solution, effectively giving up.
C1 [Clifford, Rory M. S.; Engelbrecht, Hendrik; Jung, Sungchul; Lindeman, Robert W.] Univ Canterbury, Human Interface Technol Lab New Zealand, Christchurch, New Zealand.
   [Oliver, Hamish] Univ Canterbury, Sch Mus, Christchurch, New Zealand.
   [Billinghurst, Mark] Univ Auckland, Bioengn Inst, Auckland, New Zealand.
   [Hoermann, Simon] Univ Canterbury, Sch Prod Design, Christchurch, New Zealand.
C3 University of Canterbury; University of Canterbury; University of
   Auckland; University of Canterbury
RP Clifford, RMS (corresponding author), Univ Canterbury, Human Interface Technol Lab New Zealand, Christchurch, New Zealand.
EM rory.clifford@pg.canterbury.ac.nz;
   hendrik.engelbrecht@pg.canterbury.ac.nz; sungchul.jung@canterbury.ac.nz;
   hamish.oliver@canterbury.ac.nz; mark.billinghurst@auckland.ac.nz;
   gogo@hitlabnz.org; simon.hoermann@canterbury.ac.nz
RI Lindeman, Rob/AAG-8857-2020; Billinghurst, Mark/AAJ-4236-2020; Hoermann,
   Simon/AAT-2334-2021
OI Lindeman, Rob/0000-0002-0637-7701; Billinghurst,
   Mark/0000-0003-4172-6759; Hoermann, Simon/0000-0002-4201-844X;
   Engelbrecht, Hendrik/0000-0002-4185-6476; Jung,
   Sungchul/0000-0003-3633-7767; Clifford, Rory/0000-0001-6972-0303
FU Ngai Tahu Research Centre; FENZ
FX We would like to thank Richard McNamara a.k.a Mac and the Air Attack
   cohort from Fire and Emergency New Zealand (FENZ) for their valued time
   and input. We want to thank the Ngai Tahu Research Centre as well as
   FENZ for providing funding for this research project.
CR [Anonymous], 2018, 2018 10 INT C VIRT W, DOI DOI 10.1109/VS-GAMES.2018.8493423
   Baumann MR, 2011, HUM FACTORS, V53, P548, DOI 10.1177/0018720811418224
   Bertram J, 2015, COMPUT HUM BEHAV, V43, P284, DOI 10.1016/j.chb.2014.10.032
   Bouchard S., VIRTUAL REALITY TRAI, P109, DOI [10.1007/978-3-642-17824-5_6, DOI 10.1007/978-3-642-17824-5_6]
   Bronkhorst AW, 2000, ACUSTICA, V86, P117
   Endsley Mica R, 2016, DESIGNING SITUATION, DOI DOI 10.1201/9780203485088
   Engelbrecht H, 2019, FRONT ROBOT AI, V6, DOI 10.3389/frobt.2019.00101
   GAILLARD AWK, 1993, ERGONOMICS, V36, P991, DOI 10.1080/00140139308967972
   Harvey C, 2021, VISUAL COMPUT, V37, P3, DOI 10.1007/s00371-019-01702-w
   Klein G, 2008, HUM FACTORS, V50, P456, DOI 10.1518/001872008X288385
   Lahtinen TMM, 2010, AVIAT SPACE ENVIR MD, V81, P1123, DOI 10.3357/ASEM.2468.2010
   Lee AT, 2017, FLIGHT SIMULATION
   Lorby-SI, 2019, FIRE FIGHTER XADDON
   Martin L, 2019, PREPAR3D FLIGHT SIMU
   Matthews G., 2013, Psychology of Stress, P49, DOI DOI 10.1016/J.FOODRES.2014.12.027
   McClernon C.K, 2009, TECHNICAL REPORT
   Michie S, 2002, OCCUP ENVIRON MED, V59, P67, DOI 10.1136/oem.59.1.67
   Militello L. G., 2007, Cognition, Technology & Work, V9, P25, DOI 10.1007/s10111-006-0059-3
   Mogford RH, 1997, INT J AVIAT PSYCHOL, V7, P331, DOI 10.1207/s15327108ijap0704_5
   Nazari G, 2018, BMC SPORTS SCI MED R, V10, DOI 10.1186/s13102-018-0094-4
   O'Brien KS, 2007, ERGONOMICS, V50, P1064, DOI 10.1080/00140130701276640
   Petruzzello SJ, 2009, ERGONOMICS, V52, P747, DOI 10.1080/00140130802550216
   Regenbrecht H, 2002, PRESENCE-TELEOP VIRT, V11, P425, DOI 10.1162/105474602760204318
   Samel A, 2004, AVIAT SPACE ENVIR MD, V75, P935
   Saus ER, 2006, MIL PSYCHOL, V18, pS3, DOI 10.1207/s15327876mp1803s_2
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sexton J B, 2000, Hum Perf Extrem Environ, V5, P63
   SILLARS A, 1982, COMMUN RES, V9, P201, DOI 10.1177/009365082009002002
   SimpitTechnologies, 2019, SIMPIT 270 CENT
   Smith DL, 2014, J OCCUP ENVIRON HYG, V11, P427, DOI 10.1080/15459624.2013.875184
   Sound E, 2019, Q10B TACTILE TRANSDU
   Stanney K. M., 2014, HDB VIRTUAL ENV DESI
   Tati B., 23 NAT C 4 INT C NOI, P249
   Updegrove J., 2017, 2017 IEEE/AIAA 36th digital avionics systems conference (DASC), P1, DOI [10.1109/DASC.2017.8102129, DOI 10.1109/DASC.2017.8102129]
   Varone BYC, 2012, FIREFIGHTER SAFETY R, V156, P1
   Vescoukis V, 2012, FUTURE GENER COMP SY, V28, P593, DOI 10.1016/j.future.2011.03.010
   Wiederhold B.K., 2008, Journal of CyberTherapy Rehabilitation, V1, P23
   Wiederhold BK, 2002, CYBERPSYCHOL BEHAV, V5, P77, DOI 10.1089/109493102753685908
   Zephyr, 2019, ZEPH BIOP BIOH
NR 39
TC 12
Z9 13
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2021
VL 37
IS 1
BP 63
EP 76
DI 10.1007/s00371-020-01816-6
EA FEB 2020
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QE3UO
UT WOS:000516496400001
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Duan, QT
   Shi, S
   Zhao, P
AF Liu, Zhengyi
   Duan, Quntao
   Shi, Song
   Zhao, Peng
TI Multi-level progressive parallel attention guided salient object
   detection for RGB-D images
SO VISUAL COMPUTER
LA English
DT Article
DE Salient object detection; RGB-D image; Attention mechanism; Recurrent
   convolutional layer
ID NETWORK; FUSION
AB Detecting salient objects in RGB-D images attracts more and more attention in recent years. It benefits from the widespread use of depth sensors and can be applied in the comprehensive understanding of RGB-D images. Existing models focus on double-stream networks which transfer from color stream to depth stream, but depth stream with one channel information cannot learn the same feature as color stream with three channels information even if HHA representation is adopted. In our works, RGB-D four-channels input is chosen, and meanwhile, progressive parallel spatial and channel attention mechanisms are performed to improve feature representation. Spatial and channel attention can pay more attention on partial positions and channels in the image which show higher response to salient objects. Both attentive features are optimized by attentive feature from higher layer, respectively, and parallel fed into recurrent convolutional layer to generate side-output saliency maps guided by saliency map from higher layer. Last multi-level saliency maps are fused together from multi-scale perspective. Experiments on benchmark datasets demonstrate that parallel attention mechanism and progressive optimization operation play an important role in improving the accuracy of salient object detection, and our model outperforms state-of-the-art models in evaluation matrices.
C1 [Liu, Zhengyi; Duan, Quntao; Shi, Song; Zhao, Peng] Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei, Peoples R China.
C3 Anhui University
RP Liu, ZY (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei, Peoples R China.
EM liuzywen@ahu.edu.cn; 984037793@qq.com; 1271070920@qq.com;
   18868519@qq.com
RI Liu, Zhengyi/AAB-6589-2022
OI Liu, Zhengyi/0000-0003-3265-823X
FU National Natural Science Foundation of China [61602004]; Natural Science
   Foundation of Anhui Province [1908085MF182]; Key Program of Natural
   Science Project of Educational Commission of Anhui Province
   [KJ2019A0034]
FX We thank Dr. Hao Chen from City University of Hong Kong for providing
   their result saliency maps. We also thank Prof. Ming-ming Cheng and Dr.
   Deng-ping Fan from Nankai University for providing the code of the
   evaluation metrics. We thank all anonymous reviewers for their valuable
   comments. This research is supported by National Natural Science
   Foundation of China (61602004), Natural Science Foundation of Anhui
   Province (1908085MF182) and Key Program of Natural Science Project of
   Educational Commission of Anhui Province (KJ2019A0034).
CR [Anonymous], 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, DOI DOI 10.1109/CVPR.2016.257
   [Anonymous], 2019, P IEEE C COMP VIS PA
   CHEN H, ARXIV170300122
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen H, 2017, IEEE INT C INT ROBOT, P4911, DOI 10.1109/IROS.2017.8206370
   Chen H, 2017, LECT NOTES COMPUT SC, V10528, P459, DOI 10.1007/978-3-319-68345-4_41
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   CHEN S, 2018, IEEE T CYBERN
   CHEN S, ARXIV180709940
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   CHU X, ARXIV170207432, V1
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Du D., 2018, 2018 IEEE International Conference on Multimedia and Expo (ICME), P1
   Fan D-P, ARXIV180510421
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   FAN DP, ARXIV190706781
   Guo J., 2016, P ANN REL MAINT S, P1, DOI DOI 10.1109/RAMS.2016.7448068
   Guo J, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING ICISCE 2015, P59, DOI 10.1109/ICISCE.2015.22
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Han J, 2018, INT J CONTROL AUTOM, V16, P522, DOI 10.1007/s12555-016-0338-6
   HUANG P, 2018, 2018 IEEE 23 INT C D, P1, DOI DOI 10.1109/ICDSP.2018.8631584
   Jiang LX, 2015, IEEE INT CONF ROBOT, P1323, DOI 10.1109/ICRA.2015.7139362
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Li SZ, 2015, IEEE IMAGE PROC, P4609, DOI 10.1109/ICIP.2015.7351680
   LIANG M, 2015, PROC CVPR IEEE, P3367, DOI [10.1109/CVPR.2015.7298958, DOI 10.1109/CVPR.2015.7298958]
   Lin HY, 2017, J VIS COMMUN IMAGE R, V48, P238, DOI 10.1016/j.jvcir.2017.06.011
   LIU N, ARXIV170806433
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XH, 2017, AAAI CONF ARTIF INTE, P4271
   Sun FD, 2019, MULTIMED TOOLS APPL, V78, P30793, DOI 10.1007/s11042-018-6591-3
   WANG F, ARXIV170406904
   Wang SH, 2020, EXPERT SYST, V37, DOI 10.1111/exsy.12272
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu PL, 2015, COMM COM INF SC, V547, P359, DOI 10.1007/978-3-662-48570-5_35
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   ZHANG P, ARXIV180206960
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   ZHU C, ARXIV180308636
   ZHU C, ARXIV180505132
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 51
TC 14
Z9 14
U1 1
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 529
EP 540
DI 10.1007/s00371-020-01821-9
EA FEB 2020
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000516171600001
DA 2024-07-18
ER

PT J
AU Wang, YQ
   Deng, L
   Yang, ZG
   Zhao, D
   Wang, F
AF Wang, Yueqing
   Deng, Liang
   Yang, Zhigong
   Zhao, Dan
   Wang, Fang
TI A rapid vortex identification method using fully convolutional
   segmentation network
SO VISUAL COMPUTER
LA English
DT Article
DE Vortex identification; Convolutional neural network; Segmentation;
   Scientific visualization
AB Vortex identification methods have been extensively studied in recent years due to their importance in understanding the potential physical mechanism of the flow field. Although demonstrating great success in various scenarios, these methods cannot achieve a compromise between computational speed and accuracy, which restricts their usage in large-scale applications. In specific, local methods provide results rapidly with pool accuracies. By contrast, global methods can obtain reliable results by consuming much more time. To take the advantages of both local and global methods, several methods based on convolutional neural networks are proposed. These methods use local patches around each point and the labels obtained by global methods to train the network. They convert the vortex identification tasks into binary classification problems. In this manner, these methods detect vortices rapidly and robustly. By revisiting these methods, we observe two drawbacks that limit their performance: (i) the large number of parameters and (ii) high computational complexity. To address these issues, we provide a rapid vortex identification method by using a fully convolutional segmentation network in this work. Specifically, we discard the fully connected layers to decrease the number of parameters and design a segmentation network to reduce computational complexity. Intensive experimental results show that the accuracy and recall performance of our method are comparable with those of the global methods. Moreover, the time consumption of our method is less than that of all other methods.
C1 [Wang, Yueqing; Deng, Liang; Yang, Zhigong; Zhao, Dan; Wang, Fang] China Aerodynam Res & Dev Ctr, Computat Aerodynam Inst, Mianyang, Sichuan, Peoples R China.
   [Deng, Liang] Natl Univ Def Technol, Coll Comp, Changsha, Peoples R China.
C3 National University of Defense Technology - China
RP Wang, F (corresponding author), China Aerodynam Res & Dev Ctr, Computat Aerodynam Inst, Mianyang, Sichuan, Peoples R China.
EM wangfangcardc1976@163.com
RI Deng, Liang/GXH-6710-2022
OI Wang, Fang/0000-0002-8259-8557
FU National Natural Science Foundation of China [61806205]
FX This work was supported by the National Natural Science Foundation of
   China (61806205).
CR Abbasi A, 2019, VISUAL COMPUT, V35, P271, DOI 10.1007/s00371-018-1586-7
   [Anonymous], 1995, P 3 INT C DOCUMENT A, DOI DOI 10.1109/ICDAR.1995.598994
   [Anonymous], 2018, INT J PERFORMABILITY
   [Anonymous], 2017, P 55 AIAA AER SCI M
   Biswas A, 2015, IEEE PAC VIS S, P1, DOI [10.1109/VISUAL.1998.745333, DOI 10.1109/PACIFICVIS.2015.715638]
   CHONG MS, 1990, PHYS FLUIDS A-FLUID, V2, P765, DOI 10.1063/1.857730
   Deng L, 2019, J VISUAL-JAPAN, V22, P65, DOI 10.1007/s12650-018-0523-1
   Franz K, 2018, INT GEOSCI REMOTE SE, P6887, DOI 10.1109/IGARSS.2018.8519261
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Günther T, 2018, COMPUT GRAPH FORUM, V37, P149, DOI 10.1111/cgf.13319
   Haller G, 2016, J FLUID MECH, V795, P136, DOI 10.1017/jfm.2016.151
   HUNT JCR, 1987, T CAN SOC MECH ENG, V11, P21, DOI 10.1139/tcsme-1987-0004
   JEONG J, 1995, J FLUID MECH, V285, P69, DOI 10.1017/S0022112095000462
   Kim B., 1903, ARXIV190310255
   Lguensat R., 2017, COMPUT VIS PATTERN R
   Lguensat R, 2018, INT GEOSCI REMOTE SE, P1764, DOI 10.1109/IGARSS.2018.8518411
   Liu SX, 2018, IEEE T VIS COMPUT GR, V24, P163, DOI 10.1109/TVCG.2017.2744378
   Rajendran Vinod., 2018, 2018 FLUID DYN C, DOI 10.2514/6.2018-3724
   Sadarjoen A, 2002, P VIS, P419, DOI [10.1109/VISUAL.1998.745333, DOI 10.1109/VISUAL.1998.745333]
   Schafhitzel T, 2008, COMPUT GRAPH FORUM, V27, P1023, DOI 10.1111/j.1467-8659.2008.01238.x
   Serra M., 2016, P ROY SOC A-MATH PHY, V473, P1, DOI [10.1098/rspa.2016.0807, DOI 10.1098/RSPA.2016.0807]
   Serra M, 2016, CHAOS, V26, DOI 10.1063/1.4951720
   Shu ZY, 2019, IEEE T VIS COMPUT GR, V25, P2583, DOI 10.1109/TVCG.2018.2848628
   Simard PY, 2003, PROC INT CONF DOC, P958
   Strofer C.M., 2018, FLUID DYN
   Ströfer CM, 2019, COMMUN COMPUT PHYS, V25, P625, DOI 10.4208/cicp.OA-2018-0035
   Xu XJ, 2019, IEEE T VIS COMPUT GR, V25, P2336, DOI 10.1109/TVCG.2018.2839685
   Xue CD, 2016, SCI CHINA PHYS MECH, V59, DOI 10.1007/s11433-016-0115-1
   Zhang L, 2014, COMPUT GRAPH FORUM, V33, P282, DOI 10.1111/cgf.12275
NR 29
TC 15
Z9 17
U1 2
U2 19
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2021
VL 37
IS 2
BP 261
EP 273
DI 10.1007/s00371-020-01797-6
EA JAN 2020
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR4KJ
UT WOS:000510084800001
DA 2024-07-18
ER

PT J
AU Sobczak, S
   Kapela, R
   McGuinness, K
   Swietlicka, A
   Pazderski, D
   O'Connor, NE
AF Sobczak, Szymon
   Kapela, Rafal
   McGuinness, Kevin
   Swietlicka, Aleksandra
   Pazderski, Dariusz
   O'Connor, Noel E.
TI Restricted Boltzmann machine as an aggregation technique for binary
   descriptors
SO VISUAL COMPUTER
LA English
DT Article
DE Restricted Boltzmann machine; Image local binary descriptors;
   Aggregation techniques of feature vectors
AB The article presents a novel approach to the challenge of real-time image classification with deep neural networks. The proposed architecture of the neural network exploits computationally efficient local binary descriptors and uses a restricted Boltzmann machine (RBM) as a feature space projection step so that the resulting depth of the deep neural network can be reduced. A contrastive divergence procedure is used both for RBM training and for feature projection. The resulting neural networks exhibit performance close to the current state-of-the-art but are characterized by a small model memory footprint (i.e., number of parameters) and extremely efficient computational complexity (i.e., response time). The low number of parameters makes these architectures applicable in embedded systems with limited memory or reduced computational capabilities.
C1 [Sobczak, Szymon; Kapela, Rafal; Pazderski, Dariusz] Poznan Univ Tech, Poznan, Poland.
   [Swietlicka, Aleksandra] Poznan Univ Tech, Inst Control & Robot, Poznan, Turkey.
   [Pazderski, Dariusz] Poznan Univ Tech, Inst Automat & Robot, Poznan, Turkey.
   [McGuinness, Kevin; O'Connor, Noel E.] Dublin City Univ, Insight Ctr Data Analyt, Dublin, Ireland.
C3 Poznan University of Technology; Poznan University of Technology; Poznan
   University of Technology; Dublin City University
RP Kapela, R (corresponding author), Poznan Univ Tech, Poznan, Poland.
EM szymon.k.sobczak@doctorate.put.poznan.pl; rafal.kapela@put.poznan.pl;
   Kevin.McGuinness@insight-centre.org;
   Aleksandra.Swietlicka@put.poznan.pl; Dariusz.Pazderski@put.poznan.pl;
   Noel.E.OConnor@insight-centre.org
RI Świetlicka, Aleksandra/GVS-5730-2022; McGuinness, Kevin/V-2424-2019;
   Świetlicka, Aleksandra/M-1989-2014; Pazderski, Dariusz/M-1487-2014;
   Kapela, Rafal/M-8202-2014
OI Świetlicka, Aleksandra/0000-0002-2576-7625; McGuinness,
   Kevin/0000-0003-1336-6477; Świetlicka, Aleksandra/0000-0002-2576-7625;
   Kapela, Rafal/0000-0002-0624-7608
CR Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   [Anonymous], 2014, ARXIV14064729 CORR
   [Anonymous], 2018, DO BETTER IMAGENET M
   [Anonymous], 2016, P IEEE INT S CIRC SY
   [Anonymous], CNN BENCHM
   [Anonymous], 2017, EFFECTIVE CONVOLUTIO
   [Anonymous], 2015, PROC IEEE C COMPUT V
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chatoux H, 2016, INT C PATT RECOG, P1988, DOI 10.1109/ICPR.2016.7899928
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061
   Perronnin F., 2007, P IEEE CVPR, P1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
NR 27
TC 3
Z9 3
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD MAR
PY 2021
VL 37
IS 3
BP 423
EP 432
DI 10.1007/s00371-019-01782-8
EA DEC 2019
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QY5QT
UT WOS:000541645000001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ricks, B
   Dobson, A
   Krontiris, A
   Bekris, K
   Kapadia, M
   Roberts, F
AF Ricks, Brian
   Dobson, Andrew
   Krontiris, Athanasios
   Bekris, Kostas
   Kapadia, Mubbasir
   Roberts, Fred
TI Generation of crowd arrival and destination locations/times in complex
   transit facilities
SO VISUAL COMPUTER
LA English
DT Article
DE Crowd simulation; Crowd generation; Building simulation
ID MODEL
AB In order to simulate virtual agents in the replica of a real facility across a long time span, a crowd simulation engine needs a list of agent arrival and destination locations and times that reflect those seen in the actual facility. Working together with a major metropolitan transportation authority, we propose a specification that can be used to procedurally generate this information. This specification is both uniquely compact and expressive-compact enough to mirror the mental model of building managers and expressive enough to handle the wide variety of crowds seen in real urban environments. We also propose a procedural algorithm for generating tens of thousands of high-level agent paths from this specification. This algorithm allows our specification to be used with traditional crowd simulation obstacle avoidance algorithms while still maintaining the realism required for the complex, real-world simulations of a transit facility. Our evaluation with industry professionals shows that our approach is intuitive and provides controls at the right level of detail to be used in large facilities (200,000+ people/day).
C1 [Ricks, Brian] Univ Nebraska, 6001 Dodge St, Omaha, NE 68182 USA.
   [Dobson, Andrew] Calif Dept Healthcare Serv, Sacramento, CA USA.
   [Krontiris, Athanasios] Samsung Semicond Inc, San Jose, CA USA.
   [Bekris, Kostas; Kapadia, Mubbasir; Roberts, Fred] Rutgers State Univ, Piscataway, NJ USA.
C3 University of Nebraska System; Samsung Electronics; Samsung
   Semiconductor (SSI); Rutgers University System; Rutgers University New
   Brunswick
RP Ricks, B (corresponding author), Univ Nebraska, 6001 Dodge St, Omaha, NE 68182 USA.
EM bricks@unomaha.edu
RI Ricks, Brian/AAG-9889-2020
OI Roberts, Fred/0000-0001-8421-4759
FU NSF [1718139]; DHS [2009-ST-061-CCI002-06]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1718139]
   Funding Source: National Science Foundation
FX This study was funded in part by NSF #1718139 and by DHS
   #2009-ST-061-CCI002-06 to Rutgers University. All authors were in a
   contractual agreement with the transit facility mentioned that supported
   the researchers directly or funded students.
CR [Anonymous], 2019, Maya
   Bentley, 2019, LEG SOFTW
   de Paiva DC, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P221
   Feng T, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925894
   Fiorini P, 1998, INT J ROBOT RES, V17, P760, DOI 10.1177/027836499801700706
   Guanghui L, 2016, ACM TRANS MODEL COMP, V26, P1
   He GQ, 2016, FRONT INFORM TECH EL, V17, P200, DOI 10.1631/FITEE.1500253
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Huang WJ, 2020, IEEE T VIS COMPUT GR, V26, P1502, DOI 10.1109/TVCG.2018.2874050
   Jordao K, 2014, COMPUT GRAPH FORUM, V33, P351, DOI 10.1111/cgf.12316
   Jordao K., 2015, P 8 ACM SIGGRAPH C M, P167
   Jorgensen C.-J., 2014, Tech. Rep.
   JOrgensen C.-J, 2015, THESIS U RENNES 1, P1
   Kallmann M, 1999, SPRING COMP SCI, P73
   Kapadia M, 2011, IEEE COMPUT GRAPH, V31, P45, DOI 10.1109/MCG.2011.68
   Kimmel Andrew, 2012, Simulation, Modeling, and Programming for Autonomous Robots. Proceedings of the Third International Conference, SIMPAR 2012, P137, DOI 10.1007/978-3-642-34327-8_15
   Krontiris A., 2016, P 29 INT C COMPUTER, P61
   MUBBASIR KAPADIA., 2015, SYNTHESIS LECT VISUA, V7, P1
   Pelechano N, 2017, CROWDS INTERACTIVE B
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Ricks BC, 2014, IEEE T VIS COMPUT GR, V20, P159, DOI 10.1109/TVCG.2013.110
   Rogla Pujalt O., 2018, CORR
   Schuerman M, 2010, COMPUT ANIMAT VIRT W, V21, P267, DOI 10.1002/cav.367
   Shao W., 2005, SCA 05 P 2005 ACM SI, P19, DOI DOI 10.1145/1073368.1073371
   SideFx, 2019, HOUD
   Testa E, 2019, VISUAL COMPUT, V35, P1119, DOI 10.1007/s00371-019-01684-9
   Thalmann Daniel., 2007, CROWD SIMULATION
   van den Berg J, 2008, IEEE INT CONF ROBOT, P1928, DOI 10.1109/ROBOT.2008.4543489
   Weizi Li, 2011, Motion in Games. Proceedings 4th International Conference, MIG 2011, P132, DOI 10.1007/978-3-642-25090-3_12
   Wolinski D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982442
   Yersin B., 2009, SI3D
NR 31
TC 0
Z9 0
U1 2
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD AUG
PY 2020
VL 36
IS 8
BP 1651
EP 1661
DI 10.1007/s00371-019-01761-z
EA OCT 2019
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ML7DA
UT WOS:000489922800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Abraham, F
   Celes, W
AF Abraham, Frederico
   Celes, Waldemar
TI Multiresolution visualization of massive black oil reservoir models
SO VISUAL COMPUTER
LA English
DT Article
DE Real-time rendering; Reservoir model rendering; Mesh simplification
   algorithm; Multiresolution rendering; Massive model visualization
AB Recent advances in parallel architectures for numerical simulation of natural black oil reservoirs have allowed the use of very discretized domains. As a consequence, these simulations produce an unprecedented volume of data, which must be visualized in 3D environments for careful analysis and inspection. Conventional scientific visualization techniques are not viable on such large models, creating a demand for the development of scalable visualization solutions. In this paper, we propose a hierarchical multiresolution technique to render massively large black oil reservoir meshes. A new simplification algorithm specialized for such models is presented, which accurately represents boundary surfaces, while keeping the hexahedral mesh with good quality. Original model properties, wireframe and surface normals are mapped onto the simplified meshes through texture mapping. This allows the system to reuse the structure for different simulations that use the same geometry model. The viewer application is designed to guarantee a minimum refresh rate, allocating geometric detail where it is most needed, given the available hardware. Experimental results, considering up to 1.2 billion cell models, demonstrate the effectiveness of the proposed solution.
C1 [Abraham, Frederico; Celes, Waldemar] Pontifical Catholic Univ Rio de Janeiro, Dept Comp Sci, Tecgraf PUC Rio Inst, Rua Marques de Sao Vicente 225, Rio De Janeiro, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro
RP Abraham, F (corresponding author), Pontifical Catholic Univ Rio de Janeiro, Dept Comp Sci, Tecgraf PUC Rio Inst, Rua Marques de Sao Vicente 225, Rio De Janeiro, Brazil.
EM devotion97@gmail.com; celes@tecgraf.puc-rio.br
OI Rodrigues Abraham, Frederico/0000-0002-5143-4971
FU Petrobras; CNPq (Brazilian National Research and Development council)
FX Tecgraf/PUC-Rio is a research institute mainly funded by Petrobras. This
   research was initiated during the Doctoral Program of the first author,
   financially supported by CNPq (Brazilian National Research and
   Development council).
CR Abraham F., 2009, EGPGV, P87
   [Anonymous], 2018, CORR
   Baptista R, HIGHER ACCURACY QUAN
   BLACKER TD, 1991, INT J NUMER METH ENG, V32, P811, DOI 10.1002/nme.1620320410
   Celes W, 2011, VISUAL COMPUT, V27, P939, DOI 10.1007/s00371-011-0623-6
   Cignoni P, 2004, ACM T GRAPHIC, V23, P796, DOI 10.1145/1015706.1015802
   Dake L. P., 1983, Fundamentals of Reservoir Engineering
   Daniels J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409101
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Lefebvre S, 2006, ACM T GRAPHIC, V25, P579, DOI 10.1145/1141911.1141926
   Lindstrom P., 2003, PROC SYMPOS INTERACT, P93
   Oberhumer Markus., LZO DATA COMPRESSION
   Shepherd JF, 2010, FINITE ELEM ANAL DES, V46, P17, DOI 10.1016/j.finel.2009.06.024
   SHIRMAN LA, 1993, EUROGRAPHICS, P261
   Sousa MC, 2015, GEOL SOC SPEC PUBL, V406, P447, DOI 10.1144/SP406.17
   Staten M.L., 1997, Trends in Unstructured Mesh Generation, ASME, V220, P9
   Staten ML, 2008, ENG COMPUT-GERMANY, V24, P241, DOI 10.1007/s00366-008-0097-y
   Tarini M, 2010, COMPUT GRAPH FORUM, V29, P407, DOI 10.1111/j.1467-8659.2009.01610.x
   Weiss K, 2009, COMPUT GRAPH FORUM, V28, P1289, DOI 10.1111/j.1467-8659.2009.01506.x
   Wu JH, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P241
NR 20
TC 1
Z9 1
U1 0
U2 1
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JUN
PY 2019
VL 35
IS 6-8
SI SI
BP 837
EP 848
DI 10.1007/s00371-019-01674-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC1IH
UT WOS:000470712200006
OA Bronze
DA 2024-07-18
ER

PT J
AU Chu, YJ
   Zhao, LD
   Ahmad, T
AF Chu, Yongjie
   Zhao, Lindu
   Ahmad, Touqeer
TI Multiple feature subspaces analysis for single sample per person face
   recognition
SO VISUAL COMPUTER
LA English
DT Article
DE Single sample per person; Facial symmetry; Multiple feature subspaces
   analysis; Patch-based method
ID ONE TRAINING IMAGE; 2-DIMENSIONAL PCA; REPRESENTATION; ROBUST; FLDA;
   POSE
AB Collecting samples is one of the main difficulties for face recognition, for example, in most of the real-world applications such as law enhancement, e-passport, and ID card identification, it is customary to collect a single sample per person (SSPP). Unfortunately, in such SSPP scenario, many presented face recognition methods suffer serious performance drop or fail due to their inability to learn the discriminative information of a person from a single sample. To address the SSPP problem, in this paper, we propose a multiple feature subspaces analysis (MFSA) approach, which takes advantage of facial symmetry. First, we divide each enrolled face into two halves about the bilateral symmetry axis and further partition every half into several local face patches. Second, we cluster all the patches into multiple groups according to their locations at the half face and formulate SSPP as a MFSA problem by learning a feature subspace for each group, so that the confusion between inter-class and intra-class variations of face patches is removed and more discriminative features can be extracted from each subspace. To recognize a target person, a k-NN classifier is employed in each subspace to predict the label of a face patch and majority voting strategy is used to identify the unlabeled subject. Compared with the state-of-the-art methods, MFSA is effortless and efficient in implementing, but achieves either better or competitive performance when recognizing face images taken in both constrained and unconstrained environment.
C1 [Chu, Yongjie; Zhao, Lindu] Southeast Univ, Inst Syst Engn, Nanjing 211096, Jiangsu, Peoples R China.
   [Chu, Yongjie; Ahmad, Touqeer] Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89557 USA.
C3 Southeast University - China; Nevada System of Higher Education (NSHE);
   University of Nevada Reno
RP Zhao, LD (corresponding author), Southeast Univ, Inst Syst Engn, Nanjing 211096, Jiangsu, Peoples R China.
EM yongjiechu@yeah.net; ldzhao@seu.edu.cn
RI Chu, Yongjie/S-1218-2019; Ahmad, Touqeer/AAV-2682-2021
OI Chu, Yongjie/0000-0001-6859-5731; Ahmad, Touqeer/0000-0001-9494-7585
FU National Natural Science Foundation of China [71390333]; National Key
   Technology R&D Program of China [2013BAD19B05]; Special Fund for Basic
   Research in Central University; Fund for Graduate Research Project of
   Jiangsu Province [KYZZ15_0072]; Study Abroad Program for Graduate
   Studies by China Scholarship Council
FX This work was partially supported by the National Natural Science
   Foundation of China (No. 71390333), the National Key Technology R&D
   Program of China during the 12th Five-Year Plan Period (No.
   2013BAD19B05), the Special Fund for Basic Research in Central
   University, i.e., Fund for Graduate Research Project of Jiangsu Province
   (KYZZ15_0072), and the Study Abroad Program for Graduate Studies by
   China Scholarship Council.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Benavente R, 1998, 24 COMP VIS CTR
   Chen SC, 2004, PATTERN RECOGN LETT, V25, P1173, DOI 10.1016/j.patrec.2004.03.012
   Chen SC, 2004, PATTERN RECOGN, V37, P1553, DOI 10.1016/j.patcog.2003.12.010
   Chen WP, 2010, LECT NOTES COMPUT SC, V6313, P496
   Cheng Y, 2017, VISUAL COMPUT, V33, P1483, DOI 10.1007/s00371-017-1357-x
   Cox D., 2011, 2011 IEEE INT C AUT
   De Marsico M, 2013, IEEE T SYST MAN CY-S, V43, P149, DOI 10.1109/TSMCA.2012.2192427
   Deng W., 2005, ANAL MODELING FACES
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Deng WH, 2010, PATTERN RECOGN, V43, P1748, DOI 10.1016/j.patcog.2009.12.004
   Gao QX, 2008, APPL MATH COMPUT, V205, P726, DOI 10.1016/j.amc.2008.05.019
   Gao SH, 2015, INT J COMPUT VISION, V111, P365, DOI 10.1007/s11263-014-0750-4
   González-Jiménez D, 2007, IEEE T INF FOREN SEC, V2, P413, DOI 10.1109/TIFS.2007.903543
   Gottumukkal R, 2004, PATTERN RECOGN LETT, V25, P429, DOI 10.1016/j.patrec.2003.11.005
   Harguess J., 2009, IEEE COMP SOC C COMP
   Harguess J, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Heisele B., 2001, 8 IEEE INT C COMP VI
   Huang Gary B, 2008, WORKSH FAC REAL IM D
   Kakadiaris IA, 2007, IEEE T PATTERN ANAL, V29, P640, DOI 10.1109/TPAMI.2007.1017
   Kan M., 2011, BMVC
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Li ZM, 2016, IEEE T INF FOREN SEC, V11, P2203, DOI 10.1109/TIFS.2016.2567318
   Liu C., 2001, 8 IEEE INT C COMP VI
   Liu F, 2016, INFORM SCIENCES, V346, P198, DOI 10.1016/j.ins.2016.02.001
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Pang YW, 2014, IEEE T NEUR NET LEAR, V25, P2191, DOI 10.1109/TNNLS.2014.2306844
   Passalis G, 2011, IEEE T PATTERN ANAL, V33, P1938, DOI 10.1109/TPAMI.2011.49
   Pathangay V., 2008, 8 IEEE INT C AUT FAC
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Saha S., 2007, P IEEE WIE NAT S EM
   Shan Shiguang, 2002, 2002 IEEE INT S CIRC, V2
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Su Y, 2010, COMPUTER VISION PATT
   Tan XY, 2005, IEEE T NEURAL NETWOR, V16, P875, DOI 10.1109/TNN.2005.849817
   Turk M. A., 1991, P CVPR91 IEEE COMP S
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wang B, 2012, IEICE T INF SYST, VE95D, P853, DOI 10.1587/transinf.E95.D.853
   Wei CP, 2015, IEEE INT CON MULTI
   Wei X, 2013, 2013 10 IEEE INT C W, P1
   Wei X., 2013, P IEEE C COMP VIS PA
   Wei XJ, 2014, IEEE T INF FOREN SEC, V9, P2035, DOI 10.1109/TIFS.2014.2359632
   Weng RL, 2013, IEEE I CONF COMP VIS, P601, DOI 10.1109/ICCV.2013.80
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu JX, 2002, PATTERN RECOGN LETT, V23, P1711, DOI 10.1016/S0167-8655(02)00134-4
   Yan HB, 2014, NEUROCOMPUTING, V143, P134, DOI 10.1016/j.neucom.2014.06.012
   Yan SC, 2007, IEEE T INF FOREN SEC, V2, P69, DOI 10.1109/TIFS.2006.890313
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang M, 2011, 2011 IEEE C COMP VIS
   Yang M, 2013, IEEE I CONF COMP VIS, P689, DOI 10.1109/ICCV.2013.91
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Yin F, 2014, PATTERN RECOGN, V47, P1547, DOI 10.1016/j.patcog.2013.09.013
   Yu YF, 2017, PATTERN RECOGN, V66, P302, DOI 10.1016/j.patcog.2017.01.021
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
   Zhang DQ, 2005, APPL MATH COMPUT, V163, P895, DOI 10.1016/j.amc.2004.04.016
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang PY, 2016, PATTERN RECOGN, V52, P249, DOI 10.1016/j.patcog.2015.09.024
   Zhao L, 2016, VISUAL COMPUT, V32, P1165, DOI 10.1007/s00371-015-1169-9
   Zhou Z., 2009, ICCV
   Zhu P., 2014, P AS C COMP VIS
   Zhu PF, 2012, LECT NOTES COMPUT SC, V7572, P822, DOI 10.1007/978-3-642-33718-5_59
NR 63
TC 21
Z9 21
U1 0
U2 11
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2019
VL 35
IS 2
BP 239
EP 256
DI 10.1007/s00371-017-1468-4
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN3DS
UT WOS:000460064600007
DA 2024-07-18
ER

PT J
AU Dong, YS
   Feng, JW
   Yang, CL
   Wang, XH
   Zheng, LT
   Pu, JX
AF Dong, Yongsheng
   Feng, Jinwang
   Yang, Chunlei
   Wang, Xiaohong
   Zheng, Lintao
   Pu, Jiexin
TI Multi-scale counting and difference representation for texture
   classification
SO VISUAL COMPUTER
LA English
DT Article
DE Texture classification; Multi-scale analysis; Texture representation;
   Differential excitation vector; Local counting vector
ID TRANSFORM; PATTERNS; MODEL
AB Multi-scale analysis has been widely used for constructing texture descriptors by modeling the coefficients in transformed domains. However, the resulting descriptors are not robust to the rotated textures when performing texture classification. To alleviate this problem, we in this paper propose a multi-scale counting and difference representation (CDR) of image textures for texture classification. Particularly, we first extract a single-scale CDR feature consisting of the local counting vector (LCV) and the differential excitation vector (DEV). The LCV is established to capture different types of textural structures using the discrete local counting projection, while the DEV is used to describe the difference information of textures in accordance with the differential excitation projection. Finally, the multi-scale CDR feature of a texture image is constructed by combining CDRs at different scales. Experimental results on Brodatz, VisTex, and Outex databases demonstrate that our proposed multi-scale CDR-based texture classification method outperforms five representative texture classification methods.
C1 [Dong, Yongsheng; Feng, Jinwang; Yang, Chunlei; Wang, Xiaohong; Zheng, Lintao; Pu, Jiexin] Henan Univ Sci & Technol, Sch Informat Engn, 263 Kaiyuan Ave, Luoyang 471023, Peoples R China.
   [Feng, Jinwang] Zhengzhou Tech Coll, Dept Software Engn, 81 Zhengshang Rd, Zhengzhou 450121, Henan, Peoples R China.
C3 Henan University of Science & Technology
RP Feng, JW (corresponding author), Henan Univ Sci & Technol, Sch Informat Engn, 263 Kaiyuan Ave, Luoyang 471023, Peoples R China.; Feng, JW (corresponding author), Zhengzhou Tech Coll, Dept Software Engn, 81 Zhengshang Rd, Zhengzhou 450121, Henan, Peoples R China.
EM jinwangfeng11@163.com
RI Feng, Jinwang/J-1819-2016; Chunlei, Yang/KJL-7321-2024
CR Amirolad A, 2016, VISUAL COMPUT, V32, P1633, DOI 10.1007/s00371-016-1220-5
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Choy SK, 2008, IEEE T IMAGE PROCESS, V17, P1399, DOI 10.1109/TIP.2008.925370
   Dash KS, 2015, IET IMAGE PROCESS, V9, P874, DOI 10.1049/iet-ipr.2015.0146
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   Dong Y., NEUROCOMPUTING, V116, P157
   Dong YS, 2015, IEEE T CYBERNETICS, V45, P358, DOI 10.1109/TCYB.2014.2326059
   Dong YS, 2012, IEEE T IMAGE PROCESS, V21, P909, DOI 10.1109/TIP.2011.2168231
   Dong YS, 2011, IEEE SIGNAL PROC LET, V18, P247, DOI 10.1109/LSP.2011.2111369
   Du H, 2016, VISUAL COMPUT, V32, P1537, DOI 10.1007/s00371-015-1138-3
   Feng JW, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P233, DOI 10.1109/ICInfA.2015.7279291
   Garnavi R, 2012, IEEE T INF TECHNOL B, V16, P1239, DOI 10.1109/TITB.2012.2212282
   Giachetti A, 2016, VISUAL COMPUT, V32, P693, DOI 10.1007/s00371-016-1234-z
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017
   Hegenbart S, 2015, PATTERN RECOGN, V48, P2633, DOI 10.1016/j.patcog.2015.02.024
   Ji H, 2013, IEEE T IMAGE PROCESS, V22, P286, DOI 10.1109/TIP.2012.2214040
   Lategahn H, 2010, IEEE T IMAGE PROCESS, V19, P1548, DOI 10.1109/TIP.2010.2042100
   Li L, 2010, IEEE T IMAGE PROCESS, V19, P1371, DOI 10.1109/TIP.2010.2041414
   Li Z, 2012, IEEE T IMAGE PROCESS, V21, P2130, DOI 10.1109/TIP.2011.2173697
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Lin CH, 2012, IET IMAGE PROCESS, V6, P822, DOI 10.1049/iet-ipr.2011.0445
   Liu F, 2013, NEUROCOMPUTING, V120, P325, DOI 10.1016/j.neucom.2012.06.061
   Liu L, 2014, IEEE T IMAGE PROCESS, V23, P3071, DOI 10.1109/TIP.2014.2325777
   Liu L, 2012, IEEE T PATTERN ANAL, V34, P574, DOI 10.1109/TPAMI.2011.145
   Luo GL, 2016, VISUAL COMPUT, V32, P243, DOI 10.1007/s00371-015-1178-8
   Ma MY, 2016, VISUAL COMPUT, V32, P1223, DOI 10.1007/s00371-015-1158-z
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pi MH, 2006, IEEE T IMAGE PROCESS, V15, P3078, DOI 10.1109/TIP.2006.877509
   Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450
   Pun CM, 2003, IEEE T PATTERN ANAL, V25, P590, DOI 10.1109/TPAMI.2003.1195993
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Ren JF, 2013, IEEE T IMAGE PROCESS, V22, P4049, DOI 10.1109/TIP.2013.2268976
   Ryu J, 2015, IEEE T IMAGE PROCESS, V24, P2254, DOI 10.1109/TIP.2015.2419081
   Selvan S, 2007, IEEE T IMAGE PROCESS, V16, P2688, DOI 10.1109/TIP.2007.908082
   Shi YJ, 2015, VISUAL COMPUT, V31, P1191, DOI 10.1007/s00371-014-1005-7
   Subrahmanyam M., 2011, IEEE T IMAGE PROCESS, V21, P510
   Takallou HM, 2014, IET IMAGE PROCESS, V8, P300, DOI 10.1049/iet-ipr.2013.0003
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Yongsheng Dong, 2011, Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence. 7th International Conference, ICIC 2011. Revised Selected Papers, P421, DOI 10.1007/978-3-642-25944-9_54
   Zhang J, 2013, IEEE T IMAGE PROCESS, V22, P31, DOI 10.1109/TIP.2012.2214045
   Zhao GY, 2012, IEEE T IMAGE PROCESS, V21, P1465, DOI 10.1109/TIP.2011.2175739
   Zhao L, 2016, VISUAL COMPUT, V32, P1165, DOI 10.1007/s00371-015-1169-9
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 44
TC 19
Z9 19
U1 4
U2 28
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD OCT
PY 2018
VL 34
IS 10
BP 1315
EP 1324
DI 10.1007/s00371-017-1415-4
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR0KK
UT WOS:000442204400004
DA 2024-07-18
ER

PT J
AU Torres, BS
   Pedrini, H
AF Torres, Berthin S.
   Pedrini, Helio
TI Detection of complex video events through visual rhythm
SO VISUAL COMPUTER
LA English
DT Article
DE Visual rhythm; Spatio-temporal features; Abnormal event detection; Human
   action classification
ID ACTION RECOGNITION; SURVEILLANCE
AB The recognition of complex events in videos has currently several important applications, particularly due to the wide availability of digital cameras in environments such as airports, train and bus stations, shopping centers, stadiums, hospitals, schools, buildings, roads, among others. Advances in digital technology have enhanced the capabilities for detection of video events through the development of devices with high resolution, small physical size, and high sampling rates. This work presents and evaluates the use of feature descriptors extracted from visual rhythms of video sequences in three computer vision problems: abnormal event detection, human action classification, and gesture recognition. Experiments conducted on well-known public datasets demonstrate that the method produces promising results.
C1 [Torres, Berthin S.; Pedrini, Helio] Univ Estadual Campinas, Inst Comp, BR-13083852 Campinas, SP, Brazil.
C3 Universidade Estadual de Campinas
RP Pedrini, H (corresponding author), Univ Estadual Campinas, Inst Comp, BR-13083852 Campinas, SP, Brazil.
EM berthin@liv.ic.unicamp.br; helio@ic.unicamp.br
RI SILVA, EDUARDO/IQS-1403-2023; Pedrini, Helio/A-7556-2012
FU FAPESP; CNPq; CAPES
FX The authors are grateful to FAPESP, CNPq and CAPES for the financial
   support.
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Alcantara Marlon F., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2917, DOI 10.1109/ICASSP.2014.6854134
   Alcantara MF, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.1.013020
   [Anonymous], J SCI TECHNOL
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], ADV COMPUTER VISION
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], INT C IM ANAL PROC
   [Anonymous], TECH REP
   [Anonymous], 2013, COMPUTER VISION PATT
   [Anonymous], THESIS
   [Anonymous], IB C PATT REC
   [Anonymous], IEEE COMP SOC C COMP
   [Anonymous], 1981 TECHN S E
   [Anonymous], 2013, J MATER CHEM B
   [Anonymous], 3 IEEE WORKSH COMP V
   [Anonymous], USING VARIATIONS SHA
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], P INT C COMP VIS
   [Anonymous], IEEE INT C IM PROC
   [Anonymous], INT C IM PROC
   [Anonymous], BRIT MACH VIS C
   [Anonymous], DISPL SCR EQ HLTH SA
   [Anonymous], PAC RIM S IM VID TEC
   [Anonymous], 2013, IJCAI
   Antonucci A, 2015, INT J APPROX REASON, V56, P249, DOI 10.1016/j.ijar.2014.07.005
   Berent J, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P182, DOI 10.1109/MMSP.2006.285293
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Blackburn J, 2007, LECT NOTES COMPUT SC, V4814, P285
   Bourke AK, 2007, GAIT POSTURE, V26, P194, DOI 10.1016/j.gaitpost.2006.09.012
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Calderara S, 2011, COMPUT VIS IMAGE UND, V115, P1099, DOI 10.1016/j.cviu.2011.03.003
   Candamo J, 2010, IEEE T INTELL TRANSP, V11, P206, DOI 10.1109/TITS.2009.2030963
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chen DY, 2011, J VIS COMMUN IMAGE R, V22, P178, DOI 10.1016/j.jvcir.2010.12.004
   Cong Y, 2013, PATTERN RECOGN, V46, P1851, DOI 10.1016/j.patcog.2012.11.021
   da Silva Pinto A., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P221, DOI 10.1109/SIBGRAPI.2012.38
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dee HM, 2008, MACH VISION APPL, V19, P329, DOI 10.1007/s00138-007-0077-z
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fanello SR, 2013, J MACH LEARN RES, V14, P2617
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fathi A., 2008, IEEE C COMPUTER VISI
   Fawzy F, 2013, MIDWEST SYMP CIRCUIT, P1310, DOI 10.1109/MWSCAS.2013.6674896
   Fortun Denis, 2015, Computer Vision and Image Understanding, V134, P1, DOI 10.1016/j.cviu.2015.02.008
   Gong SG, 2011, VISUAL ANALYSIS OF BEHAVIOUR: FROM PIXELS TO SEMANTICS, P301, DOI 10.1007/978-0-85729-670-2_14
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622
   Hung TY, 2013, IEEE INT SYMP CIRC S, P2844, DOI 10.1109/ISCAS.2013.6572471
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Jiang XB, 2014, VISUAL COMPUT, V30, P1021, DOI 10.1007/s00371-014-0923-8
   Jie Feng, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3599, DOI 10.1109/ICPR.2010.878
   Ke Y, 2005, IEEE I CONF COMP VIS, P166
   Kliper-Gross O, 2012, IEEE T PATTERN ANAL, V34, P615, DOI 10.1109/TPAMI.2011.209
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Lili Cui, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P362, DOI 10.1109/CISP.2011.6099933
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Nalwa V.S., 1993, GUIDED TOUR COMPUTER
   Nam Y, 2014, MULTIMED TOOLS APPL, V72, P3001, DOI 10.1007/s11042-013-1593-7
   Ngo CW, 2003, IEEE T IMAGE PROCESS, V12, P341, DOI 10.1109/TIP.2003.809020
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Niebles JuanCarlos., 2007, IEEE C COMPUTER VISI
   NIYOGI SA, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P469, DOI 10.1109/CVPR.1994.323868
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ozturk Ovgu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3533, DOI 10.1109/ICPR.2010.862
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Piciarelli C, 2006, PATTERN RECOGN LETT, V27, P1835, DOI 10.1016/j.patrec.2006.02.004
   Raja K, 2011, IEEE IMAGE PROC, P25, DOI 10.1109/ICIP.2011.6116197
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Schindler K., 2008, IEEE C COMPUTER VISI, P1
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Smola AJ, 1999, ADV NEUR IN, V11, P585
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255
   Suriani NS, 2013, SENSORS-BASEL, V13, P9966, DOI 10.3390/s130809966
   Tang X, 2013, IEEE IMAGE PROC, P3602, DOI 10.1109/ICIP.2013.6738743
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Thida M, 2013, IEEE T CYBERNETICS, V43, P2147, DOI 10.1109/TCYB.2013.2242059
   Tung PT, 2014, P 5 S INF COMM TECHN, P186
   Valio FB, 2011, LECT NOTES COMPUT SC, V7042, P157, DOI 10.1007/978-3-642-25085-9_18
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wallace E., 1998, CCTV: Making it work: CCTV control room ergonomics
   Wang SQ, 2009, IEEE IMAGE PROC, P1121, DOI 10.1109/ICIP.2009.5414532
   Weilong Yang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P482, DOI 10.1109/ICCVW.2009.5457663
   Wong Shu-Fai, 2007, IEEE 11 INT C COMPUT, P1
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yu MY, 2016, IEEE T PATTERN ANAL, V38, P1651, DOI 10.1109/TPAMI.2015.2491925
   ZACK GW, 1977, J HISTOCHEM CYTOCHEM, V25, P741, DOI 10.1177/25.7.70454
   Zhang YH, 2012, IEEE IMAGE PROC, P2689, DOI 10.1109/ICIP.2012.6467453
NR 99
TC 9
Z9 9
U1 0
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD FEB
PY 2018
VL 34
IS 2
BP 145
EP 165
DI 10.1007/s00371-016-1321-1
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU6ZO
UT WOS:000424001800002
DA 2024-07-18
ER

PT J
AU Pan, JJ
   Yan, SZ
   Qin, H
   Hao, AM
AF Pan, Junjun
   Yan, Shizeng
   Qin, Hong
   Hao, Aimin
TI Real-time dissection of organs via hybrid coupling of geometric
   metaballs and physics-centric mesh-free method
SO VISUAL COMPUTER
LA English
DT Article
DE Metaballs; Mesh-free method; Digital organ; Physics-based deformation;
   Dissection
ID SIMULATION; DEFORMATION; CUTS
AB This paper systematically describes a real-time dissection approach for digital organs by strong coupling of geometric metaballs and physically correct mesh-free method. For organ geometry, we employ a novel hybrid model comprising both inner metaballs and high-resolution surface mesh with texture information. Through the use of metaballs, the organ interior is geometrically simplified via a set of overlapping spheres with different radii. As for digital organ's physical representation, we systematically articulate a hybrid framework to interlink metaballs with physics-driven mesh-free method based on moving least squares (MLS) shape functions. MLS approach enables the direct and rapid transition from metaball geometry to local nodal formulations, which afford potential-energy-correct physical modeling and simulation over continuum domain with physical accuracy. For soft tissue dissection, the nature of our MLS-driven mesh-free method also facilitates adaptive topology modification and cutting surface reconstruction. To expedite simulation towards real-time performance, at the numerical level, we resort to position-based dynamics to simplify physical deformation to drive metaballs participating in the mesh-free formulation. Since nodal points participating in the physical process exist temporarily only in localized regions adjacent to the cutting path, our method could warrant accurate cutting surface without sacrificing real-time computational efficiency. This hybrid dissection technique has already been integrated into a VR-based laparoscopic surgery simulator with a haptic interface.
C1 [Pan, Junjun; Yan, Shizeng; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Pan, JJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM pan_junjun@hotmail.com; qin@cs.stonybrook.edu
RI Pan, Junjun/A-1316-2013
FU National Natural Science Foundation of China [61402025, 61532002,
   61672149]; National Science Foundation of USA [IIS-0949467, 1047715,
   1049448]; Fundamental Research Funds for the Central Universities;
   Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1047715] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1049448] Funding Source: National Science
   Foundation
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 61402025, 61532002 and 61672149), the National Science
   Foundation of USA (Nos. IIS-0949467, 1047715, and 1049448), and the
   Fundamental Research Funds for the Central Universities.
CR Adams B., 2009, EUROGRAPHICS 2009
   Bender J, 2014, COMPUT GRAPH FORUM, V33, P228, DOI 10.1111/cgf.12346
   Bradshaw G., 2002, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P33, DOI DOI 10.1145/545261.545267
   Chung T.J., 1996, APPL CONTINUUM MECH
   Cueto E., 2014, Advanced Modeling and Simulation in Engineering Sciences, V1, P1, DOI DOI 10.1186/2213-7467-1-11
   Eberhard P, 2013, COMPUT MECH, V51, P261, DOI 10.1007/s00466-012-0720-z
   France L., 2002, ESAIM P NOV 12, P42
   Gianluca D. N., 2010, INT C APPL BION BIOM
   Jerábková L, 2010, PROG BIOPHYS MOL BIO, V103, P217, DOI 10.1016/j.pbiomolbio.2010.09.012
   Jones B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2560795
   Kallmann M., 2011, SERIES MATH VISUALIZ, P241
   Li X, 2009, IEEE T AUTOM SCI ENG, V6, P409, DOI 10.1109/TASE.2009.2014735
   Liu T., 2013, ACM T GRAPH SIGGRAPH, V32, p[209, 1], DOI DOI 10.1145/2508363.2508406
   Macklin M, 2014, ACM T GRAPHIC, V33, DOI [10.1145/280/109/2601152, 10.1145/2601097.2601152]
   Muller M., 2004, P 2004 ACM SIGGRAPHE, P141, DOI [DOI 10.1145/1028523.1028542, 10.1145/1028523.1028542, 10]
   Pan JJ, 2015, INT J MED ROBOT COMP, V11, P194, DOI 10.1002/rcs.1582
   Pan JJ, 2015, COMPUT ANIMAT VIRT W, V26, P321, DOI 10.1002/cav.1655
   Pan JJ, 2015, VISUAL COMPUT, V31, P947, DOI 10.1007/s00371-015-1106-y
   Pan JJ, 2009, COMPUT ANIMAT VIRT W, V20, P121, DOI 10.1002/cav.284
   Pauly M., ACM T GRAPH, V24, P957
   Pietroni N, 2009, VISUAL COMPUT, V25, P227, DOI 10.1007/s00371-008-0216-1
   Rivera-Rovelo J., PATTERN RECOGNIT, V40, P171
   Sorkine-Hornung O., 2004, EUR S GEOM PROC, P1
   Steinemann D., 2006, PROC ACM SIGGRAPHEUR, P63
   Wei Y., 2012, ANEURYSM, DOI DOI 10.5772/48635
   Wu J, 2015, COMPUT GRAPH FORUM, V34, P161, DOI 10.1111/cgf.12528
   Wu J, 2013, VISUAL COMPUT, V29, P739, DOI 10.1007/s00371-013-0810-8
   Yang C, 2014, COMPUT ANIMAT VIRT W, V25, P423, DOI 10.1002/cav.1594
NR 28
TC 21
Z9 23
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
PD JAN
PY 2018
VL 34
IS 1
BP 105
EP 116
DI 10.1007/s00371-016-1317-x
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR5XF
UT WOS:000419139200011
DA 2024-07-18
ER

EF